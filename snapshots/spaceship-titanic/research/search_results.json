{
  "query": "What are the best techniques for handling missing values in tabular data for Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Missing values are a common source of noise in Kaggle tabular competitions because many real\u2011world datasets contain incomplete records, and how you treat them can dramatically affect model performance.  Proper handling prevents information loss, avoids bias, and lets tree\u2011based learners (e.g., LightGBM, CatBoost) exploit the \u201cmissingness\u201d itself as a signal\u202f([BrettRomero](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/)).  \n\nThe most effective toolkit combines simple, model\u2011aware, and advanced imputation steps:  \n\n* **Basic numeric imputation** \u2013 replace with the column mean, median, or a constant (e.g., \u2013999) and add a binary \u201cmissing\u2011indicator\u201d feature so the model can learn whether the absence is informative\u202f([BrettRomero](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/)).  \n* **Categorical handling** \u2013 fill with the most frequent category or a dedicated \u201cunknown\u201d token, then use **categorical embeddings** to let the model learn a dense representation of both known and missing levels\u202f([Deep\u2011Learning\u2011Mastery](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-1/)).  \n* **Model\u2011native support** \u2013 LightGBM, CatBoost and XGBoost can split on NaN values directly, so you often keep the raw missing entries and let the algorithm decide the optimal split\u202f([BrettRomero](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/)).  \n* **Advanced imputation** \u2013 iterative (MICE) or K\u2011Nearest\u2011Neighbors imputation captures multivariate relationships, while group\u2011wise or target\u2011based filling (e.g., median per user ID) preserves local patterns\u202f([Deep\u2011Learning\u2011Mastery](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-1/)).  \n* **Feature engineering** \u2013 treat the proportion of missing values in a row as a feature, or drop columns/rows with >\u202f50\u202f% missingness when they add little signal and only increase noise\u202f([BrettRomero](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/)).  \n\nThese techniques are demonstrated in many competition kernels and tutorials (e.g., the \u201cHandling Missing Values\u201d video series) and together form a robust pipeline that many top Kaggle solutions rely on\u202f([YouTube](https://www.youtube.com/watch?v=EYySNJU8qR0)).",
      "url": ""
    },
    {
      "title": "Handling Missing Values (with Rob Mulla)",
      "text": "In this tutorial, we will know all about handling missing values in tabular data. This video is part of the applied ml competition series. If you want to join the song popularity prediction competition, join by clicking this link: https://www.kaggle.com/t/df24a2b9ddc9...\n| view_count: 9,412 views | short_view_count: 9.4K views | num_likes: None | num_subscribers: 115 thousand",
      "url": "https://www.youtube.com/watch?v=EYySNJU8qR0"
    },
    {
      "title": "Advanced Missing Value Analysis in Tabular Data, Part 1",
      "text": "# Advanced Missing Value Analysis in Tabular Data, Part 1\n\nComprehensive techniques for visualizing and handling missing values in large datasets. The article demonstrates advanced strategies for filling missing values and employing categorical embeddings.\n\nTags:\nadvanced-missing-value-analysis\ndata-cleaning-techniques\nfastai\ncategorical-embeddings\ndata-imputation\n\nCategory:\nTabular Data\n\n# Series: Kaggle Competition - Deep Dive Tabular Data\n\n[**Advanced Missing Value Analysis in Tabular Data, Part 1**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-1/)\n\n[**Decision Tree Feature Selection Methodology, Part 2**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-2/)\n\n[**RandomForestRegressor Performance Analysis, Part 3**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-3/)\n\n[**Statistical Interpretation of Tabular Data, Part 4**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-4/)\n\n[**Addressing the Out-of-Domain Problem in Feature Selection, Part 5**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-5/)\n\n[**Kaggle Challenge Strategy: RandomForestRegressor and Deep Learning, Part 6**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-6/)\n\n[**Hyperparameter Optimization in Deep Learning for Kaggle, Part 7**](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-7/)\n\nThis series documents the process of importing raw tabular data from a CSV file,\nto submitting the final predictions on the Kaggle test set for the competition.\n\n## Complete TOC\n\n**Series: Kaggle Competition - Deep Dive Tabular Data**\n\n- [Deep Dive Tabular Data Pt. 1: Introduction](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-1/)\n\n```\n  - Imports For The Series\n  - Importing The Flat Files\n  - Looking At The Training Dataset\n    - Columns 0:9 - Observations about the data\n    - Columns 10:19\n    - Columns 20:29\n    - Columns 30:39\n    - Columns 40:49\n    - Columns 50:59\n    - Columns 60:69\n    - Columns 70:80\n  - Visualizing Missing Values\n    - Split DataFrame By Columns\n    - Handling Missing Values\n  - Categorical Embeddings\n    - Find Candidate Columns\n    - Criteria\n    - Unique Values\n    - The Transformation\n  - Final Preprocessing: TabularPandas\n    - Explanation Of Parameter Choices\n      - procs\n\n```\n\n- [Decision Tree Feature Selection Methodology, Part 2](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-2/)\n\n```\n    - Train & Validation Splits\n    - Sklearn DecisionTreeRegressor\n    - Theory\n    - Feature Importance Metric Deep Dive Experiment\n\n```\n\n- [RandomForestRegressor Performance Analysis, Part 3](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-3/)\n\n```\n    - RandomForestRegressor (RFR) For Interpretation\n      - Create Root Mean Squared Error Metric For Scoring Models\n      - Custom Function To Create And Fit A RFR Estimator\n      - RFR - Theory\n      - RFR - Average RMSE By Number Of Estimators\n    - Out-Of-Bag Error Explained\n    - RFR: Standard Deviation Of RMSE By Number Of Estimators\n      - Visualization Using Swarmplot\n      - Feature Importances And Selection Using RFR\n      - New Training Set xs_imp After Feature Elimination\n      - Interpretation Of RMSE Values\n\n```\n\n- [Statistical Interpretation of Tabular Data, Part 4](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-4/)\n\n```\n    - Dendrogram Visualization For Spearman Rank Correlations\n      - Conclusion\n    - Dendrogram Findings Applied\n      - Drop One Feature At A Time\n      - Drop Both Features Together\n      - Evaluate oob_scores\n    - New Train & Validation Sets Using Resulting Feature Set\n      - Baseline RMSE Scores\n  - Exploring The Impact of Individual Columns\n  - Partial Dependence\n  - Tree Interpreter\n\n```\n\n- [Addressing the Out-of-Domain Problem in Feature Selection, Part 5](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-5/)\n\n```\n    - Out-Of-Domain Problem\n    - Identifying Out-Of-Domain Data\n\n```\n\n- [Kaggle Challenge Strategy: RandomForestRegressor and Deep Learning, Part 6](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-6/): Creation Of The Kaggle Submission\n\n```\n    - Creating Estimators Optimized For Kaggle\n    - RandomForestRegressor Optimization\n      - Final RandomForestRegressor RMSE Values\n    - tabular_learner - Deep Learning Model\n      - Testing Of Different Values For Parameter max_card\n      - Run TabularPandas Function\n      - Create Dataloaders Object\n      - Create tabular_learner estimator\n    - Preprocessing Of The Kaggle Test Dataset\n\n```\n\n- [Hyperparameter Optimization in Deep Learning for Kaggle, Part 7](https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-7/): Optimization Routines & Final Submission\n\n```\n    - tabular_learner Optimization\n    - XGBRegressor Optimization\n    - Three Model Ensemble\n    - Kaggle Submission\n\n```\n\n## Part 1: Introduction\n\nThe name of the Kaggle competition the data is from and the submission is\nsubmitted to is the [_House Prices - Advanced Regression Techniques_\\\n_Competition_](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).\nThe data can be found [_here_](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data).\n\n- Number of participants in the competition (2023-01-09): **4345**\n- Current position on the leader board (2023-01-09): **39**\n- Problem type: **Regression**\n- Number of samples in the training set: **1460**\n- Number of samples in the test set: **1460**\n- Number of independent variables: **80**\n- Number of dependent variables: **1**\n- Submission scored on the natural Logarithm of the dependent variable.\n\n![](https://deep-learning-mastery.com/assets/img/tabular-deep-dive-series/p.png)\n\nCompetition Name: House Prices - Advanced Regression Techniques.\n\nPosition on the leader board, as of 2023-01-09.\n\n## Imports For The Series\n\n```\nfrom dtreeviz.trees import *\nfrom fastai.tabular.all import *\nfrom itertools import product\nfrom numpy.core._operand_flag_tests import inplace_add\nfrom pandas.api.types import is_categorical_dtype, is_numeric_dtype, is_string_dtype\nfrom pathlib import Path\nfrom scipy.cluster import hierarchy as hc\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import plot_partial_dependence\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import (\n    KFold, RandomizedSearchCV,\n)\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom subprocess import check_output\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\nfrom xgboost import XGBRegressor\nimport janitor\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport scipy\nimport seaborn as sns\nimport torch\nimport warnings\n\n```\n\n```\nplt.style.use(\"science\")\nwarnings.simplefilter(\"ignore\", FutureWarning)\nwarnings.simplefilter(\"ignore\", UserWarning)\npd.options.display.max_rows = 6  # display fewer rows for readability\npd.options.display.max_columns = 10  # display fewer columns for readability\nplt.ion() # make plt interactive\nseed = 42\n\n```\n\n## Importing The Flat Files\n\nThe train and validation dataset from kaggle is imported and the header names\nare cleaned. In this case, the headers have camel case words. While this can be\ngood for readability, it is of no use for machine learning and is therefore\nchanged to all lowercase instead.\n\n```\nbase = check_output([\"zsh\", \"-c\", \"echo $UKAGGLE\"]).decode(\"utf-8\")[:-1]\ntraind = base + \"/\" + \"my_competitions/kaggle_competition_house_prices/data/train.csv\"\ntestd = base + \"/\" + \"my_competitions/kaggle_competition_house_prices/data/test.csv\"\ntrain = pd.read_csv(traind, low_memory=False).clean_names()\nvalid = pd.read_csv(testd, low_memory=False).clean_names()\n\n```\n\nKaggle evaluates all submissions, after the dependent variable `saleprice` is\ntran...",
      "url": "https://deep-learning-mastery.com/projects/deep-dive-tabular-data-pt-1"
    },
    {
      "title": "Data Science: A Kaggle Walkthrough \u2013 Cleaning Data",
      "text": "Data Science: A Kaggle Walkthrough \u2013Cleaning Data &#8211; Brett Romero[Skip to the content](#site-content)\n[Brett Romero](https://brettromero.com)\nData Inspired Insights\nToggle mobile menu\nToggle search field\nSearch for:\n* [Apps](https://brettromero.com/apps/)\n* [Stats Calculator](https://statscalculator.brettromero.com)\n* [JSONify It](http://jsonifyit.com)\n* [Visual Analytics](http://visualanalyticsapp.com)\n* [Open Flashcards](http://openflashcards.com)\n* [Visualizations](https://brettromero.com/visualizations/)\n* [The Narrabundah Files](https://brettromero.com/category/the-narrabundah-files/)\n* [2009 Season](https://brettromero.com/category/the-narrabundah-files/2009-season/)\n* [2010 Season](https://brettromero.com/category/the-narrabundah-files/2010-season/)\n* [About Me](https://brettromero.com/13-2/)\n* [Buy me a coffee](https://www.buymeacoffee.com/uRjE5U8QH)\n* [Apps](https://brettromero.com/apps/)\n* [Stats Calculator](https://statscalculator.brettromero.com)\n* [JSONify It](http://jsonifyit.com)\n* [Visual Analytics](http://visualanalyticsapp.com)\n* [Open Flashcards](http://openflashcards.com)\n* [Visualizations](https://brettromero.com/visualizations/)\n* [The Narrabundah Files](https://brettromero.com/category/the-narrabundah-files/)\n* [2009 Season](https://brettromero.com/category/the-narrabundah-files/2009-season/)\n* [2010 Season](https://brettromero.com/category/the-narrabundah-files/2010-season/)\n* [About Me](https://brettromero.com/13-2/)\n* [Buy me a coffee](https://www.buymeacoffee.com/uRjE5U8QH)\n[![cleaning data](https://brettromero.com/wp-content/uploads/2016/03/data-clean_775x425_d79.jpg)](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/)# Data Science: A Kaggle Walkthrough \u2013Cleaning Data\n[March 14, 2016](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/)/[Brett Romero](https://brettromero.com/author/bromero/)/[3 Comments](https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data/#comments)\n*This article on cleaning data is Part III in a series looking at data science and machine learning by walking through a Kaggle competition. If you have not done so already, it is recommended that you go back and read[Part I](http://brettromero.com/wordpress/data-science-a-kaggle-walkthrough-introduction/)and[Part II](http://brettromero.com/wordpress/data-science-a-kaggle-walkthrough-understanding-the-data/).*\nIn this part we will focus on cleaning the data provided for the Airbnb Kaggle competition.\n## CleaningData\nWhen we talk about cleaning data, what exactly are we talking about? Generally when people talk about cleaning data, there are a few specific things they are referring to:\n1. **Fixing up formats**\u2013 Often when data is saved or translated from one format to another (for example in our case from CSV to Python), some data may not be translated correctly. We saw a good example of this in the last article in*csv*. The*timestamp\\_first\\_active*column contained numbers like 20090609231247 instead of timestamps in the expected format: 2009-06-09 23:12:47. A typical job when it comes to cleaning data is correcting these types of issues.\n2. **Filling in missing values**\u2013 As we also saw in Part II, it is quite common for some values to be missing from datasets. This typically means that a piece of information was simply not collected. There are several options for handling missing data that will be covered below.\n3. **Correcting erroneous values**\u2013 For some columns, there are values that can be identified as obviously incorrect. This may be a \u2018gender\u2019 column where someone has entered a number, or an \u2018age\u2019 column where someone has entered a value well over 100. These values either need to be corrected (if the correct value can be determined) or assumed to be missing.\n4. **Standardizing categories**\u2013 More of a subcategory of \u2018correcting erroneous values\u2019, this type of data cleansing is so common it is worth special mention. In many (all?) cases where data is collected from users directly \u2013particularly using free text fields \u2013spelling mistakes, language differences or other factors will result in a given answer being provided in multiple ways. For example, when collecting data on country of birth, if users are not provided with a standardized list of countries, the data will inevitably contain multiple spellings of the same country (e.g. USA, United States, U.S. and so on). One of the main cleaning tasks often involves standardizing these values to ensure that there is only one version of each value.## Options for Dealing with Missing Data\nMissing data in general is one of the trickier issues that is dealt with when cleaning data. Broadly there are two solutions:\n### 1. Deleting/Ignoring rows with missing values\nThe simplest solution available when faced with missing values is to not use the records with missing values when training your model. However, there are some issues to be aware of before you starting deleting masses of rows from your dataset.\nThe first is that this approach only makes sense if the number of rows with missing data is relatively small compared to the dataset. If you are finding that you will be deleting more than around 10% of your dataset due to rows having missing values, you may need to reconsider.\nThe second issue is that in order to delete the rows containing missing data, you have to be confident that the rows you are deleting do not contain information that is not contained in other rows. For example, in the current Airbnb dataset we have seen that many users have not provided their age. Can we assume that the people who chose not to provide their age are the same as the users who did? Or are they likely to represent a different type of user, perhaps an older and more privacy conscious user, and therefore a user that is likely to make different choices on which countries to visit? If the answer is the latter, we probably do not want to just delete the records.\n### 2. Filling in the Values\nThe second broad option for dealing with missing data is to fill the missing values with a value. But what value to use? This depends on a range of factors, including the type of data you are trying to fill.\nIf the data is categorical (i.e. countries, device types, etc.), it may make sense to simply create a new category that will represent \u2018unknown\u2019. Another option may be to fill the values with the most common value for that column (the mode). However, because these are broad methods for filling the missing values, this may oversimplify your data and/or make your final model less accurate.\nFor numerical values (for example the*age*column) there are some other options. Given that in this case using the mode to fill values makes less sense, we could instead use the mean or median. We could even take an average based on some other criteria \u2013for example filling the missing*age*values based on an average age for users that selected the same*country\\_destination*.\nFor both types of data (categorical and numerical), we can also use far more complicated methods to impute the missing values. Effectively, we can use a similar methodology that we are planning to use to predict the*country\\_destination*to predict the values in any of the other columns, based on the columns that do have data. And just like with modeling in general, there are an almost endless number of ways this can be done, which won\u2019t be detailed here. For more information on this topic, the[orange Python library](http://docs.orange.biolab.si/reference/rst/Orange.feature.imputation.html)provides some excellent documentation.\n## Step by Step\nWith that general overview out of the way, let\u2019s start cleaning the Airbnb data. In relation to the datasets provided for the Airbnb Kaggle competition, we will focus our cleaning efforts on two files \u2013*train\\_users\\_2.csv*and*test\\_users.csv*and leave aside*sessions.csv*.\n### Loading in the Data\nThe first step is to load the data from the CSV files using Python. To do this we will use the ...",
      "url": "https://brettromero.com/data-science-kaggle-walkthrough-cleaning-data"
    },
    {
      "title": "A Comprehensive guide to handle Missing Values",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=7e2e298babe1403df788:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/twinkle0705/a-comprehensive-guide-to-handle-missing-values"
    },
    {
      "title": "Kaggle Exercise: Missing Values",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [Kaggle Exercise: Missing Values](https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked4 years, 11 months ago\n\nModified [4 years, 3 months ago](https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values?lastactivity)\n\nViewed\n1k times\n\n1\n\nI am trying to submit my solution to [Exercise: Missing Values](https://www.kaggle.com/learn/intermediate-machine-learning) but getting the following error after submission\n\nERROR: Unable to find 1459 required key values in the 'Id' column\n\nERROR: Unable to find the required key value '1461' in the 'Id' column\n.\n.\n.\n\nERROR: Unable to find the required key value '1469' in the 'Id' column\n\nThe thing is that the test dataset only has 1459 rows, from the error it seems like validation set expecting more entries?\n\n- [artificial-intelligence](https://stackoverflow.com/questions/tagged/artificial-intelligence)\n- [linear-regression](https://stackoverflow.com/questions/tagged/linear-regression)\n- [random-forest](https://stackoverflow.com/questions/tagged/random-forest)\n- [kaggle](https://stackoverflow.com/questions/tagged/kaggle)\n\n[Share](https://stackoverflow.com/q/56893175)\n\n[Improve this question](https://stackoverflow.com/posts/56893175/edit)\n\nFollow\n\nasked Jul 4, 2019 at 19:02\n\n[![madan's user avatar](https://www.gravatar.com/avatar/1ae705d65cb63a823c9a21873e9f77ad?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/3359111/madan)\n\n[madan](https://stackoverflow.com/users/3359111/madan) madan\n\n44566 silver badges1616 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n2\n\nIf you go to `output/submission.csv`, you'll notice that the `Id` starts at `0`. Then look at `input/sample_submission.csv` and notice that it's `Id` starts at `1461`, which is what the competition is expecting. The original test data starts at `1461`. What likely happened is that you lost the original `Id` numbers in the test `DataFrame` used to write the output.\nYou can fix it like this:\n\n```\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\n\n```\n\n`X_Test` contains your original `Id` numbers and will ensure your output is correct.\n\n[Share](https://stackoverflow.com/a/60698764)\n\n[Improve this answer](https://stackoverflow.com/posts/60698764/edit)\n\nFollow\n\n[edited Mar 15, 2020 at 23:51](https://stackoverflow.com/posts/60698764/revisions)\n\nanswered Mar 15, 2020 at 23:39\n\n[![Joe Mayo's user avatar](https://www.gravatar.com/avatar/da82c98223ffb47d65daadd500c8908f?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/1454879/joe-mayo)\n\n[Joe Mayo](https://stackoverflow.com/users/1454879/joe-mayo) Joe Mayo\n\n7,50377 gold badges4343 silver badges6161 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values)\u00a0\\|\n\n0\n\nWhen you read the csv file at the beginning look that you put the `index_col='Id'` parameter.\n\n```\ndf_train_full = pd.read_csv(\"/kaggle/input/.../train.csv\", index_col='Id')\n\ndf_test_full = pd.read_csv(\"/kaggle/input/.../test.csv\", index_col='Id')\n\n```\n\n[Share](https://stackoverflow.com/a/58596060)\n\n[Improve this answer](https://stackoverflow.com/posts/58596060/edit)\n\nFollow\n\n[edited Oct 28, 2019 at 17:58](https://stackoverflow.com/posts/58596060/revisions)\n\n[![Dharman's user avatar](https://i.sstatic.net/agMKZ.png?s=64)](https://stackoverflow.com/users/1839439/dharman)\n\n[Dharman](https://stackoverflow.com/users/1839439/dharman) \u2666\n\n32.4k2525 gold badges9494 silver badges140140 bronze badges\n\nanswered Oct 28, 2019 at 17:56\n\n[![Vic's user avatar](https://www.gravatar.com/avatar/d0f0a27a857a8f00b2c33191696803f0?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/12287363/vic)\n\n[Vic](https://stackoverflow.com/users/12287363/vic) Vic\n\n1\n\n1\n\n- I checked but I already have that so this does not seem to be the issue, I can share the whole solution if that will help, please share your kaggle Id.\n\n\u2013\u00a0[madan](https://stackoverflow.com/users/3359111/madan)\n\nCommentedOct 31, 2019 at 9:09\n\n\n[Add a comment](https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values)\u00a0\\|\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack Overflow. [Learn more](https://stackoverflow.com/help/gen-ai-policy)\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f56893175%2fkaggle-exercise-missing-values%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [artificial-intelligence](https://stackoverflow.com/questions/tagged/artificial-intelligence) - [linear-regression](https://stackoverflow.com/questions/tagged/linear-regression) - [random-forest](https://stackoverflow.com/questions/tagged/random-forest) - [kaggle](https://stackoverflow.com/questions/tagged/kaggle)   or [ask your own question](https://stackoverflow.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n- [Policy: Generative AI (e.g., ChatGPT) is banned](https://meta.stackoverflow.com/questions/421831/policy-generative-ai-e-g-chatgpt-is-banned)\n\n- [The return of Staging Ground to Stack Overflow](https://meta.stackoverflow.com/questions/430404/the-return-of-staging-ground-to-stack-overflow)\n\n- [The 2024 Developer Survey Is Live](https://meta.stackoverflow.com/questions/430298/the-2024-developer-survey-is-live)\n\n\n[Visit chat](https://chat.stackoverflow.com/)\n\n#### Related\n\n[37](https://stackoverflow.com/q/9365982) [Missing values in scikits machine learning](https://stackoverflow.com/questions/9365982/missing-values-in-scikits-machine-learning)\n\n[5](https://stackoverflow.com/q/25481066) [Python Sklearn - RandomForest and Missing values](https://stackoverflow.com/questions/25481066/python-sklearn-randomforest-and-missing-values)\n\n[5](https://stackoverflow.com/q/32229427) [Missing value error in the randomForest package of R](https://stackoverflow.com/questions/32229427/missing-value-error-in-the-randomforest-package-of-r)\n\n[6](https://stackoverflow.com/q/35680426) [Missing Value in Data Analysis](https://stackoverflow.com/questions/35680426/missing-value-in-data-analysis)\n\n[5](https://stackoverflow.com/q/39320135) [imputing missing values using a predictive model](https://stackoverflow.com/questions/39320135/imputing-mis...",
      "url": "https://stackoverflow.com/questions/56893175/kaggle-exercise-missing-values"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from ...",
      "text": "[Just released: **2025 State of Foundation Model Training report** \ud83d\udcd5 \u2192\u00a0 Read now](https://neptune.ai/state-of-foundation-model-training-report)\n\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\n\nThese are the five competitions that I have gone through to create this article:\n\n- [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n- [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n- [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n- [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n- [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Faster [data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n- Data compression techniques to [reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n- Optimize the memory by r [educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n- Use [cudf](https://github.com/rapidsai/cudf).\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format.\n- Converting data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format.\n- Reducing memory usage for [optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).\n\n## Data exploration\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- EDA for microsoft [malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n- Time Series [EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n- Complete [EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n- Complete [EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n- EDA for [VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)\n\n## Data preparation\n\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n\n- Methods to t [ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n- Data augmentation by [Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n- Fast inplace [shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n- Finding [synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n- [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising) used in signal processing competitions.\n- Finding [patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n- Methods to handle [missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n- An overview of various [encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n- Building [model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n- Random [shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works) to create new synthetic training set.\n\n## Feature engineering\n\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n\n- Target [encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/) for better encoding.\n- Entity embedding to [handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n- Encoding c [yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n- Manual [feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n- Automated feature engineering techniques [using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n- Top hard crafted features used in [microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n- Denoising NN for [feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n- Feature engineering [using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n- Things to remember while processing f [eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n- [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n- [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567) for dimensionality reduction.\n- LDA for [dimensionality reduction](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567).\n- Best hand crafted LGBM features for [microsoft malware detection](https://www.kaggle.com/c/microsoft-malware-prediction/discussion/85157).\n- Generating [frequency features.](https://www.kaggle.com/philippsinger/frequency-features-without-test-data-information)\n- Dropping variables with [different train and test distribution.](https://www.kaggle.com/bogorodvo/lightgbm-baseline-model-using-sparse-matrix)\n- [Aggregate time series features](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593) for home credit competition.\n- [Time Series](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593) features used in home credit default risk.\n- Scale, Standardize and n [ormalize with sklearn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02).\n- Handcrafted features for [Home default risk competition.](https://www.kaggle.com/c/home-credit-default-risk/discussion/57750)\n- Handcrafted\u00a0 [features used in Santander Transaction Prediction.](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89070)\n\n## Feature selection\n\nAfter generating many features from your data, you need to decide which all features to use in your model to get the maximum performance out of your model. This step also includes identifying the impact each feature is having on your model. Let\u2019s see some of the most popular feature selection methods.\n\n- Six ways to do f [eatures selection using sklearn](https://www.kaggle.com/sz8416/6-ways-fo...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    }
  ]
}