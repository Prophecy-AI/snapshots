{
  "query": "What are the best ensemble techniques for gradient boosting models in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensembling is the cornerstone of most winning Kaggle solutions that rely on gradient\u2011boosted trees because it reduces both bias and variance while exploiting the complementary strengths of different learners\u202f([Medium](https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e);\u202f[LinkedIn](https://www.linkedin.com/pulse/mastering-ensemble-learning-from-bagging-boosting-guide-kharche-kkyof)). For gradient\u2011boosting libraries such as XGBoost, LightGBM and CatBoost, the most effective ensembles are **stacked models** (or \u201cblending\u201d), where each base GBM is trained on the full training set and its out\u2011of\u2011fold predictions become features for a second\u2011level meta\u2011learner. This approach lets you combine the fast, histogram\u2011based learning of LightGBM with the regularization tricks of CatBoost and the depth\u2011wise control of XGBoost, often yielding a sizable lift over any single model\u202f([Medium](https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e)).\n\nOther high\u2011performing techniques include **bagged ensembles of the same GBM** (training multiple models with different random seeds or subsamples and averaging their predictions) and **voting/weighted averaging** of several tuned GBM variants. Scikit\u2011learn\u2019s documentation describes both stacking and voting as robust ways to improve generalisation\u202f([scikit\u2011learn](https://scikit-learn.org/stable/modules/ensemble.html)). In practice, Kagglers often create a **bagged LightGBM ensemble** (e.g., 5\u201310 models with varied `feature_fraction`, `bagging_fraction`, and seed) and then blend it with XGBoost and CatBoost models using a simple linear meta\u2011learner or a ridge regression\u202f([MachineLearningMastery](https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble)). Bagging can also be applied to neural\u2011network\u2011augmented boosting pipelines, further diversifying the model pool\u202f([Medium](https://medium.com/@igangat1/ensemble-learning-boosting-neural-networks-d1c227ed6505)).\n\nThe **real\u2011world impact** of these methods is evident in Kaggle competitions: the top\u20115 teams in the ASHRAE Great Energy Predictor III contest relied almost exclusively on ensembles of gradient\u2011boosting trees\u2014primarily LightGBM\u2014combined with careful preprocessing and feature engineering\u202f([arXiv](https://arxiv.org/pdf/2202.02898)). Successful Kaggle pipelines therefore follow a three\u2011step recipe: (1) train several high\u2011quality GBM models with diverse hyper\u2011parameters; (2) generate out\u2011of\u2011fold predictions and blend them via stacking or weighted averaging; and (3) optionally bag each GBM to smooth stochastic variance. This combination consistently ranks among the best ensemble techniques for gradient boosting in competitive data\u2011science settings.",
      "url": ""
    },
    {
      "title": "Stacking Ensembles: Combining XGBoost, LightGBM and CatBoost to Improve Model Performance.",
      "text": "Stacking Ensembles: Combining XGBoost, LightGBM and CatBoost to Improve Model Performance. | by Stephen Echessa | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Stacking Ensembles: Combining XGBoost, LightGBM and CatBoost to Improve Model Performance.\n[\n![Stephen Echessa](https://miro.medium.com/v2/resize:fill:64:64/1*e-uVx3adrff1BUFQmafEGw.jpeg)\n](https://medium.com/@stevechesa?source=post_page---byline--d4247d092c2e---------------------------------------)\n[Stephen Echessa](https://medium.com/@stevechesa?source=post_page---byline--d4247d092c2e---------------------------------------)\n8 min read\n\u00b7Jul 29, 2024\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/d4247d092c2e&amp;operation=register&amp;redirect=https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e&amp;user=Stephen+Echessa&amp;userId=b4204b4fc55c&amp;source=---header_actions--d4247d092c2e---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/d4247d092c2e&amp;operation=register&amp;redirect=https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e&amp;source=---header_actions--d4247d092c2e---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\nIn the ever-evolving world of machine learning, where numerous algorithms vie for supremacy, stacked ensemble stand out as a robust technique that combines the strengths of multiple models to deliver superior predictive performance. In this article, we\u2019ll delve into the concept of stacked ensembles, explore why they\u2019re beneficial, and provide a step-by-step guide to implementing them in Python using three state-of-the-art algorithms: XGBoost, CatBoost, and LightGBM.\n## **Why Stacking, and what does it mean?**\n![]()\nImage by quickmeme.com\nPicture a relay race where each runner excels in a different segment of the track. This is the essence of stacking. It is an ensemble learning technique that aims to enhance the predictive power of machine learning models by combining multiple individual models, also known as base learners. The core idea is that by leveraging the strengths and compensating for the weaknesses of different models, you can create a final model that performs better than any single model on its own.\nWhile each individual model has its own merits and demerits, one model may outperform the others on a given machine learning problem. Stacking addresses this by feeding the predictions of the base models to a higher-level model known as the meta-learner or blender, which then combines these inputs to generate the final prediction.\nStacking is advantageous because of the following reasons:\n* **Enhanced Accuracy:**Aggregating the predictions of multiple models often leads to better performance than using any single model, thanks to the reduction of individual biases.\n* **Increased Robustness:**Stacked models generalize better across different datasets, mitigating the risk of overfitting and making the predictions more reliable.\n* **Greater Flexibility:**Stacking allows for the combination of different types of models (e.g., linear models, decision trees, neural networks), capturing a wider range of patterns in the data.## **Meet the Titans: XGBoost, CatBoost, and LightGBM**\n![]()\nImage by Stephen Echessa\nThese three algorithms are widely recognized for their superior performance in regression tasks. Each brings unique advantages to the table.\n### XGBoost (eXtreme Gradient Boosting)\nXGBoost is known for its speed and performance. It efficiently handles large datasets and is highly scalable. Its popularity in data science competitions, like those on Kaggle, is a testament to its robust performance and reliability.\n### CatBoost (Categorical Boosting)\nCatBoost excels in handling categorical data. It automatically deals with categorical features, eliminating the need for extensive preprocessing. This capability, combined with its resistance to overfitting, makes CatBoost a powerful tool for various applications.\n### LightGBM (Light Gradient Boosting Machine)\nLightGBM is designed for speed and scalability. It can handle massive datasets with ease and is known for its efficiency in both training and prediction phases. LightGBM\u2019s ability to manage large amounts of data quickly makes it a preferred choice in many scenarios.\n## How Stacking Works\nThis section provides a comprehensive overview of the stacking process.\n![]()\nImage by researchgate\n1. ***Preprocess and split the data:***The data should be prepared before modeling. This involves cleaning the data, identifying relevant features and splitting the data to training and validation sets.\n2. ***Train Base Models:***Each base model is trained on the training set. In our case, these models are XGBoost, CatBoost, and LightGBM.\n3. ***Generate Predictions:***The base models create predictions for the validation set. These predictions serve as input features for the meta-learner.\n4. ***Train Meta-Learner:***The meta-learner is trained on the predictions of the base models. Simpler machine learning algorithms like Linear regression, Logistic regression, Lasso, Ridge, or Elastic Net are commonly used as meta-learners, acting as the secondary model that is fed the aggregated results of the primary base models.\n5. ***Make Final Predictions:***The trained stacked model (base models + meta-learner) is used to make predictions on new, unseen data.\nThe stacking architecture allows you to further customize the model by**restacking**. This involves adding the original input feature X to the final estimator, alongside the predictions from the base learners. In scikit-learn, you can achieve this by setting the`passthrough=True`parameter on the stacked model.\n## Step-by-Step Implementation in Python\nLet\u2019s walk through the implementation of stacked ensembles using XGBoost, CatBoost, and LightGBM.\n### 1.***Load the Data***\nWe will be using the Boston Housing dataset which contains a total of 14 attributes. Here lies a detailed description:\n```\n1. CRIM - per capita crime rate by town\n2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n3. INDUS - proportion of non-retail business acres per town.\n4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n5. NOX - nitric oxides concentration (parts per 10 million)\n6...",
      "url": "https://medium.com/@stevechesa/stacking-ensembles-combining-xgboost-lightgbm-and-catboost-to-improve-model-performance-d4247d092c2e"
    },
    {
      "title": "Ensemble Learning: Boosting & Neural Networks - Igangat - Medium",
      "text": "<div><div><div><h2>Using gradient boosting and ensemble methods to improve boosted neural networks.</h2><div><a href=\"https://medium.com/@igangat1?source=post_page---byline--d1c227ed6505---------------------------------------\"><div><p></p></div></a></div></div><p>Machine learning has evolved significantly over the past few years. In this article, I review ensemble models, and more recent applications of ensemble learning with gradient boosting and deep learning. A Github repository is linked detailing an implementation of XGBoost and XBNet with Kaggle data.</p><p>Ensemble learning is a class of machine learning where multiple models, which are called weak learners, are combined to create a stronger model that can solve the same problem with more accuracy. Intuitively, the advantage of ensemble learning over a single model is that single models are weak learners \u2014 that is, they tend have high variance or bias. The goal of ensemble learning is to reduce bias or variance by combining weak learners into a strong learner.</p><h2>Bagging</h2><p>Bagging stands for \u201cbootstrap aggregating\u201d, and it is a parallel ensemble method that trains each weak learner independently or concurrently. Bootstrapping is a statistical method that randomly draws samples of size <em>n</em> from an initial dataset of size <em>n</em> with replacement; these samples are bootstrap samples that are meant to be representations of the true data distribution. In an ensemble learner, individual trees are trained for each bootstrap sample, such that if there are <em>T</em> samples, we can obtain a sequence of <em>T</em> outputs that can be averaged to make a final prediction. While there are some differences, bagging is the baseline for ensemble models such as random forest.</p><h2>Boosting</h2><p>Boosting works in a similar fashion to bagging. It combines multiple weak learners into a strong learner, however, it differs in that boosting is not a parallel learning method. In boosting, weak learners are not fitted independently; instead, they are trained sequentially, meaning that as weak learners are trained, new models are fitted by focusing on the observations that were poorly handled by the previous model. Thus, boosting is an adaptive process: as the next model is fitted, it adapts to the previous model and assigns more importance to the observations that the previous model struggled with. Boosting focuses on reducing bias, and so weak learners are often chosen to have high bias and low variance. By increasing the weight of samples that are more difficult to handle, boosting improves accuracy by tailoring the model towards difficult observations. This is one of the main advantages of boosting. However, the disadvantage of boosting is that it is sensitive to outliers because every tree is dependent on the errors of the previous model, so an outlier could affect the entire iterative process.</p><h2>AdaBoost</h2><p>Boosting is typically implemented in one of two ways: adaptative boosting and gradient boosting. Adaptative boosting, or AdaBoost, uses an ensemble of models. In the first iteration of the process, AdaBoost trains a model by weighting each observation equally and simply fitting the model using an objective function, in most cases least-squares or Gini impurity. In the next iteration, the model is trained by assigning higher weights to the observations that were more difficult to classify in the first tree stump. Below is a mathematical explanation of AdaBoost.</p><figure></figure><h2>Gradient Boosting</h2><p>Gradient boosting trains decision trees through boosting, but differs from AdaBoost in that Gradient boosting is based on a loss function, which computes the residuals of the model\u2019s outputs. As each new tree is constructed, the parameters of the tree that will minimize the loss function of the ensemble are selected through a gradient descent algorithm. Here is how gradient boosting is applied to a decision tree.</p><p>The process goes as follows: first a tree that only has one node is built, typically using the average value of the target variable. Next, the residuals of this tree node are computed; a larger tree, maybe with three or four leaves, is trained to predict the residuals of the original leaf. The residuals predicted by the new tree are scaled by a learning rate (which is fixed by the user), and then added back to the original prediction of the leaf to make new predictions. Next, a new tree is constructed to predict the residuals of the previous tree, and these residuals should be smaller than the previous tree. The goal is for each new tree to take a step towards making better predictions until the process is halted. The advantage of gradient boosting when compared to AdaBoost is that AdaBoost tends to be sensitive to outliers, while gradient boosting is more robust. The drawbacks of gradient boosting are that it can be prone to overfitting and its runtime tends to be longer than AdaBoost. Below is a mathematical explanation of gradient boosting.</p><figure></figure><h2><strong>XGBoost</strong></h2><p>When simple gradient boosting is implemented, the Gradient Boosting Machine (GBM) algorithm is used. Extreme Gradient Boosting, or XGBoost, is an algorithm that is used to implement a more efficient method of gradient boosting. The main difference and advantage of XGBoost over GBM is that it utilizes regularized boosting, which prevents overfitting of any tree in the boosting process and allows for the model to be more generalized. This helps the model perform better during testing because it is not overfitted to the training set and it can be generalized to the test set, while GBMs tend to be overfitted to the training set. XGBoost also uses K-fold cross validation at each iteration, which allows for the model to evaluate its performance at each iteration to find the optimal stopping point, and parallel processing, which allows for XGBoost to be used with large datasets. In fact, one of the main points that makes XGBoost so useful is its</p><h2>LightGBM</h2><p>LightGBM is another implementation of gradient boosting that tends to be faster and provide quicker results than traditional GBM models. The most unique attribute of LightGBM, and its greatest advantage over XGBoost, is that it uses gradient one-side-sampling, or GOSS. In boosting, gradient is the derivative of the loss function, so if the gradient is large, then the error of the model is large. When a tree is trained, each observation will have its own gradient; LightGBM sorts these gradients in descending order, and then the top 20% of the gradients will be selected and kept aside, and 10% of the remaining 80% will be randomly selected. Then, these two samples will be combined such that the larger gradients will now make up a higher percentage of the sample for the next iteration, and the model will be trained to handle observations with high gradients, or high error.</p><p>LightGBM also uses binning to split trees at each node, which is a technique that splits continuous variables into bins so that there are only two ranges that can be chosen from at each node, rather than multiple values that can be chosen from. This makes binning a much more efficient way of splitting and in turn makes LightGBM much faster. LightGBM also uses exclusive feature bundling, or EFB, which is a dimensionality reduction technique that combines exclusive variables into a single variable. Overall, the advantage of LightGBM over XGBoost relates to faster run times and accuracy.</p><h2><strong>Neural Networks</strong></h2><p>The base model of deep learning is the neural network, which are machine learners that are inspired by the human brain, acting in a similar manner to which biological neurons signal to one another. Neural networks take input data and fed through a series of layers to predict a response variable using a nonlinear function.</p><figure><figcaption>Source: An Introduction to Statistical Learning (ISLR) Gareth M...",
      "url": "https://medium.com/@igangat1/ensemble-learning-boosting-neural-networks-d1c227ed6505"
    },
    {
      "title": "Mastering Ensemble Learning: From Bagging to Boosting - A Practical Guide to Smarter ML Models",
      "text": "### \ud83d\udcda Table of Contents\n\n01. Introduction: Why Ensemble Learning?\n02. What is Ensemble Learning?\n03. Types of Ensemble Learning\n04. Deep Dive: Random Forest and the Power of Feature Randomization\n05. Boosting Unboxed: Gradient Boosting, XGBoost, LightGBM, CatBoost\n06. Voting and Stacking: Combining the Best of All Worlds\n07. Real-World Applications\n08. Ensemble Learning: Pros and Cons\n09. Real-World Analogies\n10. Diagnosing and Troubleshooting Ensembles\n11. Ensemble Techniques and Hyperparameter Tuning\n12. Python Code Examples\n\n* * *\n\n### \ud83c\udf1f Introduction: Why Ensemble Learning?\n\nWhat if your model could think like a crowd of experts, each offering a unique point of view?\n\nThat\u2019s the promise of Ensemble Learning: combining multiple machine learning models to achieve better, more reliable results.\n\nFrom winning Kaggle competitions to powering real-world business intelligence, ensemble methods have become a go-to tool for boosting performance and reducing both bias and variance.\n\n* * *\n\n### \ud83e\udd16 What is Ensemble Learning?\n\nEnsemble Learning is a technique where multiple models often weak learners are combined to produce a stronger predictive model.\n\nThere are two main paradigms:\n\n- Parallel ensembles (e.g., Bagging): Models learn independently.\n- Sequential ensembles (e.g., Boosting): Each model learns from the mistakes of the previous one.\n\nEnsemble Learning\n\n* * *\n\n### \ud83e\uddf1 Types of Ensemble Learning\n\n### \ud83d\udce6 1\\. Bagging (Bootstrap Aggregating)\n\n- Process: Multiple base models (typically decision trees) are trained on random bootstrapped samples of the data.\n- Goal: Reduce variance by averaging predictions.\n- Key Technique: Random Subspace Method at each tree split, a random subset of features is considered.\n\n\ud83c\udf32 Random Forest is the most popular bagging algorithm.\n\n* * *\n\n### \ud83d\ude80 2\\. Boosting\n\nBoosting corrects the errors of weak models by focusing successive learners on previously misclassified examples.\n\nGradient Boosting, a powerful variant, works as follows:\n\n- Each new model is trained on the residual errors of the previous one.\n- The ensemble learns to minimize a loss function by taking gradient steps in function space.\n\nMathematically:\n\nThis is what allows boosting to reduce bias progressively.\n\n* * *\n\n### \ud83d\udd25 3\\. Advanced Boosting Algorithms\n\n### \u2705 XGBoost (Extreme Gradient Boosting)\n\n- Regularized loss function to reduce overfitting.\n- Optimized tree pruning and handling of sparse data.\n- Uses approximate greedy algorithms for fast computation.\n- Built-in parallelization and caching mechanisms.\n\n### \u26a1 LightGBM\n\n- Uses a leaf-wise growth strategy instead of level-wise.\n- Extremely fast and memory-efficient.\n- Handles large datasets and categorical features directly.\n\n### \ud83d\udc31 CatBoost\n\n- Designed for categorical variables \u2014 no need for manual encoding.\n- Applies ordered boosting to reduce prediction shift.\n- Supports missing values natively.\n\n* * *\n\n### \ud83d\uddf3\ufe0f 4\\. Voting\n\nVoting ensembles combine predictions from multiple different models to improve overall accuracy.\n\nIn Hard Voting, the majority class is chosen, while Soft Voting averages class probabilities for a more nuanced decision.\n\nVoting combines different model types:\n\n- Hard Voting: Majority class wins.\n- Soft Voting: Averaged probabilities - often better.\n\n* * *\n\n### \ud83c\udfd7\ufe0f 5\\. Stacking\n\nStacking is an advanced ensemble technique that combines multiple base models, known as Level-0 learners.\n\nTheir predictions are fed into a meta-learner (Level-1) that learns how to best combine these outputs.\n\nUnlike bagging or boosting, stacking can use diverse model types to capture different patterns.\n\nThis layered approach often leads to improved accuracy and better generalization on unseen data.\n\n## Recommended by LinkedIn\n\n* * *\n\n### \ud83c\udf32 Deep Dive: Random Forest\n\nRandom Forest = Bagging + Feature Randomization\n\n- Bootstrap sampling \\+ Random subspace for feature selection.\n- At each split, only a subset of features is considered.\n- This increases diversity among trees, reducing correlation and improving generalization.\n\n\ud83e\uddea OOB (Out-of-Bag) Estimation:\n\n- Since each tree is trained on ~63% of the data, the remaining ~37% (OOB samples) serve as a validation set.\n- Aggregate OOB error offers a reliable generalization estimate without cross-validation.\n\n* * *\n\n### \ud83d\udcbc Real-World Applications\n\n- Banking: Credit scoring, fraud detection\n- Healthcare: Disease diagnosis, drug discovery\n- Retail: Customer segmentation, recommendation engines\n- Manufacturing: Predictive maintenance, defect detection\n- Sports Analytics: Match prediction, performance modeling\n\n* * *\n\n### \u2705 Advantages of Ensemble Learning\n\n- Superior accuracy\n- Better generalization\n- Handles imbalance and noise\n- Works well with both structured and unstructured data\n- Offers feature importance insights (especially in tree-based ensembles)\n\n* * *\n\n### \u26a0\ufe0f Disadvantages of Ensemble Learning\n\n- High training cost\n- Less interpretability\n- Complex deployment\n- Risk of overfitting if not regularized\n- Tuning is often non-trivial\n\n* * *\n\n### \ud83e\udde0 Real-World Analogy\n\nImagine consulting multiple doctors for a serious condition:\n\n- Bagging: Each doctor independently reviews your case. The final decision is an average.\n- Boosting: Each doctor learns from what the previous one missed.\n- Voting: You poll a panel of doctors from various specialties.\n- Stacking: A senior specialist (meta-model) makes the final call based on input from others.\n\n* * *\n\n### \ud83d\udee0\ufe0f Diagnosing Ensemble Model Issues\n\n1. Overfitting in Boosting: Use early stopping, regularization.\n2. Underfitting in Bagging: Increase base learner complexity.\n3. Lack of diversity: Use different algorithms or randomized data subsets.\n4. Imbalanced data: Try SMOTE or use model class weights.\n5. Slow inference: Limit number of trees or depth.\n\n* * *\n\n### \ud83d\udd27 Ensemble Techniques and Hyperparameter Tuning\n\n### \ud83d\udd0d Bagging (Random Forest)\n\n- n\\_estimators: Number of trees\n- max\\_features: Number of features at each split\n- max\\_depth: Limits complexity\n- bootstrap: Sampling strategy\n- oob\\_score: Enables OOB estimatio\n\n### \ud83e\uddee Gradient Boosting / XGBoost / LightGBM / CatBoost\n\n- learning\\_rate: Step size\n- n\\_estimators: Total iterations\n- max\\_depth or num\\_leaves: Controls complexity\n- subsample: Row sampling per tree\n- colsample\\_bytree: Feature sampling per tree\n- reg\\_alpha / reg\\_lambda: L1/L2 regularization\n- early\\_stopping\\_rounds: Prevents overfitting\n\nUse GridSearchCV or Optuna for tuning.\n\n* * *\n\n### \ud83d\udcbb Python Code Examples\n\n### \ud83c\udfaf Random Forest\n\n### \u26a1 XGBoost\n\n### \ud83d\udd17 Stacking\n\n* * *\n\n### \ud83e\uddfe Final Thoughts\n\nEnsemble methods are not just a luxury, they are a necessity in production-grade ML systems.\n\nWhen crafted carefully with diversity, tuning, and interpretation in mind, they can deliver state-of-the-art performance across domains.\n\nWhether you\u2019re starting out or scaling enterprise AI systems, ensemble learning gives you the edge.\n\n* * *\n\n- Which ensemble method do you prefer: Bagging or Boosting?\n- Have you used CatBoost or LightGBM in your projects?\n- What hyperparameter tricks worked best for you?\n\nDrop your thoughts, code snippets, or challenges in the comments \ud83d\udc47\n\n* * *\n\n#EnsembleLearning #MachineLearningModels #BoostingVsBagging #DataScienceTips #XGBoost\n\n[DataToDecision: AI & Analytics](https://www.linkedin.com/newsletters/datatodecision-ai-analytics-7309470147277168640)\n\n### DataToDecision: AI & Analytics\n\n#### 1,937 follower\n\n## More articles by Amit Kharche\n\n- [How to Serve LLMs in Production: Tools, Architecture & Strategic Considerations](https://www.linkedin.com/pulse/how-serve-llms-production-tools-architecture-amit-kharche-4sdmf)\n\n\n\n\n\nAug 6, 2025\n\n\n\n### How to Serve LLMs in Production: Tools, Architecture & Strategic Considerations\n\n\n\nIntroduction: From Notebook Demos to Enterprise Engines As someone who leads AI and GenAI transformation at scale, I\u2019ve\u2026\n\n- [Model Compression Techniques: Quantization, Pruning & Distillation for Real-World Deployment](https://www.linkedin.com/pulse/model-compression-techniques-quantiz...",
      "url": "https://www.linkedin.com/pulse/mastering-ensemble-learning-from-bagging-boosting-guide-kharche-kkyof"
    },
    {
      "title": "How to Develop a Light Gradient Boosted Machine (LightGBM) Ensemble - MachineLearningMastery.com",
      "text": "### [Navigation](https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/\\#navigation)\n\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onApril 27, 2021in[Ensemble Learning](https://machinelearningmastery.com/category/ensemble-learning/)[17](https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/#comments)\n\nShare _Tweet_Share\n\nLight Gradient Boosted Machine, or **LightGBM** for short, is an open-source library that provides an efficient and effective implementation of the gradient boosting algorithm.\n\nLightGBM extends the gradient boosting algorithm by adding a type of automatic feature selection as well as focusing on boosting examples with larger gradients. This can result in a dramatic speedup of training and improved predictive performance.\n\nAs such, LightGBM has become a de facto algorithm for machine learning competitions when working with tabular data for regression and classification predictive modeling tasks. As such, it owns a share of the blame for the increased popularity and wider adoption of gradient boosting methods in general, along with Extreme Gradient Boosting (XGBoost).\n\nIn this tutorial, you will discover how to develop Light Gradient Boosted Machine ensembles for classification and regression.\n\nAfter completing this tutorial, you will know:\n\n- Light Gradient Boosted Machine (LightGBM) is an efficient open-source implementation of the stochastic gradient boosting ensemble algorithm.\n- How to develop LightGBM ensembles for classification and regression with the scikit-learn API.\n- How to explore the effect of LightGBM model hyperparameters on model performance.\n\n**Kick-start your project** with my new book [Ensemble Learning Algorithms With Python](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/), including _step-by-step tutorials_ and the _Python source code_ files for all examples.\n\nLet\u2019s get started.\n\n![How to Develop a Light Gradient Boosted Machine (LightGBM) Ensemble](https://machinelearningmastery.com/wp-content/uploads/2020/11/How-to-Develop-a-Light-Gradient-Boosted-Machine-LightGBM-Ensemble.jpg)\n\nHow to Develop a Light Gradient Boosted Machine (LightGBM) Ensemble\n\nPhoto by [GPA Photo Archive](https://www.flickr.com/photos/iip-photo-archive/35938248601/), some rights reserved.\n\n## Tutorial Overview\n\nThis tutorial is divided into three parts; they are:\n\n1. Light Gradient Boosted Machine Algorithm\n2. LightGBM Scikit-Learn API\n1. LightGBM Ensemble for Classification\n2. LightGBM Ensemble for Regression\n3. LightGBM Hyperparameters\n1. Explore Number of Trees\n2. Explore Tree Depth\n3. Explore Learning Rate\n4. Explore Boosting Type\n\n## Light Gradient Boosted Machine Algorithm\n\n[Gradient boosting](https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/) refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n\nEnsembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\n\nModels are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, \u201c _gradient boosting_,\u201d as the loss gradient is minimized as the model is fit, much like a neural network.\n\nFor more on gradient boosting, see the tutorial:\n\n- [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n\nLight Gradient Boosted Machine, or LightGBM for short, is an open-source implementation of gradient boosting designed to be efficient and perhaps more effective than other implementations.\n\nAs such, [LightGBM](https://github.com/Microsoft/LightGBM) refers to the open-source project, the software library, and the machine learning algorithm. In this way, it is very similar to the [Extreme Gradient Boosting or XGBoost technique](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/).\n\nLightGBM was described by Guolin Ke, et al. in the 2017 paper titled \u201c [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree).\u201d The implementation introduces two key ideas: GOSS and EFB.\n\nGradient-based One-Side Sampling, or GOSS for short, is a modification to the gradient boosting method that focuses attention on those training examples that result in a larger gradient, in turn speeding up learning and reducing the computational complexity of the method.\n\n> With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size.\n\n\u2014 [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree), 2017.\n\nExclusive Feature Bundling, or EFB for short, is an approach for bundling sparse (mostly zero) mutually exclusive features, such as categorical variable inputs that have been one-hot encoded. As such, it is a type of automatic feature selection.\n\n> \u2026 we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features.\n\n\u2014 [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree), 2017.\n\nTogether, these two changes can accelerate the training time of the algorithm by up to 20x. As such, LightGBM may be considered gradient boosting decision trees (GBDT) with the addition of GOSS and EFB.\n\n> We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy\n\n\u2014 [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree), 2017.\n\n### Want to Get Started With Ensemble Learning?\n\nTake my free 7-day email crash course now (with sample code).\n\nClick to sign-up and also get a free PDF Ebook version of the course.\n\nDownload Your FREE Mini-Course\n\n## LightGBM Scikit-Learn API\n\nLightGBM can be installed as a standalone library and the LightGBM model can be developed using the scikit-learn API.\n\nThe first step is to install the LightGBM library, if it is not already installed. This can be achieved using the pip python package manager on most platforms; for example:\n\n|     |     |\n| --- | --- |\n| 1 | sudo pip install lightgbm |\n\nYou can then confirm that the LightGBM library was installed correctly and can be used by running the following script.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3 | \\# check lightgbm version<br>import lightgbm<br>print(lightgbm.\\_\\_version\\_\\_) |\n\nRunning the script will print your version of the LightGBM library you have installed.\n\nYour version should be the same or higher. If not, you must upgrade your version of the LightGBM library.\n\n|     |     |\n| --- | --- |\n| 1 | 2.3.1 |\n\nIf you require specific instructions for your development environment, see the tutorial:\n\n- [LightGBM Installation Guide](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html)\n\nThe LightGBM library has its own custom API, although we will use the method via the scikit-learn wrapper classes: [LGBMRegressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html) a...",
      "url": "https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble"
    },
    {
      "title": "1.11.  Ensembles: Gradient boosting, random forests, bagging, voting, stacking #",
      "text": "1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)](../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 1.11.Ensembles: Gradient boosting, random forests, bagging, voting, stacking[#](#ensembles-gradient-boosting-random-forests-bagging-voting-stacking)\n**Ensemble methods**combine the predictions of several\nbase estimators built with a given learning algorithm in order to improve\ngeneralizability / robustness over a single estimator.\nTwo very famous examples of ensemble methods are[gradient-boosted trees](#gradient-boosting)and[random forests](#forest).\nMore generally, ensemble models can be applied to any base learner beyond\ntrees, in averaging methods such as[Bagging methods](#bagging),[model stacking](#stacking), or[Voting](#voting-classifier), or in\nboosting, as[AdaBoost](#adaboost).\n## 1.11.1.Gradient-boosted trees[#](#gradient-boosted-trees)\n[Gradient Tree Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)or Gradient Boosted Decision Trees (GBDT) is a generalization\nof boosting to arbitrary differentiable loss functions, see the seminal work of[[Friedman2001]](#friedman2001). GBDT is an excellent model for both regression and\nclassification, in particular for tabular data.\n[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)vs[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)\nScikit-learn provides two implementations of gradient-boosted trees:[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)vs[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)for classification, and the\ncorresponding classes for regression. The former can be**orders of\nmagnitude faster**than the latter when the number of samples is\nlarger than tens of thousands of samples.\nMissing values and categorical data are natively supported by the\nHist\u2026 version, removing the need for additional preprocessing such as\nimputation.\n[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)might be preferred for small sample\nsizes since binning may lead to split points that are too approximate\nin this setting.\n### 1.11.1.1.Histogram-Based Gradient Boosting[#](#histogram-based-gradient-boosting)\nScikit-learn 0.21 introduced two new implementations of\ngradient boosted trees, namely[`HistGradientBoostingClassifier`](generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)and[`HistGradientBoostingRegressor`](generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor), inspired by[LightGBM](https://github.com/Microsoft/LightGBM)(See[[LightGBM]](#lightgbm)).\nThese histogram-based estimators can be**orders of magnitude faster**than[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)when the number of samples is larger\nthan tens of thousands of samples.\nThey also have built-in support for missing values, which avoids the need\nfor an imputer.\nThese fast estimators first bin the input samples`X`into\ninteger-valued bins (typically 256 bins) which tremendously reduces the\nnumber of splitting points to consider, and allows the algorithm to\nleverage integer-based data structures (histograms) instead of relying on\nsorted continuous values when building the trees. The API of these\nestimators is slightly different, and some of the features from[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)are not yet supported, for instance some loss functions.\nExamples\n* [Partial Dependence and Individual Conditional Expectation Plots](../auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py)\n* [Comparing Random Forests and Histogram Gradient Boosting models](../auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.html#sphx-glr-auto-examples-ensemble-plot-forest-hist-grad-boosting-comparison-py)\n#### 1.11.1.1.1.Usage[#](#usage)\nMost of the parameters are unchanged from[`GradientBoostingClassifier`](generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)and[`GradientBoostingRegressor`](generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\nOne exception is the`max\\_iter`parameter that replaces`n\\_estimators`, and\ncontrols the number of iterations of the boosting process:\n```\n&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_hastie\\_10\\_2&gt;&gt;&gt;X,y=make\\_hastie\\_10\\_2(random\\_state=0)&gt;&gt;&gt;X\\_train,X\\_test=X[:2000],X[2000:]&gt;&gt;&gt;y\\_train,y\\_test=y[:2000],y[2000:]&gt;&gt;&gt;clf=HistGradientBoostingClassifier(max\\_iter=100).fit(X\\_train,y\\_train)&gt;&gt;&gt;clf.score(X\\_test,y\\_test)0.8965\n```\nAvailable losses for**regression**are:\n* \u2018squared\\_error\u2019, which is the default loss;\n* \u2018absolute\\_error\u2019, which is less sensitive to outliers than the squared error;\n* \u2018gamma\u2019, which is well suited to model strictly positive outcomes;\n* \u2018poisson\u2019, which is well suited to model counts and frequencies;\n* \u2018quantile\u2019, which allows for estimating a conditional quantile that can later\nbe used to obtain prediction intervals.\nFor**classification**, \u2018log\\_loss\u2019 is the only option. For binary classification\nit uses the binary log loss, also known as binomial deviance or binary\ncross-entropy. For`n\\_classes&gt;=3`, it uses the multi-class log loss function,\nwith multinomial deviance and categorical cross-entropy as alternative names.\nThe appropriate loss version is selected based on[y](../glossary.html#term-y)passed to[fit](../glossary.html#term-fit).\nThe size of the trees can be controlled through the`max\\_leaf\\_nodes`,`max\\_depth`, and`min\\_samples\\_leaf`parameters.\nThe number of bins used to bin the data is controlled with the`max\\_bins`parameter. Using less bins acts as a form of regularization. It is generally\nrecommended to use as many bins as possible (255), which is the default.\nThe`l2\\_regularization`parameter acts as a regularizer for the loss function,\nand corresponds to\\\\(\\\\lambda\\\\)in the following expression (see equation (2)\nin[[XGBoost]](#xgboost)):\n\\\\[\\\\mathcal{L}(\\\\phi) = \\\\sum\\_i l(\\\\hat{y}\\_i, y\\_i) + \\\\frac12 \\\\sum\\_k \\\\lambda ||w\\_k||^2\\\\]\nDetails on l2 regularization[#](#details-on-l2-regularization)\nIt is important to notice that the loss term\\\\(l(\\\\hat{y}\\_i, y\\_i)\\\\)describes\nonly half of the actual loss function except for the pinball loss and absolute\nerror.\nThe index\\\\(k\\\\)refers to the k-th tree in the ensemble of trees. In the\ncase of regression and binary classification, gradient boosting models grow one\ntree per iteration, then\\\\(k\\\\)runs up to`max\\_iter`. In the case o...",
      "url": "https://scikit-learn.org/stable/modules/ensemble.html"
    },
    {
      "title": "",
      "text": "Gradient boosting machines and careful pre-processing work best: ASHRAE Great\nEnergy Predictor III lessons learned\nClayton Miller\u2217, Liu Hao, Chun Fu\nDepartment of the Built Environment, National University of Singapore (NUS), Singapore\n\u2217Corresponding Author: clayton@nus.edu.sg, +65 81602452\nAbstract\nThe ASHRAE Great Energy Predictor III (GEPIII) competition was held in late 2019 as one of the largest machine learning\ncompetitions ever held focused on building performance. It was hosted on the Kaggle platform and resulted in 39,402 prediction\nsubmissions, with the top five teams splitting $25,000 in prize money. This paper outlines lessons learned from participants, mainly\nfrom teams who scored in the top 5% of the competition. Various insights were gained from their experience through an online\nsurvey, analysis of publicly shared submissions and notebooks, and the documentation of the winning teams. The top-performing\nsolutions mostly used ensembles of Gradient Boosting Machine (GBM) tree-based models, with the LightGBM package being\nthe most popular. The survey participants indicated that the preprocessing and feature extraction phases were the most important\naspects of creating the best modeling approach. All the survey respondents used Python as their primary modeling tool, and it was\ncommon to use Jupyter-style Notebooks as development environments. These conclusions are essential to help steer the research\nand practical implementation of building energy meter prediction in the future.\n1. Introduction\nMachine learning (ML) for building energy prediction is a\nrich research community with hundreds of influential publica\u0002tions [1]. However, a fundamental challenge in the literature is a\nlack of comparability of prediction techniques [2], despite pre\u0002vious efforts at benchmarking [3, 4]. Machine learning compe\u0002titions provide a comparison of techniques through the crowd\u0002sourcing and benchmarking of various combinations of models\nwith a reward for the most objectively accurate solution.\nASHRAE has hosted three machine learning energy predic\u0002tion competitions since 1993. The first two competitions were\nnamed the Great Energy Predictor Shootouts I and II held in\n1993 and 1994. In the first competition, each contestant was\ngiven a four-month data set to predict building energy use and\ninsolation data for the next two months, and the final model,\nwhich was developed using Bayesian nonlinear modeling and\nartificial neural networks, was found to be the most effective\nand accurate [5]. While in the second competition, entrants\nwere asked to predict the intentionally removed portion of data\nbased on the existing building\u2019s pre-retrofit and post-retrofit\ndata [6]. Derived from the submissions that met the require\u0002ments, neural networks had shown to be the most accurate\nmodel overall, while cleverly assembled statistical models were\nadditionally found to be even more accurate than neural net\u0002works in some fields [7, 8]. Despite the passage of time, the\ncontents of these two competitions are still being investigated\nand used for references.\nAfter more than two decades, the third competition, titled the\nASHRAE Great Energy Predictor III (GEPIII), was initiated at\nthe ASHRAE Winter Conference in Chicago in January 2018.\nAfter getting the approval and financial sponsorship from the\nASHRAE Research Activities Committee (RAC), the competi\u0002tion was officially launched1 on 5 October 2019, and it ended\non 19 December 2019 [9]. The context of the GEPIII com\u0002petition was whether energy-saving building retrofitting could\nhelp to improve energy efficiency [10]. The datasets utilized in\nthe contest were collected from around 1,440 buildings from 16\nsites worldwide, of which 73% were educational, and the other\n27% were municipal and healthcare facilities. The energy me\u0002ter readings of these buildings from January 2016 to Decem\u0002ber 2018 were combined to form the dataset. Based on this\ncontext, the participants were challenged to create a counter\u0002factual model to estimate the building\u2019s pre-renovation energy\nusage rate in the post-renovation period [9]. The final rank\u0002ing of contestants was determined by the Private Leaderboard\n(PLB) scores, and the top five performers were awarded mone\u0002tary prizes.\nOne key output of the competition was learning from the con\u0002testants\u2019 solutions and understanding the general nature of what\nmakes the best machine learning solution for long-term build\u0002ing energy prediction. This paper outlines a post-competition\nsurvey used to capture the demographics, best practices, and\nlessons learned from a subset of the top-performing teams.\n2. Methodology\nThe primary goal of this analysis was to investigate the de\u0002mographics and machine learning strategy preferences of par\u00021https://www.kaggle.com/c/ashrae-energy-prediction\nPreprint submitted to ASHRAE Transactions February 8, 2022\narXiv:2202.02898v1 [cs.LG] 7 Feb 2022\nticipants from the top teams in the GEPIII competition. The pri\u0002mary component of this methodology was a survey composed\nof a segment that included questions regarding respondents\u2019\nbackground information and another with queries regarding the\nfinal solutions they submitted. The additional data collection\nprocess was done by analyzing the publicly available analysis\nnotebooks posted as part of the competition and the submis\u0002sions and interviews of the top five winning teams.\nThe first portion of the web-based survey gathered some ba\u0002sic information about the contestants, such as age, gender, ed\u0002ucational information, current job fields, and work experience.\nThe second portion of the survey was designed to gather infor\u0002mation regarding their participation experience. It can be fur\u0002ther broken down into three subsections. The first subsection is\nprimarily intended to collect information about how contestants\narrived at their final solutions, such as what programming lan\u0002guages they utilized, what platforms they selected most to run\ntheir codes and the methods and algorithms employed in each\nphase of building machine learning models. Furthermore, the\nparticipants were asked to express their opinions on the signifi\u0002cance of the five steps machine learning workflow in the second\nsubsection. In the last subsection, contestants are asked to pro\u0002vide their feedback on the competition by commenting on what\nparts they liked or disliked. These insights were designed to\nhelp the organizers understand what competition mechanisms\nthe contestants prefer, allowing improvements for future events.\nThis paper aims to outline the collection of insights target\u0002ing the teams that scored in the top 5% (180 teams who earned\ngold or silver medals) of the competition. This process seeks\nto characterize the insights and best practices of the contestants\nand teams that created solutions with the highest performance.\nTowards this effort, the survey was sent to teams from the top\u0002performing competition participants from May to August 2021.\nWe received responses from 27 individuals that included the\ncollective insights from 50 contestants, with team members of\nrespondents contained for non-demographic questions. We in\u0002cluded the data from publicly available online solutions posted\nby another 34 teams, including the top five winning teams for\nthe tools and modeling analysis. Most of the data were col\u0002lected from teams in the top 5% (including 90% of the survey\ndata).\n3. Results\n3.1. Demographics of contestants\nOne key focus of the competition was to work towards the\nbetter exchange of ideas between the building and construc\u0002tion industry and the fast-growing data science community. The\nfirst analysis from the survey data was to understand what back\u0002grounds the contestants generally had. Figure 1 illustrates the\nhigh-level demographics of the survey respondents.\nThe age range results showed that 40% of the respondents are\nbetween the ages of 30 and 39 and a further 23% are between\n20-29. These results compare well to a larger-scale generic\nsurvey collected by Kaggle in 2021, ...",
      "url": "https://arxiv.org/pdf/2202.02898"
    },
    {
      "title": "Mastering Gradient Boosting: The Algorithm That Dominates Kaggle",
      "text": "Mastering Gradient Boosting: The Algorithm That Dominates Kaggle | by Marwan eslam ouda | Nov, 2025 | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@marawaneslam145/mastering-gradient-boosting-the-algorithm-that-dominates-kaggle-1d6c96db02af&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@marawaneslam145/mastering-gradient-boosting-the-algorithm-that-dominates-kaggle-1d6c96db02af&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Mastering Gradient Boosting: The Algorithm That Dominates Kaggle\n[\n![Marwan eslam ouda](https://miro.medium.com/v2/resize:fill:64:64/1*-wkRiY5H-2FUh4QG85pAVw.jpeg)\n](https://medium.com/@marawaneslam145?source=post_page---byline--1d6c96db02af---------------------------------------)\n[Marwan eslam ouda](https://medium.com/@marawaneslam145?source=post_page---byline--1d6c96db02af---------------------------------------)\n12 min read\n\u00b7Nov 5, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/1d6c96db02af&amp;operation=register&amp;redirect=https://medium.com/@marawaneslam145/mastering-gradient-boosting-the-algorithm-that-dominates-kaggle-1d6c96db02af&amp;user=Marwan+eslam+ouda&amp;userId=2d4a6b483e03&amp;source=---header_actions--1d6c96db02af---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/1d6c96db02af&amp;operation=register&amp;redirect=https://medium.com/@marawaneslam145/mastering-gradient-boosting-the-algorithm-that-dominates-kaggle-1d6c96db02af&amp;source=---header_actions--1d6c96db02af---------------------bookmark_footer------------------)\nListen\nShare\n*A complete guide to understanding and implementing XGBoost and LightGBM for real-world machine learning*\nPress enter or click to view image in full size\n![]()\n## Mastering Gradient Boosting: The Algorithm That Dominates Kaggle\n*A complete guide to understanding and implementing XGBoost and LightGBM for real-world machine learning*\n## Introduction\nIf you\u2019ve spent any time on Kaggle or working with structured data, you\u2019ve likely encountered this pattern: the winning solution almost always includes gradient boosting. Specifically, XGBoost or LightGBM.\nWhy do these algorithms consistently outperform neural networks, random forests, and other sophisticated methods on tabular data? After using gradient boosting models to win competitions, deploy production systems, and solve countless business problems, I can tell you it\u2019s not magic \u2014it\u2019s brilliant engineering meeting solid mathematics.\nThis guide takes you from understanding the core concepts to implementing production-ready models, with real code, practical tips, and insights you won\u2019t find in the documentation.\n## Table of Contents\n1. Why Gradient Boosting Dominates\n2. Understanding Boosting from First Principles\n3. The Mathematics Behind Gradient Boosting\n4. XGBoost: The Game Changer\n5. LightGBM: Speed and Efficiency\n6. CatBoost: The Categorical Data Specialist\n7. Practical Implementation Guide\n8. Hyperparameter Tuning Mastery\n9. Advanced Techniques\n10. Production Deployment\n11. When NOT to Use Gradient Boosting\n12. Real-World Case Studies\n## 1. Why Gradient Boosting Dominates\n## The Kaggle Phenomenon\nLooking at Kaggle competition winners from 2015\u20132024, a clear pattern emerges:\n**Structured/Tabular Data:**\n* 75%+ of winning solutions use gradient boosting\n* Often XGBoost or LightGBM (sometimes both)\n* Neural networks rarely win on tabular data\n**Image/Video Data:**\n* Deep learning (CNNs) dominate\n* Gradient boosting rarely used\n**Text Data:**\n* Transformers (BERT, GPT) lead\n* Though boosting on engineered features still competes\n**Why this matters:**Most real-world business problems involve structured data \u2014customer records, transaction logs, sensor readings, financial data. Gradient boosting is the practical workhorse of data science.\n## The Performance Advantage\nGradient boosting excels because it:\n**1. Handles Complex Patterns**\n* Captures non-linear relationships automatically\n* Learns intricate feature interactions\n* Doesn\u2019t require manual feature engineering\n**2. Robust to Messy Data**\n* Works with missing values natively\n* Handles outliers well\n* Less sensitive to feature scaling\n**3. Efficient Training**\n* Faster than deep learning for tabular data\n* Works well with moderate dataset sizes\n* Reasonable computational requirements\n**4. Strong Out-of-the-Box Performance**\n* Default parameters often work well\n* Less hyperparameter tuning needed than neural networks\n* Interpretable feature importance\n## 2. Understanding Boosting from First Principles\nBefore diving into gradient boosting, let\u2019s understand boosting conceptually.\n## The Core Idea: Learning from Mistakes\nImagine you\u2019re studying for an exam:\n**Approach 1 (Traditional Learning):**\n* Study all topics equally\n* Hope you understand everything\n**Approach 2 (Boosting):**\n* Take practice test\n* Identify questions you got wrong\n* Focus extra study time on those topics\n* Take another test, repeat\nBoosting applies this second approach to machine learning.\n## Sequential Learning Process\n**Step 1:**Build a simple model (weak learner)\n* Makes predictions\n* Some correct, some wrong\n* Generally better than random guessing\n**Step 2:**Identify mistakes\n* Which samples were predicted incorrectly?\n* How far off were the predictions?\n**Step 3:**Build another model focusing on mistakes\n* Gives more weight to previously misclassified samples\n* Learns to correct previous errors\n**Step 4:**Combine models\n* First model + second model (weighted)\n* Together they\u2019re stronger than either alone\n**Step 5:**Repeat\n* Each new model focuses on remaining errors\n* Ensemble gets progressively stronger## Boosting vs. Bagging\nUnderstanding the difference clarifies why boosting works:\n**Bagging (Random Forest):**\n* Train models**in parallel**\n* Each model sees**random subset**of data\n* Models are**independent**\n* Reduces**variance**\n* Prediction:**simple average**\n**Boosting:**\n* Train models**sequentially**\n* Each model sees**all data**(weighted)\n* Models are**dependent**(learn from previous)\n* Reduces**bias**\n* Prediction:**weighted sum**\n**Key Insight:**Boosting creates a strong learner from many weak learners by making each subsequent model focus on the weaknesses of previous models.\n## 3. The Mathematics Behind Gradient Boosting\nDon\u2019t worry \u2014I\u2019ll make this intuitive, not just equations.\n## The Fundamental Equation\nGradient boosting builds an additive model:\n```\nF(x) = f\u2080(x) + f\u2081(x) + f\u2082(x) + ... + f\u2099(x)\n```\nWhere:\n* `F(x)`= final prediction\n* `f\u2080(x)`= initial model (often just the mean)\n* `f\u2081, f\u2082, ..., f\u2099`= successive trees\n* Each tree tries to correct errors of previous trees## Why \u201cGradient\u201d Boosting?\nThe \u201cgradient\u201d comes from using gradient descent to minimize loss.\n**Traditional Optimization:**\n```\nminimize: Loss(y\\_true, y\\_pred)\nby adjusting: model parameters\n```\n**Gradient Boosting:**\n```\nminimize: Loss(y\\_true, y\\_pred)\nby adjusting: predictions (by adding new trees)\n```\n**The Process:**\n**1. Calculate Residuals (Errors...",
      "url": "https://medium.com/@marawaneslam145/mastering-gradient-boosting-the-algorithm-that-dominates-kaggle-1d6c96db02af"
    },
    {
      "title": "A guide to Ensemble Learning",
      "text": "### Increase your accuracy by combining model outputs\n\nPhoto by [Pankaj Patel](https://unsplash.com/@pankajpatel?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)\n\nNobody can know everything but with help, we can overcome every obstacle. That\u2019s exactly the idea behind ensemble learning. Eventhough, individual models might produce weak results combined they might be unbeatable.\n\nAnd ensemble models are exactly that \u2013 models consisting of a combination of base models. The only difference being the way they combine the models which can range from simple methods like averaging or max voting to more complex like Boosting or stacking.\n\nEnsemble learning techniques have seen a huge jump in popularity in the last years. This is because they can help you to build a really robust model from a few \"weak\" models, which eliminates a lot of the model tuning that would else be needed to achieve good results. They are especially popular in [Data Science](https://towardsdatascience.com/tag/data-science/) competitions because for these competitions the highest accuracy is more important than the runtime or interpretability of the model. This most often isn\u2019t the case in the industry but that doesn\u2019t mean that ensembling can\u2019t be of use for such applications. Rather it means that it isn\u2019t used at the same scale for industry problems as it is for data science competitions.\n\nIn this article, we will go through the most common ensembling techniques out there. You will learn how they work and how you can use them with Python. We will work on the [House Price Regression data-set](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) which can be freely downloaded from Kaggle.\n\nAlso, the full code for both regression and classification is available on [my Github](https://github.com/TannerGilbert/Tutorials/tree/master/A%20guide%20to%20Ensemble%C2%A0Learning). If you have any questions or recommendations feel free to leave a comment down below or contact me on social media.\n\n* * *\n\n## Importing libraries and loading data-set\n\nAfter downloading the data-set we can import everything needed and load in the data-set with the following code.\n\nFigure 2: Initial data-set\n\n* * *\n\n## Feature Engineering\n\nThis data-set has quite a few interesting features and therefore we can get huge accuracy gains by creating the right features but because this isn\u2019t the topic of this article we won\u2019t go into further detail.\n\nIf you are still interested in the feature engineering I would recommend you to check out [this excellent Kaggle kernel](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard), which not only includes excellent feature engineering but also shows how to do stacking.\n\nAfter executing all the feature engineering code from the kernel we have a data-set with 221 columns \u2013 a mixture of categorical and continues once.\n\nFigure 3: Cleaned data-set\n\n* * *\n\n## Base Models\n\nBefore we can start working through the different ensembling techniques we need to define some base models which will be used for ensembling. For this data-set, we will use a Lasso regression, GradientBoosting, XGBoost, and lightGBM model.\n\nWe will validate the results using the root mean squared error and cross-validation.\n\nNow we will quickly define and train each model so we can get an idea about their base performance.\n\nThis will output the root mean squared error for each of our models:\n\n```\nLasso Regression: 0.124 (0.016)\nRandomForestRegressor: 0.152 (0.009)\nGradientBoostingRegressor: 0.126 (0.009)\nXGBRegressor: 0.128 (0.008)\nLGBMRegressor: 0.133 (0.009)\n```\n\n* * *\n\n## Ensembling Approaches\n\nAs mentioned at the start of the article there are multiple ways to ensemble models. There are simple once like max voting or averaging as well as more complex once like boosting, bagging or stacking.\n\nWhat is the same for each of them is that they hugely benefit from uncorrelated base models \u2013 models that make very different predictions. To explain why this is the case let us work through a little example from the [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/).\n\nImagine we have a data-set with all 1s as the ground truth targets. We could have 3 highly correlated models which produce the following predictions:\n\n```\n1111111100 = 80% accuracy\n1111111100 = 80% accuracy\n1011111100 = 70% accuracy\n```\n\nWhen we take a simple [majority vote](https://stats.stackexchange.com/questions/273749/majority-voting-in-ensemble-learning) \u2013 choosing the value that appears most \u2013 we see no improvement:\n\n```\n1111111100 = 80% accuracy\n```\n\nThis is because these models all learned the same thing and therefore they are also making the same mistakes. But if we instead use 3 less-performing highly uncorrelated models we can see that accuracy increases significantly:\n\n```\n1111111100 = 80% accuracy\n0111011101 = 70% accuracy\n1000101111 = 60% accuracy\n```\n\nApplying majority vote:\n\n```\n1111111101 = 90% accuracy\n```\n\nThis is a huge improvement from the 60\u201380% accuracy of the base models.\n\nNow that we have an understanding of what ensembling is and have our base models ready we can start working through the different ensembling techniques.\n\n* * *\n\n## Averaging\n\nAveraging is a simple method that is generally used for regression problems. Here the predictions are simply averaged to get a more robust result and even though this method is simple it almost always gives better results than a single model and therefore is always worth trying.\n\nFor ease of usage, we will create a class that inherits from Scikit-Learns [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), [RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html), and [TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) classes because that way we only need to overwrite the fit and predict methods to get a working model which can be used like any other Scikit-Learn model.\n\nThe model can now be used like any other Scikit Learn model. The only difference is that when creating an object we need to pass the base models which will then be trained in the fit method and used for predictions in the prediction method.\n\nBy averaging three of our base models \u2013 Lasso regression, GBM, and XGBoost \u2013 we get a significant accuracy increase.\n\n```\nRoot mean squared error loss:\nLasso Regression: 0.124 (0.016)\nGradientBoostingRegressor: 0.126 (0.009)\nXGBRegressor: 0.128 (0.008)\nAveraged Model: 0.119 (0.009)\n```\n\n* * *\n\n## Weighted Average\n\nAveraging models is great and simple but it has one major flaw and that is that most of the time one model has more predictive power than another and therefore we want to give it more weight on the final predictions.\n\nWe can achieve this by passing a weight for each of the models and then multiplying the predictions of each model with the corresponding weight. The only thing we need to look out for is that the weights need to add up to 1 in order not to change the scale of the predictions.\n\nTo now create the model we not only need to pass the base models but also an array of weights.\n\nThis even further reduces the error to an average of 0.118 with a standard deviation of 0.009 over the different folds.\n\n## Bagging\n\nBagging is a hugely popular ensembling method which is used in algorithms like Random Forest. It gains accuracy by not only averaging the models but also trying to create models that are as uncorrelated as possible by giving them different training sets.\n\nIt creates the data-set using [sampling with replacement](https://www.quora.com/Why-does-random-forest-use-sampling-with-replacement-instead-of-without-replacement) a simple but sufficient data sampling technique. To implement this we will create a method called subsample and call it for every model to create its individual data-set.\n\nWith this technique, we a...",
      "url": "https://towardsdatascience.com/a-guide-to-ensemble-learning-d3686c9bed9a"
    }
  ]
}