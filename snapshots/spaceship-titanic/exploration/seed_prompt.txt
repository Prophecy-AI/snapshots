# Seed Prompt: Binary Classification for Passenger Transport Prediction

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, key feature correlations

**Key data characteristics (from EDA):**
- Binary classification with balanced target (~50/50 split)
- ~8700 training samples, ~4300 test samples
- Significant missing values across all features (2-3% per feature)
- Mix of categorical (HomePlanet, Destination, CryoSleep, VIP) and numerical features (Age, spending columns)
- Metric: Classification Accuracy

## Feature Engineering (Critical for High Scores)

### From PassengerId (format: gggg_pp)
- Extract **Group** number (gggg) - passengers traveling together
- Calculate **GroupSize** - count of passengers per group
- Create **Solo** indicator (GroupSize == 1)

### From Cabin (format: deck/num/side)
- Extract **Deck** (A-G, T) - strong predictor (B, C decks have high transported rates)
- Extract **CabinNum** - cabin number
- Extract **Side** (P=Port, S=Starboard) - S has higher transported rate

### Spending Features
- **TotalSpending** = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck
- **NoSpending** binary indicator (TotalSpending == 0)
- Individual spending log transformations (log1p) for skewed distributions
- **SpendingPerCategory** ratios

### CryoSleep Logic (Very Important)
- CryoSleep=True passengers have ZERO spending on all amenities (use for imputation)
- CryoSleep is highly predictive: 81.8% transported rate for True vs 32.9% for False
- If any spending > 0, CryoSleep should be False

### Age Features
- Age binning (0-12, 13-17, 18-25, 26-40, 41-60, 60+)
- Young passengers (0-18) more likely transported
- Young adults (18-25) less likely transported

### Name Features (Optional Advanced)
- Extract surname for family grouping
- TFIDF-PCA on names (advanced technique from top kernels)

## Missing Value Imputation Strategy

1. **CryoSleep Logic**: If any spending > 0, impute CryoSleep=False
2. **Spending for CryoSleep**: If CryoSleep=True, impute all spending as 0
3. **Categorical**: Mode imputation or group-based imputation
4. **Numerical**: Median or KNNImputer
5. **Group-based imputation**: Use group members' values for HomePlanet, Destination, Cabin

## Models (Ranked by Performance)

### Primary Models (Use These)
1. **CatBoost** - Handles categorical features natively, strong baseline
2. **LightGBM** - Fast, good with categorical encoding
3. **XGBoost** - Robust, well-tuned hyperparameters available

### Hyperparameter Ranges (from top kernels)
**XGBoost:**
- learning_rate: 0.05-0.1
- max_depth: 4-6
- n_estimators: 500-1000
- colsample_bytree: 0.8-1.0
- subsample: 0.8-1.0
- lambda (L2): 1-5
- alpha (L1): 1-5

**LightGBM:**
- learning_rate: 0.05-0.1
- num_leaves: 30-100
- n_estimators: 500-1000
- feature_fraction: 0.6-0.9
- bagging_fraction: 0.7-0.9

**CatBoost:**
- learning_rate: 0.03-0.1
- depth: 4-8
- iterations: 500-1500

## Validation Strategy

- **StratifiedKFold** with k=5 or k=10 (10 recommended for final models)
- Use multiple random seeds and average predictions
- Track both CV accuracy and leaderboard score for consistency

## Ensembling Strategies (Critical for Top Scores)

### Soft Voting (Recommended)
- Average predicted probabilities from multiple models
- Use LightGBM + CatBoost + XGBoost ensemble

### Optuna Weight Optimization
- Use Optuna to find optimal weights for combining model predictions
- Optimize weights on validation set

### Threshold Tuning
- Default threshold is 0.5
- Tune threshold to match train/test target distribution
- Can improve accuracy by 0.1-0.5%

## Post-Processing

1. **Threshold optimization**: Tune classification threshold on validation set
2. **Distribution matching**: Ensure prediction distribution matches training distribution (~50/50)
3. **Ensemble voting**: Use mode across multiple model predictions

## Code Structure Recommendations

1. Combine train and test for consistent feature engineering
2. Apply same preprocessing pipeline to both
3. Use pipelines for reproducibility
4. Save intermediate predictions for ensembling

## Target Score Strategy

To beat 0.9642 accuracy:
1. Strong feature engineering (especially Cabin, Group, Spending features)
2. Proper missing value imputation using domain logic
3. Ensemble of 3+ gradient boosting models
4. Optuna hyperparameter tuning
5. Multiple seeds averaging
6. Threshold optimization
