## Problem Type
This is a binary classification problem predicting whether a Reddit pizza request will be successful. The evaluation metric is AUC-ROC.

## Data Characteristics (Reference: exploration/eda.ipynb)
- **Multimodal data**: Text (request title, text) + tabular meta-data (user activity, votes, timestamps)
- **Moderate class imbalance**: ~25% positive rate (pizza received)
- **Key predictive signals**: User activity in RAOP subreddit, user flair, text characteristics
- **Missing data**: User flair missing for ~75% of samples

## Core Modeling Strategy

### 1. Multimodal Architecture
**Primary approach**: Combine text embeddings with tabular features
- **Text branch**: Use transformer models (BERT, RoBERTa) or social-media specialized models (BERTweet, SocBERT) to encode request text and title
- **Tabular branch**: Gradient boosting models (LightGBM, CatBoost, XGBoost) on numerical and categorical meta-data
- **Fusion**: Concatenate text embeddings with tabular features and feed to final classifier, OR ensemble predictions from both branches

**Alternative**: Serialize meta-data as text and prepend to request text, then use single transformer model

### 2. Text Feature Engineering
- **Preprocessing**: Preserve Reddit-specific tokens (usernames, subreddit mentions), handle URLs, emojis
- **Length features**: Extract title length, text length, word count (strong signals in social media)
- **Sentiment/LIWC**: Consider adding linguistic features, sentiment scores
- **TF-IDF**: As backup for simpler models or hybrid approaches

### 3. Tabular Feature Engineering
- **User activity features**: RAOP-specific activity (posts, comments) most predictive
- **User flair encoding**: One-hot or target encoding (None/shroom/PIF highly predictive)
- **Temporal features**: Extract day of week, hour from timestamps
- **Vote ratios**: Upvote/downvote ratios and differences
- **Interaction terms**: User activity × text length, flair × activity

### 4. Handling Class Imbalance
- **Metric**: Optimize for AUC-ROC (given evaluation metric)
- **Class weights**: Use scale_pos_weight (XGBoost/LightGBM) or class_weight='balanced'
- **Resampling**: Consider SMOTE or borderline-SMOTE for minority class
- **Threshold tuning**: Optimize decision threshold on validation set

### 5. Model Selection
**Primary candidates**:
- **LightGBM**: Fast, handles class imbalance well, good with mixed data types
- **CatBoost**: Excellent with categorical features, handles missing values
- **XGBoost**: Robust baseline, good ensembling properties
- **Transformers**: BERT/RoBERTa for text, consider fine-tuning on social media data

**Ensembling strategy**:
- Stack 3-5 diverse models (e.g., LightGBM + CatBoost + Transformer)
- Use out-of-fold predictions for meta-learner
- Simple averaging often effective for homogeneous models

### 6. Validation Strategy
- **Stratified K-Fold**: Essential for maintaining class distribution
- **5 folds**: Standard for this dataset size
- **Early stopping**: Use AUC-ROC for monitoring
- **Feature importance**: Analyze to identify leakage or overfitting

### 7. Key Insights from EDA
Reference `exploration/eda.ipynb` for:
- Feature correlation analysis (RAOP activity highly predictive)
- Text length distributions and patterns
- User flair impact on success rate
- Missing value patterns

### 8. Implementation Notes
- **Memory management**: Text data can be large, use efficient tokenization
- **Cross-validation**: Generate out-of-fold predictions for ensembling
- **Submission**: Calibrate probabilities if needed, but AUC-ROC is ranking-based
- **Leakage check**: Ensure no future information leaks into features