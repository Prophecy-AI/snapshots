{
  "spans": [
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "f6ef936227807bca",
      "parent_span_id": "7afc9600afa471be",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767938501673012543,
      "end_time": 1767938501673363846,
      "duration": 0.000351303,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7df814611590>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.0KB)\\n[FILE] sampleSubmission.csv (13.0KB)\\n[FILE] test.json (2.0MB)\\n[FILE] test.zip (433.1KB)\\n[FILE] train.json (8.4MB)\\n[FILE] train.zip (1.3MB)\", \"is_error\": false}",
        "tool.duration_ms": 0.43129920959472656
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "8365cde5b20ca990",
      "parent_span_id": "7afc9600afa471be",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767938507368554625,
      "end_time": 1767938507369140443,
      "duration": 0.000585818,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7df817885510>\", \"input\": {\"file_path\": \"/home/data/description.md\"}}",
        "tool.output": "{\"content\": \"# Overview\\n\\n## Description\\n\\n[Get started on this competition through Kaggle Scripts](https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza)\\n\\nIn machine learning, it is often said there are [no free lunches](http://en.wikipedia.org/wiki/No_free_lunch_theorem). *How wrong we were.*\\n\\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/) together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\\n\\n\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?\\n\\n![Pizzas](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png)\\n\\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This [data](http://cs.stanford.edu/~althoff/raop-dataset/) was collected and graciously shared by [Althoff](http://www.timalthoff.com/) et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their [accompanying paper](http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf) and ask that you cite the following reference in any publications that result from your work:\\n\\nTim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. *How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*, Proceedings of ICWSM, 2014.\\n\\n~*Pizza icons designed by Matthew Dera from the [Noun Project](http://thenounproject.com/term/pizza/30194/)*~\\n\\n### Evaluation\\n\\nSubmissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability that a request will get pizza and the observed outcomes.\\n\\n### Submission File\\n\\nFor each request in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:\\n\\n```\\nrequest_id,requester_received_pizza\\nt3_i8iy4,0\\nt3_1mfqi0,0\\nt3_lclka,0\\n...\\n...\\n```\\n\\n### Citation\\n\\nWill Cukierski. (2014). Random Acts of Pizza. Kaggle. https://kaggle.com/competitions/random-acts-of-pizza\\n\\n\\n# Data\\n\\n## Dataset Description\\n\\n[See, fork, and run a random forest benchmark model through Kaggle Scripts](https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark)\\n\\nThis dataset includes 5671 requests collected from the Reddit community [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/) between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\\n\\nEach JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\\n\\n## Data fields\\n\\n\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).\\n\\n\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.\\n\\n\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.\\n\\n\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).\\n\\n\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".\\n\\n\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.\\n\\n\\\"request_text\\\": Full text of the request.\\n\\n\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".\\n\\n\\\"request_title\\\": Title of the request.\\n\\n\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.\\n\\n\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.\\n\\n\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\\n\\n\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.\\n\\n\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\\n\\n\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).\\n\\n\\\"requester_username\\\": Reddit username of requester.\\n\\n\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\\n\\n\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.\", \"is_error\": false}",
        "tool.duration_ms": 0.6415843963623047
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "2f1783c656a01c1b",
      "parent_span_id": "7afc9600afa471be",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767938526317496580,
      "end_time": 1767938530016396423,
      "duration": 3.698899843,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7df814610a50>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = '/home/data/train.json'\\n\\n# Read the JSON file\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of records: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First record keys: {list(train_data[0].keys())}\\\")\"}, {\"cell_type\": \"code\", \"content\": \"# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\nprint(f\\\"DataFrame shape: {df.shape}\\\")\\nprint(\\\"\\\\nColumn names:\\\")\\nfor col in df.columns:\\n    print(f\\\"- {col}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 3698.906660079956
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "7fcfeefed8f55b44",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938535170463096,
      "end_time": 1767938535765721455,
      "duration": 0.595258359,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 0, \"end_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-1\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = '/home/data/train.json'\\n\\n# Read the JSON file\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of records: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First record keys: {list(train_data[0].keys())}\\\")\\n```\\nOutput:\\nNumber of records: 2878\\nType of data: <class 'list'>\\nFirst record keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\\n\\nCell 1:\\n```\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\nprint(f\\\"DataFrame shape: {df.shape}\\\")\\nprint(\\\"\\\\nColumn names:\\\")\\nfor col in df.columns:\\n    print(f\\\"- {col}\\\")\\n```\\nOutput:\\nDataFrame shape: (2878, 32)\\n\\nColumn names:\\n- giver_username_if_known\\n- number_of_downvotes_of_request_at_retrieval\\n- number_of_upvotes_of_request_at_retrieval\\n- post_was_edited\\n- request_id\\n- request_number_of_comments_at_retrieval\\n- request_text\\n- request_text_edit_aware\\n- request_title\\n- requester_account_age_in_days_at_request\\n- requester_account_age_in_days_at_retrieval\\n- requester_days_since_first_post_on_raop_at_request\\n- requester_days_since_first_post_on_raop_at_retrieval\\n- requester_number_of_comments_at_request\\n- requester_number_of_comments_at_retrieval\\n- requester_number_of_comments_in_raop_at_request\\n- requester_number_of_comments_in_raop_at_retrieval\\n- requester_number_of_posts_at_request\\n- requester_number_of_posts_at_retrieval\\n- requester_number_of_posts_on_raop_at_request\\n- requester_number_of_posts_on_raop_at_retrieval\\n- requester_number_of_subreddits_at_request\\n- requester_received_pizza\\n- requester_subreddits_at_request\\n- requester_upvotes_minus_downvotes_at_request\\n- requester_upvotes_minus_downvotes_at_retrieval\\n- requester_upvotes_plus_downvotes_at_request\\n- requester_upvotes_plus_downvotes_at_retrieval\\n- requester_user_flair\\n- requester_username\\n- unix_timestamp_of_request\\n- unix_timestamp_of_request_utc\\n\", \"is_error\": false}",
        "tool.duration_ms": 595.2918529510498
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "c99c3c156893f98a",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938544142991724,
      "end_time": 1767938544145194066,
      "duration": 0.002202342,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze target distribution\\ntarget = 'requester_received_pizza'\\nprint(\\\"Target distribution:\\\")\\nprint(df[target].value_counts())\\nprint(f\\\"\\\\nTarget percentages:\\\")\\nprint(df[target].value_counts(normalize=True) * 100)\\nprint(f\\\"\\\\nClass imbalance ratio: {df[target].value_counts().min() / df[target].value_counts().max():.4f}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 2.273082733154297
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "96f0f4270030b020",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938552696013535,
      "end_time": 1767938552745532757,
      "duration": 0.049519222,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 2, \"end_cell\": 2, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Analyze target distribution\\ntarget = 'requester_received_pizza'\\nprint(\\\"Target distribution:\\\")\\nprint(df[target].value_counts())\\nprint(f\\\"\\\\nTarget percentages:\\\")\\nprint(df[target].value_counts(normalize=True) * 100)\\nprint(f\\\"\\\\nClass imbalance ratio: {df[target].value_counts().min() / df[target].value_counts().max():.4f}\\\")\\n```\\nOutput:\\nTarget distribution:\\nrequester_received_pizza\\nFalse    2163\\nTrue      715\\nName: count, dtype: int64\\n\\nTarget percentages:\\nrequester_received_pizza\\nFalse    75.156359\\nTrue     24.843641\\nName: proportion, dtype: float64\\n\\nClass imbalance ratio: 0.3306\\n\", \"is_error\": false}",
        "tool.duration_ms": 49.61204528808594
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "0b0faf5df2b6de93",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938569523186379,
      "end_time": 1767938569525602569,
      "duration": 0.00241619,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze feature types\\nprint(\\\"Feature types:\\\")\\nprint(df.dtypes.value_counts())\\n\\nprint(\\\"\\\\nSample of text features:\\\")\\nprint(f\\\"Request title sample: {df['request_title'].iloc[0][:100]}...\\\")\\nprint(f\\\"Request text sample: {df['request_text'].iloc[0][:100]}...\\\")\\n\\nprint(\\\"\\\\nSample of categorical features:\\\")\\ncategorical_cols = df.select_dtypes(include=['object']).columns\\nprint(f\\\"Categorical columns: {list(categorical_cols)}\\\")\\n\\nprint(\\\"\\\\nSample of numerical features:\\\")\\nnumerical_cols = df.select_dtypes(exclude=['object']).columns\\nprint(f\\\"Numerical columns count: {len(numerical_cols)}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 3\", \"is_error\": false}",
        "tool.duration_ms": 2.4688243865966797
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "931904d433f9b6d5",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938577842039477,
      "end_time": 1767938577857496104,
      "duration": 0.015456627,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 3, \"end_cell\": 3, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Executed cells 3-3\\n==================================================\\n\\nCell 3:\\n```\\n# Analyze feature types\\nprint(\\\"Feature types:\\\")\\nprint(df.dtypes.value_counts())\\n\\nprint(\\\"\\\\nSample of text features:\\\")\\nprint(f\\\"Request title sample: {df['request_title'].iloc[0][:100]}...\\\")\\nprint(f\\\"Request text sample: {df['request_text'].iloc[0][:100]}...\\\")\\n\\nprint(\\\"\\\\nSample of categorical features:\\\")\\ncategorical_cols = df.select_dtypes(include=['object']).columns\\nprint(f\\\"Categorical columns: {list(categorical_cols)}\\\")\\n\\nprint(\\\"\\\\nSample of numerical features:\\\")\\nnumerical_cols = df.select_dtypes(exc\\n```\\nOutput:\\nFeature types:\\nint64      16\\nobject      9\\nfloat64     6\\nbool        1\\nName: count, dtype: int64\\n\\nSample of text features:\\nRequest title sample: [REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy....\\nRequest text sample: I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some in...\\n\\nSample of categorical features:\\nCategorical columns: ['giver_username_if_known', 'post_was_edited', 'request_id', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_subreddits_at_request', 'requester_user_flair', 'requester_username']\\n\\nSample of numerical features:\\nNumerical columns count: 23\\n\", \"is_error\": false}",
        "tool.duration_ms": 15.47098159790039
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "280f2bc3527aa130",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938600642125623,
      "end_time": 1767938600644369649,
      "duration": 0.002244026,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Check for missing values\\nprint(\\\"Missing values per column:\\\")\\nmissing_counts = df.isnull().sum()\\nmissing_pct = (missing_counts / len(df)) * 100\\nmissing_df = pd.DataFrame({\\n    'missing_count': missing_counts,\\n    'missing_pct': missing_pct\\n}).sort_values('missing_count', ascending=False)\\n\\nprint(missing_df[missing_df['missing_count'] > 0])\\n\\nprint(f\\\"\\\\nTotal columns with missing values: {(missing_counts > 0).sum()}\\\")\\nprint(f\\\"Total missing values: {missing_counts.sum()}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 4\", \"is_error\": false}",
        "tool.duration_ms": 2.282381057739258
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "6dcf78f7354bef31",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938610244653863,
      "end_time": 1767938610299048755,
      "duration": 0.054394892,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 4, \"end_cell\": 4, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Executed cells 4-4\\n==================================================\\n\\nCell 4:\\n```\\n# Check for missing values\\nprint(\\\"Missing values per column:\\\")\\nmissing_counts = df.isnull().sum()\\nmissing_pct = (missing_counts / len(df)) * 100\\nmissing_df = pd.DataFrame({\\n    'missing_count': missing_counts,\\n    'missing_pct': missing_pct\\n}).sort_values('missing_count', ascending=False)\\n\\nprint(missing_df[missing_df['missing_count'] > 0])\\n\\nprint(f\\\"\\\\nTotal columns with missing values: {(missing_counts > 0).sum()}\\\")\\nprint(f\\\"Total missing values: {missing_counts.sum()}\\\")\\n```\\nOutput:\\nMissing values per column:\\n                      missing_count  missing_pct\\nrequester_user_flair           2163    75.156359\\n\\nTotal columns with missing values: 1\\nTotal missing values: 2163\\n\", \"is_error\": false}",
        "tool.duration_ms": 54.4276237487793
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "a76578e8f2a2d224",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938624947629931,
      "end_time": 1767938624950461214,
      "duration": 0.002831283,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze key numerical features correlation with target\\nnumerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\\n# Remove target from the list\\nif target in numerical_features:\\n    numerical_features.remove(target)\\n\\nprint(f\\\"Analyzing {len(numerical_features)} numerical features\\\")\\n\\n# Calculate correlation with target\\ncorrelations = df[numerical_features + [target]].corr()[target].sort_values(ascending=False)\\nprint(\\\"\\\\nTop 10 features most correlated with target:\\\")\\nprint(correlations.head(11))  # 11 because target correlates with itself\\n\\nprint(\\\"\\\\nBottom 10 features most correlated with target:\\\")\\nprint(correlations.tail(10))\"}}",
        "tool.output": "{\"content\": \"Added code cell at 5\", \"is_error\": false}",
        "tool.duration_ms": 2.8917789459228516
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "7112af359d8091e7",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938629108437803,
      "end_time": 1767938629129976347,
      "duration": 0.021538544,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 5, \"end_cell\": 5, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Executed cells 5-5\\n==================================================\\n\\nCell 5:\\n```\\n# Analyze key numerical features correlation with target\\nnumerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\\n# Remove target from the list\\nif target in numerical_features:\\n    numerical_features.remove(target)\\n\\nprint(f\\\"Analyzing {len(numerical_features)} numerical features\\\")\\n\\n# Calculate correlation with target\\ncorrelations = df[numerical_features + [target]].corr()[target].sort_values(ascending=False)\\nprint(\\\"\\\\nTop 10 features most correlated with target:\\\")\\nprint\\n```\\nOutput:\\nAnalyzing 22 numerical features\\n\\nTop 10 features most correlated with target:\\nrequester_received_pizza                                1.000000\\nrequester_number_of_posts_on_raop_at_retrieval          0.462165\\nrequest_number_of_comments_at_retrieval                 0.290709\\nrequester_number_of_comments_in_raop_at_retrieval       0.277129\\nrequester_number_of_posts_on_raop_at_request            0.132712\\nrequester_number_of_comments_in_raop_at_request         0.131965\\nrequester_days_since_first_post_on_raop_at_retrieval    0.127262\\nrequester_number_of_comments_at_retrieval               0.123016\\nrequester_account_age_in_days_at_retrieval              0.118863\\nrequester_days_since_first_post_on_raop_at_request      0.108662\\nnumber_of_upvotes_of_request_at_retrieval               0.090767\\nName: requester_received_pizza, dtype: float64\\n\\nBottom 10 features most correlated with target:\\nrequester_upvotes_plus_downvotes_at_retrieval    0.052447\\nrequester_number_of_subreddits_at_request        0.047001\\nrequester_account_age_in_days_at_request         0.043374\\nrequester_upvotes_minus_downvotes_at_request     0.043084\\nrequester_number_of_posts_at_request             0.037975\\nrequester_number_of_comments_at_request          0.036705\\nrequester_upvotes_plus_downvotes_at_request      0.033247\\nnumber_of_downvotes_of_request_at_retrieval      0.020646\\nunix_timestamp_of_request_utc                   -0.099327\\nunix_timestamp_of_request                       -0.099329\\nName: requester_received_pizza, dtype: float64\\n\", \"is_error\": false}",
        "tool.duration_ms": 21.544694900512695
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "6f9f3dc237208da9",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938643356567444,
      "end_time": 1767938643359413417,
      "duration": 0.002845973,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze text length features\\ndf['request_title_length'] = df['request_title'].str.len()\\ndf['request_text_length'] = df['request_text'].str.len()\\ndf['request_text_edit_aware_length'] = df['request_text_edit_aware'].str.len()\\n\\nprint(\\\"Text length statistics:\\\")\\nprint(df[['request_title_length', 'request_text_length', 'request_text_edit_aware_length']].describe())\\n\\n# Check correlation of text lengths with target\\nlength_correlations = df[['request_title_length', 'request_text_length', 'request_text_edit_aware_length', target]].corr()[target]\\nprint(f\\\"\\\\nCorrelations with target:\\\")\\nprint(length_correlations)\"}}",
        "tool.output": "{\"content\": \"Added code cell at 6\", \"is_error\": false}",
        "tool.duration_ms": 2.916097640991211
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "fcb610a03957aca9",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938647870951336,
      "end_time": 1767938647897235229,
      "duration": 0.026283893,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 6, \"end_cell\": 6, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Executed cells 6-6\\n==================================================\\n\\nCell 6:\\n```\\n# Analyze text length features\\ndf['request_title_length'] = df['request_title'].str.len()\\ndf['request_text_length'] = df['request_text'].str.len()\\ndf['request_text_edit_aware_length'] = df['request_text_edit_aware'].str.len()\\n\\nprint(\\\"Text length statistics:\\\")\\nprint(df[['request_title_length', 'request_text_length', 'request_text_edit_aware_length']].describe())\\n\\n# Check correlation of text lengths with target\\nlength_correlations = df[['request_title_length', 'request_text_length', 'request_text_\\n```\\nOutput:\\nText length statistics:\\n       request_title_length  request_text_length  \\\\\\ncount           2878.000000          2878.000000   \\nmean              71.572967           402.521543   \\nstd               36.233487           362.393727   \\nmin                7.000000             0.000000   \\n25%               46.000000           182.000000   \\n50%               64.000000           308.000000   \\n75%               90.000000           503.750000   \\nmax              272.000000          4460.000000   \\n\\n       request_text_edit_aware_length  \\ncount                     2878.000000  \\nmean                       394.567755  \\nstd                        351.922518  \\nmin                          0.000000  \\n25%                        180.000000  \\n50%                        302.000000  \\n75%                        498.000000  \\nmax                       4460.000000  \\n\\nCorrelations with target:\\nrequest_title_length              0.014568\\nrequest_text_length               0.130130\\nrequest_text_edit_aware_length    0.119936\\nrequester_received_pizza          1.000000\\nName: requester_received_pizza, dtype: float64\\n\", \"is_error\": false}",
        "tool.duration_ms": 26.311397552490234
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "52f77bcb90ae43c4",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938662144337576,
      "end_time": 1767938662147520841,
      "duration": 0.003183265,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze user flair distribution\\nprint(\\\"User flair distribution:\\\")\\nprint(df['requester_user_flair'].value_counts(dropna=False))\\n\\nprint(\\\"\\\\nUser flair distribution by target:\\\")\\nflair_crosstab = pd.crosstab(df['requester_user_flair'], df[target], normalize='index') * 100\\nprint(flair_crosstab.round(2))\\n\\n# Calculate success rate by flair\\nsuccess_by_flair = df.groupby('requester_user_flair')[target].mean() * 100\\nprint(f\\\"\\\\nSuccess rate by flair:\\\")\\nprint(success_by_flair.round(2))\"}}",
        "tool.output": "{\"content\": \"Added code cell at 7\", \"is_error\": false}",
        "tool.duration_ms": 3.2215118408203125
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "e024ea99e286fd0c",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938669023706213,
      "end_time": 1767938669171350330,
      "duration": 0.147644117,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 7, \"end_cell\": 7, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Executed cells 7-7\\n==================================================\\n\\nCell 7:\\n```\\n# Analyze user flair distribution\\nprint(\\\"User flair distribution:\\\")\\nprint(df['requester_user_flair'].value_counts(dropna=False))\\n\\nprint(\\\"\\\\nUser flair distribution by target:\\\")\\nflair_crosstab = pd.crosstab(df['requester_user_flair'], df[target], normalize='index') * 100\\nprint(flair_crosstab.round(2))\\n\\n# Calculate success rate by flair\\nsuccess_by_flair = df.groupby('requester_user_flair')[target].mean() * 100\\nprint(f\\\"\\\\nSuccess rate by flair:\\\")\\nprint(success_by_flair.round(2))\\n```\\nOutput:\\nUser flair distribution:\\nrequester_user_flair\\nNone      2163\\nshroom     677\\nPIF         38\\nName: count, dtype: int64\\n\\nUser flair distribution by target:\\nrequester_received_pizza   True\\nrequester_user_flair           \\nPIF                       100.0\\nshroom                    100.0\\n\\nSuccess rate by flair:\\nrequester_user_flair\\nPIF       100.0\\nshroom    100.0\\nName: requester_received_pizza, dtype: float64\\n\", \"is_error\": false}",
        "tool.duration_ms": 147.688627243042
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "21dfde2a02d561f1",
      "parent_span_id": "7afc9600afa471be",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1767938682773960737,
      "end_time": 1767938686880718245,
      "duration": 4.106757508,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7df814613cd0>\", \"input\": {\"query\": \"What are winning approaches for text classification with tabular features on Kaggle?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nWinning Kaggle solutions for problems that mix free‑form text with traditional numeric/categorical columns usually treat the data as **multimodal** and combine the strengths of modern language models with high‑performance tabular learners.  A common recipe is to let a pretrained transformer (e.g., BERT, RoBERTa) encode each text field into a dense vector, then feed those vectors together with the original tabular features into gradient‑boosted decision‑tree models such as LightGBM, CatBoost or Random Forest.  This “two‑stage” pipeline lets the tree‑based model exploit the rich semantic signal from the embeddings while still handling categorical encodings, missing values and interactions that trees capture well ([Papers with Code benchmark](https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular)).\\n\\nA more integrated alternative is to **train a single multimodal network** that ingests both modalities directly.  AutoGluon’s TabularPredictor treats raw text as a first‑class column and automatically stacks classical tabular models with its own pretrained NLP backbone, producing a blended predictor that often tops the leaderboard out‑of‑the‑box ([AutoGluon docs](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  The open‑source **Multimodal‑Toolkit** follows a similar idea: it converts categorical and numeric fields into embeddings, concatenates them with transformer token embeddings, and fine‑tunes the whole model end‑to‑end, simplifying the engineering effort while achieving state‑of‑the‑art results ([Multimodal‑Toolkit paper](https://aclanthology.org/2021.maiworkshop-1.10.pdf)).  A very simple but surprisingly effective trick—used in a Kaggle clothing‑review competition—is to **prepend the tabular information as textual tokens** to the original document and feed the combined string to BERT; this lets the language model learn joint representations without any extra architecture ([McCormickML blog](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).\\n\\nFinally, top teams reinforce these core ideas with classic Kaggle engineering: robust target‑encoding for high‑cardinality categories, TF‑IDF or n‑gram features for short text, extensive cross‑validation, GPU‑accelerated hyper‑parameter sweeps, and **ensembling** of pure‑text (fine‑tuned BERT) and pure‑tabular (Boosted Trees) predictions via weighted averaging or stacking.  The Kaggle Grandmasters Playbook highlights that such model‑level stacking, together with careful feature preprocessing, consistently yields the last few percentage points needed to win ([NVIDIA Grandmasters Playbook](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\\n\\n--- Result 2 ---\\nTitle: Combining BERT/Transformers and Classical Tabular Models — AutoGluon Documentation 0.7.0 documentation\\nURL: https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html\\n\\nContent:\\n<div><div>\\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\\\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\\\">¶</a></h2>\\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\\n<a href=\\\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\\\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\\naddition to text and tabular features.</p>\\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\\ntabular data that contains text, numeric, and categorical columns. In\\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\\ndata tables. AutoGluon Tabular can help you train and combine a diverse\\nset of models including classical tabular models like\\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\\nmultimodal network that is introduced in Section\\n“<span>sec_textprediction_architecture</span>” of\\n<span>sec_textprediction_multimodal</span> (used by AutoGluon’s\\n<code><span>TextPredictor</span></code>).</p>\\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\\n<span>import</span> <span>pprint</span>\\n<span>import</span> <span>random</span>\\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n</pre></div>\\n<div>\\n<h2>Product Sentiment Analysis Dataset<a href=\\\"#product-sentiment-analysis-dataset\\\">¶</a></h2>\\n<p>We consider the product sentiment analysis dataset from a <a href=\\\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\\\">MachineHack\\nhackathon</a>.\\nThe goal is to predict a user’s sentiment towards a product given their\\nreview (raw text) and a categorical feature indicating the product’s\\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\\ndataset to be 90% for training and 10% for development/testing (if\\nsubmitting your models to the hackathon, we recommend training them on\\n100% of the dataset).</p>\\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\\n</pre></div>\\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 689486 (673K) [text/csv]\\nSaving to: ‘product_sentiment_machine_hack/train.csv’\\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\\n2023-02-22 23:29:24 (73.9 MB/s) - ‘product_sentiment_machine_hack/train.csv’ saved [689486/689486]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 75517 (74K) [text/csv]\\nSaving to: ‘product_sentiment_machine_hack/dev.csv’\\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\\n2023-02-22 23:29:24 (57.6 MB/s) - ‘product_sentiment_machine_hack/dev.csv’ saved [75517/75517]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 312194 (305K) [text/csv]\\nSaving to: ‘product_sentiment_machine_hack/test.csv’\\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\\n2023-02-22 23:29:24 (137 MB/s) - ‘product_sentiment_machine_hack/test.csv’ saved [312194/312194]\\n</pre></div>\\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\\n</pre></div>\\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\\n</pre></div>\\n<p>There are two features in the dataset: the users’ review of the product\\nand the product’s type, and four possible classes to predict.</p>\\n<div>\\n<table>\\n <thead>\\n <tr>\\n <th></th>\\n <th>Product_Description</th>\\n <th>Product_Type</th>\\n <th>Sentiment</th>\\n </tr>\\n </thead>\\n <tbody>\\n <tr>\\n <th>4532</th>\\n <td>they took away the lego pit but ...\\n\\n--- Result 3 ---\\nTitle: The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\\nURL: https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data\\n\\nContent:\\nThe Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\\n[Technical Blog](https://developer.nvidia.com/blog)\\n[Subscribe](https://developer.nvidia.com/email-signup)\\n[Related Resources**](#main-content-end)\\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\\nEnglish中文\\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\\nLessons from years of competitions, made practical with GPU acceleration.\\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\\nSep 18, 2025\\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Théo Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\\n**\\nLike\\n[**Discuss (1)](#entry-content-comments)\\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\\nAI-Generated Summary\\nLike\\nDislike\\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard—no matter if we’re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn’t just a collection of modeling tricks—it’s a repeatable system for solving real-world tabular problems fast.\\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you’re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\\nWe’ve included links to example write-ups or notebooks from past competitions for each technique.\\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you’ll see below pre-installed.\\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\\nBefore diving into techniques, it’s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren’t optional best practices—they’re the foundation of how we approach every tabular modeling problem.\\n### Fast experimentation[**](#fast_experimentation)\\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover—and the faster we catch when a model is failing, drifting, or overfitting—so we can course-correct early and improve faster.\\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\\n**Here’s how we make it work:**\\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\\nGPU acceleration isn’t just for deep learning—it&#8217;s often the only way to make advanced tabular techniques practical at scale.\\n### Local Validation[**](#local_validation)\\nIf you can’t trust your validation score, you’re flying blind. That’s why cross-validation (CV) is a cornerstone to our workflow.\\n**Our approach:**\\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that’s held out.\\n* Rotate through folds so every part of the data is tested once.\\nThis gives a much more reliable measure of performance than a single train/validation split.\\n**Pro tip:**Match your CV strategy to how the test data is structured.\\n**For example:**\\n* Use TimeSeriesSplit for time-dependent data\\n* Use GroupKFold for grouped data (like users or patients)\\nWith those foundations in place—moving fast and validating carefully—we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they’re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper—a couple of quick checks that we’ve found useful, but many people miss:\\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift—test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\\nThese techniques aren&#8217;t brand new—but they’re often overlooked, and ignoring them can sink a project.\\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\\n**In action:**In the winning solution to the Amazon KDD Cup ‘23, the team uncovered both a train—test distribution shift and temporal patterns in the target—insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...\\n\\n--- Result 4 ---\\nTitle: Papers with Code - Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\nURL: https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular\\n\\nContent:\\nnew\\n\\nGet trending papers in your email inbox once a day!\\n\\nGet trending papers in your email inbox!\\n\\n[Subscribe](https://paperswithcode.com/login?next=%2Fpapers)\\n\\n# Trending Papers\\n\\n## by [AK](https://paperswithcode.com/akhaliq) and the research community\\n\\n- Daily\\n- Weekly\\n- Monthly\\n\\nTrending Papers\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n·Published on Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n·Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n·Published on Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n·Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\ncsuhan\\n\\n### [ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\\\ Automation via Modular Multimodal Agents](https://paperswithcode.com/papers/2507.22827)\\n\\nAutomating the transformation of user interface (UI) designs into front-end\\ncode holds significant promise for accelerating software development and\\ndemocratizing design workflows. While recent large language models (LLMs) have\\ndemonstrated progress in text-to-code generation, many existing approaches rely\\nsolely o...\\n\\n--- Result 5 ---\\nURL: https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert\\n\\nContent:\\n# Combining Categorical and Numerical Features with Text in BERT\\n\\n29 Jun 2021\\n\\nIn this tutorial we’ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\\n\\nTo help motivate our discussion, we’ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\\n\\n- The age of the reviewer (numerical feature)\\n- The number of upvotes on the review (numerical feature)\\n- The department and category of the clothing item (categorical features)\\n\\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\\n\\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\\n\\nIn Section 2 of this Notebook, I’ve implemented four different “baseline” strategies which score fairly well, but which don’t incorporate all of the features together.\\n\\nThen, in Section 3, I’ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\\n\\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\\n\\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\\n\\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\\n\\nBy Chris McCormick\\n\\n# Contents\\n\\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\\n\\n# S1. Clothing Review Dataset\\n\\n## 1.1. Download & Parse\\n\\nRetrieve the .csv file for the dataset.\\n\\n```\\nimport gdown\\n\\nprint('Downloading dataset...\\\\n')\\n\\n# Download the file.\\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\\n                'Womens Clothing E-Commerce Reviews.csv',\\n                quiet=False)\\n\\nprint('\\\\n\\\\nDONE.')\\n```\\n\\n```\\nDownloading dataset...\\n\\nDownloading...\\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\\nTo: /content/Womens Clothing E-Commerce Reviews.csv\\n8.48MB [00:00, 48.7MB/s]\\n\\nDONE.\\n\\n```\\n\\nParse the dataset csv file into a pandas DataFrame.\\n\\n```\\nimport pandas as pd\\n\\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\\n\\ndata_df.head()\\n```\\n\\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\\n\\n_Features_\\n\\n“ **Recommended IND**” is the label we are trying to predict for this dataset. “1” means the reviewer recommended the product and “0” means they do not.\\n\\nThe following are _categorical_ features:\\n\\n- Division Name\\n- Department Name\\n- Class Name\\n- Clothing ID\\n\\nAnd the following are _numerical_ features:\\n\\n- Age\\n- Rating\\n- Positive Feedback Count\\n\\n_Feature Analysis_\\n\\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\\n\\nNote that, in addition to the “Recommended” label, there is also a “ **Rating**” column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We’ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\\n\\n## 1.2. Train-Validation-Test Split\\n\\nI want to use the same training, validation, and test splits for all of the approaches we try so that it’s a fair comparison.\\n\\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\\n\\nTo solve this, we’re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\\n\\n```\\nimport random\\nimport numpy as np\\n\\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\\ntrain_size = int(0.8 * len(data_df))\\nval_size = int(0.1 * len(data_df))\\ntest_size = len(data_df) - (train_size + val_size)\\n\\n# Sanity check the sizes.\\nassert((train_size + val_size + test_size) == len(data_df))\\n\\n# Create a list of indeces for all of the samples in the dataset.\\nindeces = np.arange(0, len(data_df))\\n\\n# Shuffle the indeces randomly.\\nrandom.shuffle(indeces)\\n\\n# Get a list of indeces for each of the splits.\\ntrain_idx = indeces[0:train_size]\\nval_idx = indeces[train_size:(train_size + val_size)]\\ntest_idx = indeces[(train_size + val_size):]\\n\\n# Sanity check\\nassert(len(train_idx) == train_size)\\nassert(len(test_idx) == test_size)\\n\\n# With these lists, we can now select the corresponding dataframe rows using,\\n# e.g., train_df = data_df.iloc[train_idx]\\n\\nprint('  Training size: {:,}'.format(train_size))\\n...\\n\\n--- Result 6 ---\\nTitle: A Package for Learning on Tabular and Text Data with Transformers\\nURL: https://aclanthology.org/2021.maiworkshop-1.10.pdf\\n\\nContent:\\nProceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 69–73\\nJune 6, 2021. ©2021 Association for Computational Linguistics\\n69\\nMultimodal-Toolkit: A Package for Learning on Tabular and\\nText Data with Transformers\\nKen Gu\\nGeorgian\\nken.gu@georgian.io\\nAkshay Budhkar\\nGeorgian\\nakshay@georgian.io\\nAbstract\\nRecent progress in natural language process\\u0002ing has led to Transformer architectures be\\u0002coming the predominant model used for nat\\u0002ural language tasks. However, in many real\\u0002world datasets, additional modalities are in\\u0002cluded which the Transformer does not di\\u0002rectly leverage. We present Multimodal\\u0002Toolkit,1\\nan open-source Python package to\\nincorporate text and tabular (categorical and\\nnumerical) data with Transformers for down\\u0002stream applications. Our toolkit integrates\\nwell with Hugging Face’s existing API such as\\ntokenization and the model hub2 which allows\\neasy download of different pre-trained models.\\n1 Introduction\\nIn recent years, Transformers (Vaswani et al.,\\n2017) have become popular for model pre-training\\n(Howard and Ruder, 2018; Peters et al., 2018; De\\u0002vlin et al., 2019) and have yielded state-of-the-art\\nresults on many natural language processing (NLP)\\ntasks. In addition, well-documented Transformer\\nlibraries such as Hugging Face Transformers (Wolf\\net al., 2020), and AllenNLP (Gardner et al., 2018)\\nhave democratized NLP, making it easier to pro\\u0002ductionize and experiment on Transformers.\\nHowever, there are not a lot of comprehensive\\ntools for Transformers to work with tabular data.\\nOften in real-world datasets, there are tabular data\\nas well as unstructured text data which can pro\\u0002vide meaningful signals for the task at hand. For\\ninstance, in the small example in Figure 1, each\\nrow is a data point. Columns Title and Review\\nText contain text features, columns Division\\nName, Class Name, and Department Name\\ncontain categorical features, and the Age column\\nis a numerical feature. To the best of our knowl\\u0002edge, no tool exists that makes it simple for Trans\\u0002formers to handle this extra modality. Therefore,\\n1Github: https://git.io/JO5a6\\n2https://huggingface.co/docs\\nFigure 1: An example of a clothing review classifica\\u0002tion dataset. Each row is a data point consisting of text,\\ncategorical features, and numerical features.\\ngiven the advances of Transformers for natural\\nlanguage tasks and the maturity of existing Trans\\u0002former libraries, we introduce Multimodal-Toolkit,\\na lightweight Python package built on top of Hug\\u0002ging Face Transformers. Our package extends ex\\u0002isting Transformers in the Hugging Face’s Trans\\u0002formers library to seamlessly handle structured tab\\u0002ular data while keeping the existing tokenization\\n(including subword segmentation), experimental\\npipeline, and pre-trained model hub functionalities\\nof Hugging Face Transformers. We show the effec\\u0002tiveness of our toolkit on three real-world datasets.\\n2 Related Work\\nThere have been several proposed Transformer\\nmodels that aim to handle text features and addi\\u0002tional features of another modality. For pre-trained\\nTransformers on images and text, models such as\\nViLBERT (Lu et al., 2019) and VLBERT (Su et al.,\\n2020) are mainly the same as the original BERT\\nmodel but treat the extra image modality as addi\\u0002tional tokens to the input. These models require\\npre-training on multimodal image and text data.\\nOn the other hand, while treating image features\\n70\\nas additional input tokens, MMBT (Kiela et al.,\\n2019) proposes to use pre-trained BERT directly\\nand fine-tune on image and text data. This is simi\\u0002lar to Multimodal-Toolkit in which no pre-training\\non text and tabular data is needed.\\nLikewise, Transformers have been adapted to\\nalign, audio, visual, and text modalities in which\\nthere is a natural ground truth alignment. MulT\\n(Tsai et al., 2019) is similar to ViLBert in which\\nco-attention is used between pairs of modalities but\\nalso includes temporal convolutions so that input to\\u0002kens are aware of their temporal neighbors. Mean\\u0002while, Rahman et al. (2020) injects cross modality\\nattention at certain Transformer layers via a gating\\nmechanism.\\nFinally, knowledge graph embeddings have also\\nbeen effectively combined with input text tokens\\nin Transformers. Ostendorff et al. (2019) com\\u0002bines knowledge graph embeddings on authors\\nwith book titles and other metadata features via\\nsimple concatenation for book genre classification.\\nOn the other hand, for more general language tasks,\\nERNIE (Zhang et al., 2019) first matches the to\\u0002kens in the input text with entities in the knowledge\\ngraph. With this matching, the model fuses these\\nembeddings to produce entity-aware text embed\\u0002dings and text-aware entity embeddings.\\nHowever, these models do not capture categor\\u0002ical and numerical data explicitly. Hugging Face\\ndoes include LXMERT (Tan and Bansal, 2019) to\\nhandle language and vision modality but this can\\nnot be easily adapted for categorical and numeri\\u0002cal data. Nevertheless, existing multimodal Trans\\u0002former models do give good insights into how to\\ncombine categorical and numerical features. ViL\\u0002BERT and VLBERT for example include image\\nmodality as input tokens which lead to one of our\\nsimple baseline of categorical and numerical fea\\u0002tures as additional token inputs to the model. Like\\u0002wise, the gating mechanism Rahman et al. (2020),\\nattention, and different weighting schemes have\\nall been shown to be useful in combining different\\nmodalities.\\n3 Design\\nThe goal of Multimodal-Toolkit is to allow users\\nto quickly adapt state-of-the-art Transformer mod\\u0002els for situations involving text and tabular data\\nwhich occur often in real-world datasets. More\\u0002over, we want to bring the benefits of Transformers\\nto more use cases while making it simple for users\\nFigure 2: The framework of Multimodal-Toolkit.\\nThere is a data processing module that outputs pro\\u0002cessed text, numerical, and categorical features that are\\nthen fed as input to our Transformer With Tabular mod\\u0002ule consisting of a Hugging Face Transformer and our\\ncombining module.\\nof Hugging Face Transformers to adopt. Therefore,\\nwe maintain the existing interface of the popular\\nHugging Face Transformers library.\\nThis design enables us to easily include more\\nTransformer models, leverage strengths of spe\\u0002cific models, use a feature-rich training pipeline,\\nand integrate the thousands of community trained\\nmodels on Hugging Face’s model hub. We sup\\u0002port a variety of Transformers (e.g. BERT, AL\\u0002BERT, RoBERTa, XLNET) for both classification\\nand regression tasks. All together, this becomes a\\nreusable Transformer With Tabular component. We\\nalso provide a data preprocessing module for cate\\u0002gorical and numerical features. An overview of the\\nsystem is shown in Figure 2. Currently, the library\\nsupports PyTorch Transformers implementations.\\n3.1 Combining Module\\nWe implement a combining module that is model\\nagnostic that takes as input, x, the text features\\noutputted from a Transformer model and prepro\\u0002cessed categorical (c) and numerical (n) features,\\nand outputs a combined multimodal representation\\nm. Although existing multimodal Transformers\\nincorporate cross-modal attention inside middle\\nTransformer layers, we choose the design in which\\nthe modality combination comes after the Trans\\u0002former because this module can be easily included\\nwithout much adaptation of the existing Hugging\\nFace Transformer interface and can be easily ex\\u0002tended to new Transformers included in the future.\\nInside the combining module, we implement var-\\n71\\nCombine Feature Method Equation\\nText only m = x\\nConcat m = x||c||n\\nIndividual MLPs on categorical and\\u0002numerical features then concat (MLP + Concat) m = x||MLP(c)||MLP(n)\\nMLP on concatenated categorical and\\nnumerical features then concat (Concat + MLP) m = x||MLP(c||n)\\nAttention on categorical and numerical\\nfeatures (Attention)\\nm = αx,xWxx + αx,cWcc + αx,nWnn\\nαi,j =\\nexp(LeakyReLU(a\\nT\\n[Wixi||Wjxj ]))\\nP\\nk∈{x,c,n}\\nexp(LeakyReLU(aT [Wixi||Wkxk]))\\nGating on categorical and numerical\\nfeatures and then sum (Rahman et al., 2020)\\n(Gating)\\nm = x + αh\\nh = gc \\f (Wcc...\\n\\n--- Result 7 ---\\nTitle: Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers\\nURL: https://arxiv.org/abs/2411.01645\\n\\nContent:\\n[We gratefully acknowledge support from\\\\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\\n\\n--- Result 8 ---\\nTitle: Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\nURL: https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html\\n\\nContent:\\n#### Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\n\\nPart of\\n[Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021) round2\\n\\n[Bibtex](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/28-/bibtex) [Paper](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper-round2.pdf) [Reviews And Public Comment »](https://openreview.net/forum?id=Q0zOIaec8HF) [Supplemental](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Supplemental-round2.zip)\\n\\n#### Authors\\n\\n_Xingjian Shi, Jonas Mueller, Nick Erickson, Nick Erickson, Mu Li, Alexander Smola, Alexander Smola_\\n\\n#### Abstract\\n\\nWe consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.\\n\\nDo not remove: This comment is monitored to verify that the site is working properly\\n\\n--- Result 9 ---\\nTitle: Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\\n\\nContent:\\nTabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\\n**📣 BIG NEWS:****Neptune is joining OpenAI!**→ Read the message from our CEO 📣![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\\nWhat do you want to find?\\nSearch\\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\\n[Shahul ES](https://neptune.ai/blog/author/shahules)\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle’s top tabular data competitions. Without much lag, let’s begin.\\nThese are the five competitions that I have gone through to create this article:\\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\\n* Use[cudf](https://github.com/rapidsai/cudf).\\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let’s see the methods used to do it.\\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Winning Kaggle solutions for problems that mix free‑form text with traditional numeric/categorical columns usually treat the data as **multimodal** and combine the strengths of modern language models with high‑performance tabular learners.  A common recipe is to let a pretrained transformer (e.g., BERT, RoBERTa) encode each text field into a dense vector, then feed those vectors together with the original tabular features into gradient‑boosted decision‑tree models such as LightGBM, CatBoost or Random Forest.  This “two‑stage” pipeline lets the tree‑based model exploit the rich semantic signal from the embeddings while still handling categorical encodings, missing values and interactions that trees capture well ([Papers with Code benchmark](https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular)).\\n\\nA more integrated alternative is to **train a single multimodal network** that ingests both modalities directly.  AutoGluon’s TabularPredictor treats raw text as a first‑class column and automatically stacks classical tabular models with its own pretrained NLP backbone, producing a blended predictor that often tops the leaderboard out‑of‑the‑box ([AutoGluon docs](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  The open‑source **Multimodal‑Toolkit** follows a similar idea: it converts categorical and numeric fields into embeddings, concatenates them with transformer token embeddings, and fine‑tunes the whole model end‑to‑end, simplifying the engineering effort while achieving state‑of‑the‑art results ([Multimodal‑Toolkit paper](https://aclanthology.org/2021.maiworkshop-1.10.pdf)).  A very simple but surprisingly effective trick—used in a Kaggle clothing‑review competition—is to **prepend the tabular information as textual tokens** to the original document and feed the combined string to BERT; this lets the language model learn joint representations without any extra architecture ([McCormickML blog](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).\\n\\nFinally, top teams reinforce these core ideas with classic Kaggle engineering: robust target‑encoding for high‑cardinality categories, TF‑IDF or n‑gram features for short text, extensive cross‑validation, GPU‑accelerated hyper‑parameter sweeps, and **ensembling** of pure‑text (fine‑tuned BERT) and pure‑tabular (Boosted Trees) predictions via weighted averaging or stacking.  The Kaggle Grandmasters Playbook highlights that such model‑level stacking, together with careful feature preprocessing, consistently yields the last few percentage points needed to win ([NVIDIA Grandmasters Playbook](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\", \"url\": \"\"}, {\"title\": \"Combining BERT/Transformers and Classical Tabular Models — AutoGluon Documentation 0.7.0 documentation\", \"text\": \"<div><div>\\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\\\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\\\">¶</a></h2>\\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\\n<a href=\\\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\\\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\\naddition to text and tabular features.</p>\\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\\ntabular data that contains text, numeric, and categorical columns. In\\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\\ndata tables. AutoGluon Tabular can help you train and combine a diverse\\nset of models including classical tabular models like\\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\\nmultimodal network that is introduced in Section\\n“<span>sec_textprediction_architecture</span>” of\\n<span>sec_textprediction_multimodal</span> (used by AutoGluon’s\\n<code><span>TextPredictor</span></code>).</p>\\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\\n<span>import</span> <span>pprint</span>\\n<span>import</span> <span>random</span>\\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n</pre></div>\\n<div>\\n<h2>Product Sentiment Analysis Dataset<a href=\\\"#product-sentiment-analysis-dataset\\\">¶</a></h2>\\n<p>We consider the product sentiment analysis dataset from a <a href=\\\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\\\">MachineHack\\nhackathon</a>.\\nThe goal is to predict a user’s sentiment towards a product given their\\nreview (raw text) and a categorical feature indicating the product’s\\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\\ndataset to be 90% for training and 10% for development/testing (if\\nsubmitting your models to the hackathon, we recommend training them on\\n100% of the dataset).</p>\\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\\n</pre></div>\\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 689486 (673K) [text/csv]\\nSaving to: ‘product_sentiment_machine_hack/train.csv’\\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\\n2023-02-22 23:29:24 (73.9 MB/s) - ‘product_sentiment_machine_hack/train.csv’ saved [689486/689486]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 75517 (74K) [text/csv]\\nSaving to: ‘product_sentiment_machine_hack/dev.csv’\\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\\n2023-02-22 23:29:24 (57.6 MB/s) - ‘product_sentiment_machine_hack/dev.csv’ saved [75517/75517]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 312194 (305K) [text/csv]\\nSaving to: ‘product_sentiment_machine_hack/test.csv’\\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\\n2023-02-22 23:29:24 (137 MB/s) - ‘product_sentiment_machine_hack/test.csv’ saved [312194/312194]\\n</pre></div>\\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\\n</pre></div>\\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\\n</pre></div>\\n<p>There are two features in the dataset: the users’ review of the product\\nand the product’s type, and four possible classes to predict.</p>\\n<div>\\n<table>\\n <thead>\\n <tr>\\n <th></th>\\n <th>Product_Description</th>\\n <th>Product_Type</th>\\n <th>Sentiment</th>\\n </tr>\\n </thead>\\n <tbody>\\n <tr>\\n <th>4532</th>\\n <td>they took away the lego pit but ...\", \"url\": \"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html\"}, {\"title\": \"The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\", \"text\": \"The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\\n[Technical Blog](https://developer.nvidia.com/blog)\\n[Subscribe](https://developer.nvidia.com/email-signup)\\n[Related Resources**](#main-content-end)\\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\\nEnglish中文\\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\\nLessons from years of competitions, made practical with GPU acceleration.\\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\\nSep 18, 2025\\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Théo Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\\n**\\nLike\\n[**Discuss (1)](#entry-content-comments)\\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\\nAI-Generated Summary\\nLike\\nDislike\\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard—no matter if we’re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn’t just a collection of modeling tricks—it’s a repeatable system for solving real-world tabular problems fast.\\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you’re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\\nWe’ve included links to example write-ups or notebooks from past competitions for each technique.\\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you’ll see below pre-installed.\\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\\nBefore diving into techniques, it’s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren’t optional best practices—they’re the foundation of how we approach every tabular modeling problem.\\n### Fast experimentation[**](#fast_experimentation)\\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover—and the faster we catch when a model is failing, drifting, or overfitting—so we can course-correct early and improve faster.\\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\\n**Here’s how we make it work:**\\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\\nGPU acceleration isn’t just for deep learning—it&#8217;s often the only way to make advanced tabular techniques practical at scale.\\n### Local Validation[**](#local_validation)\\nIf you can’t trust your validation score, you’re flying blind. That’s why cross-validation (CV) is a cornerstone to our workflow.\\n**Our approach:**\\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that’s held out.\\n* Rotate through folds so every part of the data is tested once.\\nThis gives a much more reliable measure of performance than a single train/validation split.\\n**Pro tip:**Match your CV strategy to how the test data is structured.\\n**For example:**\\n* Use TimeSeriesSplit for time-dependent data\\n* Use GroupKFold for grouped data (like users or patients)\\nWith those foundations in place—moving fast and validating carefully—we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they’re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper—a couple of quick checks that we’ve found useful, but many people miss:\\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift—test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\\nThese techniques aren&#8217;t brand new—but they’re often overlooked, and ignoring them can sink a project.\\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\\n**In action:**In the winning solution to the Amazon KDD Cup ‘23, the team uncovered both a train—test distribution shift and temporal patterns in the target—insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...\", \"url\": \"https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data\"}, {\"title\": \"Papers with Code - Benchmarking Multimodal AutoML for Tabular Data with Text Fields\", \"text\": \"new\\n\\nGet trending papers in your email inbox once a day!\\n\\nGet trending papers in your email inbox!\\n\\n[Subscribe](https://paperswithcode.com/login?next=%2Fpapers)\\n\\n# Trending Papers\\n\\n## by [AK](https://paperswithcode.com/akhaliq) and the research community\\n\\n- Daily\\n- Weekly\\n- Monthly\\n\\nTrending Papers\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n·Published on Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n·Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n·Published on Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n·Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\ncsuhan\\n\\n### [ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\\\ Automation via Modular Multimodal Agents](https://paperswithcode.com/papers/2507.22827)\\n\\nAutomating the transformation of user interface (UI) designs into front-end\\ncode holds significant promise for accelerating software development and\\ndemocratizing design workflows. While recent large language models (LLMs) have\\ndemonstrated progress in text-to-code generation, many existing approaches rely\\nsolely o...\", \"url\": \"https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular\"}, {\"title\": \"\", \"text\": \"# Combining Categorical and Numerical Features with Text in BERT\\n\\n29 Jun 2021\\n\\nIn this tutorial we’ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\\n\\nTo help motivate our discussion, we’ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\\n\\n- The age of the reviewer (numerical feature)\\n- The number of upvotes on the review (numerical feature)\\n- The department and category of the clothing item (categorical features)\\n\\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\\n\\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\\n\\nIn Section 2 of this Notebook, I’ve implemented four different “baseline” strategies which score fairly well, but which don’t incorporate all of the features together.\\n\\nThen, in Section 3, I’ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\\n\\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\\n\\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\\n\\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\\n\\nBy Chris McCormick\\n\\n# Contents\\n\\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\\n\\n# S1. Clothing Review Dataset\\n\\n## 1.1. Download & Parse\\n\\nRetrieve the .csv file for the dataset.\\n\\n```\\nimport gdown\\n\\nprint('Downloading dataset...\\\\n')\\n\\n# Download the file.\\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\\n                'Womens Clothing E-Commerce Reviews.csv',\\n                quiet=False)\\n\\nprint('\\\\n\\\\nDONE.')\\n```\\n\\n```\\nDownloading dataset...\\n\\nDownloading...\\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\\nTo: /content/Womens Clothing E-Commerce Reviews.csv\\n8.48MB [00:00, 48.7MB/s]\\n\\nDONE.\\n\\n```\\n\\nParse the dataset csv file into a pandas DataFrame.\\n\\n```\\nimport pandas as pd\\n\\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\\n\\ndata_df.head()\\n```\\n\\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\\n\\n_Features_\\n\\n“ **Recommended IND**” is the label we are trying to predict for this dataset. “1” means the reviewer recommended the product and “0” means they do not.\\n\\nThe following are _categorical_ features:\\n\\n- Division Name\\n- Department Name\\n- Class Name\\n- Clothing ID\\n\\nAnd the following are _numerical_ features:\\n\\n- Age\\n- Rating\\n- Positive Feedback Count\\n\\n_Feature Analysis_\\n\\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\\n\\nNote that, in addition to the “Recommended” label, there is also a “ **Rating**” column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We’ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\\n\\n## 1.2. Train-Validation-Test Split\\n\\nI want to use the same training, validation, and test splits for all of the approaches we try so that it’s a fair comparison.\\n\\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\\n\\nTo solve this, we’re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\\n\\n```\\nimport random\\nimport numpy as np\\n\\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\\ntrain_size = int(0.8 * len(data_df))\\nval_size = int(0.1 * len(data_df))\\ntest_size = len(data_df) - (train_size + val_size)\\n\\n# Sanity check the sizes.\\nassert((train_size + val_size + test_size) == len(data_df))\\n\\n# Create a list of indeces for all of the samples in the dataset.\\nindeces = np.arange(0, len(data_df))\\n\\n# Shuffle the indeces randomly.\\nrandom.shuffle(indeces)\\n\\n# Get a list of indeces for each of the splits.\\ntrain_idx = indeces[0:train_size]\\nval_idx = indeces[train_size:(train_size + val_size)]\\ntest_idx = indeces[(train_size + val_size):]\\n\\n# Sanity check\\nassert(len(train_idx) == train_size)\\nassert(len(test_idx) == test_size)\\n\\n# With these lists, we can now select the corresponding dataframe rows using,\\n# e.g., train_df = data_df.iloc[train_idx]\\n\\nprint('  Training size: {:,}'.format(train_size))\\n...\", \"url\": \"https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert\"}, {\"title\": \"A Package for Learning on Tabular and Text Data with Transformers\", \"text\": \"Proceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 69–73\\nJune 6, 2021. ©2021 Association for Computational Linguistics\\n69\\nMultimodal-Toolkit: A Package for Learning on Tabular and\\nText Data with Transformers\\nKen Gu\\nGeorgian\\nken.gu@georgian.io\\nAkshay Budhkar\\nGeorgian\\nakshay@georgian.io\\nAbstract\\nRecent progress in natural language process\\u0002ing has led to Transformer architectures be\\u0002coming the predominant model used for nat\\u0002ural language tasks. However, in many real\\u0002world datasets, additional modalities are in\\u0002cluded which the Transformer does not di\\u0002rectly leverage. We present Multimodal\\u0002Toolkit,1\\nan open-source Python package to\\nincorporate text and tabular (categorical and\\nnumerical) data with Transformers for down\\u0002stream applications. Our toolkit integrates\\nwell with Hugging Face’s existing API such as\\ntokenization and the model hub2 which allows\\neasy download of different pre-trained models.\\n1 Introduction\\nIn recent years, Transformers (Vaswani et al.,\\n2017) have become popular for model pre-training\\n(Howard and Ruder, 2018; Peters et al., 2018; De\\u0002vlin et al., 2019) and have yielded state-of-the-art\\nresults on many natural language processing (NLP)\\ntasks. In addition, well-documented Transformer\\nlibraries such as Hugging Face Transformers (Wolf\\net al., 2020), and AllenNLP (Gardner et al., 2018)\\nhave democratized NLP, making it easier to pro\\u0002ductionize and experiment on Transformers.\\nHowever, there are not a lot of comprehensive\\ntools for Transformers to work with tabular data.\\nOften in real-world datasets, there are tabular data\\nas well as unstructured text data which can pro\\u0002vide meaningful signals for the task at hand. For\\ninstance, in the small example in Figure 1, each\\nrow is a data point. Columns Title and Review\\nText contain text features, columns Division\\nName, Class Name, and Department Name\\ncontain categorical features, and the Age column\\nis a numerical feature. To the best of our knowl\\u0002edge, no tool exists that makes it simple for Trans\\u0002formers to handle this extra modality. Therefore,\\n1Github: https://git.io/JO5a6\\n2https://huggingface.co/docs\\nFigure 1: An example of a clothing review classifica\\u0002tion dataset. Each row is a data point consisting of text,\\ncategorical features, and numerical features.\\ngiven the advances of Transformers for natural\\nlanguage tasks and the maturity of existing Trans\\u0002former libraries, we introduce Multimodal-Toolkit,\\na lightweight Python package built on top of Hug\\u0002ging Face Transformers. Our package extends ex\\u0002isting Transformers in the Hugging Face’s Trans\\u0002formers library to seamlessly handle structured tab\\u0002ular data while keeping the existing tokenization\\n(including subword segmentation), experimental\\npipeline, and pre-trained model hub functionalities\\nof Hugging Face Transformers. We show the effec\\u0002tiveness of our toolkit on three real-world datasets.\\n2 Related Work\\nThere have been several proposed Transformer\\nmodels that aim to handle text features and addi\\u0002tional features of another modality. For pre-trained\\nTransformers on images and text, models such as\\nViLBERT (Lu et al., 2019) and VLBERT (Su et al.,\\n2020) are mainly the same as the original BERT\\nmodel but treat the extra image modality as addi\\u0002tional tokens to the input. These models require\\npre-training on multimodal image and text data.\\nOn the other hand, while treating image features\\n70\\nas additional input tokens, MMBT (Kiela et al.,\\n2019) proposes to use pre-trained BERT directly\\nand fine-tune on image and text data. This is simi\\u0002lar to Multimodal-Toolkit in which no pre-training\\non text and tabular data is needed.\\nLikewise, Transformers have been adapted to\\nalign, audio, visual, and text modalities in which\\nthere is a natural ground truth alignment. MulT\\n(Tsai et al., 2019) is similar to ViLBert in which\\nco-attention is used between pairs of modalities but\\nalso includes temporal convolutions so that input to\\u0002kens are aware of their temporal neighbors. Mean\\u0002while, Rahman et al. (2020) injects cross modality\\nattention at certain Transformer layers via a gating\\nmechanism.\\nFinally, knowledge graph embeddings have also\\nbeen effectively combined with input text tokens\\nin Transformers. Ostendorff et al. (2019) com\\u0002bines knowledge graph embeddings on authors\\nwith book titles and other metadata features via\\nsimple concatenation for book genre classification.\\nOn the other hand, for more general language tasks,\\nERNIE (Zhang et al., 2019) first matches the to\\u0002kens in the input text with entities in the knowledge\\ngraph. With this matching, the model fuses these\\nembeddings to produce entity-aware text embed\\u0002dings and text-aware entity embeddings.\\nHowever, these models do not capture categor\\u0002ical and numerical data explicitly. Hugging Face\\ndoes include LXMERT (Tan and Bansal, 2019) to\\nhandle language and vision modality but this can\\nnot be easily adapted for categorical and numeri\\u0002cal data. Nevertheless, existing multimodal Trans\\u0002former models do give good insights into how to\\ncombine categorical and numerical features. ViL\\u0002BERT and VLBERT for example include image\\nmodality as input tokens which lead to one of our\\nsimple baseline of categorical and numerical fea\\u0002tures as additional token inputs to the model. Like\\u0002wise, the gating mechanism Rahman et al. (2020),\\nattention, and different weighting schemes have\\nall been shown to be useful in combining different\\nmodalities.\\n3 Design\\nThe goal of Multimodal-Toolkit is to allow users\\nto quickly adapt state-of-the-art Transformer mod\\u0002els for situations involving text and tabular data\\nwhich occur often in real-world datasets. More\\u0002over, we want to bring the benefits of Transformers\\nto more use cases while making it simple for users\\nFigure 2: The framework of Multimodal-Toolkit.\\nThere is a data processing module that outputs pro\\u0002cessed text, numerical, and categorical features that are\\nthen fed as input to our Transformer With Tabular mod\\u0002ule consisting of a Hugging Face Transformer and our\\ncombining module.\\nof Hugging Face Transformers to adopt. Therefore,\\nwe maintain the existing interface of the popular\\nHugging Face Transformers library.\\nThis design enables us to easily include more\\nTransformer models, leverage strengths of spe\\u0002cific models, use a feature-rich training pipeline,\\nand integrate the thousands of community trained\\nmodels on Hugging Face’s model hub. We sup\\u0002port a variety of Transformers (e.g. BERT, AL\\u0002BERT, RoBERTa, XLNET) for both classification\\nand regression tasks. All together, this becomes a\\nreusable Transformer With Tabular component. We\\nalso provide a data preprocessing module for cate\\u0002gorical and numerical features. An overview of the\\nsystem is shown in Figure 2. Currently, the library\\nsupports PyTorch Transformers implementations.\\n3.1 Combining Module\\nWe implement a combining module that is model\\nagnostic that takes as input, x, the text features\\noutputted from a Transformer model and prepro\\u0002cessed categorical (c) and numerical (n) features,\\nand outputs a combined multimodal representation\\nm. Although existing multimodal Transformers\\nincorporate cross-modal attention inside middle\\nTransformer layers, we choose the design in which\\nthe modality combination comes after the Trans\\u0002former because this module can be easily included\\nwithout much adaptation of the existing Hugging\\nFace Transformer interface and can be easily ex\\u0002tended to new Transformers included in the future.\\nInside the combining module, we implement var-\\n71\\nCombine Feature Method Equation\\nText only m = x\\nConcat m = x||c||n\\nIndividual MLPs on categorical and\\u0002numerical features then concat (MLP + Concat) m = x||MLP(c)||MLP(n)\\nMLP on concatenated categorical and\\nnumerical features then concat (Concat + MLP) m = x||MLP(c||n)\\nAttention on categorical and numerical\\nfeatures (Attention)\\nm = αx,xWxx + αx,cWcc + αx,nWnn\\nαi,j =\\nexp(LeakyReLU(a\\nT\\n[Wixi||Wjxj ]))\\nP\\nk∈{x,c,n}\\nexp(LeakyReLU(aT [Wixi||Wkxk]))\\nGating on categorical and numerical\\nfeatures and then sum (Rahman et al., 2020)\\n(Gating)\\nm = x + αh\\nh = gc \\f (Wcc...\", \"url\": \"https://aclanthology.org/2021.maiworkshop-1.10.pdf\"}, {\"title\": \"Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers\", \"text\": \"[We gratefully acknowledge support from\\\\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\", \"url\": \"https://arxiv.org/abs/2411.01645\"}, {\"title\": \"Benchmarking Multimodal AutoML for Tabular Data with Text Fields\", \"text\": \"#### Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\n\\nPart of\\n[Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021) round2\\n\\n[Bibtex](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/28-/bibtex) [Paper](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper-round2.pdf) [Reviews And Public Comment »](https://openreview.net/forum?id=Q0zOIaec8HF) [Supplemental](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Supplemental-round2.zip)\\n\\n#### Authors\\n\\n_Xingjian Shi, Jonas Mueller, Nick Erickson, Nick Erickson, Mu Li, Alexander Smola, Alexander Smola_\\n\\n#### Abstract\\n\\nWe consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.\\n\\nDo not remove: This comment is monitored to verify that the site is working properly\", \"url\": \"https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html\"}, {\"title\": \"Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\\n**📣 BIG NEWS:****Neptune is joining OpenAI!**→ Read the message from our CEO 📣![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\\nWhat do you want to find?\\nSearch\\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\\n[Shahul ES](https://neptune.ai/blog/author/shahules)\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle’s top tabular data competitions. Without much lag, let’s begin.\\nThese are the five competitions that I have gone through to create this article:\\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\\n* Use[cudf](https://github.com/rapidsai/cudf).\\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let’s see the methods used to do it.\\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...\", \"url\": \"https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\"}]}",
        "tool.duration_ms": 4105.646848678589
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "13fa19c33da2fc18",
      "parent_span_id": "7afc9600afa471be",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767938707257993686,
      "end_time": 1767938707261580080,
      "duration": 0.003586394,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7df814613ad0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze subreddit distribution\\nprint(\\\"Subreddit distribution:\\\")\\nprint(df['requester_subreddits_at_request'].value_counts().head(10))\\n\\n# Check if subreddit contains multiple values (it appears to be a string, not a list)\\nprint(f\\\"\\\\nSample subreddit values:\\\")\\nprint(df['requester_subreddits_at_request'].head(5).tolist())\\n\\n# Analyze request time patterns\\ndf['request_date'] = pd.to_datetime(df['request_date'])\\ndf['request_hour'] = df['request_date'].dt.hour\\ndf['request_dayofweek'] = df['request_date'].dt.dayofweek\\n\\nprint(f\\\"\\\\nRequest hour distribution:\\\")\\nprint(df['request_hour'].value_counts().sort_index())\\n\\nprint(f\\\"\\\\nRequest day of week distribution:\\\")\\nprint(df['request_dayofweek'].value_counts().sort_index())\\n\\n# Check correlation of time features with target\\nprint(f\\\"\\\\nSuccess rate by hour:\\\")\\nhourly_success = df.groupby('request_hour')[target].mean() * 100\\nprint(hourly_success.round(2))\"}}",
        "tool.output": "{\"content\": \"Added code cell at 8\", \"is_error\": false}",
        "tool.duration_ms": 3.6606788635253906
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "d9e238ba69caf0b0",
      "parent_span_id": "7afc9600afa471be",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767938718687984220,
      "end_time": 1767938719748807442,
      "duration": 1.060823222,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7df8146128d0>\", \"input\": {\"start_cell\": 8, \"end_cell\": 8, \"duration\": 10}}",
        "tool.output": "{\"content\": \"Error: KeyError: 'request_date'\", \"is_error\": true}",
        "tool.duration_ms": 1060.8644485473633
      }
    },
    {
      "trace_id": "ec8c4bd406802fd0284ce82f01edf213",
      "span_id": "4cd993eb66712b05",
      "parent_span_id": "7afc9600afa471be",
      "name": "ReadNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767938725491949506,
      "end_time": 1767938725493420612,
      "duration": 0.001471106,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ReadNotebookTool object at 0x7df814613610>\", \"input\": {}}",
        "tool.output": "{\"content\": \"Notebook: /home/code/exploration/eda.ipynb\\nCells: 9\\n==================================================\\n\\nCell 0 [code] In[1]:\\nimport pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = '/home/data/train.json'\\n\\n# Read the JSON file\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of records: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First record keys: {list(train_data[0].keys())}\\\")\\nOutput:\\nNumber of records: 2878\\nType of data: <class 'list'>\\nFirst record keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_\\n\\nCell 1 [code] In[2]:\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\nprint(f\\\"DataFrame shape: {df.shape}\\\")\\nprint(\\\"\\\\nColumn names:\\\")\\nfor col in df.columns:\\n    print(f\\\"- {col}\\\")\\nOutput:\\nDataFrame shape: (2878, 32)\\n\\nColumn names:\\n- giver_username_if_known\\n- number_of_downvotes_of_request_at_retrieval\\n- number_of_upvotes_of_request_at_retrieval\\n- post_was_edited\\n- request_id\\n- request_number_of_comments_at_retrieval\\n- request_text\\n- request_text_edit_aware\\n- request_title\\n- requester_account_age_in_days_at_request\\n- requester_account_age_in_days_at_retrieval\\n- requester_days_since_first_post_on_raop_at_request\\n- requester_days_since_first_post_on_raop_at_retrieval\\n- requester_num\\n\\nCell 2 [code] In[3]:\\n# Analyze target distribution\\ntarget = 'requester_received_pizza'\\nprint(\\\"Target distribution:\\\")\\nprint(df[target].value_counts())\\nprint(f\\\"\\\\nTarget percentages:\\\")\\nprint(df[target].value_counts(normalize=True) * 100)\\nprint(f\\\"\\\\nClass imbalance ratio: {df[target].value_counts().min() / df[target].value_counts().max():.4f}\\\")\\nOutput:\\nTarget distribution:\\nrequester_received_pizza\\nFalse    2163\\nTrue      715\\nName: count, dtype: int64\\n\\nTarget percentages:\\nrequester_received_pizza\\nFalse    75.156359\\nTrue     24.843641\\nName: proportion, dtype: float64\\n\\nClass imbalance ratio: 0.3306\\n\\n\\nCell 3 [code] In[4]:\\n# Analyze feature types\\nprint(\\\"Feature types:\\\")\\nprint(df.dtypes.value_counts())\\n\\nprint(\\\"\\\\nSample of text features:\\\")\\nprint(f\\\"Request title sample: {df['request_title'].iloc[0][:100]}...\\\")\\nprint(f\\\"Request text sample: {df['request_text'].iloc[0][:100]}...\\\")\\n\\nprint(\\\"\\\\nSample of categorical features:\\\")\\ncategorical_cols = df.select_dtypes(include=['object']).columns\\nprint(f\\\"Categorical columns: {list(categorical_cols)}\\\")\\n\\nprint(\\\"\\\\nSample of numerical features:\\\")\\nnumerical_cols = df.select_dtypes(exclude=['object']).columns\\nprint(f\\\"Numerical columns count: {len(numerical_cols)}\\\")\\nOutput:\\nFeature types:\\nint64      16\\nobject      9\\nfloat64     6\\nbool        1\\nName: count, dtype: int64\\n\\nSample of text features:\\nRequest title sample: [REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy....\\nRequest text sample: I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some in...\\n\\nSample of categorical features:\\nCategorical columns: ['giver_username_if_known', 'post_was_edited', 'request_id', 'request_text', 'request_text_edit_aware', 're\\n\\nCell 4 [code] In[5]:\\n# Check for missing values\\nprint(\\\"Missing values per column:\\\")\\nmissing_counts = df.isnull().sum()\\nmissing_pct = (missing_counts / len(df)) * 100\\nmissing_df = pd.DataFrame({\\n    'missing_count': missing_counts,\\n    'missing_pct': missing_pct\\n}).sort_values('missing_count', ascending=False)\\n\\nprint(missing_df[missing_df['missing_count'] > 0])\\n\\nprint(f\\\"\\\\nTotal columns with missing values: {(missing_counts > 0).sum()}\\\")\\nprint(f\\\"Total missing values: {missing_counts.sum()}\\\")\\nOutput:\\nMissing values per column:\\n                      missing_count  missing_pct\\nrequester_user_flair           2163    75.156359\\n\\nTotal columns with missing values: 1\\nTotal missing values: 2163\\n\\n\\nCell 5 [code] In[6]:\\n# Analyze key numerical features correlation with target\\nnumerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\\n# Remove target from the list\\nif target in numerical_features:\\n    numerical_features.remove(target)\\n\\nprint(f\\\"Analyzing {len(numerical_features)} numerical features\\\")\\n\\n# Calculate correlation with target\\ncorrelations = df[numerical_features + [target]].corr()[target].sort_values(ascending=False)\\nprint(\\\"\\\\nTop 10 features most correlated with target:\\\")\\nprint(correlations.head(11))  # 11 because target correlates with itself\\n\\nprint(\\\"\\\\nBottom 10 features most correlated with target:\\\")\\nprint(correlations.tail(10))\\nOutput:\\nAnalyzing 22 numerical features\\n\\nTop 10 features most correlated with target:\\nrequester_received_pizza                                1.000000\\nrequester_number_of_posts_on_raop_at_retrieval          0.462165\\nrequest_number_of_comments_at_retrieval                 0.290709\\nrequester_number_of_comments_in_raop_at_retrieval       0.277129\\nrequester_number_of_posts_on_raop_at_request            0.132712\\nrequester_number_of_comments_in_raop_at_request         0.131965\\nrequester_days_since_first_post_\\n\\nCell 6 [code] In[7]:\\n# Analyze text length features\\ndf['request_title_length'] = df['request_title'].str.len()\\ndf['request_text_length'] = df['request_text'].str.len()\\ndf['request_text_edit_aware_length'] = df['request_text_edit_aware'].str.len()\\n\\nprint(\\\"Text length statistics:\\\")\\nprint(df[['request_title_length', 'request_text_length', 'request_text_edit_aware_length']].describe())\\n\\n# Check correlation of text lengths with target\\nlength_correlations = df[['request_title_length', 'request_text_length', 'request_text_edit_aware_length', target]].corr()[target]\\nprint(f\\\"\\\\nCorrelations with target:\\\")\\nprint(length_correlations)\\nOutput:\\nText length statistics:\\n       request_title_length  request_text_length  \\\\\\ncount           2878.000000          2878.000000   \\nmean              71.572967           402.521543   \\nstd               36.233487           362.393727   \\nmin                7.000000             0.000000   \\n25%               46.000000           182.000000   \\n50%               64.000000           308.000000   \\n75%               90.000000           503.750000   \\nmax              272.000000          4460.000000   \\n\\n       \\n\\nCell 7 [code] In[8]:\\n# Analyze user flair distribution\\nprint(\\\"User flair distribution:\\\")\\nprint(df['requester_user_flair'].value_counts(dropna=False))\\n\\nprint(\\\"\\\\nUser flair distribution by target:\\\")\\nflair_crosstab = pd.crosstab(df['requester_user_flair'], df[target], normalize='index') * 100\\nprint(flair_crosstab.round(2))\\n\\n# Calculate success rate by flair\\nsuccess_by_flair = df.groupby('requester_user_flair')[target].mean() * 100\\nprint(f\\\"\\\\nSuccess rate by flair:\\\")\\nprint(success_by_flair.round(2))\\nOutput:\\nUser flair distribution:\\nrequester_user_flair\\nNone      2163\\nshroom     677\\nPIF         38\\nName: count, dtype: int64\\n\\nUser flair distribution by target:\\nrequester_received_pizza   True\\nrequester_user_flair           \\nPIF                       100.0\\nshroom                    100.0\\n\\nSuccess rate by flair:\\nrequester_user_flair\\nPIF       100.0\\nshroom    100.0\\nName: requester_received_pizza, dtype: float64\\n\\n\\nCell 8 [code] In[ ]:\\n# Analyze subreddit distribution\\nprint(\\\"Subreddit distribution:\\\")\\nprint(df['requester_subreddits_at_request'].value_counts().head(10))\\n\\n# Check if subreddit contains multiple values (it appears to be a string, not a list)\\nprint(f\\\"\\\\nSample subreddit values:\\\")\\nprint(df['requester_subreddits_at_request'].head(5).tolist())\\n\\n# Analyze request time patterns\\ndf['request_date'] = pd.to_datetime(df['request_date'])\\ndf['request_hour'] = df['request_date'].dt.hour\\ndf['request_dayofweek'] = df['request_date'].dt.dayofweek\\n\\nprint(f\\\"\\\\nRequest hour distribution:\\\")\\nprint(df['request_hour'].value_counts().sort_index())\\n\\nprint(f\\\"\\\\nRequest day of week distribution:\\\")\\nprint(df['request_dayofweek'].value_counts().sort_index())\\n\\n# Check correlation of time features with target\\nprint(f\\\"\\\\nSuccess rate by hour:\\\")\\nhourly_success = df.groupby('request_hour')[target].mean() * 100\\nprint(hourly_success.round(2))\", \"is_error\": false}",
        "tool.duration_ms": 1.4579296112060547
      }
    }
  ],
  "metadata": {
    "span_count": 20,
    "last_updated": "2026-01-09T06:05:27.789512"
  }
}