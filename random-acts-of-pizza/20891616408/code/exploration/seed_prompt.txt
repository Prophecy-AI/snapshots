# Seed Prompt: Random Acts of Pizza Classification

## Problem Overview
This is a binary classification problem predicting whether a Reddit pizza request will be successful (24.8% success rate). The dataset combines text features (request title, text) with tabular metadata (user activity, timestamps, votes).

## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target imbalance, correlations, text length analysis
- Key findings from EDA: Strong correlation with RAOP-specific activity metrics, class imbalance (75/25 split), text features with edit-aware versions

## Core Modeling Strategy

### Two-Stage Multimodal Approach
Based on winning Kaggle solutions for text+tabular competitions:
1. **Text Feature Extraction**: Use pretrained transformers (BERT/RoBERTa) or sentence embeddings to convert request_text and request_title into dense vectors
2. **Tabular Feature Engineering**: Engineer features from user activity metrics, timestamps, and vote counts
3. **Model Fusion**: Concatenate text embeddings with engineered tabular features and feed to gradient boosting models (LightGBM/CatBoost/XGBoost)

### Alternative: End-to-End Multimodal
Use AutoGluon or similar frameworks that handle text+tabular data natively, training ensembles of tree models and text predictors automatically.

## Text Feature Engineering

### Preprocessing Pipeline
- Clean text: Remove URLs, special characters, normalize whitespace
- Combine request_title and request_text for richer context
- Use edit_aware version to avoid leakage from successful requests
- Extract meta-features: text length, word count, presence of specific keywords ("please", "thank you", "hungry", "family")

### Text Representations
- **TF-IDF**: Use for baseline models and feature importance analysis
- **Transformer Embeddings**: BERT/RoBERTa sentence embeddings (CLS token or pooled)
- **Keyword Features**: Binary indicators for emotionally charged or need-related words

## Tabular Feature Engineering

### High-Value Features (from correlation analysis)
- RAOP-specific activity: `requester_number_of_posts_on_raop`, `requester_number_of_comments_in_raop`
- Request engagement: `request_number_of_comments_at_retrieval`
- User reputation: `requester_upvotes_minus_downvotes` metrics
- Temporal features: Extract day of week, hour from timestamps

### Feature Interactions
- Ratio features: comments/posts ratios, upvote/downvote ratios
- Difference features: between request and retrieval times (activity growth)
- User flair encoding: One-hot or target encoding for "shroom" and "PIF" statuses

## Handling Class Imbalance

### Training Strategies
- **Class Weights**: Use `scale_pos_weight` in LightGBM/XGBoost (approx 3:1 ratio)
- **Focal Loss**: Alternative to cross-entropy for focusing on hard examples
- **Stratified CV**: Essential for reliable validation with imbalanced data
- **SMOTE/ADASYN**: Consider synthetic minority oversampling for tree models

### Evaluation
- Optimize for AUC-ROC (competition metric)
- Use stratified 5-fold cross-validation
- Monitor both AUC and PR-AUC to ensure good minority class performance

## Model Selection

### Primary Models
1. **LightGBM**: Fast, handles mixed data types well after proper encoding
2. **CatBoost**: Excellent for categorical features, handles text via tokenization
3. **XGBoost**: Robust baseline, good with engineered features

### Advanced Options
- **Neural Networks**: For large-scale text processing, combine with tabular data
- **Ensemble Stacking**: Layer 2-3 diverse models (tree + neural + linear)

## Ensembling Strategy

### Model Diversity
- Combine gradient boosting models with different text embedding approaches
- Include both tree-based and neural models in ensemble
- Use different feature subsets to increase diversity

### Stacking Approach
- Level-1: Multiple models with different architectures
- Level-2: Logistic regression or LightGBM on out-of-fold predictions
- Weighted averaging based on validation performance

## Validation Strategy

### Cross-Validation
- **Stratified K-Fold**: 5 folds to preserve class distribution
- **Time-based splits**: If temporal patterns exist (check timestamp correlations)
- **Group K-Fold**: By requester_username to prevent leakage

### Validation Monitoring
- Track AUC-ROC on each fold
- Monitor calibration of probability predictions
- Check for overfitting on high-correlation features

## Implementation Notes

### Memory Efficiency
- Process text embeddings in batches
- Use sparse matrices for TF-IDF features
- Consider dimensionality reduction (PCA/UMAP) on text embeddings

### Feature Store
- Cache text embeddings to avoid recomputation
- Pre-compute interaction features
- Store engineered features in efficient formats (parquet/feather)

## Key Success Factors
1. **Leverage RAOP-specific activity**: These show highest correlation with success
2. **Handle text carefully**: Use edit_aware version, engineer sentiment/need indicators
3. **Address imbalance**: Use appropriate class weights and evaluation metrics
4. **Ensemble diverse models**: Combine different text representations and model types
5. **Validate properly**: Stratified CV essential for reliable performance estimates