{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d831f3fe",
   "metadata": {},
   "source": [
    "# Baseline Model: LightGBM with Basic Features\n",
    "\n",
    "This notebook implements a baseline model using:\n",
    "- Basic text features (length, word count)\n",
    "- Numerical features with log transforms\n",
    "- User flair encoding\n",
    "- TF-IDF for text representation\n",
    "- LightGBM with class weighting for imbalance\n",
    "\n",
    "Based on existing findings:\n",
    "- Class imbalance: 75.2% False, 24.8% True\n",
    "- User flair is highly predictive ('shroom' = received pizza)\n",
    "- Numerical features are skewed and need log transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70053466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T06:12:23.323077Z",
     "iopub.status.busy": "2026-01-13T06:12:23.322316Z",
     "iopub.status.idle": "2026-01-13T06:12:23.327330Z",
     "shell.execute_reply": "2026-01-13T06:12:23.326719Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea197bb0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a56bf80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T06:12:23.329454Z",
     "iopub.status.busy": "2026-01-13T06:12:23.329021Z",
     "iopub.status.idle": "2026-01-13T06:12:23.458545Z",
     "shell.execute_reply": "2026-01-13T06:12:23.457884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (2878, 32)\n",
      "Columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "Test data shape: (1162, 17)\n",
      "\n",
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    0.751564\n",
      "True     0.248436\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "        \n",
    "train_df = pd.DataFrame(train_data)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Load test data\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "        \n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac91cb2",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### 1. Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(df):\n",
    "    \"\"\"Extract basic text features\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    features['request_id'] = df['request_id']\n",
    "    \n",
    "    # Title features\n",
    "    features['title_length'] = df['request_title'].fillna('').str.len()\n",
    "    features['title_word_count'] = df['request_title'].fillna('').str.split().str.len()\n",
    "    \n",
    "    # Text features (handle missing columns)\n",
    "    if 'request_text' in df.columns:\n",
    "        features['text_length'] = df['request_text'].fillna('').str.len()\n",
    "        features['text_word_count'] = df['request_text'].fillna('').str.split().str.len()\n",
    "    else:\n",
    "        features['text_length'] = 0\n",
    "        features['text_word_count'] = 0\n",
    "    \n",
    "    # Edit-aware text features\n",
    "    features['edit_aware_length'] = df['request_text_edit_aware'].fillna('').str.len()\n",
    "    features['edit_aware_word_count'] = df['request_text_edit_aware'].fillna('').str.split().str.len()\n",
    "    \n",
    "    # Combined text length\n",
    "    features['total_text_length'] = features['title_length'] + features['text_length']\n",
    "    features['total_word_count'] = features['title_word_count'] + features['text_word_count']\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e6ff5",
   "metadata": {},
   "source": [
    "### 2. Numerical Features with Log Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numerical_features(df):\n",
    "    \"\"\"Extract and transform numerical features\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    features['request_id'] = df['request_id']\n",
    "    \n",
    "    # Account age features (handle missing columns)\n",
    "    if 'requester_account_age_in_days_at_request' in df.columns:\n",
    "        features['account_age_at_request'] = df['requester_account_age_in_days_at_request']\n",
    "    else:\n",
    "        features['account_age_at_request'] = 0\n",
    "        \n",
    "    if 'requester_account_age_in_days_at_retrieval' in df.columns:\n",
    "        features['account_age_at_retrieval'] = df['requester_account_age_in_days_at_retrieval']\n",
    "    else:\n",
    "        features['account_age_at_retrieval'] = 0\n",
    "    \n",
    "    # Upvotes/downvotes features (apply log transform due to skewness)\n",
    "    if 'requester_upvotes_plus_downvotes_at_request' in df.columns:\n",
    "        upvotes_plus_downvotes_at_request = df['requester_upvotes_plus_downvotes_at_request'].fillna(0)\n",
    "        features['log_upvotes_plus_downvotes_at_request'] = np.log1p(upvotes_plus_downvotes_at_request)\n",
    "    else:\n",
    "        features['log_upvotes_plus_downvotes_at_request'] = 0\n",
    "    \n",
    "    if 'requester_upvotes_plus_downvotes_at_retrieval' in df.columns:\n",
    "        upvotes_plus_downvotes_at_retrieval = df['requester_upvotes_plus_downvotes_at_retrieval'].fillna(0)\n",
    "        features['log_upvotes_plus_downvotes_at_retrieval'] = np.log1p(upvotes_plus_downvotes_at_retrieval)\n",
    "    else:\n",
    "        features['log_upvotes_plus_downvotes_at_retrieval'] = 0\n",
    "    \n",
    "    # Upvotes minus downvotes (net score)\n",
    "    if 'requester_upvotes_minus_downvotes_at_request' in df.columns:\n",
    "        features['upvotes_minus_downvotes_at_request'] = df['requester_upvotes_minus_downvotes_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['upvotes_minus_downvotes_at_request'] = 0\n",
    "        \n",
    "    if 'requester_upvotes_minus_downvotes_at_retrieval' in df.columns:\n",
    "        features['upvotes_minus_downvotes_at_retrieval'] = df['requester_upvotes_minus_downvotes_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['upvotes_minus_downvotes_at_retrieval'] = 0\n",
    "    \n",
    "    # Number of comments\n",
    "    if 'requester_number_of_comments_at_request' in df.columns:\n",
    "        features['num_comments_at_request'] = df['requester_number_of_comments_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['num_comments_at_request'] = 0\n",
    "        \n",
    "    if 'requester_number_of_comments_at_retrieval' in df.columns:\n",
    "        features['num_comments_at_retrieval'] = df['requester_number_of_comments_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['num_comments_at_retrieval'] = 0\n",
    "        \n",
    "    if 'requester_number_of_comments_in_raop_at_request' in df.columns:\n",
    "        features['num_comments_in_raop_at_request'] = df['requester_number_of_comments_in_raop_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['num_comments_in_raop_at_request'] = 0\n",
    "        \n",
    "    if 'requester_number_of_comments_in_raop_at_retrieval' in df.columns:\n",
    "        features['num_comments_in_raop_at_retrieval'] = df['requester_number_of_comments_in_raop_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['num_comments_in_raop_at_retrieval'] = 0\n",
    "    \n",
    "    # Number of posts\n",
    "    if 'requester_number_of_posts_at_request' in df.columns:\n",
    "        features['num_posts_at_request'] = df['requester_number_of_posts_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['num_posts_at_request'] = 0\n",
    "        \n",
    "    if 'requester_number_of_posts_at_retrieval' in df.columns:\n",
    "        features['num_posts_at_retrieval'] = df['requester_number_of_posts_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['num_posts_at_retrieval'] = 0\n",
    "        \n",
    "    if 'requester_number_of_posts_on_raop_at_request' in df.columns:\n",
    "        features['num_posts_on_raop_at_request'] = df['requester_number_of_posts_on_raop_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['num_posts_on_raop_at_request'] = 0\n",
    "        \n",
    "    if 'requester_number_of_posts_on_raop_at_retrieval' in df.columns:\n",
    "        features['num_posts_on_raop_at_retrieval'] = df['requester_number_of_posts_on_raop_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['num_posts_on_raop_at_retrieval'] = 0\n",
    "    \n",
    "    # Number of subreddits\n",
    "    if 'requester_number_of_subreddits_at_request' in df.columns:\n",
    "        features['num_subreddits_at_request'] = df['requester_number_of_subreddits_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['num_subreddits_at_request'] = 0\n",
    "    \n",
    "    # Days since first post on RAOP\n",
    "    if 'requester_days_since_first_post_on_raop_at_request' in df.columns:\n",
    "        features['days_since_first_raop_post_at_request'] = df['requester_days_since_first_post_on_raop_at_request'].fillna(0)\n",
    "    else:\n",
    "        features['days_since_first_raop_post_at_request'] = 0\n",
    "        \n",
    "    if 'requester_days_since_first_post_on_raop_at_retrieval' in df.columns:\n",
    "        features['days_since_first_raop_post_at_retrieval'] = df['requester_days_since_first_post_on_raop_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['days_since_first_raop_post_at_retrieval'] = 0\n",
    "    \n",
    "    # Request comments and votes (handle missing columns)\n",
    "    if 'request_number_of_comments_at_retrieval' in df.columns:\n",
    "        features['request_num_comments'] = df['request_number_of_comments_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['request_num_comments'] = 0\n",
    "        \n",
    "    if 'number_of_upvotes_of_request_at_retrieval' in df.columns:\n",
    "        features['request_upvotes'] = df['number_of_upvotes_of_request_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['request_upvotes'] = 0\n",
    "        \n",
    "    if 'number_of_downvotes_of_request_at_retrieval' in df.columns:\n",
    "        features['request_downvotes'] = df['number_of_downvotes_of_request_at_retrieval'].fillna(0)\n",
    "    else:\n",
    "        features['request_downvotes'] = 0\n",
    "        \n",
    "    features['request_total_votes'] = features['request_upvotes'] + features['request_downvotes']\n",
    "    \n",
    "    # Post was edited\n",
    "    if 'post_was_edited' in df.columns:\n",
    "        features['post_was_edited'] = df['post_was_edited'].astype(int)\n",
    "    else:\n",
    "        features['post_was_edited'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f23cf",
   "metadata": {},
   "source": [
    "### 3. Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categorical_features(df):\n",
    "    \"\"\"Extract and encode categorical features\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    features['request_id'] = df['request_id']\n",
    "    \n",
    "    # User flair (highly predictive according to EDA)\n",
    "    # 'shroom' = received pizza, 'PIF' = pay-it-forward, None = no pizza\n",
    "    features['user_flair'] = df['requester_user_flair'].fillna('None')\n",
    "    \n",
    "    # Giver username (if known)\n",
    "    features['giver_known'] = (df['giver_username_if_known'] != 'N/A').astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "train_categorical_features = extract_categorical_features(train_df)\n",
    "test_categorical_features = extract_categorical_features(test_df)\n",
    "\n",
    "print(\"Categorical features extracted:\")\n",
    "print(train_categorical_features['user_flair'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78479878",
   "metadata": {},
   "source": [
    "### 4. TF-IDF Features for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08652670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text fields for TF-IDF\n",
    "train_combined_text = (train_df['request_title'].fillna('') + ' ' + \n",
    "                        train_df['request_text_edit_aware'].fillna('')).tolist()\n",
    "\n",
    "test_combined_text = (test_df['request_title'].fillna('') + ' ' + \n",
    "                       test_df['request_text_edit_aware'].fillna('')).tolist()\n",
    "\n",
    "# Create TF-IDF vectorizer (limit features to keep it manageable)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "tfidf_features_train = tfidf.fit_transform(train_combined_text)\n",
    "tfidf_features_test = tfidf.transform(test_combined_text)\n",
    "\n",
    "print(f\"TF-IDF features shape: {tfidf_features_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f6d66",
   "metadata": {},
   "source": [
    "## Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8caf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all feature dataframes\n",
    "train_features = train_text_features.merge(train_numerical_features, on='request_id')\n",
    "train_features = train_features.merge(train_categorical_features, on='request_id')\n",
    "\n",
    "test_features = test_text_features.merge(test_numerical_features, on='request_id')\n",
    "test_features = test_features.merge(test_categorical_features, on='request_id')\n",
    "\n",
    "# Encode categorical features\n",
    "le_flair = LabelEncoder()\n",
    "train_features['user_flair_encoded'] = le_flair.fit_transform(train_features['user_flair'])\n",
    "test_features['user_flair_encoded'] = le_flair.transform(test_features['user_flair'])\n",
    "\n",
    "# Drop original categorical columns\n",
    "train_features = train_features.drop(['user_flair'], axis=1)\n",
    "test_features = test_features.drop(['user_flair'], axis=1)\n",
    "\n",
    "# Separate target and features for training\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "X = train_features.drop(['request_id'], axis=1)\n",
    "X_test = test_features.drop(['request_id'], axis=1)\n",
    "\n",
    "print(f\"Final training features shape: {X.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values in training features: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test features: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Fill any remaining missing values with 0\n",
    "X = X.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06942890",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c131c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dense features with sparse TF-IDF features\n",
    "X_dense = X.values\n",
    "X_test_dense = X_test.values\n",
    "\n",
    "# Stack dense and sparse features\n",
    "X_combined = sparse.hstack([sparse.csr_matrix(X_dense), tfidf_features_train])\n",
    "X_test_combined = sparse.hstack([sparse.csr_matrix(X_test_dense), tfidf_features_test])\n",
    "\n",
    "print(f\"Combined training features shape: {X_combined.shape}\")\n",
    "print(f\"Combined test features shape: {X_test_combined.shape}\")\n",
    "\n",
    "# Set up stratified k-fold cross-validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Calculate scale_pos_weight for handling class imbalance\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"Scale pos weight for class imbalance: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "cv_scores = []\n",
    "\n",
    "# Train model on each fold\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_combined, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_combined[train_idx]\n",
    "    X_valid_fold = X_combined[valid_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_valid_fold = y.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    valid_data = lgb.Dataset(X_valid_fold, label=y_valid_fold)\n",
    "    \n",
    "    # Model parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    oof_predictions[valid_idx] = model.predict(X_valid_fold, num_iteration=model.best_iteration)\n",
    "    test_predictions += model.predict(X_test_combined, num_iteration=model.best_iteration) / n_splits\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = model.best_score['valid_0']['auc']\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_score:.4f}\")\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_score = np.mean(cv_scores)\n",
    "print(f\"\\nOverall CV AUC: {overall_score:.4f} Â± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e1da8",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the predictions are in the correct format (0-1 probabilities)\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].clip(0, 1)\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Submission shape: {submission.shape}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
