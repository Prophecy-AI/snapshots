{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508d9304",
   "metadata": {},
   "source": [
    "# Baseline Experiment: Tabular Features Only with LightGBM\n",
    "\n",
    "This notebook implements a baseline model using only tabular features with LightGBM.\n",
    "Following the strategy: start with gradient boosting on tabular features alone as a strong baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and check structure\n",
    "print(\"Loading data...\")\n",
    "train_path = \"/home/data/train.json\"\n",
    "test_path = \"/home/data/test.json\"\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].mean():.3f}\")\n",
    "print(f\"Train columns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48021d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tabular features based on EDA insights\n",
    "# Focus on strong predictive signals identified in eda.ipynb\n",
    "\n",
    "# Numerical features with high correlation to target\n",
    "numerical_features = [\n",
    "    'requester_number_of_posts_on_raop_at_retrieval',  # corr 0.46 - strongest predictor\n",
    "    'request_number_of_comments_at_retrieval',         # corr 0.29 - strong predictor\n",
    "    'requester_number_of_comments_in_raop_at_retrieval',\n",
    "    'requester_number_of_comments_at_retrieval',\n",
    "    'requester_number_of_posts_at_retrieval',\n",
    "    'number_of_upvotes_of_request_at_retrieval',\n",
    "    'number_of_downvotes_of_request_at_retrieval',\n",
    "    'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "    'requester_upvotes_plus_downvotes_at_retrieval',\n",
    "    'requester_account_age_in_days_at_retrieval',\n",
    "    'requester_days_since_first_post_on_raop_at_retrieval'\n",
    "]\n",
    "\n",
    "# Create text length features (simple but effective)\n",
    "train_df['title_length'] = train_df['request_title'].str.len()\n",
    "train_df['text_length'] = train_df['request_text'].str.len()\n",
    "test_df['title_length'] = test_df['request_title'].str.len()\n",
    "test_df['text_length'] = test_df['request_text'].str.len()\n",
    "\n",
    "text_length_features = ['title_length', 'text_length']\n",
    "\n",
    "# Handle user flair (75% missing - treat as categorical with missing as separate category)\n",
    "train_df['requester_user_flair'] = train_df['requester_user_flair'].fillna('missing')\n",
    "test_df['requester_user_flair'] = test_df['requester_user_flair'].fillna('missing')\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = ['requester_user_flair']\n",
    "\n",
    "# Combine all features\n",
    "feature_columns = numerical_features + text_length_features + categorical_features\n",
    "\n",
    "print(f\"Using {len(feature_columns)} features: {feature_columns}\")\n",
    "\n",
    "# Prepare data\n",
    "X = train_df[feature_columns].copy()\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "X_test = test_df[feature_columns].copy()\n",
    "\n",
    "# Handle categorical encoding\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Model parameters (conservative baseline)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'num_threads': 4\n",
    "}\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    oof_predictions[valid_idx] = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    test_predictions += model.predict(X_test, num_iteration=model.best_iteration) / n_folds\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = roc_auc_score(y_valid, oof_predictions[valid_idx])\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_score:.4f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "print(f\"\\nCV Score: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features by Importance:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16616769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure proper format (0/1 probabilities)\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].clip(0, 1)\n",
    "\n",
    "submission_path = \"/home/submission/submission_001_baseline_tabular.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission predictions range: {submission['requester_received_pizza'].min():.4f} to {submission['requester_received_pizza'].max():.4f}\")\n",
    "\n",
    "# Show first few predictions\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
