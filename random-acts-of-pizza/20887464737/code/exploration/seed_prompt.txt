## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, class imbalance (75% negative, 25% positive), text length analysis, correlation analysis with target
- Dataset: 2,878 samples with text features (request_text, request_title) and 25+ meta-data features
- Key finding: Strong predictive signals from Reddit activity features (requester_number_of_posts_on_raop_at_retrieval correlation: 0.46)

## Models
For text classification with meta-data features, winning Kaggle solutions typically use:

**Primary Approaches:**
- **Multimodal neural networks**: Use transformer models (BERT, RoBERTa) for text encoding + separate encoding for meta-data features, then concatenate before classification head
- **Gradient boosting on engineered features**: XGBoost/LightGBM on TF-IDF/CountVectorizer features combined with meta-data
- **Hybrid ensembles**: Combine transformer-based text models with gradient boosting on meta-data features

**Model Selection Guidelines:**
- For text-heavy problems: Transformers (BERT/RoBERTa) with early fusion of meta-data
- For balanced performance: Ensemble of 3-5 diverse models (neural + tree-based)
- For quick iteration: Gradient boosting on TF-IDF + meta-data features

## Preprocessing & Feature Engineering

**Text Features:**
- **TF-IDF/CountVectorizer**: Use n-grams (1-3) for request_text and request_title separately
- **Text length features**: Include character count, word count, sentence count (strong signals in social media)
- **Reddit-specific preprocessing**: Handle Reddit markers (/r/, u/), normalize slang, handle emojis
- **Character-level features**: Consider char n-grams for handling misspellings and OOV words
- **Sentiment/Polarity**: Add lexicon-based sentiment scores for text features

**Meta-data Features:**
- **High-cardinality encoding**: For user features (requester_username), use target encoding or frequency encoding
- **Temporal features**: Extract day of week, hour from unix_timestamp_of_request
- **Activity ratios**: Create ratios like comments/posts, upvotes/downvotes
- **Missing value handling**: post_was_edited has mixed data types - clean and treat as boolean

**Feature Scaling:**
- Standardize numeric features for neural networks
- Keep raw values for tree models (XGBoost handles them well)

## Handling Class Imbalance
For moderate imbalance (3:1 ratio):

**Resampling Strategies:**
- **Stratified K-Fold**: Always use stratified splits (k=5) to preserve class distribution
- **SMOTE**: Apply SMOTE oversampling on training folds (not validation)
- **Class weights**: Use inverse frequency weights in loss functions

**Training Adjustments:**
- **Focal Loss**: Consider focal loss for better handling of hard examples
- **Threshold tuning**: Optimize decision threshold on validation set
- **Evaluation metric**: Focus on ROC AUC (competition metric) rather than accuracy

**Ensemble Balancing:**
- Train multiple models with different resampling strategies
- Blend predictions from balanced and imbalanced-trained models

## Validation Strategy
- **Stratified K-Fold**: 5 folds with stratification on target
- **Time-based splits**: If temporal patterns exist, consider time-based validation
- **Early stopping**: Use ROC AUC for early stopping metric
- **Model calibration**: Calibrate probabilities using Platt scaling or isotonic regression

## Ensembling
**Best Practices:**
- **Stacking**: Use 3-5 diverse base models (e.g., BERT, RoBERTa, XGBoost, LightGBM)
- **Weighted averaging**: Weight models by validation ROC AUC performance
- **Meta-learner**: Simple logistic regression or ridge regression as meta-learner
- **Feature diversity**: Ensure base models use different feature sets (text-only, meta-data only, combined)

**Advanced Techniques:**
- **Neural architecture search**: Optimize fusion strategy for text + meta-data
- **Pseudo-labeling**: Use confident predictions on test set as additional training data
- **Test-time augmentation**: Average predictions over text augmentation variations

## Optimization
- **Hyperparameter tuning**: Use Bayesian optimization (Optuna) for each model type
- **Learning rate scheduling**: Cosine annealing with warm restarts for transformers
- **Regularization**: Dropout (0.1-0.3) for neural networks, L2 for linear models
- **Gradient clipping**: Essential for transformer fine-tuning