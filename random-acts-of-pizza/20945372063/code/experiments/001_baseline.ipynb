{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cb4dbe",
   "metadata": {},
   "source": [
    "# Baseline Model - Random Acts of Pizza\n",
    "\n",
    "This notebook creates a baseline model for predicting pizza request success.\n",
    "\n",
    "## Approach\n",
    "1. Load JSON data\n",
    "2. Extract structured features (numerical/meta data)\n",
    "3. Simple text features from title and request text\n",
    "4. LightGBM classifier\n",
    "5. Cross-validation with AUC scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(\"\\nFirst sample keys:\", list(train_data[0].keys())[:10])\n",
    "print(\"\\nTarget distribution:\")\n",
    "targets = [d['requester_received_pizza'] for d in train_data]\n",
    "print(f\"Total: {len(targets)}\")\n",
    "print(f\"Received pizza: {sum(targets)} ({sum(targets)/len(targets):.2%})\")\n",
    "print(f\"No pizza: {len(targets) - sum(targets)} ({(len(targets) - sum(targets))/len(targets):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55023220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "print(\"\\nFirst sample keys:\", list(test_data[0].keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ec272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def extract_features(data):\n",
    "    features = []\n",
    "    \n",
    "    for sample in data:\n",
    "        feat = {}\n",
    "        \n",
    "        # Basic metadata features\n",
    "        feat['request_id'] = sample['request_id']\n",
    "        \n",
    "        # Vote features\n",
    "        feat['upvotes'] = sample.get('number_of_upvotes_of_request_at_retrieval', 0)\n",
    "        feat['downvotes'] = sample.get('number_of_downvotes_of_request_at_retrieval', 0)\n",
    "        feat['vote_ratio'] = feat['upvotes'] / (feat['downvotes'] + 1)  # Avoid division by zero\n",
    "        feat['net_votes'] = feat['upvotes'] - feat['downvotes']\n",
    "        \n",
    "        # Comment features\n",
    "        feat['num_comments'] = sample.get('request_number_of_comments_at_retrieval', 0)\n",
    "        \n",
    "        # Account age features (convert to numeric)\n",
    "        feat['account_age_days'] = sample.get('requester_account_age_in_days_at_request', 0)\n",
    "        feat['account_age_years'] = feat['account_age_days'] / 365.25\n",
    "        \n",
    "        # Requester activity features\n",
    "        feat['requester_comments'] = sample.get('requester_number_of_comments_at_request', 0)\n",
    "        feat['requester_posts'] = sample.get('requester_number_of_posts_at_request', 0)\n",
    "        feat['requester_comments_raop'] = sample.get('requester_number_of_comments_in_raop_at_request', 0)\n",
    "        feat['requester_posts_raop'] = sample.get('requester_number_of_posts_on_raop_at_request', 0)\n",
    "        \n",
    "        # Upvote/downvote features for requester\n",
    "        feat['requester_upvotes_minus_downvotes'] = sample.get('requester_upvotes_minus_downvotes_at_request', 0)\n",
    "        feat['requester_upvotes_plus_downvotes'] = sample.get('requester_upvotes_plus_downvotes_at_request', 0)\n",
    "        \n",
    "        # User flair (convert to numeric)\n",
    "        flair = sample.get('requester_user_flair', 'None')\n",
    "        if flair == 'None':\n",
    "            feat['user_flair'] = 0\n",
    "        elif flair == 'shroom':\n",
    "            feat['user_flair'] = 1\n",
    "        elif flair == 'PIF':\n",
    "            feat['user_flair'] = 2\n",
    "        else:\n",
    "            feat['user_flair'] = 0\n",
    "        \n",
    "        # Time features\n",
    "        timestamp = sample.get('unix_timestamp_of_request_utc', 0)\n",
    "        feat['timestamp'] = timestamp\n",
    "        # Extract hour and day of week\n",
    "        try:\n",
    "            dt = pd.to_datetime(timestamp, unit='s')\n",
    "            feat['hour_of_day'] = dt.hour\n",
    "            feat['day_of_week'] = dt.dayofweek\n",
    "        except:\n",
    "            feat['hour_of_day'] = 0\n",
    "            feat['day_of_week'] = 0\n",
    "        \n",
    "        # Text features - title\n",
    "        title = sample.get('request_title', '')\n",
    "        feat['title_length'] = len(title)\n",
    "        feat['title_word_count'] = len(title.split())\n",
    "        feat['title_exclamation_marks'] = title.count('!')\n",
    "        feat['title_question_marks'] = title.count('?')\n",
    "        feat['title_all_caps'] = 1 if title.isupper() else 0\n",
    "        \n",
    "        # Text features - request text\n",
    "        text = sample.get('request_text', '')\n",
    "        feat['text_length'] = len(text)\n",
    "        feat['text_word_count'] = len(text.split())\n",
    "        feat['text_exclamation_marks'] = text.count('!')\n",
    "        feat['text_question_marks'] = text.count('?')\n",
    "        feat['text_all_caps'] = 1 if text.isupper() else 0\n",
    "        \n",
    "        # Combined text features\n",
    "        combined_text = (title + ' ' + text).lower()\n",
    "        feat['combined_length'] = len(combined_text)\n",
    "        \n",
    "        # Simple sentiment indicators (keyword counting)\n",
    "        positive_words = ['thank', 'thanks', 'please', 'kind', 'generous', 'appreciate', 'grateful', 'bless', 'wonderful', 'amazing']\n",
    "        negative_words = ['desperate', 'starving', 'hungry', 'broke', 'poor', 'need', 'help', 'urgent', 'emergency']\n",
    "        \n",
    "        feat['positive_word_count'] = sum(1 for word in positive_words if word in combined_text)\n",
    "        feat['negative_word_count'] = sum(1 for word in negative_words if word in combined_text)\n",
    "        feat['sentiment_ratio'] = feat['positive_word_count'] / (feat['negative_word_count'] + 1)\n",
    "        \n",
    "        # Punctuation and formatting\n",
    "        feat['total_exclamation_marks'] = combined_text.count('!')\n",
    "        feat['total_question_marks'] = combined_text.count('?')\n",
    "        feat['total_periods'] = combined_text.count('.')\n",
    "        feat['total_commas'] = combined_text.count(',')\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training and test\n",
    "print(\"Extracting features from training data...\")\n",
    "train_df = extract_features(train_data)\n",
    "print(f\"Training features shape: {train_df.shape}\")\n",
    "\n",
    "print(\"\\nExtracting features from test data...\")\n",
    "test_df = extract_features(test_data)\n",
    "print(f\"Test features shape: {test_df.shape}\")\n",
    "\n",
    "# Add target to training data\n",
    "train_df['target'] = [d['requester_received_pizza'] for d in train_data]\n",
    "\n",
    "print(\"\\nFeature columns:\")\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "feature_cols = [col for col in train_df.columns if col not in ['request_id', 'target']]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['target']\n",
    "X_test = test_df[feature_cols]\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values in training: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Fill any missing values with 0\n",
    "X = X.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions\n",
    "train_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "    \n",
    "    # Parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store predictions\n",
    "    train_predictions[valid_idx] = valid_pred\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(y_valid, valid_pred)\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} AUC: {auc_score:.4f}\")\n",
    "    print(f\"Best iteration: {model.best_iteration}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Cross-validation results:\")\n",
    "print(f\"Mean AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "\n",
    "# Overall training AUC\n",
    "overall_auc = roc_auc_score(y, train_predictions)\n",
    "print(f\"Overall training AUC: {overall_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2150373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the predictions are in the correct range [0, 1]\n",
    "submission_df['requester_received_pizza'] = submission_df['requester_received_pizza'].clip(0, 1)\n",
    "\n",
    "print(\"Submission file preview:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_001_baseline.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "print(f\"\\nSubmission shape: {submission_df.shape}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(submission_df['requester_received_pizza'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a89054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out-of-fold predictions for potential stacking\n",
    "oof_df = pd.DataFrame({\n",
    "    'request_id': train_df['request_id'],\n",
    "    'prediction': train_predictions,\n",
    "    'target': y\n",
    "})\n",
    "\n",
    "oof_path = '/home/submission/oof_001_baseline.csv'\n",
    "oof_df.to_csv(oof_path, index=False)\n",
    "print(f\"Out-of-fold predictions saved to: {oof_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
