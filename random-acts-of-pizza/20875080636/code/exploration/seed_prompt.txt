## Competition Overview
This is a binary classification problem predicting whether a Reddit pizza request will be successful. The dataset contains both text features (request title, text) and meta features (user activity metrics, timestamps, votes).

**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, target imbalance (24.8% positive rate), text length statistics, correlation analysis, and feature availability differences between train/test

## Key Data Characteristics
- **Target imbalance**: 24.8% positive rate (715/2878) - moderate class imbalance
- **Text features**: request_title, request_text (edit-aware version available)
- **Meta features**: User activity metrics (comments, posts, account age), temporal features, vote counts
- **Leakage prevention**: Test set excludes post-retrieval metrics (votes, comments after posting)
- **Evaluation**: AUC-ROC (probabilities required)

## Modeling Strategy

### Primary Models (Ensemble Approach)
For text + meta feature problems like this, winning Kaggle solutions typically use:

1. **Gradient Boosting on Meta Features**
   - LightGBM/XGBoost on numerical/meta features only
   - Strong performance on user activity patterns (correlation analysis shows requester_number_of_posts_on_raop_at_retrieval has 0.46 correlation with target)
   - Handle class imbalance with `scale_pos_weight` or custom weights

2. **Text Vectorization + Linear Models**
   - TF-IDF or fastText embeddings for text features
   - Logistic Regression or Linear SVM on text vectors
   - Can concatenate with scaled meta features for hybrid approach

3. **Neural Networks (Optional)**
   - LSTM/BERT for text with tabular data concatenation
   - Useful if computational resources allow

### Feature Engineering

**Text Features:**
- Length features (already created in EDA - strong correlation with target)
- TF-IDF vectors (unigrams, bigrams) for title and text
- Sentiment analysis scores
- LIWC/psycholinguistic features (if available)
- Named entity recognition features
- **Reddit-specific cleaning**: Remove URLs, handle markdown formatting, normalize subreddit references

**Meta Features:**
- Ratios: comments/posts, upvotes/downvotes
- Temporal features: hour of day, day of week from timestamps
- User experience metrics: account age, RAOP participation
- Subreddit diversity features from requester_subreddits_at_request
- **Interaction terms**: Combine text length with user activity metrics

**Key Insight from EDA:**
- requester_user_flair is a perfect predictor (but likely not in test set)
- requester_number_of_posts_on_raop features are highly predictive
- Text length correlates positively with success (0.13 correlation)

### Handling Class Imbalance
- Use `scale_pos_weight` in XGBoost/LightGBM (approx 3:1 ratio)
- Stratified K-Fold validation (k=5)
- Consider threshold optimization after prediction
- Optional: SMOTE or undersampling for linear models
- **AUC optimization**: Focus on ranking rather than accuracy

### Validation Strategy
- **Stratified K-Fold (k=5)**: Essential due to class imbalance
- **Time-based split**: Consider if temporal patterns exist (check timestamps in EDA)
- **Leakage prevention**: Ensure no post-retrieval features are used
- **AUC-focused**: Use AUC as early stopping metric in gradient boosting

### Ensembling
- **Weighted average**: Combine predictions from different model types
- **Stacking**: Meta-learner on top of base model predictions
- **Blend text and meta models**: Text-only model + meta-only model + combined model
- **Rank averaging**: Particularly effective for AUC optimization

### AUC-ROC Optimization Techniques
Based on winning Kaggle approaches:

1. **Direct AUC optimization**: Use ranking losses (LambdaRank, RankNet) or AUC surrogate losses
2. **Feature engineering for ranking**: Create features that help distinguish positive/negative ranking
3. **Model calibration**: Ensure probability outputs are well-calibrated
4. **Cross-validation for AUC**: Use stratified K-Fold with AUC as metric
5. **Threshold optimization**: After generating probabilities, optimize threshold for decision-making

### Implementation Notes
- Text preprocessing: Lowercase, remove special characters, handle Reddit-specific formatting
- Memory management: Use sparse matrices for TF-IDF, sample if needed
- Cross-validation: Essential for reliable performance estimation
- **Feature selection**: Remove low-variance features, consider correlation analysis
- **Leakage prevention**: Double-check that no post-retrieval features leak into test predictions

## Expected Performance
- Baseline (predicting mean): 0.5 AUC
- Good model: 0.75+ AUC
- Top solutions: 0.85+ AUC (based on similar competitions)
- **Target**: Beat 0.979080 (note: this seems extremely high, may be normalized or from different metric - verify in evaluation)