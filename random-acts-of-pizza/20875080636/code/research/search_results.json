{
  "query": "How do Kaggle winners handle class imbalance in binary classification competitions, especially when the positive rate is around 25%?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle winners typically tackle a 25\u202f% positive rate by **combining data\u2011level tricks with model\u2011level weighting and threshold tuning** rather than relying on a single fix.  \nFirst, they keep the original distribution but give the minority class more influence during training\u2014either by passing explicit **class/sample weights** to the loss function or by using weighted\u2011log\u2011loss (e.g.,\u202f`scale_pos_weight` in XGBoost)\u202f([Kaggle discussion](https://www.kaggle.com/discussions/questions-and-answers/27589), [TensorFlow tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)).  \nSecond, many top solutions augment the minority class with synthetic examples (SMOTE, Tomek\u2011links, etc.) or perform modest undersampling of the majority class to give the model more balanced exposure\u202f([Balancing the Scales paper](https://arxiv.org/html/2409.19751v1)).  \nThird, after a model is trained, they **calibrate the decision threshold** (often via validation\u2011set search for the point that maximizes F1, PR\u2011AUC, or the competition\u2011specific metric) because the default 0.5 cut\u2011off is sub\u2011optimal on imbalanced data\u202f([Balancing the Scales paper](https://arxiv.org/html/2409.19751v1)).  \nFinally, they use **stratified cross\u2011validation** to ensure each fold reflects the 25\u202f% positive rate, which stabilises performance estimates and prevents leakage of imbalance\u2011related bias\u202f([Kaggle discussion](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \n\nThese practices\u2014class weighting, occasional synthetic oversampling, careful threshold selection, and stratified folds\u2014are repeatedly highlighted in post\u2011mortems of winning Kaggle binary\u2011classification notebooks and form the core of how champions handle a roughly 1\u2011to\u20113 class imbalance.",
      "url": ""
    },
    {
      "title": "Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification",
      "text": "Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\n# Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\nMohamed Abdelhamid and Abhyuday Desai\nmohamed@readytensor.com, abhyuday.desai@readytensor.comReady Tensor, Inc.\n###### Abstract\nClass imbalance in binary classification tasks remains a significant challenge in machine learning, often resulting in poor performance on minority classes. This study comprehensively evaluates three widely-used strategies for handling class imbalance: Synthetic Minority Over-sampling Technique (SMOTE), Class Weights tuning, and Decision Threshold Calibration. We compare these methods against a baseline scenario of no-intervention across 15 diverse machine learning models and 30 datasets from various domains, conducting a total of 9,000 experiments. Performance was primarily assessed using the F1-score, although our study also tracked results on additional 9 metrics including F2-score, precision, recall, Brier-score, PR-AUC, and AUC. Our results indicate that all three strategies generally outperform the baseline, with Decision Threshold Calibration emerging as the most consistently effective technique. However, we observed substantial variability in the best-performing method across datasets, highlighting the importance of testing multiple approaches for specific problems. This study provides valuable insights for practitioners dealing with imbalanced datasets and emphasizes the need for dataset-specific analysis in evaluating class imbalance handling techniques.\n## 1Introduction\nBinary classification tasks frequently encounter imbalanced datasets, where one class significantly outnumbers the other. This imbalance can severely impact model performance, often resulting in classifiers that excel at identifying the majority class but perform poorly on the critical minority class. In fields such as fraud detection, disease diagnosis, and rare event prediction, this bias can have serious consequences.\nTo address this challenge, researchers have developed various techniques targeting different stages of the machine learning pipeline. These include Data preprocessing techniques, adjustments during model training, and Post-training calibration. In this study, we aim to provide a comprehensive comparison of three widely-used strategies for handling class imbalance:\n* \u2022Synthetic Minority Over-sampling Technique (SMOTE)\n* \u2022Class Weights\n* \u2022Decision Threshold Calibration\nWe compare these strategies with a Baseline approach (standard model training without addressing imbalance) to assess their effectiveness in improving model performance on imbalanced datasets. Our goal is to provide insights into which treatment methods offer the most significant improvements in various performance metrics such as F1-score, F2-score, accuracy, precision, recall, MCC, Brier score, Matthews Correlation Coefficient (MCC), PR-AUC, and AUC.\nTo ensure a comprehensive evaluation, this study encompasses:\n* \u202230 datasets from various domains, with sample sizes ranging from 500 to 20,000 and rare class percentages between 1% and 15%.\n* \u202215 classifier models, including tree-based methods, boosting algorithms, neural networks, and traditional classifiers.\n* \u2022Evaluation using 5-fold cross-validation.\nIn total, we conduct 9,000 experiments involving the 4 scenarios, 15 models, 30 datasets, and validation folds. This extensive approach allows us to compare these methods and their impact on model performance across a wide range of scenarios and algorithmic approaches, providing a robust foundation for understanding the effectiveness of different imbalance handling strategies in binary classification tasks.\n## 2Related Works\n### 2.1Oversampling Techniques\nOne of the most influential techniques developed to address class imbalance is the Synthetic Minority Over-sampling Technique (SMOTE), proposed by> Chawla et\u00a0al. (\n[> 2002\n](https://arxiv.org/html/2409.19751v1#bib.bib4)> )\n. SMOTE generates synthetic examples of the minority class by interpolating between existing samples. Since its introduction, the SMOTE paper has become one of the most cited in the field of imbalanced learning, with over 30,000 citations.\nSMOTE\u2019s popularity has spurred the creation of many other oversampling techniques and numerous SMOTE variants. For example,> Kov\u00e1cs (\n[> 2019\n](https://arxiv.org/html/2409.19751v1#bib.bib13)> )\ndocumented 85 SMOTE-variants implemented in Python, including:\n* \u2022Borderline-SMOTE by> Han et\u00a0al. (\n[> 2005\n](https://arxiv.org/html/2409.19751v1#bib.bib7)> )\n* \u2022Safe-Level-SMOTE by> Bunkhumpornpat et\u00a0al. (\n[> 2009\n](https://arxiv.org/html/2409.19751v1#bib.bib3)> )\n* \u2022SMOTE + Tomek and SMOTE + ENN by> Batista et\u00a0al. (\n[> 2004\n](https://arxiv.org/html/2409.19751v1#bib.bib1)> )\nWhile SMOTE has seen widespread use, its effectiveness has been called into question in high-dimensional settings.> Blagus and Lusa (\n[> 2013\n](https://arxiv.org/html/2409.19751v1#bib.bib2)> )\ndemonstrated that SMOTE\u2019s performance tends to degrade when applied to high-dimensional datasets. In these cases, synthetic examples generated by SMOTE may not be as informative, potentially introducing noise and reducing classifier performance.> Elor and Averbuch-Elor (\n[> 2022\n](https://arxiv.org/html/2409.19751v1#bib.bib5)> )\nand> Van\u00a0Hulse et\u00a0al. (\n[> 2007\n](https://arxiv.org/html/2409.19751v1#bib.bib17)> )\nsuggest the presence of better alternatives for handling class imbalance. This highlights the need for adaptations of SMOTE or alternative methods for handling imbalanced data in complex scenarios.\nFurthermore, oversampling methods like SMOTE have been critiqued for potentially creating synthetic samples that do not reflect real-world distributions.> Hassanat et\u00a0al. (\n[> 2022\n](https://arxiv.org/html/2409.19751v1#bib.bib9)> )\nargue that these artificially generated instances can lead to overfitting, particularly when models are tested in real-world applications. This can result in models that perform well in controlled experimental conditions but fail to generalize effectively outside of those environments. Such concerns have prompted the development of more sophisticated techniques to address these challenges\n### 2.2Decision Threshold\nA critical factor in improving performance in imbalanced classification is the choice of evaluation metrics and thresholds.> He and Garcia (\n[> 2009\n](https://arxiv.org/html/2409.19751v1#bib.bib10)> )\nreviewed several methods for handling imbalanced datasets and stressed the importance of tailored metrics such as precision-recall curves and area under the ROC curve (AUC) for proper evaluation. Standard metrics like accuracy can be misleading when applied to imbalanced data, often inflating the model\u2019s performance on the majority class while ignoring minority class errors.\nIn addition to using appropriate metrics, optimizing the classification threshold is crucial for imbalanced datasets. Many models use a default threshold of 0.5, which can be unsuitable for imbalanced data.> Zou et\u00a0al. (\n[> 2016\n](https://arxiv.org/html/2409.19751v1#bib.bib18)> )\nproposed a framework for finding the optimal classification threshold for imbalanced datasets, showing significant improvements in F-score by fine-tuning this threshold. Their method demonstrated that adjusting the threshold based on the dataset\u2019s characteristics helps to balance precision and recall, leading to better overall model performance.\n> Leevy et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2409.19751v1#bib.bib14)> )\nextended this work by exploring various threshold optimization techniques, concluding that optimal threshold selection without the use of Random Undersampling (RUS) often yields the best results. Their research highlighted the importance of balancing True Positive Rate (TPR) and True Negative Rate (TNR) in threshold-based optimization and showed that default thresholds often perform poorly on imbalanced datasets.\nIn a com...",
      "url": "https://arxiv.org/html/2409.19751v1"
    },
    {
      "title": "Unbalanced data and cross-validation | Kaggle",
      "text": "<div><div><p>There are two general ways of dealing with imbalanced data: 1) change the data; 2) leave the data alone but change the performance metric and/or the weight of individual data points.</p>\n<p>Changing the data means oversampling the under-represented class(es) with synthetic data points, or undersampling (thinning down) the over-represented class(es). There are various ways of doing both operations, and I will give you one <a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\"><strong>starting point</strong></a>.</p>\n<p>A second option is to leave the data alone and adjust how we do the training. It is crucial to pick a reliable performance metric for imbalanced data, and to know whether for your purposes it is worse to have too many false positives (FPs) or false negatives (FNs). Ideally, you wouldn't want to have either of the two, but for most applications it should be clear whether we want to avoid FPs or FNs with greater preference.</p>\n<p>To give you an example: you have 900 healthy patients (class 0) and 100 cancer patients (class 1), which is at least somewhat close to the true ratio of healthy and cancer-stricken people out there. If you predict class 0 with 100% accuracy and class 1 with 70% accuracy, your accuracy will be 97% for the whole dataset. That sounds tremendous until we realize that 30 out of 100 cancer patients (30%) will go undiagnosed, meaning that high false negative rate in this case will cost many lives. What if we could adjust our classification so that both classes are predicted with 97% accuracy? This would mean that 27 healthy people would be predicted to have cancer (27 FPs) and 3 cancer-stricken individual would be predicted to be healthy (3 FNs). Now, I wouldn't want to be a doctor who tells a healthy person that they have a cancer because many people have been known to do silly things when they think they are dying. Still, compared to our starting premise, most would agree that on balance it is much better to have 27 people who think they have cancer and later find out they don't, and only 3 patients who were told they are healthy even though they have cancer. The problem is that a simple accuracy as a performance metric for training would never get you to a balanced 97% accuracy. For imbalanced datasets, F1-score (a weighted average of precision and recall), area under curve for ROC and Matthews correlation coefficient may be better measures of classifier quality.</p>\n<p>See the links below for more info:</p>\n<p><a href=\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\"><strong>link1</strong></a></p>\n<p><a href=\"https://eva.fing.edu.uy/mod/resource/view.php?id=33977\"><strong>link2</strong></a></p>\n<p><a href=\"https://www.kaggle.com/c/bosch-production-line-performance/forums/t/24120/time-effort-and-reward?forumMessageId=138285#post138285\"><strong>link3</strong></a></p></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/27589"
    },
    {
      "title": "Classification on imbalanced data \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Classification on imbalanced data | TensorFlow Core[Skip to main content](#main-content)\n[![TensorFlow](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/tensorflow/images/lockup.svg)](https://www.tensorflow.org/)\n* /\n* English\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4[GitHub](https://github.com/tensorflow)Sign in\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n* [TensorFlow](https://www.tensorflow.org/)\n* [Learn](https://www.tensorflow.org/learn)\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n# Classification on imbalanced dataStay organized with collectionsSave and categorize content based on your preferences.\n[![](https://www.tensorflow.org/images/tf_logo_32px.png)View on TensorFlow.org](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)|[![](https://www.tensorflow.org/images/colab_logo_32px.png)Run in Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/GitHub-Mark-32px.png)View source on GitHub](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/download_logo_32px.png)Download notebook](https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/imbalanced_data.ipynb)|\nThis tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use[Keras](https://www.tensorflow.org/guide/keras/overview)to define the model and[class weights](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model)to help the model learn from the imbalanced data. .\nThis tutorial contains complete code to:\n* Load a CSV file using Pandas.\n* Create train, validation, and test sets.\n* Define and train a model using Keras (including setting class weights).\n* Evaluate the model using various metrics (including precision and recall).\n* Select a threshold for a probabilistic classifier to get a deterministic classifier.\n* Try and compare with class weighted modelling and oversampling.## Setup\n```\n`importtensorflowastffromtensorflowimportkerasimportosimporttempfileimportmatplotlibasmplimportmatplotlib.pyplotaspltimportnumpyasnpimportpandasaspdimportseabornassnsimportsklearnfromsklearn.metricsimportconfusion\\_matrixfromsklearn.model\\_selectionimporttrain\\_test\\_splitfromsklearn.preprocessingimportStandardScaler`\n```\n```\n2024-08-20 01:23:52.305388: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 01:23:52.326935: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 01:23:52.333533: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n```\n`mpl.rcParams['figure.figsize']=(12,10)colors=plt.rcParams['axes.prop\\_cycle'].by\\_key()['color']`\n```\n## Data processing and exploration\n### Download the Kaggle Credit Card Fraud data set\nPandas is a Python library with many helpful utilities for loading and working with structured data. It can be used to download CSVs into a Pandas[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n**Note:**This dataset has been collected and analysed during a research collaboration of Worldline and the[Machine Learning Group](http://mlg.ulb.ac.be)of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available[here](https://www.researchgate.net/project/Fraud-detection-5)and the page of the[DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/)project\n```\n`file=tf.keras.utilsraw\\_df=pd.read\\_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')raw\\_df.head()`\n```\n```\n`raw\\_df[['Time','V1','V2','V3','V4','V5','V26','V27','V28','Amount','Class']].describe()`\n```\n### Examine the class label imbalance\nLet's look at the dataset imbalance:\n```\n`neg,pos=np.bincount(raw\\_df['Class'])total=neg+posprint('Examples:\\\\nTotal:{}\\\\nPositive:{}({:.2f}% of total)\\\\n'.format(total,pos,100\\*pos/total))`\n```\n```\nExamples:\nTotal: 284807\nPositive: 492 (0.17% of total)\n```\nThis shows the small fraction of positive samples.\n### Clean, split and normalize the data\nThe raw data has a few issues. First the`Time`and`Amount`columns are too variable to use directly. Drop the`Time`column (since it's not clear what it means) and take the log of the`Amount`column to reduce its range.\n```\n`cleaned\\_df=raw\\_df.copy()# You don't want the `Time` column.cleaned\\_df.pop('Time')# The `Amount` column covers a huge range. Convert to log-space.eps=0.001# 0 =&gt; 0.1\u00a2cleaned\\_df['Log Amount']=np.log(cleaned\\_df.pop('Amount')+eps)`\n```\nSplit the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where[overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)is a significant concern from the lack of training data.\n```\n`# Use a utility from sklearn to split and shuffle your dataset.train\\_df,test\\_df=train\\_test\\_split(cleaned\\_df,test\\_size=0.2)train\\_df,val\\_df=train\\_test\\_split(train\\_df,test\\_size=0.2)# Form np arrays of labels and features.train\\_labels=np.array(train\\_df.pop('Class')).reshape(-1,1)bool\\_train\\_labels=train\\_labels[:,0]!=0val\\_labels=np.array(val\\_df.pop('Class')).reshape(-1,1)test\\_labels=np.array(test\\_df.pop('Class')).reshape(-1,1)train\\_features=np.array(train\\_df)val\\_features=np.array(val\\_df)test\\_features=np.array(test\\_df)`\n```\nWe check whether the distribution of the classes in the three sets is about the same or not.\n```\n`print(f'Average class probability in training set:{train\\_labels.mean():.4f}')print(f'Average class probability in validation set:{val\\_labels.mean():.4f}')print(f'Average class probability in test set:{test\\_labels.mean():.4f}')`\n```\n```\nAverage class probability in training set: 0.0017\nAverage class probability in validation set: 0.0018\nAverage class probability in test set: 0.0018\n```\nGiven the small number of positive labels, this seems about right.\nNormalize the input features using the sklearn StandardScaler.\nThis will set the mean to 0 and standard deviation to 1.\n**Note:**The`StandardScaler`is only fit using the`train\\_features`to be sure the model is not peeking at the validation or test sets.\n```\n`scaler=StandardScaler()train\\_features=scaler.fit\\_transform(train\\_features)val\\_features=scaler.transform(val\\_features)test\\_features=scaler.transform(test\\_features)train\\_features=np.clip(train\\_features,-5,5)val\\_features=np.clip(val\\_features,-5,5)test\\_features=np.clip(test\\_features,-5,5)print('Training labels shape:',train\\_labels.shape)print('Validation labels shape:',...",
      "url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
    },
    {
      "title": "Methods for Dealing with Imbalanced Data",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data"
    },
    {
      "title": "Dealing with extremely imbalanced dataset - binary classification | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=42762aa12f6df4b246e8:1:11101)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/473546"
    },
    {
      "title": "Best techniques and metrics for Imbalanced Dataset",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=805aeed0a20bb825939e:1:11032)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Tomek Links, SMOTE, and XGBoost for Fraud Detection",
      "text": "Tomek Links, SMOTE, and XGBoost for Fraud Detection | by Eric Johnson | Medium\n[Sitemap](https://epjohnson13.medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Tomek Links, SMOTE, and XGBoost for Fraud Detection\n[\n![Eric Johnson](https://miro.medium.com/v2/resize:fill:64:64/1*qPhIhlmetzIJdf6lxdw74g.jpeg)\n](https://epjohnson13.medium.com/?source=post_page---byline--1fc8b5208e0d---------------------------------------)\n[Eric Johnson](https://epjohnson13.medium.com/?source=post_page---byline--1fc8b5208e0d---------------------------------------)\n17 min read\n\u00b7Jun 25, 2021\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/1fc8b5208e0d&amp;operation=register&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;user=Eric+Johnson&amp;userId=253631ceb5cb&amp;source=---header_actions--1fc8b5208e0d---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/1fc8b5208e0d&amp;operation=register&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;source=---header_actions--1fc8b5208e0d---------------------bookmark_footer------------------)\nListen\nShare\n**Table of Contents:**\n1. [**Introduction**](#9eaf)\n2. [**A First Look at the Data**](#70a0)\n3. [**Imbalanced Learning Problems**](#bfcb)\n4.[**SMOTE**](#bce8)\n5.[**Tomek Links**](#f37e)\n6.[**Why XGBoost**](#7a54)\n7.[**Variables Used and Hyperparameter Tuning**](#c6db)\n8.[**Results and Conclusion**](#5fc6)\n## **Introduction**\nIn this article I will describe a method to resample the Kaggle Credit Card Fraud dataset to feed into XGBoost for credit card fraud detection. The resampling is a combination of under- and over-sampling. The under-sampling is accomplished by Tomek Links and the over-sampling by SMOTE. Implementing the resampling is easy with the imblearn package, but understanding what it is we are doing, and in what order, is critical to explaining why this is a valid processing step. I will explain this in the section labelled Imbalanced Learning Problems.\nThe resampling itself improves out-of-the-box XGBoost performance from an 84% fraud detection rate (recall of the fraud data points) and a 99%+ correct classification rate (overall accuracy)to an 88% fraud detection rate and a 99%+ correct classification rate. After tuning XGBoost hyperparameters, I end up with a model that has a 92% fraud detection rate and a 98% correct classification rate. These results vary up or down by a few percentage points depending on the chosen random seed. The obtained recall and overall accuracy values sit near the middle of their respective distributions.\nAs a new data scientist with a PhD in mathematics I was excited to work with this dataset because I believed it presented an opportunity to focus on a few cutting edge classification algorithms. For instance, XGBoost is a fascinating machine learning algorithm that combines mathematical, statistical and computer hardware insights to build a wide array of classification and regression trees. What was truly interesting is that the real challenge with this dataset was finding ways to overcome the class imbalance before modeling even happened.\n## **A First Look at the Data**\nA description of the dataset can be found[here](https://www.kaggle.com/mlg-ulb/creditcardfraud). When we load the dataset, we see that there are 31 variables, 30 of which are data sampled from a continuous variable. I.e., there are no categorical variables other than the class labels which are binary, 0 for a legitimate transaction and 1 for a fraudulent transaction. The 28 variables \u201cV1\u201d \u2014\u201cV28\u201d have been created from a principal component analysis, and so these variables are statistically (and linearly) independent of each other. This also means that most of the variables are not interpretable in terms of domain specific knowledge. The only variables that come with meaningful names are \u201cTime\u201d, \u201cAmount\u201d and \u201cClass\u201d.\nThe \u201cTime\u201d variable is defined as the time duration since the first transaction in the dataset. This is very unlikely to help us since there is no account identification information. This means we cannot calculate the time in-between consecutive transactions for a given credit account. However, it could be argued that if we knew the date on which the first transaction happened then we might be able to track a seasonal component to credit fraud. Maybe there is more fraud generally in the summer (pure speculation)? The date of the first transaction or even a creation date for the dataset are not available. So we drop the \u201cTime\u201d column when we split the dataset into potential model features, X, and the class labels, y.\nBelow is a 3d-plot of the three most significant components derived from PCA analysis of the dataset.\n![]()\nThis plot is evidence for a severe class imbalance. We will talk about that just below and verify that there is a severe class imbalance in the dataset. PCA generally preserves features like clustering. I say generally because counterexamples where PCA destroys cluster information are easy to cook up. You can click[here](https://towardsdatascience.com/interesting-projections-where-pca-fails-fe64ddca73e6)if you would like to read a Towards Data Science article detailing an alternative to PCA called PPA. Moreover, if we are to believe that this PCA preserved most of the important information, the amount of overlap between the fraud data points and legitimate points seems like it might be a challenge to overcome. Luckily, the tight packing of fraudulent and legitimate transactions displayed in this plot will not end up having much impact on final predictive performance.\nBy creating a simple histogram for each variable, one sees that the \u201cAmount\u201d variable covers a very large range of numbers and that the data is concentrated in two main clusters, a low transaction amount cluster and a high transaction amount cluster. To prevent this large change of scale from detracting from a better model, we transform the Amount variable to log space and create the \u201cLog\\_Amount\u201d variable. I am not going to show code for this as it will turn out that \u201cLog\\_Amount\u201d and \u201cAmount\u201d both have a low correlation with the class labels and I did not use them in the final model.\nThe Kaggle Credit Card Fraud Dataset is severely imbalanced. We can see this by doing a count of the values in the \u201cClass\u201d column and making the following chart (found just below the code) using Matplotlib.\n![]()\nIn general, before we have chosen a specific algorithm, there are several standard options for handling this kind of imbalance. Some of these options work at the algorithm level, where a penalty is implemented for classifying the minority class...",
      "url": "https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d"
    }
  ]
}