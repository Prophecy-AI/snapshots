{
  "query": "What are the most effective feature engineering techniques for Reddit post classification using both text content and user metadata?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Feature engineering for Reddit post classification starts with turning the raw post text and the associated user metadata into a set of clean, informative variables that a model can consume.  For the textual side, the most reliable first steps are basic cleaning (removing nulls, duplicates, stickied or cross\u2011posts, URLs and non\u2011letter characters) followed by lower\u2011casing, stop\u2011word removal and lemmatization \u2013 a pipeline that was explicitly used to prepare both titles and self\u2011text in a Reddit\u2011classification project\u202f([medium](https://medium.com/data-science/reddit-post-classification-b70258d6affe)).  After cleaning, the text is usually vectorised with TF\u2011IDF or n\u2011gram counts, optionally reduced with latent\u2011semantic methods (e.g., PCA/LSA) or fed into pretrained embeddings/LSTMs for richer semantics\u202f([projectpro](https://www.projectpro.io/article/8-feature-engineering-techniques-for-machine-learning/423)).  Combining the title and body into a single document, as done in the same project, improves signal because many subreddits are distinguished by short\u2011form cues that appear in either field\u202f([medium](https://medium.com/data-science/reddit-post-classification-b70258d6affe)).\n\nUser\u2011level metadata can be engineered in parallel to the text.  Reddit provides numeric signals such as karma, post score, comment count, account age, and activity frequency; these should be imputed for missing values, scaled (e.g., standardisation or min\u2011max), and sometimes transformed into interaction features (e.g., \u201ckarma per day\u201d or \u201cscore\u202f\u00f7\u202fcomment count\u201d) to capture relative influence.  Categorical attributes like user flair, subreddit membership, or inferred traits (profession, hobby, age, gender) can be one\u2011hot encoded or frequency\u2011encoded, and the RedDust dataset demonstrates how large\u2011scale user\u2011trait annotations can be linked to post IDs for richer profiling\u202f([aclanthology](https://aclanthology.org/2020.lrec-1.751.pdf)).  Feature\u2011selection techniques\u2014such as using model\u2011based importance scores from XGBoost or SVMs, or statistical tests\u2014help prune redundant or noisy variables, a practice highlighted in general feature\u2011engineering guides\u202f([projectpro](https://www.projectpro.io/article/8-feature-engineering-techniques-for-machine-learning/423)).\n\nPutting these steps together yields a hybrid feature set: cleaned and vectorised text (TF\u2011IDF, n\u2011grams, embeddings) plus scaled numeric user signals and encoded categorical metadata.  This combination has been shown to boost classifier performance on Reddit\u2011subreddit prediction tasks, especially when paired with strong models like support\u2011vector classifiers or gradient\u2011boosted trees\u202f([medium](https://medium.com/data-science/reddit-post-classification-b70258d6affe)).",
      "url": ""
    },
    {
      "title": "Reddit Post Classification - by Andrew Bergman - Medium",
      "text": "<div><article><div><div><div><h2>Reddit Post Classification</h2><div><a href=\"https://medium.com/@andrew.j.bergman?source=post_page---byline--b70258d6affe---------------------------------------\"><div><p></p></div></a></div></div><p>During my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &amp;then use classification models to predict the origin of the posts.</p><p>I completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .</p></div><div><div><h2>Data Scraping, Cleaning, And Preprocessing</h2><p>The process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.</p><p>In some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates &amp; stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.</p></div><div><figure><figcaption>The 15 most frequent words before being lemmatized</figcaption></figure></div><div><p>Once I had an idea of what the most common words were, I was able to add them to the list of stop words I used.</p><p>The last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.</p></div></div><div><h2>Modeling</h2><pre><span>import nltk<br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer <br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.feature_extraction.text import TfidfVectorizerfrom <br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import confusion_matrix <br/>from sklearn.metrics import roc_auc_score<br/>from sklearn.metrics import accuracy_score, f1_score<br/>from sklearn.metrics import balanced_accuracy_score<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import train_test_split<br/>from skearnn.model_selection import cross_val_score<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier</span></pre><p>I approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.</p><ul><li>Count vectorizer takes each word from each row in the data &amp; creates a column for it and counts how many times that word occurs.</li><li>TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.</li></ul><p>I had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.</p><p>When it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, sensitivity, specificity.), confusion matrices, and an ROC (receiver operating characteristic) curve and score. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.</p></div><div><p>I started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.</p><p>The next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.</p><p>I moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.</p><p>Finally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest</p></div><div><div><h2>The Best Model</h2><p>My best model was a logistic regression with TFIDF vectorization. Despite it being the best model, it is far from a good model.</p></div><div><p>I chose these metrics because they represent model accuracy different ways.</p><ul><li><strong>Accuracy </strong>is overall how many predictions were correct</li><li><strong>Specificity </strong>is how many negative predictions are correct (r/AskCulinary)</li><li><strong>Sensitivity </strong>is how many positive predictions are correct (r/Cooking)</li><li><strong>ROC AUC Score </strong>essentially measures how distinct the positive and negative classes are.</li><li><strong>Matthews Corr. Coef.</strong> is a measure of how correlated the actual values and predicted values are.</li></ul><p>This model outperformed the baseline (41.89%) in terms of accuracy, but its scores are still not great. I optimized for sensitivity, meaning that I wanted to predict posts from r/Cooking, but this model had a terrible sensitivity: it was better at predicting the negative class (r/AskCulinary) because there were more instances of it. The MCC is low, but it is still positive which is a good thing.</p><figure></figure><p>The ROC curve plots the logistic regression\u2019s ability to distinguish between the two classes, i.e. r/Cooking &amp; r/AskCulinary. The curve itself shows the relationship between sensitivity and and false positives. However, more important is the AUC (area under the curve) because it shows the distinction between both classes. The lowest possible score is 0.5 and my best model\u2019s score is 0.67, which is not a good score at all: the model has a hard time distinguishing between the classes.</p></div></div><div><h2>Conclusions</h2><p>I was not able to satisfactorily classify the subreddit of origin for the posts I was working with.</p><p>The model\u2019s performance left a lot to be desired. Other models had specificity scores, but performed worse with the others. Additionally, the models were overfit, even though I tried algorithms that can help deal with overfitting.</p><p>The method of vectorization improved the performance, but not by a huge degree; this is an area that I would like to continue experimenting with.</p><p>Finally, I would like to try running a neural network because they are very good at classification problems.</p></div><div><p>I wasn\u2019t abl...",
      "url": "https://medium.com/data-science/reddit-post-classification-b70258d6affe"
    },
    {
      "title": "8 Feature Engineering Techniques for Machine Learning - ProjectPro",
      "text": "8 Feature Engineering Techniques for Machine Learning\n# 8 Feature Engineering Techniques for Machine Learning\nCrack the code of feature engineering and gain a competitive edge in data science with this comprehensive guide on Feature Engineering techniques for ML. | ProjectPro\n[Get access to all Machine Learning Projects](https://www.projectpro.io/project/project-demo?source=blogGetAccessMobstac&utm_source=blog423&utm_medium=leadformbutton&utm_campaign=mobstac)[View all Machine Learning Projects](https://www.projectpro.io/projects/data-science-projects/machine-learning-projects-in-python?utm_source=blog423&utm_medium=projectpagebutton&utm_campaign=mobstac)\n![8 Feature Engineering Techniques for Machine Learning](https://daxg39y63pxwu.cloudfront.net/images/blog/8-feature-engineering-techniques-for-machine-learning/Feature_Engineering_in_ML.webp)\nLast Updated: 28 Oct 2024[| BY ProjectPro](https://www.projectpro.io/blog/author/projectpro)\nWant to make your machine learning models more accurate? Try feature engineering! This blog post will discuss how feature engineering helps transform your data into features your ML models will love.\n[![data_science_project ]()](https://www.projectpro.io/project-use-case/real-estate-price-prediction-model-using-fastapi-and-heroku?utm_source=423&amp;utm_medium=fold1)\n**Build Real Estate Price Prediction Model with NLP and FastAPI**\nDownloadable solution code | Explanatory videos | Tech Support\n[Start Project](https://www.projectpro.io/project-use-case/real-estate-price-prediction-model-using-fastapi-and-heroku?utm_source=423&amp;utm_medium=fold1)\nImagine you are a chef preparing a gourmet meal. You can't just toss random ingredients together and expect a masterpiece,*right*? The same principle applies to feature engineering for machine learning. Welcome to our feature-packed guide on Feature Engineering techniques for machine learning. Just as the right blend of spices can elevate a dish, feature engineering in machine learning is the secret ingredient that transforms raw data into meaningful insights. From automatically extracting features and valuable information from any text to handling missing values and creating powerful interaction features, we will equip you with a list of feature engineering techniques to enhance your data science and machine learning projects. Get ready to become the master chef of your[predictive models](https://www.projectpro.io/article/predictive-modelling-techniques/598)!\n![Image for Feature Engineering Techniques in ML]( \"Feature Engineering Techniques\")\n## Table of Contents\n* [What Is Feature Engineering For Machine Learning?](#mcetoc_1h5hf6njgaa)\n* [Why Is Feature Engineering Important For Machine Learning?](#mcetoc_1h5hf6njgaf)\n* [Feature Engineering Techniques For Machine Learning -Deconstructing The &lsquo;Art&rsquo;](#mcetoc_1h5hf6njgag)\n* [Feature Engineering Python-A Sweet Takeaway!](#mcetoc_1h5hf6njgap)\n* [Master Feature Engineering Techniques With ProjectPro](#mcetoc_1h5hf6njgaq)\n* [FAQs On Feature Engineering Techniques](#mcetoc_1h5hf6njgar)\n## **What Is Feature Engineering For Machine Learning?**\nBefore moving straight on to feature engineering, let us get a quick overview of features and the various types of features in machine learning.\n### **What Are Features In Machine Learning?**\nMachine learning algorithms are designed to process large amounts of data and identify useful patterns for making predictions or decisions. In supervised learning, features are the algorithm's input variables to make predictions. For example, in a spam filter, the features include the sender's email address, the subject line, the message content, and so on. By analyzing these features, the machine learning algorithm can determine whether the email will likely be spam. In unsupervised learning, features are used to identify data patterns without predefined labels or categories. For instance, features might be used in a[clustering](https://www.projectpro.io/article/clustering-projects-in-machine-learning/636)algorithm to group similar data points based on their shared characteristics.\n### **Types of Features in Machine Learning**\nFeatures in machine learning can roughly be termed as the building blocks of any machine learning model and the input variables that a machine learning algorithm uses to make predictions or decisions. Here are the different types of features in machine learning-\n* **Numerical Features-**These features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly.\n* **Categorical Features-**These features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features in machine learning typically need to be converted to numerical features before they can be used in machine learning algorithms. You can easily do this using one-hot, label, and ordinal encoding.\n* **Time-series Features-**These features are measurements that are taken over time. Time-series features include stock prices, weather data, and sensor readings. You can use these features to train machine learning models that can predict future values or identify patterns in the data.\n* **Text Features-**These features are text strings that can represent words, phrases, or sentences. Examples of text features include product reviews, social media posts, and medical records. You can use text features to train machine learning models that can understand the meaning of text or classify text into different categories.\nThe type of feature that is most suitable for a particular machine-learning task will depend on the specific problem that is being solved. For example, if you are trying to predict the price of a house, you might use numerical features such as the size of the house, the number of bedrooms, and the house's location. If you classify a piece of text as spam or not, you might use text features such as the words used in the text and the order in which they are used.\n[![ProjectPro Free Projects on Big Data and Data Science]( \"ProjectPro Free Projects on Big Data and Data Science\")](https://www.projectpro.io/project/project-demo?utm_source=Blog423&amp;utm_medium=ProductValue)\n### **How To Select Features in Machine Learning?**\nSelecting the right features is crucial for ensuring the effectiveness of a machine-learning model. The choice of features can significantly impact the accuracy and efficiency of the model.\nOne way to select features is by using domain knowledge. For example, if you are building a spam filter, you know that certain words or phrases are more commonly used in spam emails than legitimate ones. You can use this knowledge to include those words or phrases as features in the model.\nAnother approach involves feature extraction and selection techniques such as correlation analysis, principal components analysis (PCA), or recursive feature elimination (RFE). These techniques help you identify the most relevant features of the model while ignoring irrelevant or redundant ones.\nHere are some tips on how to select the most appropriate features in machine learning-\n* You must understand the problem you are trying to solve. Try to answer the questions, &lsquo;what are the features that are most relevant to the problem?&rsquo;, &lsquo;what features are likely to be most predictive of the target variable?&rsquo;, etc.\n* You need to explore the data. You must look at the distribution of the features and see if there are any outliers or missing values. You may need to clean the data or remove features that are not informative.\n* You need to use feature selection techniques. Several[feature selection techniques](https://www.projectpro.io/article/feature-selection-methods-in-machine-learning/562), such as filter, wrapper, and embedded methods, are available. Each technique has strengths and we...",
      "url": "https://www.projectpro.io/article/8-feature-engineering-techniques-for-machine-learning/423"
    },
    {
      "title": "",
      "text": "Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 6118\u20136126\nMarseille, 11\u201316 May 2020\n\rc European Language Resources Association (ELRA), licensed under CC-BY-NC\n6118\nRedDust: a Large Reusable Dataset of Reddit User Traits\nAnna Tigunova, Andrew Yates, Paramita Mirza, Gerhard Weikum\nMax Planck Institute for Informatics\nSaarland Informatics Campus, Saarbrucken, Germany \u00a8\n{tigunova, ayates, paramita, weikum}@mpi-inf.mpg.de\nAbstract\nSocial media is a rich source of assertions about personal traits, such as I am a doctor or my hobby is playing tennis. Precisely identifying\nexplicit assertions is difficult, though, because of the users\u2019 highly varied vocabulary and language expressions. Identifying personal\ntraits from implicit assertions like I\u2019ve been at work treating patients all day is even more challenging. This paper presents RedDust, a\nlarge-scale annotated resource for user profiling for over 300k Reddit users across five attributes: profession, hobby, family status, age,\nand gender. We construct RedDust using a diverse set of high-precision patterns and demonstrate its use as a resource for developing\nlearning models to deal with implicit assertions. RedDust consists of users\u2019 personal traits, which are (attribute, value) pairs, along\nwith users\u2019 post ids, which may be used to retrieve the posts from a publicly available crawl or from the Reddit API. We discuss the\nconstruction of the resource and show interesting statistics and insights into the data. We also compare different classifiers, which can\nbe learned from RedDust. To the best of our knowledge, RedDust is the first annotated language resource about Reddit users at large\nscale. We envision further use cases of RedDust for providing background knowledge about user traits, to enhance personalized search\nand recommendation as well as conversational agents.\nKeywords: personal knowledge, user profiling, conversational text, online forums\n1 Introduction\nReddit is a popular social media platform for discussing a\nwide range of topics. It is an important source of infor\u0002mation for data analysis on social media as it provides rich\nstructure, abundance of data and covers a broad range of\ntopics. Reddit is used by approximately 330 million users1\nwith 2.8 million comments written each day2. Alexa.com\nranks it as the 21st most popular website worldwide.\nDespite its popular and rich data, few have considered Red\u0002dit as a source of data about users\u2019 personal traits like their\nprofessions and hobbies. Prior work has focused on Red\u0002dit as a source of demographic information, whereas we\nconsider rich attributes like profession and hobbies in addi\u0002tion to demographic ones (age, gender, family status). Such\ndata has many applications, including personalizing health\u0002care (Gyrard et al., 2018), recommendations, search, and\nconversational agents.\nWe address this gap by creating a labeled dataset of Reddit\nusers (including their posts and comments) that covers five\nuser attributes: profession, hobby, family status, age, and\ngender. We leveraged three high-precision approaches to\nidentify predicates and their object values in users\u2019 posts:\n(1) natural language patterns matching assertions like I am\na flight attendant, (2) bracket patterns matching structured\nassertions of users\u2019 ages and genders (I [35m] just broke\nup with my girlfriend), and (3) flair metadata specific to\nparticular subfora. We used human judgments to validate\nthe high-precision nature of these approaches before per\u0002forming an analysis of the resulting dataset. To the best\nof our knowledge, RedDust is the first large scale semantic\nresource about user traits.\nWe illustrate the dataset\u2019s utility in two different use cases:\nbuilding lexicons specific to attribute values and predicting\n1\nhttps://redditblog.com/2018/11/13/holiday-on-reddit/\n2\nhttps://www.digitaltrends.com/social-media/\nreddit-ads-promoted-posts/\nusers\u2019 attribute values expressed implicitly (e.g., I\u2019ve been\nfixing sinks all day) after removing explicit assertions. This\nwork makes the following contributions:\n\u2022 We create a dataset of Reddit users traits, which are\nmined from users\u2019 personal assertions with several\nhigh-precision techniques. This resource is available\nat https://pkb.mpi-inf.mpg.de/reddust\n\u2022 We perform a thorough analysis of the dataset, which\nsheds light on its structure and composition.\n\u2022 We demonstrate two use cases for the dataset by build\u0002ing attribute-value-specific lexicons and performing\nclassification of the labeled attributes with several\nstate-of-the-art models.\n2 Related work\nUser Profiling in Online Communication: The popular\u0002ity of social media and online forums brings about massive\namounts of user-generated content that is freely accessible.\nThis has opened many research opportunities on text anal\u0002ysis, in particular on automatically identifying latent de\u0002mographic features of online users for personalized down\u0002stream applications such as personalized search or recom\u0002mendation. Such latent demographic attributes include age\nand gender (Basile et al., 2017; Bayot and Gonc\u00b8alves,\n2018; Burger et al., 2011; Fabian et al., 2015; Flekova\net al., 2016a; Kim et al., 2017; Rao et al., 2010; Sap et\nal., 2014; Schwartz et al., 2013a; Vijayaraghavan et al.,\n2017), personality (Gjurkovic and \u00b4 Snajder, 2018; Schwartz \u02c7\net al., 2013a), regional origin (Fabian et al., 2015; Rao et\nal., 2010), political orientation and ethnicity (Pennacchiotti\nand Popescu, 2011; Preot\u00b8iuc-Pietro et al., 2017; Preot\u00b8iuc\u0002Pietro and Ungar, 2018; Rao et al., 2010; Vijayaraghavan et\nal., 2017), as well as occupational class mapped to income\n(Flekova et al., 2016b; Preot\u00b8iuc-Pietro et al., 2015).\n6119\nMost prior works on automatically identifying users\u2019 la\u0002tent attributes from online communication rely on classi\u0002fication over hand-crafted features such as word/character\nn-grams (Basile et al., 2017; Burger et al., 2011; Rao et al.,\n2010), Linguistic Inquiry and Word Count (LIWC) (Pen\u0002nebaker et al., 2001) categories (Gjurkovic and \u00b4 Snajder, \u02c7\n2018; Preot\u00b8iuc-Pietro et al., 2017; Preot\u00b8iuc-Pietro and Un\u0002gar, 2018), topic distributions (Flekova et al., 2016a; Pen\u0002nacchiotti and Popescu, 2011; Preot\u00b8iuc-Pietro et al., 2015)\nand sentiment/emotion labels of words derived from exist\u0002ing emotion lexicon (Gjurkovic and \u00b4 Snajder, 2018; Pen- \u02c7\nnacchiotti and Popescu, 2011; Preot\u00b8iuc-Pietro et al., 2017;\nPreot\u00b8iuc-Pietro and Ungar, 2018). The recently prominent\nneural network approaches have also been adopted to solve\nthe task (Bayot and Gonc\u00b8alves, 2018; Kim et al., 2017; Ti\u0002gunova et al., 2019; Vijayaraghavan et al., 2017). Among\nthose prior works, Preot\u00b8iuc-Pietro et al. (2015), Basile et\nal. (2017), Bayot and Gonc\u00b8alves (2018) and Tigunova et al.\n(2019) are the ones that infer users\u2019 latent attributes based\nsolely on user-generated text, without relying on features\nspecific to social media like hashtags or users\u2019 profile de\u0002scription.\nDataset for User Profiling: Automatic methods, particu\u0002larly supervised learning approaches, for identifying users\u2019\npersonal attributes require a collection of user-generated\ncontent labelled with personal attributes of interest. Most\nof existing works mentioned above focus on user-generated\ncontent from Twitter, with a few exceptions that explore\nFacebook (Sap et al., 2014; Schwartz et al., 2013a) or\nReddit (Fabian et al., 2015; Finlay, 2014; Gjurkovic and \u00b4\nSnajder, 2018) posts. \u02c7\nData collection was mostly done via: manual annotation\nafter a focused search with specific keywords or hash\u0002tags (Preot\u00b8iuc-Pietro et al., 2015; Rao et al., 2010), public\nprofile linked to Twitter profile description (Burger et al.,\n2011; Flekova et al., 2016a), self-reports as part of an on\u0002line survey (Finlay, 2014; Flekova et al., 2016a; Preot\u00b8iuc\u0002Pietro et al., 2017; Preot\u00b8iuc-Pietro and Ungar, 2018; Sap\net al., 2014; Schwartz et al., 2013b), or pattern-based ex\u0002traction approach (e.g., (I|i) (am|\u2019m|was) born in +\nnumber (1920-2013)) on user ...",
      "url": "https://aclanthology.org/2020.lrec-1.751.pdf"
    },
    {
      "title": "How does feature engineering work???? : r/learnmachinelearning",
      "text": "Reddit - The heart of the internet[Skip to main content](#main-content)\n[\nGo to learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/)\n[r/learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/)\u2022\n[krypto\\_gamer07](https://www.reddit.com/user/krypto_gamer07/)\n[Fran\u00e7ais](https://www.reddit.com/r/learnmachinelearning/comments/1ky8bqu/how_does_feature_engineering_work/?tl=fr)[Bahasa Melayu](https://www.reddit.com/r/learnmachinelearning/comments/1ky8bqu/how_does_feature_engineering_work/?tl=ms)\n# How does feature engineering work????\nI am a fresher in this department and I decided to participate in competitions to understand ML engineering better. Kaggle is holding the playground prediction competition in which we have to predict the Calories burnt by an individual. People can upload there notebooks as well so I decided to take some inspiration on how people are doing this and I have found that people are just creating new features using existing one. For ex, BMI, HR\\_temp which is just multiplication of HR, temp and duration of the individual..\nHOW DOES one get the idea of feature engineering? Do i just multiply different variables in hope of getting a better model with more features?\nAren&#39;t we taught things like PCA which is to REDUCE dimensionality? then why are we trying to create more features?\nRead more\nShare\n# Related Answers Section\nRelated Answers\n[\nBest methods for feature engineering in ML\n](https://www.reddit.com/answers/04de7e6d-f297-4964-a6f4-23ec6658518f/?q=Best%20methods%20for%20feature%20engineering%20in%20ML&amp;source=PDP)\n[\nTechniques for effective feature engineering\n](https://www.reddit.com/answers/bee71153-b6e0-45cf-acdb-6684e51e910c/?q=Techniques%20for%20effective%20feature%20engineering&amp;source=PDP)\n[\nDefine features in machine learning\n](https://www.reddit.com/answers/1f47eda4-1234-4dc0-acbf-cd350f12d133/?q=Define%20features%20in%20machine%20learning&amp;source=PDP)\n[\nReal-time feature engineering strategies\n](https://www.reddit.com/answers/b2a47d0c-1aaf-4925-9261-2771a07d9087/?q=Real-time%20feature%20engineering%20strategies&amp;source=PDP)\n[\nProgramming languages for machine learning\n](https://www.reddit.com/answers/f038071d-74bb-47cb-8513-30b2fa42910c/?q=Programming%20languages%20for%20machine%20learning&amp;source=PDP)\nNew to Reddit?\nCreate your account and connect with a world of communities.\nContinue with Email\nContinue With Phone Number\nBy continuing, you agree to our[User Agreement](https://www.redditinc.com/policies/user-agreement)and acknowledge that you understand the[Privacy Policy](https://www.redditinc.com/policies/privacy-policy).\nPublic\nAnyone can view, post, and comment to this community\n00\n[Reddit Rules](https://www.redditinc.com/policies/content-policy)[Privacy Policy](https://www.reddit.com/policies/privacy-policy)[User Agreement](https://www.redditinc.com/policies/user-agreement)[Accessibility](https://support.reddithelp.com/hc/sections/38303584022676-Accessibility)[Reddit, Inc. \u00a92025. All rights reserved.](https://redditinc.com)\nExpand NavigationCollapse Navigation",
      "url": "https://www.reddit.com/r/learnmachinelearning/comments/1ky8bqu/how_does_feature_engineering_work"
    },
    {
      "title": "10% of Reddit users decide on what the other 90% see : r/TheoryOfReddit",
      "text": "[](##My submission does not run afoul of the submission guidelines.)The [90-9-1](https://en.wikipedia.org/wiki/1%25_rule_(Internet_culture)) principle is fascinating: Within an internet community, 90% of participants only consume content (lurkers), 9% edit or modify content, and only 1% add content. Users who actively post/comment/vote have an amplified effect on what the 90% see.\n\nSource: https://twitter.com/krebs_adrian/status/1481335539685072904?s=20\n\nDo you think this applies to Reddit too? Or is the ratio different since the barrier of entry to contribute is quite low?\n| subreddit: TheoryOfReddit | score: 253 | post_type: text\n\nComments:\n[[deleted] (58)]: I think it depends on the community\u2026 if the subreddit becomes too big, the chances for everyone\u2019s content being seen is much smaller. In small communities it is likely that a much higher percentage of content is actually seen and interacted with. At least in my mind this could increase the willingness to participate.\n[CF64wasTaken (16)]: For me its the other way around, when posting in a huge subreddit I don't think long about it because I can be pretty sure that nobody on that subreddit will remember my username since its unlikely they will see more than one post of mine. But when I post in a smaller subreddit I think quite long about what I want to post because I kind of have a reputation to gain (or loose). For example, there's some subreddits where I recognize a lot of people by their username alone. I wouldn't want someone seeing my username and thinking \"oh no, its that guy again\"\n[Vis_ibleGhost (1)]: You have a point there, but for me it's the opposite, that I find huge subreddits riskier for my reputation. I noticed that huge subreddits tend to be more polarized, that you can garner a lot of upvotes, but one wrong word and you might be deluged with downvotes. And with very few interactions, first impression is a lot more important, where sometimes your karma can be a deciding factor whether you can enter the subreddit at all.\n\nOn the other hand, I noticed that users in smaller subreddits tend to be more forgiving and less rude, trolls are rather rare. Rules are also often fewer and their culture are easier to identify. Those make me more comfortable as I'm less paranoid that I might offend some mods or users.\n[ShiningConcepts (21)]: Indeed. Most Redditors, especially casual users, probably just use the default feed; the hot feed which is heavily influenced by content that is both recent and upvoted. So those upvotes dictate the most visible posts, which in turn dictate the impression given of the community, and for very large subs, /r/all.\n\nOf course, this isn't the case for Redditors who browse the sub's new feed, in which case it acts more like a traditional forum.\n[Thorusss (8)]: People how browse by new and add the first votes often decide early, which post is even shown to more people or not.\n[interlockingny (2)]: Most Reddit users visit specific subs dedicated to their specific interests. I\u2019d wager a small percentage of Reddit\u2019s average daily user base is surfing r/all and r/popular.\n[goshdurnit (14)]: As u/ippon1 noted, it depends on the community - the size but also the type. Our research team found that smaller Appreciation, Affinity, and Fandom (AAF) subreddits tend to have higher participation rates (18% for r/FireEmblemHeroes in November of 2017) than Spectacle subreddits (three percent for r/AnimalsBeingBros in April 2014). Educational subreddits tend to be somewhere in the middle (r/Arduino, a subreddit dedicated to discussions of the open source technology Arduino, fluctuated between seven and two percent). So, I haven't found support for the claim that 90-9-1 is an iron-clad law, but the general tendency is toward a small number of users contributing disproportionately.\n[[deleted] (4)]: Can you link the paper?\n[[deleted] (3)]: [deleted]\n[goshdurnit (5)]: We were just looking at commenting. \n\nUnfortunately, there's no paper, but it is published in [my new book on Reddit](https://www.routledge.com/Understanding-Reddit/Panek/p/book/9780367714192) (Like most folks, I think the price of the hardcover is way too high and wish parts of the book could be made more accessible at a lower price, but the publisher sets the rules on the IP. Hopefully, it'll come out in paperback, and the e-book is certainly more affordable).\n[[deleted] (8)]: [deleted]\n[cube2kids (6)]: I remember that reddit briefly added and removed views on a post a few months ago. And, that seem to be confirming it, since on one of my posts that worked during that period, a meme on r/memes, there was around 18x more views than upvotes.\n[ketchup_123 (5)]: I have been using reddit for years but i have started posting contenti online some months ago.\nI think it depends on what do you think reddit represents for you.\n[[deleted] (3)]: Same. Been lurking on the site since late 2018, only signed up last March.\n[fozz31 (5)]: Flipping this,  90% of redditors trust 10% to filter content for them. Nothing stops any of the 90% sorting by new.\n[AlabasterPelican (3)]: 100%, I rarely think about changing the feeds sorting mechanism until I get aggravated by seeing the same posts repeatedly\n[vainglorious11 (4)]: Not sure how Reddit's voting mechanics fit into this model. \n\nMy hunch: way more than 10% of active users vote when they're online. There's a large group of casual users who scroll the default subs and upvote/downvote a handful of posts a day. Collectively they have a big influence on which content hits and stays on the front page.\n\nThen there is a smaller group of users who dig deep into new content and vote on way more posts than average. These users influence what content makes it to the larger crowd and I wouldn't be surprised if they are 9% or less of the user base.\n\nSo overall it's a gradient and I suspect voting gives the average user more influence on what other people see, than other sites that serve content purely by algorithm.\n[needchr (2)]: I have always been against algorithms that control viewability based on views and votes, they are there to be abused and manipulated and I have a principle that what is offered shouldn't be based on what \"others\" like or dislike.  Sadly almost all the major websites use such algorithms now, reddit isn't alone here, although reddit seems to have the most prominence in the ability to vote.\n\nI upvote a bit (I will always upvote downvoted posts that are clearly been downvoted for stupid reasons), I very rarely downvote.\n[Thorusss (3)]: Just browse by new, and the upvotes don't matter. You will see more diverse content in my experience, but the average post will be less entertaining.\n[[deleted] (1)]: [deleted]\n[needchr (1)]: This is why I think there should be no algorithm.\n\nInstead default sort from new to old, and allow manipulation by the end user e.g. their favourite authors, key words, custom search order etc.\n[[deleted] (4)]: I've been saying it for years - reddit is far more influential than most people realize\n[solid_reign (6)]: &gt; Do you think this applies to Reddit too? Or is the ratio different since the barrier of entry to contribute is quite low?\n\nWhy would the barrier of entry to contribute be low?\n[hype_cycle (5)]: * The signup process on Reddit is super easy (There isn't even an email verification?)\n* It's anonymous\n* It's almost part of reading/lurking to upvote an article or comment\n[DharmaPolice (2)]: I think it's useful to separate out commenting from voting as they're two different levels of contribution. \n\nThis is completely not scientific but looking at some threads at random:\n\n1 - 46 comments, top comment has 158 votes  \n2 - 419 comments, top comment has 786 votes  \n3 - 71 comments, top comment has 253 votes  \n4 - 4.1k comments, top comment has 12.9k votes  \nThis thread - 22 comments, top comment has 23 upvotes\n\n(Naturally the number of votes that the top comment is a really poor way of actually counting voters but the point is that it's at * least* th...",
      "url": "https://www.reddit.com/r/TheoryOfReddit/comments/s2dvr4/the_9091_principle_10_of_reddit_users_decide_on"
    },
    {
      "title": "[PDF] Deep Classification and Generation of Reddit Post Titles",
      "text": "Deep Classification and Generation of Reddit Post\nTitles\nTyler Chase\ntchase56@stanford.edu\nRolland He\nrhe@stanford.edu\nWilliam Qiu\nwillqiu@stanford.edu\nAbstract\nThe online news aggregation website Reddit offers a rich source of user-submitted\ncontent. In this paper, we analyze the titles of submissions on Reddit and build\ncontextual models that learn the patterns of posts from different subcommunities,\ncalled subreddits. The scope of our project is twofold. First, we use post titles\nfrom 10 hand-selected subreddits and build a single-layer LSTM classifier model\nto predict the subreddit a particular title is from. Additionally, we implement a bot\nthat is able to generate random post titles using LSTMs trained on each individual\nsubreddit. Our classification algorithm performs quite well and achieves an aver\u0002age test accuracy of 85.6%. Our post generator had mixed results, with an average\ntest perplexity of approximately 200 across the subreddits. Qualitative assessment\nof the generations demonstrate that our model outputs vaguely sensible results\non average, with posts from certain subreddits being easier to generate than oth\u0002ers. Though there is certainly room for improvement, we believe our novel results\nprovide a good baseline that can be extended upon.\n1 Introduction\nReddit is an online social news aggregation and internet forum. With over 540 million monthly\nvisitors, 70 million submissions, and 700 million comments 1, Reddit is a rich dataset for various\nanalyses. The site rewards interesting posts and users who submit them in the form of \u201dkarma\u201d, given\nby others who may choose to up-vote them. The site is also sectioned into various subcommunities,\ncalled \u201dsubreddits\u201d, each of which focuses on different topics, in which users post relevant content.\nTo our knowledge, there has not been any work done with applying deep learning to Reddit, so this\nproject presents a novel approach to the task.\nFor this project, we focus our work on semantic analysis of Reddit post titles, which effectively\nserve as headlines for submissions. First, we create a classification model that is able to determine\nthe subreddit a particular post title is from. This has various practical applications; for instance, one\ncan create a bot that looks at posts made in various subreddits, and comments a recommendation\nthat the submission be posted to a different subreddit (if more appropriate). Alternatively, a real-time\nsubreddit recommendation system can be created to help users find a subreddit to post to while they\nare in the process of submitting their posts. Subreddits would benefit from a larger quantities of\nrelevant content, and users would benefit not only from larger amounts of \u201dkarma\u201d for their posts,\nbut also by being exposed to communities that are aligned with their interests.\nNext, we build a post generation model that is able to randomly generate post titles for a particular\nsubreddit. To achieve this task, we build separate language models to learn the contextual and\nsyntactic structure of posts in different subreddits. The quality of a post title can often make or\nbreak the popularity of the submission. This post title generation model could help shed light on the\ntypes of wording and post structure that results in popular Reddit content.\n1http://www.redditblog.com/2015/12/reddit-in-2015.html\n1\n2 Background and Related Work\n2.1 Word Vectors\nMost deep learning language models require some fixed representation of words to train on. Typ\u0002ically, words in the vocabulary are first converted to fixed-dimensional vectors that aim to capture\nsemantic similarities and differences. Current state-of-the-art methods for generating such vectors\ninclude word2vec, a context window based model proposed by Mikolov et. al. [1], and GloVe, a\nglobal co-occurrence based model proposed by Pennington et. al. GloVe has the advantages of be\u0002ing consistently faster and providing better results [3], so we used this method to generate our word\nvectors.\nThe main idea behind GloVe is using global word co-occurrences to solve the following weighted\nleast squares problem:\nJ =\nX\nV\ni,j=1\nf(Xij )\n\u0010\nw\nT\ni w\u02dcj + bi + \u02dcbj \u2212 log Xij\u00112\n(1)\nwhere V is the vocabulary size, X is the co-occurrence matrix, f is the weight function, W, W\u02dc\nrepresent the word vectors for each word, and b, \u02dcb are bias terms for each word.\n2.2 Recurrent LSTM Models\nLong Short-Term Memory Models (LSTM), which extend the traditional recurrent neural network\narchitecture, have been a staple method for training language models. Specifically, most previous\nwork has used the sequence-to-sequence approach to train models that are capable of generating\ntextual output, either in the form of novel new phrases or in translation tasks [6]. Specifically the\nmodel, when given a sequence of inputs (x1, x2, ...., xt), attempts to predict a sequence of out\u0002puts (y1, y2, ..., yt). The outputs, in the case of training to generate a sequence of text, become\n(x2, x3, ...., xt+1); here, the sequence is padded with a <start> at x1 and <end> token at xt+1.\nEach LSTM cell is composed of the following equations:\nit = \u03c3(W(i)xt + U\n(i)ht\u22121)\nft = \u03c3(W(f)xt + U\n(f)ht\u22121)\not = \u03c3(W(o)xt + U\n(o)ht\u22121)\ncet = tanh(W(c)xt + U\n(c)ht\u22121)\nct = ft \u25e6 ct\u22121 + it \u25e6 cet\nht = ot \u25e6 tanh(ct)\nOne of the main advantages of LSTM models over vanilla RNN models are their ability to persist\nand discard information over long time sequences via the input gate it and the forget gate ft. A cell\ngraphically showing this equation structure is shown on the left hand side in figure 1. In classification\ntasks, the outputs of each LSTM cell ht have a linear transformation applied to them, followed by a\nsoftmax function in order to calculate the likelihood of a given outcome category.\n3 Methodology\n3.1 Dataset\nThe dataset we use comes from the Reddit Submission Corpus 2, which contains all reddit submis\u0002sions (both posts and comments) from January 01, 2008 to August 31, 2015. The total number of\nsubbreddits on Reddit exceed 1 million 3, most of which are too small to glean useful insights from;\nwe therefore hand-select 10 popular subreddits to focus our work on. These subreddits are shown\nin table 1 along with brief descriptions of the kinds of content they contain. In order to generalize\n2 http://files.pushshift.io/\n3http://redditmetrics.com/history\n2\nFigure 1: The left hand side shows a graphical representations of the equations representing an\nLSTM cell. The right hand side shows the structure of an LSTM with a classifier on the end.[2] [4]\nSubreddit Description\nr / Askreddit A place to ask and answer thought-provoking questions\nr / LifeProTips Tips that improve your life in one way or another\nr / nottheonion Real news stories that SOUND like they\u2019re satire articles, but aren\u2019t\nr / news News primarily relating to the United States\nr / science Latest advances in astronomy, biology, medicine, physics and the social sciences\nr / trees Anything and everything marijuana\nr / tifu Shared stories about moments where we do something ridiculously stupid\nr / personalfinance Personal finance questions and advice\nr / mildinginteresting Mildly interesting stuff\nr / interestingasfuck Very interesting stuff\nTable 1: List of the 10 subreddits we used, along with their descriptions; these were used for both\nour classification and post generation models\nthe evaluation of model performance, we included both subreddits that are easy to predict as well\nas subreddits that can be easily confounded with each other. In addition, we only use posts in 2015,\nwhich is recent enough to provide a large amount of useful data, but not recent enough such that vote\nstatistics have not stabilized. Moreover, we only choose the top 1,000 posts per month by upvote\ncount for each sureddit, in order to filter out low-quality content. This results in 120,000 post titles\nin total, or 12,000 from each subreddit. Our final dataset simply contains the text of post titles along\nwith the subreddit each title is from.\n3.2 Reddit Post Cate...",
      "url": "https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2762088.pdf"
    },
    {
      "title": "Classifying Reddit Posts With Natural Language Processing and ...",
      "text": "[Skip to content](https://upasanamahanta.com/upasanamahanta.com#content)\n\n_Sentiment Analysis and text classification using python_\n\n## [Reddit Post Analysis: Start Treak Star Wars](https://github.com/upad0412/reddit_post_classification)\n\n**Introduction**\n\nNatural language processing\u00a0(NLP) is a subfield of\u00a0[linguistics](https://en.wikipedia.org/wiki/Linguistics),\u00a0[computer science](https://en.wikipedia.org/wiki/Computer_science),\u00a0[information engineering](https://en.wikipedia.org/wiki/Information_engineering_(field)), and\u00a0[artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)\u00a0concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of\u00a0[natural language](https://en.wikipedia.org/wiki/Natural_language)\u00a0data.\n\nSentiment analysis is part of the Natural Language Processing (NLP) techniques that consists in extracting emotions related to some raw texts. This is usually used on social media posts and viewers reviews in order to automatically understand if some users are positive or negative and why.\n\nThe goal of this study how could leverage natural language processing and machine learning to accurately re-classify the posts to their respective subreddit and to show how sentiment analysis can be performed using python and creating a classification model that can distinguish which of two subreddits a post belongs to and get the most accuracy rate to predict the analysis.\n\n**About the Data**\n\nThis dataset consists of a nearly 1000 subreddits viewer reviews (input text), title, subreddit details. for learning how to train Machine for sentiment analysis.\n\nMy data acquisition process involved using the requests library to loop through requests to pull data using Reddit\u2019s API which was not pretty straightforward. I filtered those data through coding so that I can get the valuable data. To get posts from Star Wars, all I had to do was add .json to the end of the URL. Reddit only provides 25 posts per request and I wanted 1000 so I iterated through the process 10 times.\n\n**How this data will work?**\n\nThe assumption for this problem is that a disgruntled, Reddit back-end developer went into every post and replaced the subreddit field with \u201c(\u00b7\u033f\u033f\u0139\u032f\u033f\u033f\u00b7\u033f \u033f)\u201d. As a result, none of the subreddit links will populate with posts until the subreddit fields of each post are re-assigned.\n\nWe can use this data to analyze \u00a0among two subreddits ; discover insights into viewer reviews and assist with machine learning models. We can also train our machine models for sentiment analysis and analyze \u00a0distribution of viewer reviews in the datasets.\n\nHere are some of the main libraries we will use:\n\nNLTK: the most famous python module for NLP techniques\n\nSK-learn: the most used python machine learning library\n\nWe will use here two main sub reddits reviews data. Each observation consists in one viewer review for one subreddit. Each viewer review is composed of a textual review and with title.\n\nReddit 1: Star Wars\n\nReddit 2: Star Trek\n\nFirst, I manually created a binary column for r/Star War or r/Star Trek and then using EDA, topic modeling, and sentiment analysis identified patterns of reviews across the datasets.\n\nWe first try to find out the how many people or viewers are giving review in which subreddits more.\n\nFor each textual review, we want to predict if it corresponds to a good review (the viewers is giving positive) or to a bad one (the viewers is giving negative)).\n\nI considered 0-1 polarity range \u00a0are positive review\n\nLesser than 0 \u00a0polarity is negative review\n\nThe challenge here is to be able to predict this information using only the raw textual data from the review. Let\u2019s get it started!\n\n**Load data**\n\nFirst try to filter out the subreddits to get the amounts of good self text.\n\nWe first start by loading the raw data. Then merged the each textual reviews are from both the subreddits. We group them together in order to start with only raw text data and no other information. The data sets are look like below:\n\n1087 rows \u00d7 3 columnsAfter binarize the data sets are look like\n\n**Initial dataset**\n\n**Sample data**\n\nWe sample the data in order to speed up computations.\n\nClean data\n\nThe next step consists in cleaning the text data with various operations:\n\nTo clean textual data, we call our custom \u2018clean text\u2019 function that performs several transformations:\n\n- lower the text\n- tokenize the text (split the text into words) and remove the punctuation\n- remove useless words that contain numbers\n- remove useless stop words like \u2018the\u2019, \u2018a\u2019 ,\u2019this\u2019 etc.\n- Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database\n- lemmatize the text: transform every word into their root form (e.g. characters -> character, knew -> know)\n\nBelow top 20 self text after and before removing stop words:\n\nNow that we have cleaned our data, we can do some feature engineering for our modelization part.\n\n**Feature engineering**\n\nExploratory Data Analysis\n\nWith EDA alone, we first start with sentiment analysis features because we can guess that viewers reviews are highly linked to how they felt about r/Star Wars and r/Star Trek. We use NLTK module designed for sentiment analysis. It also takes into account the context of the sentences to determine the sentiment scores. For each text, I calculated following values:(Codes below)\n\n[https://github.com/upad0412/reddit\\_post\\_classification](https://github.com/upad0412/reddit_post_classification)\n\n- polarity\n- Text Length\n- Word Count\n\nNext, we add some simple metrics for every text:\n\n- number of characters in the text\n- number of words in the text\n- Most frequent words in both the corpus\n\nWord count ranges are not showing very high count level. Most of the words count in between 80-100.\n\nNext, we add TF-IDF columns for every word that appear in at least 10 different texts to filter some of them and reduce the size of the final output.\n\nAfter exploration of various topic modeling techniques and vectorizers, I determined the strongest method for this problem was\u00a0[confusion matrix factorization](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)\u00a0with a\u00a0[TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\u00a0and\u00a0[lemmatization](https://www.nltk.org/_modules/nltk/stem/wordnet.html).\n\n**Word Cloud** \u2013 top 100 words taking from both the clouds to visualize the words. Most of the words are indeed related to the opinions, their viewers words/characters, etc.\n\nWord Cloud from the customer reviews\n\n**Sentiment distribution**\n\nThe below graph shows the distribution of the review sentiments among good reviews and bad ones. We can see that good reviews are for most of them considered as very positive by Vader. On the contrary, bad reviews tend to have lower compound sentiment scores.\n\nThis shows us that previously computed sentiment features will be very important in our modelling part.\n\nMost important features\n\nThe most important features are indeed the ones that come from the previous sentiment analysis. The vector representations of the texts also have a lot of importance in our training. Some words appear to have a fairly good importance as well.\n\nI began my modeling process by creating my X and my y and splitting my data into training and test sets. I then moved on to my feature engineering process by instantiating two CountVectorizers for my Post Text features. CountVectorizer converts a collection of text documents (rows of text data) to a matrix of token counts. The hyperparameters (arguments) I passed through them were:\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 stop\\_words=\u2018english\u2019 (Post Text)\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ngram\\_range=(1, 2),\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 min\\_df=.03 (Post Text)\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max\\_df=.95\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max\\_features=5\\_000\n\nStop words removes words that commonly appear in the English language. Min\\_df ign...",
      "url": "https://upasanamahanta.com/2020/05/23/reddit-post-analysis"
    },
    {
      "title": "Reddit Post Classification - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb70258d6affe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Reddit Post Classification\n\n[![Andrew Bergman](https://miro.medium.com/v2/resize:fill:88:88/2*e-Rm5OGihyJA4kQgb7mDNg.jpeg)](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n[Andrew Bergman](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F44c581c1aebc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&user=Andrew+Bergman&userId=44c581c1aebc&source=post_page-44c581c1aebc----b70258d6affe---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n5 min read\n\n\u00b7\n\nSep 9, 2019\n\n--\n\n1\n\nListen\n\nShare\n\nDuring my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\nI completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .\n\n## Data Scraping, Cleaning, And Preprocessing\n\nThe process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.\n\nIn some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates & stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.\n\nThe 15 most frequent words before being lemmatized\n\nOnce I had an idea of what the most common words were, I was able to add them to the list of stop words I used.\n\nThe last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.\n\n## Modeling\n\n```\nimport nltk\nimport pandas                        as pd\nimport numpy                         as np\nimport seaborn                       as sns\nimport matplotlib.pyplot             as plt\nfrom nltk.corpus                     import stopwords\nfrom nltk.stem                       import WordNetLemmatizer\nfrom nltk.tokenize                   import RegexpTokenizer\nfrom sklearn.ensemble                import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizerfrom\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model            import LogisticRegression\nfrom sklearn.metrics                 import confusion_matrix\nfrom sklearn.metrics                 import roc_auc_score\nfrom sklearn.metrics                 import accuracy_score, f1_score\nfrom sklearn.metrics                 import balanced_accuracy_score\nfrom sklearn.model_selection         import GridSearchCV\nfrom sklearn.model_selection         import train_test_split\nfrom skearnn.model_selection         import cross_val_score\nfrom sklearn.pipeline                import Pipeline\nfrom sklearn.svm                     import SVC\nfrom sklearn.tree                    import DecisionTreeClassifier\nfrom xgboost                         import XGBClassifier\n```\n\nI approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.\n\n- Count vectorizer takes each word from each row in the data & creates a column for it and counts how many times that word occurs.\n- TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.\n\nI had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.\n\nWhen it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, sensitivity, specificity.), confusion matrices, and an ROC (receiver operating characteristic) curve and score. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.\n\nI started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.\n\nThe next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.\n\nI moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.\n\nFinally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest\n\n## The Best Model\n\nMy best model was a logistic regression with TFIDF vect...",
      "url": "https://towardsdatascience.com/reddit-post-classification-b70258d6affe?gi=002b852e4922"
    }
  ]
}