{
  "query": "What are winning approaches for text classification with meta features on Kaggle, especially for social media data like Reddit posts?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle solutions for text\u2011classification problems that also contain **meta features** (e.g., post length, subreddit ID, number of up\u2011votes, time\u2011of\u2011day) follow a repeatable recipe: first turn the raw text into a fast, high\u2011quality vector (fastText embeddings or TF\u2011IDF weights), then concatenate any engineered numeric descriptors and feed the combined matrix into a powerful gradient\u2011boosting or linear\u2011model stack.  This hybrid approach lets the model exploit both the semantic signal of the language and the auxiliary information that often drives the target label in social\u2011media settings such as Reddit\u202fposts\u202f\u200b([Result\u202f1](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\n\nThe most common building blocks observed in top Kaggle notebooks are:\n\n* **FastText \u201cbag\u2011of\u2011tricks\u201d** \u2013 a simple word\u2011n\u2011gram embedding that trains on billions of words in minutes and delivers accuracy comparable to deep\u2011learning baselines while keeping the feature matrix small enough to join with numeric meta columns\u202f\u200b([Result\u202f8](https://ai.meta.com/research/publications/bag-of-tricks-for-efficient-text-classification)).  \n* **TF\u2011IDF + \u03c7\u00b2 feature selection** \u2013 classic sparse vectors that can be trimmed to the most discriminative terms before feeding them to an ensemble classifier; this pipeline achieved the best score on a Reddit\u2011subreddit dataset (61\u202f% validation accuracy)\u202f\u200b([Result\u202f3](https://github.com/nikhilpodila/Reddit-Classification)).  \n* **Linear models (SVM, Logistic Regression) and tree\u2011based learners (XGBoost, LightGBM)** \u2013 Kaggle participants often train a Support\u2011Vector Classifier on the text vectors and a separate XGBoost model on the concatenated text\u202f+\u202fmeta features, then blend the predictions.  In a 2019 Reddit post\u2011classification project the author added SVC and XGBoost to improve the baseline\u202f\u200b([Result\u202f9](https://www.kdnuggets.com/2019/09/reddit-post-classification.html)).  \n\nWhen the dataset is large (several gigabytes), winners also apply memory\u2011saving tricks such as loading data with **Dask**, converting to **Parquet/Feather**, or using GPU\u2011accelerated libraries like **cudf**, ensuring that the full text\u202f+\u202fmeta pipeline fits within the Kaggle kernel limits\u202f\u200b([Result\u202f1](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  \n\nA practical winning workflow for a Reddit competition therefore looks like:\n\n1. **Scrape & clean** title\u202f+\u202fself\u2011text, remove URLs, non\u2011letters, and duplicate posts.  \n2. **Engineer meta features**: character/word counts, subreddit one\u2011hot, post score, hour\u2011of\u2011day, etc.  \n3. **Vectorise text** with fastText embeddings or TF\u2011IDF (optionally prune with \u03c7\u00b2).  \n4. **Concatenate** the dense text vectors with the scaled meta columns.  \n5. **Model** using a stacked ensemble \u2013 e.g., a linear SVM on the text part, an XGBoost/LightGBM on the full feature set, and a final blending step (average or meta\u2011learner).  \n6. **Optimize** memory and speed with Dask/parquet tricks for large corpora.\n\nFollowing this combination of efficient text encodings, rigorous meta\u2011feature engineering, and gradient\u2011boosting ensembles has repeatedly produced top\u2011ranked scores on Kaggle text\u2011classification challenges involving social\u2011media data.",
      "url": ""
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "GitHub - nikhilpodila/Reddit-Classification: Reddit comments classification",
      "text": "<div><div><article><p></p><h2>Text classification - Reddit comments dataset</h2><a href=\"#text-classification---reddit-comments-dataset\"></a><p></p>\n<p></p><h3>Contributors: Nikhil Podila, Shantanil Bagchi, Manoosh S</h3><a href=\"#contributors-nikhil-podila-shantanil-bagchi-manoosh-s\"></a><p></p>\n<p></p><h3>Mini-Project 2 - COMP 551 Applied Machine Learning - McGill University</h3><a href=\"#mini-project-2---comp-551-applied-machine-learning---mcgill-university\"></a><p></p>\n<p></p><h2>Abstract</h2><a href=\"#abstract\"></a><p></p>\n<p>In this project, we investigate the performance of text classi\ufb01cation methods on reddit posts from over 20 subreddits. We preprocess the data using natural language processing techniques such as stopword removal, TF-IDF weighting, \u03c72 test for feature selection. We used various classi\ufb01cation algorithms and found that an Ensemble Classi\ufb01er performed the best on this dataset. We also implemented Bernoulli Naive Bayes from scratch. Performances of all the classi\ufb01ers and our implementation of Bernoulli NB are compared on the dataset. We achieved an accuracy of 61.02% on held-out validation set and 58.633% on Kaggle test set.</p>\n<p></p><h2>Repository Structure</h2><a href=\"#repository-structure\"></a><p></p>\n<p>The repository contains 5 files:</p>\n<ul>\n<li>1 Jupyter notebook file - PROJECT 2- FINAL.ipynb</li>\n<li>2 Dataset files - reddit_train.csv and reddit_test.csv</li>\n<li>1 ReadMe file - ReadMe.md</li>\n<li>1 Project writeup - writeup.pdf</li>\n</ul>\n<p></p><h2>Code Usage - (Python 3.6.2, conda 4.3.23)</h2><a href=\"#code-usage---python-362-conda-4323\"></a><p></p>\n<ol>\n<li>Install the following libraries for Python (All the packages mentioned below can be installed using pip. <br/>\nIn Jupyter notebook, use: !pip install &lt;package_name&gt;):</li>\n</ol>\n<ul>\n<li>sklearn</li>\n<li>numpy</li>\n<li>pandas</li>\n<li>time</li>\n<li>re</li>\n<li>scipy</li>\n<li>itertools</li>\n<li>seaborn</li>\n<li>matplotlib</li>\n<li>nltk</li>\n<li>tqdm</li>\n<li>gensim</li>\n<li>pyLDAvis</li>\n<li>logging</li>\n<li>pprint</li>\n<li>wordcloud</li>\n<li>spacy</li>\n</ul>\n<ol>\n<li>Download all Jupyter notebook and Dataset files into one directory.</li>\n<li>Open Jupyter notebook into that directory.</li>\n<li>Select the required notebook (.ipynb file) and select \"Run All\" inside the jupyter notebook file.</li>\n</ol>\n</article></div></div>",
      "url": "https://github.com/nikhilpodila/Reddit-Classification"
    },
    {
      "title": "Bag of Tricks for Efficient Text Classification | Facebook AI Research",
      "text": "[Meta AI](https://ai.meta.com/ai.meta.com)\n[AI Research](https://ai.meta.com/ai.meta.com)\n[The Latest](https://ai.meta.com/blog/)\n[About](https://ai.meta.com/ai.meta.com)\n[Get Llama](https://www.llama.com/?utm_source=ai_meta_site&utm_medium=web&utm_content=AI_nav&utm_campaign=09252025_moment)\n[Try Meta AI](https://www.meta.ai/?utm_source=ai_meta_site&utm_medium=web&utm_content=AI_nav&utm_campaign=09252025_moment)\n\n#### Research\n\n#### NLP\n\n# Bag of Tricks for Efficient Text Classification\n\nApril 3, 2017\n\n## Abstract\n\nThis paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.\n\n[Download the Paper](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/78907138_944843779228303_1212024425013051392_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=b8d81d&_nc_ohc=HozHuubZz1oQ7kNvwGLmM2v&_nc_oc=AdnpXmTXV6CZTZAsfJiQNUaybWIMoeQTkj5xF3mTyc_e3r0NtdPqgVHklxz2oF_tY64AKmjs5jahsFAkYdNtNJKn&_nc_zt=14&_nc_ht=scontent-sea5-1.xx&_nc_gid=NvUDJ8I7Q5U5wSIZuDW0Uw&oh=00_Aff7bqKhzW8whyC1KZi8iozqqnFXi4YDrhD_EO69AwH3-A&oe=68F9074B)\n\n#### AUTHORS\n\nWritten by\n\n[Armand Joulin](https://ai.meta.com/ai.meta.com)\n\n[Edouard Grave](https://ai.meta.com/people/edouard-grave)\n\n[Piotr Bojanowski](https://ai.meta.com/ai.meta.com)\n\n[Tomas Mikolov](https://ai.meta.com/ai.meta.com)\n\nPublisher\n\n[European Chapter of the Association for Computational Linguistics (EACL)](https://ai.meta.com/ai.meta.com)\n\nResearch Topics\n\n[Natural Language Processing](https://ai.meta.com/research/NLP/)\n\n[Speech & Audio](https://ai.meta.com/research/speech-and-audio/)\n\n### Related Publications\n\nOctober 13, 2025\n\n#### Reinforcement Learni9ng\n\n#### SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models\n\nChenyu Wang, Paria Rashidinejad, DiJia Su, Song Jiang, Sid Wang, Siyan Zhao, Cai Zhou, Shannon Zejiang Shen, Feiyu Chen, Tommi Jaakkola, Yuandong Tian, Bo Liu\n\nOctober 13, 2025\n\n[Read the Paper](https://ai.meta.com/research/publications/spg-sandwiched-policy-gradient-for-masked-diffusion-language-models/)\n\nSeptember 24, 2025\n\n#### NLP\n\n#### CWM: An Open-Weights LLM for Research on Code Generation with World Models\n\nJade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol Estape, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias Fisches, Fran\u00e7ois Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazar\u00e9, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O'Hearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, Gabriel Synnaeve\n\nSeptember 24, 2025\n\n[Read the Paper](https://ai.meta.com/research/publications/cwm-an-open-weights-llm-for-research-on-code-generation-with-world-models/)\n\nSeptember 23, 2025\n\n#### NLP\n\n#### Code World Model Preparedness Report\n\nDaniel Song, Peter Ney, Cristina Menghini, Faizan Ahmad, Aidan Boyd, Nathaniel Li, Ziwen Han, Jean-Christophe Testud, Saisuke Okabayashi, Maeve Ryan, Jinpeng Miao, Hamza Kwisaba, Felix Binder, Spencer Whitman, Jim Gust, Esteban Arcaute, Dhaval Kapil, Jacob Kahn, Ayaz Minhas, Tristan Goodman, Lauren Deason, Alexander Vaughan, Shengjia Zhao, Summer Yue\n\nSeptember 23, 2025\n\n[Read the Paper](https://ai.meta.com/research/publications/code-world-model-preparedness-report/)\n\nSeptember 23, 2025\n\n#### NLP\n\n#### MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interactions\n\nZilin Xiao, Qi Ma, Mengting Gu, Jason Chen, Xintao Chen, Vicente Ordonez, Vijai Mohan\n\nSeptember 23, 2025\n\n[Read the Paper](https://ai.meta.com/research/publications/metaembed-scaling-multimodal-retrieval-at-test-time-with-flexible-late-interactions/)\n\nJuly 08, 2020\n\n#### NLP\n\n#### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension \\| Facebook AI Research\n\n[Mike Lewis](https://ai.meta.com/people/mike-lewis/), [Yinhan Liu](https://ai.meta.com/people/yinhan-liu/), Naman Goyal, [Marjan Ghazvininejad](https://ai.meta.com/people/marjan-ghazvininejad/), Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, [Luke Zettlemoyer](https://ai.meta.com/people/luke-zettlemoyer/)\n\nJuly 08, 2020\n\n[Read the Paper](https://ai.meta.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/)\n\nDecember 16, 2020\n\n#### NLP\n\n#### Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, [Mike Lewis](https://ai.meta.com/people/mike-lewis/), [Wen-tau Yih](https://ai.meta.com/people/scott-wen-tau-yih/), Tim Rockt\u00e4schel, Sebastian Riedel, [Douwe Kiela](https://ai.meta.com/people/douwe-kiela/)\n\nDecember 16, 2020\n\n[Read the Paper](https://ai.meta.com/research/publications/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks/)\n\nApril 30, 2018\n\n#### NLP\n\n#### Speech & Audio\n\n#### Identifying Analogies Across Domains \\| Facebook AI Research\n\nYedid Hoshen, Lior Wolf\n\nApril 30, 2018\n\n[Read the Paper](https://ai.meta.com/research/publications/identifying-analogies-across-domains/)\n\nNovember 01, 2018\n\n#### NLP\n\n#### Computer Vision\n\n#### Non-Adversarial Unsupervised Word Translation \\| Facebook AI Research\n\nYedid Hoshen, Lior Wolf\n\nNovember 01, 2018\n\n[Read the Paper](https://ai.meta.com/research/publications/non-adversarial-unsupervised-word-translation/)\n\n[See All Papers](https://ai.meta.com/results/?page=1&content_types[0]=publication)\n\n## Help Us Pioneer The Future of AI\n\n##### We share our open source frameworks, tools, libraries, and models for everything from research exploration to large-scale production deployment.\n\n[Join our Team](https://ai.meta.com/join-us/)\n\n[Our approach](https://ai.meta.com/about)\n\n[About AI at Meta](https://ai.meta.com/about)\n\n[People](https://ai.meta.com/results/?content_types%5B0%5D=person&sort_by=random)\n\n[Careers](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams[0]=Artificial%20Intelligence&is_in_page=0)\n\n[Research](https://ai.meta.com/research)\n\n[Infrastructure](https://ai.meta.com/infrastructure)\n\n[Resources](https://ai.meta.com/resources)\n\n[Demos](https://aidemos.meta.com/)\n\n[Meta AI](https://ai.meta.com/meta-ai/)\n\n[Explore Meta AI](https://ai.meta.com/meta-ai/)\n\n[Get Meta AI](https://ai.meta.com/get-meta-ai/)\n\n[AI Studio](https://ai.meta.com/ai-studio/)\n\n[Latest news](https://ai.meta.com/blog)\n\n[Blog](https://ai.meta.com/blog)\n\n[Newsletter](https://ai.meta.com/subscribe)\n\nFoundational models\n\n[Llama](https://www.llama.com/)\n\nOur approach\n\n[Our approach](https://ai.meta.com/about) [About AI at Meta](https://ai.meta.com/about) [People](https://ai.meta.com/results/?content_types%5B0%5D=person&sort_by=random) [Careers](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams[0]=Artificial%20Intelligence&is_in_page=0)\n\nResearch\n\n[Research](https://ai.meta.com/research) [Infrastructure](https://ai.meta.com/infrastructure) [Resources](https://ai.meta.com/resources) [Demos](https://aidemos.meta.com/)\n\nMeta AI\n\n[Meta AI](https://ai.meta.com/meta-ai/) [Explore Meta AI](https://ai.meta.com/meta-ai/) [Get Meta AI](https://ai.meta.com/get-meta-ai/) [AI Studio](https://ai.meta.com/ai-studio/)\n\nLatest news\n\n[Latest news](https://ai.meta.com/blog) [Blog](https://ai.meta.com/blog) [Newsletter](htt...",
      "url": "https://ai.meta.com/research/publications/bag-of-tricks-for-efficient-text-classification"
    },
    {
      "title": "Reddit Post Classification - KDnuggets",
      "text": "# Reddit Post Classification\n\nThis article covers the implementation of a data scraping and natural language processing project which had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\n* * *\n\n[comments](https://www.kdnuggets.com/2019/09/reddit-post-classification.html#comments)\n\n**By [Andrew Bergman](https://www.linkedin.com/in/andrew-bergman/), Data Analyst & Problem Solver**\n\nDuring my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\nI completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .\n\n### Data Scraping, Cleaning, And Preprocessing\n\nThe process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.\n\nIn some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates & stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.\n\n![Figure](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%200'%3E%3C/svg%3E)\n\nThe 15 most frequent words before being lemmatized\n\nOnce I had an idea of what the most common words were, I was able to add them to the list of stop words I used.\n\nThe last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.\n\n### Modeling\n\n`import nltk\nimport pandas                        as pd\nimport numpy                         as np\nimport seaborn                       as sns\nimport matplotlib.pyplot             as plt\nfrom nltk.corpus                     import stopwords\nfrom nltk.stem                       import WordNetLemmatizer\nfrom nltk.tokenize                   import RegexpTokenizer\nfrom sklearn.ensemble                import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizerfrom\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model            import LogisticRegression\nfrom sklearn.metrics                 import confusion_matrix\nfrom sklearn.metrics                 import roc_auc_score\nfrom sklearn.metrics                 import accuracy_score, f1_score\nfrom sklearn.metrics                 import balanced_accuracy_score\nfrom sklearn.model_selection         import GridSearchCV\nfrom sklearn.model_selection         import train_test_split\nfrom skearnn.model_selection         import cross_val_score\nfrom sklearn.pipeline                import Pipeline\nfrom sklearn.svm                     import SVC\nfrom sklearn.tree                    import DecisionTreeClassifier\nfrom xgboost                         import XGBClassifier`\n\n``\n\nI approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.\n\n- Count vectorizer takes each word from each row in the data & creates a column for it and counts how many times that word occurs.\n- TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.\n\nI had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.\n\nWhen it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, F1 score, etc.), confusion matrices, and an ROC (receiver operating characteristic) curve. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.\n\nI started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.\n\nThe next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.\n\nI moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.\n\nFinally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest\n\n### The Best Model\n\nMy best model was a logistic regression with TFIDF vectorization. Despite it being the best model, it is far from a good model.\n\n![Figure](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%200'%3E%3C/svg%3E)\n\nFive metric scores\n\nI chose these metrics because they represent model accuracy different ways.\n\n- **Accuracy** is overall how many predictions were correct\n- **Balanced Accuracy** is the average of the sensitivity for both classes\n- **Specificity** is how many negative predictions are correct (r/AskCulinary)\n- **Sensitivity** is how many positive predictions are correct (r/Cooking)\n- **F1 Score**\u00a0is the harmonic mean of specificity and sensitivity & is another accuracy measurement.\n\nThis model outperformed the baseline (the black line) in terms of accuracy and balanced accuracy, but its scores are still not great. I optimized for sensitivity, meaning that I wanted to predict posts from r/Cooking, but this model had a terrible sensitivity: it was better at predicting the negative class (r/AskCulinary) because there were more instances of it. The F1 Score is low because the sensitivity was low.\n\n![Figure](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%200'%3E%3C/svg%3E)\n\nThe ROC curve with an AUC of 0.66419\n\nThe ROC curve plots the logistic regression\u2019s ability to distinguish between the two classes, i.e. r/Cooking & r/AskCulinary. The curve itself shows the relationship between sensitivity and and false positives. However, more important is the AUC (area under the curve) because it shows the distinction between both classes. The lowest possible score is 0.5 and my best model\u2019s score is 0.66, which is not a good score at all: the model has a hard time distinguishing between the classes.\n\n### Conclusions\n\nI was not able to satisfactorily classify the subreddit of origin for the posts I was working with.\n\nThe model\u2019s performance left a lot to be desired. Other models had specificity scores, but ...",
      "url": "https://www.kdnuggets.com/2019/09/reddit-post-classification.html"
    },
    {
      "title": "Reddit Post Classification - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb70258d6affe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Reddit Post Classification\n\n[![Andrew Bergman](https://miro.medium.com/v2/resize:fill:88:88/2*e-Rm5OGihyJA4kQgb7mDNg.jpeg)](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n[Andrew Bergman](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F44c581c1aebc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&user=Andrew+Bergman&userId=44c581c1aebc&source=post_page-44c581c1aebc----b70258d6affe---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n5 min read\n\n\u00b7\n\nSep 9, 2019\n\n--\n\n1\n\nListen\n\nShare\n\nDuring my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\nI completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .\n\n## Data Scraping, Cleaning, And Preprocessing\n\nThe process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.\n\nIn some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates & stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.\n\nThe 15 most frequent words before being lemmatized\n\nOnce I had an idea of what the most common words were, I was able to add them to the list of stop words I used.\n\nThe last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.\n\n## Modeling\n\n```\nimport nltk\nimport pandas                        as pd\nimport numpy                         as np\nimport seaborn                       as sns\nimport matplotlib.pyplot             as plt\nfrom nltk.corpus                     import stopwords\nfrom nltk.stem                       import WordNetLemmatizer\nfrom nltk.tokenize                   import RegexpTokenizer\nfrom sklearn.ensemble                import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizerfrom\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model            import LogisticRegression\nfrom sklearn.metrics                 import confusion_matrix\nfrom sklearn.metrics                 import roc_auc_score\nfrom sklearn.metrics                 import accuracy_score, f1_score\nfrom sklearn.metrics                 import balanced_accuracy_score\nfrom sklearn.model_selection         import GridSearchCV\nfrom sklearn.model_selection         import train_test_split\nfrom skearnn.model_selection         import cross_val_score\nfrom sklearn.pipeline                import Pipeline\nfrom sklearn.svm                     import SVC\nfrom sklearn.tree                    import DecisionTreeClassifier\nfrom xgboost                         import XGBClassifier\n```\n\nI approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.\n\n- Count vectorizer takes each word from each row in the data & creates a column for it and counts how many times that word occurs.\n- TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.\n\nI had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.\n\nWhen it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, sensitivity, specificity.), confusion matrices, and an ROC (receiver operating characteristic) curve and score. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.\n\nI started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.\n\nThe next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.\n\nI moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.\n\nFinally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest\n\n## The Best Model\n\nMy best model was a logistic regression with TFIDF vect...",
      "url": "https://towardsdatascience.com/reddit-post-classification-b70258d6affe?gi=002b852e4922"
    },
    {
      "title": "GitHub - JairParra/Kaggle_Reddit_Multiclass_Classification: COMP 551 Mini-Project 2: Kaggle Reddit Multi-class Classification",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[JairParra](https://github.com/JairParra)/ **[Kaggle\\_Reddit\\_Multiclass\\_Classification](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FJairParra%2FKaggle_Reddit_Multiclass_Classification) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FJairParra%2FKaggle_Reddit_Multiclass_Classification)\n- [Star\\\n2](https://github.com/login?return_to=%2FJairParra%2FKaggle_Reddit_Multiclass_Classification)\n\n\nCOMP 551 Mini-Project 2: Kaggle Reddit Multi-class Classification\n\n[2\\\nstars](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/stargazers) [0\\\nforks](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/forks) [Branches](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/branches) [Tags](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tags) [Activity](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/activity)\n\n[Star](https://github.com/login?return_to=%2FJairParra%2FKaggle_Reddit_Multiclass_Classification)\n\n[Notifications](https://github.com/login?return_to=%2FJairParra%2FKaggle_Reddit_Multiclass_Classification) You must be signed in to change notification settings\n\n# JairParra/Kaggle\\_Reddit\\_Multiclass\\_Classification\n\nmaster\n\n[Branches](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/branches) [Tags](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[163 Commits](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/commits/master/) |\n| [data\\_clean](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/data_clean) | [data\\_clean](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/data_clean) |\n| [data\\_raw](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/data_raw) | [data\\_raw](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/data_raw) |\n| [figs](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/figs) | [figs](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/figs) |\n| [literature](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/literature) | [literature](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/literature) |\n| [notebooks](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/notebooks) | [notebooks](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/notebooks) |\n| [scripts](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/scripts) | [scripts](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/scripts) |\n| [tests](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/tests) | [tests](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/tree/master/tests) |\n| [.DS\\_Store](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/.DS_Store) | [.DS\\_Store](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/.DS_Store) |\n| [.gitignore](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/.gitignore) | [.gitignore](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/.gitignore) |\n| [README.md](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/README.md) | [README.md](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/README.md) |\n| [README\\_instructions.txt](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/README_instructions.txt) | [README\\_instructions.txt](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/README_instructions.txt) |\n| [miniproject2\\_spec.pdf](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/miniproject2_spec.pdf) | [miniproject2\\_spec.pdf](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/miniproject2_spec.pdf) |\n| [writeup.pdf](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/writeup.pdf) | [writeup.pdf](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/writeup.pdf) |\n| View all files |\n\n## Repository files navigation\n\nKaggle\\_Reddit\\_Multiclass\\_Classification by [Hair Albeiro Parra Barrera](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification) is licensed under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.](http://creativecommons.org/licenses/by-nc-nd/4.0/). Every unauthorized infraction will be legally prosecuted.\n\nBernoulli\\_NB.py by [Ashray Mallesh & Hamza Rizwan](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/scripts/Bernoulli_NB.py) is licensed under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n# Mini-Project 2: Kaggle Reddit multi-class classification\n\n- Project for implementing multi-output classification of reddit data.\n\n## Our paper:\n\n- [https://drive.google.com/file/d/1PQ20eYt0ywUCgG7qd\\_r5dn\\_DSV9kVfRk/view?usp=sharing](https://drive.google.com/file/d/1PQ20eYt0ywUCgG7qd_r5dn_DSV9kVfRk/view?usp=sharing)\n\n## Naive Bayes Formulation\n\nSee\n\n- [https://sklearn.org/modules/naive\\_bayes.html](https://sklearn.org/modules/naive_bayes.html)\n- [https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/NB.pdf](https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/NB.pdf)\n\n## Dataset labels distribution\n\n- We observe that the labels have a very well balanced distribution.\n\n## Current Best Model: Scikit-learn Multinomial Naive Bayes (Kaggle acc: 57.65,%, local cv acc: 57.10 %)\n\n- **Note:** The following confussion matrices the original training data which we split it into `X_train` (63000 samples), `X_test` (7000 samples), `y_train` (63000 samples) and `y_test`(7000) samples. Our second best model comes from the **Multinomial Naive Bayes classifier**. We display confussion matrix for it:\n\nKaggle\\_Reddit\\_Multiclass\\_Classification by [Hair Albeiro Parra Barrera](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification) is licensed under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.](http://creativecommons.org/licenses/by-nc-nd/4.0/). Every unauthorized infraction will be legally prosecuted.\n\nBernoulli\\_NB.py by [Ashray Mallesh & Hamza Rizwan](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/blob/master/scripts/Bernoulli_NB.py) is licensed under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License](http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n## About\n\nCOMP 551 Mini-Project 2: Kaggle Reddit Multi-class Classification\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/activity)\n\n### Stars\n\n[**2**\\\nstars](https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification/stargazers)\n\n### Watchers\n\n[**2**\\\nwatching](https://github.com/JairParra/Kagg...",
      "url": "https://github.com/JairParra/Kaggle_Reddit_Multiclass_Classification"
    },
    {
      "title": "GitHub - vaquierm/RedditCommentTextClassification: \ud83d\udcac Classification of Reddit comments to the subreddit they were posted in",
      "text": "[Skip to content](https://github.com/vaquierm/RedditCommentTextClassification#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n{{ message }}\n\n[vaquierm](https://github.com/vaquierm)/ **[RedditCommentTextClassification](https://github.com/vaquierm/RedditCommentTextClassification)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fvaquierm%2FRedditCommentTextClassification) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fvaquierm%2FRedditCommentTextClassification)\n- [Star\\\n0](https://github.com/login?return_to=%2Fvaquierm%2FRedditCommentTextClassification)\n\n\n\ud83d\udcac Classification of Reddit comments to the subreddit they were posted in\n\n[0\\\nstars](https://github.com/vaquierm/RedditCommentTextClassification/stargazers) [0\\\nforks](https://github.com/vaquierm/RedditCommentTextClassification/forks) [Branches](https://github.com/vaquierm/RedditCommentTextClassification/branches) [Tags](https://github.com/vaquierm/RedditCommentTextClassification/tags) [Activity](https://github.com/vaquierm/RedditCommentTextClassification/activity)\n\n[Star](https://github.com/login?return_to=%2Fvaquierm%2FRedditCommentTextClassification)\n\n[Notifications](https://github.com/login?return_to=%2Fvaquierm%2FRedditCommentTextClassification) You must be signed in to change notification settings\n\n# vaquierm/RedditCommentTextClassification\n\nmaster\n\n[Branches](https://github.com/vaquierm/RedditCommentTextClassification/branches) [Tags](https://github.com/vaquierm/RedditCommentTextClassification/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[42 Commits](https://github.com/vaquierm/RedditCommentTextClassification/commits/master/) |\n| [data](https://github.com/vaquierm/RedditCommentTextClassification/tree/master/data) | [data](https://github.com/vaquierm/RedditCommentTextClassification/tree/master/data) |  |  |\n| [results](https://github.com/vaquierm/RedditCommentTextClassification/tree/master/results) | [results](https://github.com/vaquierm/RedditCommentTextClassification/tree/master/results) |  |  |\n| [src](https://github.com/vaquierm/RedditCommentTextClassification/tree/master/src) | [src](https://github.com/vaquierm/RedditCommentTextClassification/tree/master/src) |  |  |\n| [.gitignore](https://github.com/vaquierm/RedditCommentTextClassification/blob/master/.gitignore) | [.gitignore](https://github.com/vaquierm/RedditCommentTextClassification/blob/master/.gitignore) |  |  |\n| [README.md](https://github.com/vaquierm/RedditCommentTextClassification/blob/master/README.md) | [README.md](https://github.com/vaquierm/RedditCommentTextClassification/blob/master/README.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# RedditCommentTextClassification\n\nThis repository explores an implementation of text preprocessing in order to perform a multi-class classification task on reddit comments. The goal is to predict the subreddit from which a particular reddit comment comes from. The dataset contains comments from 20 different subreddits. [(Dataset here)](https://www.kaggle.com/c/reddit-comment-classification-comp-551/data)\n\n- 'AskReddit'\n- 'GlobalOffensive'\n- 'Music'\n- 'Overwatch'\n- 'anime'\n- 'baseball'\n- 'canada'\n- 'conspiracy'\n- 'europe'\n- 'funny'\n- 'gameofthrones'\n- 'hockey'\n- 'leagueoflegends'\n- 'movies'\n- 'nba'\n- 'nfl'\n- 'soccer'\n- 'trees'\n- 'worldnews'\n- 'wow'\n\n## Best Results\n\nThe best results for this classification task were obtained with this preprocessing:\n\n- Stemmed vocabulary\n\n- Custom regex filters applied to:\n\n  - Remap all YouTube links to a token called youtubelink\n  - Remap all other links to a token called internetlink\n  - Remap different smiley faces ( ;) :) :') :-) etc... ) to a common token\n  - Remove tripple letters to remap words like (loool, or looooool to lol)\n  - Remap years such as 20xx and 19xx to common tokens\n- The Tf-Idf vectorizer to encode the data into a sparse matrix\n\n- The custom Lazy Naive Bayes model which performs feature selection at prediction time using the ANOVA F-value score of each feature.\n\n- An accuracy of 57.03% was obtained with a 5-fold cross validation\n\n\n| [![](https://github.com/vaquierm/RedditCommentTextClassification/raw/master/results/accuracies_STEM.png)](https://github.com/vaquierm/RedditCommentTextClassification/blob/master/results/accuracies_STEM.png) | [![](https://github.com/vaquierm/RedditCommentTextClassification/raw/master/results/STEM_TFIDF_LAZYNB_confusion.png)](https://github.com/vaquierm/RedditCommentTextClassification/blob/master/results/STEM_TFIDF_LAZYNB_confusion.png) |\n| --- | --- |\n\n# How to run the program!\n\n1. Download both the training data and testing data from [Kaggle](https://www.kaggle.com/c/reddit-comment-classification-comp-551/data) and place them in the `data/raw_data directory`\n\n1. Open the `src/config.py` file.\n\n- Make sure that all data paths defined at the top of the file are correct. (They should be fine without being changed)\n- In the `vocabularies_to_run` array, add which vocabularies you would like to test. (Possible values of \"LEMMA\" and \"STEM\")\n- In the `vectorizers_to_run` array, add which encodings you would like to test. (Possible values of \"BINARY\" and \"TFIDF\")\n- In the `models_to_run` array, add all the models that you want to run on all the above configurations. (Possible values of \"LAZYNB\", \"LR\", \"BNB\", \"MNNB\", \"DT\", \"RF\", \"SVM\")\n- Modify the `kaggle_vocab`, `kaggle_vectorizer`, `kaggle_model` variables to be the configuration and model you would like to run to generate results to be submitted to kaggle.\n\n1. Run the `main.py` file.\n\n- First the `create_vocabulary.py` script will be ran, which preprocesses all the initial raw data to be lemmatized and stemmed. All custom regex filters are applied as well to reduce the feature space. Once the raw data is processed, it is saved into another csv file in the `data/preprocessed_data/` folder.\n- Then the `validation_pipeline.py` script runs, which runs all different configurations and models that are defined in the `config.py` file and calculates the accuracy, confusion matrices, and saves all this data in the `results/` folder.\n- Then finally `generate_kaggle_results.py` runs which: based on the submission configuration and model defined in the `config.py` file, predicts the test data and generates a `predictions.csv` file in the `results/` folder to be submitted to Kaggle.\n\nNote: In step 3, the three top level scripts `create_vocabulary.py`, `validation_pipeline.py`, and `generate_kaggle_results.py` can also be ran individually as long as they are ran in this order.\n\n## Directory Structure\n\n```\n.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 processed_data\n\u2502   \u2502   \u251c\u2500\u2500 LEMMA_test_clean.csv\n\u2502   \u2502   \u251c\u2500\u2500 LEMMA_train_clean.csv\n\u2502   \u2502   \u251c\u2500\u2500 STEM_test_clean.csv\n\u2502   \u2502   \u2514\u2500\u2500 STEM_train_clean.csv\n\u2502   \u2514\u2500\u2500 raw_data\n\u2502       \u251c\u2500\u2500 reddit_test.csv\n\u2502       \u2514\u2500\u2500 reddit_train.csv\n|\n\u251c\u2500\u2500 results\n\u2502   \u251c\u2500\u2500 predictions.csv\n\u2502   \u251c\u2500\u2500 results.txt\n\u2502   \u2514\u2500\u2500 STEM_BINARY_DT_confusion.png\n|\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main.py\n    \u251c\u2500\u2500 config.py\n    |\n    \u251c\u2500\u2500 create_vocabularies.py\n    \u251c\u2500\u2500 validation_pipeline.py\n    \u251c\u2500\u2500 generate_kaggle_results.py\n    |\n    \u251c\u2500\u2500 Data_Analysis.ipynb\n    |\n    \u251c\u2500\u2500 data_processing\n    \u2502   \u2514\u2500\u2500 vocabulary.py\n    |\n    \u251c\u2500\u2500 models\n    \u2502   \u251c\u2500\u2500 LazyNaiveBayes.py\n    \u2502   \u251c\u2500\u2500 Model.py\n    \u2502   \u2514\u2500\u2500 BernouilliNaiveBayes.py\n    |\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 factory.py\n        \u2514\u2500\u2500 utils.py\n\n```\n\nThe `data/` folder contains all raw and parsed csv files that are used for training the model and generate predictions for the Kaggle competition.\n\n- The `data/raw_data/` contains the raw data downloaded from Kaggle containing all reddit comments. (One training file and one test file)\n- The `data/processed_data/` folder conta...",
      "url": "https://github.com/vaquierm/RedditCommentTextClassification"
    },
    {
      "title": "GitHub - InPhyT/Reddit_Text_Classification: Kaggle Challenge: User Gender Classification on Reddit using NLP.",
      "text": "[Skip to content](https://github.com/InPhyT/Reddit_Text_Classification#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/InPhyT/Reddit_Text_Classification) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/InPhyT/Reddit_Text_Classification) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/InPhyT/Reddit_Text_Classification) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[InPhyT](https://github.com/InPhyT)/ **[Reddit\\_Text\\_Classification](https://github.com/InPhyT/Reddit_Text_Classification)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FInPhyT%2FReddit_Text_Classification) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FInPhyT%2FReddit_Text_Classification)\n- [Star\\\n1](https://github.com/login?return_to=%2FInPhyT%2FReddit_Text_Classification)\n\n\nKaggle Challenge: User Gender Classification on Reddit using NLP.\n\n[inphyt.github.io/reddit\\_text\\_classification/](https://inphyt.github.io/Reddit_Text_Classification/)\n\n### License\n\n[MIT license](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/LICENSE)\n\n[1\\\nstar](https://github.com/InPhyT/Reddit_Text_Classification/stargazers) [0\\\nforks](https://github.com/InPhyT/Reddit_Text_Classification/forks) [Branches](https://github.com/InPhyT/Reddit_Text_Classification/branches) [Tags](https://github.com/InPhyT/Reddit_Text_Classification/tags) [Activity](https://github.com/InPhyT/Reddit_Text_Classification/activity)\n\n[Star](https://github.com/login?return_to=%2FInPhyT%2FReddit_Text_Classification)\n\n[Notifications](https://github.com/login?return_to=%2FInPhyT%2FReddit_Text_Classification) You must be signed in to change notification settings\n\n# InPhyT/Reddit\\_Text\\_Classification\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/InPhyT/Reddit_Text_Classification/branches) [Tags](https://github.com/InPhyT/Reddit_Text_Classification/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[9 Commits](https://github.com/InPhyT/Reddit_Text_Classification/commits/main/) |\n| [.github](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/.github) | [.github](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/.github) |  |  |\n| [.ipynb\\_checkpoints](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/.ipynb_checkpoints) | [.ipynb\\_checkpoints](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/.ipynb_checkpoints) |  |  |\n| [Notebooks](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/Notebooks) | [Notebooks](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/Notebooks) |  |  |\n| [images](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/images) | [images](https://github.com/InPhyT/Reddit_Text_Classification/tree/main/images) |  |  |\n| [.DS\\_Store](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/.DS_Store) | [.DS\\_Store](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/.DS_Store) |  |  |\n| [LICENSE](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/LICENSE) | [LICENSE](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/LICENSE) |  |  |\n| [README.md](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/README.md) | [README.md](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/README.md) |  |  |\n| [config.yml](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/config.yml) | [config.yml](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/config.yml) |  |  |\n| [index.md](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/index.md) | [index.md](https://github.com/InPhyT/Reddit_Text_Classification/blob/main/index.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n[![Size](https://camo.githubusercontent.com/8fcf8b9ae6f0dcd2e24a457549c4471047373b9c54a466eaeb0fc172391410ad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)](https://camo.githubusercontent.com/8fcf8b9ae6f0dcd2e24a457549c4471047373b9c54a466eaeb0fc172391410ad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7265706f2d73697a652f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)[![Forks](https://camo.githubusercontent.com/fb8512719cf44aa507bf794a825aa76c3a06d04325d80de56f1d38f30da2b381/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)](https://camo.githubusercontent.com/fb8512719cf44aa507bf794a825aa76c3a06d04325d80de56f1d38f30da2b381/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)[![Stars](https://camo.githubusercontent.com/3bd946ece52065c7dea214aad887e934bf5cfe032f1c7262bdae2f689b3c1d3c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)](https://camo.githubusercontent.com/3bd946ece52065c7dea214aad887e934bf5cfe032f1c7262bdae2f689b3c1d3c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)[![Languages](https://camo.githubusercontent.com/b25562ab40b4cf61c6a179791de6dbc581de9a8ce8bbb716deca892bfdd96316/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f636f756e742f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)](https://camo.githubusercontent.com/b25562ab40b4cf61c6a179791de6dbc581de9a8ce8bbb716deca892bfdd96316/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f636f756e742f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)[![Licence](https://camo.githubusercontent.com/f843dd03aac8aa62d5effc480074a69ebe6fd0b3f961734408d78937fd15af2e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f496e506879542f5265646469745f546578745f436c617373696669636174696f6e)](https://github.com/InPhyT/Reddit_Text_Classification/graphs/contributors)[![Twitter](https://camo.githubusercontent.com/be295f8de1e7d15c6c4a54066770c39de24a79e7c79e8ae2078c375833fbe770/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c3f75726c3d68747470732533412532462532466769746875622e636f6d253246496e506879542532465265646469745f546578745f436c617373696669636174696f6e)](https://camo.githubusercontent.com/be295f8de1e7d15c6c4a54066770c39de24a79e7c79e8ae2078c375833fbe770/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c3f75726c3d68747470732533412532462532466769746875622e636f6d253246496e506879542532465265646469745f546578745f436c617373696669636174696f6e)\n\n# Reddit Text Classification\n\n### User Gender Classification on Reddit using NLP\n\n[![Kaggle](https://camo.githubusercontent.com/1398db766d8ca60e6f296aac9ac429e344705c6c2c7e1ceb024230aac69fd6be/68747470733a2f2f6b6167676c652e636f6d2f7374617469632f696d616765732f6f70656e2d696e2d6b6167676c652e737667)](https://www.kaggle.com/inphyt2020/dataminingchallange)[![nbviewer](https://github.com/jupyter/design/raw/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/InPhyT/Reddit_Text_Classification/)[![Colab](https://camo.githubusercontent.com/f5e0d0538a9c2972b5d413e0ace04cecd8efd828d133133933dfffec282a4e1b/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/InPhyT/DataMiningChallange/blob/master)\n\n## How to Explore this Work\n\n- Read the code in the [Jupyter notebooks](https://nbviewer.jupyter.org/github/InPhyT/...",
      "url": "https://github.com/InPhyT/Reddit_Text_Classification"
    }
  ]
}