{
  "query": "How do top Kagglers optimize AUC-ROC specifically, and what techniques work best for probabilistic predictions in binary classification?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kagglers boost AUC\u2011ROC by treating the problem as a ranking task and by squeezing every bit of signal out of the data before they blend many calibrated probability models\u202f([numberanalytics.com](https://numberanalytics.com/blog/10-advanced-techniques-optimize-auc), [cis.jhu.edu](https://www.cis.jhu.edu/~xye/papers_and_ppts/ppts/TACOS24.pdf.pdf), [medium.com](https://medium.com/data-science/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7)).\n\n**Step\u202f1 \u2013 Aggressive feature creation**  \nGenerate rich numeric features (target\u2011encoded categorical variables, interaction terms, statistical aggregates, time\u2011based summaries) to give the model more discriminative power\u202f([cis.jhu.edu](https://www.cis.jhu.edu/~xye/papers_and_ppts/ppts/TACOS24.pdf.pdf)).\n\n**Step\u202f2 \u2013 Choose high\u2011capacity, probability\u2011output models and tune for AUC**  \nUse gradient\u2011boosting libraries (LightGBM, XGBoost, CatBoost) or deep\u2011learning classifiers that natively produce probabilities. Optimize hyper\u2011parameters (learning rate, depth, regularisation) with AUC\u2011ROC as the validation metric and employ early\u2011stopping on AUC\u202f([numberanalytics.com](https://numberanalytics.com/blog/10-advanced-techniques-optimize-auc)).\n\n**Step\u202f3 \u2013 Robust cross\u2011validation and class\u2011aware loss**  \nApply stratified K\u2011fold (or group\u2011aware) CV to preserve the positive\u2011negative ratio in each fold. When the target is highly imbalanced, set class weights or use a balanced version of the loss so that the ranking signal is not dominated by the majority class\u202f([cis.jhu.edu](https://www.cis.jhu.edu/~xye/papers_and_ppts/ppts/TACOS24.pdf.pdf)).\n\n**Step\u202f4 \u2013 Direct AUC\u2011oriented training objectives**  \nReplace plain log\u2011loss with pair\u2011wise ranking losses (e.g., LambdaRank, RankNet) or AUC\u2011surrogate losses that are proven to be consistent with AUC\u2011ROC\u202f([numberanalytics.com](https://numberanalytics.com/blog/10-advanced-techniques-optimize-auc)).\n\n**Step\u202f5 \u2013 Blend / stack multiple calibrated models**  \nTrain several diverse learners (different algorithms, feature sets, random seeds) and combine them via weighted averaging or a meta\u2011learner (stacking). Blending usually yields a smoother ranking and higher AUC than any single model\u202f([cis.jhu.edu](https://www.cis.jhu.edu/~xye/papers_and_ppts/ppts/TACOS24.pdf.pdf)).\n\n**Step\u202f6 \u2013 Post\u2011process and calibrate probabilities**  \nAfter blending, apply probability calibration (Platt scaling or isotonic regression) and optional smoothing (e.g., clipping extreme values) to ensure the final scores are well\u2011behaved probabilities, which further improves the AUC\u2011ROC on the leaderboard\u202f([numberanalytics.com](https://numberanalytics.com/blog/10-advanced-techniques-optimize-auc)).",
      "url": ""
    },
    {
      "title": "11 Evaluation Metrics Data Scientists Should Be Familiar ... - Medium",
      "text": "11 Evaluation Metrics Data Scientists Should Be Familiar with\u2014 Lessons from A High-rank Kagglers\u2019 New Book | by Moto DEI | TDS Archive | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/data-science/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/data-science/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## TDS Archive\n](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-8596f75e58a7---------------------------------------)\n\u00b7[\n![TDS Archive](https://miro.medium.com/v2/resize:fill:76:76/1*JEuS4KBdakUcjg9sC7Wo4A.png)\n](https://medium.com/data-science?source=post_page---post_publication_sidebar-7f60cf5620c9-8596f75e58a7---------------------------------------)\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n## Book \u201cData Analysis Techniques to Win Kaggle\u201d\n# 11 Evaluation Metrics Data Scientists Should Be Familiar with\u2014 Lessons from A High-rank Kagglers\u2019 New Book\n## It is about loss function, right? No, it isn\u2019t.\n[\n![Moto DEI](https://miro.medium.com/v2/resize:fill:64:64/2*HH8fBXznwSxoz6rx4QcNWg.jpeg)\n](https://medium.com/@daydreamersjp?source=post_page---byline--8596f75e58a7---------------------------------------)\n[Moto DEI](https://medium.com/@daydreamersjp?source=post_page---byline--8596f75e58a7---------------------------------------)\n9 min read\n\u00b7Nov 29, 2019\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/data-science/8596f75e58a7&amp;operation=register&amp;redirect=https://medium.com/data-science/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7&amp;user=Moto+DEI&amp;userId=ecff78021d2d&amp;source=---header_actions--8596f75e58a7---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/8596f75e58a7&amp;operation=register&amp;redirect=https://medium.com/data-science/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7&amp;source=---header_actions--8596f75e58a7---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[Andreas Wagner](https://unsplash.com/@waguluz_?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\nThis is a post to pick up tips introduced in a new book*\u201cData Analysis Techniques to Win Kaggle\u201d*, authored by three high-rank Kagglers (not including myself thus this is not a personal promotion! :) )\nFor the full table of contents of this book, see my[other post](https://medium.com/@daydreamersjp/a-new-book-data-analysis-techniques-to-win-kaggle-is-a-current-best-and-complete-for-table-data-4af66a88388).\n## Table of Contents:\n### [Evaluation Metric vs. Loss Function](#ad3e)\n### [Evaluation Metrics Used in Regression Task](#96c6)\n[#1 \u2014RMSE (Root Mean Squared Error)](#a94a)\n[#2 \u2014RMSLE (Root Mean Squared Logarithmic Error)](#5185)\n[#3 \u2014MAE (Mean Absolute Error)](#2b4e)\n[#4 \u2014R-Squared (R\u00b2)](#ad72)\n### [Evaluation Metrics Used in 0/1 Prediction in Binary Classification Task](#89e8)\n[#5 \u2014Accuracy and Error Rate](#c637)\n[#6 \u2014Precision and Recall](#a2e7)\n[#7 \u2014F1-Score and Fbeta-Score](#0213)\n[#8 \u2014MCC (Matthews Correlation Coefficient)](#9e24)\n[#9 \u2014Balanced Accuracy](#96e3)\n[#10 \u2014Log Loss (or Cross Entropy or Negative Log-Likelihood)](#e1bc)\n[#11 \u2014AUCROC (Area Under the Receiver Operating Characteristic Curve)](#bd6b)\n## Evaluation Metric vs. Loss Function\n*Evaluation metric*, a theme of this post, is a somewhat confusing concept for ML beginners with another related but separate concept,*loss function*. They are similar in a sense they could be the same when we are lucky enough, but it will not happen every time.\n*> Evaluation metric *> is a metric \u201cwe want\u201d to minimize or maximize through the modeling process, while *> loss function\n*> is a metric \u201cthe model will\u201d minimize through the model training.\nPress enter or click to view image in full size\n![]()\nPhoto by[AbsolutVision](https://unsplash.com/@freegraphictoday?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\nGiving an example in simple logistic regression:\n* **Loss function**is the quantity which the model will minimize over the training. It is also called as cost function or objective function. Very basic version of logistic regression uses[negative log likelihood](#e1bc)as loss function. Searching the parameters of the model to minimize the negative log likelihood is something which is done in training the model.\n* **Evaluation metric**is the metric we want the model to maximize. It is separate from the model training process, and*ideally evaluation metric should reflect our business needs*. Depending on business application, we may want to maximize[AUC](#bd6b), or we may care how good the[recall](#a2e7)is.\nLoss function is tightly linked to the model and typically a model has restrictive list of loss function candidates \u2014e.g. [*negative log likelihood, negative log likelihood with penalty term(s)]*for logistic regression \u2014, because the selection of loss function is the part of core determination by model algorithm creator.\nOn the other hand, the evaluation metric can be whatever we want to set. Ultimately we can use \u20181\u2019 for any model, though using universal \u20181\u2019 for evaluation metric never makes sense. How high the evaluation metric is usually measured by the data not used in training such as out-of-fold data or test data.\nEvaluation score is widely used in the hyper parameter tuning, but for more experienced data science people, one of the most useful cases to understand the distinction of loss function and evaluation metric is**early stopping**. Early stopping is a technique typically used to determine when to stop training in order to avoid overfitting in boosting type of models or neural network type of models.\n> [Early Stopping]\n> While the model is tuning the parameter based on minimization of\n**> loss function\n**> , check how much one iteration improve the **> evaluation metric\n**> and if not a lot improvement any more, stop learning.\nIn Kaggle, the competition participants are ranked by \u201cleaderboard score\u201d. There are two types of leaderboard scores, public and private, but it\u2019s another story. Leaderboard score can be categorized to an evaluation metric the competition host sets to meet their business goal or needs.\n> Then, understanding evaluations score and how to maximize them through the model training should be a road to win the Kaggle competition.\nSo, now you understand the difference of evaluation metric and loss function. Next, we ...",
      "url": "https://medium.com/data-science/11-evaluation-metrics-data-scientists-should-be-familiar-with-lessons-from-a-high-rank-kagglers-8596f75e58a7"
    },
    {
      "title": "10 Advanced Techniques to Optimize Area Under the ROC Curve",
      "text": "# 10 Advanced Techniques to Optimize Area Under the ROC Curve\n\n10 Advanced Techniques to Optimize Area Under the ROC Curve\n\nSarah Lee\nAI generatedo3-mini 0 min read\u00b7 March 18, 2025\n\n502 views\n\nThe performance of your machine learning model is often judged not just by accuracy, but by how well it distinguishes between classes. One of the most popular and robust evaluation metrics is the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). In this blog article, we explore ten advanced techniques that can help you optimize your model\u2019s AUC, ensuring your model is both discriminative and reliable. We will cover everything from data preprocessing to model calibration, algorithm tuning, and even real-world case studies to give your models that extra edge.\n\n## Introduction\n\nThe Area Under the ROC Curve (AUC) is a critical metric in assessing the ability of a classification algorithm to distinguish between different classes. Optimizing this metric is essential when the cost of misclassification is high. Here are some key points to consider:\n\n- **Importance of AUC in Advanced Modeling:** AUC provides a comprehensive evaluation by measuring how well the model ranks positive observations higher than negatives, even when the classification threshold varies.\n- **Overview of Optimization Strategies:** Each technique in this article is designed to address a specific aspect of the model-building process, ensuring that you consider every angle from data preprocessing to post-processing.\n- **Setting Performance Improvement Goals:** Before implementing any strategies, define your performance metrics and improvement thresholds. Consider questions like: Is a 0.05 increase in AUC significant for your application? How do you balance between precision and recall?\n\nRemember that the ROC curve plots the true positive rate (TPR) against the false positive rate (FPR). The mathematical representation can be written as:\n\nAUC=\u222b01TPR(t)dFPR(t)\n\\\\text{AUC} = \\\\int\\_{0}^{1} \\\\text{TPR}(t) , d\\\\text{FPR}(t)\nAUC=\u222b01\u200bTPR(t)dFPR(t)\n\nThis formula signifies the cumulative performance of your classifier across all decision thresholds.\n\nIn this article, we will walk through each advanced technique step by step, providing practical examples and mathematical insights to help you optimize your AUC.\n\n## Technique 1: Improving Data Preprocessing\n\nData preprocessing is a vital step in machine learning. Poor data quality can obscure the true signal in your data, reducing the effectiveness of your model. Here are essential aspects of data preprocessing:\n\n- **Data Cleaning and Normalization:**\nEnsure that your data is free from noise and errors. Remove outliers, correct anomalies, and standardize your data to have zero mean and unit variance. Normalized data reduces the chance that features with larger scales will dominate the AUC score optimization.\n\n- **Feature Scaling Impact on ROC:**\nScaling techniques like z-score standardization or min-max scaling can dramatically affect model performance. For instance, the min-max transformation is defined as:\n\nx\u2032=x\u2212xminxmax\u2212xmin\nx' = \\\\frac{x - x\\_{\\\\text{min}}}{x\\_{\\\\text{max}} - x\\_{\\\\text{min}}}\nx\u2032=xmax\u200b\u2212xmin\u200bx\u2212xmin\u200b\u200b\n\nThis ensures all features are on a similar scale, preventing any unintentional weighting.\n\n- **Managing Missing Values:**\nHow you handle missing data is crucial. Imputation methods such as mean substitution, k-nearest neighbors (KNN), or advanced methods like multiple imputation can all improve model accuracy and hence its AUC.\n\n\nBy ensuring a robust preprocessing pipeline, the model has accurate and normalized data, which ultimately leads to a better AUC score.\n\n## Technique 2: Model Calibration\n\nEven when your model outputs clear predictions, the estimated probabilities might not be well-calibrated. This miscalibration can impact the ROC as the line between classes becomes blurred.\n\n- **Overview of Calibration Curves:**\nCalibration curves allow you to visualize and adjust the differences between predicted probabilities and actual outcomes. Ideally, the predicted probability should closely match the observed frequency. A well-calibrated model will have its calibration plot near the diagonal line represented by:\n\nIdeal\u00a0calibration:\u00a0y=x\n\\\\text{Ideal calibration: } y = x\nIdeal\u00a0calibration:\u00a0y=x\n\n- **Calibrating Probability Outputs:**\nTechniques like Platt scaling and isotonic regression are common methodologies to achieve proper calibration. These methods can adjust the model\u2019s output so that the predicted probabilities become more reflective of actual likelihoods.\n\n- **Effect on AUC Scores:**\nWhile calibration does not directly improve the AUC, it improves the interpretation of probabilities, leading to more reliable decision thresholds and, consequently, better practical model performance.\n\n\n## Technique 3: Algorithm Tuning\n\nChoosing the right algorithm is only part of the challenge; fine-tuning the parameters (or hyperparameters) can have a substantial impact on your model\u2019s performance.\n\n- **Selecting the Right Algorithm:**\nEvaluate various algorithms like logistic regression, decision trees, gradient boosting, and support vector machines. Each algorithm comes with its strengths and weaknesses depending on your data structure and application domain.\n\n- **Parameter Tuning for Performance Boost:**\nEmploy grid search or random search techniques to find the optimal hyperparameters. Techniques like Bayesian optimization can further refine this process. For example, when tuning a gradient boosting machine, parameters like learning rate, maximum depth, and subsample ratio are key.\n\n- **Experimentation and Validation:**\nMake sure to validate your tuning using cross-validation or bootstrapping methods. This prevents overfitting and provides a more generalized increase in AUC.\n\n\nCarefully tuned algorithms not only perform better in specific subsets of the data but also offer robust and consistent performance across different validation scenarios.\n\n## Technique 4: Advanced Feature Engineering\n\nFeature engineering is the art of extracting more information from your dataset. New or transformed features can enhance the discriminative power of the model.\n\n- **Creating and Selecting New Features:**\nUse domain-specific knowledge or automated methods such as principal component analysis (PCA) and factor analysis to create new features. Derived features often capture nuances in the data that raw features do not.\n\n- **Feature Extraction Methods:**\nTechniques such as PCA, t-SNE, or autoencoders can be used to reduce dimensionality while preserving the most important variance. The effectiveness of PCA can be measured by the explained variance ratio, which serves as a guide for how many principal components to retain.\n\n- **Impact on Model Discrimination:**\nImproved feature sets allow the model to better differentiate between classes. As a result, the ROC curve becomes steeper \u2013 a clear indicator that maximizing the TPR for lower values of FPR is achievable.\n\n\nAdvanced feature engineering often leads to a transformation of the input space where non-linearly separable data becomes linearly separable, thereby boosting the AUC.\n\n## Technique 5: Handling Imbalanced Datasets\n\nClass imbalance is a common problem where one class significantly outnumbers the other. This imbalance can lead to biased predictions and a distorted ROC curve.\n\n- **Resampling Techniques and Synthetic Data:**\nMethods like oversampling the minority class (using techniques such as SMOTE) or undersampling the majority class can rebalance your dataset.\n\n- **Adjusting Class Weights:**\nMany algorithms allow you to assign higher weights to minority classes to penalize misclassifications more severely. This adjustment forces the model to be more sensitive to underrepresented classes.\n\n- **Improving Minority Class Detection:**\nEffective handling of imbalanced datasets leads to improved discrimination for the minority class\u2014often the most critical in applications like fraud detection or dise...",
      "url": "https://numberanalytics.com/blog/10-advanced-techniques-optimize-auc"
    },
    {
      "title": "Maximize AUC in Default Prediction: Modeling and Blending",
      "text": "Maximize AUC in Default Prediction: Modeling and Blending\nLiang Sun lsun@operasolutions.com\nTomonori Honda thonda@operasolutions.com\nVesselin Diev, Gregory Gancarz, Jeong-Yoon Lee, Ying Liu, Mona Mahmoudi, Raghav Mathur,\nShahin Rahman, Steve Wickert, Xugang Ye, Hang Zhang\nAbstract\nIn this paper we present models and blending\nalgorithms to maxmize the Area Under Re\u0002ceiver Operating Characteristic (ROC) curve\nin default prediction. We summarize all tech\u0002niques and algorithms we have applied in the\nGive Me Some Credit competition so that fu\u0002ture users can benefit from our experiences in\nthis competition, including feature creation\nalgorithms, single model constructions and\nblending algorithms. In particular, in this pa\u0002per we highlight the following aspects: (i) dif\u0002ferent feature creation methods we explored,\n(ii) diverse packages utilized for single model\nbuilding and the parameter tuning for each\nmodel, (iii) different blending, and (iv) addi\u0002tional postprocessing methods.\n1. Introduction\nThe recent Give Me Some Credit (GMSC ) competi\u0002tion1 organized by Kaggle focused on predicting the\nprobability that some bank customers would experi\u0002ence financial distress in the next two years. This is a\nvery interesting classification problem having limited\n(11) original features and low target rate ( 5%) which\nrepresents customers who actually default. The ap\u0002proaches developed for this competition are applicable\nnot only to the credit & risk community, but also to\nother industries with similar classification problems.\nIn this paper, we summarize all techniques and algo\u0002rithms we applied in the GMSC competition, includ\u0002ing feature creation, single models and blending mod\u0002els. In particular, we would like to highlight (i) dif\u0002ferent feature creation methods, (ii) diverse packages\n1http://www.kaggle.com/c/GiveMeSomeCredit\nAppearing in Proceedings of the 1 st Technical and Analyt\u0002ical Conference of Opera Solutions, San Diego, USA, 2012.\nand algorithms utilized for single model building, (iii)\ndifferent blending, and (iv) additional postprocessing\nof these models.\nDuring the GMSC competition, several data sets were\ncreated. One of the main differences between these\ndata sets was the handling of missing values and out\u0002liers. Additionally, many data sets transformed orig\u0002inal features into weight of evidences. In particular,\na novel effective variable 2D weight of evidence was\ncreated to capture the underlying information. Based\non the created data sets, many different single models\nwere developed; e.g., random forest, gradient boost\u0002ing decision tree, alternating decision tree, logistic re\u0002gression, artificial neural networks, support vector ma\u0002chine, elastic net, and k-nearest neighbors. In addi\u0002tion, we applied residual postprocessing to improve the\npeformance of single models. For example, the resid\u0002ual of the Gradient Boosted Machine (GBM) model\nwas successfully modeled using a random forest model\nand improved performance was achieved.\nMany different blending algorithms were explored dur\u0002ing the GMSC competition. The transformation\nwhich converts the probability of default into a rank\ntends to improve the performance of blending algo\u0002rithms when the evaluation criterion is AUC. We have\ndesigned a class of statistical aggregation algorithms to\ncombine a set of predictions together. In order to effec\u0002tively utilize the public leaderboard score for each in\u0002dividual prediction, we further propose the rank-based\noracle blending. Compared with the statistical aggre\u0002gation algorithms, the weight of each single model is a\nfunction of its public AUC score so that better models\nhave larger weights in rank-based oracle blending.\nNote that blending is typically the last step in creating\nfinal predictions. However, we can further utilize the\nblending result to improve the performance of single\nmodels and the overall performance. One approach\nwe followed was to use predictions from blending as\nnew targets for single models. This can help remove\ntarget outliers and improve performance. Another ap\u0002proach we followed was to determine the problematic\nMaximize AUC in Default Prediction: Modeling and Blending\npopulation (that segment of data which does not rank\norder well) using the blended results and build sep\u0002arate models for this problematic population. Note\nthat the original blended prediction can be a new key\nfeature for this problematic segment. It works best\nwhen we build many different single models again for\nthis problematic segment. A key requirement to im\u0002plement this approach is that predictions on training,\nvalidation and test data sets should be available for\neach single model.\n2. Evaluation Criterion: Area Under\nROC Curve\nThe evaluation criterion for this GMSC competition\nwas the Area Under ROC Curve (AUC). The Receiver\nOperating Characteristic (ROC) curve of a decision\nfunction f plots the true positive rate on the y-axis\nagainst the false postive rate on the x-axis. Thus the\nROC curve characterizes every possible trade-off be\u0002tween false positives and true positives that can be\nachieved by comparing f(x) to a threshold. Note that\nthe ROC curve is a 2-dimensional measure of classifi\u0002cation performance, and the AUC is a scalar measure\nto summarize the ROC curve. Formally, the AUC is\nequal to the probability that the decision function f\nassigns a higher value to a randomly drawn positive\nexample x\n+ than to a randomly drawn negative ex\u0002ample x\n\u2212, i.e.,\nAUC(f) = P r(f(x\n+) > f(x\u2212)). (1)\nTheoretically, the AUC refers to the true distribution\nof positive and negative instances, but it is usually es\u0002timated from samples. It can be shown that the nor\u0002malized Wilconxon-Mann-Whitney statistic gives the\nmaximum likelihood estimate of the true AUC given\nn\n+ positive and n\u2212 negative examples (Yan et al.,\n2003):\nAUC\u02c6 (f) =\nPn\n+\ni=1\nPn\n\u2212\nj=1 1f(x\n+\ni\n)>f(x\n\u2212\nj\n)\nn+n\u2212\n, (2)\nwhere 1fi>fjis the indictator function which is 1 if\nfi > fj and 0 otherwise. In fact, the two sums in\nEq. (2) iterate all pairs of positive and negative ex\u0002amples. Each pair that satisfies f(x\n+) > f(x\u2212) con\u0002tributes 1/(n\n+n\u2212) to the overall AUC estimate. As a\nresult, maximizing AUC is equivalent to maximizing\nthe number of pairs satisfying f(x\n+) > f(x\u2212). Note\nthat the number of all pairs is O(n\n2\n) where n is the\nsample size in the training data set. We will discuss\nbelow how to focus attention on the problematic pop\u0002ulation, which happens to be the top 20% people with\nhighest blended scores for the GMSC Competition.\nThe AUC can also be calculated in a few different\nways, e.g., numerically integrating the ROC curve.\nAnother alternative to compute it is via the Wilcoxon\nrank sum test as follows:\nAUC\u02c6 (f) = U1 \u2212\nn\n+(n++1)\n2\nn+n\u2212\n(3)\nwhere U1 is the sum of ranks of members in the pos\u0002itive class (or members who defaulted). This AUC\nequation helps us to blend models in the ranking space\nrather than the probability space.\n3. Data Description and Feature\nCreation\n3.1. Raw Data Description\nThere were 10 raw feature variables and 1 target\nvariable. These raw feature variables were the\nfollowing: RevolvingUtilizationOfUnsecuredLines,\nAge, NumberOfTime30-59DaysPastDueNotWorse,\nDebtRatio, MonthlyIncome, NumberOfOpenCred\u0002itLinesAndLoans, NumberOfTimes90DaysLate,\nNumberRealEstateLoansOrLines, NumberOfTime60-\n89DaysPastDueNotWorse, NumberOfDependents.\nThe meaning of these variables should be self\u0002explanatory. Note that there were some missing\nvalues in the variables DebtRatio and MonthlyIn\u0002come. Also there were some obvious outliers in a few\nvariables.\n3.2. Data Imputation and Feature Creation\nDuring the GMSC competition, at least 12 different\ndata sets were created and the main differences were\nin the handling of the missing values and outliers. In\nsome data sets, new derived features such as the weight\nof evidence were also created. In this subsection, we\nfocus on the imputation of missing values and the new\nvariable 2D weight of evidence.\nIn a few data sets that utilized binning, missing val\u0002ues were assigned separat...",
      "url": "https://www.cis.jhu.edu/~xye/papers_and_ppts/ppts/TACOS24.pdf.pdf"
    },
    {
      "title": "[Tutorial] - ROC & AUC clearly explained - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/jacoporepossi/tutorial-roc-auc-clearly-explained"
    },
    {
      "title": "Prediction with 7-Classification Models | ROC AUC - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/mechatronixs/prediction-with-7-classification-models-roc-auc"
    },
    {
      "title": "Kaggle Titanic Competition Part X - ROC Curves and AUC",
      "text": "[Skip to content](https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-x-roc-curves-and-auc/#content)\n\n# Kaggle Titanic Competition Part X \u2013 ROC Curves and AUC\n\nKaggle Titanic Competition Part X \u2013 ROC Curves and AUC\n\nIn the last post, we looked at how to generate and interpret learning curves to validate how well our model is performing. Today we\u2019ll take a look at another popular diagnostic used to figure out how well our model is performing.\n\nThe Receiver Operating Characteristic (ROC curve) is a chart that illustrates how the true positive rate and false positive rate of a binary classifier vary as the discrimination threshold changes. Did that make any sense? Probably not, hopefully it will by the time we\u2019re finished. An important thing to keep in mind is that ROC is all about **confidence**!\n\n[![titanic_roc_confidence](https://www.ultravioletanalytics.com/wp-content/uploads/2014/12/titanic_roc_confidence.jpg)](https://www.ultravioletanalytics.com/wp-content/uploads/2014/12/titanic_roc_confidence.jpg)\n\nNo. Not that confidence. Although\u2026 we are aiming to find out if we\u2019re good enough and smart enough.\n\nLet\u2019s start with a binary classifier. It\u2019s entire aim is to determine which of two possible classes an example belongs to. We can arbitrarily label one as False and one as True. Sometimes it will be easy to determine which is which, but sometimes the classes are qualitative like \u201cblue\u201d or \u201cred\u201d and then you might think about it as True == Blue and False == Not Blue. At any rate, let\u2019s just think about true and false.\n\nThe classifier will (hopefully) identify some examples as true and some as false. Some of these predicted true examples actually are true. The percentage of known true examples that the model identifies as true is the True Positive Rate (TPR). Similarly, the False Positive Rate (FPR) is the percentage of known false examples that the model identifies as true. We clearly want TPR to be 1.0 and FPR to be 0.0, as that would be a perfect classifier. We can build a very simple chart called a confusion matrix (which is also used in cases with more than 2 classes) which is a summary table for the actual values compared to the predicted values. The wikipedia page has several great versions of this chart so look there (link at the end of the post) but here\u2019s another one of my fantastic illustrations:\n\n[![titanic_confusion_matrix](https://www.ultravioletanalytics.com/wp-content/uploads/2014/12/titanic_confusion_matrix.jpg)](https://www.ultravioletanalytics.com/wp-content/uploads/2014/12/titanic_confusion_matrix.jpg)\n\nThis confusion matrix shows the TPR and FPR for the model output. This output is generated by scikit-learn\u2019s RandomForestClassifier which only outputs 1\u2019s and 0\u2019s. Technically, you can\u2019t use this for a ROC curve, because there is no concept of confidence in the Classifier output \u2013 it\u2019s either a 1 or 0. But, we CAN use the probability scores for each prediction which is a slightly different function in the RandomForestClassifier. A score of .5 basically is a coin-flip, the model really can\u2019t tell at all what the classification is. When determining predictions, a score of .5 represents the decision boundary for the two classes output by the RandomForest \u2013 under .5 is 0, .5 or greater is 1. To generate the ROC curve, we generate the TPR and FPR values using different decision boundaries, and plot each point TPR/FPR as we go.\n\nWhat does this tell us? It doesn\u2019t tell us how accurate the model is, we only need to know the classification error for that. By moving the decision boundary and testing along the way, we discover how accurate the model\u2019s confidence is. For example: Let\u2019s say there\u2019s a model that any time it scores an example less than .4, or any time it scores above .6, it is correct. In between the two the model makes lots of mistakes though, but whenever it has strong predictions (or even medium ones) it\u2019s always correct. Perhaps there\u2019s another model that is only perfectly correct below .2 and above .8, but in between the two it makes mistakes uniformly at a low rate. These two models may have the exact same classification error, but we know that the first model is more \u201ctrustworthy\u201d for the clear-cut cases, and the second is more trustworthy for the more vague cases.\n\nAdditionally, we can use the ROC curve to adjust the model output based on business decisions. Let\u2019s say a False Positive doesn\u2019t have much real-world consequences but a False Negative is VERY costly. We can use the ROC curve to adjust the decision boundary in a way that may reduce the overall accuracy of the model, but may be beneficial for the organization\u2019s overall objective.\n\nWe can compare the curves of two different models (wait, what are we talking about here?) to get an idea of which model is more trustworthy. A single, objective number can tell us definitively which model is more trustworthy: AUC. It\u2019s as simple as it sounds, just calculate the area under the curve and the value will be between 0 and 1. So what does AUC mean mathematically? Technically, it\u2019s the probability that a randomly selected positive example will be scored higher by the classifier than a randomly selected negative example. If you\u2019ve got two models with nearly identical overall accuracy, but one has a higher AUC\u2026 it\u2019s may be best to go with the higher AUC.\n\nCode to generate the plot in scikit-learn and matplotlib:\n\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.cross_validation import train_test_split\n\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\nforest.fit(X_train, y_train)\n\n# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(y_test, forest.predict_proba(X_test)[:,1])\n\n# Calculate the AUC\nroc_auc = auc(fpr, tpr)\nprint 'ROC AUC: %0.2f' % roc_auc\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\nand it will look something like this:\n\n[![titanic_roc_curve](https://www.ultravioletanalytics.com/wp-content/uploads/2014/12/titanic_roc_curve.jpg)](https://www.ultravioletanalytics.com/wp-content/uploads/2014/12/titanic_roc_curve.jpg)\n\nSo that\u2019s about the end of this series. In the last post, we\u2019re going to wrap this whole thing up!\n\nMore on ROC Curves and AUC:\n\n- [(Wikipedia) Receiver Operating Characteristic](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n- [(Scikit-learn) Receiver Operating Characteristic](http://scikit-learn.org/stable/auto_examples/plot_roc.html)\n- [More on ROC/AUC](http://www.win-vector.com/blog/2013/01/more-on-rocauc/)\n- [ROC curves and Area Under the Curve explained (video)](http://www.dataschool.io/roc-curves-and-auc-explained/)\n\n* * *\n\n### Kaggle Titanic Tutorial in Scikit-learn\n\n[Part I \u2013 Intro](https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-i-intro/)\n\n[Part II \u2013 Missing Values](https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-ii-missing-values/)\n\n[Part III \u2013 Feature Engineering: Variable Transformations](https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-iii-variable-transformations/)\n\n[Part IV \u2013 Feature Engineering: Derived Variables](https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-iv-derived-variables/)\n\n[Part V \u2013 Feature Engineering: Interaction Variables and Correlation](https://www.ultravioletanalytics.com/blog/kaggle-titantic-competition-part-v-interaction-variables/)\n\n[Part VI \u2013 Feature Engineering: Dimensionality Reduction w/ PCA](https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-vi-dimensionality-reduction/)\n\n[Part VII \u2013 Modeling: Random Forests and Feature Importance](https:...",
      "url": "https://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-x-roc-curves-and-auc"
    },
    {
      "title": "AUC ROC metric on a Kaggle competition",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [AUC ROC metric on a Kaggle competition](https://datascience.stackexchange.com/questions/68841/auc-roc-metric-on-a-kaggle-competition)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years, 3 months ago\n\nModified [4 years, 3 months ago](https://datascience.stackexchange.com/questions/68841/auc-roc-metric-on-a-kaggle-competition?lastactivity)\n\nViewed\n441 times\n\n4\n\n$\\\\begingroup$\n\nI am trying to learn data modeling by working on a dataset from [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview/evaluation). As the competition was closed 2 years back, I am asking my question here. The competition uses AUC-ROC as the evaluation metric. This is a classification problem with 5 labels. I am modeling it as 5 independent binary classification problems. Interestingly, the data is highly imbalanced across labels. In one case, there is an imbalance of 333:1. I did some research into interpreting the AUC-ROC metric. During my research, I found [this](https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc) and [this](https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve). Both these articles basically say that AUC-ROC is not a good metric for an imbalanced data set. So, I am wondering why would they be using this metric to evaluate models in the competition? Is it even a reasonable metric in such a context? If yes, why?\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [class-imbalance](https://datascience.stackexchange.com/questions/tagged/class-imbalance)\n- [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)\n\n[Share](https://datascience.stackexchange.com/q/68841)\n\n[Improve this question](https://datascience.stackexchange.com/posts/68841/edit)\n\nFollow\n\nasked Feb 28, 2020 at 0:37\n\n[![Sahil Gupta's user avatar](https://www.gravatar.com/avatar/0716330e93bb6f09ee12892398c20c52?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/62630/sahil-gupta)\n\n[Sahil Gupta](https://datascience.stackexchange.com/users/62630/sahil-gupta) Sahil Gupta\n\n7544 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/68841/auc-roc-metric-on-a-kaggle-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/68841/auc-roc-metric-on-a-kaggle-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n6\n\n$\\\\begingroup$\n\nAs you would have seen in the research, AUC ROC prioritizes getting the _order_ of the predictions correct, rather than approximating the true frequencies.\n\nUsually, like in the credit card fraud problem you link to, the impact of one or two false negative is more devastating that many false positives. If those classes are imbalanced, like they are in the fraud case, AUC ROC is a bad idea.\n\nIt appears that in the competition you are referring to, the hosts are more interested in labeling which comments are more toxic than others rather than rating _how toxic_ they each are. This makes sense since in reality the labels are subjective.\n\n[Share](https://datascience.stackexchange.com/a/68842)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/68842/edit)\n\nFollow\n\nanswered Feb 28, 2020 at 0:55\n\n[![nigelhenry's user avatar](https://www.gravatar.com/avatar/caf52a7431824cbc5b133c6c71918738?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/68939/nigelhenry)\n\n[nigelhenry](https://datascience.stackexchange.com/users/68939/nigelhenry) nigelhenry\n\n25622 silver badges33 bronze badges\n\n$\\\\endgroup$\n\n4\n\n- $\\\\begingroup$Your answer makes a lot of sense. I guess you pointing at the business intuition behind the technical problem at hand to choose an appropriate metric. I am still confused about why AUC ROC is a bad idea for the credit card problem. For instance, we are interested in minimizing the no. of false negatives. ROC is the plot of TPR v/s FPR. The ideal point (1,0) signifies FN = 0 at the cost of high FP. Then why is AUC a bad idea? Also, can you elaborate on what you mean by \"more interested in labeling which comments are more toxic than others rather than rating how toxic they each are\".$\\\\endgroup$\n\n\u2013\u00a0[Sahil Gupta](https://datascience.stackexchange.com/users/62630/sahil-gupta)\n\nCommentedFeb 28, 2020 at 3:27\n\n- 2\n\n\n\n\n\n$\\\\begingroup$@SahilGupta, I think the blog post you linked to gives a clear answer to your two questions: 1. AUC ROC is a bad idea for that problem since \"false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives\". and 2. \"You should use \\[AUC ROC\\] when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilities\". Perhaps your question is really: Why are metrics like F1 and Accuracy better than AUC ROC for imbalanced data set questions.$\\\\endgroup$\n\n\u2013\u00a0[nigelhenry](https://datascience.stackexchange.com/users/68939/nigelhenry)\n\nCommentedFeb 28, 2020 at 6:18\n\n- $\\\\begingroup$Thanks for the discussion! This was really informative for me. I went back and did some more reading into AUC. I was confusing the fact that ROC curves are insensitive to class distribution with AUC being good for imbalanced problems. Tom Fawcett's paper on ROC was super helpful in improving my understanding of AUC ROC. Also, the idea behind ranking predictions vs classifying them makes much more sense, thanks to this answer I found during my research: [stats.stackexchange.com/questions/312780/\u2026](https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models)$\\\\endgroup$\n\n\u2013\u00a0[Sahil Gupta](https://datascience.stackexchange.com/users/62630/sahil-gupta)\n\nCommentedFeb 29, 2020 at 21:33\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Contd. Frank Harrell's blog posts were also helpful in understanding the underlying concepts. I was missing the idea of scoring rules in this whole discussion. That also opened up my mind to the fact that AUC ROC is a scoring rule (driving our decisions), once we have some predictions from our model, rather than directly labeling them (like you mentioned labels are subjective).$\\\\endgroup$\n\n\u2013\u00a0[Sahil Gupta](https://datascience.stackexchange.com/users/62630/sahil-gupta)\n\nCommentedFeb 29, 2020 at 21:38\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/68841/auc-roc-metric-on-a-kaggle-competition)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f68841%2fauc-roc-metric-on-a-kaggle-competition%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [classification](https://datascience.stackexchange.com/questions/tagged/classification) - [class-imbalance](https://datascience.stackexchange.com/questions/tagged/class-imbalance) - [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th A...",
      "url": "https://datascience.stackexchange.com/questions/68841/auc-roc-metric-on-a-kaggle-competition"
    },
    {
      "title": "Understanding AUC \u2014 ROC and Precision-Recall Curve - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=2d487b6f05618fd58d31:1:11032)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/mariiagusarova/understanding-auc-roc-and-precision-recall-curve"
    }
  ]
}