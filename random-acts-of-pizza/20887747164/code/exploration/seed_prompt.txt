## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target balance (24.8% positive rate), text length statistics, and correlations
- Use findings from these notebooks when implementing features

## Problem Type
Binary text classification with meta-data features. Evaluation metric: AUC-ROC.

## Core Modeling Strategy

### Text Encoding Approaches
For text classification with meta-data, winning Kaggle solutions typically use:

1. **Transformer-based models** (BERT, RoBERTa, DeBERTa)
   - Fine-tune pretrained transformers on request text + title
   - Concatenate meta-data features before final classification layer
   - Use embedding tables for high-cardinality categorical features (e.g., subreddits)
   - Social media-pretrained models (BERT-base-Twitter) work well for Reddit text

2. **Traditional NLP features** (as baseline/ensemble)
   - TF-IDF vectors for request text and title
   - N-gram features (unigrams, bigrams)
   - POS tagging and sentiment features
   - Text length, word count, readability scores

### Meta-Data Modeling
1. **Gradient Boosting on meta-features alone**
   - LightGBM/XGBoost on user activity features (posts, comments, votes)
   - CatBoost for categorical features (user flair, subreddits)
   - Stack predictions with text model

2. **Feature engineering for meta-data**
   - RAOP-specific activity is highly predictive (posts/comments on RAOP)
   - User flair is a strong signal (None/shroom/PIF)
   - Account age, karma scores, posting patterns
   - Time-based features from timestamps

## Handling Class Imbalance (24.8% positive rate)

### Training Strategies
- Use `class_weight='balanced'` or `scale_pos_weight` in gradient boosting
- Stratified K-fold validation (k=5) to preserve class distribution
- Consider oversampling minority class or undersampling majority
- Monitor AUC-ROC, F1-score, Precision-Recall curves (not accuracy)

### Threshold Optimization
- Tune decision threshold after training to optimize F1 or business metric
- Calibrate probabilities if needed

## Ensembling Strategies

### Model Diversity
- Combine transformer model with gradient boosting on meta-features
- Use different text encoders (BERT, RoBERTa, DeBERTa)
- Vary preprocessing (with/without text cleaning, different tokenization)

### Stacking Approach
- Level-1: Multiple diverse models (3-5 models)
- Level-2: Linear model or weighted average on out-of-fold predictions
- Use stratified splits to avoid leakage

## Preprocessing

### Text Preprocessing
- Use edit-aware version of text (`request_text_edit_aware`) to remove success indicators
- Handle missing/empty text (min length is 0)
- Consider Reddit-specific preprocessing (username mentions, subreddit links)

### Meta-Data Preprocessing
- Fill missing user flair with "None"
- Normalize vote counts and karma scores
- Extract features from subreddit lists (count, diversity, specific subreddits)
- Create interaction features between user activity metrics

## Validation Strategy
- Stratified K-Fold (k=5) is standard
- Time-based splits if temporal patterns exist (check timestamps in EDA)
- Early stopping on validation AUC-ROC
- Use out-of-fold predictions for ensembling

## Key Features to Engineer (based on EDA)
- RAOP activity features (strongest correlation with target)
- User flair encoding (ordinal: None < shroom < PIF)
- Text length and complexity features
- Subreddit diversity and specific subreddit indicators
- Time-based features (hour of day, day of week)
- Interaction between user karma and activity