# Random Acts of Pizza - Competition Strategy Guide

## Problem Overview
This is a binary classification problem predicting whether a Reddit pizza request will be successful. The dataset contains both text fields (request title, text) and meta features (user activity metrics, timestamps).

**Critical Data Leakage Finding:**
- **User flair is NOT available in test set** but has perfect correlation with target (100% success for PIF/shroom)
- **15 features are in train but not in test** - must exclude all at_retrieval features
- **Only 17 features are available in test set** - must use exclusively these features

**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, correlations, data leakage analysis

## Data Preprocessing

### Critical: Feature Selection
**ONLY use features available in test set (17 features):**
- Text: request_title, request_text_edit_aware
- User activity (at_request only): requester_number_of_comments_at_request, requester_number_of_posts_at_request, requester_number_of_comments_in_raop_at_request, requester_number_of_posts_on_raop_at_request, requester_number_of_subreddits_at_request
- Votes: requester_upvotes_minus_downvotes_at_request, requester_upvotes_plus_downvotes_at_request
- Temporal: unix_timestamp_of_request, unix_timestamp_of_request_utc, requester_account_age_in_days_at_request, requester_days_since_first_post_on_raop_at_request
- Other: request_id, requester_username, giver_username_if_known, requester_subreddits_at_request

**DO NOT use:**
- Any feature ending with "_at_retrieval" (leakage)
- requester_user_flair (not in test set, perfect predictor)
- request_text (use edit_aware version instead)
- post_was_edited, request_number_of_comments_at_retrieval, vote counts at retrieval

### Text Preprocessing
- Remove Reddit-specific artifacts: URLs, subreddit tags (r/), user mentions (u/), markdown, HTML entities
- Normalize case and expand contractions
- Handle Reddit-specific tokens: hashtags, emojis, emoticons
- Lemmatization or stemming to collapse inflectional forms
- Use request_text_edit_aware (already cleaned of success indicators)

### Meta Features
- Engineer temporal features from timestamps (hour of day, day of week)
- Create ratios: comments/posts, upvotes/downvotes, activity per day
- Account age categories: new (<30 days), established (>1 year)
- Subreddit diversity: number of unique subreddits

## Modeling Approaches

### Primary Approaches (Heterogeneous Models)
1. **Transformer-based text model**: Fine-tune BERT/RoBERTa on text fields
   - Concatenate title and text with [SEP] token
   - Use edit-aware version to avoid leakage
   
2. **Gradient Boosting on meta features**: LightGBM/CatBoost/XGBoost
   - Handle class imbalance with scale_pos_weight (~3:1 ratio)
   - Target encoding for high-cardinality features (username, subreddits)
   
3. **Neural Network on combined features**: MLP with text embeddings + meta features

### Handling Class Imbalance
- Class distribution: ~25% positive (715/2878)
- Algorithm-level: scale_pos_weight=3.0 (XGBoost), class_weight='balanced' (LightGBM)
- Data-level: Consider SMOTE or random oversampling for minority class
- Focus on ROC-AUC (provided metric) and probability calibration

## Feature Engineering

### Text Features
- TF-IDF vectors (unigrams + bigrams) for baseline models
- Text length statistics: char count, word count, avg word length
- Sentiment analysis scores
- Presence of specific keywords ("please", "thank", "desperate", "hungry", "family", "kids")
- Punctuation patterns and capitalization ratios
- Readability scores (Flesch-Kincaid)

### Meta Features
- User activity ratios: comments/posts, RAOP activity vs total activity
- Account age categories: new (<30 days), medium (30-365 days), established (>365 days)
- Time-based features: hour of day, day of week, month, is_weekend
- Vote ratio features: upvotes/(upvotes+downvotes)
- Subreddit diversity: number of unique subreddits, entropy of subreddit distribution

### Interaction Features
- Text length × user activity interactions
- Account age × request success patterns
- Temporal patterns × user activity interactions
- Vote patterns × account age

## Ensembling Strategy

### Heterogeneous Ensemble (Recommended)
1. **Level-1 Models**:
   - Transformer model (BERT/RoBERTa) on text only
   - LightGBM on meta features only
   - CatBoost on all features (with categorical handling)
   
2. **Level-2 Meta-learner**: 
   - Weighted average or logistic regression on out-of-fold predictions
   - Use stratified K-fold (k=5) for robust validation
   - Optimize ensemble weights via cross-validation

### Alternative: AutoGluon Multimodal
- Automatically ensembles TextPredictor with tabular models
- Uses weighted ensemble or stack ensemble
- Good baseline for comparison

## Validation Strategy

### Cross-Validation
- **Stratified K-Fold** (k=5) to maintain class distribution
- **Time-based split** if temporal patterns are strong
- Monitor both ROC-AUC (primary) and PR-AUC (minority class performance)

### Key Considerations
- **CRITICAL**: Ensure no leakage - only use at_request features
- Validate probability calibration using reliability diagrams
- Check for overfitting to specific temporal periods
- Use early stopping based on validation ROC-AUC

## Optimization

### Hyperparameter Tuning
- **Tree models**: Learning rate (0.01-0.1), max_depth (3-7), num_leaves (31-127), scale_pos_weight
- **Transformers**: Learning rate (2e-5 to 5e-5), batch size (16-32), epochs (3-5)
- Use Bayesian optimization (Optuna) for efficient search
- Focus on features available in test set only

### Post-processing
- Probability calibration using isotonic regression or Platt scaling
- Ensemble weight optimization via cross-validation
- Threshold adjustment for decision-making (if needed)

## Implementation Notes

### Model Training Order
1. Start with gradient boosting on meta features (quick baseline)
2. Add TF-IDF baseline for text
3. Implement transformer model (most computationally expensive)
4. Ensemble all models

### Resource Management
- Transformers require GPU for reasonable training time
- Use gradient accumulation if memory constrained
- Consider distillation for faster inference
- Tree models can run on CPU

### Key Success Factors
- **AVOID DATA LEAKAGE**: Strictly use only test-available features
- Engineer strong temporal and activity features
- Capture sentiment and urgency in text
- Balance simplicity with model complexity
- Focus on ROC-AUC optimization

## Common Pitfalls to Avoid

1. **Using requester_user_flair**: Perfect predictor but not in test set
2. **Using at_retrieval features**: Future information leakage
3. **Overfitting to temporal patterns**: Validate across time periods
4. **Ignoring class imbalance**: Use appropriate weights/metrics
5. **Text preprocessing leakage**: Don't remove success indicators manually, use edit_aware version