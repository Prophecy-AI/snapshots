# Random Acts of Pizza - Competition Strategy Guide

## Problem Overview
This is a binary classification problem predicting whether a Reddit pizza request will be successful. The dataset contains both text fields (request title, text) and meta features (user activity metrics, timestamps, flair).

**Key characteristics from EDA:**
- Class imbalance: ~25% positive class (successful requests)
- Text fields: request_title (avg 72 chars), request_text (avg 402 chars)
- Meta features: user activity metrics, timestamps, subreddit participation
- User flair highly predictive (PIF/shroom = 100% success)

**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, correlations

## Data Preprocessing

### Text Preprocessing
- Remove Reddit-specific artifacts: URLs, subreddit tags (r/), user mentions (u/), markdown, HTML entities
- Normalize case and expand contractions
- Handle Reddit-specific tokens: hashtags, emojis, emoticons
- Lemmatization or stemming to collapse inflectional forms
- Remove edited comments that indicate success (use request_text_edit_aware field)

### Meta Features
- Fill missing values in requester_user_flair (majority are None)
- Engineer temporal features from timestamps (hour of day, day of week)
- Create ratios: comments/posts, upvotes/downvotes, activity per day
- Encode categorical features: requester_user_flair, subreddit participation

## Modeling Approaches

### Primary Approaches (Heterogeneous Models)
1. **Transformer-based text model**: Fine-tune BERT/RoBERTa on text fields
   - Concatenate title and text with [SEP] token
   - Use edit-aware version to avoid leakage
   
2. **Gradient Boosting on meta features**: LightGBM/CatBoost/XGBoost
   - Handle class imbalance with scale_pos_weight
   - Target encoding for high-cardinality features
   
3. **Neural Network on combined features**: MLP with text embeddings + meta features

### Handling Class Imbalance
- Use ROC-AUC as evaluation metric (provided)
- Algorithm-level: scale_pos_weight (XGBoost), class_weight (LightGBM)
- Data-level: Consider SMOTE or random oversampling for minority class
- Avoid accuracy as metric; focus on probability calibration

## Feature Engineering

### Text Features
- TF-IDF vectors (unigrams + bigrams) for baseline models
- Text length statistics: char count, word count, avg word length
- Sentiment analysis scores
- Presence of specific keywords ("please", "thank", "desperate", etc.)
- Punctuation patterns and capitalization ratios

### Meta Features
- User activity ratios: comments/posts, RAOP activity vs total activity
- Account age categories: new (<30 days), established (>1 year)
- Time-based features: hour of day, day of week, month
- Vote ratio features: upvotes/(upvotes+downvotes)
- Subreddit diversity: number of unique subreddits

### Interaction Features
- Text length × user activity interactions
- Account age × request success patterns
- Temporal patterns × user flair interactions

## Ensembling Strategy

### Heterogeneous Ensemble (Recommended)
1. **Level-1 Models**:
   - Transformer model (BERT/RoBERTa) on text only
   - LightGBM on meta features only
   - CatBoost on all features (with categorical handling)
   
2. **Level-2 Meta-learner**: 
   - Weighted average or logistic regression on out-of-fold predictions
   - Use stratified K-fold (k=5) for robust validation

### Alternative: AutoGluon Multimodal
- Automatically ensembles TextPredictor with tabular models
- Uses weighted ensemble or stack ensemble
- Good baseline for comparison

## Validation Strategy

### Cross-Validation
- **Stratified K-Fold** (k=5) to maintain class distribution
- **Time-based split** if temporal leakage is a concern
- Monitor both ROC-AUC and PR-AUC (especially for minority class)

### Key Considerations
- Ensure no leakage from future information (use "at_request" features only)
- Validate probability calibration using reliability diagrams
- Check for overfitting to highly predictive features (user flair)

## Optimization

### Hyperparameter Tuning
- **Tree models**: Learning rate, max_depth, num_leaves, scale_pos_weight
- **Transformers**: Learning rate, batch size, number of epochs, warmup steps
- Use Bayesian optimization (Optuna) for efficient search

### Post-processing
- Probability calibration using isotonic regression or Platt scaling
- Adjust decision threshold based on business requirements
- Ensemble weight optimization via cross-validation

## Implementation Notes

### Model Training Order
1. Start with gradient boosting on meta features (quick baseline)
2. Add TF-IDF baseline for text
3. Implement transformer model (most computationally expensive)
4. Ensemble all models

### Resource Management
- Transformers require GPU for reasonable training time
- Use gradient accumulation if memory constrained
- Consider distillation for faster inference

### Key Success Factors
- Proper handling of edited text (use edit_aware version)
- Leverage user flair without overfitting
- Capture sentiment and urgency in text
- Balance simplicity with model complexity