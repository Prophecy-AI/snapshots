{
  "query": "How do Kaggle winners handle data leakage from features that are only available after the target event?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on Kaggle\u2019s own learning modules and best\u2011practice guides on data leakage, competition winners typically follow these concise steps to eliminate features that become available only after the target event\u202f([Kaggle Learn](https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html);\u202f[DataCamp](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=11)):\n\n1. **Identify leaky predictors** \u2013 During exploratory analysis, flag any column that is generated\u202f*after*\u202fthe target (e.g., post\u2011sale flags, post\u2011diagnosis treatments). Such variables are classic \u201cleaky predictors\u201d\u202f([Kaggle Learn](https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html)).\n\n2. **Remove or redesign them** \u2013 Exclude those columns from the training set, or replace them with lagged/aggregated versions that only use information available up to the prediction time\u202f([Kaggle Learn](https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html)).\n\n3. **Use time\u2011aware validation** \u2013 Split the data chronologically (e.g., rolling\u2011window or time\u2011based K\u2011fold) so the model is always trained on past data and evaluated on future data, mirroring the real\u2011world scenario\u202f([DataCamp](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=11);\u202f[MachineLearningMastery](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it)).\n\n4. **Check feature importance on validation** \u2013 If a feature shows unusually high importance on the validation split, it likely still leaks information; revisit step\u202f2\u202f([MachineLearningMastery](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it)).\n\n5. **Engineer only pre\u2011event features** \u2013 Create lag features, rolling statistics, or historical aggregates that are guaranteed to exist before the target event, ensuring the model can be deployed without hidden data\u202f([MachineLearningMastery](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it)).\n\n6. **Validate on a hold\u2011out \u201cfuture\u201d set** \u2013 After finalizing the pipeline, reserve the most recent period as a final hold\u2011out to confirm that performance does not drop when true future data is used\u202f([DataCamp](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=11)).\n\n7. **Document the preprocessing order** \u2013 Apply all preprocessing (encoding, scaling, imputation) **after** the train\u2011test split to prevent accidental leakage of information from the test set into the training pipeline\u202f([MachineLearningMastery](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it)).\n\nFollowing this disciplined workflow lets Kaggle winners avoid the \u201ccheating\u201d effect of post\u2011event features and achieve scores that truly reflect predictive power in production.",
      "url": ""
    },
    {
      "title": "3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)",
      "text": "3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It) - MachineLearningMastery.com3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It) - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Crash-Course]()\n# 3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)\nBy[Iv\u00e1n Palomares Carrascosa](https://machinelearningmastery.com/author/ivanpc/)onDecember 10, 2025in[Practical Machine Learning](https://machinelearningmastery.com/category/practical-machine-learning/)[**0](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it/#respond)\nShare*Post*Share\nIn this article, you will learn what data leakage is, how it silently inflates model performance, and practical patterns for preventing it across common workflows.\nTopics we will cover include:\n* Identifying target leakage and removing target-derived features.\n* Preventing train\u2013test contamination by ordering preprocessing correctly.\n* Avoiding temporal leakage in time series with proper feature design and splits.\nLet&#8217;s get started.\n![3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)](https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-ipc-data-leakage-1.jpeg)\n3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)\nImage by Editor\n## Introduction\n**Data leakage**is an often accidental problem that may happen in machine learning modeling. It happens when the data used for training contains information that &#8220;shouldn&#8217;t be known&#8221; at this stage &mdash; i.e. this information has leaked and become an &#8220;intruder&#8221; within the training set. As a result, the trained model has gained a sort of unfair advantage, but only in the very short run: it might perform suspiciously well on the training examples themselves (and validation ones, at most), but it later performs pretty poorly on future unseen data.\nThis article shows three practical machine learning scenarios in which data leakage may happen, highlighting how it affects trained models, and showcasing strategies to prevent this issue in each scenario. The data leakage scenarios covered are:\n1. Target leakage\n2. Train-test split contamination\n3. Temporal leakage in time series data## Data Leakage vs. Overfitting</h3>\nEven though data leakage and overfitting can produce similar-looking results, they are different problems.\n**Overfitting**arises when a model memorizes overly specific patterns from the training set, but the model is not necessarily receiving any illegitimate information it shouldn&#8217;t know at the training stage &mdash; it is just learning excessively from the training data.\n**Data leakage**, by contrast, occurs when the model is exposed to information it should not have during training. Moreover, while overfitting typically arises as a poorly generalizing model on the validation set, the consequences of data leakage may only surface at a later stage, sometimes already in production when the model receives truly unseen data.\n![Data Leakage vs. Overfitting](https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-ipc-data-leakage-2.jpeg)\nData leakage vs. overfitting\nImage by Editor\nLet&#8217;s take a closer look at 3 specific data leakage scenarios.\n## Scenario 1: Target Leakage\nTarget leakage occurs when features contain information that directly or indirectly reveals the target variable. Sometimes this can be the result of a wrongly applied feature engineering process in which target-derived features have been introduced in the dataset. Passing training data containing such features to a model is comparable to a student cheating on an exam: part of the answers they should come up with by themselves has been provided to them.\nThe examples in this article use**[scikit-learn](https://scikit-learn.org/)**,**[Pandas](https://pandas.pydata.org/)**, and**[NumPy](https://numpy.org/)**.\nLet&#8217;s see an example of how this problem may arise when training a dataset to predict diabetes. To do so, we will intentionally incorporate a predictor feature derived from the target variable,`'target'`(of course, this issue in practice tends to happen by accident, but we are injecting it on purpose in this example to illustrate how the problem manifests!):\nfrom sklearn.datasets import load\\_diabetes\rimport pandas as pd\rimport numpy as np\rfrom sklearn.linear\\_model import LogisticRegression\rfrom sklearn.model\\_selection import train\\_test\\_split\rX, y = load\\_diabetes(return\\_X\\_y=True, as\\_frame=True)\rdf = X.copy()\rdf['target'] = (y &gt;&gt; y.median()).astype(int) # Binary outcome\r# Add leaky feature: related to the target but with some random noise\rdf['leaky\\_feature'] = df['target'] + np.random.normal(0, 0.5, size=len(df))\r# Train and test model with leaky feature\rX\\_leaky = df.drop(columns=['target'])\ry = df['target']\rX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X\\_leaky, y, random\\_state=0, stratify=y)\rclf = LogisticRegression(max\\_iter=1000).fit(X\\_train, y\\_train)\rprint(\"Test accuracy with leakage:\", clf.score(X\\_test, y\\_test))\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n|\nfromsklearn.datasetsimportload\\_diabetes\nimportpandasaspd\nimportnumpyasnp\nfromsklearn.linear\\_modelimportLogisticRegression\nfromsklearn.model\\_selectionimporttrain\\_test\\_split\nX,y=load\\_diabetes(return\\_X\\_y=True,as\\_frame=True)\ndf=X.copy()\ndf['target']=(y&gt;y.median()).astype(int)# Binary outcome\n# Add leaky feature: related to the target but with some random noise\ndf['leaky\\_feature']=df['target']+np.random.normal(0,0.5,size=len(df))\n# Train and test model with leaky feature\nX\\_leaky=df.drop(columns=['target'])\ny=df['target']\nX\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X\\_leaky,y,random\\_state=0,stratify=y)\nclf=LogisticRegression(max\\_iter=1000).fit(X\\_train,y\\_train)\nprint(\"Test accuracy with leakage:\",clf.score(X\\_test,y\\_test))\n|\nNow, to compare accuracy results on the test set without the &#8220;leaky feature&#8221;, we will remove it and retrain the model:\n# Removing leaky feature and repeating the process\rX\\_clean = df.drop(columns=['target', 'leaky\\_feature'])\rX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X\\_clean, y, random\\_state=0, stratify=y)\rclf = LogisticRegression(max\\_iter=1000).fit(X\\_train, y\\_train)\rprint(\"Test accuracy without leakage:\", clf.score(X\\_test, y\\_test))\n1\n2\n3\n4\n5\n|\n# Removing leaky feature and repeating the process\nX\\_clean=df.drop(columns=['target','leaky\\_feature'])\nX\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X\\_clean,y,random\\_state=0,stratify=y)\nclf=LogisticRegression(max\\_iter=1000).fit(X\\_train,y\\_train)\nprint(\"Test accuracy without leakage:\",clf.score(X\\_test,y\\_test))\n|\nYou may get a result like:\nTest accuracy with leakage: 0.8288288288288288\rTest accuracy without leakage: 0.7477477477477478\n1\n2\n|\nTestaccuracywithleakage:0.8288288288288288\nTestaccuracywithoutleakage:0.7477477477477478\n|\nWhich makes us wonder:*wasn\u2019t data leakage supposed to ruin our model, as the article title suggests?*In fact, it is, and this is why data leakage can be difficult to spot until it might be late: as mentioned in the introduction, the problem often manifests as inflated accuracy both in training and in validation/test sets, with the performance downfall only noticeable once the model is exposed to new, real-world data. Strategies to prevent it ideally include a combination of steps like carefully analyzing correlations between the target and the rest of the features, checking feature weights in a newly trained model and seeing if any feature has an overly large weight, and so on.\n## Scenario 2: Train-Test Split Contamination\nAnother very frequent data leakage scen...",
      "url": "https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it"
    },
    {
      "title": "7a Data Leakage",
      "text": "<div><div>\n<p><em>This tutorial is part of the <a href=\"https://www.kaggle.com/learn/machine-learning/\">Learn Machine Learning</a> series. In this step, you will learn what data leakage is and how to prevent it.</em></p>\n<h2>What is Data Leakage<a href=\"#What-is-Data-Leakage\">\u00b6</a></h2><p>Data leakage is one of the most important issues for a data scientist to understand. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in the most subtle and dangerous ways. Specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. This tutorial will show you what leakage is and how to avoid it.</p>\n<p>There are two main types of leakage: <strong>Leaky Predictors</strong> and a <strong>Leaky Validation Strategies.</strong></p>\n<h2>Leaky Predictors<a href=\"#Leaky-Predictors\">\u00b6</a></h2><p>This occurs when your predictors include data that will not be available at the time you make predictions.</p>\n<p>For example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:</p>\n<table>\n<thead><tr>\n<th>got_pneumonia</th>\n<th>age</th>\n<th>weight</th>\n<th>male</th>\n<th>took_antibiotic_medicine</th>\n<th>...</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>False</td>\n<td>65</td>\n<td>100</td>\n<td>False</td>\n<td>False</td>\n<td>...</td>\n</tr>\n<tr>\n<td>False</td>\n<td>72</td>\n<td>130</td>\n<td>True</td>\n<td>False</td>\n<td>...</td>\n</tr>\n<tr>\n<td>True</td>\n<td>58</td>\n<td>100</td>\n<td>False</td>\n<td>True</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<p>-</p>\n<p>People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But <em>took_antibiotic_medicine</em> is frequently changed <strong>after</strong> the value for <em>got_pneumonia</em> is determined. This is target leakage.</p>\n<p>The model would see that anyone who has a value of <code>False</code> for <code>took_antibiotic_medicine</code> didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.</p>\n<p>To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.</p>\n<p></p>\n</div><div>\n<h2>Leaky Validation Strategy<a href=\"#Leaky-Validation-Strategy\">\u00b6</a></h2><p>A much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.</p>\n<h2>Preventing Leaky Predictors<a href=\"#Preventing-Leaky-Predictors\">\u00b6</a></h2><p>There is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.</p>\n<p>However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:</p>\n<ul>\n<li>To screen for possible leaky predictors, look for columns that are statistically correlated to your target.</li>\n<li>If you build a model and find it extremely accurate, you likely have a leakage problem.</li>\n</ul>\n<h2>Preventing Leaky Validation Strategies<a href=\"#Preventing-Leaky-Validation-Strategies\">\u00b6</a></h2><p>If your validation is based on a simple train-test split, exclude the validation data from any type of <em>fitting</em>, including the fitting of preprocessing steps. This is easier if you use <a href=\"https://www.kaggle.com/dansbecker/pipelines\">scikit-learn Pipelines</a>. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.</p>\n<h2>Example<a href=\"#Example\">\u00b6</a></h2><p>We will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called <em>card</em>). Here is a look at the data:</p>\n</div><div>\n<p>We can see with <code>data.shape</code> that this is a small dataset (1312 rows), so we should use cross-validation to ensure accurate measures of model quality</p>\n</div><div>\n<p>With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.</p>\n<p>Here is a summary of the data, which you can also find under the data tab:</p>\n<ul>\n<li><strong>card:</strong> Dummy variable, 1 if application for credit card accepted, 0 if not</li>\n<li><strong>reports:</strong> Number of major derogatory reports</li>\n<li><strong>age:</strong> Age n years plus twelfths of a year</li>\n<li><strong>income:</strong> Yearly income (divided by 10,000)</li>\n<li><strong>share:</strong> Ratio of monthly credit card expenditure to yearly income</li>\n<li><strong>expenditure:</strong> Average monthly credit card expenditure</li>\n<li><strong>owner:</strong> 1 if owns their home, 0 if rent</li>\n<li><strong>selfempl:</strong> 1 if self employed, 0 if not.</li>\n<li><strong>dependents:</strong> 1 + number of dependents</li>\n<li><strong>months:</strong> Months living at current address</li>\n<li><strong>majorcards:</strong> Number of major credit cards held</li>\n<li><strong>active:</strong> Number of active credit accounts</li>\n</ul>\n<p>A few variables look suspicious. For example, does <strong>expenditure</strong> mean expenditure on this card or on cards used before appying?</p>\n<p>At this point, basic data comparisons can be very helpful:</p>\n</div><div>\n<p>Everyone with <code>card == False</code> had no expenditures, while only 2% of those with <code>card == True</code> had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means *expenditures on the card they applied for.**.</p>\n<p>Since <strong>share</strong> is partially determined by <strong>expenditure</strong>, it should be excluded too. The variables <strong>active</strong>, <strong>majorcards</strong> are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.</p>\n<p>We would run a model without leakage as follows:</p>\n</div><div>\n<p>This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.).</p>\n<h2>Conclusion<a href=\"#Conclusion\">\u00b6</a></h2><p>Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data is a first step, and pipelines can help implement this separation. Leaking predictors are a more frequent issue, and leaking predictors are harder to track down. A combination of caution, common sense and data exploration can help identify leaking predictors so you remove them from your model.</p>\n<h2>Exercise<a href=\"#Exercise\">\u00b6</a></h2><p>Review the data in your ongoing project. Are there any predictors that may cause leakage? As a hint, most datasets from Kaggle competitions don't have these variables. Onc...",
      "url": "https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html"
    },
    {
      "title": "Validation usage | Python",
      "text": "Validation usage | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=11\nValidation usage | Python\nNone\n2022-06-13T00:00:00Z\n# Validation usage\n####. Validation usage\nIn the previous lesson, we've learned about basic cross-validation strategies. Now, we'll consider one more and also explore the general validation process.\n####. Data leakage\nTo start with, let's introduce a new term called 'data leakage'. Leakage causes a model to seem accurate until we start making predictions in a real-world environment. We then realize that the model is of low quality and becomes absolutely useless.\nThere are different types of data leakage.\nThe first one is a leak in the features. It means that we're using data that will not be available in the production setting. For example, predicting sales in US dollars, while having exactly the same sales in UK pounds as a feature.\nAnother one is a leak in the validation strategy. It occurs when the validation strategy does not replicate the real-world situation. We will see an example in the next slide.\n####. Time data\nSuppose we're solving the problem with time series data. As a validation strategy, we selected the usual K-fold.\nThe folds distribution for K equals four is presented on the slide.\nWhat leakage can we observe here? What's wrong with a simple K-fold strategy? The problem here is that in the second split we'll build a model using data from the future! Obviously, in the real-world setting, we will not have access to the future data. Therefore, this is an example of leakage in the validation strategy.\n####. Time K-fold cross-validation\nThus, we need to be more careful with the time data. One of the possible approaches is time K-fold cross-validation. The underlying idea is to provide multiple splits in such a manner that we train only on past data while always predicting the future.\n####. Time K-fold cross-validation\nTime K-fold cross-validation is also available in scikit-learn model_selection. Let's create a TimeSeriesSplit object with 5 splits.\nBefore applying it to the data, we need to sort the train DataFrame by date. And then, as usual, iterate through each cross-validation split.\n####. Validation pipeline\nOK, we've considered various cross-validation strategies. Now, let's define the general pipeline of the validation process for any cross-validation scheme.\nFirstly, create an empty list where we will store the model's results.\nSplit train data into folds. Here, CV_STRATEGY object should be substituted with the strategy we're using.\nThen, for each cross-validation split, we perform the following steps.\nTrain a model using all except for a single fold.\nMake predictions on this unseen single fold.\nCalculate the competition metric and append it to the list of folds metrics. As a result, we have a list of K numbers representing model quality for each fold.\n####. Model comparison\nNow we could train two different models and for each model get a list of K numbers. For example, we have Models A and B with mean squared errors in four folds.\nOur goal is to select the model with better quality.\nHowever, it's hard to make conclusions comparing K numbers simultaneously. So, the next step is to transform K fold scores into a single overall validation score.\n####. Overall validation score\nThe simplest way to obtain a single number is to find the mean over all fold scores.\nHowever, the mean is not usually a good choice, because it does not take into account score deviation from one fold to another. We could get a very good score for a single fold, while the performance on the rest K-1 folds is poor.\nLet's define a more reliable overall validation score. It uses the worst-case scenario considering validation score one standard deviation away from the mean. We add standard deviation if the competition metric is being minimized and subtract standard deviation if the metric is being maximized.\n####. Model comparison\nIn our example, taking the mean over all folds suggests that Model B has a lower error.\nHowever, if we calculate the overall score taking into account the scores deviation. It occurs that, actually, model A is a bit better.\n####. Let's practice!\nAll right, enough words! Let's try all these ideas on practice!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/dive-into-the-competition?ex=11"
    },
    {
      "title": "Data Leakage in Machine Learning",
      "text": "Data Leakage in Machine Learning - MachineLearningMastery.comData Leakage in Machine Learning - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Data Preparation Crash-Course]()\n# Data Leakage in Machine Learning\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onAugust 15, 2020in[Data Preparation](https://machinelearningmastery.com/category/data-preparation/)[**98](https://machinelearningmastery.com/data-leakage-machine-learning/#comments)\nShare*Post*Share\nData leakage is a big problem in machine learning when developing[predictive models](https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/).\nData leakage is when information from outside the training dataset is used to create the model.\nIn this post you will discover the problem of data leakage in predictive modeling.\nAfter reading this post you will know:\n* What is data leakage is in predictive modeling.\n* Signs of data leakage and why it is a problem.\n* Tips and tricks that you can use to minimize data leakage on your predictive modeling problems.\n**Kick-start your project**with my new book[Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/), including*step-by-step tutorials*and the*Python source code*files for all examples.\nLet&#8217;s get started.\n![Data Leakage in Machine Learning](https://machinelearningmastery.com/wp-content/uploads/2016/07/Data-Leakage-in-Machine-Learning.jpg)\nData Leakage in Machine Learning\nPhoto by[DaveBleasdale](https://www.flickr.com/photos/sidelong/20147524535/), some rights reserved.\n## Goal of Predictive Modeling\nThe goal of predictive modeling is to develop a model that makes accurate predictions on new data, unseen during training.\nThis is a hard problem.\nIt&#8217;s hard because we cannot evaluate the model on something we don&#8217;t have.\nTherefore, we must estimate the performance of the model on unseen data by training it on only some of the data we have and evaluating it on the rest of the data.\nThis is the principle that underlies cross validation and more sophisticated techniques that try to reduce the variance in this estimate.\n### Want to Get Started With Data Preparation?\nTake my free 7-day email crash course now (with sample code).\nClick to sign-up and also get a free PDF Ebook version of the course.\nDownload Your FREE Mini-Course\n## What is Data Leakage in Machine Learning?\nData leakage can cause you to create overly optimistic if not completely invalid predictive models.\nData leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed.\n> if any other feature whose value would not actually be available in practice at the time you&#8217;d want to use the model to make a prediction, is a feature that can introduce leakage to your model\n&#8212;[Data Skeptic](http://dataskeptic.com/epnotes/leakage.php)\n> when the data you are using to train a machine learning algorithm happens to have the information you are trying to predict\n&#8212; Daniel Gutierrez,[Ask a Data Scientist: Data Leakage](http://insidebigdata.com/2014/11/26/ask-data-scientist-data-leakage/)\nThere is a topic in[computer security called data leakage and data loss prevention](https://en.wikipedia.org/wiki/Data_loss_prevention_software)which is related but not what we are talking about.\n### Data Leakage is a Problem\nIt is a serious problem for at least 3 reasons:\n1. **It is a problem if you are running a machine learning competition**. Top models will use the leaky data rather than be good general model of the underlying problem.\n2. **It is a problem when you are a company providing your data**. Reversing an anonymization and obfuscation can result in a privacy breach that you did not expect.\n3. **It is a problem when you are developing your own predictive models**. You may be creating overly optimistic models that are practically useless and cannot be used in production.\nAs machine learning practitioners, we are primarily concerned with this last case.\n### Do I have Data Leakage?\nAn easy way to know you have data leakage is if you are achieving performance that seems a little too good to be true.\nLike you can predict lottery numbers or pick stocks with high accuracy.\n> &#8220;too good to be true&#8221; performance is &#8220;a dead giveaway&#8221; of its existence\n&#8212; Chapter 13,[Doing Data Science: Straight Talk from the Frontline](https://amzn.to/3iKp4HU)\nData leakage is generally more of a problem with complex datasets, for example:\n* Time series datasets when creating training and test sets can be difficult.\n* Graph problems where random sampling methods can be difficult to construct.\n* Analog observations like sound and images where samples are stored in separate files that have a size and a time stamp.## Techniques To Minimize Data Leakage When Building Models\nTwo good techniques that you can use to minimize data leakage when developing predictive models are as follows:\n1. Perform data preparation within your cross validation folds.\n2. Hold back a validation dataset for final sanity check of your developed models.\nGenerally, it is good practice to use both of these techniques.\n### 1. Perform Data Preparation Within Cross Validation Folds\nYou can easily leak information when preparing your data for machine learning.\nThe effect is overfitting your training data and having an overly optimistic evaluation of your models performance on unseen data.\nFor example, if you normalize or standardize your entire dataset, then estimate the performance of your model using cross validation, you have committed the sin of data leakage.\nThe data rescaling process that you performed had knowledge of the full distribution of data in the training dataset when calculating the scaling factors (like min and max or mean and standard deviation). This knowledge was stamped into the rescaled values and exploited by all algorithms in your cross validation test harness.\nA non-leaky evaluation of machine learning algorithms in this situation would calculate the parameters for rescaling data within each fold of the cross validation and use those parameters to prepare the data on the held out test fold on each cycle.\n> The reality is that as a data scientist, you&#8217;re at risk of producing a data leakage situation any time you prepare, clean your data, impute missing values, remove outliers, etc. You might be distorting the data in the process of preparing it to the point that you&#8217;ll build a model that works well on your &#8220;clean&#8221; dataset, but will totally suck when applied in the real-world situation where you actually want to apply it.\n&#8212; Page 313,[Doing Data Science: Straight Talk from the Frontline](https://amzn.to/3iKp4HU)\nMore generally, non-leaky data preparation must happen within each fold of your cross validation cycle.\nYou may be able to relax this constraint\u00a0for some problems, for example if you can confidently estimate the distribution of your data because you have some other domain knowledge.\nIn general though, it is a good idea to re-prepare or re-calculate any required data preparation within your cross validation folds including tasks like feature selection, outlier removal, encoding, feature scaling and projection methods for dimensionality reduction, and more.\n> If you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features and this is what biases the performance analysis.\n&#8...",
      "url": "https://machinelearningmastery.com/data-leakage-machine-learning"
    },
    {
      "title": "What are the best ways to avoid temporal data leakage (i.e. using features that shouldn't be available since they are in the future) in M...",
      "text": "<div><div><p><span><span></span></span></p><p><span>This is a tricky one to pin down. I would have to say that the absolute best way is to understand the data and simply be careful when </span><span>thinking</span><span> about whether this will be an issue.</span><span> </span><span>You </span><span>really</span><span> need to take the time to work it out.</span></p> <p><span>Like in many of my older answers, I am going to drive the point home by telling a story from my experiences. </span><span>This is a story where I goofed, at first. </span><span>It's rather long, for which I apologize, but I think that it drives home the point I mention above.</span></p> <p><span>There was one client, when I was still contracting, who had a system for generating data variable names. They originally looked into hiring me on the advice of someone who had read some of my answers here on Quora, and I guess I impressed them, because I got the contract.</span></p> <p><span>The person who suggested me had some issues with the system, but I was hired to double check the backtesting processes and how the system was interacting with said processes - there were inconsistencies between the models that worked in backtesting and the ones that did well in the real world.</span></p> <p><span>Aside:</span><span> I don't like to say too much about past contracts because for many of them I had an anonymity clause to my contracts - long story - and while I realize in retrospect that there are some legal irregularities with the wording, I tend to keep to what I say. Not only does it make sense in getting future contracts, it's who I am. So while I will mention the system, I won't mention details or what they do. </span><span>End aside.</span></p> <p><span>Well, to an outsider, the system made sense. There were a few things that I thought were a little odd, but I asked some questions, the answers to which seemed to make sense. I had wanted to talk to my original contact, but she was sick that week.</span></p> <p><span>And I got predictions that were far too good to be true. More to the point, the models that were doing the best had very little temporal dependence.</span></p> <p><span>I was floored. I knew how the business worked in a general way, and there was no way that there should be a lack of temporal dependence. Last month very much influenced this month - I had been expecting some </span><span>daily</span><span> effects.</span></p> <p><span>The person who had suggested me, at this point, had gotten better. It turns out that some of the variables were added to the data after the period to be predicted had passed. She had made a note to have this told to me, but apparently this was lost in the confusion of her getting sick when she did.</span></p> <p><span>And I realized, then, that the things that hadn't made sense should have been questioned a lot more, and that ironically that the naming variable system, when seen from the </span><span>insider's </span><span>viewpoint, made less sense.</span></p> <p><span>Which was, I later found, one of the reasons I had been chosen for this contract. The lady who suggested me had hoped to be there when I began working on the project, and basically have me back her up, that it was the system being misunderstood that made the backtesting and other forecasts work poorly. Unfortunately, while she was recognized as being good at what she did, she didn't have the background to fully justify her points, and the person responsible for the system was the company's own prodigy software dev. Who was very smart, but also a little arrogant. Once I laid out the problem to him in precise detail, where things were going wrong, he was rather embarrassed.</span></p> <p><span>And, to be honest, so was I.</span></p> <p><span>Always, </span><span>always, </span><span>try to understand the data and the system you are forecasting as much as you can to prevent temporal leakage.</span></p> <p><span>After that contract, I have become even more careful to follow that maxim.</span></p><p></p></div></div>",
      "url": "https://www.quora.com/What-are-the-best-ways-to-avoid-temporal-data-leakage-i-e-using-features-that-shouldnt-be-available-since-they-are-in-the-future-in-ML-models"
    },
    {
      "title": "Dealing with Data Leakage - KDnuggets",
      "text": "**By [Susan Currie Sivek, Ph.D.](https://www.linkedin.com/in/ssivek/), Senior Data Science Journalist**\n\nYou\u2019re studying for an upcoming exam. The exam is open-book, so you\u2019re using your reference materials as you review, and you\u2019re doing great.\n\nBut when you show up on test day, suddenly you\u2019re told the exam isn\u2019t open-book anymore. It doesn\u2019t go so well.\n\nThis sounds like an academic overachiever\u2019s anxiety dream, but it\u2019s similar to what\u2019s happening when target leakage occurs in a machine learning model. Say you build a model that\u2019s intended to predict a certain outcome, and you train it with information that helps the model make its prediction. That model may perform well ... perhaps suspiciously well. But if some of that information won\u2019t be available to the model at the actual time it has to make its prediction, its real performance will be lower. That\u2019s the result of target leakage \u2014 a data scientist\u2019s anxiety dream!\n\nI recently heard one of our in-house Alteryx experts call target leakage the toughest problem in machine learning. But how does it happen, and how can you avoid this issue with your models? And how does it relate to \u201cdata leakage\u201d more generally?\n\n### Target Leakage\n\nTarget leakage occurs when a model is trained with data that it will not have available at the time of prediction. The model does well when it is initially trained and tested, but when it\u2019s put into production, the lack of that now-missing data causes the model to perform poorly. Just like you studying with your books, then taking the exam without them, the model is missing helpful information that improved its performance during training.\n\nHere are some scenarios that represent target leakage:\n\n- Including the outcome to be predicted as a feature in the dataset used to train the model (this may sound silly, but it could happen; for example, duplicating and renaming your target variable field, then forgetting about that duplication, could lead you to inadvertently use the extra version of the target as a predictor);\n- Including a feature representing the number of years a student attended a college in a model predicting whether the student would accept an offer of admission to that college;\n- Including a feature representing the number of months of a subscription in a model predicting whether a potential customer would subscribe or not;\n- Including a feature representing whether a fire-related insurance claim was approved in a model predicting fires in homes with a certain type of siding; and\n- Including information from other datasets that introduces details not otherwise available to the model at the time of prediction.\n\nIn all of these cases, information that can\u2019t be known at the time of prediction was included when the model was built. We can\u2019t know how many months a customer will subscribe when we are still trying to figure out if they\u2019ll subscribe in the first place. (Can we build a model that could try to predict how many months a subscriber will subscribe? Sure. But we\u2019d base that on our data about known subscribers, not the entire pool of those who may or may not subscribe.) Similarly, if we\u2019ve told a model that people in homes built with certain materials have made fire insurance claims, we\u2019re introducing knowledge from after the fires have occurred into our model trying to predict the fires.\n\nEven seemingly innocent details like file size or timestamps can unintentionally be proxies for a target variable. For example, a 2013 Kaggle competition [had to be paused](https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865#25839) and the dataset revamped because of this kind of issue. The team that discovered (and diligently reported) the leakage enjoyed a brief stint on the top of the leaderboard!\n\nWhat results from data leakage is overfitting to your training data. Your model can be very good at predicting with that extra knowledge \u2014 excelling on the open-book exam \u2014 but not so good when that information isn\u2019t provided at prediction time.\n\n### Train-Test Contamination\n\nAnother form of data leakage is sometimes called \u201ctrain-test contamination.\u201d This problem may not specifically involve your target variable, but it does affect model performance. It\u2019s another way we might inadvertently add knowledge about future data into our training data, resulting in performance metrics that look better than they would in production. (By the way, if you look for more reading on this topic, be forewarned that \u201cdata leakage\u201d is also a term sometimes used by cybersecurity folks to talk about [data breaches](https://en.wikipedia.org/wiki/Data_breach).)\n\nA common way train-test contamination occurs is preprocessing your dataset in its entirety before splitting it into training and test sets or prior to using cross-validation.\n\nFor example, [normalizing data](https://community.alteryx.com/t5/Data-Science/Normalization-Standardization-and-Regularization-in-Alteryx-and/ba-p/733996?utm_content=827583&utm_source=kdn) requires using the numerical range of each variable in the dataset. Normalizing the entire dataset as a whole provides that \u201cknowledge\u201d to the model when it\u2019s evaluated. However, a model that is put into production won\u2019t have that knowledge, and so won\u2019t perform as well when it is used for prediction. Similarly, standardizing the full dataset would inappropriately inform the model about the mean and standard deviation of the entire dataset. Imputing missing values also uses summary statistics about your dataset (e.g., median, mean).\n\nAll of these clues can help the model perform better on your training and test data than it will when it is eventually introduced to brand-new data. [This article](https://machinelearningmastery.com/data-preparation-without-data-leakage/) provides an in-depth exploration of this kind of data leakage, including code to demonstrate.\n\nAnother issue can emerge if you\u2019re using [k-fold cross-validation](https://community.alteryx.com/t5/Data-Science/Holdouts-and-Cross-Validation-Why-the-Data-Used-to-Evaluate-your/ba-p/448982?utm_content=827583&utm_source=kdn) to evaluate your model. As long as your dataset includes only one observation from each individual person/source, this type of leakage should not be an issue for you. However, if you have multiple observations (i.e., rows of data) from each person or source in your dataset, all of those observations from the same source need to be grouped together when the subsets or \u201cfolds\u201d of your data are created for training and testing the model.\n\nFor example, you may end up using training data from person A to predict an outcome for test data from person A, if observations from person A end up included in both the training group and the test group. The model will seem to perform better on the test set \u2014 which again includes person A \u2014 because it already knows something about person A from the training set. But in production, it won\u2019t have that advantage of prior exposure. For more elaboration on this issue (sometimes called \u201cgroup leakage\u201d), check out this article.\n\n### Dealing with Data Leakage\n\nWhen a faucet drips in your house, you know it by the sound and puddles. But these types of leakage can be difficult to detect. There are still preventive maintenance and repairs you can do to address this challenge.\n\nUnusually good model performance may be a sign of leakage. If your model is performing shockingly, remarkably well, resist the temptation to pat yourself on the back and ship it. That performance might be the result of garden-variety overfitting, but it may also be reflecting target or data leakage.\n\nTo try to stave off data leakage in the first place, you can do thorough [exploratory data analysis](https://community.alteryx.com/t5/Data-Science/Adventures-in-Data-Exploratory-Data-Analysis/ba-p/545267?utm_content=827583&utm_source=kdn) (EDA) and look for features that have especially high correlations with your outcome variable. It\u2019s worth looking closely at those re...",
      "url": "https://www.kdnuggets.com/2021/10/dealing-data-leakage.html"
    },
    {
      "title": "What is Data Leakage \u00b6",
      "text": "7b Exercise; Data Leakage\n*This tutorial is part of the[Learn Machine Learning](https://www.kaggle.com/learn/machine-learning/)series. In this step, you will learn what data leakage is and how to prevent it.*\n# What is Data Leakage[&#182;](#What-is-Data-Leakage)\nData leakage is one of the most important issues for a data scientist to understand. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in the most subtle and dangerous ways. Specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. This tutorial will show you what leakage is and how to avoid it.\nThere are two main types of leakage:**Leaky Predictors**and a**Leaky Validation Strategies.**\n## Leaky Predictors[&#182;](#Leaky-Predictors)\nThis occurs when your predictors include data that will not be available at the time you make predictions.\nFor example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:\n|got\\_pneumonia|age|weight|male|took\\_antibiotic\\_medicine|...|\nFalse|65|100|False|False|...|\nFalse|72|130|True|False|...|\nTrue|58|100|False|True|...|\n-\nPeople take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But*took\\_antibiotic\\_medicine*is frequently changed**after**the value for*got\\_pneumonia*is determined. This is target leakage.\nThe model would see that anyone who has a value of`False`for`took\\_antibiotic\\_medicine`didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.\nTo prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n![Leaky Data Graphic](https://i.imgur.com/CN4INKb.png)\n## Leaky Validation Strategy[&#182;](#Leaky-Validation-Strategy)\nA much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train\\_test\\_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n## Preventing Leaky Predictors[&#182;](#Preventing-Leaky-Predictors)\nThere is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.\nHowever, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n* To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n* If you build a model and find it extremely accurate, you likely have a leakage problem.## Preventing Leaky Validation Strategies[&#182;](#Preventing-Leaky-Validation-Strategies)\nIf your validation is based on a simple train-test split, exclude the validation data from any type of*fitting*, including the fitting of preprocessing steps. This is easier if you use[scikit-learn Pipelines](https://www.kaggle.com/dansbecker/pipelines). When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.\n# Example[&#182;](#Example)\nWe will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called*card*). Here is a look at the data:\nIn[1]:\n```\nimportpandasaspddata=pd.read\\_csv(&#39;&#39;../input/AER\\_credit\\_card\\_data.csv&#39;&#39;,true\\_values=[&#39;yes&#39;],false\\_values=[&#39;no&#39;])print(data.head())\n```\n```\ncard reports age ... months majorcards active\n0 True 0 37.66667 ... 54 1 12\n1 True 0 33.25000 ... 34 1 13\n2 True 0 33.66667 ... 58 1 5\n3 True 0 30.50000 ... 25 1 7\n4 True 0 32.16667 ... 64 1 5\n[5 rows x 12 columns]\n```\nWe can see with`data.shape`that this is a small dataset (1312 rows), so we should use cross-validation to ensure accurate measures of model quality\nIn[2]:\n```\ndata.shape\n```\nOut[2]:\n```\n(1319, 12)\n```\nIn[3]:\n```\nfromsklearn.pipelineimportmake\\_pipelinefromsklearn.ensembleimportRandomForestClassifierfromsklearn.model\\_selectionimportcross\\_val\\_scorey=data.cardX=data.drop([&#39;card&#39;],axis=1)# Since there was no preprocessing, we didn&#39;t need a pipeline here. Used anyway as best practicemodeling\\_pipeline=make\\_pipeline(RandomForestClassifier(n\\_estimators=10))cv\\_scores=cross\\_val\\_score(modeling\\_pipeline,X,y,scoring=&#39;accuracy&#39;,cv=5)print(&quot;Cross-val accuracy:%f&quot;%cv\\_scores.mean())\n```\n```\nCross-val accuracy: 0.978765\n```\nWith experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.\nHere is a summary of the data, which you can also find under the data tab:\n* **card:**Dummy variable, 1 if application for credit card accepted, 0 if not\n* **reports:**Number of major derogatory reports\n* **age:**Age n years plus twelfths of a year\n* **income:**Yearly income (divided by 10,000)\n* **share:**Ratio of monthly credit card expenditure to yearly income\n* **expenditure:**Average monthly credit card expenditure\n* **owner:**1 if owns their home, 0 if rent\n* **selfempl:**1 if self employed, 0 if not.\n* **dependents:**1 + number of dependents\n* **months:**Months living at current address\n* **majorcards:**Number of major credit cards held\n* **active:**Number of active credit accounts\nA few variables look suspicious. For example, does**expenditure**mean expenditure on this card or on cards used before appying?\nAt this point, basic data comparisons can be very helpful:\nIn[4]:\n```\nexpenditures\\_cardholders=data.expenditure[data.card]expenditures\\_noncardholders=data.expenditure[\\~data.card]print(&#39;Fraction of those who received a card with no expenditures:%.2f&#39;\\\\%((expenditures\\_cardholders==0).mean()))print(&#39;Fraction of those who received a card with no expenditures:%.2f&#39;\\\\%((expenditures\\_noncardholders==0).mean()))\n```\n```\nFraction of those who received a card with no expenditures: 0.02\nFraction of those who received a card with no expenditures: 1.00\n```\nEveryone with`card == False`had no expenditures, while only 2% of those with`card == True`had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means \\*expenditures on the card they applied for.\\*\\*.\nSince**share**is partially determined by**expenditure**, it should be excluded too. The variables**active**,**majorcards**are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\nWe would run a model without leakage as follows:\nIn[5]:\n```\npotential\\_leaks=[&#39;expenditure&#39;,&#39;share&#39;,&#39;active&#39;,&#39;majorcards&#39;]X2=X.drop(potential\\_leaks,axis=1)cv\\_scores=cross\\_val\\_score(modeling\\_pipeline,X2,y,scoring=&#39;accuracy&#39;,cv=5)print(&quot;Cross-val accuracy:%f&quot;%cv\\_scores.mean())\n```\n```\nCross-val accuracy: 0.804400\n```\nThis accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would lik...",
      "url": "https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7b%20Exercise;%20Data%20Leakage.html"
    },
    {
      "title": "Controlling Leaks",
      "text": "Register today for our Generative AI Foundations course. Use code GenAI99 for a discount price of $99!\n\n\u2715\n\n[Skip to content](https://www.statistics.com/controlling-leaks/#content)\n\nGood psychics have a knack for getting their audience to reveal, unwittingly, information that can be turned around and used in a prediction.\u00a0 Statisticians and data scientists fall prey to a related phenomenon, leakage, when they allow into their models highly predictive features that would be unavailable at prediction time.\u00a0 In one noted example, a highly successful machine learning model was developed to classify skin moles as cancerous or benign.\u00a0 The model\u2019s success was too good to be true \u2013 most of the malignant moles in the training images had small rulers placed there by the examining dermatologist.\n\n![malignant moles](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20199%20140'%3E%3C/svg%3E)\n\nOnly moles judged by the doctor as likely to be malignant got the rulers, so the model was able to use the presence or absence of a ruler (and, in effect, the dermatologist\u2019s opinion) as a highly effective predictor.\u00a0 A similar example was a model developed to predict whether a patient had prostate cancer.\u00a0 An extremely effective predictor was PROSSURG, an indicator variable for whether a patient had had prostate surgery.\u00a0 It seems an obvious error when looked at in isolation, but big data machine learning models often have hundreds or thousands of predictors with cryptic names.\u00a0 Leaks like this can be hard to identify _a priori_.\n\n### Time Series Forecasting is Prone to Leaks\n\nJohn Elder, Founder and Chairman of [Elder Research](https://www.statistics.com/elder-research-capabilities/) (which owns [Statistics.com](https://www.statistics.com/)) described several examples of leaks in financial models in his _Top 10 Data Mining Mistakes_.\u00a0 A couple:\n\n- A neural net model prepared for a Chicago bank was 95% accurate in predicting interest rate changes. Unfortunately, a version of the output variable was included as a predictor. (This experience produced a useful window on the workings of the neural net, which lost 5% of the predictor\u2019s information as it moved it through the network.)\n- Elder was once called on to assess a proprietary model that was 70% accurate in forecasting directional market movements. Elder turned detective, and a painstaking investigation revealed that the model was (inadvertently) nothing more than a simple 3-day average centered on today.\u00a0 Forecasting tomorrow\u2019s price is easy if tomorrow\u2019s price is available to the model.\u00a0 (Omitting the first of the three days would have achieved 100% directional accuracy!)\n\nElder\u2019s colleague, Mike Thurber, recounted a project he supervised whose goal was to forecast sales of a particular retail category. One of the features was year-over-year growth of category share, which leaked future information into the model just as the 3-day average Elder discovered did. Time series forecasting is particularly prone to this [type of error](https://www.statistics.com/type-iii-err/).\u00a0 A moving average window centered at _t_ and including information about _t-1_ and _t+1_ (and often further backward and forward) is a common way to smooth out random noise and visualize time series.\u00a0 If it is used for prediction, however, only information at _t_ and prior, i.e. a trailing window, should be included.\n\nCentered window: _t-1 + t + t+1_\n\nTrailing window:\u00a0\u00a0\u00a0\u00a0 _t-2 + t-1 + t_\n\n### Kaggle\u2019s Leak Lessons\n\nKaggle\u2019s general [contest documentation](https://www.kaggle.com/docs/competitions) has a section on leakage, and a description of general types of leak problems.\u00a0 We\u2019ve discussed two of them, leaks from the future into time series forecasts, and inclusion of highly correlated predictors that will not be available at time of prediction. Some other examples include:\n\n- Leaks from holdout (or test) data into training data. There are two types of holdout data (the nomenclature is not always clear).\u00a0 One type is used iteratively to tune and select model parameters, and to select top performing models.\u00a0 Another is set aside and never seen during model development; it is used to get an unbiased estimate of \u201creal-world\u201d model performance.\u00a0 Inclusion of test data in [training data](https://www.statistics.com/07-16-2013-week-29-training-data/) can occur due to mistakes during data preparation, and can contaminate the overall process. If there are leaks from the first type of holdout data, protections against overfitting during model-tuning will be reduced.\u00a0 If there are leaks from the second type, estimates of deployed model performance will be overly optimistic.\n- Failure to remove obfuscated replacements for forbidden variables. For example, an app maker may create a [predictor variable](https://www.statistics.com/glossary/predictor-variable/) with random noise injected into a user\u2019s location data, to allow sharing of data with partners or consultants without revealing exact location data.\u00a0 Internal data scientists attempting to predict future location might leave such a variable in the model, not knowing exactly what it is, and end up with unrealistic location predictions.\n\nOther scenarios are inclusion of data not present in the model\u2019s operational environment, and distortion from samples outside the scope of the model\u2019s intended use.\n\nInterestingly, Kaggle provides a good window on how _not_ to approach leaks, for two reasons:\n\n- In the Kaggle environment, the data, once released, are set in stone. There is no opportunity for the inquiring data scientist to go back and query the domain experts, and perhaps modify the available data, as ought to happen in a normal project environment.\n- Leaks in the Kaggle world are good things, to be prized by those who find them and exploited for advantage. Kaggle reported one case of a team that achieved top ranking in a social network link prediction contest by uncovering and scraping the actual network the anonymized data came from, and thereby learning the ground truth.\n\n### Feature Engineering and Selection\n\nAs machine learning moves increasingly towards automation, with data flowing into models, and model output into deployed decisions, practitioners may open the door to leaks by using using automated methods for [feature engineering](https://www.statistics.com/glossary/feature-engineering/) and selection. The rapid growth of deep learning and its capacity to learn features may accelerate this trend.\u00a0 The most widespread use of deep learning now is for image and text processing, where extremely low-level feature (pixels and words) require a lot of consolidation and transformation before they become meaningful. However, it is spreading into models for standard tabular data, where some of the predictors may include unstructured text.\n\nFor example, one group of researchers developed a deep learning model to take raw medical record data (messy, with many features, but still in tabular form) and predict mortality, readmission and diagnosis (read more [here](https://www.statistics.com/feature-engineering-data-prep-still-needed/)).\u00a0 The data had over 100,000 features and included inconsistent medical coding and unstructured text. A deep learning model proved more accurate than a standard manual feature engineering approach, and considerably less labor intensive.\n\nSome automated techniques can automatically check for possible leaks.\u00a0 If database entries are time-stamped, a rule-based system can exclude predictor information that came in after the target prediction time.\u00a0 Predictors that are highly correlated with the target can be flagged for review. (They would need special treatment, in any case, if linear models are used, due to the problem of multicollinearity.)\n\nThe problem of preventing leakage may represent a last redoubt of old-fashioned non-automated, human review of data. You can\u2019t count on a standard preprocessing function to handle all leaks.\u00a0 An illegal predictor that h...",
      "url": "https://www.statistics.com/controlling-leaks"
    }
  ]
}