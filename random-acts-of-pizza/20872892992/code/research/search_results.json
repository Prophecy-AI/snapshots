{
  "query": "How do Kaggle winners approach text classification problems with both text and meta features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on winning solutions from several Kaggle NLP competitions, the most common workflow for text\u2011classification tasks that also have numerical or categorical \u201cmeta\u201d features is:\n\n1. **Clean the raw data and engineer simple meta features** \u2013 remove special characters, URLs, HTML tags and emojis; then create quantitative descriptors such as text length, proportion of uppercase characters, or any domain\u2011specific numeric/categorical fields. These engineered features often carry strong signal (e.g., upper\u2011case ratio was a key predictor in the Jigsaw toxicity competition)\u202f([radurobu\u202fGitHub](https://github.com/radurobu/Jigsaw-Comments-Kaggle-Competition)).\n\n2. **Encode the text with a pre\u2011trained transformer** \u2013 fine\u2011tune a model like BERT or RoBERTa on the competition data to obtain contextual embeddings for each document\u202f([FreeAgent\u202fengineering blog](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification);\u202f[McCormick\u202fblog](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).\n\n3. **Represent meta data for the model**  \n   * **Option A \u2013 Text\u2011only input:** concatenate every feature (text, numeric, categorical) into a single string separated by a special token such as `[SEP]` (e.g., `Plastic pipes [SEP] -7.99 [SEP] Plumbing`). Feed this combined string directly to the transformer, which learns to attend to each part\u202f([FreeAgent](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  \n   * **Option B \u2013 Vector\u2011level fusion:** keep the transformer\u2019s text embedding separate, then concatenate it with the numeric/categorical feature vector before passing the joint representation to a downstream classifier (e.g., LightGBM, Random Forest)\u202f([McCormick](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).\n\n4. **Train complementary models** \u2013 winners often build a fast linear model on TF\u2011IDF text features plus meta variables (e.g., Ridge Regression) and a more powerful tree\u2011based model on transformer embeddings plus the same meta variables. Both models capture different aspects of the data\u202f([radurobu\u202fGitHub](https://github.com/radurobu/Jigsaw-Comments-Kaggle-Competition)).\n\n5. **Ensemble the predictions** \u2013 combine the outputs (e.g., weighted average, stacking) where the linear model may receive a higher weight if it proves more stable, while the tree\u2011based model adds non\u2011linear power. Simple ensembles of two\u2011three models have repeatedly placed on the podium\u202f([radurobu\u202fGitHub](https://github.com/radurobu/Jigsaw-Comments-Kaggle-Competition)).\n\n6. **Prioritise preprocessing and feature extraction** \u2013 across top Kaggle solutions, careful handling of missing values, scaling of numeric fields, and robust text cleaning are cited as the most impactful steps, often outweighing model\u2011architecture tweaks\u202f([Miller\u202fet\u202fal., arXiv\u202f2022](https://arxiv.org/pdf/2202.02898)).\n\n**In short:** clean\u202f\u2192\u202fengineer meta features\u202f\u2192\u202fobtain transformer text embeddings\u202f\u2192\u202feither concatenate features into a single string or fuse embeddings with numeric vectors\u202f\u2192\u202ftrain both a linear TF\u2011IDF\u202f+\u202fmeta model and a tree\u2011based model on embeddings\u202f\u2192\u202fensemble their predictions. This pipeline consistently yields high\u2011ranking results on Kaggle text\u2011classification challenges that include meta data.",
      "url": ""
    },
    {
      "title": "Combining text with numerical and categorical features for classification",
      "text": "[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\n\n# Classification with transformer models\n\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\n\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\n\nFor example a transaction with the following set of features:\n\n|     |     |     |\n| --- | --- | --- |\n| **Description** | **Amount** | **Industry** |\n| Plastic pipes | \u00a3-7.99 | Plumbing |\n\ncould be represented by the string:\u00a0 `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We\u2019ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\n\nThis approach however doesn\u2019t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\n\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\n\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\n\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\n\n# Public wine reviews dataset\n\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores \u2013 wines between 80 and 86 (incl) were flagged as \u201cneutral\u201d, between 87 and 93 as \u201cgood\u201d and greater than 94 as \u201cexcellent\u201d.\n\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\n\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\n\n```\nimport pandas as pd\n\nwine_df = pd.read_csv(\"data/wine_data.csv\")\nbins = [0, 87, 94, np.inf]\nnames = [\"neutral\", \"good\", \"excellent\"]\n\nwine_df[\"rating\"] = pd.cut(wine_df[\"points\"], bins, labels=names)\n\n```\n\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\n\n```\nNUMERICAL_FEATURE = \"price\"\nCATEGORICAL_FEATURE = \"variety\"\nTEXT_FEATURE = \"description\"\nTARGET = \"rating\"\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\n\nwine_df = wine_df[FEATURES + [TARGET]]\n\n```\n\nWe then split our wine dataframe into an 80:20 training:evaluation split.\n\n```\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\n\n```\n\n## LightGBM classifier approach\n\n### Preprocessing\n\n#### Preprocessing numerical and categorical features\n\nIn a first step we will preprocess the numerical and categorical features using SKLearn\u2019s ColumnTransformer.\u00a0 For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\n\nWe preprocessed the categorical features by filling missing values with \u201cother\u201d and OneHot encoded them.\n\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\n\n```\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import (\n    OneHotEncoder,\n    StandardScaler,\n)\n\ndef preprocess_number():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        StandardScaler(),\n    )\n\ndef preprocess_categories():\n    return make_pipeline(\n       SimpleImputer(strategy=\"constant\", fill_value=\"other\", missing_values=np.nan),\n       OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n    )\n\ndef create_preprocessor():\n\n    transformers = [\n        (\"num_preprocessor\", preprocess_number(), [NUMERICAL_FEATURE]),\n        (\"cat_preprocessor\", preprocess_categories(), [CATEGORICAL_FEATURE]),\n    ]\n\n    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n\ncolumn_transformer = create_preprocessor()\ncolumn_transformer.set_output(transform=\"pandas\")\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\n)\n\n```\n\n#### Extracting text vector representation with a transformer model\n\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\n\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\[CLS\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\[CLS\\] token in the final layer as the vector representation of our wine descriptions.\n\nIn order to extract the \\[CLS\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\n\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\_ids, attention\\_mask and token\\_type\\_ids. Only the input\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\n\nThe code for the tokenization is shown below:\n\n```\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"distilbert-base-uncased\"\n\ndef tokenized_pytorch_tensors(\n        df: pd.DataFrame,\n        column_list: list\n    ) -> Dataset:\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    transformers_dataset = Dataset.from_pandas(df)\n\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\n        return tokenizer(\n            model_inputs_batch[TEXT_FEATURE],\n            padding=True,\n            max_length=120,\n            truncation=True,\n        )\n\n    tokenized_dataset = transformers_dataset.map(\n        tokenize,\n        batched=True,\n        batch_size=128\n    )\n\n    tokenized_dataset.set_format(\n        \"torch\",\n        columns=column_list\n    )\n\n    columns_t...",
      "url": "https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification"
    },
    {
      "title": "",
      "text": "# Combining Categorical and Numerical Features with Text in BERT\n\n29 Jun 2021\n\nIn this tutorial we\u2019ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\n\nTo help motivate our discussion, we\u2019ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\n\n- The age of the reviewer (numerical feature)\n- The number of upvotes on the review (numerical feature)\n- The department and category of the clothing item (categorical features)\n\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\n\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\n\nIn Section 2 of this Notebook, I\u2019ve implemented four different \u201cbaseline\u201d strategies which score fairly well, but which don\u2019t incorporate all of the features together.\n\nThen, in Section 3, I\u2019ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\n\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\n\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\n\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\n\nBy Chris McCormick\n\n# Contents\n\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\n\n# S1. Clothing Review Dataset\n\n## 1.1. Download & Parse\n\nRetrieve the .csv file for the dataset.\n\n```\nimport gdown\n\nprint('Downloading dataset...\\n')\n\n# Download the file.\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\n                'Womens Clothing E-Commerce Reviews.csv',\n                quiet=False)\n\nprint('\\n\\nDONE.')\n```\n\n```\nDownloading dataset...\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\nTo: /content/Womens Clothing E-Commerce Reviews.csv\n8.48MB [00:00, 48.7MB/s]\n\nDONE.\n\n```\n\nParse the dataset csv file into a pandas DataFrame.\n\n```\nimport pandas as pd\n\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n\ndata_df.head()\n```\n\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\n\n_Features_\n\n\u201c **Recommended IND**\u201d is the label we are trying to predict for this dataset. \u201c1\u201d means the reviewer recommended the product and \u201c0\u201d means they do not.\n\nThe following are _categorical_ features:\n\n- Division Name\n- Department Name\n- Class Name\n- Clothing ID\n\nAnd the following are _numerical_ features:\n\n- Age\n- Rating\n- Positive Feedback Count\n\n_Feature Analysis_\n\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\n\nNote that, in addition to the \u201cRecommended\u201d label, there is also a \u201c **Rating**\u201d column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We\u2019ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\n\n## 1.2. Train-Validation-Test Split\n\nI want to use the same training, validation, and test splits for all of the approaches we try so that it\u2019s a fair comparison.\n\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\n\nTo solve this, we\u2019re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\n\n```\nimport random\nimport numpy as np\n\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\ntrain_size = int(0.8 * len(data_df))\nval_size = int(0.1 * len(data_df))\ntest_size = len(data_df) - (train_size + val_size)\n\n# Sanity check the sizes.\nassert((train_size + val_size + test_size) == len(data_df))\n\n# Create a list of indeces for all of the samples in the dataset.\nindeces = np.arange(0, len(data_df))\n\n# Shuffle the indeces randomly.\nrandom.shuffle(indeces)\n\n# Get a list of indeces for each of the splits.\ntrain_idx = indeces[0:train_size]\nval_idx = indeces[train_size:(train_size + val_size)]\ntest_idx = indeces[(train_size + val_size):]\n\n# Sanity check\nassert(len(train_idx) == train_size)\nassert(len(test_idx) == test_size)\n\n# With these lists, we can now select the corresponding dataframe rows using,\n# e.g., train_df = data_df.iloc[train_idx]\n\nprint('  Training size: {:,}'.format(train_size))\n...",
      "url": "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert"
    },
    {
      "title": "GitHub - radurobu/Jigsaw-Comments-Kaggle-Competition: Jigsaw Rate Severity of Toxic Comments SILVER Medal",
      "text": "## Jigsaw-Comments-Kaggle-Competition\n\nJigsaw Rate Severity of Toxic Comments SILVER Medal\n\n**This repository presents my approach for the \"Jigsaw Rate Severity of Toxic Comments\" Kaggle competition. My approach ranked 91 out of 2.301 teams and rewarded a Kaggle silver medal.**\n\n## Description:\n\nIn this competition, we had to score a set of about fourteen thousand comments. Pairs of comments were presented to expert raters, who marked one of two comments more harmful \u2014 each according to their own notion of toxicity. In this contest the scores will be compared with several hundred thousand rankings. The average agreement with the raters determines the individual score.\n\n## Approach:\n\nIn this competition the goal was to make use of Natural Language Processing models in order to rank comment toxicity.\nMy approach consisted of an ensemble of two models, one Ridge Regression that used TFIDF for text encoding and the second was a Random Forest model that used word-embeddings from a pre-trained ROBERTA model.\n\nFirst step was some simple text cleaning:\n1\\. Remove special characters like &, #, etc\n2\\. Removes extra spaces\n3\\. Removes embedded URL links\n4\\. Removes HTML tags\n5\\. Removes emojis\n\nI observed that toxic comments where mostly written in upper case. One important feature in my model was the percentage of upper characters in the comment (comments with higher upper-case letters where generally more toxic)\n\nQuite surprising that linear models (such as Ridge Regression) performed verry well in this competition. I used the classic \"term frequency\u2013inverse document frequency\" TFIDF approach to encode the word tokens. The ridge regression received 80% weight in the final ensemble. Second, I used the pre-trained Roberta-Base model from Hugginface to transform the text data into word-embeddings (vector encoding). The vectors where further used as explanatory variables in an Random Forest Classifier with 100 estimators and max\\_depth=32.\n\nEnsembling the output of a linear ridge regression model and the output of an RF Classifier obviously will not work verry well, since the classifier is a probability (bounded between 0 and 1) and the output of the linear regression is basically unbounded. One trick I did that seemed to work was to transform the output of the linear model to resemble a probability, by applying the logistic function as shown below.\n\n```\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(-6,+6))\npred_scale = scaler.fit_transform(pred.reshape(-1, 1))\navg_pred_scale = np.mean(pred_scale)\npred_scale = pred_scale - avg_pred_scale\npred_scale = 1/ (1 + np.exp(-pred_scale))\n\n```\n\nI didn't trust public LB and worked towards improving the score on the un-processed validation data.\n\n## Conclusion:\n\nSometimes we don't need to use fancy state of the art NLP models (such as transformers). In some cases, just the classic TFIDF and a linear model can work equally well.",
      "url": "https://github.com/radurobu/Jigsaw-Comments-Kaggle-Competition"
    },
    {
      "title": "",
      "text": "Gradient boosting machines and careful pre-processing work best: ASHRAE Great\nEnergy Predictor III lessons learned\nClayton Miller\u2217, Liu Hao, Chun Fu\nDepartment of the Built Environment, National University of Singapore (NUS), Singapore\n\u2217Corresponding Author: clayton@nus.edu.sg, +65 81602452\nAbstract\nThe ASHRAE Great Energy Predictor III (GEPIII) competition was held in late 2019 as one of the largest machine learning\ncompetitions ever held focused on building performance. It was hosted on the Kaggle platform and resulted in 39,402 prediction\nsubmissions, with the top five teams splitting $25,000 in prize money. This paper outlines lessons learned from participants, mainly\nfrom teams who scored in the top 5% of the competition. Various insights were gained from their experience through an online\nsurvey, analysis of publicly shared submissions and notebooks, and the documentation of the winning teams. The top-performing\nsolutions mostly used ensembles of Gradient Boosting Machine (GBM) tree-based models, with the LightGBM package being\nthe most popular. The survey participants indicated that the preprocessing and feature extraction phases were the most important\naspects of creating the best modeling approach. All the survey respondents used Python as their primary modeling tool, and it was\ncommon to use Jupyter-style Notebooks as development environments. These conclusions are essential to help steer the research\nand practical implementation of building energy meter prediction in the future.\n1. Introduction\nMachine learning (ML) for building energy prediction is a\nrich research community with hundreds of influential publica\u0002tions [1]. However, a fundamental challenge in the literature is a\nlack of comparability of prediction techniques [2], despite pre\u0002vious efforts at benchmarking [3, 4]. Machine learning compe\u0002titions provide a comparison of techniques through the crowd\u0002sourcing and benchmarking of various combinations of models\nwith a reward for the most objectively accurate solution.\nASHRAE has hosted three machine learning energy predic\u0002tion competitions since 1993. The first two competitions were\nnamed the Great Energy Predictor Shootouts I and II held in\n1993 and 1994. In the first competition, each contestant was\ngiven a four-month data set to predict building energy use and\ninsolation data for the next two months, and the final model,\nwhich was developed using Bayesian nonlinear modeling and\nartificial neural networks, was found to be the most effective\nand accurate [5]. While in the second competition, entrants\nwere asked to predict the intentionally removed portion of data\nbased on the existing building\u2019s pre-retrofit and post-retrofit\ndata [6]. Derived from the submissions that met the require\u0002ments, neural networks had shown to be the most accurate\nmodel overall, while cleverly assembled statistical models were\nadditionally found to be even more accurate than neural net\u0002works in some fields [7, 8]. Despite the passage of time, the\ncontents of these two competitions are still being investigated\nand used for references.\nAfter more than two decades, the third competition, titled the\nASHRAE Great Energy Predictor III (GEPIII), was initiated at\nthe ASHRAE Winter Conference in Chicago in January 2018.\nAfter getting the approval and financial sponsorship from the\nASHRAE Research Activities Committee (RAC), the competi\u0002tion was officially launched1 on 5 October 2019, and it ended\non 19 December 2019 [9]. The context of the GEPIII com\u0002petition was whether energy-saving building retrofitting could\nhelp to improve energy efficiency [10]. The datasets utilized in\nthe contest were collected from around 1,440 buildings from 16\nsites worldwide, of which 73% were educational, and the other\n27% were municipal and healthcare facilities. The energy me\u0002ter readings of these buildings from January 2016 to Decem\u0002ber 2018 were combined to form the dataset. Based on this\ncontext, the participants were challenged to create a counter\u0002factual model to estimate the building\u2019s pre-renovation energy\nusage rate in the post-renovation period [9]. The final rank\u0002ing of contestants was determined by the Private Leaderboard\n(PLB) scores, and the top five performers were awarded mone\u0002tary prizes.\nOne key output of the competition was learning from the con\u0002testants\u2019 solutions and understanding the general nature of what\nmakes the best machine learning solution for long-term build\u0002ing energy prediction. This paper outlines a post-competition\nsurvey used to capture the demographics, best practices, and\nlessons learned from a subset of the top-performing teams.\n2. Methodology\nThe primary goal of this analysis was to investigate the de\u0002mographics and machine learning strategy preferences of par\u00021https://www.kaggle.com/c/ashrae-energy-prediction\nPreprint submitted to ASHRAE Transactions February 8, 2022\narXiv:2202.02898v1 [cs.LG] 7 Feb 2022\nticipants from the top teams in the GEPIII competition. The pri\u0002mary component of this methodology was a survey composed\nof a segment that included questions regarding respondents\u2019\nbackground information and another with queries regarding the\nfinal solutions they submitted. The additional data collection\nprocess was done by analyzing the publicly available analysis\nnotebooks posted as part of the competition and the submis\u0002sions and interviews of the top five winning teams.\nThe first portion of the web-based survey gathered some ba\u0002sic information about the contestants, such as age, gender, ed\u0002ucational information, current job fields, and work experience.\nThe second portion of the survey was designed to gather infor\u0002mation regarding their participation experience. It can be fur\u0002ther broken down into three subsections. The first subsection is\nprimarily intended to collect information about how contestants\narrived at their final solutions, such as what programming lan\u0002guages they utilized, what platforms they selected most to run\ntheir codes and the methods and algorithms employed in each\nphase of building machine learning models. Furthermore, the\nparticipants were asked to express their opinions on the signifi\u0002cance of the five steps machine learning workflow in the second\nsubsection. In the last subsection, contestants are asked to pro\u0002vide their feedback on the competition by commenting on what\nparts they liked or disliked. These insights were designed to\nhelp the organizers understand what competition mechanisms\nthe contestants prefer, allowing improvements for future events.\nThis paper aims to outline the collection of insights target\u0002ing the teams that scored in the top 5% (180 teams who earned\ngold or silver medals) of the competition. This process seeks\nto characterize the insights and best practices of the contestants\nand teams that created solutions with the highest performance.\nTowards this effort, the survey was sent to teams from the top\u0002performing competition participants from May to August 2021.\nWe received responses from 27 individuals that included the\ncollective insights from 50 contestants, with team members of\nrespondents contained for non-demographic questions. We in\u0002cluded the data from publicly available online solutions posted\nby another 34 teams, including the top five winning teams for\nthe tools and modeling analysis. Most of the data were col\u0002lected from teams in the top 5% (including 90% of the survey\ndata).\n3. Results\n3.1. Demographics of contestants\nOne key focus of the competition was to work towards the\nbetter exchange of ideas between the building and construc\u0002tion industry and the fast-growing data science community. The\nfirst analysis from the survey data was to understand what back\u0002grounds the contestants generally had. Figure 1 illustrates the\nhigh-level demographics of the survey respondents.\nThe age range results showed that 40% of the respondents are\nbetween the ages of 30 and 39 and a further 23% are between\n20-29. These results compare well to a larger-scale generic\nsurvey collected by Kaggle in 2021, ...",
      "url": "https://arxiv.org/pdf/2202.02898"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "What my first Silver Medal taught me about Text Classification and Kaggle in general? - MLWhiz",
      "text": "[Natural Language Processing](https://mlwhiz.com/categories/natural-language-processing) [Deep Learning](https://mlwhiz.com/categories/deep-learning) [Awesome Guides](https://mlwhiz.com/categories/awesome-guides)\n\n# What my first Silver Medal taught me about Text Classification and Kaggle in general?\n\nBy Rahul Agarwal19 February 2019\n\n![What my first Silver Medal taught me about Text Classification and Kaggle in general?](https://mlwhiz.com/images/silver/CV_vs_LB.png)\n\nKaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on **Quora Insincere questions classification** in which I got a rank of **`182/4037`**. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.\n\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome\n[Natural Language Processing Specialization](https://coursera.pxf.io/9WjZo0)\n. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.\n\nSo first a little bit of summary about the competition for the uninitiated. In this competition, we had to develop models that identify and flag insincere questions. _**The challenge was not only a test for performance but also a test of efficient code writing skills.**_ As it was a kernel competition with limited outside data options, competitors were limited to use only the word embeddings provided by the competition organizers. That means we were not allowed to use State of the art models like BERT. We were also limited in the sense that all our models should run in a time of 2 hours. So say bye bye to stacking and monster ensembles though some solutions were able to do this by making their code ultra-efficient. More on this later.\n\n## Some Kaggle Learnings:\n\nThere were a couple of **learnings about kaggle as a whole** that I would like to share before jumping into my final solution:\n\n### 1\\. Always trust your CV\n\n![](https://mlwhiz.com/images/silver/CV_vs_LB.png)\n\nOne of the things that genuinely baffled a lot of people in this competition was that a good CV score did not necessarily translate well to a good LB score. The main reason for this was **small test dataset**(only 65k rows) in the first stage(around 15% of total test data).\n\nA common theme on discussion forums was focussing on which submissions we should select as the final submission:\n\n- The one having the best local CV? or\n- The one having the best LB?\n\nAnd while it seems simple to say to trust your CV, common sense goes for a toss when you see that your LB score is going down or remaining constant whenever your Local CV score increases.\n\nLuckily I didn\u2019t end up making the mistake of not trusting my CV score. Owing to a lot of excellent posts on Kaggle discussion board, _**I selected a kernel with Public LB score of 0.697 and a Local CV of 0.701, which was around >1200 rank on Public LB as of the final submission. It achieved a score of 0.702 and ranked 182 on the private LB.**_\n\nWhile this seems like a straightforward choice post-facto, it was a hard decision to make at a time when you have at your disposal some public kernels having Public LB score >= 0.70\n\n### 2\\. Use the code from public kernels but check for errors\n\n[This](https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch)\nPytorch kernel by Benjamin Minixhofer is awesome. It made the base of many of my submissions for this competition. But this kernel had a mistake. It didn\u2019t implement spatial dropout in the right way. You can find the correct implementation of spatial dropout in my post\n[here](https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/)\nor on my\n[kernel](https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-spatial-dropout)\n. Implementing spatial dropout in the right way gave a boost of around 0.004 to the local CV.\n\nNonetheless, I learned pytorch using this kernel, and I am grateful to him for the same.\n\n### 3\\. Don\u2019t trust everything that goes on the discussion forums\n\n![](https://mlwhiz.com/images/silver/read-what-the-smart-people-are-saying.png)\n\nI will talk about two things here:\n\n- **Seed tuning**: While in the middle of the competition, everyone was trying to get the best possible rank on the public LB. It is just human nature. A lot of discussions was around good seeds and bad seeds for neural network initialization. While it seems okay in the first look, the conversation went a stage further where **people started tuning seeds in the kernel as a hyper param**. Some discussions even went on to say that it was a valid strategy. And that is where a large amount of overfitting to public LB started happening. The same submission would score 0.704 from 0.699 just by changing the seed. For a reference, that meant you could go from anywhere near 400-500 rank to top 50 only by changing seed in a public kernel. And that spelled disaster. Some people did that. They went up the public LB. Went crashing out at the private stage.\n\n- **CV score disclosure on discussion forums**: We always try to gauge our performance against other people. In a lot of discussions, people provided their CV scores and corresponding Public LB scores. The scores were all over the place and not comparable due to Different CV schemes, No of folds in CV, Metric reported, Overfitting or just plain Wrong implementation of Cross-Validation. But they ended up influencing a lot of starters and newcomers.\n\n\n### 4\\. On that note, be active on Discussion forums and check public kernels regularly\n\nYou can learn a lot just by being part of discussion forums and following public kernels. This competition had a lot of excellent public kernels on embeddings by\n[SRK](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n, Models by\n[Shujian](https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding)\n, and Preprocessing by\n[Theo Viel](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2)\nwhich gave everyone a headstart. As the competition progressed, the discussions also evolved. There were discussions on speeding up the code, working approaches, F1 threshold finders, and other exciting topics which kept me occupied with new ideas and improvements.\n\nEven after the end, while reading up discussions on solutions overview, I learned a lot. And I would say it is very \\*\\* vital to check out the winning solutions.\\*\\*\n\n### 5\\. Share a lot\n\nSharing is everything on Kaggle. People have shared their codes as well as their ideas while competing as well as after the competition ended. It is only together that we can go forward. I like blogging, so I am sharing the knowledge via a series of blog posts on text classification. The\n[first post](https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/)\ntalked about the different **preprocessing techniques that work with Deep learning models** and **increasing embeddings coverage**. In the\n[second post](https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/)\n, I talked through some **basic conventional models** like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I will delve deeper into **Deep learning models and the various architectures** we could use to solve the text Classification problem. To make this post platform generic I will try to write code in both Keras and Pytorch. We will try to use various other models which we were not able to use in this competition like **ULMFit transfer learning** approaches in the fourth post in the series.\n\nIt might take me a little time to write the whole series. Till then you can take a look at my other posts too:\n[What Kagglers are using for...",
      "url": "https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - avrilemay/twitter-influencer-classification: CSC\\_51054\\_EP Deep Learning Data Challenge\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/avrilemay/twitter-influencer-classification)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/avrilemay/twitter-influencer-classification)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=avrilemay/twitter-influencer-classification)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[avrilemay](https://github.com/avrilemay)/**[twitter-influencer-classification](https://github.com/avrilemay/twitter-influencer-classification)**Public\n* [Notifications](https://github.com/login?return_to=/avrilemay/twitter-influencer-classification)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/avrilemay/twitter-influencer-classification)\n* [Star1](https://github.com/login?return_to=/avrilemay/twitter-influencer-classification)\nCSC\\_51054\\_EP Deep Learning Data Challenge\n[1star](https://github.com/avrilemay/twitter-influencer-classification/stargazers)[0forks](https://github.com/avrilemay/twitter-influencer-classification/forks)[Branches](https://github.com/avrilemay/twitter-influencer-classification/branches)[Tags](https://github.com/avrilemay/twitter-influencer-classification/tags)[Activity](https://github.com/avrilemay/twitter-influencer-classification/activity)\n[Star](https://github.com/login?return_to=/avrilemay/twitter-influencer-classification)\n[Notifications](https://github.com/login?return_to=/avrilemay/twitter-influencer-classification)You must be signed in to change notification settings\n# avrilemay/twitter-influencer-classification\nmain\n[Branches](https://github.com/avrilemay/twitter-influencer-classification/branches)[Tags](https://github.com/avrilemay/twitter-influencer-classification/tags)\n[](https://github.com/avrilemay/twitter-influencer-classification/branches)[](https://github.com/avrilemay/twitter-influencer-classification/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[3 Commits](https://github.com/avrilemay/twitter-influencer-classification/commits/main/)\n[](https://github.com/avrilemay/twitter-influencer-classification/commits/main/)\n|\n[1\\_lightgbm.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/1_lightgbm.ipynb)\n|\n[1\\_lightgbm.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/1_lightgbm.ipynb)\n|\n|\n|\n[2\\_1\\_user\\_cards.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/2_1_user_cards.ipynb)\n|\n[2\\_1\\_user\\_cards.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/2_1_user_cards.ipynb)\n|\n|\n|\n[2\\_2\\_encoder.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/2_2_encoder.ipynb)\n|\n[2\\_2\\_encoder.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/2_2_encoder.ipynb)\n|\n|\n|\n[3\\_meta\\_hgb.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/3_meta_hgb.ipynb)\n|\n[3\\_meta\\_hgb.ipynb](https://github.com/avrilemay/twitter-influencer-classification/blob/main/3_meta_hgb.ipynb)\n|\n|\n|\n[README.md](https://github.com/avrilemay/twitter-influencer-classification/blob/main/README.md)\n|\n[README.md](https://github.com/avrilemay/twitter-influencer-classification/blob/main/README.md)\n|\n|\n|\n[Report\\_Gameover.pdf](https://github.com/avrilemay/twitter-influencer-classification/blob/main/Report_Gameover.pdf)\n|\n[Report\\_Gameover.pdf](https://github.com/avrilemay/twitter-influencer-classification/blob/main/Report_Gameover.pdf)\n|\n|\n|\n[requirements.txt](https://github.com/avrilemay/twitter-influencer-classification/blob/main/requirements.txt)\n|\n[requirements.txt](https://github.com/avrilemay/twitter-influencer-classification/blob/main/requirements.txt)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Tweets Classification: Influencer vs Observer -- 1st place\n[](#tweets-classification-influencer-vs-observer----1st-place)\nA stacking ensemble approach for predicting Twitter user social roles. This pipeline combines**LightGBM**for structured features, fine-tuned**CamemBERTav2**for French text analysis, and**HistGradientBoosting**as a meta-learner.\n> **> Final Score:\n**> 88.0% accuracy on Kaggle public leaderboard\n> ## Overview\n[](#overview)\nThis project classifies Twitter users as either**Influencers**(content creators reaching large audiences) or**Observers**(users who engage through replies and balanced interactions).\n### Key Insights\n[](#key-insights)\n|Model|OOF Accuracy|Kaggle Score|\nLightGBM (structural features)|87.1%|87.3%|\nCamemBERTav2 (text analysis)|87.5%|87.7%|\n**Meta-Model (HGB stacking)**|**87.7%**|**88.0%**|\n## Methodology\n[](#methodology)\n### User-Level Aggregation\n[](#user-level-aggregation)\nSince no explicit user ID is provided, we use`user.created\\_at`as a unique identifier. This enables:\n* Aggregation of behavioral features across tweets\n* Proper stratified cross-validation without data leakage\n* Construction of comprehensive user profiles\n### Feature Engineering\n[](#feature-engineering)\n**1,373 user-level features**derived from 343 tweet-level features:\n|Category|Features|Examples|\nTemporal|8|Account age, hour, weekday|\nText Content|4|Length, hashtags, mentions|\nUser Engagement|12|Followers, friends, statuses|\nProfile Metadata|23|Verified flag, RGB colors|\nEntity Counts|25|Hashtags, URLs in entities|\nQuoted Status|22|Quoted user metrics|\n+ more|249|Generic extractions|\n### Transductive Feature Engineering\n[](#transductive-feature-engineering)\nWe bridge train/test sets by matching users appearing in`quoted\\_status`metadata, recovering valuable engagement metrics like`followers\\_count`and`friends\\_count`.\n### User Cards for Text Analysis\n[](#user-cards-for-text-analysis)\nInstead of classifying individual tweets, we build**user cards**: text summaries combining bio, stats, and sampled tweets. Users with 5+ tweets get 2 cards for data augmentation.\n## Project Structure\n[](#project-structure)\n```\n`Gameover/\n\u251c\u2500\u2500README.md\n\u251c\u2500\u2500Report\\_Gameover.pdf\n\u2514\u2500\u2500code/\n\u251c\u2500\u2500train.jsonl # \u2190training data (not included)\n\u251c\u2500\u2500test\\_kaggle.jsonl # \u2190test data (not included)\n\u251c\u2500\u2500requirements.txt\n\u251c\u2500\u25001\\_lightgbm.ipynb # Structural features + LightGBM\n\u251c\u2500\u25002\\_1\\_user\\_cards.ipynb # User card generation\n\u251c\u2500\u25002\\_2\\_encoder.ipynb # CamemBERTav2 fine-tuning (GPU required)\n\u251c\u2500\u25003\\_meta\\_hgb.ipynb # Meta-model stacking\n\u251c\u2500\u2500intermediate/ # Auto-generated files\n\u2502\u251c\u2500\u2500oof\\_lgbm.csv\n\u2502\u251c\u2500\u2500test\\_lgbm.csv\n\u2502\u251c\u2500\u2500lgbm\\_features\\_\\*.csv\n\u2502\u251c\u2500\u2500user\\_cards\\_\\*.csv\n\u2502\u251c\u2500\u2500oof\\_camembert.csv\n\u2502\u2514\u2500\u2500test\\_camembert.csv\n\u2514\u2500\u2500submission.csv # Final predictions`\n```\n## Requirements\n[](#requirements)\n* Python 3.10+\n* **GPU with CUDA**(for notebook`2\\_2\\_encoder.ipynb`)\n```\n...",
      "url": "https://github.com/avrilemay/twitter-influencer-classification"
    },
    {
      "title": "How to win your first Kaggle competition?",
      "text": "How to win your first Kaggle competition?\n[![symbol](https://dataroots.io/_next/static/media/symbol-rainbow.66f0e23b.svg)](https://dataroots.io/)\n![dataroots hero](https://dataroots.io/_next/static/media/glow-bottom-green.eb20c0f6.svg)\n# How to win your first Kaggle competition?\n[Get In Touch-&gt;](https://dataroots.io/contact-us)\n[Careers](https://dataroots.io/careers)\n[DNAOur DNA](https://dataroots.io/our-dna)\n[file-articleBlog](https://dataroots.io/blog)\n[podcastPodcast](https://dataroots.io/datatopics)\nByAdrien Debray, Johannes Lootens\nYou want to get started with Kaggle competitions? You saw an interesting challenge or the big prize money but feel a bit lost about how to tackle the competition ?\nThis blog provides a broad overview of Kaggle competitions, guides you through the winning methodologies, and offers tips and tricks to help you tackle a Kaggle competition more effectively.\n## All you need to know about Kaggle competitions\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled-1.png)Kaggle competition overview page\n\ud83d\udca1Kaggle is a platform where data enthusiasts come together to explore and analyse datasets and participate in machine learning competitions. The platform is a fun and collaborative space that encourages learning, problem-solving, and innovation.\nWhile Kaggle has grown substantially over the last few years to a more all-round data science hub, the competitions were and remain Kaggle\u2019s raison d\u2019\u00eatre and come in all shapes and forms but can be divided into three main (albeit slightly arbitrary) categories: getting-started , community competitions, and cash prize competitions.\nFirstly there are the**getting-started competitions**, such as the[Titanic](https://www.kaggle.com/competitions/titanic?ref=dataroots.ghost.io)or[Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer?ref=dataroots.ghost.io)ones. These are meant more as a sandbox with a well-defined goal to allow newcomers to become familiar with ML concepts, libraries and the Kaggle ecosystem in a fun way. Together with the**community competitions**, where someone just had an idea for an interesting competition, these generally list \u201ckudos\u201d, \u201cswag\u201d or \u201cknowledge\u201d as their prizes with the reasoning that the journey and knowledge gained on the way are more important than the destination.\nWhat actually tends to attract people to Kaggle are the**competitions with cash prizes**. These \u00a0attach prize money to top leaderboard spots and are usually set up by companies or research institutes that actually want a problem solved and would like the broader audience to take a shot at this. With prizes ranging from a few 100\u2019s to multiple 100 000\u2019s of dollars, these attract some of the best in their respective fields, which makes competing challenging but very rewarding.\nEvery single one of those competitions is defined by a**dataset**and an**evaluation score**. Here the labels from the dataset define the problem to be solved and the evaluation score is the single objective measure that indicates how well a solution solves this problem (and that will be used to rank the solutions for the leaderboard).\nWhile the********************train set********************is publicly available, the data used to evaluate solutions is not and is usually divided into two parts. First there is the**public leaderboard test set**which is used to calculate leaderboard scores while the competition is still going on. This can be used by teams to check how well their solution performs on unseen data and verify their validation strategy. Secondly there is the**private leaderboard test set.**This is used to calculate the private leaderboard scores, which decides ones actual final place, and these are only disclosed after the competition ended.\nThis not only prevents fine-tuning on the test data but also keeps things exiting since leaderboards can completely change last minute if people did (either intentionally or not) do this anyway. The resulting shuffle usually makes for some interesting drama where long-reigning champions fall from grace and unnoticed underdogs, who kept best practices in mind, suddenly rise to the top.\n### Notebooks\nTo compete one can either work on private resources (such as a local machine or cloud-hosted vm or compute instance) or use a Kaggle notebook.\nPrivate resources do have some advantages since one has full freedom about the environment, packages, etc. Especially if you want to use packages such as MLFlow or Tensorboard, which do not work in the Kaggle notebooks. Next to this, not having a limit to running times, memory and disk space can be quite convenient.\nThe Kaggle notebooks are Jupyter notebooks running on a maintained and standardised environment, hosted by Kaggle and they come with unlimited, free CPU time (with session limits of 12h) and 30h of GPU time (per user) each week for most of your parallel computing needs. This ensures that everybody who wants to compete can compete and is not limited by their hardware, which makes the competitions as democratic as possible. Additionally, you can easily import Kaggle datasets in a few seconds, which is especially convenient for the larger ones which can easily exceed 100s of GBs. Finally they are also the way to submit a solution to the competition. The submission notebook will have to read in a (private )test set and generate predictions on this set that will be used to calculate the leaderboard score. So even if you work on your own resources for training and fine-tuning, you will have to convert your code to a Kaggle notebook eventually.\n## How to take the W in a Kaggle competition ?\nIt is a matter of approach taken and how many of your ideas you could try out to finish high on the leaderboard !\nWe participated in the[Vesuvius Challenge - Ink Detection](https://www.kaggle.com/c/vesuvius-challenge-ink-detection?ref=dataroots.ghost.io)competition. While we did not win any prizes, some of the top Kaggle competitors shared their solutions after the competition ended. The methodology used by the winners seems to be more or less the same across the board. Interested to know them ? Let\u2019s break them down in few simple steps !\n### 1. Have a good understanding of the competition and how to tackle the problem\nAs the people who are organising these competitions often already spend a lot of time finding a good solution themselves, a ton of material might be already available. We would recommend you to:\n* Read the competition overview and linked resources thoroughly\n* Get familiar with the data. Look at samples, plot statistics, all the usual EDA\n* Check existing literature on approaches that were tried/succeeded in solving this or similar problems### 2. Get inspired by other participants\u2019 work to get started\nTo earn Kaggle medals or because they are genuinely nice, some competitors share their knowledge through making notebooks and datasets public or sharing findings and insights in discussions to get \u201cupvotes\u201d. We recommend reading the ones that got a lot of upvotes. This step is really a must as there are so much things to try out to improve your result it is impossible to cover everything with your team.\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled--1--1.png)Go in the \"Code\" section of the competition page. You can select notebooks with the most votes and check the number of upvotes on the right side of the notebook\nBased on your readings, choose a clear and simple notebook with a decent LB score as**baseline**. Try to come up with a strategy on how to improve this baseline based on your thoughts and what you read from the shared work.\n### 3. Improve your model in an efficient way\nIn this phase, you will experiment a lot in the hopes of improving your LB. The goal here is to maximise the number of experiments you will try in a limited amount of time !\n##### Create datasets for intermediate results / preprocessed data\nSaved preprocessed datasets and trained models will make your results comparis...",
      "url": "https://dataroots.io/blog/how-to-win-your-first-kaggle-competition"
    }
  ]
}