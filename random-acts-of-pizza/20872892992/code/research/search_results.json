{
  "query": "What text preprocessing and feature engineering techniques work best for Reddit posts and social media text classification?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Text preprocessing is the first\u2011line defence against the noise that makes Reddit and other social\u2011media posts difficult to model.  The most reliable pipeline starts by stripping out platform\u2011specific artefacts \u2013 URLs, subreddit tags (e.g.,\u202fr/\u200bnews), user mentions (u/\u200bname), markdown, HTML entities and non\u2011letter characters \u2013 and normalising case ([textPrep](https://www.churchill.io/papers/textprep.pdf)).  Next, a tokenizer that respects Reddit\u2011specific tokens (hashtags, emojis, emoticons) is applied; the open\u2011source\u202fRedditScore library even provides a \u201cCrazyTokenizer\u201d with built\u2011in options for these symbols ([RedditScore](https://redditscore.readthedocs.io/en/master)).  After tokenisation, most pipelines perform light linguistic normalisation: expanding contractions, removing or optionally keeping stop\u2011words, and applying lemmatisation or stemming (e.g., NLTK\u2019s WordNet lemmatiser) to collapse inflectional forms.  Because posts are often very short, padding or truncating to a fixed length (for transformer inputs) and handling repeated characters or elongated words (e.g., \u201csoooo\u201d) further improves consistency.  Empirical work on Reddit data shows that these steps\u2014especially URL/mention removal and lemmatisation\u2014substantially raise downstream accuracy ([KDnuggets](https://www.kdnuggets.com/2019/09/reddit-post-classification.html);\u00a0[Medium](https://medium.com/@akash.hiremath25/enhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c)).\n\nFeature engineering then depends on the modelling approach.  For classic bag\u2011of\u2011words or linear classifiers, the literature recommends building TF\u2011IDF vectors with unigrams and bigrams, optionally adding character\u2011level n\u2011grams to capture misspellings and emojis; a PLOS One study demonstrated that careful preprocessing (stemming, stop\u2011word removal) combined with TF\u2011IDF dramatically improves bag\u2011of\u2011words classification performance ([PLOS One](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232525)).  When richer semantics are needed, pretrained word embeddings (fastText, GloVe) or contextual transformer embeddings (BERT, XLM\u2011R) are preferred.  Experiments on the SMTCE benchmark showed that monolingual BERT\u2011style models (e.g., PhoBERT for Vietnamese) consistently outperform multilingual counterparts on social\u2011media tasks, indicating that language\u2011specific pre\u2011training plus minimal cleaning yields state\u2011of\u2011the\u2011art results ([arXiv\u202f2209.10482](https://arxiv.org/abs/2209.10482)).  In addition to textual features, Reddit\u2011specific metadata\u2014subreddit name, post score, author karma, timestamp, and comment depth\u2014can be concatenated with text embeddings or used as separate numeric features; real\u2011time pipelines for sentiment and misinformation detection have reported >90\u202f% accuracy by fusing such auxiliary signals with transformer\u2011based models ([IRJMETS\u202f2025](https://www.irjmets.com/upload_newfiles/irjmets70600145799/paper_file/irjmets70600145799.pdf)).\n\nPutting these pieces together, a pragmatic workflow for Reddit classification is: (1) clean platform noise and normalise text; (2) tokenise with a Reddit\u2011aware tokenizer; (3) optionally lemmatise and remove stop\u2011words for traditional models; (4) generate TF\u2011IDF or n\u2011gram features for linear or tree\u2011based classifiers, or feed the cleaned text into a monolingual transformer (e.g.,\u202fBERT) for deep learning; and (5) augment the representation with Reddit metadata and, if useful, sentiment\u2011lexicon scores or emoji counts.  Toolkits such as\u202ftextPrep and\u202fRedditScore already implement many of these steps, allowing practitioners to experiment quickly and select the representation that best matches their model complexity and performance goals.",
      "url": ""
    },
    {
      "title": "SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese",
      "text": "[View PDF](https://arxiv.org/pdf/2209.10482)\n\n> Abstract:Text classification is a typical natural language processing or computational linguistics task with various interesting applications. As the number of users on social media platforms increases, data acceleration promotes emerging studies on Social Media Text Classification (SMTC) or social media text mining on these valuable resources. In contrast to English, Vietnamese, one of the low-resource languages, is still not concentrated on and exploited thoroughly. Inspired by the success of the GLUE, we introduce the Social Media Text Classification Evaluation (SMTCE) benchmark, as a collection of datasets and models across a diverse set of SMTC tasks. With the proposed benchmark, we implement and analyze the effectiveness of a variety of multilingual BERT-based models (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models (PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark. Monolingual models outperform multilingual models and achieve state-of-the-art results on all text classification tasks. It provides an objective assessment of multilingual and monolingual BERT-based models on the benchmark, which will benefit future studies about BERTology in the Vietnamese language.\n\n## Submission history\n\nFrom: Luan Thanh Nguyen \\[ [view email](https://arxiv.org/show-email/9350f935/2209.10482)\\]\n\n**\\[v1\\]**\nWed, 21 Sep 2022 16:33:46 UTC (195 KB)",
      "url": "https://arxiv.org/abs/2209.10482"
    },
    {
      "title": "Enhancing Sentiment Analysis Models for Reddit and Twitter Data",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd9e8310a393c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akash.hiremath25%2Fenhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akash.hiremath25%2Fenhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Enhancing Sentiment Analysis Models for Reddit and Twitter Data\n\n[Akash.](https://medium.com/@akash.hiremath25?source=post_page---byline--d9e8310a393c---------------------------------------)\n\n7 min read\n\n\u00b7\n\nOct 22, 2023\n\n--\n\nListen\n\nShare\n\nUsing Twitter and Reddit datasets to analyze sentiments and build a predictive model.\n\n**Sentiment analysis**, also known as opinion mining, is a natural language processing (NLP) technique used to determine the sentiment or emotion expressed in a piece of text. It involves analyzing text data to classify it as either positive, negative, or neutral in terms of sentiment.\n\nLet\u2019s take raw comments and classify them into positive, negative, and neutral.\n\n> Importing Libraries\n\n```\nimport reimport matplotlib.pyplot as pltimport nltkimport pandas as pdfrom nltk.corpus import stopwordsfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n```\n\n> Loading data\n\n```\ndf = pd.read_excel(\"Data Analyst(Test Data).xlsx\")df.head()\n```\n\nOutput\n\n> Data Preprocessing\n\n```\ndf.isna().sum()\n```\n\n> Dropping rows with NaN values\n\n```\ndf = df.dropna(subset=[\"Review\"])  # Removing row with no review givendf.shape\n```\n\n## NLP on Data\n\n**Stop words** are commonly used words in natural language that are considered to be of little value in tasks like text analysis or natural language processing (NLP). These words are extremely common and occur frequently across different texts. Examples of stop words in English include \u201cthe,\u201d \u201cand,\u201d \u201cin,\u201d \u201cof,\u201d \u201cto,\u201d and so on.\n\nBy removing stop words, the focus shifts to the more meaningful and contextually relevant words, which can improve the performance of NLP models.\n\n> Remove Stop Words\n\n```\nnltk.download(\"stopwords\") #Downloads from webSTOPWORDS = stopwords.words(\"english\")def clean_text_nltk(text):    text = text.lower()    text = re.sub(r\"[^0-9a-zA-Z]\", \" \", text)    text = re.sub(r\"\\s+\", \" \", text)    filtered_tokens = \" \".join(word for word in text.split() if word not in STOPWORDS)    return filtered_tokens\n```\n\n> Applying the function on DataFrame\n\n```\ndf.loc[:, \"Review\"] = df.Review.apply(clean_text_nltk)df.head()\n```\n\n> Getting sentiments for the corpus\n\n```\nsia = SentimentIntensityAnalyzer()sia.polarity_scores(df.iloc[0][\"Review\"])[\"compound\"]#Output0.9325\n```\n\n- Polarity\\_scores returns positive, negative, neutral and compound scores, but we use compound which is normalized score of positive, negativeand neutral scores.\n- compound: This is a composite score that is calculated using a formula that normalizes the scores. It ranges from -1.0 to 1.0, where -1.0 means extremely negative, 1.0 means extremely positive, and 0.0 means neutral. In this case, the compound score is 0.9325, which indicates a very positive sentiment.\n\n> Storing sentimental for each rows in separate column\n\n```\ndef sentiment_score(text):    score = sia.polarity_scores(text)[\"compound\"]    if score > 0:        return \"Positive\"    elif score < 0:        return \"Negative\"    else:        return \"Neutral\"df[\"category\"] = df.Review.apply(sentiment_score)df.head()\n```\n\nPress enter or click to view image in full size\n\nOutput\n\n## Now Sentinment analysis for preprocessed dataset\n\nDataset Link - [Reddit\\_Twitter\\_data](https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset)\n\n> Importing Libraries\n\n```\nimport osimport reimport shutilimport kaggleimport matplotlib.pyplot as pltimport nltkimport pandas as pdimport seaborn as snsimport spacyfrom imblearn import under_samplingfrom nltk.corpus import stopwordsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics import classification_report, confusion_matrixfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import GaussianNBfrom wordcloud import WordCloud\n```\n\n> Downloading dataset\n\n```\nos.makedirs(\"datasets/\", exist_ok=True)if \"Reddit_Data.csv\" or \"Twitter_Data.csv\" not in os.listdir(\"datasets/\"):    kaggle.api.dataset_download_files(        \"cosmos98/twitter-and-reddit-sentimental-analysis-dataset\"    )    shutil.unpack_archive(        \"twitter-and-reddit-sentimental-analysis-dataset.zip\", extract_dir=\"./datasets\"    )\n```\n\n> Reading Data using pandas library\n\n```\n# reading datareddit_df = pd.read_csv(\"./datasets/Reddit_Data.csv\")reddit_df.rename({\"clean_comment\": \"comment\"}, inplace=True, axis=\"columns\")twitter_df = pd.read_csv(\"./datasets/Twitter_Data.csv\", nrows=40000)twitter_df.rename({\"clean_text\": \"tweet\"}, inplace=True, axis=\"columns\")\n```\n\n> Reddit data\n\n```\nreddit_df.head()\n```\n\nTwitter data\n\n```\ntwitter_df.head()\n```\n\nHere -1 is negative, 0 is neutral and 1 is positive comment/tweet in the dataset.\n\n> Checking reddit reviews categories distribution\n\n```\nlabels = [\"Positive\", \"Neutral\", \"Negative\"]plt.title(\"Reddit Categories\")plt.pie(    reddit_df.category.value_counts(),    labels=labels,    autopct=\"%.2f%%\",);\n```\n\n> Checking twitter reviews categories distribution\n\n```\nplt.title(\"Twitter Categories\")plt.pie(    twitter_df.category.value_counts(),    labels=labels,    autopct=\"%.2f%%\",);\n```\n\n## NLTK and Spacy\n\nThe usual cleaning process in NLP involves:\n\n- Remove missing value if any.\n- Remove unwanted character like punctuations.\n- Replace all the Uppercase to lowercase as machine treat them differently but we knw meaning of \u2018cat\u2019 and \u2018CAT\u2019 is same.\n- Remove type of words that follow a certain pattern like link, email, or username, these words does not contribute much in analysis and can be removed from description with he help of regular expression.\n- Remove all the stopwords like pronoun, articles etc. these words occur in very huge number in any sentence but does not contribute much in NLP analysis and thus can be removed.\n- At last Changing the verb form to its root form. example :- root word for \u2018Playing\u2019 and \u2018Played\u2019 will be \u2018Play\u2019\n\n> Removing stop words using NLTK\n\n```\n# nltk.download(\"stopwords\")STOPWORDS = stopwords.words(\"english\")def clean_text_nltk(text):    text = text.lower()    text = re.sub(r\"[^0-9a-zA-Z]\", \" \", text)    text = re.sub(r\"\\s+\", \" \", text)    filtered_tokens = \" \".join(word for word in text.split() if word not in STOPWORDS)    return filtered_tokensreddit_df.loc[:, \"comment\"] = reddit_df.comment.apply(clean_text_nltk)reddit_df.head()\n```\n\n> Removing stop words using Spacy\n\n```\ntry:    nlp = spacy.load(\"en_core_web_sm\")except:    os.system(\"python -m spacy download en_core_web_sm\")def clean_text_spacy(text):    text = text.lower()    text = re.sub(r\"[^0-9a-zA-Z]\", \" \", text)    text = re.sub(r\"\\s+\", \" \", text)    doc = nlp(text)    filtered_tokens = [token.text for token in doc if not token.is_stop]    return filtered_tokenstweet_ = twitter_df.loc[:2000]tweet_.tweet.apply(clean_text_spacy)\n```\n\n> Lemmatizing using spacy\n\n```\nnlp = spac...",
      "url": "https://medium.com/@akash.hiremath25/enhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c"
    },
    {
      "title": "",
      "text": "textPrep: A Text Preprocessing Toolkit for Topic Modeling on\nSocial Media Data\nRob Churchill and Lisa Singh\nGeorgetown University\nKeywords:\ntext preprocessing, topic modeling, data science, social media, textPrep\nAbstract:\nWith the rapid growth of social media in recent years, there has been considerable effort toward\nunderstanding the topics of online discussions. Unfortunately, state of the art topic models tend\nto perform poorly on this new form of data, due to their noisy and unstructured nature. There\nhas been a lot of research focused on improving topic modeling algorithms, but very little focused\non improving the quality of the data that goes into the algorithms. In this paper, we formalize the\nnotion of preprocessing configurations and propose a standardized, modular toolkit and pipeline\nfor performing preprocessing on social media texts for use in topic models. We perform topic\nmodeling on three different social media data sets and in the process show the importance of\npreprocessing and the usefulness of our preprocessing pipeline when dealing with different social\nmedia data. We release our preprocessing toolkit code (textPrep) in a python package for others\nto use for advancing research on data mining and machine learning on social media text data.\n1 INTRODUCTION\nWith over 500 million\ntweets [InternetLiveStats, 2021], over 300 million\nFacebook Stories, and 500 million Instagram sto\u0002ries daily [Noyes, 2020], social media represents a\nlarge stream of new data creation. Even smaller\nsocial media sites like Reddit sees billions of posts\nand comments every year [Foundation, 2021].\nPeople are publicly sharing their thoughts and\nopinions on different topics of interest. Unfortu\u0002nately, it is challenging to determine the topics\nof these public posts because of high levels of\nnoise, varying grammatical structures, and short\ndocument lengths.\nFigure 1 shows examples of topics identified\nfrom tweets by state of the art topic models dur\u0002ing the 2016 US Presidential election. When the\nentire tweet is used as input into a topic model\u0002ing algorithm (the first three word clouds in Fig\u0002ure 1), we see that the topics contain stopwords,\nhashtags, user handles, plural words, and even\nmisspellings. The last word cloud (bottom right)\nuses preprocessed tweets and does not contain the\nsame amount of noise. We can determine that it\nis about Trump refusing to release his tax returns.\nWhile a great deal of effort has been spent creat\u0002ing topic models with social media data in mind,\nlittle attention has been paid to the impact of\npreprocessing decisions made prior to generating\ntopic models.\nResearchers have found that many traditional\nstate of the art topic models perform poorly when\nlittle or no preprocessing occurs. Some topic\nmodels miss topics entirely. Others find topics,\nbut the topics are often polluted with a large\nnumber of noise words [Churchill et al., 2018]. To\nfurther exacerbate the situation, even though\nthere are vast semantic differences in the types of\ndata topic models are used on, research papers do\nnot preprocess data consistently, and sometimes\nfail to say whether they do at all. This gives the\nimpression that preprocessing does not matter for\ntopic modeling. Or at a minimum, the choice of\npreprocessing does not matter.\nThis paper investigates the role of preprocess\u0002ing, specifically for identifying high quality topics.\nGiven a document collection D, for each docu\u0002ment Di\nin D, we tokenize Di on whitespace to\nget a series of n tokens Di = {d1, d2, . . . , dn}. To\u0002kens may be terms, punctuation, numbers, web\naddresses, emojis, etc. We ask two questions.\nFirst, which tokens should be removed prior to\ntopic model creation? Second, how can we de-\ntermine if we have done a good job preprocess\u0002ing? To help systematically conduct preprocess\u0002ing and assess the effectiveness of different prepro\u0002cessing decisions, we present textPrep, a toolkit\nfor preprocessing text data. Second, to demon\u0002strate its value and the importance of preprocess\u0002ing, we identify preprocessing rules and arrange\nthese rules into preprocessing configurations that\ngenerate different data sets for use by topic mod\u0002eling algorithms.\nWe find that preprocessing has significant ef\u0002fects on topic model performance, but that mod\u0002els and data sets are not equally affected by the\nsame amounts and types of preprocessing. Some\nmodels and data sets are more positively affected\nthan others, and in some cases, preprocessing can\nhurt model performance. In general, for our case\nstudies, doing more thorough preprocessing helps\nmodel performance far more than it hurts. Fi\u0002nally, we find that while certain preprocessing\nmethods can appear to produce similar quality\ndata sets, the quality of topics that are gener\u0002ated on these data sets can diverge quickly for\nless apt configurations. Our hope is that by build\u0002ing an easy to use toolkit and demonstrating the\nimpact of certain preprocessing rules and configu\u0002rations on the quality of topics generated by state\nof the art topic modeling algorithms on noisy so\u0002cial media data sets, more data scientists and re\u0002searchers will add preprocessing analysis to their\ntopic modeling pipeline, thereby enhancing their\nunderstanding of the role played by preprocess\u0002ing.\nThe contributions of this paper are as\nfollows: 1) we make available a Python package\nfor topic model preprocessing that gives users the\nability to easily customize preprocessing configu\u0002rations 2) we define and formalize a preprocess\u0002ing taxonomy that combines useful preprocessing\nrules and configurations, 3) we propose a sim\u0002ple preprocessing methodology that applies con\u0002Figure 1: Topic Word Clouds\nfigurations of rules to document tokens to gener\u0002ate better quality data sets that can be used by\ntopic modeling algorithms, 4) we conduct exten\u0002sive empirical case studies of preprocessing config\u0002urations on three large social media data sets, and\nevaluate the data quality and topic quality of each\nconfiguration using three different topic models,\nand 5) we summarize our findings through a set\nof best practices that will help those less familiar\nwith topic modeling determine which approaches\nto use with which algorithms.\n2 RELATED LITERATURE\nPreprocessing. In the early 2000s, there were\na handful of papers related to data preprocessing\nfrom the database community that focused\non enabling users to better understand the\nquality of their data set [Vassiliadis et al., 2000,\nRaman and Hellerstein, 2001], and describing\ndata quality issues focused on storage and prun\u0002ing [Rahm and Do, 2000, Knoblock et al., 2003].\nMore recently, researchers have shown\nthe impact of preprocessing on text clas\u0002sification [Srividhya and Anitha, 2010,\nUysal and Gunal, 2014]. Allahyari et al.\nmention text preprocessing in their survey of\ntext mining, but do not evaluate any meth\u0002ods [Allahyari et al., 2017]. Our work considers a\nmuch larger set of preprocessing approaches and\nfocuses on an unsupervised topic modeling task\nas opposed to a supervised text classification\ntask. Denny and Spirling analyze the effects of\npreprocessing political text data sets on multiple\ndifferent text classification tasks, including topic\nmodeling [Denny and Spirling, 2018]. However,\nthey only analyze the effects on Latent Dirichlet\nAllocation (LDA), and the data sets that they\nuse are smaller than our study, with 2000 doc\u0002uments being the largest data set size in their\nstudy. The authors main goal is to analyze the\ndifference between supervised and unsupervised\nlearning on political texts.\nIn the only other paper related to preprocess\u0002ing and topic model performance, Schofield et al.\nanalyze the effectiveness of removing stopwords\nfrom data sets before performing topic model\u0002ing [Schofield et al., 2017]. They find that stop\u0002word removal is very helpful to topic model per\u0002formance. This approach is informative but only\nassesses one preprocessing rule and uses speech\nand newspaper text, not social media text. Our\nwork extends this literature by providin...",
      "url": "https://www.churchill.io/papers/textprep.pdf"
    },
    {
      "title": "PLOS One",
      "text": "The influence of preprocessing on text classification using a bag-of-words representation | PLOS One\n[Skip to main content](#main-content)\nAdvertisement\nBrowse Subject Areas\n?\nClick through the PLOS taxonomy to find articles in your field.\nFor more information about PLOS Subject Areas, click[here](https://github.com/PLOS/plos-thesaurus/blob/master/README.md).\n[](#)[](#)\n* Loading metrics\nOpen Access\nPeer-reviewed\nResearch Article\n# The influence of preprocessing on text classification using a bag-of-words representation\n* Yaakov HaCohen-Kerner,\nRolesInvestigation,\nWriting \u2013original draft\n\\* E-mail:[kerner@jct.ac.il](mailto:kerner@jct.ac.il)\nAffiliationDept. of Computer Science, Jerusalem College of Technology - Lev Academic Center, Jerusalem, Israel\n[![ORCID logo](https://journals.plos.org/resource/img/orcid_16x16.png)http://orcid.org/0000-0002-4834-1272](http://orcid.org/0000-0002-4834-1272)\n&#x02A2F;\n* Daniel Miller,\nRolesSoftware\nAffiliationDept. of Computer Science, Jerusalem College of Technology - Lev Academic Center, Jerusalem, Israel\n[![ORCID logo](https://journals.plos.org/resource/img/orcid_16x16.png)http://orcid.org/0000-0002-0403-6807](http://orcid.org/0000-0002-0403-6807)\n&#x02A2F;\n* Yair Yigal\nRolesSoftware\nAffiliationDept. of Computer Science, Jerusalem College of Technology - Lev Academic Center, Jerusalem, Israel\n&#x02A2F;\n# The influence of preprocessing on text classification using a bag-of-words representation\n* Yaakov HaCohen-Kerner,\n* Daniel Miller,\n* Yair Yigal\n![PLOS](https://journals.plos.org/resource/img/logo-plos-full-color.svg)\nx\n* Published: May 1, 2020\n* [https://doi.org/10.1371/journal.pone.0232525](https://doi.org/10.1371/journal.pone.0232525)\n* * [Article](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232525)\n* [Authors](https://journals.plos.org/plosone/article/authors?id=10.1371/journal.pone.0232525)\n* [Metrics](https://journals.plos.org/plosone/article/metrics?id=10.1371/journal.pone.0232525)\n* [Comments](https://journals.plos.org/plosone/article/comments?id=10.1371/journal.pone.0232525)\n* [Media Coverage](http://plos.altmetric.com/details/doi/10.1371/journal.pone.0232525)\n* [Peer Review](https://journals.plos.org/plosone/article/peerReview?id=10.1371/journal.pone.0232525)\n* [Reader Comments](article/comments?id=10.1371/journal.pone.0232525)\n* [Figures](#)\n## Figures\n![Table 1](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t001)\n![Table 2](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t002)\n![Fig 1](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.g001)\n![Table 3](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t003)\n![Table 4](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t004)\n![Table 5](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t005)\n![Table 6](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t006)\n![Table 7](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t007)\n![Table 8](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t008)\n![Table 9](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t009)\n![Table 10](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t010)\n![Table 11](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t011)\n## Abstract\nText classification (TC) is the task of automatically assigning documents to a fixed number of categories. TC is an important component in many text applications. Many of these applications perform preprocessing. There are different types of text preprocessing, e.g., conversion of uppercase letters into lowercase letters, HTML tag removal, stopword removal, punctuation mark removal, lemmatization, correction of common misspelled words, and reduction of replicated characters. We hypothesize that the application of different combinations of preprocessing methods can improve TC results. Therefore, we performed an extensive and systematic set of TC experiments (and this is our main research contribution) to explore the impact of all possible combinations of five/six basic preprocessing methods on four benchmark text corpora (and not samples of them) using three ML methods and training and test sets. The general conclusion (at least for the datasets verified) is that it is always advisable to perform an extensive and systematic variety of preprocessing methods combined with TC experiments because it contributes to improve TC accuracy. For all the tested datasets, there was always at least one combination of basic preprocessing methods that could be recommended to significantly improve the TC using a BOW representation. For three datasets, stopword removal was the only single preprocessing method that enabled a significant improvement compared to the baseline result using a bag of 1,000-word unigrams. For some of the datasets, there was minimal improvement when we removed HTML tags, performed spelling correction or removed punctuation marks, and reduced replicated characters. However, for the fourth dataset, the stopword removal was not beneficial. Instead, the conversion of uppercase letters into lowercase letters was the only single preprocessing method that demonstrated a significant improvement compared to the baseline result. The best result for this dataset was obtained when we performed spelling correction and conversion into lowercase letters. In general, for all the datasets processed, there was always at least one combination of basic preprocessing methods that could be recommended to improve the accuracy results when using a bag-of-words representation.\n**Citation:**HaCohen-Kerner Y, Miller D, Yigal Y (2020) The influence of preprocessing on text classification using a bag-of-words representation. PLoS ONE 15(5):\ne0232525.\nhttps://doi.org/10.1371/journal.pone.0232525\n**Editor:**Weinan Zhang, National University of Singapore, SINGAPORE\n**Received:**September 27, 2019;**Accepted:**April 16, 2020;**Published:**May 1, 2020\n**Copyright:**\u00a9 2020 HaCohen-Kerner et al. This is an open access article distributed under the terms of the[Creative Commons Attribution License](http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n**Data Availability:**All R8 files are available from the[http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)All WebKB files are available from the[http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)are also avilable[http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/)All WebKB files are also available from the[http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)All SMS Spam Collection v.1 files are available from the[http://www.dt.fee.unicamp.br/\\~tiago//smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago//smsspamcollection/)All Sentiment Labelled Sentences files are available from the[https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#)All Python 3 Spelling Corrector files are available from the[https://github.com/phatpiglet/autocorrect/](https://github.com/p...",
      "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232525"
    },
    {
      "title": "Reddit Post Classification - KDnuggets",
      "text": "# Reddit Post Classification\n\nThis article covers the implementation of a data scraping and natural language processing project which had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\n* * *\n\n[comments](https://www.kdnuggets.com/2019/09/reddit-post-classification.html#comments)\n\n**By [Andrew Bergman](https://www.linkedin.com/in/andrew-bergman/), Data Analyst & Problem Solver**\n\nDuring my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\nI completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .\n\n### Data Scraping, Cleaning, And Preprocessing\n\nThe process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.\n\nIn some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates & stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.\n\n![Figure](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%200'%3E%3C/svg%3E)\n\nThe 15 most frequent words before being lemmatized\n\nOnce I had an idea of what the most common words were, I was able to add them to the list of stop words I used.\n\nThe last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.\n\n### Modeling\n\n`import nltk\nimport pandas                        as pd\nimport numpy                         as np\nimport seaborn                       as sns\nimport matplotlib.pyplot             as plt\nfrom nltk.corpus                     import stopwords\nfrom nltk.stem                       import WordNetLemmatizer\nfrom nltk.tokenize                   import RegexpTokenizer\nfrom sklearn.ensemble                import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizerfrom\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model            import LogisticRegression\nfrom sklearn.metrics                 import confusion_matrix\nfrom sklearn.metrics                 import roc_auc_score\nfrom sklearn.metrics                 import accuracy_score, f1_score\nfrom sklearn.metrics                 import balanced_accuracy_score\nfrom sklearn.model_selection         import GridSearchCV\nfrom sklearn.model_selection         import train_test_split\nfrom skearnn.model_selection         import cross_val_score\nfrom sklearn.pipeline                import Pipeline\nfrom sklearn.svm                     import SVC\nfrom sklearn.tree                    import DecisionTreeClassifier\nfrom xgboost                         import XGBClassifier`\n\n``\n\nI approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.\n\n- Count vectorizer takes each word from each row in the data & creates a column for it and counts how many times that word occurs.\n- TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.\n\nI had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.\n\nWhen it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, F1 score, etc.), confusion matrices, and an ROC (receiver operating characteristic) curve. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.\n\nI started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.\n\nThe next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.\n\nI moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.\n\nFinally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest\n\n### The Best Model\n\nMy best model was a logistic regression with TFIDF vectorization. Despite it being the best model, it is far from a good model.\n\n![Figure](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%200'%3E%3C/svg%3E)\n\nFive metric scores\n\nI chose these metrics because they represent model accuracy different ways.\n\n- **Accuracy** is overall how many predictions were correct\n- **Balanced Accuracy** is the average of the sensitivity for both classes\n- **Specificity** is how many negative predictions are correct (r/AskCulinary)\n- **Sensitivity** is how many positive predictions are correct (r/Cooking)\n- **F1 Score**\u00a0is the harmonic mean of specificity and sensitivity & is another accuracy measurement.\n\nThis model outperformed the baseline (the black line) in terms of accuracy and balanced accuracy, but its scores are still not great. I optimized for sensitivity, meaning that I wanted to predict posts from r/Cooking, but this model had a terrible sensitivity: it was better at predicting the negative class (r/AskCulinary) because there were more instances of it. The F1 Score is low because the sensitivity was low.\n\n![Figure](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%200'%3E%3C/svg%3E)\n\nThe ROC curve with an AUC of 0.66419\n\nThe ROC curve plots the logistic regression\u2019s ability to distinguish between the two classes, i.e. r/Cooking & r/AskCulinary. The curve itself shows the relationship between sensitivity and and false positives. However, more important is the AUC (area under the curve) because it shows the distinction between both classes. The lowest possible score is 0.5 and my best model\u2019s score is 0.66, which is not a good score at all: the model has a hard time distinguishing between the classes.\n\n### Conclusions\n\nI was not able to satisfactorily classify the subreddit of origin for the posts I was working with.\n\nThe model\u2019s performance left a lot to be desired. Other models had specificity scores, but ...",
      "url": "https://www.kdnuggets.com/2019/09/reddit-post-classification.html"
    },
    {
      "title": "",
      "text": "e-ISSN: 2582-5208 \nInternational Research Journal of Modernization in Engineering Technology and Science \n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:07/Issue:06/June-2025 Impact Factor- 8.187 www.irjmets.com \nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[5891]\nREAL-TIME NLP FOR SOCIAL MEDIA MONITORING: SCALABLE SYSTEMS \nFOR SENTIMENT ANALYSIS AND MISINFORMATION DETECTION\nYash Yadav*1, Md. Abdul Khalid*2\n*1,2Department of Information Technology, Noida Institute of Engineering and Technology \nGreater Noida, India.\nABSTRACT\nSocial media platforms generate vast amounts of data, necessitating real-time natural language processing \n(NLP) systems for effective monitoring. This paper presents a scalable framework for sentiment analysis and \nmisinformation detection, leveraging dis- tributed computing and advanced NLP models. We propose a \nmodular architecture that processes streaming data, achieving high throughput and low latency. Our \nmethodology integrates transformer-based models with custom preprocessing pipelines and active learn- ing for\nmisinformation detection. Experimental results demonstrate 92% accuracy in sen- timent classification and\n87% precision in identifying misinformation across Twitter and Reddit datasets. The system is designed for \nscalability, handling millions of posts daily, and provides actionable insights for stakeholders. This work \ncontributes to the develop- ment of robust tools for real-time social media analytics.\nI. INTRODUCTION\nThe proliferation of social media has transformed communication, with platforms like Twit- ter and Reddit \ngenerating billions of posts annually. Real-time analysis of this data is critical for applications such as brand \nmonitoring, public sentiment tracking, and combating misin- formation. However, the volume, velocity, and \nvariety of social media data pose significant challenges for traditional NLP systems. This paper addresses these \nchallenges by proposing a scalable, real-time NLP framework for sentiment analysis and misinformation \ndetection.\nOur contributions include:\n\u2022 A distributed architecture for processing streaming social media data.\n\u2022 A hybrid model combining transformer-based NLP with rule-based filters for misinfor- mation detection.\n\u2022 A comprehensive evaluation on real-world datasets, demonstrating scalability and accu- racy.\nII. METHODOLOGY\nOur methodology encompasses data ingestion, preprocessing, model design, and system de- ployment. The\nframework is designed to handle high-velocity data streams while maintaining low latency.\nData Ingestion\nWe use Apache Kafka to ingest streaming data from social media APIs (e.g., Twitter Streaming API, Reddit\nPushshiftAPI). Messages are partitioned by platform and timestamp to ensure load balancing.\nPreprocessing\nRaw posts undergo tokenization, normalization, and entity recognition using spaCy. Emojis and hashtags are\npreserved as sentiment indicators. We implement a custom profanity filter to reduce noise in sentiment \nanalysis.\nModel Design\nFor sentiment analysis, we fine-tune a BERT-base model on a labeled dataset of 1M tweets, achieving robust\nperformance across positive, negative, and neutral classes. For misinformation detection, we combine BERT with \na rule-based classifier that flags posts containing known false narratives (e.g., COVID-19 myths). Active \nlearning is employed to iteratively improve the model using human annotations.\nAlgorithm 1 Sentiment Analysis and Misinformation Detection Pipeline\n1: Input: Stream of social media posts P\n2: Output: Sentiment labels S, Misinformation flags M\n3: for each post p \u2208P do\n4: p\u2032 \u2192Preprocess(p) \u25b7 Tokenization, normalization\n e-ISSN: 2582-5208 \nInternational Research Journal of Modernization in Engineering Technology and Science \n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:07/Issue:06/June-2025 Impact Factor- 8.187 www.irjmets.com \nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[5892]\n5: s \u2192BERT(p\u2032) \u25b7 Sentiment classification\n6: m \u2192RuleBased(p\u2032) + BERT(p\u2032) \u25b7 Misinformation detection\n7: S \u2192S \u222a{s}, M \u2192M \u222a{m}\n8: end for\n9: Return S, M\nSystem Architecture\nThe system is deployed on a Kubernetes cluster, with Spark for distributed processing and Redis for caching \nintermediate results. Figure 1 illustrates the architecture.\nIII. EXPERIMENTS\nTo evaluate the proposed framework, we conducted experiments on two real-world datasets, comparing our \napproach against baseline models. This section describes the datasets, evalua- tion metrics, experimental setup, \nand baseline comparisons.\n[node distance=2cm, auto] [draw, rectangle, minimum height=1cm, minimum width=2cm] (input) Social Media \nAPIs; [draw, rectangle, minimum height=1cm, minimum width=2cm, right of=input, xshift=2cm] (kafka) Kafka\nIngestion; [draw, rectangle, minimum height=1cm, minimum width=2cm, right of=kafka, xshift=2cm] (spark)\nSpark Processing; [draw, rectangle, minimum height=1cm, minimum width=2cm, above of=spark, yshift=1cm] \n(bert) BERT Models; [draw, rectangle, minimum height=1cm, minimum width=2cm, below of=spark, yshift=-\n1cm] (rules) Rule-Based Filters; [draw, rectangle, minimum height=1cm, minimum width=2cm, right of=spark, \nxshift=2cm] (redis) Redis Cache; [draw, rectangle, minimum height=1cm, minimum width=2cm, right of=redis, \nxshift=2cm] (output) Dashboard/Reports;\n[->] (input) \u2013 (kafka); [->] (kafka) \u2013 (spark); [->] (spark) \u2013 (bert); [->] (spark) \u2013 (rules); [->] (bert) \u2013 (redis); [->] \n(rules) \u2013 (redis); [->] (redis) \u2013 (output);\nFigure 1: Block diagram of the proposed NLP system for social media monitoring.\nDatasets\nWe used two datasets:\n\u2022 Twitter Dataset: 1M tweets collected in January 2025 via the Twitter Streaming API, labeled for sentiment \n(positive, negative, neutral) and misinformation (true, false). The dataset includes diverse topics, such as \npolitics, entertainment, and public health.\n\u2022 Reddit Dataset: 500K posts from Reddit, collected in February 2025 using the Pushshift API, labeled \nsimilarly. This dataset covers subreddits related to news, technology, and health.\nBoth datasets were split into 80% training, 10% validation, and 10% test sets. Misinformation labels were \nverified by cross-referencing with fact-checking databases (e.g., Snopes).\nEvaluation Metrics\nFor sentiment analysis, we used:\n\u2022 Accuracy: Percentage of correctly classified posts.\n\u2022 Precision: Ratio of correctly predicted positive/negative instances to total predicted in- stances for each \nclass.\n\u2022 F1-Score: Harmonic mean of precision and recall. For misinformation detection, we used:\n\u2022 Precision: To prioritize minimizing false positives.\n\u2022 Recall: To measure the system\u2019s ability to identify misinformation.\n\u2022 AUC-ROC: To assess overall classifier performance.\nExperimental Setup\nThe system was deployed on a Kubernetes cluster with 10 nodes (each with 16 cores, 64GB RAM). The BERT \nmodel was fine-tuned using PyTorch with a learning rate of 2e \u2212 5 for 5 epochs. Active learning iterations \ninvolved 100 human annotations per cycle. We processed 10M posts daily to test scalability, measuring latency \nand throughput.\n e-ISSN: 2582-5208 \nInternational Research Journal of Modernization in Engineering Technology and Science \n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:07/Issue:06/June-2025 Impact Factor- 8.187 www.irjmets.com \nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[5893]\nBaseline Comparisons\nWe compared our approach against:\n\u2022 VADER: A rule-based sentiment analysis tool[5].\n\u2022 TextBlob: Alexicon-based sentiment classifier.\n\u2022 RoBERTa: Atransformer-based modelwithout our custom preprocessing [6].\n\u2022 Keyword-Based Misinformation Detection: A rule-based system using known false phrases.\nOur framework outperformed baselines due to its contextual modeling and hybrid misinforma- tion dete...",
      "url": "https://www.irjmets.com/upload_newfiles/irjmets70600145799/paper_file/irjmets70600145799.pdf"
    },
    {
      "title": "RedditScore Overview \u00b6",
      "text": "RedditScore Overview &mdash; RedditScore 0.7.0 documentation\n* [Docs](#)&raquo;\n* RedditScore Overview\n* [Edit on GitHub](https://github.com/crazyfrogspb/RedditScore/blob/master/docs/source/index.rst)\n# RedditScore Overview[\u00b6](#redditscore-overview)\nRedditScore is a library that contains tools for building Reddit-based text classification models\nRedditScore includes:\n> > * > Document tokenizer with myriads of options, including Reddit- and Twitter-specific options\n> * > Tools to build and tune the most popular text classification models without any hassle\n> * > Functions to easily collect Reddit comments from Google BigQuery and Twitter data (including tweets beyond 3200 tweets limit)\n> * > Instruments to help you build more efficient Reddit-based models and to obtain RedditScores (\n[> Nikitin2018\n](#nikitin2018)> )\n> * > Tools to use pre-built Reddit-based models to obtain RedditScores for your data\n> > **Note:**RedditScore library and this tutorial are work-in-progress.[Let me know if you experience any issues](https://github.com/crazyfrogspb/RedditScore/issues).\nUsage example:\n```\nimportosimportpandasaspdfromredditscoreimporttokenizerfromredditscore.modelsimportfasttext\\_moddf=pd.read\\_csv(os.path.join(&#39;redditscore&#39;,&#39;&#39;reddit\\_small\\_sample.csv&#39;&#39;))df=df.sample(frac=1.0,random\\_state=24)# shuffling datatokenizer=CrazyTokenizer(hashtags=&#39;split&#39;)# initializing tokenizer objectX=df[&#39;body&#39;].apply(tokenizer.tokenize)# tokenizing Reddit commentsy=df[&#39;subreddit&#39;]fasttext\\_model=fasttext\\_mod.FastTextModel()# initializing fastText modelfasttext\\_model.tune\\_params(X,y,cv=5,scoring=&#39;accuracy&#39;)# tune hyperparameters of the model using default gridfasttext\\_model.fit(X,y)# fit modelfasttext\\_model.save\\_model(&#39;&#39;models/fasttext\\_model&#39;&#39;)# save modelfasttext\\_model=fasttext.load\\_model(&#39;&#39;models/fasttext\\_model&#39;&#39;)# load modeldendrogram\\_pars={&#39;&#39;leaf\\_font\\_size&#39;&#39;:14}tsne\\_pars={&#39;perplexity&#39;:30.0}fasttext\\_model.plot\\_analytics(dendrogram\\_pars=dendrogram\\_pars,# plot dendrogram and T-SNE plottsne\\_pars=tsne\\_pars,fig\\_sizes=((25,20),(22,22)))probs=fasttext\\_model.predict\\_proba(X)av\\_scores,max\\_scores=fasttext\\_model.similarity\\_scores(X)\n```\nReferences:\n[Nikitin2018]|Nikitin Evgenii, Identyifing Political Trends on Social Media Using Reddit Data, in progress|\nContents:\n* [RedditScore Overview](overview.html)\n* [Installation](installation.html)\n* [Data Collection](data_collection.html)\n* [Reddit Data](data_collection.html#reddit-data)\n* [Twitter Data](data_collection.html#twitter-data)\n* [Tokenizing](tokenizing.html)\n* [Tokenizer description](tokenizing.html#tokenizer-description)\n* [Initializing](tokenizing.html#initializing)\n* [Features](tokenizing.html#features)\n* [Lowercasing and all caps](tokenizing.html#lowercasing-and-all-caps)\n* [Normalizing](tokenizing.html#normalizing)\n* [Ignoring quotes](tokenizing.html#ignoring-quotes)\n* [Removing stop words](tokenizing.html#removing-stop-words)\n* [Word stemming and lemmatizing](tokenizing.html#word-stemming-and-lemmatizing)\n* [Removing punctuation and linebreaks](tokenizing.html#removing-punctuation-and-linebreaks)\n* [Decontracting](tokenizing.html#decontracting)\n* [Dealing with hashtags](tokenizing.html#dealing-with-hashtags)\n* [Dealing with special tokens](tokenizing.html#dealing-with-special-tokens)\n* [URLs](tokenizing.html#urls)\n* [Extra patterns and keeping untokenized](tokenizing.html#extra-patterns-and-keeping-untokenized)\n* [Converting whitespaces to underscores](tokenizing.html#converting-whitespaces-to-underscores)\n* [Removing non-unicode characters](tokenizing.html#removing-non-unicode-characters)\n* [Emojis](tokenizing.html#emojis)\n* [Unicode and hex characters](tokenizing.html#unicode-and-hex-characters)\n* [n-grams](tokenizing.html#n-grams)\n* [Modelling](modelling.html)\n* [Fitting models](modelling.html#fitting-models)\n* [Model persistence](modelling.html#model-persistence)\n* [Predictions and similarity scores](modelling.html#predictions-and-similarity-scores)\n* [Model tuning and validation](modelling.html#model-tuning-and-validation)\n* [Visualization of the class embeddings](modelling.html#visualization-of-the-class-embeddings)\n* [API Documentation](apis/api_main.html)\n* [CrazyTokenizer](apis/tokenizer.html)\n* [Models](apis/models.html)\n* [BoWModel](apis/bow_mod.html)\n* [FastTextModel](apis/fasttext_mod.html)\n* [Neural networks](apis/nn_mod.html)\n# Indices and tables[\u00b6](#indices-and-tables)\n* [Index](genindex.html)\n* [Module Index](py-modindex.html)\n* [Search Page](search.html)",
      "url": "https://redditscore.readthedocs.io/en/master"
    },
    {
      "title": "The Pushshift Reddit Dataset",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2001.08435"
    }
  ]
}