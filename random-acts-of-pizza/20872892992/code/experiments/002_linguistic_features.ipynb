{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a8566e",
   "metadata": {},
   "source": [
    "# Experiment 002: Linguistic Features from Stanford Research\n",
    "\n",
    "This notebook implements linguistic features based on Stanford ICWSM 2014 paper analyzing this exact dataset.\n",
    "\n",
    "**Features engineered:**\n",
    "- Gratitude indicators (thank, thanks, grateful, appreciate)\n",
    "- Evidentiality markers (URLs, numbers, evidence words)\n",
    "- Reciprocity language (pay it forward, return the favor, etc.)\n",
    "- Narrative indicators (length, pronouns, sentence count)\n",
    "- Politeness markers (please, polite phrasing)\n",
    "- Interaction features from meta data\n",
    "\n",
    "**Expected improvement:** +0.05 to +0.10 AUC over baseline (target: ~0.72-0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c7f34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:00.446410Z",
     "iopub.status.busy": "2026-01-10T05:59:00.446193Z",
     "iopub.status.idle": "2026-01-10T05:59:01.428493Z",
     "shell.execute_reply": "2026-01-10T05:59:01.428044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (2878, 32)\n",
      "Test shape: (1162, 17)\n",
      "Target distribution: {False: 2163, True: 715}\n",
      "Positive rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].value_counts().to_dict()}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac536fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.431571Z",
     "iopub.status.busy": "2026-01-10T05:59:01.431213Z",
     "iopub.status.idle": "2026-01-10T05:59:01.434333Z",
     "shell.execute_reply": "2026-01-10T05:59:01.434000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 meta features\n"
     ]
    }
   ],
   "source": [
    "# Define meta features (same as baseline)\n",
    "meta_features = [\n",
    "    # User activity (at_request only)\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_comments_in_raop_at_request',\n",
    "    'requester_number_of_posts_on_raop_at_request',\n",
    "    'requester_number_of_subreddits_at_request',\n",
    "    \n",
    "    # Vote counts (at_request only)\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    \n",
    "    # Temporal features\n",
    "    'unix_timestamp_of_request',\n",
    "    'unix_timestamp_of_request_utc',\n",
    "    \n",
    "    # Account age\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_days_since_first_post_on_raop_at_request'\n",
    "]\n",
    "\n",
    "print(f\"Using {len(meta_features)} meta features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e4b47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.435654Z",
     "iopub.status.busy": "2026-01-10T05:59:01.435390Z",
     "iopub.status.idle": "2026-01-10T05:59:01.443061Z",
     "shell.execute_reply": "2026-01-10T05:59:01.442729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering temporal features...\n",
      "Total features so far: 13\n"
     ]
    }
   ],
   "source": [
    "# Engineer temporal features (same as baseline)\n",
    "print(\"Engineering temporal features...\")\n",
    "\n",
    "train_df['request_datetime'] = pd.to_datetime(train_df['unix_timestamp_of_request_utc'], unit='s')\n",
    "test_df['request_datetime'] = pd.to_datetime(test_df['unix_timestamp_of_request_utc'], unit='s')\n",
    "\n",
    "# Extract hour and day of week\n",
    "train_df['request_hour'] = train_df['request_datetime'].dt.hour\n",
    "test_df['request_hour'] = test_df['request_datetime'].dt.hour\n",
    "\n",
    "train_df['request_dayofweek'] = train_df['request_datetime'].dt.dayofweek\n",
    "test_df['request_dayofweek'] = test_df['request_datetime'].dt.dayofweek\n",
    "\n",
    "# Add engineered features to feature list\n",
    "engineered_features = ['request_hour', 'request_dayofweek']\n",
    "all_features = meta_features + engineered_features\n",
    "\n",
    "print(f\"Total features so far: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45888f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.444974Z",
     "iopub.status.busy": "2026-01-10T05:59:01.444732Z",
     "iopub.status.idle": "2026-01-10T05:59:01.806645Z",
     "shell.execute_reply": "2026-01-10T05:59:01.806317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering linguistic features from Stanford research...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 17 linguistic features\n",
      "Total features: 30\n"
     ]
    }
   ],
   "source": [
    "# ENGINEER LINGUISTIC FEATURES BASED ON STANFORD RESEARCH\n",
    "print(\"Engineering linguistic features from Stanford research...\")\n",
    "\n",
    "def engineer_linguistic_features(df):\n",
    "    \"\"\"Engineer linguistic features based on Stanford ICWSM 2014 paper\"\"\"\n",
    "    \n",
    "    # Combine title and text for analysis\n",
    "    df['combined_text'] = df['request_title'].fillna('') + ' ' + df['request_text_edit_aware'].fillna('')\n",
    "    \n",
    "    # 1. GRATITUDE INDICATORS\n",
    "    gratitude_words = ['thank', 'thanks', 'grateful', 'appreciate', 'appreciation', 'gratitude']\n",
    "    df['gratitude_count'] = df['combined_text'].str.lower().apply(\n",
    "        lambda x: sum(1 for word in gratitude_words if word in x)\n",
    "    )\n",
    "    \n",
    "    # 2. EVIDENTIALITY MARKERS\n",
    "    # URLs, numbers, specific details\n",
    "    df['has_url'] = df['combined_text'].str.contains('http|www|\\.com|\\.org|\\.net', case=False, regex=True).astype(int)\n",
    "    df['number_count'] = df['combined_text'].str.count(r'\\d+')\n",
    "    \n",
    "    evidence_words = ['proof', 'photo', 'picture', 'link', 'show', 'demonstrate', 'evidence']\n",
    "    df['evidence_word_count'] = df['combined_text'].str.lower().apply(\n",
    "        lambda x: sum(1 for word in evidence_words if word in x)\n",
    "    )\n",
    "    \n",
    "    # 3. RECIPROCITY LANGUAGE\n",
    "    reciprocity_phrases = [\n",
    "        'pay it forward', 'return the favor', 'help others', 'give back',\n",
    "        'help someone else', 'pass it on', 'when i can', 'once i get'\n",
    "    ]\n",
    "    df['reciprocity_count'] = df['combined_text'].str.lower().apply(\n",
    "        lambda x: sum(1 for phrase in reciprocity_phrases if phrase in x)\n",
    "    )\n",
    "    \n",
    "    # 4. NARRATIVE INDICATORS\n",
    "    # Length features\n",
    "    df['title_length'] = df['request_title'].fillna('').str.len()\n",
    "    df['text_length'] = df['request_text_edit_aware'].fillna('').str.len()\n",
    "    df['total_length'] = df['title_length'] + df['text_length']\n",
    "    \n",
    "    # First-person pronoun count\n",
    "    first_person_pronouns = ['i', 'me', 'my', 'we', 'our', 'us']\n",
    "    df['first_person_pronoun_count'] = df['combined_text'].str.lower().apply(\n",
    "        lambda x: sum(1 for word in first_person_pronouns if re.search(r'\\b' + word + r'\\b', x))\n",
    "    )\n",
    "    \n",
    "    # Sentence count (periods, exclamation marks, question marks)\n",
    "    df['sentence_count'] = df['combined_text'].str.count(r'[.!?]+')\n",
    "    \n",
    "    # Paragraph breaks (double newlines)\n",
    "    df['paragraph_breaks'] = df['combined_text'].str.count(r'\\n\\n')\n",
    "    \n",
    "    # 5. POLITENESS MARKERS\n",
    "    df['please_count'] = df['combined_text'].str.lower().str.count(r'\\bplease\\b')\n",
    "    \n",
    "    # 6. NARRATIVE CATEGORY (simple keyword-based classification)\n",
    "    # Based on Stanford's 5 narrative types: desire, family, job, money, student\n",
    "    \n",
    "    narrative_keywords = {\n",
    "        'family': ['family', 'kid', 'child', 'children', 'mom', 'dad', 'parent', 'brother', 'sister', 'wife', 'husband'],\n",
    "        'job': ['job', 'work', 'employ', 'unemployed', 'laid off', 'fired', 'interview', 'hire'],\n",
    "        'money': ['money', 'broke', 'poor', 'bills', 'rent', 'paycheck', 'debt', 'financial'],\n",
    "        'student': ['student', 'school', 'college', 'university', 'class', 'tuition', 'textbook', 'dorm'],\n",
    "        'desire': ['craving', 'want', 'wish', 'hope', 'desire', 'hungry', 'starving', 'appetite']\n",
    "    }\n",
    "    \n",
    "    for category, keywords in narrative_keywords.items():\n",
    "        df[f'narrative_{category}'] = df['combined_text'].str.lower().apply(\n",
    "            lambda x: sum(1 for word in keywords if word in x)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to both train and test\n",
    "train_df = engineer_linguistic_features(train_df)\n",
    "test_df = engineer_linguistic_features(test_df)\n",
    "\n",
    "# Add all linguistic features to feature list\n",
    "linguistic_features = [\n",
    "    'gratitude_count', 'has_url', 'number_count', 'evidence_word_count', 'reciprocity_count',\n",
    "    'title_length', 'text_length', 'total_length', 'first_person_pronoun_count',\n",
    "    'sentence_count', 'paragraph_breaks', 'please_count',\n",
    "    'narrative_family', 'narrative_job', 'narrative_money', 'narrative_student', 'narrative_desire'\n",
    "]\n",
    "\n",
    "all_features.extend(linguistic_features)\n",
    "\n",
    "print(f\"Added {len(linguistic_features)} linguistic features\")\n",
    "print(f\"Total features: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f93d679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.807718Z",
     "iopub.status.busy": "2026-01-10T05:59:01.807455Z",
     "iopub.status.idle": "2026-01-10T05:59:01.815547Z",
     "shell.execute_reply": "2026-01-10T05:59:01.815229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering interaction features...\n",
      "Added 7 interaction features\n",
      "Total features: 37\n"
     ]
    }
   ],
   "source": [
    "# ENGINEER INTERACTION FEATURES\n",
    "print(\"Engineering interaction features...\")\n",
    "\n",
    "def engineer_interaction_features(df):\n",
    "    \"\"\"Create interaction features from meta data\"\"\"\n",
    "    \n",
    "    # 1. Activity ratios\n",
    "    df['comments_per_day'] = df['requester_number_of_comments_at_request'] / (df['requester_account_age_in_days_at_request'] + 1)\n",
    "    df['posts_per_day'] = df['requester_number_of_posts_at_request'] / (df['requester_account_age_in_days_at_request'] + 1)\n",
    "    \n",
    "    # 2. Comment to post ratio\n",
    "    df['comment_to_post_ratio'] = df['requester_number_of_comments_at_request'] / (df['requester_number_of_posts_at_request'] + 1)\n",
    "    \n",
    "    # 3. Activity score (weighted combination)\n",
    "    df['activity_score'] = (\n",
    "        df['requester_number_of_comments_at_request'] * 0.3 +\n",
    "        df['requester_number_of_posts_at_request'] * 0.5 +\n",
    "        df['requester_upvotes_minus_downvotes_at_request'] * 0.2\n",
    "    )\n",
    "    \n",
    "    # 4. Subreddit diversity\n",
    "    df['subreddit_diversity'] = df['requester_number_of_subreddits_at_request'] / (\n",
    "        df['requester_number_of_comments_at_request'] + df['requester_number_of_posts_at_request'] + 1\n",
    "    )\n",
    "    \n",
    "    # 5. RAOP experience ratio\n",
    "    df['raop_experience'] = df['requester_days_since_first_post_on_raop_at_request'] / (\n",
    "        df['requester_account_age_in_days_at_request'] + 1\n",
    "    )\n",
    "    \n",
    "    # 6. Vote efficiency\n",
    "    df['vote_efficiency'] = df['requester_upvotes_minus_downvotes_at_request'] / (\n",
    "        df['requester_upvotes_plus_downvotes_at_request'] + 1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to both train and test\n",
    "train_df = engineer_interaction_features(train_df)\n",
    "test_df = engineer_interaction_features(test_df)\n",
    "\n",
    "# Add interaction features to feature list\n",
    "interaction_features = [\n",
    "    'comments_per_day', 'posts_per_day', 'comment_to_post_ratio',\n",
    "    'activity_score', 'subreddit_diversity', 'raop_experience', 'vote_efficiency'\n",
    "]\n",
    "\n",
    "all_features.extend(interaction_features)\n",
    "\n",
    "print(f\"Added {len(interaction_features)} interaction features\")\n",
    "print(f\"Total features: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ceac3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.816318Z",
     "iopub.status.busy": "2026-01-10T05:59:01.816223Z",
     "iopub.status.idle": "2026-01-10T05:59:01.832750Z",
     "shell.execute_reply": "2026-01-10T05:59:01.832425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for modeling...\n",
      "X shape: (2878, 37)\n",
      "y shape: (2878,)\n",
      "X_test shape: (1162, 37)\n",
      "Missing values in training: 0\n",
      "Missing values in test: 0\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"Preparing data for modeling...\")\n",
    "\n",
    "X = train_df[all_features].copy()\n",
    "y = train_df['requester_received_pizza'].astype(int).values\n",
    "X_test = test_df[all_features].copy()\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Handle any missing values (fill with median)\n",
    "for col in all_features:\n",
    "    median_val = X[col].median()\n",
    "    X[col].fillna(median_val, inplace=True)\n",
    "    X_test[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(f\"Missing values in training: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f88ef07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.833800Z",
     "iopub.status.busy": "2026-01-10T05:59:01.833559Z",
     "iopub.status.idle": "2026-01-10T05:59:01.836524Z",
     "shell.execute_reply": "2026-01-10T05:59:01.836204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 5-fold stratified CV...\n",
      "Total features: 37\n",
      "Model parameters: {'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt', 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': -1, 'random_state': 42, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Model parameters (same as baseline for fair comparison)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'class_weight': 'balanced'  # Handle class imbalance\n",
    "}\n",
    "\n",
    "print(f\"Training with {n_folds}-fold stratified CV...\")\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "print(f\"Model parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd4a8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:01.837392Z",
     "iopub.status.busy": "2026-01-10T05:59:01.837154Z",
     "iopub.status.idle": "2026-01-10T05:59:38.151214Z",
     "shell.execute_reply": "2026-01-10T05:59:38.150681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[22]\tval's auc: 0.682755\n",
      "Fold 1 ROC-AUC: 0.6828\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[85]\tval's auc: 0.675043\n",
      "Fold 2 ROC-AUC: 0.6750\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[89]\tval's auc: 0.719311\n",
      "Fold 3 ROC-AUC: 0.7193\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tval's auc: 0.659042\n",
      "Fold 4 ROC-AUC: 0.6590\n",
      "\n",
      "Fold 5/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tval's auc: 0.671353\n",
      "Fold 5 ROC-AUC: 0.6714\n",
      "\n",
      "==================================================\n",
      "Cross-Validation ROC-AUC: 0.6815 ± 0.0204\n",
      "Individual folds: ['0.6828', '0.6750', '0.7193', '0.6590', '0.6714']\n",
      "OOF ROC-AUC: 0.6759\n",
      "Improvement over baseline: +0.0124\n"
     ]
    }
   ],
   "source": [
    "# Train with cross-validation\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        valid_names=['val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store predictions\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = roc_auc_score(y_val, val_pred)\n",
    "    fold_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} ROC-AUC: {fold_score:.4f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_score = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Cross-Validation ROC-AUC: {cv_score:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in fold_scores]}\")\n",
    "\n",
    "# OOF score\n",
    "oof_score = roc_auc_score(y, oof_predictions)\n",
    "print(f\"OOF ROC-AUC: {oof_score:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "baseline_score = 0.6691\n",
    "improvement = cv_score - baseline_score\n",
    "print(f\"Improvement over baseline: {improvement:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8c607a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:38.153096Z",
     "iopub.status.busy": "2026-01-10T05:59:38.152847Z",
     "iopub.status.idle": "2026-01-10T05:59:38.161618Z",
     "shell.execute_reply": "2026-01-10T05:59:38.161160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features by importance:\n",
      "                                           feature  importance\n",
      "                         unix_timestamp_of_request  913.936492\n",
      "                                      total_length  352.291242\n",
      "                                       text_length  287.649543\n",
      "                                    activity_score  275.167849\n",
      "                                      title_length  262.533881\n",
      "                                   vote_efficiency  246.786720\n",
      "                                 request_dayofweek  222.473410\n",
      "requester_days_since_first_post_on_raop_at_request  216.778719\n",
      "          requester_account_age_in_days_at_request  195.461360\n",
      "       requester_upvotes_plus_downvotes_at_request  193.962250\n",
      "                                    sentence_count  174.718747\n",
      "                             comment_to_post_ratio  172.581781\n",
      "         requester_number_of_subreddits_at_request  169.673309\n",
      "                               subreddit_diversity  164.409842\n",
      "                                      request_hour  135.779680\n",
      "                                     posts_per_day  133.504019\n",
      "                                  comments_per_day  128.820019\n",
      "   requester_number_of_comments_in_raop_at_request  126.637761\n",
      "                     unix_timestamp_of_request_utc  121.052069\n",
      "      requester_upvotes_minus_downvotes_at_request  112.856059\n",
      "\n",
      "==================================================\n",
      "Feature importance by category:\n",
      "Meta features: 2686.40 (51.1%)\n",
      "Linguistic features: 1446.38 (27.5%)\n",
      "Interaction features: 1121.27 (21.3%)\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 features by importance:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Categorize features by type\n",
    "meta_importance = importance_df[importance_df['feature'].isin(meta_features + engineered_features)]['importance'].sum()\n",
    "linguistic_importance = importance_df[importance_df['feature'].isin(linguistic_features)]['importance'].sum()\n",
    "interaction_importance = importance_df[importance_df['feature'].isin(interaction_features)]['importance'].sum()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Feature importance by category:\")\n",
    "print(f\"Meta features: {meta_importance:.2f} ({meta_importance/importance_df['importance'].sum()*100:.1f}%)\")\n",
    "print(f\"Linguistic features: {linguistic_importance:.2f} ({linguistic_importance/importance_df['importance'].sum()*100:.1f}%)\")\n",
    "print(f\"Interaction features: {interaction_importance:.2f} ({interaction_importance/importance_df['importance'].sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0959add",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T05:59:38.162546Z",
     "iopub.status.busy": "2026-01-10T05:59:38.162379Z",
     "iopub.status.idle": "2026-01-10T05:59:38.169949Z",
     "shell.execute_reply": "2026-01-10T05:59:38.169515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Submission saved to: /home/submission/submission_002_linguistic_features.csv\n",
      "Submission shape: (1162, 2)\n",
      "Prediction range: [0.0934, 0.6608]\n",
      "Mean prediction: 0.2409\n",
      "\n",
      "==================================================\n",
      "EXPERIMENT SUMMARY:\n",
      "Model: LightGBM with linguistic features\n",
      "Features: 37 total\n",
      "  - Meta features: 13\n",
      "  - Linguistic features: 17\n",
      "  - Interaction features: 7\n",
      "CV ROC-AUC: 0.6815 ± 0.0204\n",
      "Improvement over baseline: +0.0124\n"
     ]
    }
   ],
   "source": [
    "# Save predictions for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "submission_path = '/home/submission/submission_002_linguistic_features.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "print(f\"Mean prediction: {test_predictions.mean():.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EXPERIMENT SUMMARY:\")\n",
    "print(f\"Model: LightGBM with linguistic features\")\n",
    "print(f\"Features: {len(all_features)} total\")\n",
    "print(f\"  - Meta features: {len(meta_features + engineered_features)}\")\n",
    "print(f\"  - Linguistic features: {len(linguistic_features)}\")\n",
    "print(f\"  - Interaction features: {len(interaction_features)}\")\n",
    "print(f\"CV ROC-AUC: {cv_score:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Improvement over baseline: {improvement:+.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
