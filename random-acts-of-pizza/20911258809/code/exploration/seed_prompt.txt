## Data Understanding

**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: 2,878 training samples, 24.8% positive class (significant imbalance), text features (request_text, request_title) with mean lengths of 403 and 72 characters respectively, rich metadata about user activity
- Test set contains only features available at request time ("_at_request" suffix), while train has additional retrieval-time features
- Key challenge: Class imbalance (approx 1:3 ratio) and combining text with tabular metadata

## Models

For text classification with metadata on Kaggle, winning approaches typically use:

**Primary Models:**
- Gradient boosting (LightGBM/XGBoost) on engineered features - fast, strong baseline for tabular + text features
- Fine-tuned transformers (BERT/RoBERTa) for text understanding, especially effective for social media text
- Neural networks combining text embeddings with metadata through concatenation

**Model Selection Guidelines:**
- Use LightGBM/XGBoost when you have strong feature engineering and limited compute
- Use transformers when text semantics are critical and you have sufficient compute
- For best results: Ensemble 3-5 diverse models (e.g., transformer + gradient boosting + neural net)

## Preprocessing & Feature Engineering

**Text Processing:**
- Clean Reddit-specific noise: URLs, HTML tags, emojis, special characters
- Use both request_text and request_title; consider request_text_edit_aware to remove success indicators
- TF-IDF or count-based features work well for classical models
- For transformers: minimal preprocessing (lowercase, basic cleaning) is often sufficient

**Metadata Features:**
- Account activity metrics (comments, posts, upvotes) are strong predictors
- Account age and RAOP-specific activity (days_since_first_post_on_raop) are valuable
- Subreddit diversity (requester_number_of_subreddits) may indicate user engagement patterns
- Handle missing values carefully - many fields have missing data

**Combining Text and Metadata:**
Two effective approaches:
1. **Concatenation approach**: Join text fields with metadata as strings, feed to transformer
2. **Separate embeddings**: Text embedding from transformer + processed metadata â†’ concatenated classifier

## Handling Class Imbalance

**Critical for this competition (24.8% positive rate):**
- Use class_weight='balanced' or equivalent in models
- Consider focal loss or other cost-sensitive approaches
- Oversampling minority class (SMOTE or simple duplication) can help
- Monitor AUC-ROC rather than accuracy for validation
- Stratified K-fold (k=5) essential to maintain class distribution

## Validation Strategy

**Essential approaches:**
- Stratified K-fold (k=5) to handle class imbalance
- Time-based splits if temporal patterns exist (check timestamps in eda.ipynb)
- Use AUC-ROC for validation metric (matches competition evaluation)
- Early stopping on validation AUC to prevent overfitting

## Ensembling

**Winning ensemble strategies:**
- Stack 3-5 diverse models (e.g., BERT + LightGBM + Neural Net + Logistic Regression)
- Use out-of-fold predictions for stacking to avoid leakage
- Simple averaging can work but stacking often performs better for heterogeneous models
- Weight models by validation performance

## Optimization

**Hyperparameter tuning:**
- Bayesian optimization (Optuna) effective for tree models and neural nets
- Learning rate scheduling important for transformers
- For LightGBM: focus on num_leaves, learning_rate, min_child_samples
- For transformers: focus on learning_rate, batch_size, warmup_steps