## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution (24.8% positive), correlations, and text statistics
- Key finding: `requester_user_flair` is highly predictive (shroom/PIF have 100% success) but has 75% missing values
- Text features: request_text (mean 403 chars), request_title (mean 72 chars)
- Strongest numerical correlation: request_number_of_comments_at_retrieval (0.29)

## Models
For text classification with metadata features, winning Kaggle solutions typically use:

**Primary Approaches:**
- Gradient boosting (XGBoost/LightGBM/CatBoost) on engineered features
- Transformers (BERT/RoBERTa) for text understanding combined with metadata
- Logistic regression with TF-IDF features as strong baseline

**Model Selection Guidelines:**
- Use tree models when feature engineering is strong and dataset is small (<5K samples)
- Use transformers when text semantics are important and compute allows
- Consider hybrid approaches: transformer embeddings + gradient boosting

**Class Imbalance Handling:**
- Weighted loss functions (focal loss, weighted cross-entropy)
- Class weights inversely proportional to class frequencies
- Oversampling minority class or undersampling majority class
- For severe imbalance (like 25% positive), use class_weight='balanced' in sklearn or scale_pos_weight in XGBoost

## Feature Engineering

**Text Features:**
- TF-IDF vectors (unigrams, bigrams) - proven effective in original competition
- Text length statistics (char count, word count, sentence count)
- Sentiment analysis scores
- Presence of specific keywords ("please", "thank", "help", "family", "kids")
- Punctuation patterns (exclamation marks, question marks)
- Readability scores (Flesch-Kincaid)
- Named entity recognition (locations, amounts)

**Metadata Features:**
- User activity metrics (posts, comments, karma) - log transforms help
- Account age (days) - log transform
- Subreddit diversity (number of unique subreddits)
- Time-based features (hour of day, day of week)
- Vote ratios (upvotes/downvotes)
- Requester flair encoded as categorical (None/shroom/PIF)

**Feature Combination:**
- Interaction terms between text length and user activity
- Ratio features (comments/posts, karma/age)
- Polynomial features for strong individual predictors

## Preprocessing

**Text Preprocessing:**
- Lowercase conversion
- Remove special characters but keep punctuation for sentiment
- Handle edited text using request_text_edit_aware field
- Remove Reddit-specific markup
- Lemmatization or stemming optional (test both)

**Numerical Preprocessing:**
- Log transformation for skewed distributions (karma, comment counts)
- Standardization for neural networks
- No scaling needed for tree models
- Handle missing values in flair with "Unknown" category

**Categorical Encoding:**
- Target encoding for high-cardinality features (username, subreddits)
- One-hot encoding for low-cardinality features (flair)
- Frequency encoding for usernames and subreddits

## Validation Strategy

**Cross-Validation:**
- Stratified K-Fold (k=5) to preserve class distribution
- Time-based splits if temporal patterns exist (check timestamps)
- Group K-Fold by requester to prevent leakage if same users appear multiple times

**Evaluation:**
- Primary metric: ROC-AUC (as per competition)
- Also monitor PR-AUC for imbalanced problems
- Use early stopping on validation AUC

## Ensembling

**Approaches:**
- Weighted average of diverse models (TF-IDF + XGBoost, BERT, metadata-only model)
- Stacking with logistic regression meta-learner
- Bagging ensembles of same model type with different seeds

**Model Diversity:**
- Different feature sets (text-only, metadata-only, combined)
- Different algorithms (trees, linear, neural)
- Different preprocessing pipelines

## Optimization

**Hyperparameter Tuning:**
- Bayesian optimization (Optuna, Hyperopt) for efficient search
- Random search for tree models
- Learning rate scheduling for transformers
- Focus on: max_depth/min_child_samples for trees, learning_rate, subsample/colsample

**Training Tips:**
- Early stopping based on validation AUC
- Use class weights to handle imbalance
- For transformers: gradual unfreezing, discriminative learning rates
- Ensemble 3-5 models typically sufficient