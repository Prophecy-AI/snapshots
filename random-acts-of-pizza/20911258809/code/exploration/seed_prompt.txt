## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution (24.8% positive), correlations, and text statistics
- Key finding: `requester_user_flair` is highly predictive (shroom/PIF have 100% success) but has 75% missing values
- Text features: request_text (mean 403 chars), request_title (mean 72 chars)
- Strongest numerical correlation: request_number_of_comments_at_retrieval (0.29)

## Models
For text classification with metadata features, winning Kaggle solutions typically use:

**Primary Approaches:**
- Gradient boosting (XGBoost/LightGBM/CatBoost) on engineered features
- Transformers (BERT/RoBERTa) for text understanding combined with metadata
- Logistic regression with TF-IDF features as strong baseline

**Model Selection Guidelines:**
- Use tree models when feature engineering is strong and dataset is small (<5K samples)
- Use transformers when text semantics are important and compute allows
- Consider hybrid approaches: transformer embeddings + gradient boosting

**Class Imbalance Handling:**
- Weighted loss functions (focal loss, weighted cross-entropy)
- Class weights inversely proportional to class frequencies
- Oversampling minority class or undersampling majority class
- For severe imbalance (like 25% positive), use class_weight='balanced' in sklearn or scale_pos_weight in XGBoost

## Feature Engineering

**Text Features (Based on Research on Reddit Requests):**
- TF-IDF vectors (unigrams, bigrams) - proven effective in original competition
- **Explicit need expressions**: Clear narrative describing the problem/situation
- **Gratitude language**: Presence of "thank you", "appreciate", "grateful"
- **Evidentiality**: Concrete evidence, photos, links mentioned
- **Reciprocity**: Offers to "pay it forward", "help others", future promises
- **Text length statistics**: char count, word count, sentence count (longer posts often more successful)
- **Readability scores**: Flesch-Kincaid, ease of reading
- **Sentiment analysis**: Positive/negative sentiment scores
- **Specific keywords**: "please", "help", "family", "kids", "hungry", "broke", "bills"
- **Punctuation patterns**: Exclamation marks, question marks, ellipsis
- **Named entity recognition**: Locations, monetary amounts, time periods
- **LIWC categories**: Psychological and linguistic dimensions

**Metadata Features:**
- User activity metrics (posts, comments, karma) - log transforms help with skewness
- Account age (days) - log transform
- Subreddit diversity (number of unique subreddits)
- Time-based features (hour of day, day of week, weekend vs weekday)
- Vote ratios (upvotes/downvotes) - normalize by total activity
- Requester flair encoded as categorical (None/shroom/PIF) - handle 75% missing as "Unknown"
- Request statistics (comment count, vote counts at retrieval)
- **Social status indicators**: High karma, older accounts, diverse subreddit participation

**Feature Combination:**
- Interaction terms between text length and user activity
- Ratio features (comments/posts, karma/age, upvotes/total_votes)
- Polynomial features for strong individual predictors
- Text-metadata interactions (e.g., long text + high karma)

## Preprocessing

**Text Preprocessing:**
- Lowercase conversion
- Remove special characters but keep punctuation for sentiment analysis
- Handle edited text using request_text_edit_aware field (removes success indicators)
- Remove Reddit-specific markup (usernames, subreddit links)
- Lemmatization or stemming optional (test both, often not needed for TF-IDF)
- Preserve emotional content and emphasis (exclamation marks, caps)

**Numerical Preprocessing:**
- Log transformation for skewed distributions (karma, comment counts, post counts)
- Standardization for neural networks
- No scaling needed for tree models
- Handle missing values in flair with "Unknown" category
- Clip extreme outliers (e.g., karma > 99th percentile)

**Categorical Encoding:**
- Target encoding for high-cardinality features (username, subreddits) - be careful of leakage
- One-hot encoding for low-cardinality features (flair)
- Frequency encoding for usernames and subreddits
- Hashing trick for very high cardinality features

## Validation Strategy

**Cross-Validation:**
- Stratified K-Fold (k=5) to preserve class distribution (24.8% positive)
- Time-based splits if temporal patterns exist (check unix timestamps)
- Group K-Fold by requester to prevent leakage if same users appear multiple times
- Use same splits across all model iterations for fair comparison

**Evaluation:**
- Primary metric: ROC-AUC (as per competition)
- Also monitor PR-AUC for imbalanced problems
- Use early stopping on validation AUC to prevent overfitting
- Track confusion matrix to understand false positive/negative rates

## Ensembling

**Approaches:**
- Weighted average of diverse models (TF-IDF + XGBoost, BERT, metadata-only model)
- Stacking with logistic regression meta-learner
- Bagging ensembles of same model type with different seeds
- Rank averaging for robustness against outliers

**Model Diversity:**
- Different feature sets (text-only, metadata-only, combined)
- Different algorithms (trees, linear, neural)
- Different preprocessing pipelines
- Different validation splits

**Weighting Strategy:**
- Weight by validation performance
- Use simple average for first iteration
- Optimize weights via linear regression on validation predictions

## Optimization

**Hyperparameter Tuning:**
- Bayesian optimization (Optuna, Hyperopt) for efficient search
- Random search for tree models (often as good as grid search)
- Learning rate scheduling for transformers
- Focus on: max_depth/min_child_samples for trees, learning_rate, subsample/colsample_bytree

**Training Tips:**
- Early stopping based on validation AUC (patience=50-100 rounds)
- Use class weights to handle imbalance (scale_pos_weight ~ 3 for 25% positive)
- For transformers: gradual unfreezing, discriminative learning rates
- Ensemble 3-5 models typically sufficient (diminishing returns beyond 5)
- Save OOF (out-of-fold) predictions for stacking

**Computational Efficiency:**
- Use TF-IDF + linear models for quick baseline
- Use gradient boosting for strong performance with reasonable compute
- Reserve transformers for final ensembles if compute allows
- Parallelize feature engineering where possible