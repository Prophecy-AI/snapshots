{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47024c8",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: Analysis of exp_003 Results\n",
    "\n",
    "## Goal\n",
    "Analyze the successful TF-IDF fixes from exp_003 and identify next steps to reach gold threshold (0.979080).\n",
    "\n",
    "Current best: 0.6555 AUC (exp_003)\n",
    "Gold threshold: 0.979080\n",
    "Gap: 0.3236 points\n",
    "\n",
    "## Key Findings from exp_003\n",
    "- Character n-grams dominate feature importance (4 of top 10 features)\n",
    "- Feature selection worked: 3,000 features better than 10,000\n",
    "- Removed stop words to keep domain vocabulary\n",
    "- CV improved from 0.6217 → 0.6555 (+0.0338)\n",
    "- Low variance (±0.0104) indicates stable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train = pd.DataFrame(train_data)\n",
    "test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train: {len(train)} samples, {sum(train['requester_received_pizza'])} positive ({sum(train['requester_received_pizza'])/len(train):.3f})\")\n",
    "print(f\"Test: {len(test)} samples\")\n",
    "\n",
    "# Extract text\n",
    "y = train['requester_received_pizza'].values\n",
    "text_train = train['request_text_edit_aware'].fillna('').str.lower()\n",
    "text_test = test['request_text_edit_aware'].fillna('').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3331f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character n-gram patterns from exp_003\n",
    "# We need to understand what the top character n-grams represent\n",
    "\n",
    "# Recreate the char n-gram vectorizer used in exp_003\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=2000,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "print(\"Fitting character n-gram vectorizer...\")\n",
    "X_char_train = char_vectorizer.fit_transform(text_train)\n",
    "X_char_test = char_vectorizer.transform(text_test)\n",
    "\n",
    "char_feature_names = char_vectorizer.get_feature_names_out()\n",
    "print(f\"Character n-grams shape: {X_char_train.shape}\")\n",
    "print(f\"Top 20 character n-grams:\")\n",
    "for i, name in enumerate(char_feature_names[:20]):\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "\n",
    "# Let's see what these n-grams actually correspond to in the text\n",
    "# by finding examples where they appear\n",
    "\n",
    "def find_ngram_examples(text_series, ngram, n_examples=3):\n",
    "    \"\"\"Find examples of texts containing a specific n-gram\"\"\"\n",
    "    examples = []\n",
    "    for idx, text in enumerate(text_series):\n",
    "        if ngram in text:\n",
    "            # Find the context around the n-gram\n",
    "            pos = text.find(ngram)\n",
    "            start = max(0, pos - 50)\n",
    "            end = min(len(text), pos + len(ngram) + 50)\n",
    "            context = text[start:end].replace('\\n', ' ')\n",
    "            examples.append(context)\n",
    "            if len(examples) >= n_examples:\n",
    "                break\n",
    "    return examples\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYZING TOP CHARACTER N-GRAMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze the top 10 character n-grams\n",
    "for i in range(min(10, len(char_feature_names))):\n",
    "    ngram = char_feature_names[i]\n",
    "    print(f\"\\n{i+1:2d}. '{ngram}'\")\n",
    "    \n",
    "    # Find examples in successful and failed requests\n",
    "    success_examples = find_ngram_examples(\n",
    "        text_train[y == 1], ngram, n_examples=2\n",
    "    )\n",
    "    fail_examples = find_ngram_examples(\n",
    "        text_train[y == 0], ngram, n_examples=2\n",
    "    )\n",
    "    \n",
    "    if success_examples:\n",
    "        print(f\"   In SUCCESSFUL requests:\")\n",
    "        for ex in success_examples:\n",
    "            print(f\"      - ...{ex}...\")\n",
    "    \n",
    "    if fail_examples:\n",
    "        print(f\"   In FAILED requests:\")\n",
    "        for ex in fail_examples:\n",
    "            print(f\"      - ...{ex}...\")\n",
    "    \n",
    "    # Count frequency\n",
    "    success_count = sum(text_train[y == 1].str.contains(ngram, na=False))\n",
    "    fail_count = sum(text_train[y == 0].str.contains(ngram, na=False))\n",
    "    success_rate = success_count / len(text_train[y == 1]) if len(text_train[y == 1]) > 0 else 0\n",
    "    fail_rate = fail_count / len(text_train[y == 0]) if len(text_train[y == 0]) > 0 else 0\n",
    "    \n",
    "    print(f\"   Frequency: {success_count}/{len(text_train[y == 1])} ({success_rate:.3f}) in successes\")\n",
    "    print(f\"   Frequency: {fail_count}/{len(text_train[y == 0])} ({fail_rate:.3f}) in failures\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
