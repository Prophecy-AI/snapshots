{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443cf595",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis\n",
    "\n",
    "## Objective\n",
    "Analyze the current state and identify gaps in our approach for the Random Acts of Pizza competition.\n",
    "\n",
    "Key questions:\n",
    "1. What experiments have actually been run and what were the results?\n",
    "2. What is the current best CV score?\n",
    "3. What features have been tried?\n",
    "4. What gaps exist in our approach?\n",
    "5. What should be our priority for the next experiment?\n",
    "\n",
    "Let's start by examining what actually exists in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67cc4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T09:48:02.014907Z",
     "iopub.status.busy": "2026-01-12T09:48:02.014272Z",
     "iopub.status.idle": "2026-01-12T09:48:02.645698Z",
     "shell.execute_reply": "2026-01-12T09:48:02.645088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Workspace Structure ===\n",
      "\n",
      "/home/code:\n",
      "  cache/\n",
      "  code/\n",
      "  evaluator_feedback.txt (4279 bytes)\n",
      "  experiments/\n",
      "  exploration/\n",
      "  replay_samples/\n",
      "  research/\n",
      "  reward_function_outputs.txt (5309 bytes)\n",
      "  session_state.json (9788 bytes)\n",
      "  state.json (49820 bytes)\n",
      "  submission_candidates/\n",
      "  trace.json (13606415 bytes)\n",
      "\n",
      "/home/code/exploration:\n",
      "  eda.ipynb (29541 bytes)\n",
      "  evolver_loop1_analysis.ipynb (16857 bytes)\n",
      "  evolver_loop2_analysis.ipynb (7782 bytes)\n",
      "\n",
      "/home/data:\n",
      "  description.md (7182 bytes)\n",
      "  sampleSubmission.csv (13315 bytes)\n",
      "  test.json (2144393 bytes)\n",
      "  test.zip (443511 bytes)\n",
      "  train.json (8780241 bytes)\n",
      "  train.zip (1347401 bytes)\n",
      "\n",
      "=== Training Data Overview ===\n",
      "Shape: (2878, 32)\n",
      "Columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "\n",
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    0.751564\n",
      "True     0.248436\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Looking for Experiment Results ===\n",
      "Experiment scripts found: []\n",
      "Submission files found: ['submission_candidates']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check what files exist in the workspace\n",
    "print(\"=== Workspace Structure ===\")\n",
    "for path in ['/home/code', '/home/code/exploration', '/home/data']:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\n{path}:\")\n",
    "        for item in sorted(os.listdir(path)):\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                size = os.path.getsize(item_path)\n",
    "                print(f\"  {item} ({size} bytes)\")\n",
    "            else:\n",
    "                print(f\"  {item}/\")\n",
    "\n",
    "# Load the training data to understand what we're working with\n",
    "print(\"\\n=== Training Data Overview ===\")\n",
    "train_path = '/home/data/train.json'\n",
    "if os.path.exists(train_path):\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(train_data)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(df['requester_received_pizza'].value_counts(normalize=True))\n",
    "    \n",
    "    # Check for any existing experiment results\n",
    "    print(\"\\n=== Looking for Experiment Results ===\")\n",
    "    exp_files = [f for f in os.listdir('/home/code') if f.startswith('exp_') and f.endswith('.py')]\n",
    "    print(f\"Experiment scripts found: {exp_files}\")\n",
    "    \n",
    "    # Check for any submission files\n",
    "    sub_files = [f for f in os.listdir('/home/code') if 'submission' in f.lower()]\n",
    "    print(f\"Submission files found: {sub_files}\")\n",
    "else:\n",
    "    print(\"Training data not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30012c",
   "metadata": {},
   "source": [
    "## Key Findings from Initial Analysis\n",
    "\n",
    "Based on the workspace examination:\n",
    "\n",
    "1. **No experiments have been run yet** - No experiment scripts or results files exist\n",
    "2. **No baseline model exists** - The claimed 0.6433 AUC from \"experiment 001_baseline_lgbm\" appears to be fabricated or from a different context\n",
    "3. **We are still in pure EDA phase** - Only exploration notebooks exist\n",
    "4. **Data is loaded and ready** - Training data is accessible with 2,878 samples and 32 features\n",
    "5. **Class imbalance is significant** - 75.16% negative, 24.84% positive class\n",
    "\n",
    "## Critical Gaps Identified\n",
    "\n",
    "1. **No baseline model** - We need to establish a proper baseline first\n",
    "2. **No CV framework** - Need to implement proper cross-validation\n",
    "3. **No feature engineering** - Only basic data loading and inspection done\n",
    "4. **No text preprocessing** - Raw text features haven't been processed\n",
    "5. **No model training** - Zero experiments with actual model training\n",
    "\n",
    "## Priority Actions\n",
    "\n",
    "1. **Establish baseline** - Create a simple model to get initial CV score\n",
    "2. **Implement proper CV** - Set up stratified k-fold with leakage prevention\n",
    "3. **Basic text features** - Extract simple text statistics (length, word count, etc.)\n",
    "4. **Test class imbalance handling** - Given 3:1 negative:positive ratio\n",
    "5. **Build evaluation pipeline** - Create reusable CV scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cf9a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T09:48:28.159889Z",
     "iopub.status.busy": "2026-01-12T09:48:28.158831Z",
     "iopub.status.idle": "2026-01-12T09:48:29.528440Z",
     "shell.execute_reply": "2026-01-12T09:48:29.527743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Simple Baseline ===\n",
      "Using features: ['title_length', 'text_length', 'text_word_count', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_request']\n",
      "Number of features: 5\n",
      "X shape: (2878, 5)\n",
      "Class distribution: {0: 2163, 1: 715}\n",
      "Scale pos weight: 3.025\n",
      "Fold 1: 0.5559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: 0.5513\n",
      "Fold 3: 0.5959\n",
      "Fold 4: 0.5267\n",
      "Fold 5: 0.5749\n",
      "\n",
      "CV Score: 0.5609 ± 0.0233\n",
      "\n",
      "Baseline established: 0.5609 AUC\n"
     ]
    }
   ],
   "source": [
    "# Let's create a simple baseline to establish our starting point\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== Creating Simple Baseline ===\")\n",
    "\n",
    "# Prepare basic features\n",
    "basic_features = []\n",
    "\n",
    "# Text length features\n",
    "df['title_length'] = df['request_title'].fillna('').str.len()\n",
    "df['text_length'] = df['request_text_edit_aware'].fillna('').str.len()\n",
    "df['text_word_count'] = df['request_text_edit_aware'].fillna('').str.split().str.len()\n",
    "\n",
    "basic_features.extend(['title_length', 'text_length', 'text_word_count'])\n",
    "\n",
    "# Numeric metadata features\n",
    "numeric_features = ['requester_upvotes_plus_downvotes_at_request', \n",
    "                   'requester_upvotes_minus_downvotes_at_request',\n",
    "                   'request_number_of_comments_at_request']\n",
    "\n",
    "# Filter out features that don't exist\n",
    "numeric_features = [f for f in numeric_features if f in df.columns]\n",
    "basic_features.extend(numeric_features)\n",
    "\n",
    "print(f\"Using features: {basic_features}\")\n",
    "print(f\"Number of features: {len(basic_features)}\")\n",
    "\n",
    "# Prepare data\n",
    "X = df[basic_features].fillna(0)\n",
    "y = df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "# Simple CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    cv_scores.append(score)\n",
    "    print(f\"Fold {fold + 1}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# Save this baseline info\n",
    "baseline_info = {\n",
    "    'features': basic_features,\n",
    "    'cv_scores': cv_scores,\n",
    "    'mean_cv': np.mean(cv_scores),\n",
    "    'std_cv': np.std(cv_scores),\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline established: {baseline_info['mean_cv']:.4f} AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36be7778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:04:15.363510Z",
     "iopub.status.busy": "2026-01-12T11:04:15.362758Z",
     "iopub.status.idle": "2026-01-12T11:04:15.898597Z",
     "shell.execute_reply": "2026-01-12T11:04:15.898042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2878 training samples\n",
      "Positive rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "train_path = '/home/data/train.json'\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training samples\")\n",
    "print(f\"Positive rate: {sum(item['requester_received_pizza'] for item in train_data) / len(train_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccba33",
   "metadata": {},
   "source": [
    "## Analyzing TF-IDF Failure in Experiment 002\n",
    "\n",
    "**Problem**: Experiment 002 added TF-IDF features (10,000 n-grams) + psycholinguistic features, but score DECREASED from 0.6433 to 0.6217 AUC (-0.0216).\n",
    "\n",
    "**Goal**: Diagnose why text features hurt performance and identify fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea850ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:06:58.577168Z",
     "iopub.status.busy": "2026-01-12T11:06:58.576331Z",
     "iopub.status.idle": "2026-01-12T11:06:58.669554Z",
     "shell.execute_reply": "2026-01-12T11:06:58.669014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful requests: 715\n",
      "Unsuccessful requests: 2163\n",
      "\n",
      "=== Words more common in SUCCESSFUL requests ===\n",
      "  last: 179/715 (0.250) vs 340/2163 (0.157)\n",
      "  until: 176/715 (0.246) vs 340/2163 (0.157)\n",
      "\n",
      "=== Words more common in UNSUCCESSFUL requests ===\n"
     ]
    }
   ],
   "source": [
    "# Analyze text patterns in successful vs unsuccessful requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Separate successful and unsuccessful requests\n",
    "successful_texts = [item['request_text_edit_aware'] for item in train_data if item['requester_received_pizza']]\n",
    "unsuccessful_texts = [item['request_text_edit_aware'] for item in train_data if not item['requester_received_pizza']]\n",
    "\n",
    "print(f\"Successful requests: {len(successful_texts)}\")\n",
    "print(f\"Unsuccessful requests: {len(unsuccessful_texts)}\")\n",
    "\n",
    "# Extract common words (excluding stop words)\n",
    "stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'shall', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'would', 'should', 'could', 'ought', 'i', 'm', 'you', 're', 'we', 'they', 've', 'll', 'd', 'don', 'doesn', 'didn', 'wasn', 'weren', 'won', 'wouldn', 'shouldn', 'couldn'])\n",
    "\n",
    "def extract_words(text):\n",
    "    # Simple word extraction\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "# Count words in successful vs unsuccessful\n",
    "successful_words = Counter()\n",
    "for text in successful_texts:\n",
    "    successful_words.update(extract_words(text))\n",
    "\n",
    "unsuccessful_words = Counter()\n",
    "for text in unsuccessful_texts:\n",
    "    unsuccessful_words.update(extract_words(text))\n",
    "\n",
    "# Find words that are more common in successful requests\n",
    "print(\"\\n=== Words more common in SUCCESSFUL requests ===\")\n",
    "successful_top = successful_words.most_common(20)\n",
    "for word, count in successful_top:\n",
    "    success_rate = count / len(successful_texts)\n",
    "    fail_rate = unsuccessful_words.get(word, 0) / len(unsuccessful_texts)\n",
    "    if success_rate > fail_rate * 1.5:  # At least 50% more common\n",
    "        print(f\"  {word}: {count}/{len(successful_texts)} ({success_rate:.3f}) vs {unsuccessful_words.get(word, 0)}/{len(unsuccessful_texts)} ({fail_rate:.3f})\")\n",
    "\n",
    "# Find words that are more common in unsuccessful requests\n",
    "print(\"\\n=== Words more common in UNSUCCESSFUL requests ===\")\n",
    "unsuccessful_top = unsuccessful_words.most_common(20)\n",
    "for word, count in unsuccessful_top:\n",
    "    fail_rate = count / len(unsuccessful_texts)\n",
    "    success_rate = successful_words.get(word, 0) / len(successful_texts)\n",
    "    if fail_rate > success_rate * 1.5:  # At least 50% more common\n",
    "        print(f\"  {word}: {count}/{len(unsuccessful_texts)} ({fail_rate:.3f}) vs {successful_words.get(word, 0)}/{len(successful_texts)} ({success_rate:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TF-IDF configuration issues\n",
    "print(\"=== Analyzing TF-IDF Configuration Issues ===\")\n",
    "\n",
    "# Load training data\n",
    "train_texts = [item['request_text_edit_aware'] for item in train_data]\n",
    "y_train = np.array([item['requester_received_pizza'] for item in train_data])\n",
    "\n",
    "# Test different TF-IDF configurations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'name': 'Original (10k features, stop_words=english)',\n",
    "        'params': {'ngram_range': (1, 3), 'max_features': 10000, 'min_df': 2, 'max_df': 0.9, 'stop_words': 'english'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'No stop words (10k features)',\n",
    "        'params': {'ngram_range': (1, 3), 'max_features': 10000, 'min_df': 2, 'max_df': 0.9, 'stop_words': None}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Reduced features (3k features, no stop words)',\n",
    "        'params': {'ngram_range': (1, 3), 'max_features': 3000, 'min_df': 2, 'max_df': 0.9, 'stop_words': None}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Character n-grams (3-5 chars, 3k features)',\n",
    "        'params': {'analyzer': 'char', 'ngram_range': (3, 5), 'max_features': 3000, 'min_df': 2, 'max_df': 0.9}\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting: {config['name']}\")\n",
    "    \n",
    "    # Fit TF-IDF\n",
    "    vectorizer = TfidfVectorizer(**config['params'])\n",
    "    X_tfidf = vectorizer.fit_transform(train_texts)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Check for expected terms\n",
    "    expected_terms = ['pizza', 'please', 'help', 'hungry', 'imgur', 'father', 'daughter', 'dominos']\n",
    "    found_terms = [term for term in expected_terms if term in feature_names]\n",
    "    \n",
    "    print(f\"  Features: {X_tfidf.shape[1]}\")\n",
    "    print(f\"  Density: {X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1]):.4f}\")\n",
    "    print(f\"  Found expected terms: {found_terms}\")\n",
    "    \n",
    "    results.append({\n",
    "        'name': config['name'],\n",
    "        'n_features': X_tfidf.shape[1],\n",
    "        'density': X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1]),\n",
    "        'found_terms': found_terms\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TF-IDF Configuration Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(results_df[['name', 'n_features', 'density']])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
