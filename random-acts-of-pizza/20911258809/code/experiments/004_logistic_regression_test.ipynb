{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418006aa",
   "metadata": {},
   "source": [
    "# Experiment 004: Logistic Regression Test\n",
    "\n",
    "**Goal**: Test Logistic Regression vs LightGBM on TF-IDF features\n",
    "\n",
    "**Hypothesis**: Research shows Logistic Regression often outperforms tree models on TF-IDF features\n",
    "\n",
    "**Expected Improvement**: 0.67-0.70 AUC (from current 0.6555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf6e179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:53:08.478719Z",
     "iopub.status.busy": "2026-01-12T14:53:08.478118Z",
     "iopub.status.idle": "2026-01-12T14:53:09.866218Z",
     "shell.execute_reply": "2026-01-12T14:53:09.865570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train samples: 2878\n",
      "Test samples: 1162\n",
      "Positive rate: 0.248\n",
      "Class imbalance ratio: 4.025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# Load train data\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Extract target and create DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Target variable\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "print(f\"Positive rate: {y.mean():.3f}\")\n",
    "print(f\"Class imbalance ratio: {1/y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9a024d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:53:09.868513Z",
     "iopub.status.busy": "2026-01-12T14:53:09.868207Z",
     "iopub.status.idle": "2026-01-12T14:53:09.880583Z",
     "shell.execute_reply": "2026-01-12T14:53:09.880082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Functions\n",
    "\n",
    "def create_psycholinguistic_features(df):\n",
    "    \"\"\"Create psycholinguistic features with phrase patterns\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Reciprocity phrases (more specific patterns)\n",
    "    reciprocity_patterns = [\n",
    "        r'\\bpay\\s+(?:back|forward|it\\s+forward)',\n",
    "        r'\\breturn\\s+(?:the\\s+)?favor',\n",
    "        r'\\bwhen\\s+i\\s+get\\s+(?:paid|paycheck)',\n",
    "        r'\\bnext\\s+(?:week|month|payday)',\n",
    "        r'\\bforward\\s+the\\s+kindness',\n",
    "        r'\\bpass\\s+it\\s+on'\n",
    "    ]\n",
    "    \n",
    "    # Hardship phrases\n",
    "    hardship_patterns = [\n",
    "        r'\\b(?:lost|lose|losing)\\s+(?:my\\s+)?job',\n",
    "        r'\\bmedical\\s+(?:bills?|expenses?)',\n",
    "        r'\\bcar\\s+(?:broke\\s+down?|repair)',\n",
    "        r'\\beviction\\s+(?:notice)?',\n",
    "        r'\\bsingle\\s+(?:parent|mom|dad)',\n",
    "        r'\\bunexpected\\s+(?:expenses?|bills?)',\n",
    "        r'\\bran\\s+out\\s+of\\s+(?:money|cash)',\n",
    "        r'\\bno\\s+(?:food|money|cash)'\n",
    "    ]\n",
    "    \n",
    "    # Family phrases\n",
    "    family_patterns = [\n",
    "        r'\\bfamily\\b',\n",
    "        r'\\bkids?\\b',\n",
    "        r'\\bchildren\\b',\n",
    "        r'\\bmother\\b',\n",
    "        r'\\bfather\\b',\n",
    "        r'\\bdaughter\\b',\n",
    "        r'\\bson\\b',\n",
    "        r'\\bwife\\b',\n",
    "        r'\\bhusband\\b',\n",
    "        r'\\bbaby\\b'\n",
    "    ]\n",
    "    \n",
    "    # Gratitude phrases\n",
    "    gratitude_patterns = [\n",
    "        r'\\bthank\\s+(?:you|you\\s+so\\s+much|you\\s+in\\s+advance)',\n",
    "        r'\\bwould\\s+(?:appreciate|be\\s+grateful)',\n",
    "        r'\\bgrateful\\s+(?:for|if)',\n",
    "        r'\\bappreciate\\s+(?:it|any|your)',\n",
    "        r'\\bbless\\s+(?:you|your\\s+heart)',\n",
    "        r'\\bkind\\s+(?:stranger|soul|person)',\n",
    "        r'\\bgod\\s+bless',\n",
    "        r'\\bso\\s+(?:thankful|grateful)'\n",
    "    ]\n",
    "    \n",
    "    # Pizza-specific terms\n",
    "    pizza_patterns = [\n",
    "        r'\\bpizza\\b',\n",
    "        r'\\bhungry\\b',\n",
    "        r'\\bfood\\b',\n",
    "        r'\\bmeal\\b',\n",
    "        r'\\beat\\b',\n",
    "        r'\\bstarving\\b',\n",
    "        r'\\bdinner\\b',\n",
    "        r'\\blunch\\b',\n",
    "        r'\\bsupper\\b',\n",
    "        r'\\bdominos\\b',\n",
    "        r'\\bpapa\\s+johns?\\b',\n",
    "        r'\\bpizza\\s+hut\\b'\n",
    "    ]\n",
    "    \n",
    "    text_column = 'request_text_edit_aware'\n",
    "    \n",
    "    # Count patterns for each category\n",
    "    features['reciprocity_phrases'] = df[text_column].apply(\n",
    "        lambda x: sum(len(re.findall(pattern, x.lower())) for pattern in reciprocity_patterns)\n",
    "    )\n",
    "    \n",
    "    features['hardship_phrases'] = df[text_column].apply(\n",
    "        lambda x: sum(len(re.findall(pattern, x.lower())) for pattern in hardship_patterns)\n",
    "    )\n",
    "    \n",
    "    features['family_phrases'] = df[text_column].apply(\n",
    "        lambda x: sum(len(re.findall(pattern, x.lower())) for pattern in family_patterns)\n",
    "    )\n",
    "    \n",
    "    features['gratitude_phrases'] = df[text_column].apply(\n",
    "        lambda x: sum(len(re.findall(pattern, x.lower())) for pattern in gratitude_patterns)\n",
    "    )\n",
    "    \n",
    "    features['pizza_terms'] = df[text_column].apply(\n",
    "        lambda x: sum(len(re.findall(pattern, x.lower())) for pattern in pizza_patterns)\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def create_metadata_features(df):\n",
    "    \"\"\"Create metadata features from the dataframe\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Text length features\n",
    "    features['text_length'] = df['request_text_edit_aware'].str.len()\n",
    "    features['word_count'] = df['request_text_edit_aware'].str.split().str.len()\n",
    "    features['sentence_count'] = df['request_text_edit_aware'].str.count(r'[.!?]+') + 1\n",
    "    \n",
    "    # Account features\n",
    "    features['account_age_days'] = df['requester_account_age_in_days_at_request']\n",
    "    features['raop_activity_days'] = df['requester_days_since_first_post_on_raop_at_request']\n",
    "    \n",
    "    # Activity features\n",
    "    features['total_comments'] = df['requester_number_of_comments_at_request']\n",
    "    features['total_posts'] = df['requester_number_of_posts_at_request']\n",
    "    features['raop_comments'] = df['requester_number_of_comments_in_raop_at_request']\n",
    "    features['raop_posts'] = df['requester_number_of_posts_on_raop_at_request']\n",
    "    features['subreddit_diversity'] = df['requester_number_of_subreddits_at_request']\n",
    "    \n",
    "    # Voting features (if available)\n",
    "    if 'requester_upvotes_minus_downvotes_at_request' in df.columns:\n",
    "        features['upvotes_minus_downvotes'] = df['requester_upvotes_minus_downvotes_at_request']\n",
    "        features['upvotes_plus_downvotes'] = df['requester_upvotes_plus_downvotes_at_request']\n",
    "    else:\n",
    "        features['upvotes_minus_downvotes'] = 0\n",
    "        features['upvotes_plus_downvotes'] = 0\n",
    "    \n",
    "    # Time features\n",
    "    features['unix_timestamp'] = df['unix_timestamp_of_request']\n",
    "    features['hour_of_day'] = (df['unix_timestamp_of_request'] // 3600) % 24\n",
    "    features['day_of_week'] = (df['unix_timestamp_of_request'] // (24 * 3600)) % 7\n",
    "    \n",
    "    # Specific high-value features\n",
    "    # Imgur links\n",
    "    features['has_imgur_link'] = df['request_text_edit_aware'].str.contains(\n",
    "        r'https?://(?:www\\.)?imgur\\.com', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Key phrases based on character n-gram analysis\n",
    "    features['has_for_a'] = df['request_text_edit_aware'].str.contains(\n",
    "        r'\\bfor\\s+a\\b', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    features['has_please'] = df['request_text_edit_aware'].str.contains(\n",
    "        r'\\bplease\\b', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    features['has_this'] = df['request_text_edit_aware'].str.contains(\n",
    "        r'\\bthis\\b', case=False, na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac331f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:53:46.286600Z",
     "iopub.status.busy": "2026-01-12T14:53:46.285700Z",
     "iopub.status.idle": "2026-01-12T14:53:54.615960Z",
     "shell.execute_reply": "2026-01-12T14:53:54.615289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF shape: (2878, 3000)\n",
      "Char TF-IDF shape: (2878, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF features\n",
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "# Word n-grams (1-3)\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=3000,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# Character n-grams (3-5) - captures patterns like 'pizza', 'please', 'help'\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=2000,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# Fit on combined train + test to ensure consistent vocabulary\n",
    "combined_text = pd.concat([\n",
    "    train_df['request_text_edit_aware'],\n",
    "    test_df['request_text_edit_aware']\n",
    "], axis=0)\n",
    "\n",
    "word_vectorizer.fit(combined_text)\n",
    "char_vectorizer.fit(combined_text)\n",
    "\n",
    "# Transform train and test\n",
    "X_train_word = word_vectorizer.transform(train_df['request_text_edit_aware'])\n",
    "X_test_word = word_vectorizer.transform(test_df['request_text_edit_aware'])\n",
    "\n",
    "X_train_char = char_vectorizer.transform(train_df['request_text_edit_aware'])\n",
    "X_test_char = char_vectorizer.transform(test_df['request_text_edit_aware'])\n",
    "\n",
    "print(f\"Word TF-IDF shape: {X_train_word.shape}\")\n",
    "print(f\"Char TF-IDF shape: {X_train_char.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa869494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:53:54.618022Z",
     "iopub.status.busy": "2026-01-12T14:53:54.617799Z",
     "iopub.status.idle": "2026-01-12T14:53:56.248294Z",
     "shell.execute_reply": "2026-01-12T14:53:56.247341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating psycholinguistic features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating metadata features...\n",
      "Dense features shape: (2878, 24)\n",
      "Dense feature columns: ['reciprocity_phrases', 'hardship_phrases', 'family_phrases', 'gratitude_phrases', 'pizza_terms', 'text_length', 'word_count', 'sentence_count', 'account_age_days', 'raop_activity_days', 'total_comments', 'total_posts', 'raop_comments', 'raop_posts', 'subreddit_diversity', 'upvotes_minus_downvotes', 'upvotes_plus_downvotes', 'unix_timestamp', 'hour_of_day', 'day_of_week', 'has_imgur_link', 'has_for_a', 'has_please', 'has_this']\n"
     ]
    }
   ],
   "source": [
    "# Create all features\n",
    "print(\"Creating psycholinguistic features...\")\n",
    "psy_train = create_psycholinguistic_features(train_df)\n",
    "psy_test = create_psycholinguistic_features(test_df)\n",
    "\n",
    "print(\"Creating metadata features...\")\n",
    "meta_train = create_metadata_features(train_df)\n",
    "meta_test = create_metadata_features(test_df)\n",
    "\n",
    "# Combine all features\n",
    "X_train_dense = pd.concat([psy_train, meta_train], axis=1)\n",
    "X_test_dense = pd.concat([psy_test, meta_test], axis=1)\n",
    "\n",
    "print(f\"Dense features shape: {X_train_dense.shape}\")\n",
    "print(f\"Dense feature columns: {list(X_train_dense.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbffd37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:53:56.252951Z",
     "iopub.status.busy": "2026-01-12T14:53:56.252268Z",
     "iopub.status.idle": "2026-01-12T14:53:56.304500Z",
     "shell.execute_reply": "2026-01-12T14:53:56.303883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse features shape: (2878, 5000)\n",
      "Final training matrix shape: (2878, 5024)\n",
      "Final test matrix shape: (1162, 5024)\n"
     ]
    }
   ],
   "source": [
    "# Combine sparse and dense features\n",
    "print(\"Combining features...\")\n",
    "\n",
    "# For sparse features (TF-IDF)\n",
    "X_train_sparse = hstack([X_train_word, X_train_char])\n",
    "X_test_sparse = hstack([X_test_word, X_test_char])\n",
    "\n",
    "print(f\"Sparse features shape: {X_train_sparse.shape}\")\n",
    "\n",
    "# For dense features, convert to sparse matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_train_dense_sparse = csr_matrix(X_train_dense.values)\n",
    "X_test_dense_sparse = csr_matrix(X_test_dense.values)\n",
    "\n",
    "# Combine all features\n",
    "X_train = hstack([X_train_sparse, X_train_dense_sparse])\n",
    "X_test = hstack([X_test_sparse, X_test_dense_sparse])\n",
    "\n",
    "print(f\"Final training matrix shape: {X_train.shape}\")\n",
    "print(f\"Final test matrix shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f213a356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:55:07.685190Z",
     "iopub.status.busy": "2026-01-12T14:55:07.684422Z",
     "iopub.status.idle": "2026-01-12T14:55:07.688960Z",
     "shell.execute_reply": "2026-01-12T14:55:07.688422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Store results\n",
    "logreg_scores = []\n",
    "lgbm_scores = []\n",
    "oof_predictions_lr = np.zeros(len(train_df))\n",
    "oof_predictions_lgb = np.zeros(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42794b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:55:07.691130Z",
     "iopub.status.busy": "2026-01-12T14:55:07.690956Z",
     "iopub.status.idle": "2026-01-12T14:57:47.659636Z",
     "shell.execute_reply": "2026-01-12T14:57:47.658854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold CV...\n",
      "\n",
      "Fold 1/5\n",
      "  Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logistic Regression AUC: 0.6005\n",
      "  Training LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM AUC: 0.6903\n",
      "\n",
      "Fold 2/5\n",
      "  Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logistic Regression AUC: 0.5737\n",
      "  Training LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM AUC: 0.6631\n",
      "\n",
      "Fold 3/5\n",
      "  Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logistic Regression AUC: 0.5618\n",
      "  Training LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM AUC: 0.6587\n",
      "\n",
      "Fold 4/5\n",
      "  Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logistic Regression AUC: 0.5540\n",
      "  Training LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM AUC: 0.6344\n",
      "\n",
      "Fold 5/5\n",
      "  Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Logistic Regression AUC: 0.5818\n",
      "  Training LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM AUC: 0.6532\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "==================================================\n",
      "Logistic Regression: 0.5743 ± 0.0162\n",
      "LightGBM:          0.6599 ± 0.0181\n",
      "\n",
      "OOF Logistic Regression AUC: 0.5260\n",
      "OOF LightGBM AUC:            0.6591\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation loop\n",
    "print(f\"Starting {n_folds}-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train Logistic Regression\n",
    "    print(\"  Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        solver='saga',\n",
    "        max_iter=1000,\n",
    "        C=1.0,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lr_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict with Logistic Regression\n",
    "    val_pred_lr = lr_model.predict_proba(X_val)[:, 1]\n",
    "    score_lr = roc_auc_score(y_val, val_pred_lr)\n",
    "    logreg_scores.append(score_lr)\n",
    "    oof_predictions_lr[val_idx] = val_pred_lr\n",
    "    print(f\"  Logistic Regression AUC: {score_lr:.4f}\")\n",
    "    \n",
    "    # Train LightGBM\n",
    "    print(\"  Training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        scale_pos_weight=3.025,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict with LightGBM\n",
    "    val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    score_lgb = roc_auc_score(y_val, val_pred_lgb)\n",
    "    lgbm_scores.append(score_lgb)\n",
    "    oof_predictions_lgb[val_idx] = val_pred_lgb\n",
    "    print(f\"  LightGBM AUC: {score_lgb:.4f}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Logistic Regression: {np.mean(logreg_scores):.4f} ± {np.std(logreg_scores):.4f}\")\n",
    "print(f\"LightGBM:          {np.mean(lgbm_scores):.4f} ± {np.std(lgbm_scores):.4f}\")\n",
    "print(f\"\\nOOF Logistic Regression AUC: {roc_auc_score(y, oof_predictions_lr):.4f}\")\n",
    "print(f\"OOF LightGBM AUC:            {roc_auc_score(y, oof_predictions_lgb):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6d0fde0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T14:57:47.662059Z",
     "iopub.status.busy": "2026-01-12T14:57:47.661566Z",
     "iopub.status.idle": "2026-01-12T14:58:26.951636Z",
     "shell.execute_reply": "2026-01-12T14:58:26.950543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final models on full training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions on test data...\n",
      "Predictions generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train final models on full data\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "\n",
    "# Logistic Regression\n",
    "final_lr = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_lr.fit(X_train, y)\n",
    "\n",
    "# LightGBM\n",
    "final_lgb = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=31,\n",
    "    scale_pos_weight=3.025,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "final_lgb.fit(X_train, y)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nGenerating predictions on test data...\")\n",
    "pred_lr = final_lr.predict_proba(X_test)[:, 1]\n",
    "pred_lgb = final_lgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Simple average ensemble\n",
    "pred_ensemble = (pred_lr + pred_lgb) / 2\n",
    "\n",
    "print(\"Predictions generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d574280c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T15:02:10.299468Z",
     "iopub.status.busy": "2026-01-12T15:02:10.298866Z",
     "iopub.status.idle": "2026-01-12T15:02:10.319801Z",
     "shell.execute_reply": "2026-01-12T15:02:10.319223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating submission files...\n",
      "Logistic Regression submission saved: /home/submission/submission_lr.csv\n",
      "LightGBM submission saved: /home/submission/submission_lgb.csv\n",
      "Ensemble submission saved: /home/submission/submission_ensemble.csv\n",
      "\n",
      "All submissions have 1162 rows\n",
      "\n",
      "Submission files created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create submission files\n",
    "print(\"\\nCreating submission files...\")\n",
    "\n",
    "# Create submission directory\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "\n",
    "# Logistic Regression submission\n",
    "submission_lr = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': pred_lr\n",
    "})\n",
    "submission_lr.to_csv('/home/submission/submission_lr.csv', index=False)\n",
    "print(\"Logistic Regression submission saved: /home/submission/submission_lr.csv\")\n",
    "\n",
    "# LightGBM submission\n",
    "submission_lgb = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': pred_lgb\n",
    "})\n",
    "submission_lgb.to_csv('/home/submission/submission_lgb.csv', index=False)\n",
    "print(\"LightGBM submission saved: /home/submission/submission_lgb.csv\")\n",
    "\n",
    "# Ensemble submission\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': pred_ensemble\n",
    "})\n",
    "submission_ensemble.to_csv('/home/submission/submission_ensemble.csv', index=False)\n",
    "print(\"Ensemble submission saved: /home/submission/submission_ensemble.csv\")\n",
    "\n",
    "print(f\"\\nAll submissions have {len(submission_lr)} rows\")\n",
    "print(\"\\nSubmission files created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7691c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T15:02:10.321688Z",
     "iopub.status.busy": "2026-01-12T15:02:10.321263Z",
     "iopub.status.idle": "2026-01-12T15:02:10.330188Z",
     "shell.execute_reply": "2026-01-12T15:02:10.329620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Top 10 Positive Coefficients (Logistic Regression):\n",
      "  upvotes_plus_downvotes         0.0000\n",
      "  upvotes_minus_downvotes        0.0000\n",
      "  text_length                    0.0000\n",
      "  account_age_days               0.0000\n",
      "  word_count                     0.0000\n",
      "  raop_activity_days             0.0000\n",
      "  total_comments                 0.0000\n",
      "  total_posts                    0.0000\n",
      "  subreddit_diversity            0.0000\n",
      "  sentence_count                 0.0000\n",
      "\n",
      "Top 10 Negative Coefficients (Logistic Regression):\n",
      "  unix_timestamp                 -0.0000\n",
      "  day_of_week                    -0.0000\n",
      "  word_1167                      -0.0000\n",
      "  char_1076                      -0.0000\n",
      "  char_1600                      -0.0000\n",
      "  char_1601                      -0.0000\n",
      "  char_923                       -0.0000\n",
      "  char_1077                      -0.0000\n",
      "  char_922                       -0.0000\n",
      "  word_182                       -0.0000\n",
      "\n",
      "Top 10 LightGBM Feature Importances:\n",
      "  unix_timestamp                 196.00\n",
      "  upvotes_minus_downvotes        110.00\n",
      "  raop_activity_days             68.00\n",
      "  upvotes_plus_downvotes         67.00\n",
      "  total_comments                 63.00\n",
      "  text_length                    60.00\n",
      "  char_852                       51.00\n",
      "  raop_comments                  48.00\n",
      "  account_age_days               46.00\n",
      "  char_342                       43.00\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# For Logistic Regression - get top coefficients\n",
    "feature_names = []\n",
    "feature_names.extend([f'word_{i}' for i in range(X_train_word.shape[1])])\n",
    "feature_names.extend([f'char_{i}' for i in range(X_train_char.shape[1])])\n",
    "feature_names.extend(list(X_train_dense.columns))\n",
    "\n",
    "coefficients = final_lr.coef_[0]\n",
    "top_positive_idx = np.argsort(coefficients)[-10:][::-1]\n",
    "top_negative_idx = np.argsort(coefficients)[:10]\n",
    "\n",
    "print(\"\\nTop 10 Positive Coefficients (Logistic Regression):\")\n",
    "for idx in top_positive_idx:\n",
    "    print(f\"  {feature_names[idx]:<30} {coefficients[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Negative Coefficients (Logistic Regression):\")\n",
    "for idx in top_negative_idx:\n",
    "    print(f\"  {feature_names[idx]:<30} {coefficients[idx]:.4f}\")\n",
    "\n",
    "# For LightGBM - get feature importances\n",
    "lgb_importances = final_lgb.feature_importances_\n",
    "top_lgb_idx = np.argsort(lgb_importances)[-10:][::-1]\n",
    "\n",
    "print(\"\\nTop 10 LightGBM Feature Importances:\")\n",
    "for idx in top_lgb_idx:\n",
    "    print(f\"  {feature_names[idx]:<30} {lgb_importances[idx]:.2f}\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
