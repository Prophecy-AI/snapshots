{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f195222",
   "metadata": {},
   "source": [
    "# Experiment 005: Remove Psycholinguistic Features\n",
    "\n",
    "**Goal**: Remove redundant psycholinguistic features to reduce dimensionality from ~5,015 to ~5,000 features\n",
    "\n",
    "**Hypothesis**: These features are redundant with TF-IDF and not in top 10 importance for either LightGBM or Logistic Regression. Removing them will simplify the model without performance loss.\n",
    "\n",
    "**Expected outcome**: Maintain or improve CV AUC (currently 0.6599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c77416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:25:58.631637Z",
     "iopub.status.busy": "2026-01-12T17:25:58.630918Z",
     "iopub.status.idle": "2026-01-12T17:25:58.750585Z",
     "shell.execute_reply": "2026-01-12T17:25:58.749872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (2878, 32)\n",
      "Test shape: (1162, 17)\n",
      "Positive rate: 0.2484\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032d4915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:41:24.390312Z",
     "iopub.status.busy": "2026-01-12T17:41:24.389493Z",
     "iopub.status.idle": "2026-01-12T17:41:24.399643Z",
     "shell.execute_reply": "2026-01-12T17:41:24.398977Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(df, is_train=True):\n",
    "    \"\"\"Extract features WITHOUT psycholinguistic features\"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # 1. Text length features\n",
    "    text_length = df['request_text_edit_aware'].str.len()\n",
    "    word_count = df['request_text_edit_aware'].str.split().str.len()\n",
    "    sentence_count = df['request_text_edit_aware'].str.count(r'[.!?]+') + 1\n",
    "    \n",
    "    features.extend([text_length.values, word_count.values, sentence_count.values])\n",
    "    feature_names.extend(['text_length', 'word_count', 'sentence_count'])\n",
    "    \n",
    "    # 2. Account age and activity features (at request time)\n",
    "    features.extend([\n",
    "        df['requester_account_age_in_days_at_request'].values,\n",
    "        df['requester_days_since_first_post_on_raop_at_request'].values,\n",
    "        df['requester_number_of_comments_in_raop_at_request'].values,\n",
    "        df['requester_number_of_posts_on_raop_at_request'].values,\n",
    "        df['requester_number_of_comments_at_request'].values,\n",
    "        df['requester_number_of_posts_at_request'].values,\n",
    "        df['requester_upvotes_minus_downvotes_at_request'].values,\n",
    "        df['requester_upvotes_plus_downvotes_at_request'].values\n",
    "    ])\n",
    "    feature_names.extend([\n",
    "        'account_age_days', 'days_since_first_post', 'comments_in_raop', \n",
    "        'posts_in_raop', 'total_comments', 'total_posts', \n",
    "        'upvotes_minus_downvotes', 'upvotes_plus_downvotes'\n",
    "    ])\n",
    "    \n",
    "    # 3. Subreddit-specific features\n",
    "    features.extend([\n",
    "        df['requester_number_of_subreddits_at_request'].values\n",
    "    ])\n",
    "    feature_names.extend(['number_of_subreddits'])\n",
    "    \n",
    "    # 4. Time-based features\n",
    "    # Convert unix timestamps to datetime features\n",
    "    request_time = pd.to_datetime(df['unix_timestamp_of_request_utc'], unit='s')\n",
    "    features.extend([\n",
    "        request_time.dt.hour.values,  # Hour of day\n",
    "        request_time.dt.dayofweek.values,  # Day of week\n",
    "        request_time.dt.day.values  # Day of month\n",
    "    ])\n",
    "    feature_names.extend(['request_hour', 'request_dayofweek', 'request_day'])\n",
    "    \n",
    "    # 5. Text-based features (non-psycholinguistic)\n",
    "    # Count of numbers in text (might indicate specific amounts)\n",
    "    number_count = df['request_text_edit_aware'].str.count(r'\\d+')\n",
    "    features.append(number_count.values)\n",
    "    feature_names.append('number_count')\n",
    "    \n",
    "    # Count of dollar signs\n",
    "    dollar_count = df['request_text_edit_aware'].str.count(r'\\$')\n",
    "    features.append(dollar_count.values)\n",
    "    feature_names.append('dollar_count')\n",
    "    \n",
    "    # Has imgur link (visual evidence)\n",
    "    has_imgur = df['request_text_edit_aware'].str.contains('imgur', case=False, na=False).astype(int)\n",
    "    features.append(has_imgur.values)\n",
    "    feature_names.append('has_imgur')\n",
    "    \n",
    "    # Return as numpy array\n",
    "    return np.column_stack(features), feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bffc778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:41:24.402367Z",
     "iopub.status.busy": "2026-01-12T17:41:24.401606Z",
     "iopub.status.idle": "2026-01-12T17:41:24.569180Z",
     "shell.execute_reply": "2026-01-12T17:41:24.568282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Dense feature shape: (2878, 18)\n",
      "Number of dense features: 18\n",
      "Dense feature names: ['text_length', 'word_count', 'sentence_count', 'account_age_days', 'days_since_first_post', 'comments_in_raop', 'posts_in_raop', 'total_comments', 'total_posts', 'upvotes_minus_downvotes', 'upvotes_plus_downvotes', 'number_of_subreddits', 'request_hour', 'request_dayofweek', 'request_day', 'number_count', 'dollar_count', 'has_imgur']\n"
     ]
    }
   ],
   "source": [
    "# Extract dense features for train and test\n",
    "print(\"Extracting dense features...\")\n",
    "X_train_dense, dense_feature_names = extract_features(train_df, is_train=True)\n",
    "X_test_dense, _ = extract_features(test_df, is_train=False)\n",
    "\n",
    "print(f\"Dense feature shape: {X_train_dense.shape}\")\n",
    "print(f\"Number of dense features: {len(dense_feature_names)}\")\n",
    "print(f\"Dense feature names: {dense_feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5fc7966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:41:24.572022Z",
     "iopub.status.busy": "2026-01-12T17:41:24.571324Z",
     "iopub.status.idle": "2026-01-12T17:41:25.199637Z",
     "shell.execute_reply": "2026-01-12T17:41:25.199105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF word n-grams...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF word shape: (2878, 3000)\n",
      "Top word features: ['00' '10' '100' '11' '12' '15' '15th' '18' '19' '1st']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 8. TF-IDF word n-grams\n",
    "print(\"Creating TF-IDF word n-grams...\")\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "X_train_tfidf_word = tfidf_word.fit_transform(train_df['request_text_edit_aware'])\n",
    "X_test_tfidf_word = tfidf_word.transform(test_df['request_text_edit_aware'])\n",
    "\n",
    "print(f\"TF-IDF word shape: {X_train_tfidf_word.shape}\")\n",
    "print(f\"Top word features: {tfidf_word.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95cad3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:41:25.201674Z",
     "iopub.status.busy": "2026-01-12T17:41:25.201447Z",
     "iopub.status.idle": "2026-01-12T17:41:28.685742Z",
     "shell.execute_reply": "2026-01-12T17:41:28.685004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF character n-grams...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF char shape: (2878, 2000)\n",
      "Top char features: [' a ' ' a b' ' a c' ' a f' ' a l' ' a n' ' a p' ' a pi' ' a r' ' a s']\n"
     ]
    }
   ],
   "source": [
    "# 9. TF-IDF character n-grams\n",
    "print(\"Creating TF-IDF character n-grams...\")\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    max_features=2000,\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "X_train_tfidf_char = tfidf_char.fit_transform(train_df['request_text_edit_aware'])\n",
    "X_test_tfidf_char = tfidf_char.transform(test_df['request_text_edit_aware'])\n",
    "\n",
    "print(f\"TF-IDF char shape: {X_train_tfidf_char.shape}\")\n",
    "print(f\"Top char features: {tfidf_char.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc524a52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:41:28.687929Z",
     "iopub.status.busy": "2026-01-12T17:41:28.687698Z",
     "iopub.status.idle": "2026-01-12T17:41:28.716784Z",
     "shell.execute_reply": "2026-01-12T17:41:28.716053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling dense features...\n",
      "Final train shape: (2878, 5018)\n",
      "Final test shape: (1162, 5018)\n",
      "Target shape: (2878,)\n",
      "Positive rate: 0.2484\n"
     ]
    }
   ],
   "source": [
    "# Scale dense features\n",
    "print(\"Scaling dense features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_dense_scaled = scaler.fit_transform(X_train_dense)\n",
    "X_test_dense_scaled = scaler.transform(X_test_dense)\n",
    "\n",
    "# Combine all features\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_train_dense_sparse = csr_matrix(X_train_dense_scaled)\n",
    "X_test_dense_sparse = csr_matrix(X_test_dense_scaled)\n",
    "\n",
    "X_train = hstack([X_train_dense_sparse, X_train_tfidf_word, X_train_tfidf_char])\n",
    "X_test = hstack([X_test_dense_sparse, X_test_tfidf_word, X_test_tfidf_char])\n",
    "\n",
    "print(f\"Final train shape: {X_train.shape}\")\n",
    "print(f\"Final test shape: {X_test.shape}\")\n",
    "\n",
    "# Target\n",
    "y = train_df['requester_received_pizza'].values\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Positive rate: {y.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f017230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:57:32.726773Z",
     "iopub.status.busy": "2026-01-12T17:57:32.725911Z",
     "iopub.status.idle": "2026-01-12T17:58:16.979239Z",
     "shell.execute_reply": "2026-01-12T17:58:16.977721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tval's auc: 0.634797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[145]\tval's auc: 0.637607\n",
      "Fold 1 AUC: 0.6376\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tval's auc: 0.611832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[113]\tval's auc: 0.620698\n",
      "Fold 2 AUC: 0.6207\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tval's auc: 0.670763\n",
      "Fold 3 AUC: 0.6708\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tval's auc: 0.6081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[65]\tval's auc: 0.617586\n",
      "Fold 4 AUC: 0.6176\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tval's auc: 0.63141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tval's auc: 0.65829\n",
      "Fold 5 AUC: 0.6583\n",
      "\n",
      "CV Results: 0.6410 ± 0.0208\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Model parameters\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': 3.025,  # Handle class imbalance\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Starting cross-validation...\")\n",
    "cv_scores = []\n",
    "fold_predictions = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_set,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_set],\n",
    "        valid_names=['val'],\n",
    "        callbacks=[\n",
    "            lgb.log_evaluation(100),\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.record_evaluation({})\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(val_score)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} AUC: {val_score:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Results: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe3d37f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:58:16.982524Z",
     "iopub.status.busy": "2026-01-12T17:58:16.981765Z",
     "iopub.status.idle": "2026-01-12T17:59:24.611695Z",
     "shell.execute_reply": "2026-01-12T17:59:24.611129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on full data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on test set...\n",
      "Submission shape: (1162, 2)\n",
      "Prediction range: [0.0000, 0.9946]\n",
      "Mean prediction: 0.1040\n",
      "Submission saved to: /home/code/submission_candidates/candidate_005.csv\n"
     ]
    }
   ],
   "source": [
    "# Train final model on full training data\n",
    "print(\"Training final model on full data...\")\n",
    "train_set = lgb.Dataset(X_train, label=y)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_set,\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "print(f\"Mean prediction: {test_predictions.mean():.4f}\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/code/submission_candidates/candidate_005.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to: {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d706515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:59:24.613524Z",
     "iopub.status.busy": "2026-01-12T17:59:24.613333Z",
     "iopub.status.idle": "2026-01-12T17:59:24.639130Z",
     "shell.execute_reply": "2026-01-12T17:59:24.638570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Feature Importances:\n",
      "                feature  importance\n",
      "            text_length  921.294869\n",
      "upvotes_minus_downvotes  720.686824\n",
      "       comments_in_raop  702.540417\n",
      " upvotes_plus_downvotes  624.865646\n",
      "         total_comments  444.988090\n",
      "  days_since_first_post  415.146118\n",
      "            request_day  365.047904\n",
      "       account_age_days  329.146109\n",
      "                    i w  312.800116\n",
      "                   . i   309.919118\n",
      "                     an  293.874672\n",
      "                    ere  291.693144\n",
      "                    e w  276.026127\n",
      "                    aid  272.826835\n",
      "                     my  262.587784\n",
      "             word_count  259.931909\n",
      "                    ien  253.576614\n",
      "           request_hour  244.830690\n",
      "                    ide  241.517045\n",
      "                     ho  230.690708\n",
      "\n",
      "Feature importance saved to: /home/code/experiments/005_feature_importance.csv\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT 005 SUMMARY\n",
      "============================================================\n",
      "Model: LightGBM\n",
      "Features: 5018 total\n",
      "  - Dense metadata: 18\n",
      "  - TF-IDF word n-grams: 3000\n",
      "  - TF-IDF char n-grams: 2000\n",
      "Psycholinguistic features: REMOVED (9 features)\n",
      "CV AUC: 0.6410 ± 0.0208\n",
      "Previous best (exp_004): 0.6599\n",
      "Change: -0.0189\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\nTop 20 Feature Importances:\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': dense_feature_names + list(tfidf_word.get_feature_names_out()) + list(tfidf_char.get_feature_names_out()),\n",
    "    'importance': final_model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = '/home/code/experiments/005_feature_importance.csv'\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"\\nFeature importance saved to: {importance_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EXPERIMENT 005 SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: LightGBM\")\n",
    "print(f\"Features: {X_train.shape[1]} total\")\n",
    "print(f\"  - Dense metadata: {len(dense_feature_names)}\")\n",
    "print(f\"  - TF-IDF word n-grams: {X_train_tfidf_word.shape[1]}\")\n",
    "print(f\"  - TF-IDF char n-grams: {X_train_tfidf_char.shape[1]}\")\n",
    "print(f\"Psycholinguistic features: REMOVED (9 features)\")\n",
    "print(f\"CV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Previous best (exp_004): 0.6599\")\n",
    "print(f\"Change: {cv_mean - 0.6599:+.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
