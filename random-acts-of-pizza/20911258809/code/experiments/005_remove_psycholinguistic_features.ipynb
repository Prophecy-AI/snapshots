{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f195222",
   "metadata": {},
   "source": [
    "# Experiment 005: Remove Psycholinguistic Features\n",
    "\n",
    "**Goal**: Remove redundant psycholinguistic features to reduce dimensionality from ~5,015 to ~5,000 features\n",
    "\n",
    "**Hypothesis**: These features are redundant with TF-IDF and not in top 10 importance for either LightGBM or Logistic Regression. Removing them will simplify the model without performance loss.\n",
    "\n",
    "**Expected outcome**: Maintain or improve CV AUC (currently 0.6599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c77416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('/home/data/train.csv')\n",
    "test_data = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}\")\n",
    "print(f\"Test shape: {test_data.shape}\")\n",
    "print(f\"Positive rate: {train_data['requester_received_pizza'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, is_train=True):\n",
    "    \"\"\"Extract features WITHOUT psycholinguistic features\"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # 1. Text length features\n",
    "    text_length = df['request_text_edit_aware'].str.len()\n",
    "    word_count = df['request_text_edit_aware'].str.split().str.len()\n",
    "    sentence_count = df['request_text_edit_aware'].str.count(r'[.!?]+') + 1\n",
    "    \n",
    "    features.extend([text_length.values, word_count.values, sentence_count.values])\n",
    "    feature_names.extend(['text_length', 'word_count', 'sentence_count'])\n",
    "    \n",
    "    # 2. Account age and activity features (at request time)\n",
    "    features.extend([\n",
    "        df['requester_account_age_in_days_at_request'].values,\n",
    "        df['requester_days_since_first_post_on_raop_at_request'].values,\n",
    "        df['requester_number_of_comments_in_raop_at_request'].values,\n",
    "        df['requester_number_of_posts_on_raop_at_request'].values,\n",
    "        df['requester_number_of_comments_at_request'].values,\n",
    "        df['requester_number_of_posts_at_request'].values,\n",
    "        df['requester_number_of_subreddits_at_request'].values\n",
    "    ])\n",
    "    feature_names.extend([\n",
    "        'account_age_days', 'days_since_first_raop_post', 'raop_comments', \n",
    "        'raop_posts', 'total_comments', 'total_posts', 'subreddit_diversity'\n",
    "    ])\n",
    "    \n",
    "    # 3. Upvotes and karma features (at request time)\n",
    "    features.extend([\n",
    "        df['requester_upvotes_plus_downvotes_at_request'].values,\n",
    "        df['requester_upvotes_minus_downvotes_at_request'].values\n",
    "    ])\n",
    "    feature_names.extend(['total_votes', 'net_karma'])\n",
    "    \n",
    "    # 4. Time features (at request time)\n",
    "    features.extend([\n",
    "        df['requester_hours_since_first_post_on_raop_at_request'].values,\n",
    "        df['unix_timestamp_of_request_utc'].values\n",
    "    ])\n",
    "    feature_names.extend(['hours_since_first_raop', 'timestamp_utc'])\n",
    "    \n",
    "    # 5. Subreddit features (at request time)\n",
    "    features.extend([\n",
    "        df['requester_subreddits_at_request'].notna().astype(int).values\n",
    "    ])\n",
    "    feature_names.extend(['has_subreddit_info'])\n",
    "    \n",
    "    # 6. Visual evidence features (imgur links)\n",
    "    imgur_pattern = r'imgur\\.com'\n",
    "    imgur_count = df['request_text_edit_aware'].str.count(imgur_pattern, flags=re.IGNORECASE)\n",
    "    features.extend([imgur_count.values])\n",
    "    feature_names.extend(['imgur_link_count'])\n",
    "    \n",
    "    # 7. Key phrase features (specific high-value patterns)\n",
    "    key_phrases = [\n",
    "        r'\\bfor a\\b',\n",
    "        r'\\bplease\\b',\n",
    "        r'\\bthis\\b',\n",
    "        r'\\bhere\\b',\n",
    "        r'\\bthere\\b',\n",
    "        r'\\bmy\\b',\n",
    "        r'\\bwould appreciate\\b',\n",
    "        r'\\bwould be grateful\\b'\n",
    "    ]\n",
    "    \n",
    "    for phrase in key_phrases:\n",
    "        count = df['request_text_edit_aware'].str.count(phrase, flags=re.IGNORECASE)\n",
    "        features.extend([count.values])\n",
    "        feature_names.extend([f'phrase_{phrase.replace(\"\\\\b\", \"\").replace(\" \", \"_\")}'])\n",
    "    \n",
    "    # Stack all features\n",
    "    X_dense = np.column_stack(features)\n",
    "    \n",
    "    return X_dense, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dense features for train and test\n",
    "print(\"Extracting dense features...\")\n",
    "X_train_dense, dense_feature_names = extract_features(train_data, is_train=True)\n",
    "X_test_dense, _ = extract_features(test_data, is_train=False)\n",
    "\n",
    "print(f\"Dense feature shape: {X_train_dense.shape}\")\n",
    "print(f\"Number of dense features: {len(dense_feature_names)}\")\n",
    "print(f\"Dense feature names: {dense_feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 8. TF-IDF word n-grams\n",
    "print(\"Creating TF-IDF word n-grams...\")\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "X_train_tfidf_word = tfidf_word.fit_transform(train_data['request_text_edit_aware'])\n",
    "X_test_tfidf_word = tfidf_word.transform(test_data['request_text_edit_aware'])\n",
    "\n",
    "print(f\"TF-IDF word shape: {X_train_tfidf_word.shape}\")\n",
    "print(f\"Top word features: {tfidf_word.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. TF-IDF character n-grams\n",
    "print(\"Creating TF-IDF character n-grams...\")\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    max_features=2000,\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "X_train_tfidf_char = tfidf_char.fit_transform(train_data['request_text_edit_aware'])\n",
    "X_test_tfidf_char = tfidf_char.transform(test_data['request_text_edit_aware'])\n",
    "\n",
    "print(f\"TF-IDF char shape: {X_train_tfidf_char.shape}\")\n",
    "print(f\"Top char features: {tfidf_char.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc524a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale dense features\n",
    "print(\"Scaling dense features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_dense_scaled = scaler.fit_transform(X_train_dense)\n",
    "X_test_dense_scaled = scaler.transform(X_test_dense)\n",
    "\n",
    "# Combine all features\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_train_dense_sparse = csr_matrix(X_train_dense_scaled)\n",
    "X_test_dense_sparse = csr_matrix(X_test_dense_scaled)\n",
    "\n",
    "X_train = hstack([X_train_dense_sparse, X_train_tfidf_word, X_train_tfidf_char])\n",
    "X_test = hstack([X_test_dense_sparse, X_test_tfidf_word, X_test_tfidf_char])\n",
    "\n",
    "print(f\"Final train shape: {X_train.shape}\")\n",
    "print(f\"Final test shape: {X_test.shape}\")\n",
    "\n",
    "# Target\n",
    "y = train_data['requester_received_pizza'].values\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Positive rate: {y.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f017230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Model parameters\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': 3.025,  # Handle class imbalance\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Starting cross-validation...\")\n",
    "cv_scores = []\n",
    "fold_predictions = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_set,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_set],\n",
    "        valid_names=['val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} AUC: {fold_auc:.4f}\")\n",
    "    \n",
    "    # Store out-of-fold predictions\n",
    "    fold_predictions.append((val_idx, val_pred))\n",
    "\n",
    "# Overall CV score\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "print(f\"\\nCV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training data\n",
    "print(\"Training final model on full data...\")\n",
    "train_set = lgb.Dataset(X_train, label=y)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_set,\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_data['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "print(f\"Mean prediction: {test_predictions.mean():.4f}\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/code/submission_candidates/candidate_005.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to: {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d706515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\nTop 20 Feature Importances:\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': dense_feature_names + list(tfidf_word.get_feature_names_out()) + list(tfidf_char.get_feature_names_out()),\n",
    "    'importance': final_model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = '/home/code/experiments/005_feature_importance.csv'\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"\\nFeature importance saved to: {importance_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EXPERIMENT 005 SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: LightGBM\")\n",
    "print(f\"Features: {X_train.shape[1]} total\")\n",
    "print(f\"  - Dense metadata: {len(dense_feature_names)}\")\n",
    "print(f\"  - TF-IDF word n-grams: {X_train_tfidf_word.shape[1]}\")\n",
    "print(f\"  - TF-IDF char n-grams: {X_train_tfidf_char.shape[1]}\")\n",
    "print(f\"Psycholinguistic features: REMOVED (9 features)\")\n",
    "print(f\"CV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Previous best (exp_004): 0.6599\")\n",
    "print(f\"Change: {cv_mean - 0.6599:+.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
