{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d21e4d",
   "metadata": {},
   "source": [
    "# Experiment 002: TF-IDF + Psycholinguistic Features\n",
    "\n",
    "**Goal**: Implement TF-IDF n-gram features and psycholinguistic word categories to significantly improve from baseline 0.6433 AUC.\n",
    "\n",
    "**Strategy**: \n",
    "- Add TF-IDF vectorizer (1-3 n-grams, max_features=10000)\n",
    "- Add psycholinguistic category counts (reciprocity, hardship, family, gratitude, food)\n",
    "- Combine with existing metadata features\n",
    "- Use LightGBM with proper class imbalance handling (scale_pos_weight=3.025)\n",
    "- Target: >0.75 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cb8d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:25.029368Z",
     "iopub.status.busy": "2026-01-12T10:36:25.028557Z",
     "iopub.status.idle": "2026-01-12T10:36:25.116745Z",
     "shell.execute_reply": "2026-01-12T10:36:25.115956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train samples: 2878\n",
      "Test samples: 1162\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81eec6b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:25.119679Z",
     "iopub.status.busy": "2026-01-12T10:36:25.119383Z",
     "iopub.status.idle": "2026-01-12T10:36:26.106486Z",
     "shell.execute_reply": "2026-01-12T10:36:26.105923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting basic features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting psycholinguistic features...\n",
      "Basic features shape: (2878, 9)\n",
      "Psycholinguistic features shape: (2878, 10)\n"
     ]
    }
   ],
   "source": [
    "# Extract basic features (same as baseline)\n",
    "def extract_basic_features(data):\n",
    "    features = []\n",
    "    for item in data:\n",
    "        # Text features\n",
    "        text = item['request_text_edit_aware']\n",
    "        text_length = len(text)\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        # Account age in days (at request time)\n",
    "        account_age_days = item['requester_account_age_in_days_at_request']\n",
    "        \n",
    "        # Activity features (at request time)\n",
    "        comments_at_request = item['requester_number_of_comments_at_request']\n",
    "        posts_at_request = item['requester_number_of_posts_at_request']\n",
    "        \n",
    "        # Voting features (at request time)\n",
    "        upvotes_at_request = item['requester_upvotes_plus_downvotes_at_request']\n",
    "        \n",
    "        # Subreddit diversity (at request time)\n",
    "        subreddits_at_request = item['requester_number_of_subreddits_at_request']\n",
    "        \n",
    "        # Time features\n",
    "        request_hour = pd.to_datetime(item['unix_timestamp_of_request'], unit='s').hour\n",
    "        request_dayofweek = pd.to_datetime(item['unix_timestamp_of_request'], unit='s').dayofweek\n",
    "        \n",
    "        features.append({\n",
    "            'text_length': text_length,\n",
    "            'word_count': word_count,\n",
    "            'account_age_days': account_age_days,\n",
    "            'comments_at_request': comments_at_request,\n",
    "            'posts_at_request': posts_at_request,\n",
    "            'upvotes_at_request': upvotes_at_request,\n",
    "            'subreddits_at_request': subreddits_at_request,\n",
    "            'request_hour': request_hour,\n",
    "            'request_dayofweek': request_dayofweek,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extract psycholinguistic features\n",
    "def extract_psycholinguistic_features(data):\n",
    "    \"\"\"Extract word counts for psycholinguistic categories\"\"\"\n",
    "    \n",
    "    # Define word categories based on data findings\n",
    "    categories = {\n",
    "        'reciprocity': ['offer', 'pay', 'return', 'favor', 'back', 'help', 'kindness', 'generous', 'repay', 'owe'],\n",
    "        'hardship': ['struggle', 'broke', 'bills', 'unemployed', 'hungry', 'hard', 'difficult', 'tough', 'poor', 'broke', 'starving'],\n",
    "        'family': ['family', 'kids', 'children', 'mother', 'father', 'daughter', 'son', 'wife', 'husband', 'parent', 'baby'],\n",
    "        'gratitude': ['thank', 'appreciate', 'grateful', 'bless', 'kind', 'generous', 'thanks', 'blessed', 'appreciation'],\n",
    "        'food_specific': ['pizza', 'hungry', 'food', 'meal', 'eat', 'starving', 'dinner', 'lunch', 'breakfast', 'craving']\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    for item in data:\n",
    "        text = item['request_text_edit_aware'].lower()\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        # Count words in each category\n",
    "        category_counts = {}\n",
    "        for category, words in categories.items():\n",
    "            count = sum(1 for word in words if word in text)\n",
    "            # Normalize by text length to avoid bias toward longer texts\n",
    "            category_counts[f'{category}_count'] = count\n",
    "            category_counts[f'{category}_ratio'] = count / max(word_count, 1)\n",
    "        \n",
    "        features.append(category_counts)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "print(\"Extracting basic features...\")\n",
    "X_train_basic = extract_basic_features(train_data)\n",
    "X_test_basic = extract_basic_features(test_data)\n",
    "\n",
    "print(\"Extracting psycholinguistic features...\")\n",
    "X_train_psycho = extract_psycholinguistic_features(train_data)\n",
    "X_test_psycho = extract_psycholinguistic_features(test_data)\n",
    "\n",
    "print(f\"Basic features shape: {X_train_basic.shape}\")\n",
    "print(f\"Psycholinguistic features shape: {X_train_psycho.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f333c9fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:26.108749Z",
     "iopub.status.busy": "2026-01-12T10:36:26.108292Z",
     "iopub.status.idle": "2026-01-12T10:36:26.921725Z",
     "shell.execute_reply": "2026-01-12T10:36:26.921129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features shape: (2878, 10000)\n",
      "TF-IDF density: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Extract TF-IDF features\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "\n",
    "# Use request_text_edit_aware (leakage-free version)\n",
    "train_texts = [item['request_text_edit_aware'] for item in train_data]\n",
    "test_texts = [item['request_text_edit_aware'] for item in test_data]\n",
    "\n",
    "# Configure TF-IDF vectorizer based on research findings\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # 1-3 word n-grams\n",
    "    max_features=10000,  # Limit vocabulary size\n",
    "    min_df=2,            # Ignore very rare terms\n",
    "    max_df=0.9,          # Ignore very common terms\n",
    "    sublinear_tf=True,   # Sublinear term frequency scaling\n",
    "    stop_words='english' # Remove common English stop words\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
    "\n",
    "print(f\"TF-IDF features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF density: {X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab02287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:26.923943Z",
     "iopub.status.busy": "2026-01-12T10:36:26.923747Z",
     "iopub.status.idle": "2026-01-12T10:36:26.938088Z",
     "shell.execute_reply": "2026-01-12T10:36:26.937567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features...\n",
      "Final training features shape: (2878, 10019)\n",
      "Feature types: TF-IDF (10000) + Dense (19)\n",
      "Class distribution: [2163  715]\n",
      "Positive rate: 0.248\n",
      "Scale pos weight: 3.025\n"
     ]
    }
   ],
   "source": [
    "# Combine all features\n",
    "print(\"Combining features...\")\n",
    "\n",
    "# Combine basic and psycholinguistic features (dense)\n",
    "X_train_dense = pd.concat([X_train_basic, X_train_psycho], axis=1)\n",
    "X_test_dense = pd.concat([X_test_basic, X_test_psycho], axis=1)\n",
    "\n",
    "# Convert dense to sparse matrix for efficient stacking\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_dense_sparse = csr_matrix(X_train_dense.values)\n",
    "X_test_dense_sparse = csr_matrix(X_test_dense.values)\n",
    "\n",
    "# Stack TF-IDF (sparse) with dense features\n",
    "X_train = hstack([X_train_tfidf, X_train_dense_sparse])\n",
    "X_test = hstack([X_test_tfidf, X_test_dense_sparse])\n",
    "\n",
    "print(f\"Final training features shape: {X_train.shape}\")\n",
    "print(f\"Feature types: TF-IDF ({X_train_tfidf.shape[1]}) + Dense ({X_train_dense_sparse.shape[1]})\")\n",
    "\n",
    "# Target variable\n",
    "y_train = np.array([item['requester_received_pizza'] for item in train_data])\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Positive rate: {y_train.mean():.3f}\")\n",
    "\n",
    "# Calculate scale_pos_weight for LightGBM\n",
    "scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db86641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:26.939998Z",
     "iopub.status.busy": "2026-01-12T10:36:26.939523Z",
     "iopub.status.idle": "2026-01-12T10:36:26.944033Z",
     "shell.execute_reply": "2026-01-12T10:36:26.943502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up cross-validation...\n",
      "Starting 5-fold CV...\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "print(\"Setting up cross-validation...\")\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(train_data))\n",
    "test_predictions = np.zeros(len(test_data))\n",
    "\n",
    "# Feature importance tracking\n",
    "feature_importance_list = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold CV...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c9afa44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:26.946087Z",
     "iopub.status.busy": "2026-01-12T10:36:26.945457Z",
     "iopub.status.idle": "2026-01-12T10:36:30.064175Z",
     "shell.execute_reply": "2026-01-12T10:36:30.063647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tval's auc: 0.650592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[105]\tval's auc: 0.652401\n",
      "Fold 1 AUC: 0.6524\n",
      "\n",
      "Fold 2/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tval's auc: 0.572296\n",
      "Early stopping, best iteration is:\n",
      "[63]\tval's auc: 0.582051\n",
      "Fold 2 AUC: 0.5821\n",
      "\n",
      "Fold 3/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tval's auc: 0.667663\n",
      "Fold 3 AUC: 0.6677\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[15]\tval's auc: 0.603163\n",
      "Fold 4 AUC: 0.6032\n",
      "\n",
      "Fold 5/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[45]\tval's auc: 0.603293\n",
      "Fold 5 AUC: 0.6033\n",
      "\n",
      "==================================================\n",
      "CV AUC: 0.6217 ± 0.0326\n",
      "==================================================\n",
      "OOF AUC: 0.6207\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM model with cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Model parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': scale_pos_weight,  # Handle class imbalance\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_set],\n",
    "        valid_names=['val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate AUC for this fold\n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_auc:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    # Store feature importance\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importance_list.append(importance)\n",
    "\n",
    "# Overall CV score\n",
    "mean_auc = np.mean(cv_scores)\n",
    "std_auc = np.std(cv_scores)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CV AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Overall OOF AUC\n",
    "oof_auc = roc_auc_score(y_train, oof_predictions)\n",
    "print(f\"OOF AUC: {oof_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10e9013a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:30.069916Z",
     "iopub.status.busy": "2026-01-12T10:36:30.069624Z",
     "iopub.status.idle": "2026-01-12T10:36:30.096805Z",
     "shell.execute_reply": "2026-01-12T10:36:30.095999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing feature importance...\n",
      "\n",
      "Top 20 most important features:\n",
      "                     feature  importance\n",
      "10000            text_length  962.610988\n",
      "10005     upvotes_at_request  952.753360\n",
      "10002       account_age_days  817.467276\n",
      "10001             word_count  586.462694\n",
      "10018    food_specific_ratio  556.745971\n",
      "10007           request_hour  493.486891\n",
      "7164              tfidf_7164  462.433078\n",
      "10016        gratitude_ratio  409.539497\n",
      "10003    comments_at_request  401.731164\n",
      "10014           family_ratio  355.439571\n",
      "10010      reciprocity_ratio  340.353073\n",
      "10006  subreddits_at_request  333.723573\n",
      "10004       posts_at_request  331.293396\n",
      "4185              tfidf_4185  329.700911\n",
      "10008      request_dayofweek  294.658733\n",
      "10012         hardship_ratio  232.115586\n",
      "7856              tfidf_7856  228.050166\n",
      "2820              tfidf_2820  212.684536\n",
      "3614              tfidf_3614  210.209794\n",
      "5811              tfidf_5811  200.043876\n",
      "\n",
      "Among top 50 features:\n",
      "- TF-IDF features: 36\n",
      "- Dense features: 14\n",
      "\n",
      "Psycholinguistic features total importance: 2147.90\n",
      "Top psycholinguistic features:\n",
      "                   feature  importance\n",
      "10018  food_specific_ratio  556.745971\n",
      "10016      gratitude_ratio  409.539497\n",
      "10014         family_ratio  355.439571\n",
      "10010    reciprocity_ratio  340.353073\n",
      "10012       hardship_ratio  232.115586\n",
      "10009    reciprocity_count   77.523450\n",
      "10011       hardship_count   55.830461\n",
      "10017  food_specific_count   51.308434\n",
      "10015      gratitude_count   39.381406\n",
      "10013         family_count   29.658194\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "# Average feature importance across folds\n",
    "mean_importance = np.mean(feature_importance_list, axis=0)\n",
    "\n",
    "# Create feature names\n",
    "n_tfidf_features = X_train_tfidf.shape[1]\n",
    "dense_feature_names = list(X_train_dense.columns)\n",
    "feature_names = [f'tfidf_{i}' for i in range(n_tfidf_features)] + dense_feature_names\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': mean_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Check if TF-IDF features dominate\n",
    "n_top_features = 50\n",
    "tfidf_important = sum(1 for f in importance_df.head(n_top_features)['feature'] if f.startswith('tfidf_'))\n",
    "print(f\"\\nAmong top {n_top_features} features:\")\n",
    "print(f\"- TF-IDF features: {tfidf_important}\")\n",
    "print(f\"- Dense features: {n_top_features - tfidf_important}\")\n",
    "\n",
    "# Check psycholinguistic feature importance\n",
    "psycho_features = [f for f in feature_names if any(cat in f for cat in ['reciprocity', 'hardship', 'family', 'gratitude', 'food_specific'])]\n",
    "psycho_importance = importance_df[importance_df['feature'].isin(psycho_features)]\n",
    "print(f\"\\nPsycholinguistic features total importance: {psycho_importance['importance'].sum():.2f}\")\n",
    "print(\"Top psycholinguistic features:\")\n",
    "print(psycho_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47e1cb4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:30.102803Z",
     "iopub.status.busy": "2026-01-12T10:36:30.102527Z",
     "iopub.status.idle": "2026-01-12T10:36:30.123937Z",
     "shell.execute_reply": "2026-01-12T10:36:30.123108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating submission file...\n",
      "Sample submission shape: (1162, 2)\n",
      "Sample submission columns: ['request_id', 'requester_received_pizza']\n",
      "Submission shape: (1162, 2)\n",
      "Prediction range: [0.0889, 0.7291]\n",
      "Submission saved to: /home/submission/submission.csv\n",
      "Submission also saved to: /home/code/experiments/002_tfidf_text_features/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "print(\"\\nCreating submission file...\")\n",
    "\n",
    "# Load sample submission to get format\n",
    "sample_sub = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "print(f\"Sample submission columns: {sample_sub.columns.tolist()}\")\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': [item['request_id'] for item in test_data],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "\n",
    "# Also save to experiment folder\n",
    "import os\n",
    "os.makedirs('/home/code/experiments/002_tfidf_text_features', exist_ok=True)\n",
    "exp_submission_path = '/home/code/experiments/002_tfidf_text_features/submission.csv'\n",
    "submission.to_csv(exp_submission_path, index=False)\n",
    "print(f\"Submission also saved to: {exp_submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3384f03b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:30.129353Z",
     "iopub.status.busy": "2026-01-12T10:36:30.129009Z",
     "iopub.status.idle": "2026-01-12T10:36:30.136225Z",
     "shell.execute_reply": "2026-01-12T10:36:30.135547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "Model: LightGBM with TF-IDF + psycholinguistic + metadata features\n",
      "CV AUC: 0.6217 ± 0.0326\n",
      "OOF AUC: 0.6207\n",
      "Improvement over baseline: -0.0216\n",
      "\n",
      "Key findings:\n",
      "- TF-IDF features: 10000 terms\n",
      "- Psycholinguistic categories: 10 features\n",
      "- Dense metadata features: 19 features\n",
      "- TF-IDF dominates top features: 36/50\n",
      "- Psycholinguistic importance: 2147.90\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: LightGBM with TF-IDF + psycholinguistic + metadata features\")\n",
    "print(f\"CV AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "print(f\"OOF AUC: {oof_auc:.4f}\")\n",
    "print(f\"Improvement over baseline: {mean_auc - 0.6433:.4f}\")\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"- TF-IDF features: {n_tfidf_features} terms\")\n",
    "print(f\"- Psycholinguistic categories: {len(psycho_features)} features\")\n",
    "print(f\"- Dense metadata features: {len(dense_feature_names)} features\")\n",
    "print(f\"- TF-IDF dominates top features: {tfidf_important}/{n_top_features}\")\n",
    "print(f\"- Psycholinguistic importance: {psycho_importance['importance'].sum():.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a45a76c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:36:30.139393Z",
     "iopub.status.busy": "2026-01-12T10:36:30.139110Z",
     "iopub.status.idle": "2026-01-12T10:36:30.147801Z",
     "shell.execute_reply": "2026-01-12T10:36:30.147162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "Model: LightGBM with TF-IDF + psycholinguistic + metadata features\n",
      "CV AUC: 0.6217 ± 0.0326\n",
      "OOF AUC: 0.6207\n",
      "Improvement over baseline: -0.0216\n",
      "\n",
      "Key findings:\n",
      "- TF-IDF features: 10000 terms\n",
      "- Psycholinguistic categories: 10 features\n",
      "- Dense metadata features: 19 features\n",
      "- TF-IDF dominates top features: 36/50\n",
      "- Psycholinguistic importance: 2147.90\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: LightGBM with TF-IDF + psycholinguistic + metadata features\")\n",
    "print(f\"CV AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "print(f\"OOF AUC: {oof_auc:.4f}\")\n",
    "print(f\"Improvement over baseline: {mean_auc - 0.6433:.4f}\")\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"- TF-IDF features: {n_tfidf_features} terms\")\n",
    "print(f\"- Psycholinguistic categories: {len(psycho_features)} features\")\n",
    "print(f\"- Dense metadata features: {len(dense_feature_names)} features\")\n",
    "print(f\"- TF-IDF dominates top features: {tfidf_important}/{n_top_features}\")\n",
    "print(f\"- Psycholinguistic importance: {psycho_importance['importance'].sum():.2f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
