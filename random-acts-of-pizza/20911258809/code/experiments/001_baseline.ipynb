{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bd3694",
   "metadata": {},
   "source": [
    "# Baseline Model for Random Acts of Pizza\n",
    "\n",
    "This notebook creates a baseline model for predicting pizza request success using LightGBM with basic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0910e4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T08:57:05.210277Z",
     "iopub.status.busy": "2026-01-12T08:57:05.209424Z",
     "iopub.status.idle": "2026-01-12T08:57:06.967932Z",
     "shell.execute_reply": "2026-01-12T08:57:06.967244Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434a54d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129ace9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T08:57:06.970971Z",
     "iopub.status.busy": "2026-01-12T08:57:06.970353Z",
     "iopub.status.idle": "2026-01-12T08:57:07.087080Z",
     "shell.execute_reply": "2026-01-12T08:57:07.086352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "\n",
      "Class distribution in training data:\n",
      "requester_received_pizza\n",
      "False    2163\n",
      "True      715\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Convert to DataFrames for easier manipulation\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution in training data:\")\n",
    "print(train_df['requester_received_pizza'].value_counts())\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52ccea",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89e2f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T08:57:07.089489Z",
     "iopub.status.busy": "2026-01-12T08:57:07.089281Z",
     "iopub.status.idle": "2026-01-12T08:57:09.550169Z",
     "shell.execute_reply": "2026-01-12T08:57:09.549200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features for training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features for test data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (2878, 32)\n",
      "Test features shape: (1162, 32)\n",
      "\n",
      "NaN values in train features: 0\n",
      "NaN values in test features: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"Extract basic text features\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return {\n",
    "            'text_length': 0,\n",
    "            'word_count': 0,\n",
    "            'sentence_count': 0,\n",
    "            'avg_word_length': 0,\n",
    "            'exclamation_count': 0,\n",
    "            'question_count': 0,\n",
    "            'caps_count': 0,\n",
    "            'digit_count': 0\n",
    "        }\n",
    "    \n",
    "    # Basic text stats\n",
    "    text_length = len(text)\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentence_count = max(1, len([s for s in sentences if s.strip()]))\n",
    "    \n",
    "    # Character-based features\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    caps_count = sum(1 for c in text if c.isupper())\n",
    "    digit_count = sum(1 for c in text if c.isdigit())\n",
    "    \n",
    "    return {\n",
    "        'text_length': text_length,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'question_count': question_count,\n",
    "        'caps_count': caps_count,\n",
    "        'digit_count': digit_count\n",
    "    }\n",
    "\n",
    "def extract_metadata_features(row):\n",
    "    \"\"\"Extract metadata features - ONLY use features available at request time\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Account age at request (NOT retrieval)\n",
    "    features['account_age_days'] = row.get('requester_account_age_in_days_at_request', 0)\n",
    "    features['account_age_log'] = np.log1p(features['account_age_days'])\n",
    "    \n",
    "    # Activity features at request (NOT retrieval)\n",
    "    features['total_comments'] = row.get('requester_number_of_comments_at_request', 0)\n",
    "    features['total_posts'] = row.get('requester_number_of_posts_at_request', 0)\n",
    "    features['raop_comments'] = row.get('requester_number_of_comments_in_raop_at_request', 0)\n",
    "    features['raop_posts'] = row.get('requester_number_of_posts_on_raop_at_request', 0)\n",
    "    \n",
    "    # Voting features at request (NOT retrieval)\n",
    "    features['upvotes_minus_downvotes'] = row.get('requester_upvotes_minus_downvotes_at_request', 0)\n",
    "    features['upvotes_plus_downvotes'] = row.get('requester_upvotes_plus_downvotes_at_request', 0)\n",
    "    features['vote_ratio'] = features['upvotes_minus_downvotes'] / max(1, features['upvotes_plus_downvotes'])\n",
    "    \n",
    "    # Subreddit diversity at request\n",
    "    features['num_subreddits'] = row.get('requester_number_of_subreddits_at_request', 0)\n",
    "    \n",
    "    # Time features from request timestamp\n",
    "    timestamp = row.get('unix_timestamp_of_request', 0)\n",
    "    if timestamp and timestamp > 0:\n",
    "        features['hour_of_day'] = pd.to_datetime(timestamp, unit='s').hour\n",
    "        features['day_of_week'] = pd.to_datetime(timestamp, unit='s').dayofweek\n",
    "    else:\n",
    "        features['hour_of_day'] = 0\n",
    "        features['day_of_week'] = 0\n",
    "    \n",
    "    # User flair (this is a static attribute, not time-dependent)\n",
    "    flair = row.get('requester_user_flair')\n",
    "    features['has_flair'] = 1 if flair is not None else 0\n",
    "    features['flair_is_shroom'] = 1 if flair == 'shroom' else 0\n",
    "    features['flair_is_pif'] = 1 if flair == 'PIF' else 0\n",
    "    \n",
    "    # Days since first post on RAOP at request\n",
    "    features['days_since_first_raop'] = row.get('requester_days_since_first_post_on_raop_at_request', 0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create all features for a dataframe\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Text features from title and request text\n",
    "        title_features = extract_text_features(row.get('request_title', ''))\n",
    "        text_features = extract_text_features(row.get('request_text_edit_aware', ''))\n",
    "        \n",
    "        # Metadata features\n",
    "        meta_features = extract_metadata_features(row)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = {}\n",
    "        for prefix, feat_dict in [('title_', title_features), ('text_', text_features), ('meta_', meta_features)]:\n",
    "            for key, value in feat_dict.items():\n",
    "                combined_features[prefix + key] = value\n",
    "        \n",
    "        features_list.append(combined_features)\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# Create features for train and test\n",
    "print(\"Creating features for training data...\")\n",
    "train_features = create_features(train_df)\n",
    "\n",
    "print(\"Creating features for test data...\")\n",
    "test_features = create_features(test_df)\n",
    "\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "# Check for any NaN values\n",
    "print(f\"\\nNaN values in train features: {train_features.isnull().sum().sum()}\")\n",
    "print(f\"NaN values in test features: {test_features.isnull().sum().sum()}\")\n",
    "\n",
    "# Fill any NaN values with 0\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = test_features.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfa2c7",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89076d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T08:57:09.553532Z",
     "iopub.status.busy": "2026-01-12T08:57:09.552838Z",
     "iopub.status.idle": "2026-01-12T08:57:10.056732Z",
     "shell.execute_reply": "2026-01-12T08:57:10.055780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 5-fold stratified cross-validation...\n",
      "\n",
      "Fold 1/5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 1\n",
      "Fold 1 AUC: 1.0000\n",
      "\n",
      "Fold 2/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 1\n",
      "Fold 2 AUC: 1.0000\n",
      "\n",
      "Fold 3/5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 1\n",
      "Fold 3 AUC: 1.0000\n",
      "\n",
      "Fold 4/5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 1\n",
      "Fold 4 AUC: 1.0000\n",
      "\n",
      "Fold 5/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 1\n",
      "Fold 5 AUC: 1.0000\n",
      "\n",
      "Overall CV AUC: 1.0000\n",
      "Mean CV AUC: 1.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "X = train_features\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions and scores\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "test_predictions = np.zeros(len(test_features))\n",
    "\n",
    "print(f\"Training with {n_splits}-fold stratified cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store predictions\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    test_predictions += test_pred / n_splits\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_score:.4f}\")\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_score = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_score:.4f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588fd0d7",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14035b2b",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the submission has the correct format\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].astype(float)\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nFirst 5 rows of submission:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
