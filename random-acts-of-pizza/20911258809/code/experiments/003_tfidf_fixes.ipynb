{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d999b6d7",
   "metadata": {},
   "source": [
    "# Experiment 003: TF-IDF Noise Fixes\n",
    "\n",
    "**Goal**: Fix the TF-IDF noise problem that caused a 0.0216 AUC drop in exp_002\n",
    "\n",
    "**Key Changes**:\n",
    "1. Remove stop_words='english' to keep domain-specific words\n",
    "2. Reduce TF-IDF features from 10,000 to 3,000 using chi-square selection\n",
    "3. Add character n-grams (3-5) for robust pattern matching\n",
    "4. Improve psycholinguistic features with phrase patterns and word boundaries\n",
    "5. Add high-value specific features (imgur links, key phrases)\n",
    "\n",
    "**Expected Outcome**: Recover to baseline (0.6433) and ideally exceed 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8593d826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:54:20.510863Z",
     "iopub.status.busy": "2026-01-12T11:54:20.509959Z",
     "iopub.status.idle": "2026-01-12T11:54:20.646820Z",
     "shell.execute_reply": "2026-01-12T11:54:20.646221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train samples: 2878\n",
      "Test samples: 1162\n",
      "Positive rate: 0.2484\n",
      "Train shape: (2878, 32)\n",
      "Test shape: (1162, 17)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Positive rate: {sum([x['requester_received_pizza'] for x in train_data]) / len(train_data):.4f}\")\n",
    "\n",
    "# Convert to DataFrames for easier processing\n",
    "train = pd.DataFrame(train_data)\n",
    "test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cba690",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Positive rate: {sum([x['requester_received_pizza'] for x in train_data]) / len(train_data):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0157cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:54:20.648841Z",
     "iopub.status.busy": "2026-01-12T11:54:20.648623Z",
     "iopub.status.idle": "2026-01-12T11:54:20.655056Z",
     "shell.execute_reply": "2026-01-12T11:54:20.654527Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_metadata_features(df):\n",
    "    \"\"\"Extract metadata features that are safe from leakage\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Text length features\n",
    "    features['text_length'] = df['request_text_edit_aware'].fillna('').str.len()\n",
    "    features['word_count'] = df['request_text_edit_aware'].fillna('').str.split().str.len()\n",
    "    features['sentence_count'] = df['request_text_edit_aware'].fillna('').str.count(r'[.!?]+') + 1\n",
    "    \n",
    "    # Account age (convert to days)\n",
    "    features['account_age_days'] = df['requester_account_age_in_days_at_request']\n",
    "    features['account_age_log'] = np.log1p(features['account_age_days'])\n",
    "    \n",
    "    # Activity features (at request time)\n",
    "    features['total_comments_at_request'] = df['requester_number_of_comments_at_request']\n",
    "    features['total_posts_at_request'] = df['requester_number_of_posts_at_request']\n",
    "    features['raop_comments_at_request'] = df['requester_number_of_comments_in_raop_at_request']\n",
    "    features['raop_posts_at_request'] = df['requester_number_of_posts_on_raop_at_request']\n",
    "    \n",
    "    # Upvotes/downvotes (at request time)\n",
    "    features['upvotes_minus_downvotes_at_request'] = df['requester_upvotes_minus_downvotes_at_request']\n",
    "    features['upvotes_plus_downvotes_at_request'] = df['requester_upvotes_plus_downvotes_at_request']\n",
    "    features['vote_ratio_at_request'] = features['upvotes_minus_downvotes_at_request'] / (features['upvotes_plus_downvotes_at_request'] + 1)\n",
    "    \n",
    "    # Subreddit diversity\n",
    "    features['num_subreddits_at_request'] = df['requester_number_of_subreddits_at_request']\n",
    "    \n",
    "    # Time features\n",
    "    features['hour_of_day'] = pd.to_datetime(df['unix_timestamp_of_request'], unit='s').dt.hour\n",
    "    features['day_of_week'] = pd.to_datetime(df['unix_timestamp_of_request'], unit='s').dt.dayofweek\n",
    "    \n",
    "    # Days since first RAOP post\n",
    "    features['days_since_first_raop_at_request'] = df['requester_days_since_first_post_on_raop_at_request']\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921e3d8",
   "metadata": {},
   "source": [
    "## 2. Improved TF-IDF Features\n",
    "\n",
    "**Fixes**:\n",
    "- Remove stop_words='english' to keep domain words\n",
    "- Reduce to 3,000 features using chi-square selection\n",
    "- Add character n-grams (3-5) for robust patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05cb6321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T11:54:20.657008Z",
     "iopub.status.busy": "2026-01-12T11:54:20.656819Z",
     "iopub.status.idle": "2026-01-12T11:54:26.210444Z",
     "shell.execute_reply": "2026-01-12T11:54:26.209802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF shape: (2878, 5000)\n",
      "Char TF-IDF shape: (2878, 2000)\n",
      "Applying chi-square feature selection...\n",
      "Selected word TF-IDF shape: (2878, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values with empty string\n",
    "train_text = train['request_text_edit_aware'].fillna('')\n",
    "test_text = test['request_text_edit_aware'].fillna('')\n",
    "\n",
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "# Word n-grams (1-3) - WITHOUT stop words removal\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=5000,  # Start with more, then select top 3k\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=None,  # Keep domain-specific words!\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Character n-grams (3-5) - more robust for short text\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=2000,  # Fewer char n-grams needed\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_word_train = word_vectorizer.fit_transform(train_text)\n",
    "X_char_train = char_vectorizer.fit_transform(train_text)\n",
    "\n",
    "# Transform test data\n",
    "X_word_test = word_vectorizer.transform(test_text)\n",
    "X_char_test = char_vectorizer.transform(test_text)\n",
    "\n",
    "print(f\"Word TF-IDF shape: {X_word_train.shape}\")\n",
    "print(f\"Char TF-IDF shape: {X_char_train.shape}\")\n",
    "\n",
    "# Apply feature selection to word n-grams (keep top 3000 most predictive)\n",
    "print(\"Applying chi-square feature selection...\")\n",
    "selector = SelectKBest(chi2, k=3000)\n",
    "X_word_train_selected = selector.fit_transform(X_word_train, train['requester_received_pizza'])\n",
    "X_word_test_selected = selector.transform(X_word_test)\n",
    "\n",
    "print(f\"Selected word TF-IDF shape: {X_word_train_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5cc0cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:05:06.861497Z",
     "iopub.status.busy": "2026-01-12T12:05:06.860708Z",
     "iopub.status.idle": "2026-01-12T12:05:06.981270Z",
     "shell.execute_reply": "2026-01-12T12:05:06.980415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata features...\n",
      "Metadata features shape: (2878, 16)\n",
      "Metadata features: ['text_length', 'word_count', 'sentence_count', 'account_age_days', 'account_age_log', 'total_comments_at_request', 'total_posts_at_request', 'raop_comments_at_request', 'raop_posts_at_request', 'upvotes_minus_downvotes_at_request', 'upvotes_plus_downvotes_at_request', 'vote_ratio_at_request', 'num_subreddits_at_request', 'hour_of_day', 'day_of_week', 'days_since_first_raop_at_request']\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata features\n",
    "print(\"Extracting metadata features...\")\n",
    "X_meta_train = extract_metadata_features(train)\n",
    "X_meta_test = extract_metadata_features(test)\n",
    "\n",
    "print(f\"Metadata features shape: {X_meta_train.shape}\")\n",
    "print(f\"Metadata features: {X_meta_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0933e4",
   "metadata": {},
   "source": [
    "## 3. Improved Psycholinguistic Features\n",
    "\n",
    "**Fixes**:\n",
    "- Use phrase patterns instead of single words\n",
    "- Add word boundaries to prevent false positives\n",
    "- Add intensity modifiers (adverb + adjective patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73417e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:05:06.983428Z",
     "iopub.status.busy": "2026-01-12T12:05:06.983214Z",
     "iopub.status.idle": "2026-01-12T12:05:08.211353Z",
     "shell.execute_reply": "2026-01-12T12:05:08.210760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating psycholinguistic features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psycholinguistic features shape: (2878, 12)\n",
      "Psycholinguistic features: ['reciprocity_phrases', 'hardship_phrases', 'gratitude_phrases', 'family_phrases', 'pizza_terms', 'intensity_modifiers', 'reciprocity_phrases_ratio', 'hardship_phrases_ratio', 'gratitude_phrases_ratio', 'family_phrases_ratio', 'pizza_terms_ratio', 'intensity_modifiers_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Extract psycholinguistic features\n",
    "print(\"Creating psycholinguistic features...\")\n",
    "X_psych_train = create_psycholinguistic_features(train)\n",
    "X_psych_test = create_psycholinguistic_features(test)\n",
    "\n",
    "print(f\"Psycholinguistic features shape: {X_psych_train.shape}\")\n",
    "print(f\"Psycholinguistic features: {X_psych_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "793febf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:05:08.213458Z",
     "iopub.status.busy": "2026-01-12T12:05:08.213253Z",
     "iopub.status.idle": "2026-01-12T12:05:09.434975Z",
     "shell.execute_reply": "2026-01-12T12:05:09.434211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating psycholinguistic features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psycholinguistic features shape: (2878, 12)\n"
     ]
    }
   ],
   "source": [
    "def create_psycholinguistic_features(df):\n",
    "    \"\"\"Create improved psycholinguistic features with phrase patterns\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    text = df['request_text_edit_aware'].fillna('').str.lower()\n",
    "    \n",
    "    # Reciprocity phrases (with word boundaries)\n",
    "    reciprocity_patterns = [\n",
    "        r'\\bpay back\\b', r'\\breturn the favor\\b', r'\\bwhen i get paid\\b',\n",
    "        r'\\bpayday\\b', r'\\bnext week\\b', r'\\bnext month\\b',\n",
    "        r'\\bwill pay\\b', r'\\bcan pay\\b', r'\\bpay it forward\\b'\n",
    "    ]\n",
    "    features['reciprocity_phrases'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in reciprocity_patterns))\n",
    "    \n",
    "    # Hardship phrases\n",
    "    hardship_patterns = [\n",
    "        r'\\blost my job\\b', r'\\bmedical bills\\b', r'\\bcar broke down\\b',\n",
    "        r'\\beviction notice\\b', r'\\bsingle parent\\b', r'\\bno money\\b',\n",
    "        r'\\bcan\\'t afford\\b', r'\\bunemployed\\b', r'\\bhomeless\\b'\n",
    "    ]\n",
    "    features['hardship_phrases'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in hardship_patterns))\n",
    "    \n",
    "    # Gratitude phrases\n",
    "    gratitude_patterns = [\n",
    "        r'\\bwould appreciate\\b', r'\\bwould be grateful\\b', r'\\bthank you in advance\\b',\n",
    "        r'\\bbless you\\b', r'\\bso thankful\\b', r'\\bvery grateful\\b'\n",
    "    ]\n",
    "    features['gratitude_phrases'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in gratitude_patterns))\n",
    "    \n",
    "    # Family phrases\n",
    "    family_patterns = [\n",
    "        r'\\bmy kids\\b', r'\\bmy children\\b', r'\\bmy family\\b',\n",
    "        r'\\bsingle mom\\b', r'\\bsingle dad\\b', r'\\bmy daughter\\b', r'\\bmy son\\b'\n",
    "    ]\n",
    "    features['family_phrases'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in family_patterns))\n",
    "    \n",
    "    # Pizza-specific terms (with word boundaries)\n",
    "    pizza_words = ['pizza', 'pizzas', 'pizzeria', 'dominos', 'papa johns', 'pizza hut', 'little caesars']\n",
    "    pizza_pattern = r'\\b(' + '|'.join(pizza_words) + r')\\b'\n",
    "    features['pizza_terms'] = text.str.count(pizza_pattern)\n",
    "    \n",
    "    # Intensity modifiers (adverb + adjective patterns)\n",
    "    intensity_patterns = [\n",
    "        r'\\breally\\s+\\w+\\b', r'\\bso\\s+\\w+\\b', r'\\bvery\\s+\\w+\\b',\n",
    "        r'\\bextremely\\s+\\w+\\b', r'\\bsuper\\s+\\w+\\b'\n",
    "    ]\n",
    "    features['intensity_modifiers'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in intensity_patterns))\n",
    "    \n",
    "    # Normalize by text length (words)\n",
    "    word_count = text.str.split().str.len().replace(0, 1)  # Avoid division by zero\n",
    "    for col in features.columns:\n",
    "        features[f'{col}_ratio'] = features[col] / word_count\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Creating psycholinguistic features...\")\n",
    "X_psych_train = create_psycholinguistic_features(train)\n",
    "X_psych_test = create_psycholinguistic_features(test)\n",
    "\n",
    "print(f\"Psycholinguistic features shape: {X_psych_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022aa895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:05:09.438105Z",
     "iopub.status.busy": "2026-01-12T12:05:09.437418Z",
     "iopub.status.idle": "2026-01-12T12:05:09.925749Z",
     "shell.execute_reply": "2026-01-12T12:05:09.925092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating specific features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific features shape: (2878, 8)\n",
      "Specific features: ['imgur_link_count', 'has_imgur_link', 'need_phrases', 'time_references', 'question_marks', 'exclamation_marks', 'need_phrases_ratio', 'time_references_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Extract specific features\n",
    "print(\"Creating specific features...\")\n",
    "X_specific_train = create_specific_features(train)\n",
    "X_specific_test = create_specific_features(test)\n",
    "\n",
    "print(f\"Specific features shape: {X_specific_train.shape}\")\n",
    "print(f\"Specific features: {X_specific_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b1496",
   "metadata": {},
   "source": [
    "## 4. High-Value Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "102ff56a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:06:48.691097Z",
     "iopub.status.busy": "2026-01-12T12:06:48.689568Z",
     "iopub.status.idle": "2026-01-12T12:06:48.741357Z",
     "shell.execute_reply": "2026-01-12T12:06:48.740507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all features...\n",
      "Combined training features shape: (2878, 5036)\n",
      "Combined test features shape: (1162, 5036)\n",
      "Target distribution: [2163  715]\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature matrices\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Convert dense features to sparse format for efficient stacking\n",
    "X_meta_train_sparse = csr_matrix(X_meta_train.values)\n",
    "X_meta_test_sparse = csr_matrix(X_meta_test.values)\n",
    "\n",
    "X_psych_train_sparse = csr_matrix(X_psych_train.values)\n",
    "X_psych_test_sparse = csr_matrix(X_psych_test.values)\n",
    "\n",
    "X_specific_train_sparse = csr_matrix(X_specific_train.values)\n",
    "X_specific_test_sparse = csr_matrix(X_specific_test.values)\n",
    "\n",
    "# Stack all features\n",
    "X_train_combined = hstack([\n",
    "    X_word_train_selected,  # 3000 selected word n-grams\n",
    "    X_char_train,             # 2000 char n-grams\n",
    "    X_meta_train_sparse,      # metadata features\n",
    "    X_psych_train_sparse,     # psycholinguistic features\n",
    "    X_specific_train_sparse   # specific high-value features\n",
    "], format='csr')\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_word_test_selected,\n",
    "    X_char_test,\n",
    "    X_meta_test_sparse,\n",
    "    X_psych_test_sparse,\n",
    "    X_specific_test_sparse\n",
    "], format='csr')\n",
    "\n",
    "print(f\"Combined training features shape: {X_train_combined.shape}\")\n",
    "print(f\"Combined test features shape: {X_test_combined.shape}\")\n",
    "\n",
    "# Target variable\n",
    "y = train['requester_received_pizza'].values\n",
    "print(f\"Target distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a69a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:06:48.744651Z",
     "iopub.status.busy": "2026-01-12T12:06:48.743873Z",
     "iopub.status.idle": "2026-01-12T12:06:49.220354Z",
     "shell.execute_reply": "2026-01-12T12:06:49.219550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating specific features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific features shape: (2878, 8)\n"
     ]
    }
   ],
   "source": [
    "def create_specific_features(df):\n",
    "    \"\"\"Create targeted features based on analysis findings\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    text = df['request_text_edit_aware'].fillna('').str.lower()\n",
    "    \n",
    "    # Imgur links (2.6x more common in successful requests)\n",
    "    features['imgur_link_count'] = text.str.count(r'imgur\\.com')\n",
    "    features['has_imgur_link'] = (features['imgur_link_count'] > 0).astype(int)\n",
    "    \n",
    "    # Key phrases that indicate need/specificity\n",
    "    need_patterns = [\n",
    "        r'\\bneed\\b', r'\\bplease\\b', r'\\bhelp\\b', r'\\banyone\\b',\n",
    "        r'\\bkind\\b', r'\\bgenerous\\b', r'\\bappreciate\\b'\n",
    "    ]\n",
    "    features['need_phrases'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in need_patterns))\n",
    "    \n",
    "    # Time references (indicates planning/payback ability)\n",
    "    time_patterns = [\n",
    "        r'\\bnext week\\b', r'\\bnext month\\b', r'\\bthis week\\b',\n",
    "        r'\\bthis month\\b', r'\\btomorrow\\b', r'\\bsoon\\b'\n",
    "    ]\n",
    "    features['time_references'] = text.apply(lambda x: sum(len(re.findall(p, x)) for p in time_patterns))\n",
    "    \n",
    "    # Question marks (might indicate asking/pleading)\n",
    "    features['question_marks'] = text.str.count(r'\\?')\n",
    "    \n",
    "    # Exclamation marks (might indicate urgency/emotion)\n",
    "    features['exclamation_marks'] = text.str.count(r'!')\n",
    "    \n",
    "    # Normalize by text length\n",
    "    word_count = text.str.split().str.len().replace(0, 1)\n",
    "    for col in ['need_phrases', 'time_references']:\n",
    "        features[f'{col}_ratio'] = features[col] / word_count\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Creating specific features...\")\n",
    "X_specific_train = create_specific_features(train)\n",
    "X_specific_test = create_specific_features(test)\n",
    "\n",
    "print(f\"Specific features shape: {X_specific_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6af7b",
   "metadata": {},
   "source": [
    "## 5. Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0329e31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T12:08:39.311416Z",
     "iopub.status.busy": "2026-01-12T12:08:39.310562Z",
     "iopub.status.idle": "2026-01-12T12:08:39.346702Z",
     "shell.execute_reply": "2026-01-12T12:08:39.345983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all features...\n",
      "Combined training features shape: (2878, 5036)\n",
      "Combined test features shape: (1162, 5036)\n",
      "Target distribution: [2163  715]\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature matrices\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "# Stack sparse matrices (TF-IDF) and dense matrices (metadata, psych, specific)\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Convert dense features to sparse format for efficient stacking\n",
    "X_meta_train_sparse = csr_matrix(X_meta_train.values)\n",
    "X_meta_test_sparse = csr_matrix(X_meta_test.values)\n",
    "\n",
    "X_psych_train_sparse = csr_matrix(X_psych_train.values)\n",
    "X_psych_test_sparse = csr_matrix(X_psych_test.values)\n",
    "\n",
    "X_specific_train_sparse = csr_matrix(X_specific_train.values)\n",
    "X_specific_test_sparse = csr_matrix(X_specific_test.values)\n",
    "\n",
    "# Stack all features\n",
    "X_train_combined = hstack([\n",
    "    X_word_train_selected,  # 3000 selected word n-grams\n",
    "    X_char_train,             # 2000 char n-grams\n",
    "    X_meta_train_sparse,      # metadata features\n",
    "    X_psych_train_sparse,     # psycholinguistic features\n",
    "    X_specific_train_sparse   # specific high-value features\n",
    "], format='csr')\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_word_test_selected,\n",
    "    X_char_test,\n",
    "    X_meta_test_sparse,\n",
    "    X_psych_test_sparse,\n",
    "    X_specific_test_sparse\n",
    "], format='csr')\n",
    "\n",
    "print(f\"Combined training features shape: {X_train_combined.shape}\")\n",
    "print(f\"Combined test features shape: {X_test_combined.shape}\")\n",
    "\n",
    "# Target variable\n",
    "y = train['requester_received_pizza'].values\n",
    "print(f\"Target distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de75402",
   "metadata": {},
   "source": [
    "## 6. Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold stratified CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Class imbalance handling\n",
    "scale_pos_weight = (len(y) - sum(y)) / sum(y)\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "# Store predictions\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(train))\n",
    "test_predictions = np.zeros(len(test))\n",
    "\n",
    "print(\"\\nTraining LightGBM model with 5-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_combined, y)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = X_train_combined[train_idx]\n",
    "    X_val_fold = X_train_combined[val_idx]\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "    \n",
    "    # Parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = roc_auc_score(y_val_fold, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_score:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(X_test_combined, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / 5\n",
    "\n",
    "# Overall CV score\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "print(f\"\\nCV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "\n",
    "# OOF score\n",
    "oof_score = roc_auc_score(y, oof_predictions)\n",
    "print(f\"OOF AUC: {oof_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9de827",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cf89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names for analysis\n",
    "feature_names = []\n",
    "\n",
    "# Word n-gram features (selected)\n",
    "word_feature_names = word_vectorizer.get_feature_names_out()\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = word_feature_names[selected_mask]\n",
    "feature_names.extend([f'word_{f}' for f in selected_features])\n",
    "\n",
    "# Char n-gram features\n",
    "char_feature_names = char_vectorizer.get_feature_names_out()\n",
    "feature_names.extend([f'char_{f}' for f in char_feature_names])\n",
    "\n",
    "# Metadata features\n",
    "feature_names.extend(X_meta_train.columns.tolist())\n",
    "\n",
    "# Psycholinguistic features\n",
    "feature_names.extend(X_psych_train.columns.tolist())\n",
    "\n",
    "# Specific features\n",
    "feature_names.extend(X_specific_train.columns.tolist())\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "\n",
    "# Get feature importances from final model\n",
    "importances = model.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features by importance:\")\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# Analyze feature types\n",
    "feature_importance_df['feature_type'] = feature_importance_df['feature'].apply(lambda x: x.split('_')[0])\n",
    "type_importance = feature_importance_df.groupby('feature_type')['importance'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nImportance by feature type:\")\n",
    "print(type_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893909a1",
   "metadata": {},
   "source": [
    "## 8. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "# Check sample submission format\n",
    "sample_sub = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(f\"Sample submission columns: {sample_sub.columns.tolist()}\")\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"EXPERIMENT SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: LightGBM with improved TF-IDF features\")\n",
    "print(f\"CV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"OOF AUC: {oof_score:.4f}\")\n",
    "print(f\"Baseline AUC: 0.6433\")\n",
    "print(f\"Improvement: {cv_mean - 0.6433:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
