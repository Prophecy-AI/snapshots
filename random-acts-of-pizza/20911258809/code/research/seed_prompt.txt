## Current Status
- Best CV: 0.6433 from exp_000 (baseline metadata only)
- Latest experiment exp_001 scored 0.6217 - DROPPED 0.0216 points after adding TF-IDF + psycholinguistic features
- **Critical issue**: Text features are adding noise, not signal

## Response to Evaluator
- **Technical verdict was TRUSTWORTHY**. Execution is sound, but feature engineering has fundamental issues.
- **Evaluator's top priority: Diagnose and fix TF-IDF noise problem**. I completely agree - the 0.0216 drop is alarming and must be addressed before any other improvements.
- **Key concerns raised**:
  - *"TF-IDF is too generic"* → Using stop_words='english' removes domain-specific words like 'pizza', 'please', 'help'
  - *"Psycholinguistic features are too crude"* → Simple word counting creates false positives ("pay attention" counts as reciprocity)
  - *"No analysis of predictive patterns"* → Need to identify which specific n-grams actually drive predictions
  - *"10,000 features is too many"* → High dimensionality causing overfitting

**My response**: The evaluator correctly identified that we're adding quantity over quality. We need to:
1. **Reduce dimensionality** - Use feature selection to keep only predictive n-grams
2. **Improve text representation** - Try character n-grams which work better for short text
3. **Refine psycholinguistic features** - Use phrase patterns and word boundaries
4. **Analyze what works** - Identify predictive patterns before adding more features

## Data Understanding
- **Reference notebooks**: 
  - `exploration/evolver_loop2_analysis.ipynb` - TF-IDF configuration analysis
  - `exploration/evolver_loop1_analysis.ipynb` - Text pattern correlations
- **Key patterns discovered**:
  - **Imgur links**: 2.6x more common in successful requests (6.9% vs 2.6%) - visual evidence is highly predictive
  - **Text length**: Successful requests are 26% longer (89.5 vs 71.1 words)
  - **All psycholinguistic categories higher in successful requests**: family_terms (1.22x), gratitude_terms (1.20x), reciprocity_terms (1.19x), hardship_terms (1.14x), pizza_terms (1.10x)
  - **But differences are modest**: Simple category ratios alone insufficient - need better feature engineering

## Recommended Approaches (Priority Order)

### 1. FIX TF-IDF NOISE PROBLEM (HIGHEST PRIORITY)
**Goal**: Reduce from 10,000 noisy features to 2,000-3,000 predictive features

**Actions**:
- **Remove stop_words='english'**: Keep domain-specific words ('pizza', 'please', 'help', 'hungry')
- **Add character n-grams**: Use `analyzer='char'`, `ngram_range=(3,5)` to capture patterns like 'pizza', 'please', 'help' - more robust to spelling variations
- **Feature selection**: Use chi-square test to select top 2,000-3,000 most predictive n-grams
- **Try both separately**: Test word n-grams vs character n-grams vs combination

**Why**: Research shows character-level models work better for short noisy text. Analysis shows standard TF-IDF with English stop words is removing important domain vocabulary.

### 2. IMPROVE PSYCHOLINGUISTIC FEATURES
**Goal**: Move from crude word counting to phrase-based patterns

**Actions**:
- **Use phrase patterns**: Instead of counting "pay", detect "pay back", "return the favor", "when I get paid", "pay it forward"
- **Add word boundaries**: Use regex `\bword\b` to prevent false positives ("pay attention" vs "pay back")
- **Add intensity modifiers**: Count phrases like "really hungry", "so broke", "very grateful" (adverb + adjective patterns)
- **Position features**: Create features for key terms appearing in first/last sentence vs middle

**Why**: Evaluator correctly identified that simple substring matching creates false positives. Phrase patterns capture context and meaning better.

### 3. ADD TOPIC MODELING FEATURES
**Goal**: Extract semantic topics from text as dense features

**Actions**:
- **Run LDA**: Use `LatentDirichletAllocation` on TF-IDF matrix to extract 10-20 topics
- **Use NMF**: Try Non-negative Matrix Factorization as alternative
- **Add topic distributions**: For each post, add features showing probability of each topic
- **Interpret topics**: Manually inspect top words per topic to ensure meaningful themes (hardship, family, gratitude, etc.)

**Why**: Research on competition winners shows topic modeling captures high-level semantic structure that bag-of-words misses. Provides dense features that complement sparse TF-IDF.

### 4. ENGINEER HIGH-VALUE SPECIFIC FEATURES
**Goal**: Create targeted features based on analysis findings

**Actions**:
- **Imgur link feature**: Binary flag + count of imgur links (2.6x more common in successes)
- **Text length features**: Word count, character count, sentence count (successful requests are longer)
- **Key phrase detectors**: 
  - Reciprocity phrases: "pay back", "return the favor", "when I get paid", "payday", "next week", "next month"
  - Hardship phrases: "lost my job", "medical bills", "car broke down", "eviction notice", "single parent"
  - Gratitude phrases: "would appreciate", "would be grateful", "thank you in advance", "bless you"
- **Pizza-specific terms**: Count mentions of specific chains (dominos, papajohns, pizzahut) - shows specificity

**Why**: Analysis identified specific patterns that are predictive. Engineering these directly is more reliable than hoping TF-IDF learns them.

### 5. TEST DIFFERENT MODEL APPROACHES
**Goal**: Find best model for the improved feature set

**Actions**:
- **Logistic Regression**: With class_weight='balanced', good baseline for sparse features
- **LightGBM**: With scale_pos_weight=3.025, handles mixed sparse/dense features
- **Naive Bayes**: Specifically MultinomialNB, works well with TF-IDF features
- **Compare**: Run all three on same feature set to identify best performer

**Why**: Different models have different strengths with text features. Need to find which works best for our specific feature engineering approach.

### 6. IMPLEMENT PROPER FEATURE ANALYSIS
**Goal**: Understand what's working to guide further engineering

**Actions**:
- **After each experiment**: Extract feature importances/coefficients
- **Analyze top features**: Identify which specific n-grams, phrases, and metadata drive predictions
- **Create diagnostic plots**: Feature importance distributions, coefficient values for linear models
- **Iterate based on findings**: If certain phrase patterns are strong, engineer more variations

**Why**: Evaluator emphasized we need to analyze predictive patterns. Can't improve without understanding what's working.

## What NOT to Try
- **More TF-IDF features**: 10,000 was already too many. Focus on quality over quantity.
- **Hyperparameter tuning**: Feature quality is the problem, not hyperparameters. Fix features first.
- **Complex ensembles**: Need better base models before ensembling helps.
- **Neural networks/LLMs**: Too complex and slow. Stick with proven Kaggle approaches (TF-IDF + linear models + gradient boosting).
- **Advanced preprocessing**: Lemmatization, stemming, etc. - minimal impact compared to fixing core feature engineering issues.

## Validation Notes
- **CV scheme**: Continue with 5-fold stratified CV (proven reliable)
- **Leakage prevention**: Continue using only `request_text_edit_aware` and "at_request" timestamps
- **Class imbalance**: Continue with scale_pos_weight=3.025 (LightGBM) or class_weight='balanced' (Logistic Regression)
- **Success criteria**: Next experiment should recover to baseline (0.6433) and ideally exceed 0.70
- **Confidence**: Medium - We identified the problems, but need to verify the fixes work

## Expected Outcomes
- **Minimum**: Recover to baseline 0.6433 AUC by fixing TF-IDF noise
- **Target**: Achieve 0.70+ AUC with improved feature engineering
- **Stretch**: Reach 0.75+ AUC by combining all recommended approaches

## Implementation Order
1. Start with TF-IDF fixes (remove stop words, add character n-grams, feature selection)
2. Add improved psycholinguistic features (phrase patterns, word boundaries)
3. Engineer specific high-value features (imgur links, key phrases)
4. Test different models (Logistic Regression, LightGBM, Naive Bayes)
5. Add topic modeling if time permits
6. Always analyze feature importances to guide next steps