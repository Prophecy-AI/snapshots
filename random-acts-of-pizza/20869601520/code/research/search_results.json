{
  "query": "How do Kaggle winners encode high-cardinality categorical features with 20K+ unique values?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the current search results, the available information only covers general advice for handling high\u2011cardinality categorical variables\u2014such as matching cardinality between train and test sets, bucketing low\u2011frequency categories, and avoiding one\u2011hot encoding when there are hundreds of levels\u2014but it does **not** specify the exact encoding techniques (e.g., target encoding, hashing, embeddings, etc.) that Kaggle competition winners use for features with\u202f20\u202f000\u202f+ unique values\u30103\u2020L1-L9\u3011. Consequently, a detailed, step\u2011by\u2011step description of the winners\u2019 encoding strategy cannot be provided from the sources at hand.",
      "url": ""
    },
    {
      "title": "Handling High Cardinality Categorical Features: From Basics to Super-Advanced",
      "text": "Handling High Cardinality Categorical Features: From Basics to Super-Advanced | by Adnan Mazraeh | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Handling High Cardinality Categorical Features: From Basics to Super-Advanced\n[\n![Adnan Mazraeh](https://miro.medium.com/v2/resize:fill:64:64/1*FDIg4KL7Wt6hpW-ID7zkow.jpeg)\n](https://medium.com/@adnan.mazraeh1993?source=post_page---byline--45dd9949b31b---------------------------------------)\n[Adnan Mazraeh](https://medium.com/@adnan.mazraeh1993?source=post_page---byline--45dd9949b31b---------------------------------------)\n3 min read\n\u00b7Mar 7, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/45dd9949b31b&amp;operation=register&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;user=Adnan+Mazraeh&amp;userId=4d6bb2142c5f&amp;source=---header_actions--45dd9949b31b---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/45dd9949b31b&amp;operation=register&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;source=---header_actions--45dd9949b31b---------------------bookmark_footer------------------)\nListen\nShare\n## 1. Introduction\nHigh-cardinality categorical features refer to categorical variables with a large number of unique values (e.g., user IDs, product codes, city names). Handling them efficiently is crucial for avoiding performance issues and improving model accuracy.\n## Examples of High-Cardinality Features\n* **Zip codes**in customer data.\n* **Product IDs**in an e-commerce dataset.\n* **Device types**in a web analytics dataset.\n* **IP addresses**in cybersecurity.\n## 2. Challenges of High-Cardinality Categorical Variables\nPress enter or click to view image in full size\n![]()\n## 3. Basic Techniques for Handling High Cardinality\n## 3.1 Dropping the Feature (if Irrelevant)\n* If a high-cardinality feature does not contribute to model performance, removing it can simplify the dataset.### Python Example\n```\ndf.drop(columns=[&#x27;&#x27;product\\_id&#x27;&#x27;], inplace=True)\n```\n### R Example\n```\ndf &lt;&lt;- df[, !names(df) %in% c(&quot;&quot;product\\_id&quot;&quot;)]\n```\n## 3.2 Frequency Encoding\n* Replaces categories with their frequency counts.\n* Helps capture important information without creating too many new columns.### Python Example\n```\ndf[&#x27;&#x27;category\\_freq&#x27;&#x27;] = df[&#x27;&#x27;category&#x27;&#x27;].map(df[&#x27;&#x27;category&#x27;&#x27;].value\\_counts())\n```\n### R Example\n```\ndf$category\\_freq &lt;&lt;- ave(df$category, df$category, FUN=length)\n```\n## 3.3 One-Hot Encoding (Not Recommended for High Cardinality)\n* Converts categorical variables into binary columns (dummy variables).\n* Can cause**memory explosion**with many unique values.### Python Example\n```\nimport pandas as pd\ndf = pd.get\\_dummies(df, columns=[&#x27;&#x27;category&#x27;&#x27;])\n```\n### R Example\n```\nlibrary(caret)\ndf &lt;&lt;- dummyVars(\\~category, data=df)\ndf\\_transformed &lt;&lt;- predict(df, df)\n```\n## 4. Intermediate Techniques\n## 4.1 Target Encoding (Mean Encoding)\n* Replaces categories with the**mean of the target variable**.\n* Works well for**classification and regression**but requires careful handling to avoid**data leakage**.### Python Example\n```\ndf[&#x27;&#x27;category\\_encoded&#x27;&#x27;] = df.groupby(&#x27;&#x27;category&#x27;&#x27;)[&#x27;&#x27;target&#x27;&#x27;].transform(&#x27;&#x27;mean&#x27;&#x27;)\n```\n### R Example\n```\ndf$category\\_encoded &lt;&lt;- ave(df$target, df$category, FUN=mean)\n```\n## 4.2 Weight of Evidence (WoE) Encoding\n* Used in**binary classification**.\n* Transforms categories based on their probability of belonging to each class.### Python Example\n```\nimport numpy as np\ndf[&#x27;woe&#x27;] = np.log(df.groupby(&#x27;category&#x27;)[&#x27;target&#x27;].mean() / (1 - df.groupby(&#x27;category&#x27;)[&#x27;target&#x27;].mean()))\n```\n### R Example\n```\nlibrary(smbinning)\ndf$woe &lt;- smbinning(df, &quot;category&quot;, &quot;target&quot;)$woe\n```\n## 4.3 Hash Encoding\n* Converts categories into**hashed numerical representations**.\n* Reduces dimensionality while preserving some uniqueness.### Python Example\n```\nfrom category\\_encoders import HashingEncoder\nencoder = HashingEncoder(n\\_components=8)\ndf\\_encoded = encoder.fit\\_transform(df[&#x27;&#x27;category&#x27;&#x27;])\n```\n### R Example\n```\nlibrary(FeatureHashing)\ndf\\_encoded &lt;&lt;- hashed.model.matrix(\\~category, data=df, hash.size=8)\n```\n## 5. Advanced Techniques\n## 5.1 Embedding Encoding (Using Neural Networks)\n* Converts categorical variables into**low-dimensional dense vectors**.\n* Requires**deep learning**frameworks like**TensorFlow**or**PyTorch**.### Python Example (TensorFlow)\n```\nimport tensorflow as tf\nimport numpy as np\n```\n```\nvocab\\_size = df[&#x27;&#x27;category&#x27;&#x27;].nunique()\nembedding\\_dim = 10 # Adjust based on dataset sizeembedding\\_layer = tf.keras.layers.Embedding(input\\_dim=vocab\\_size, output\\_dim=embedding\\_dim)\n```\n### R Example\n```\nlibrary(keras)\nembedding\\_layer &lt;&lt;- layer\\_embedding(input\\_dim = length(unique(df$category)), output\\_dim = 10)\n```\n## 5.2 Principal Component Analysis (PCA) on Encoded Data\n* Reduces the**dimensionality**of One-Hot or Hash Encoded data.\n* Useful when the number of categories is**too high**.### Python Example\n```\nfrom sklearn.decomposition import PCA\npca = PCA(n\\_components=10)\ndf\\_pca = pca.fit\\_transform(df\\_encoded)\n```\n### R Example\n```\ndf\\_pca &lt;&lt;- prcomp(df\\_encoded, center=TRUE, scale=TRUE)\n```\n## 5.3 Clustering-Based Encoding\n* Groups similar categories using**K-means clustering**.\n* Categories with similar effects are assigned the same cluster.### Python Example\n```\nfrom sklearn.cluster import KMeans\nn\\_clusters = 5\nkmeans = KMeans(n\\_clusters=n\\_clusters)\ndf[&#x27;&#x27;category\\_cluster&#x27;&#x27;] = kmeans.fit\\_predict(df[[&#x27;&#x27;category\\_encoded&#x27;&#x27;]])\n```\n### R Example\n```\nlibrary(cluster)\ndf$category\\_cluster &lt;&lt;- kmeans(df$category\\_encoded, centers=5)$cluster\n```\n## 6. Best Practices for Handling High-Cardinality Categorical Features\nPress enter or click to view image in full size\n![]()\n## 7. Libraries for Handling High-Cardinality Categorical Data\n## Python Libraries\nPress enter or click to view image in full size\n![]()\n## R Libraries\nPress enter or click to view image in full size\n![]()\n## 8. Summary\n1. **Start Simple**: If the categorical feature is unimportant, remove it.\n2. **Use Frequency Encoding**: If uniqueness does not add meaningful information.\n3. **Leverage...",
      "url": "https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=e9bf286d9f0fdb9d889e:1:11032)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/aichabokbot/encoding-high-cardinality-categorical-features"
    },
    {
      "title": "How to deal with Features having high cardinality | Kaggle",
      "text": "<div><div><p>One thing I do is try to ensure that the cardinality of the categorical information in the training set resembles that in the test/validation sets. That is, if I have a feature with values {A,A,A,B,C,C,D} in train, but test only has {A,B,B}, then eliminating the C and D records, and undersampling the A or oversampling the B records may resist overfitting.</p>\n<p>Also, for individual featuers with low cardinality, it's often worth bucketing them. In the above example, you may end up replacement values for A and C, and then bucketing B and D into an \"Other\" category (similar to Triskelion's trick with COUNT replacement).</p>\n<p>But overfitting is also something you can solve for by careful training -- split your training set in multiple ways, relying on cross-validation to minimize and test for over fitting before going to the private test data sets.</p></div></div>",
      "url": "https://www.kaggle.com/discussions/general/16927"
    },
    {
      "title": "Categorical Feature Encoding Challenge II",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/c/cat-in-the-dat-ii#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/c/cat-in-the-dat-ii)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/c/cat-in-the-dat-ii#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fc%2Fcat-in-the-dat-ii)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fc%2Fcat-in-the-dat-ii)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/c/cat-in-the-dat-ii"
    },
    {
      "title": "Categorical Feature Encoding Challenge",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.",
      "url": "https://www.kaggle.com/c/cat-in-the-dat"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - baasitsharief/kaggle-cat-in-the-dat: My take on Kaggle competition on Categorical Feature Encoding Challenge\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/baasitsharief/kaggle-cat-in-the-dat)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/baasitsharief/kaggle-cat-in-the-dat)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=baasitsharief/kaggle-cat-in-the-dat)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[baasitsharief](https://github.com/baasitsharief)/**[kaggle-cat-in-the-dat](https://github.com/baasitsharief/kaggle-cat-in-the-dat)**Public\n* [Notifications](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)\n* [Star4](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)\nMy take on Kaggle competition on Categorical Feature Encoding Challenge\n[4stars](https://github.com/baasitsharief/kaggle-cat-in-the-dat/stargazers)[0forks](https://github.com/baasitsharief/kaggle-cat-in-the-dat/forks)[Branches](https://github.com/baasitsharief/kaggle-cat-in-the-dat/branches)[Tags](https://github.com/baasitsharief/kaggle-cat-in-the-dat/tags)[Activity](https://github.com/baasitsharief/kaggle-cat-in-the-dat/activity)\n[Star](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)\n[Notifications](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)You must be signed in to change notification settings\n# baasitsharief/kaggle-cat-in-the-dat\nmaster\n[Branches](https://github.com/baasitsharief/kaggle-cat-in-the-dat/branches)[Tags](https://github.com/baasitsharief/kaggle-cat-in-the-dat/tags)\n[](https://github.com/baasitsharief/kaggle-cat-in-the-dat/branches)[](https://github.com/baasitsharief/kaggle-cat-in-the-dat/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[4 Commits](https://github.com/baasitsharief/kaggle-cat-in-the-dat/commits/master/)\n[](https://github.com/baasitsharief/kaggle-cat-in-the-dat/commits/master/)\n|\n[README.md](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/README.md)\n|\n[README.md](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/README.md)\n|\n|\n|\n[classification-with-xgboost-classifier.ipynb](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/classification-with-xgboost-classifier.ipynb)\n|\n[classification-with-xgboost-classifier.ipynb](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/classification-with-xgboost-classifier.ipynb)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# kaggle-cat-in-the-dat\n[](#kaggle-cat-in-the-dat)\nMy take on Kaggle competition on Categorical Feature Encoding Challenge [[https://www.kaggle.com/c/cat-in-the-dat](https://www.kaggle.com/c/cat-in-the-dat)]\nKernel Link -[https://www.kaggle.com/pr1c3f1eld/classification-with-xgboost-classifier](https://www.kaggle.com/pr1c3f1eld/classification-with-xgboost-classifier)\n## About\nMy take on Kaggle competition on Categorical Feature Encoding Challenge\n### Resources\n[Readme](#readme-ov-file)\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n[Activity](https://github.com/baasitsharief/kaggle-cat-in-the-dat/activity)\n### Stars\n[**4**stars](https://github.com/baasitsharief/kaggle-cat-in-the-dat/stargazers)\n### Watchers\n[**1**watching](https://github.com/baasitsharief/kaggle-cat-in-the-dat/watchers)\n### Forks\n[**0**forks](https://github.com/baasitsharief/kaggle-cat-in-the-dat/forks)\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/baasitsharief/kaggle-cat-in-the-dat&amp;report=baasitsharief+(user))\n## [Releases](https://github.com/baasitsharief/kaggle-cat-in-the-dat/releases)\nNo releases published\n## [Packages0](https://github.com/users/baasitsharief/packages?repo_name=kaggle-cat-in-the-dat)\nNo packages published\n## Languages\n* [Jupyter Notebook100.0%](https://github.com/baasitsharief/kaggle-cat-in-the-dat/search?l=jupyter-notebook)\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/baasitsharief/kaggle-cat-in-the-dat"
    },
    {
      "title": "Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas",
      "text": "Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/04/NVIDIA-feature-engineering-kaggle-blog-1024x576.jpg)\nApr 17, 2025\nBy[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [T](https://twitter.com/intent/tweet?text=Grandmaster+Pro+Tip:+Winning+First+Place+in+Kaggle+Competition+with+Feature+Engineering+Using+cuDF+pandas+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/&amp;title=Grandmaster+Pro+Tip:+Winning+First+Place+in+Kaggle+Competition+with+Feature+Engineering+Using+cuDF+pandas+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/>)\nAI-Generated Summary\nLike\nDislike\n* Using NVIDIA cuDF-pandas to accelerate pandas operations on GPUs allowed for the rapid generation and testing of over 10,000 engineered features for a Kaggle competition, significantly boosting the accuracy of an XGBoost model.\n* The most effective feature engineering techniques included groupby aggregations, such as computing statistics (e.g., mean, std, count) and quantiles, as well as creating new columns from NaNs and binning numerical columns.\n* Techniques like extracting digits from float32 values and combining categorical columns also proved useful, and leveraging the original dataset that the synthetic data was created from provided additional predictive power.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nFeature engineering remains one of the most effective ways to improve model accuracy when working with tabular data. Unlike domains such as NLP and computer vision, where neural networks can extract rich patterns from raw inputs, the best-performing tabular models\u2014particularly gradient-boosted decision trees\u2014still gain a significant advantage from well-crafted features. However, the sheer potential number of useful features means that exploring them thoroughly is often computationally prohibitive. Trying to generate and validate hundreds or thousands of feature ideas using standard pandas on a CPU is simply too slow to be practical.\nThis is where GPU acceleration changes the game. Using NVIDIA cuDF-pandas, which accelerates pandas operations on GPUs with zero code changes, allowed me to rapidly generate and test over 10,000 engineered features for Kaggle\u2019s February playground competition. This accelerated discovery process was the key differentiator. In a drastically reduced timeframe &#8211; days instead of potential months &#8211;\u00a0the best 500 discovered features significantly boosted the accuracy of my XGBoost model, securing 1st place in the competition predicting backpack prices. Below, I share the core feature engineering techniques, accelerated by cuDF-pandas, that led to this result.\n## Groupby(COL1)[COL2].agg(STAT)[**](#groupbycol1col2aggstat)\nThe most powerful feature engineering technique is groupby aggregations. Namely, we execute the code`groupby(COL1)[COL2].agg(STAT)`. This is where we group by`COL1`column and aggregate (i.e. compute) a statistic`STAT`over another column`COL2`. We use the speed of NVIDIA cuDF-Pandas to explore thousands of`COL1`,`COL2`,`STAT`combinations. We try statistics (`STAT`) like &#8220;mean&#8221;, &#8220;std&#8221;, &#8220;count&#8221;, &#8220;min&#8221;, &#8220;max&#8221;, &#8220;nunique&#8221;, &#8220;skew&#8221; etc etc. We choose`COL1`and`COL2`from our tabular data\u2019s existing columns. When`COL2`is the target column, then we use nested cross-validation to avoid leakage in our validation computation. When`COL2`is the target, this operation is called Target Encoding.\n## Groupby(COL1)[&#8216;Price&#8217;].agg(HISTOGRAM BINS)[**](#groupbycol1&#8216;price&#8217;agghistogram_bins)\nWhen we`groupby(COL1)[COL2]`we have a distribution (set) of numbers for each group. Instead of computing a single statistic (and making one new column), we can compute any collection of numbers that describe this distribution of numbers and make many new columns together.\nBelow we display a histogram for the group`Weight Capacity = 21.067673`. We can count the number of elements in each (equally spaced) bucket and create a new engineered feature for each bucket count to return to the groupby operation! Below we display seven buckets, but we can treat the number of buckets as a hyperparameter.\n```\nresult = X\\_train2.groupby(&quot;&quot;WC&quot;&quot;)&#x5B;&#x5B;&quot;&quot;Price&quot;&quot;].apply(make\\_histogram)\nX\\_valid2 = X\\_valid2.merge(result, on=&quot;&quot;WC&quot;&quot;, how=&quot;&quot;left&quot;&quot;)\n```\n*Figure 1. Histogram of price values when weight capacity equals 21.067673*\n## Groupby(COL1)[&#8216;Price&#8217;].agg(QUANTILES)[**](#groupbycol1&#8216;price&#8217;aggquantiles)\nWe can groupby and compute the quantiles for`QUANTILES = [5,10,40,45,55,60,90,95]`and return the eight values to create eight new columns.\n```\nfor k in QUANTILES:\nresult = X\\_train2.groupby(&#039;&#039;Weight Capacity (kg)&#039;&#039;).\\\\\nagg({&#039;&#039;Price&#039;&#039;: lambda x: x.quantile(k/100)})\n```\n## All NANs as Single Base-2 Column[**](#all_nans_as_single_base-2_column)\nWe can create a new column from all the NANs over multiple columns. This is a powerful column which we can subsequently use for groupby aggregations or combinations with other columns.\n```\ntrain&#x5B;&#x5B;&quot;&quot;NaNs&quot;&quot;] = np.float32(0)\nfor i,c in enumerate(CATS):\ntrain&#x5B;&#x5B;&quot;&quot;NaNs&quot;&quot;] += train&#x5B;&#x5B;c].isna()\\*2\\*\\*i\n```\n## Put Numerical Column into Bins[**](#put_numerical_column_into_bins)\nThe most powerful (predictive) column in this competition is Weight Capacity. We can create more powerful columns based on this column by binning this column with rounding.\n```\nfor k in range(7,10):\nn = f&quot;round{k}&quot;\ntrain&#x5B;n] = train&#x5B;&quot;Weight Capacity (kg)&quot;].round(k)\n```\n## Extract Float32 as Digits[**](#extract_float32_as_digits)\nThe most powerful (predictive) column in this competition is Weight Capacity. We can create more powerful columns based on this column by extracting digits. This technique seems weird, but it is often used to extract info from a product ID where individual digits within a product ID convey info about a product such as brand, color, etc.\n```\nfor k in range(1,10):\ntrain&#x5B;&#x5B;f&#039;&#039;digit{k}&#039;&#039;] = ((train&#x5B;&#x5B;&#039;&#039;Weight Capacity (kg)&#039;&#039;] \\* 10\\*\\*k) % 10).fillna(-1).astype(&quot;&quot;int8&quot;&quot;)\n```\n## Combination of Categorical Columns[**](#combination_of_...",
      "url": "https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) 2 [3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3) [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Outlier**\n\n[![Outlier Example](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n**Dummy Variables**\n\nFor categorical variables, a common practice is\u00a0**[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with\u00a0`n`\u00a0possible values, we create a group of\u00a0`n`\u00a0dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to\u00a0`1`\u00a0while other dummies in the same group are all set to\u00a0`0`.\n\n[![Dummies Example](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)\n\nIn this example, we transform\u00a0`DayOfWeek`\u00a0into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n**Feature Engineering**\n\nSome describe the essence of Kaggle competitions as\u00a0**feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense.\u00a0**Feature engineering gets your very far.**\u00a0Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically,\u00a0**when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n[![Checking Feature Validity](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)\n\n**Feature Selection**\n\nGenerally speaking,\u00a0**we should try to craft as many features as we can and have faith in the model\u2019s ability to pick up the most significant features**. Yet there\u2019s still something to gain from feature selection beforehand:\n\n- Less features mean faster training\n- Some features are linearly related to others. This might put a strain on the model.\n- By picking up the most important features, we can use interactions between them as new features. Sometimes this gives surprising improvement.\n\nThe simplest way to inspect feature importance is by fitting a random forest model. There are more robust feature selection algorithms (e.g.\u00a0[this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)) which are theoretically superior but not practicable due to the absence of efficient implementation. You can combat noisy data (to an extent) simply by increasing number of trees used in a random forest.\n\nThis is important for competitions in which data is\u00a0**[anonymized](https://en.wikipedia.org/wiki/Data_anonymization)**\u00a0because you won\u2019t waste time trying to figure out the meaning of a variable that\u2019s of no significance.\n\n**Feature Encoding**\n\nSometimes raw features have to be converted to some other formats for them to work properly.\n\nFor example, suppose we have a categorical variable which can take more than 10K different values. Then naively creating dummy variables is not a feasible option. An acceptable solution is to create dummy variables for only a subset of the values (e.g. values that constitute 95% of the feature importance) and assign everything else to an \u2018others\u2019 class.\n\n**Updated on Oct 28th, 2016:** For the scenario described above, another possible solution is to use\u00a0**Factorized Machines**. Please refer to\u00a0[this post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)\u00a0by Kaggle user \u201cidle\\_speculation\u201d for details.\n\n**Model Selection**\n\nWhen the features are set, we can start training models. Kaggle competitions usually favor **tree-based models**:\n\n- **Gradient Boosted Trees**\n- Random Forest\n- Extra Randomized Trees\n\nThe following models are slightly worse in terms of general performance, but are suitable as base models in ensemble learning (will be discussed later):\n\n- SVM\n- Linear Regression\n- Logistic Regression\n- Neural Networks\n\nNote that this does not apply to computer vision competitions which are pretty much dominated by neural network models.\n\nAll these models are implemented in\u00a0**[Sklearn](http://scikit-learn.org/)**.\n\nHere I want to emphasize the greatness of\u00a0**[Xgboost](https://github.com/dmlc/xgboost)**. The outstanding performance of gradient boosted trees and Xgboost\u2019s efficient implementation makes it very popular in Kaggle competitions. Nowadays almost every winner uses Xgboost in one way or another.\n\n**Updated on Oct 28th, 2016:** Recently Microsoft open sourced\u00a0**[LightGBM](https://github.com/Microsoft/LightGBM)**, a potentially better library than Xgboost for gradient boosting.\n\nBy the way, for Windows users installing Xgboost could be a painstaking process. You can refer to\u00a0[this post](https://dnc1994.com/2016/03/installing-xgboost-on-windows/)\u00a0by me if you run into problems.\n\n### More On This Topic\n\n- [How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)\n- [Deploying Your First Machine Learning API](https://www.kdnuggets.com/2021/10/deploying-first-machine-learning-api.html)\n- [5 Tips to Get Your First Data Scientist Job](https://www.kdnuggets.com/2021/11/5-tips-first-data-scientist-job.html)\n- [How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)\n- [Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)\n- [It's alive! Build your first robots with Python and some cheap,\u2026](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)\n\n![](https://www.kdnuggets.com/wp-content/uploads/newsletter.png)\n\n[Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox.](https://www.kdnuggets.com/news/subscribe.html)\n\nBy subscribing you accept KDnuggets [Privacy Policy](https://www.kdnuggets.com/news/privacy-policy.html)\n\nLeave this field empty if you're human:\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) 2 [3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3) [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n[<= Previous post](https://www.kdnuggets.com/2016/11/trump-shows-limits-prediction.html)\n\n[Next post =>](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html)\n\n![Search](https://www.kdnuggets.com/wp-content/themes/kdn17/images/search.svg)\n\n### [Latest Posts](https://www.kdnuggets.com/news/index.html)\n\n- [Unlocking Data Insights: Key Pandas Functions for Effective Analysis](https://www.kdnuggets.com/unlocking-data-insigh...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2"
    }
  ]
}