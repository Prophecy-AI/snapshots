{
  "query": "What are winning approaches for combining text and tabular features in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle solutions for problems that contain both free\u2011text and structured columns usually treat the data as a **multimodal tabular** task: the raw text is first turned into dense embeddings with a pretrained language model (e.g., BERT, RoBERTa, or GPT\u20114) and those vectors are then concatenated with the numeric and categorical features before feeding the whole matrix to a strong tabular learner such as LightGBM, CatBoost, or a gradient\u2011boosted tree ensemble.  AutoGluon\u2019s multimodal tabular API automates exactly this workflow \u2013 it ingests raw text as a first\u2011class column, trains a pretrained transformer on the text side, trains classical tabular models on the structured side, and finally blends them into a single predictor that often outperforms any single model\u202f([auto.gluon.ai](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  \n\nBeyond simple concatenation, the most competitive pipelines layer **stacking/ensembling** and **LLM\u2011generated features**.  Kaggle Grandmasters routinely train separate \u201ctext\u2011only\u201d models (e.g., a fine\u2011tuned BERT) and \u201ctabular\u2011only\u201d models (e.g., LightGBM), then use a meta\u2011learner or weighted average to combine their predictions\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).  Recent work shows that prompting large language models (GPT\u20114, Claude, etc.) to produce synthetic features or direct predictions can give a strong baseline with virtually no hand\u2011crafted engineering, and these LLM\u2011derived features can be added to the tabular matrix and further boosted by tree ensembles\u202f([medium.com](https://medium.com/data-science/boosting-tabular-data-predictions-with-large-language-models-531337f834dc); [arize.com](https://arize.com/blog-course/applying-large-language-models-to-tabular-data)).  The ELF\u2011Gym benchmark confirms that LLM\u2011generated \u201cgolden\u201d features from historic Kaggle wins improve downstream model scores when combined with traditional features\u202f([arxiv.org](https://arxiv.org/html/2410.12865v1)).  \n\nFinally, foundation\u2011model research suggests **lightweight tabular\u2011only models** (e.g., TabPFN) can be extended with text embeddings using simple ablation strategies\u2014replace the raw text column with its embedding, or add TF\u2011IDF / n\u2011gram statistics as extra features\u2014allowing a single, fast model to handle mixed data without a full transformer stack\u202f([arxiv.org](https://arxiv.org/pdf/2507.07829)).  In practice, the winning recipe is a hybrid of (1) high\u2011quality text embeddings, (2) powerful gradient\u2011boosted trees on the combined feature set, and (3) careful stacking or ensembling of modality\u2011specific models, often augmented with LLM\u2011generated features for an extra edge.",
      "url": ""
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.7.0 documentation",
      "text": "<div><div>\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\">\u00b6</a></h2>\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\n<a href=\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\naddition to text and tabular features.</p>\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c<span>sec_textprediction_architecture</span>\u201d of\n<span>sec_textprediction_multimodal</span> (used by AutoGluon\u2019s\n<code><span>TextPredictor</span></code>).</p>\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\n<span>import</span> <span>pprint</span>\n<span>import</span> <span>random</span>\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n</pre></div>\n<div>\n<h2>Product Sentiment Analysis Dataset<a href=\"#product-sentiment-analysis-dataset\">\u00b6</a></h2>\n<p>We consider the product sentiment analysis dataset from a <a href=\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\">MachineHack\nhackathon</a>.\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).</p>\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n</pre></div>\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\n2023-02-22 23:29:24 (73.9 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\n2023-02-22 23:29:24 (57.6 MB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\n2023-02-22 23:29:24 (137 MB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n</pre></div>\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\n</pre></div>\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\n</pre></div>\n<p>There are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.</p>\n<div>\n<table>\n <thead>\n <tr>\n <th></th>\n <th>Product_Description</th>\n <th>Product_Type</th>\n <th>Sentiment</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>4532</th>\n <td>they took away the lego pit but ...",
      "url": "https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "Towards Benchmarking Foundation Models for Tabular Data With Text",
      "text": "Towards Benchmarking Foundation Models for Tabular Data With Text\nMartin Mraz\u00b4\n* 1 Breenda Das * 1 Anshul Gupta * 1 Lennart Purucker 1 Frank Hutter 2 3 1\nAbstract\nFoundation models for tabular data are rapidly\nevolving, with increasing interest in extending\nthem to support additional modalities such as free\u0002text features. However, existing benchmarks for\ntabular data rarely include textual columns, and\nidentifying real-world tabular datasets with se\u0002mantically rich text features is non-trivial. We\npropose a series of simple yet effective ablation\u0002style strategies for incorporating text into con\u0002ventional tabular pipelines. Moreover, we bench\u0002mark how state-of-the-art tabular foundation mod\u0002els can handle textual data by manually curating\na collection of real-world tabular datasets with\nmeaningful textual features. Our study is an im\u0002portant step towards improving benchmarking of\nfoundation models for tabular data with text.\n1. Introduction\nFoundation models have begun to transform tabular learn\u0002ing (Erickson et al., 2020b; Hollmann et al., 2025), echo\u0002ing the trajectory of other research fields. A natural next\nstep is mixed-modality tabular modeling, where structured\ncolumns may also include free-text fields such as job de\u0002scriptions, clinical notes, or product summaries. Current\ntabular benchmarks, however, almost never contain textual\ncolumns, cf. (Bischl et al., 2019; Liu et al., 2024; McEl\u0002fresh et al., 2024). Moreover, locating real-world datasets\nwith semantically rich text features is exceptionally difficult,\nwith even exhaustive searches of OpenML and Kaggle only\nyielding a handful of usable candidates (Shi et al., 2021).\nConsequently, current tabular foundation models are rarely\nevaluated for tabular data with text.\nPipelines that can handle tabular data with text vary greatly\n*Equal contribution 1Department of Computer Science, Uni\u0002versity of Freiburg, Freiburg, Germany 2\nPrior Labs, Freiburg,\nGermany 3ELLIS Institute, Tubingen, Germany. Correspon\u0002dence to: Martin Mraz <mrazm@informatik.uni-freiburg.de>,\nBreenda Das <dasb@informatik.uni-freiburg.de>, Anshul Gupta\n<guptaa@informatik.uni-freiburg.de>.\nProceedings of the 42 nd International Conference on Machine\nLearning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\nin their implementation. AutoGluon\u2019s AutoMLPipeline\u0002FeatureGenerator (Erickson et al., 2020a) converts text to\nsparse vectors; CARTE (Kim et al., 2024) applies fastText\nsentence embeddings (Bojanowski et al., 2017). and the\nTabPFNv2 API accepts raw text but does not disclose its\nmethodology. These divergent choices raise a fundamental\nquestion: Which embedding strategy works best, and under\nwhat conditions?\nTo answer this question, we present the first systematic\nstudy of predictive machine learning with foundation mod\u0002els for tabular data with text. We study the performance\nof three representative embedding routes: fastText, Skrub\u2019s\nTableVectorizer, and AutoGluon\u2019s text encoder. We show\nqualitatively, with a simple synthetic counter-example, that\nboth n-gram based and off-the-shelf sentence embeddings\ncan fail to recover highly predictive semantic patterns. More\u0002over, quantitatively, we evaluate these methods on a man\u0002ually curated set of real-world tabular benchmark that (i)\ncontain genuinely informative free-text columns (ii) spans\nover a variety of domains and samples.\nOur contributions are: (I) A qualitative study show cas\u0002ing the limitations of standard n-gram based and generic\nNLP-based embeddings for tabular tasks with text. (II) A\nmanually curated set of real-world tabular datasets with se\u0002mantically rich textual columns. (III) An empirical study of\nthree text embedding pipelines for TabPFNv2 and XGBoost\nwith the TabPFNv2 API and AutoGluon Tabular Predictor\nas baselines.\nOur study reveals the limitations of current methods and\nunderscores the need for new methods to handle tabular data\nwith free text.\n2. Related Work\nApproaches incorporating free-text in tabular learning\nlargely follow two paradigms. First, row-as-text meth\u0002ods serialize entire rows into prompts and delegate pre\u0002diction to a large language model (LLM), as seen in\nTABLLM (Hegselmann et al., 2022), TABLE-LLM (Zhang\net al., 2024). These work well when textual fields dominate\nor in few-shot settings. Second, per-column embedding\nstrategies extract textual embeddings from a single or groups\nof features, while preserving the structural nature of tabular\n1\narXiv:2507.07829v1 [cs.LG] 10 Jul 2025\nTowards Benchmarking Foundation Models for Tabular Data With Text\nFigure 1. Qualitative investigation of textual embeddings for tabular prediction tasks: TabPFN v2 accuracy across five datasets.\nBaselines \u201cNo-Text\u201d uses original input features, \u201cComplete Leak\u201d has target leakage, therefore 100% accuracy for all. Following tests\nembed targets into textual modality. \u201cN-Gram Break\u201d shows TF-IDF breaking under unseen synonyms, \u201cSimple NLP break\u201d shows\nFastText underperforming under noise, \u201cLLM Breaks\u201d shows BERT variant breaking under semantic ambiguity.\ndata. They embed each column using fastText or LLMs\nand then concatenate the resulting embeddings to the table,\nor replace the feature with its individual textual embed\u0002ding, cf. (Koloski et al., 2025; Carballo et al., 2023; Kim\net al., 2024). Grinsztajn et al. (2023) compare 30+ LLM\nand substring-based methods for embedding string columns,\nshowing when LLMs yield better representations.\nIn this study, we investigate pre-column embeddings be\u0002cause we focus on a many-shot setting (e.g., more than 32\ntraining samples), in which LLM-based predictive methods\ndo not perform well, cf. (Hegselmann et al., 2023) and (Ma\net al., 2024), or require a prohibitive amount of fine-tuning\n(Shysheya et al., 2025).\nMost popular prior tabular benchmarks contain no free-text\nfield, cf. (Bischl et al., 2019; Gijsbers et al., 2022; McEl\u0002fresh et al., 2023). For tabular data with text, Shi et al.\n(2021) curated 18 datasets with mixed modalities, but many\nrely almost entirely on text, with only one or two tabular\nfeatures, or features derived from the long text, e.g. # of\nwords (Tang et al., 2024). Thus, they benchmark text-based\nmodels rather than tabular models, focusing on text data\nwith tabular features. In contrast, we focus on tabular mod\u0002els extended to handle text, focusing on tabular data with\nadditional text features. Other studies were often limited to\nan evaluation with just one or two datasets (Carballo et al.,\n2023; Lu et al., 2023). Overall, existing studies for tabular\ndata with text lack a domain-agnostic benchmark where\ntextual features complement, rather than dominate, tabular\ndata. This motivates the new benchmark we present.\nThe CARTE Benchmark. The latest seminal work on\nbenchmarking for tabular data with text is the CARTE (Kim\net al., 2024) benchmark. It includes 51 datasets for tabu\u0002lar data with text. However, when we investigated these\ndatasets more closely, we found that at most 11 out of the\n51 datasets are suited to evaluate tabular data with text.\nMoreover, our manually curated collection of datasets for\nbenchmarking only includes 1 out of the 51 datasets.\nWe share an extensive report of our investigation in Ap\u0002pendix G. In short, we found that most datasets (a) do not\nrepresent predictive machine learning tasks for tabular data\nwith text; (b) are skewed towards text data representing\nmany categories instead of longer free text; (c) were pre\u0002processed manually per dataset with logic that seems to\nfavor CARTE; (d) or were very similar datasets from the\nsame domain, with the same features, or similar semantic\ncontent. Thus, while CARTE was a significant step toward\nbenchmarking and learning for tabular data with text, it falls\nshort in several aspects. Our work complements CARTE\u2019s\nefforts and aims to improve benchmarking for tabular data\nwith text further.\n3. Qualitative Investigation\nIn this section, we evaluate popular text embedding tech\u0002niques: N-Grams, simple NLP models and LLMs for tabular\npr...",
      "url": "https://arxiv.org/pdf/2507.07829"
    },
    {
      "title": "ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction",
      "text": "ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction\n# ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction\nYanlin ZhangFudan UniversityShanghaiChina[21210720043@m.fudan.edu.cn](mailto:21210720043@m.fudan.edu.cn),Ning LiShanghai Jiao Tong UniversityShanghaiChina[lining01@sjtu.edu.cn](mailto:lining01@sjtu.edu.cn),Quan GanAmazon Shanghai AI LabShanghaiChina[quagan@amazon.com](mailto:quagan@amazon.com),Weinan ZhangShanghai Jiao Tong UniversityShanghaiChina[wnzhang@sjtu.edu.cn](mailto:wnzhang@sjtu.edu.cn),David WipfAmazon Shanghai AI LabShanghaiChina[daviwipf@amazon.com](mailto:daviwipf@amazon.com)andMinjie WangAmazon Shanghai AI LabShanghaiChina[minjiw@amazon.com](mailto:minjiw@amazon.com)\n(2024)\n###### Abstract.\nCrafting effective features is a crucial yet labor-intensive and domain-specific task within machine learning pipelines. Fortunately, recent advancements in Large Language Models (LLMs) have shown promise in automating various data science tasks, including feature engineering. But despite this potential, evaluations thus far are primarily based on the end performance of a complete ML pipeline, providing limited insight into precisely how LLMs behave relative to human experts in feature engineering. To address this gap, we propose ELF-Gym, a framework forEvaluatingLLM-generatedFeatures. We curated a new dataset from historical Kaggle competitions, including 251 \u201cgolden\u201d features used by top-performing teams. ELF-Gym then quantitatively evaluates LLM-generated features by measuring their impact on downstream model performance as well as their alignment with expert-crafted features through semantic and functional similarity assessments. This approach provides a more comprehensive evaluation of disparities between LLMs and human experts, while offering valuable insights into specific areas where LLMs may have room for improvement. For example, using ELF-Gym we empirically demonstrate that, in the best-case scenario, LLMs can semantically capture approximately 56% of the golden features, but at the more demanding implementation level this overlap drops to 13%. Moreover, in other cases LLMs may fail completely, particularly on datasets that require complex features, indicating broad potential pathways for improvement.\nLarge Language Models, Feature Engineering, Data Science\n\u2020\u2020journalyear:2024\u2020\u2020copyright:acmlicensed\u2020\u2020conference:Proceedings of the 33rd ACM International Conference on Information and Knowledge Management; October 21\u201325, 2024; Boise, ID, USA\u2020\u2020booktitle:Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM \u201924), October 21\u201325, 2024, Boise, ID, USA\u2020\u2020doi:10.1145/3627673.3679153\u2020\u2020isbn:979-8-4007-0436-9/24/10\u2020\u2020ccs:Computing methodologies\u00a0Artificial intelligence\n## 1.Introduction\nFeature engineering is a crucial step in the machine learning pipeline, transforming raw data into meaningful features that improve model performance and interpretability. Effective feature engineering can significantly enhance the predictive power of models, making it a vital component in various data-driven applications. This importance is particularly evident in competitive data science environments like Kaggle> (Kaggle, [> [n.\u2009d.]\n](https://arxiv.org/html/2410.12865v1#bib.bib24)> )\n, where top-performing models often rely heavily on sophisticated feature engineering techniques. For instance, in one interview111[https://medium.com/kaggle-blog/grupo-bimbo-inventory-demand-winners-interview-clustifier-alex-andrey-1e3b6cec8a20](https://medium.com/kaggle-blog/grupo-bimbo-inventory-demand-winners-interview-clustifier-alex-andrey-1e3b6cec8a20), the winners of the Grupo Bimbo Inventory Prediction competition reported that 95% of their time was on feature engineering while only 5% was on modeling.\nDespite its importance, traditional feature engineering is labor-intensive and requires extensive domain knowledge. Automated tools like AutoFeat> (Horn et\u00a0al\n> .\n> , [> 2020\n](https://arxiv.org/html/2410.12865v1#bib.bib19)> )\n, OpenFE> (Zhang et\u00a0al\n> .\n> , [> 2022\n](https://arxiv.org/html/2410.12865v1#bib.bib37)> )\n, SAFE> (Shi et\u00a0al\n> .\n> , [> 2020\n](https://arxiv.org/html/2410.12865v1#bib.bib33)> )\n, and Deep Feature Synthesis (DFS)> (Kanter and Veeramachaneni, [> 2015\n](https://arxiv.org/html/2410.12865v1#bib.bib26)> )\nhave emerged to streamline this process. AutoFeat automates feature selection and generation using statistical methods and heuristics but suffers from high feature generation costs. OpenFE and SAFE mitigate these costs\nby optimizing the feature selection phase using feedback from model evaluation.\nDFS extends feature engineering to multi-table scenarios by utilizing data relationships to generate features. Despite their effectiveness in reducing manual effort, these tools often fall short in leveraging the domain knowledge that human experts typically rely on for crafting relevant features.\nThe advent of Large Language Models (LLMs) such as GPTs> (Achiam et\u00a0al\n> .\n> , [> 2023\n](https://arxiv.org/html/2410.12865v1#bib.bib2)> )\nhas opened new possibilities for automating various data science tasks. LLMs have demonstrated remarkable capabilities in natural language understanding> (Min et\u00a0al\n> .\n> , [> 2023\n](https://arxiv.org/html/2410.12865v1#bib.bib29)> ; Du et\u00a0al\n> .\n> , [> 2023\n](https://arxiv.org/html/2410.12865v1#bib.bib11)> )\n, text generation> (Yuan et\u00a0al\n> .\n> , [> 2022\n](https://arxiv.org/html/2410.12865v1#bib.bib34)> ; Lu et\u00a0al\n> .\n> , [> 2023\n](https://arxiv.org/html/2410.12865v1#bib.bib28)> )\n, summarization> (Goyal et\u00a0al\n> .\n> , [> 2022\n](https://arxiv.org/html/2410.12865v1#bib.bib14)> ; Zhang et\u00a0al\n> .\n> , [> 2024\n](https://arxiv.org/html/2410.12865v1#bib.bib36)> ; Basyal and Sanghvi, [> 2023\n](https://arxiv.org/html/2410.12865v1#bib.bib7)> )\n, and even code synthesis> (Austin et\u00a0al\n> .\n> , [> 2021\n](https://arxiv.org/html/2410.12865v1#bib.bib6)> ; Chen et\u00a0al\n> .\n> , [> 2021\n](https://arxiv.org/html/2410.12865v1#bib.bib8)> ; Zhang et\u00a0al\n> .\n> , [> 2023\n](https://arxiv.org/html/2410.12865v1#bib.bib35)> )\n. Their ability to process and generate human-like text makes them particularly well-suited for tasks that require semantic understanding and contextual reasoning. Of particular relevance to data science, LLMs have shown potential in automating tasks such as data cleaning, feature generation, and model selection.\nFor example, recent work such as CAAFE> (Hollmann et\u00a0al\n> .\n> , [> 2024\n](https://arxiv.org/html/2410.12865v1#bib.bib18)> )\n, DS-Agent> (Guo et\u00a0al\n> .\n> , [> 2024\n](https://arxiv.org/html/2410.12865v1#bib.bib15)> )\nand FeatLLM> (Han et\u00a0al\n> .\n> , [> 2024\n](https://arxiv.org/html/2410.12865v1#bib.bib17)> )\nhave explored the application of LLMs to feature engineering. In brief, CAAFE leverages LLMs to generate additional features based on dataset descriptions, iteratively improving model performance through semantic understanding. Meanwhile DS-Agent employs a case-based reasoning approach, combining LLMs with expert knowledge from Kaggle competitions to automate the entire data science workflow. Finally, FeatLLM utilizes LLMs to engineer binary features through rule generation and rule parsing, significantly improving down-stream tabular prediction tasks.\nDespite the potential, their actual evaluations thus far are primarily based on the end performance of a complete machine learning pipeline, providing limited insight into the reason behind the distinction between the solutions built by LLMs and human experts especially with respect to feature engineering. As LLM-based data science is increasingly becoming an active research area, this calls for more transparent and insightful evaluation tools and benchmarks to effectively assess and improve these systems.\nTo make strides in this direction, we proposed ELF-Gym, a framework forEvaluatingLLM-generatedFeatures in machine learning pipelines. We curate a new dataset specifically designed fo...",
      "url": "https://arxiv.org/html/2410.12865v1"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Boosting Tabular Data Predictions with Large Language Models",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F531337f834dc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fboosting-tabular-data-predictions-with-large-language-models-531337f834dc&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-531337f834dc---------------------------------------)\n\n\u00b7\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\nPress enter or click to view image in full size\n\nImage by author\n\n# Boosting Tabular Data Predictions with Large Language Models\n\n## What happens when you unleash GPT-4 on a tabular Kaggle competition to predict home prices?\n\n[Aparna Dhinakaran](https://medium.com/@aparnadhinak?source=post_page---byline--531337f834dc---------------------------------------)\n\n9 min read\n\n\u00b7\n\nApr 6, 2023\n\n--\n\n11\n\nListen\n\nShare\n\n**_Follow along with this blog\u2019s_** [**_accompanying Colab_**](https://colab.research.google.com/gist/PubliusAu/3c0b73ecf5558e4dda2a483693be2b93/arize-wide-llm-kaggle-example-v2-0-1.ipynb) **.**\n\n_This blog is a collaboration with Jason Lopatecki, CEO and Co-Founder of Arize AI, and Christopher Brown, CEO and Founder of Decision Patterns_\n\n## Introduction\n\nThere are two distinct groups in the ML ecosystem. One works with highly organized data collected in tables \u2014 the tabular-data-focused data scientist. The other works on deep learning applications including vision, audio, large language models (LLMs), etc. For the purposes of this piece, we call the former the \u201ctabular\u201d or \u201ctraditional\u201d group and the latter the \u201cLLM\u201d group. Each group uses its own techniques and models that have, in large part, developed separately. With the recent successes of large language models including OpenAI\u2019s GPT-4 and others, we wanted to see if we could use modern LLM results to help make predictions on tabular datasets.\n\nIn order to demonstrate the efficacy of the approach, we submitted results to several blind Kaggle competitions (including the popular \u201cHouse Prices \u2014 Advanced Regression Techniques\u201d [competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)). The typical Kaggle competition supplies tabular data and is dominated by traditional ML approaches. However, we found with little background knowledge, zero data cleaning, and zero feature development required by traditional methods, LLMs were able to return results with predictive power. LLM predictions were not competitive with the leading models produced with lengthy and extensive tabular methods, but were strong enough to place well higher than the median score on the leaderboard rankings.\n\nWe expect this to be the beginning of a number of techniques that use LLMs on tabular data and would not be surprised to see their use widen and compete favorably to more traditional model development processes.\n\nIncluded in this write up is the first approach we have seen that merges traditional tabular datasets and XGBoost models with LLMs using latent structure embeddings, allowing the tabular approaches to work off of the numerical \u201cfeatures\u201d produced internally by the LLM.\n\nTo date, we haven\u2019t seen an LLM used this way to date and hope this is the beginning of something exciting.\n\n## Challenges of Applying Deep Learning to Tabular Data\n\nThe typical machine learning application involves cleaning and training a narrow set of data typically collected, held, or acquired by an organization. At a high level, the process can be thought of developing a \u201ccontext\u201d for which only one specific type of questions can be asked. When that type of question arises, the ML model produces one or more predictions. Further improving models comes from three areas: adding more data, improving methods, or acquiring more and different features. The last is often the most interesting here, as the data scientist is generally always asking herself: \u201cwhat different data can I get to make my predictions better?\u201d\n\nPartitioning, boosting, and/or bagging models have been developed for and do exceedingly well in this domain. Despite much effort, deep learning has not shown to be as effective in this area. Observations show that XGBoost and cousins generalize better in production, where deep learning models tend to overfit. A large number of teams have tried to improve deep learning on tabular datasets, but these efforts have largely lagged behind established, high performing tabular approaches.\n\n### The Problem of Training with Narrow Data\n\nA common deep learning approach is to apply a neural network and multilayer perceptron (MLP) to a relatively \u201csmall\u201d dataset consisting of an organization\u2019s data. This approach has been repeatedly shown to require more work (data scientist time), resource consumption (training time), and parameter tuning to get similar or worse performance than tabular approaches. The failure of deep learning here may be a mismatch between the approach and narrow data available to it. Deep Learning seems somewhat gated by its ability to learn from narrow data.\n\nPress enter or click to view image in full size\n\nImage by author\n\nIn the image above, a large parameter neural network model is trained on the \u201csmall data\u201d of a single company. Training a large model on a relatively small dataset causes the model to almost always be over-parameterized. This is because the information contained to make decisions is not that large, there are only so many \u201cerror surfaces\u201d related to the data to optimize performance against.\n\n## Applying a Large Language Model To a Tabular Dataset: Enter Prompt Engineering\n\nLLMs have come to the fore through two innovations. The first is the transformer architecture pioneered by Google and others. The second is the application of these architectures to colossal data sets on the order of dozens or hundreds of terabytes. A reasonable hypothesis is that these LLMs are able to sidestep the \u201cnarrow\u201d data problem that beleagures deep learning approaches. By training on Internet-scale data, LLMs have built an internal representation for the context of many applications. This is a needed step in order to have a model that can respond to any number of prompts. A happy and necessary consequence is that the LLMs may have developed the context for answering questions related to an organization\u2019s prediction problems or those of a Kaggle competition.\n\nBy way of analogy, LLMs have come to understand the context of your problem in a similar way that traditional/tabular machine learning has done in its training step. Surprisingly, this has been done using a broader source of data and not the organization\u2019s specific data. Another way to look at it is that the LLMs has trained a model capable of predictions from all the data it acquired elsewhere. To the data scientist, this provides access to a divers...",
      "url": "https://medium.com/data-science/boosting-tabular-data-predictions-with-large-language-models-531337f834dc"
    },
    {
      "title": "Applying Large Language Models To Tabular Data: A New Approach",
      "text": "# Applying Large Language Models To Tabular Data: A New Approach\n\n## Published April 5, 2023\n\n## An adventure in unleashing large language models (LLMs) on tabular Kaggle competitions\n\n_This blog is co-authored by Aparna Dhinakaran, CPO and Co-Founder of Arize AI, and [Christopher Brown](https://www.linkedin.com/in/decisionpatterns/), CEO and Founder of Decision Patterns_\n\n****\ud83d\udca1** Want to try it yourself? Follow along with this blog\u2019s [accompanying Colab](https://colab.research.google.com/gist/PubliusAu/3c0b73ecf5558e4dda2a483693be2b93/arize-wide-llm-kaggle-example-v2-0-1.ipynb) \u2728**\n\n## Introduction\n\nThere are two distinct groups in the ML ecosystem. One works with highly organized data collected in tables \u2013 the tabular focused data scientist. The other works on deep learning applications including vision, audio, large language models (LLMs), etc. For the purposes of this piece, we call the former the \u201ctabular\u201d or \u201ctraditional\u201d group and the latter the \u201cLLM\u201d group. Each group uses its own techniques and models that have, in large part, developed separately. With the recent successes of large language models including OpenAI\u2019s GPT-4 and others, we wanted to see if we could use modern LLM results to help make predictions on tabular datasets.\n\nIn order to demonstrate the efficacy of the approach, we submitted results to several blind Kaggle competitions (including the popular \u201cHouse Prices \u2013 Advanced Regression Techniques\u201d [competition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)). The typical Kaggle competition supplies tabular data and is dominated by traditional ML approaches. However, we found with little background knowledge, zero data cleaning, and zero feature development required by traditional methods, LLMs were able to return results with predictive power. LLM predictions were not competitive with the leading models produced with lengthy and extensive tabular methods, but were strong enough to place well higher than the median score on the leaderboard rankings.\n\nWe expect this to be the beginning of a number of techniques that use LLMs on tabular data and would not be surprised to see the use of LLMs in tabular data widen and compete favorably to more traditional model development processes.\n\nIncluded in this write-up is the first approach we have seen that merges traditional tabular datasets and XGBoost models with LLMs using latent structure embeddings, allowing the tabular approaches to work off of the numerical \u201cfeatures\u201d produced internally by the LLM.\n\nTo date, we haven\u2019t seen an LLM used this way and hope this is the beginning of something exciting!\n\n## Challenges of Applying Deep Learning to Tabular Data\n\nThe typical machine learning application involves cleaning and training a narrow set of data typically collected, held, or acquired by an organization. At a high level, the process can be thought of developing a \u201ccontext\u201d for which only one specific type of questions can be asked. When that type of question arises, the ML model produces one or more predictions. Further improving models comes from three areas: adding more data, improving methods, or acquiring more and different features. The last is often the most interesting here, as the data scientist is generally always asking herself \u201cwhat different data can I get to make my predictions better?\u201d\n\nPartitioning, boosting, and/or bagging models have been developed for and do exceedingly well in this domain. Despite much effort, deep learning has not shown to be as effective in this area. Observations show that XGBoost and cousins generalize better in production, where deep learning models tend to overfit. A large number of teams have tried to improve deep learning on tabular datasets, but these efforts have largely lagged behind established, high performing tabular approaches.\n\n### Training with Narrow Data\n\nA common deep learning approach is to apply a neural network and multilayer perceptron (MLP) to a relatively \u201csmall\u201d dataset consisting of an organization\u2019s data. This approach has been repeatedly shown to require more work (data scientist time), resource consumption (training time), and parameter tuning to get similar or worse performance than tabular approaches. The failure of deep learning here may be a mismatch between the approach and narrow data available to it. Deep Learning seems somewhat gated by its ability to learn from narrow data.\n\nIn the image above, a large parameter neural network model is trained on the \u201csmall data\u201d of a single company. Training a large model on a relatively small dataset causes the model to almost always be over-parameterized. This is because the information contained to make decisions is not that large, there are only so many \u201cerror surfaces\u201d related to the data to optimize performance against.\n\n## Applying an LLM to a Tabular Dataset \u2013 Enter Prompt Engineering\n\nLLMs have come to the fore through two innovations. The first is the transformer architecture pioneered by Google and others. The second is the application of these architectures to colossal data sets on the order of dozens or hundreds of terabytes. A reasonable hypothesis is that these LLMs are able to sidestep the \u201cnarrow\u201d data problem that beleaguer deep learning approaches. By training on Internet-scale data, LLMs have built an internal representation for the context of many applications. This is a needed step in order to have a model that can respond to any number of prompts. A happy and necessary consequence is that the LLMs may have developed the context for answering questions related to an organization\u2019s prediction problems or those of a Kaggle competition.\n\nBy way of analogy, LLMs have come to understand the context of your problem in a similar way that traditional/tabular machine learning has done in its training step. Surprisingly, this has been done using a broader source of data and not the organization\u2019s specific data. Another way to look at it is that the LLMs has trained a model capable of predictions from all the data it acquired elsewhere. To the data scientist, this provides access to a diverse dataset and a potential treasure trove of information \u2013 or they may just provide noise.\n\nUnlocking the information in LLMs for tabular models encounters two obstacles. The first is that LLMs are accessed via prompts and [prompt engineering](https://arize.com/blog-course/prompt-engineering/) and not tabular data (DataFrames). The second is that LLMs primarily produce textual output.\n\nTo overcome the first obstacle, we supply our tabular data through prompts. Here the prompt is created on each row of the table. The construction of the prompt is pretty simple: a paragraph comprised of sentences, one for each cell in the table row, as follows:\n\nThe \u201crow\u201d prompt consists of the following:\n_The is . The is . \u2026_\n\nTwo things to note:\n\n- It is not necessary to generate prompts for training data, only the data about which the prediction needs to be made.\n- It is not strictly necessary to ask what prediction will be made of the data.\n\nThe second obstacle is that the LLMs produce textual responses. In some instances, LLMs can provide predictions. As of this writing, the predictions are not very good \u2013 likely because the LLMs that are not trained with specific predictions in mind. Instead of accessing LLM predictions, we find the flexibility to work with the features produced by the LLM preferable. In the parlance of LLMs, the features are latent structure embeddings or simply \u201cembeddings.\u201d These embeddings are accessible through LLM APIs. It is important to note that the embedding vectors are typically of values-per-row. Once we extract the embeddings, we can run them through a tabular model (XGBoost).\n\nThe embeddings are be used in two examples here:\n\n1. To make predictions for home prices in a Kaggle data competition (this blog)\n2. Measure multivariate drift and anomaly detection using embedding drift (an upcoming blog)\n\nTable -> Prompt -> LLM -> Em...",
      "url": "https://arize.com/blog-course/applying-large-language-models-to-tabular-data"
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.1.1 documentation",
      "text": "# Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models)\n\nHere we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, **raw text data** is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c [What\u2019s happening inside?](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-architecture)\u201d of\n[Text Prediction - Multimodal Table with Text](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-multimodal) (used by AutoGluon\u2019s\n`TextPredictor`).\n\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nfrom autogluon.tabular import TabularPredictor\nimport mxnet as mx\n\nnp.random.seed(123)\nrandom.seed(123)\nmx.random.seed(123)\n```\n\n## Product Sentiment Analysis Dataset [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#product-sentiment-analysis-dataset)\n\nWe consider the product sentiment analysis dataset from a [MachineHack\\\nhackathon](https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard).\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).\n\n```\n!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n\n```\n\n```\n--2021-03-10 04:16:17--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 673.33K  2.05MB/s    in 0.3s\n\n2021-03-10 04:16:18 (2.05 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n\n--2021-03-10 04:16:18--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\n\nproduct_sentiment_m 100%[===================>]  73.75K   490KB/s    in 0.2s\n\n2021-03-10 04:16:19 (490 KB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n\n--2021-03-10 04:16:19--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 304.88K   943KB/s    in 0.3s\n\n2021-03-10 04:16:20 (943 KB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n\n```\n\n```\nsubsample_size = 2000  # for quick demo, try setting to larger values\nfeature_columns = ['Product_Description', 'Product_Type']\nlabel = 'Sentiment'\n\ntrain_df = pd.read_csv('product_sentiment_machine_hack/train.csv', index_col=0).sample(2000, random_state=123)\ndev_df = pd.read_csv('product_sentiment_machine_hack/dev.csv', index_col=0)\ntest_df = pd.read_csv('product_sentiment_machine_hack/test.csv', index_col=0)\n\ntrain_df = train_df[feature_columns + [label]]\ndev_df = dev_df[feature_columns + [label]]\ntest_df = test_df[feature_columns]\nprint('Number of training samples:', len(train_df))\nprint('Number of dev samples:', len(dev_df))\nprint('Number of test samples:', len(test_df))\n```\n\n```\nNumber of training samples: 2000\nNumber of dev samples: 637\nNumber of test samples: 2728\n```\n\nThere are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.\n\n```\ntrain_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 4532 | they took away the lego pit but replaced it wi... | 0 | 1 |\n| 1831 | #Apple to Open Pop-Up Shop at #SXSW \\[REPORT\\]: ... | 9 | 2 |\n| 3536 | RT @mention False Alarm: Google Circles Not Co... | 5 | 1 |\n| 5157 | Will Google reveal a new social network called... | 9 | 2 |\n| 4643 | Niceness RT @mention Less than 2 hours until w... | 6 | 3 |\n\n```\ndev_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 3170 | Do it. RT @mention Come party w/ Google tonigh... | 3 | 3 |\n| 6301 | Line for iPads at #SXSW. Doesn't look too bad!... | 6 | 3 |\n| 5643 | First up: iPad Design Headaches (2 Tablets, Ca... | 6 | 2 |\n| 1953 | #SXSW: Mint Talks Mobile App Development Chall... | 9 | 2 |\n| 2658 | \u0089\u00db\u00cf@mention Apple store downtown Austin open t... | 9 | 2 |\n\n```\ntest_df.head()\n```\n\n| Product\\_Description | Product\\_Type |\n| --- | --- |\n| Text\\_ID |\n| --- |\n| 5786 | RT @mention Going to #SXSW? The new iPhone gui... | 7 |\n| 5363 | RT @mention 95% of iPhone and Droid apps have ... | 9 |\n| 6716 | RT @mention Thank you to @mention for letting ... | 9 |\n| 4339 | #Thanks @mention we're lovin' the @mention app... | 7 |\n| 66 | At #sxsw? @mention / @mention wanna buy you a ... | 9 |\n\n## AutoGluon Tabular with Multimodal Support [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#autogluon-tabular-with-multimodal-support)\n\nTo utilize the `TextPredictor` model inside of `TabularPredictor`,\nwe must specify the `hyperparameters = 'multimodal'` in AutoGluon\nTabular. Internally, this will train multiple tabular models as well as\nthe TextPredictor model, and then combine them via either a weighted\nensemble or stack ensemble, as explained in [AutoGluon Tabular\\\nPaper](https://arxiv.org/pdf/2003.06505.pdf). If you do not specify\n`hyperparameters = 'multimodal'`, then AutoGluon Tabular will simply\nfeaturize text fields using N-grams and train only tabular models (which\nmay work better if your text is mostly uncommon strings/vocabulary).\n\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\npredictor.fit(train_df, hyperparameters='multimodal')\n```\n\n```\nBeginning AutoGluon training ...\nAutoGluon will save models to \"ag_tabular_product_sentiment_multimodal/\"\nAutoGluon Version:  0.1.1b20210310\nTrain Data Rows:    2000\nTrain Data Columns: 2\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n    4 unique label values:  [1, 2, 3, 0]\n    If 'multiclass' is not the correct ...",
      "url": "https://auto.gluon.ai/scoredebugweight/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2511.08667] TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2511.08667\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2511.08667**(cs)\n[Submitted on 11 Nov 2025]\n# Title:TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models\nAuthors:[L\u00e9o Grinsztajn](https://arxiv.org/search/cs?searchtype=author&amp;query=Grinsztajn,+L),[Klemens Fl\u00f6ge](https://arxiv.org/search/cs?searchtype=author&amp;query=Fl\u00f6ge,+K),[Oscar Key](https://arxiv.org/search/cs?searchtype=author&amp;query=Key,+O),[Felix Birkel](https://arxiv.org/search/cs?searchtype=author&amp;query=Birkel,+F),[Philipp Jund](https://arxiv.org/search/cs?searchtype=author&amp;query=Jund,+P),[Brendan Roof](https://arxiv.org/search/cs?searchtype=author&amp;query=Roof,+B),[Benjamin J\u00e4ger](https://arxiv.org/search/cs?searchtype=author&amp;query=J\u00e4ger,+B),[Dominik Safaric](https://arxiv.org/search/cs?searchtype=author&amp;query=Safaric,+D),[Simone Alessi](https://arxiv.org/search/cs?searchtype=author&amp;query=Alessi,+S),[Adrian Hayler](https://arxiv.org/search/cs?searchtype=author&amp;query=Hayler,+A),[Mihir Manium](https://arxiv.org/search/cs?searchtype=author&amp;query=Manium,+M),[Rosen Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+R),[Felix Jablonski](https://arxiv.org/search/cs?searchtype=author&amp;query=Jablonski,+F),[Shi Bin Hoo](https://arxiv.org/search/cs?searchtype=author&amp;query=Hoo,+S+B),[Anurag Garg](https://arxiv.org/search/cs?searchtype=author&amp;query=Garg,+A),[Jake Robertson](https://arxiv.org/search/cs?searchtype=author&amp;query=Robertson,+J),[Magnus B\u00fchler](https://arxiv.org/search/cs?searchtype=author&amp;query=B\u00fchler,+M),[Vladyslav Moroshan](https://arxiv.org/search/cs?searchtype=author&amp;query=Moroshan,+V),[Lennart Purucker](https://arxiv.org/search/cs?searchtype=author&amp;query=Purucker,+L),[Clara Cornu](https://arxiv.org/search/cs?searchtype=author&amp;query=Cornu,+C),[Lilly Charlotte Wehrhahn](https://arxiv.org/search/cs?searchtype=author&amp;query=Wehrhahn,+L+C),[Alessandro Bonetto](https://arxiv.org/search/cs?searchtype=author&amp;query=Bonetto,+A),[Bernhard Sch\u00f6lkopf](https://arxiv.org/search/cs?searchtype=author&amp;query=Sch\u00f6lkopf,+B),[Sauraj Gambhir](https://arxiv.org/search/cs?searchtype=author&amp;query=Gambhir,+S),[Noah Hollmann](https://arxiv.org/search/cs?searchtype=author&amp;query=Hollmann,+N),[Frank Hutter](https://arxiv.org/search/cs?searchtype=author&amp;query=Hutter,+F)\nView a PDF of the paper titled TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models, by L\\\\&#39;&#39;eo Grinsztajn and 25 other authors\n[View PDF](https://arxiv.org/pdf/2511.08667)[HTML (experimental)](https://arxiv.org/html/2511.08667v1)> > Abstract:\n> The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases. This report introduces TabPFN-2.5, the next generation of our tabular foundation model, built for datasets with up to 50,000 data points and 2,000 features, a 20x increase in data cells compared to TabPFNv2. TabPFN-2.5 is now the leading method for the industry standard benchmark TabArena (which contains datasets with up to 100,000 training data points), substantially outperforming tuned tree-based models and matching the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2. Remarkably, default TabPFN-2.5 has a 100% win rate against default XGBoost on small to medium-sized classification datasets (&lt;=10,000 data points, 500 features) and a 87% win rate on larger datasets up to 100K samples and 2K features (85% for regression). For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment. This new release will immediately strengthen the performance of the many applications and methods already built on the TabPFN ecosystem. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2511.08667](https://arxiv.org/abs/2511.08667)[cs.LG]|\n|(or[arXiv:2511.08667v1](https://arxiv.org/abs/2511.08667v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2511.08667](https://doi.org/10.48550/arXiv.2511.08667)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: L\u00e9o Grinsztajn [[view email](https://arxiv.org/show-email/aa1d0383/2511.08667)]\n**[v1]**Tue, 11 Nov 2025 18:57:15 UTC (3,855 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models, by L\\\\&#39;&#39;eo Grinsztajn and 25 other authors\n* [View PDF](https://arxiv.org/pdf/2511.08667)\n* [HTML (experimental)](https://arxiv.org/html/2511.08667v1)\n* [TeX Source](https://arxiv.org/src/2511.08667)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2511.08667&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2511.08667&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-11](https://arxiv.org/list/cs.LG/2025-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2511.08667?context=cs)\n[stat](https://arxiv.org/abs/2511.08667?context=stat)\n[stat.ML](https://arxiv.org/abs/2511.08667?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.08667)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.08667)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.08667)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2511.08667&amp;description=TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2511.08667&amp;title=TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are S...",
      "url": "https://arxiv.org/abs/2511.08667"
    }
  ]
}