{
  "query": "What are the best encoding techniques for categorical features with over 20,000 unique categories in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Encoding a column that contains\u202f>\u202f20\u202f000 distinct values is impractical with one\u2011hot or dummy variables because the resulting matrix becomes huge and extremely sparse.  In recent Kaggle competitions the most reliable work\u2011arounds are **target\u2011based encodings**, **frequency/count encodings**, **feature\u2011hashing**, and **learned embeddings**.  Target (or mean) encoding replaces each category with the conditional expectation of the target (often smoothed or using leave\u2011one\u2011out to avoid leakage) and works well for both regression and classification tasks\u202f([Towards Data Science \u2013 4 ways](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13)).  Count or frequency encoding simply substitutes the raw occurrence count (or proportion) of each category, which preserves information about rarity without exploding dimensionality\u202f([DataScience\u202fSE \u2013 count frequencies](https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories)).  \n\nWhen the cardinality is truly massive, the **hashing trick** (feature hashing) maps categories to a fixed\u2011size numeric space via a hash function, trading a small amount of collision\u2011induced noise for a constant\u2011size representation that scales to millions of categories\u202f([Towards Data Science \u2013 4 ways](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13)).  For deep\u2011learning pipelines, **entity embeddings** learned jointly with the model (e.g., using TensorFlow or PyTorch) provide dense low\u2011dimensional vectors that capture similarity between rare categories and often outperform hand\u2011crafted encodings\u202f([Towards Data Science \u2013 4 ways](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13)).  \n\nA recent benchmark of\u202f55\u202fcategorical encoders confirmed that the above four families consistently rank among the top performers on high\u2011cardinality datasets, while more exotic methods (e.g., ordinal encoders with smoothing) usually lag behind\u202f([Towards Data Science \u2013 Benchmark & Taxonomy](https://towardsdatascience.com/a-benchmark-and-taxonomy-of-categorical-encoders-9b7a0dc47a8c)).  In practice, Kagglers often combine techniques: group very rare levels into an \u201cOther\u201d bucket, apply count or target encoding, and optionally add a hashed feature for residual information, which together give a compact yet informative representation that scales to tens of thousands of categories.",
      "url": ""
    },
    {
      "title": "A Benchmark and Taxonomy of Categorical Encoders",
      "text": "A Benchmark and Taxonomy of Categorical Encoders | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Data Science](https://towardsdatascience.com/category/data-science/)\n# A Benchmark and Taxonomy of Categorical Encoders\nNew. Comprehensive. Extendable.\n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\nMar 29, 2024\n14 min read\nShare\n![Image created by author with recraft.ai](https://towardsdatascience.com/wp-content/uploads/2024/03/17bXer7VY3RT9gBppCrhOpQ.png)Image created by author with recraft.ai\nA large share of datasets contain categorical features. For example, out of 665 datasets on the[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/)[1], 42 are fully categorical and 366 are reported as mixed. However, distance-based ML models and almost all[scikit-learn implementations](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)require features in a numerical format. Categorical encoders replace the categories in such features with real numbers.\nA variety of categorical encoders exist, but there have been few attempts to compare them on many datasets, with various ML models, and in different pipelines. This article is about one of the latest benchmarks of encoders from our[recent publication](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ac01e21bb14609416760f790dd8966ae-Abstract-Datasets_and_Benchmarks.html)[2] ([poster](<https://nips.cc/media/PosterPDFs/NeurIPS 2023/73555.png?t=1699521284.38544>),[code on GitHub](https://github.com/DrCohomology/EncoderBenchmarking)). In this story, I focus on the content that complements the publication and is of practical importance. In particular, beyond the summary of our benchmark results, I will:\n* Provide a list of 55 categorical encoders and the links to find their explanations and implementations for most of them.\n* Explain that you can also use our code as a supplement to the`[Category Encoders](https://contrib.scikit-learn.org/category\\_encoders/)`python module for the encoders not yet implemented there.\n* Categorize the encoders into families so that you do not have to remember each individual encoder, but instead have an idea of how to build a member of each family.\n* Explain how you can reuse the code from [2] and detailed benchmark data to include your encoder, dataset, or ML models in the comparison without having to re-run the existing experiments. Depending on the scope of your experiments and your computational resources, this can save you weeks of computation.### **Why another benchmark?**\nThere are already several scientific studies comparing categorical encoders [3\u201312] and at least one categorical encoder[benchmark on TDS](https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8)[13]. The study [2] that I will present here differs mainly in scope: We compared representative encoders in different configurations from a variety of encoder families. We experimented with**5**ML models (decision tree, kNN, SVM, logistic regression, LGBM),**4**quality metrics (AUC, accuracy, balanced accuracy, F1-score),**3**tuning strategies (which I will describe shortly),**50**datasets, and**32**encoder configurations.\nThe following table shows the encoders covered in our benchmark (the dark green column) and in other experimental comparisons (the light green columns). The blue columns show the encoders described in two additional sources: in the article dedicated to contrast encoders [15] and on Medium [14]:\n[**> Categorical Encoding: Key Insights\n**](https://medium.com/@vadim.arzamasov/navigating-categorical-encoder-maze-c04e49b165fe)\nThe last yellow column shows the encoders covered by the`[Category Encoders](https://contrib.scikit-learn.org/category\\_encoders/)`module [16]. Note that the code from [2] implements some encoders &#8211; from the similarity, binning, and data constraining families &#8211; that are not part of the`Category Encoders`module. In addition, we have found that the interface to the GLMM encoder implemented in R and used in [2] is much faster than the GLMM encoder from`Category Encoders`. Therefore, you may find our code useful for these implementations.\n![Table 1. Encoder families and their coverage by various resources. Author owns copyright](https://towardsdatascience.com/wp-content/uploads/2024/03/1BYX8Ac3sphPE_LcWjgHAaQ.jpeg)Table 1. Encoder families and their coverage by various resources. Author owns copyright\nTable 1 already contains a number of encoders, and the list is by no means exhaustive. To navigate the encoder landscape, it is therefore useful to classify encoders in order to understand the principles of encoder design, rather than to memorize a large number of individual encoders.\n### Families of encoders\nIn the following, we consider a categorical feature of length`n`with cardinality`k`. At the top level, categorical encoders are supervised or unsupervised.\n**1. Unsupervised encoders**do not include the target variable in the encoding process.\n***1.1. Identifier encoders***transform categorical variables using a injective function, i.e., they assign each category a unique numeric value or a unique combination of numeric values. They create from 1 up to`k`new features. For example, One-Hot encoder creates`k`features, label or ordinal encoders create a single new feature, Base N encoders create \u2308`log(k)`\u2309 new features, where the logarithm is of base N.\nThese encoders are useful for categorical variables that denote unique identifiers, such as product codes or zip codes. Typically, with the exception of ordinal encoder, identifier encoders do not assume that any inherent order in the categories conveys meaningful information for analysis, and thus ignore any such order during the encoding process.\n***1.2. Contrast encoders***transform categorical variables by assigning numerical values based on comparisons between categories. Typically, a set of`k-1`new features represents a categorical variable with`k`categories. Unlike identifier encoders, these encoders are specifically designed to explore the relationships between different levels of a categorical variable in a regression analysis.\nTo create a contrast encoder, one has a choice of different[coding schemes](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#ORTHOGONAL)[15]. Each contrasts a category with other categories in a particular way to test a related hypothesis about the data. For example, Helmert coding contrasts each level with the mean of subsequent levels, while sum coding contrasts each level with the overall mean.\n***1.3. Frequency encoders***replace the categories of a categorical variable with corresponding values that are a function of the frequency of those categories in the data set. Unlike identifier or contrast encoders, frequency encoders create a single new feature and are not necessarily injective functions. They provide a numerical representation of occurrence rates, which can be particularly useful when an ML model benefits from understanding the commonality of category values. All three frequency encoders in Table 1 are monotonically increasing functions of frequency. However, this is not a necessary condition for defining this group.\n***1.4. Similarity encoders***[5, 8, 18] transform categorical data into numerical form by applying similarity or distance metrics that capture similarities between different categories.\nOne group of similarity encoders [8, 18] is based on a morphological comparison between two categories treated as ...",
      "url": "https://towardsdatascience.com/a-benchmark-and-taxonomy-of-categorical-encoders-9b7a0dc47a8c"
    },
    {
      "title": "Strategies to encode categorical variables with many categories",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Strategies to encode categorical variables with many categories](https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years, 6 months ago\n\nModified [1 year, 2 months ago](https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories?lastactivity)\n\nViewed\n4k times\n\n4\n\n$\\\\begingroup$\n\nI was going over the Kaggle competitions [IEEE](https://www.kaggle.com/c/ieee-fraud-detection), [Categorical Feature Encoding Challenge](https://www.kaggle.com/c/cat-in-the-dat) and one of the ways in which categorical variables have been handled is by replacing the variables by the respective count frequencies of the variables in the column. I understand why we need to transform the variables, but can someone please explain why this approach was used. What exactly is the logic behind it?\n\nSecondly, I was wondering if anyone may be willing to share other techniques of dealing with categorical variables (excluding the obvious one hot encoding) or point me some interesting approaches for the same. Thank you!\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [preprocessing](https://datascience.stackexchange.com/questions/tagged/preprocessing)\n- [categorical-data](https://datascience.stackexchange.com/questions/tagged/categorical-data)\n- [categorical-encoding](https://datascience.stackexchange.com/questions/tagged/categorical-encoding)\n\n[Share](https://datascience.stackexchange.com/q/64460)\n\n[Improve this question](https://datascience.stackexchange.com/posts/64460/edit)\n\nFollow\n\n[edited Apr 15, 2023 at 17:13](https://datascience.stackexchange.com/posts/64460/revisions)\n\n[![Pluviophile's user avatar](https://i.sstatic.net/Jbc0a.png?s=64)](https://datascience.stackexchange.com/users/83275/pluviophile)\n\n[Pluviophile](https://datascience.stackexchange.com/users/83275/pluviophile)\n\n3,9181313 gold badges3131 silver badges5454 bronze badges\n\nasked Dec 9, 2019 at 9:24\n\nuser86099user86099\n\n$\\\\endgroup$\n\n5\n\n- $\\\\begingroup$maybe wise to provide the Kaggle scripts you're referring to$\\\\endgroup$\n\n\u2013\u00a0[Valentin Calomme](https://datascience.stackexchange.com/users/38887/valentin-calomme)\n\nCommentedDec 9, 2019 at 9:26\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Check this [post](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02). It covers most of vanilla techniques.$\\\\endgroup$\n\n\u2013\u00a0[Piotr Rarus](https://datascience.stackexchange.com/users/85398/piotr-rarus)\n\nCommentedDec 9, 2019 at 9:43\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Frequency encoding is mostly used to alleviate dimensionality explosion. When you encounter data with a lot of categories used, encoding might result in very sparse set.$\\\\endgroup$\n\n\u2013\u00a0[Piotr Rarus](https://datascience.stackexchange.com/users/85398/piotr-rarus)\n\nCommentedDec 9, 2019 at 9:45\n\n- 1\n\n\n\n\n\n$\\\\begingroup$@PiotrRarus-ReinstateMonica Sorry I haven't checked your comment. Updated the same link you posted in the comment, into my answer.$\\\\endgroup$\n\n\u2013\u00a0[Pluviophile](https://datascience.stackexchange.com/users/83275/pluviophile)\n\nCommentedDec 9, 2019 at 10:02\n\n- $\\\\begingroup$For the first question, see [datascience.stackexchange.com/q/63749/55122](https://datascience.stackexchange.com/q/63749/55122)$\\\\endgroup$\n\n\u2013\u00a0[Ben Reiniger](https://datascience.stackexchange.com/users/55122/ben-reiniger) \u2666\n\nCommentedJul 28, 2021 at 13:58\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n4\n\n$\\\\begingroup$\n\nGenerally, the logic of the categorical count transformation lies in the fact that features with similar frequencies tend to behave similarly. Have words in a corpus as an example, common words share little or no real information whereas uncommon words share more information with an algorithm.\n\nSpecifically, certain algorithms (tree-based methods) could even yield rules given an unspecified category from an event count. Say, for example, we have an unknown category whose count is 4. The algorithm may give a rule:\n\n> If Column Count is < 5 and N is > 3 = X\n\nThat will be exactly the same as if an algorithm took a One-Hot encoded column and gave a rule:\n\n> If One-Hot-Encoded-Column is > 0 = X\n\nIn that case, a tree-based algorithm will make several rules from many categories using the same count column. But how I said at the beginning, algorithms generalize among populations of similar counts so most likely you will find rules like:\n\n> If Column Counts is < 10 and N is > 3 = X\n\nWhich will often contain different categories that behave similarly. Just check the models and look for parameters/ importance of the column in question to see this for yourself.\n\n> `Secondly, I was wondering if anyone may be willing to share other techniques of dealing with categorical variables.`\n\nFeature hashing became really popular there not so long ago. Hashing has very nice properties and it's a whole topic learned at schools but the main principle is that if you have a category with high cardinality you decide a minimum number of reduced categories (hashes) that all the categories will have to share. if two categories share the same hash or bucket, that is called a hash collision. Feature hashing doesn't deal with hash collisions because according to some authors (I don't have the reference here) may improve accuracy by forcing the algorithm to pick more carefully the features.\n\nThere are many ways we can encode these categorical variables as numbers and use them in the algorithm.\n\n```\n1) One Hot Encoding\n2) Label Encoding\n3) Ordinal Encoding\n4) Helmert Encoding\n5) Binary Encoding\n6) Frequency Encoding\n7) Mean Encoding\n8) Weight of Evidence Encoding\n9) Probability Ratio Encoding\n10) Hashing Encoding\n11) Backward Difference Encoding\n12) Leave One Out Encoding\n13) James-Stein Encoding\n14) M-estimator Encoding\n\n```\n\nFind the below cheatsheet\n\n[![enter image description here](https://i.sstatic.net/OY2lu.png)](https://i.sstatic.net/OY2lu.png)\n\n[More Info](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)\n\n[Share](https://datascience.stackexchange.com/a/64469)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/64469/edit)\n\nFollow\n\nanswered Dec 9, 2019 at 9:59\n\n[![Pluviophile's user avatar](https://i.sstatic.net/Jbc0a.png?s=64)](https://datascience.stackexchange.com/users/83275/pluviophile)\n\n[Pluviophile](https://datascience.stackexchange.com/users/83275/pluviophile) Pluviophile\n\n3,9181313 gold badges3131 silver badges5454 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f64460%2fstrategies-to-encode-categorical-variables-with-many-categories%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\nHappy 10th ...",
      "url": "https://datascience.stackexchange.com/questions/64460/strategies-to-encode-categorical-variables-with-many-categories"
    },
    {
      "title": "Encoding High Cardinality Categorical Variables | Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1bc6d8fd7b13&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# 4 Ways to Encode Categorical Features with High Cardinality \u2014 with Python Implementation\n\n## Learn to apply target encoding, count encoding, feature hashing and Embedding using scikit-learn and TensorFlow\n\n[![Aicha Bokbot](https://miro.medium.com/v2/resize:fill:88:88/1*jJAsvCE_o2AHOXeWdjseKQ.jpeg)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Aicha Bokbot](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nJun 26, 2023\n\n--\n\nShare\n\n\u201cClick\u201d \u2014 Photo by [Cleo Vermij](https://unsplash.com/@cleovermij?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nIn this article, we will go through 4 popular methods to encode categorical variables with high cardinality: **(1) Target encoding, (2) Count encoding, (3) Feature hashing** and **(4) Embedding**.\n\nWe will explain how each method works, discuss its pros and cons and observe its impact on the performance of a classification task.\n\n## **Table of content**\n\n\u2014 [Introducing categorical features](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#8744)\n\n_(1)_ [_Why do we need to encode categorical features?_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#2429) _(2)_ [_Why one-hot encoding is not suited to high cardinality?_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#b13b)\n\n\u2014 [Application on an AdTech dataset](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#706a)\n\n\u2014 [Overview of each encoding method](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#a959)\n\n_(1)_ [_Target encoding_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#ffbc) _(2)_ [_Count encoding_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#fbd1) _(3)_ [_Feature hashing_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#e278) _(4)_ [_Embedding_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#99d8)\n\n\u2014 [Benchmarking the performance to predict CTR](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#892c)\n\n\u2014 [Conclusion](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#033c)\n\n\u2014 [To go further](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#3bd1)\n\n# **Introducing**\u2026\n\n[![Aicha Bokbot](https://miro.medium.com/v2/resize:fill:144:144/1*jJAsvCE_o2AHOXeWdjseKQ.jpeg)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------follow_profile-----------)\n\n[**Written by Aicha Bokbot**](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[46 Followers](https://medium.com/@aichabokbot/followers?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n\u00b7Writer for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\nMachine learning engineering & Data science \\| [www.linkedin.com/in/aichabokbot/](http://www.linkedin.com/in/aichabokbot/)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Text to speech](https://speechify.com/mediu...",
      "url": "https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe"
    },
    {
      "title": "A Benchmark and Taxonomy of Categorical Encoders - Towards Data Science",
      "text": "[Data Science](https://towardsdatascience.com/category/data-science/)\n\n# A Benchmark and Taxonomy of Categorical Encoders\n\nNew. Comprehensive. Extendable.\n\n[Vadim Arzamasov](https://towardsdatascience.com/author/vadim-arzamasov/)\n\nMar 29, 2024\n\n14 min read\n\nShare\n\nImage created by author with recraft.ai\n\nA large share of datasets contain categorical features. For example, out of 665 datasets on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/) \\[1\\], 42 are fully categorical and 366 are reported as mixed. However, distance-based ML models and almost all [scikit-learn implementations](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) require features in a numerical format. Categorical encoders replace the categories in such features with real numbers.\n\nA variety of categorical encoders exist, but there have been few attempts to compare them on many datasets, with various ML models, and in different pipelines. This article is about one of the latest benchmarks of encoders from our [recent publication](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ac01e21bb14609416760f790dd8966ae-Abstract-Datasets_and_Benchmarks.html) \\[2\\] ( [poster](https://nips.cc/media/PosterPDFs/NeurIPS%202023/73555.png?t=1699521284.38544), [code on GitHub](https://github.com/DrCohomology/EncoderBenchmarking)). In this story, I focus on the content that complements the publication and is of practical importance. In particular, beyond the summary of our benchmark results, I will:\n\n- Provide a list of 55 categorical encoders and the links to find their explanations and implementations for most of them.\n- Explain that you can also use our code as a supplement to the `[Category Encoders](https://contrib.scikit-learn.org/category_encoders/)` python module for the encoders not yet implemented there.\n- Categorize the encoders into families so that you do not have to remember each individual encoder, but instead have an idea of how to build a member of each family.\n- Explain how you can reuse the code from \\[2\\] and detailed benchmark data to include your encoder, dataset, or ML models in the comparison without having to re-run the existing experiments. Depending on the scope of your experiments and your computational resources, this can save you weeks of computation.\n\n### **Why another benchmark?**\n\nThere are already several scientific studies comparing categorical encoders \\[3\u201312\\] and at least one categorical encoder [benchmark on TDS](https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8) \\[13\\]. The study \\[2\\] that I will present here differs mainly in scope: We compared representative encoders in different configurations from a variety of encoder families. We experimented with **5** ML models (decision tree, kNN, SVM, logistic regression, LGBM), **4** quality metrics (AUC, accuracy, balanced accuracy, F1-score), **3** tuning strategies (which I will describe shortly), **50** datasets, and **32** encoder configurations.\n\nThe following table shows the encoders covered in our benchmark (the dark green column) and in other experimental comparisons (the light green columns). The blue columns show the encoders described in two additional sources: in the article dedicated to contrast encoders \\[15\\] and on Medium \\[14\\]:\n\n> [**Categorical Encoding: Key Insights**](https://medium.com/@vadim.arzamasov/navigating-categorical-encoder-maze-c04e49b165fe)\n\nThe last yellow column shows the encoders covered by the `[Category Encoders](https://contrib.scikit-learn.org/category_encoders/)` module \\[16\\]. Note that the code from \\[2\\] implements some encoders \u2013 from the similarity, binning, and data constraining families \u2013 that are not part of the `Category Encoders` module. In addition, we have found that the interface to the GLMM encoder implemented in R and used in \\[2\\] is much faster than the GLMM encoder from `Category Encoders`. Therefore, you may find our code useful for these implementations.\n\nTable 1. Encoder families and their coverage by various resources. Author owns copyright\n\nTable 1 already contains a number of encoders, and the list is by no means exhaustive. To navigate the encoder landscape, it is therefore useful to classify encoders in order to understand the principles of encoder design, rather than to memorize a large number of individual encoders.\n\n### Families of encoders\n\nIn the following, we consider a categorical feature of length `n` with cardinality `k`. At the top level, categorical encoders are supervised or unsupervised.\n\n**1\\. Unsupervised encoders** do not include the target variable in the encoding process.\n\n_**1.1. Identifier encoders**_ transform categorical variables using a injective function, i.e., they assign each category a unique numeric value or a unique combination of numeric values. They create from 1 up to `k` new features. For example, One-Hot encoder creates `k` features, label or ordinal encoders create a single new feature, Base N encoders create \u2308 `log(k)`\u2309 new features, where the logarithm is of base N.\n\nThese encoders are useful for categorical variables that denote unique identifiers, such as product codes or zip codes. Typically, with the exception of ordinal encoder, identifier encoders do not assume that any inherent order in the categories conveys meaningful information for analysis, and thus ignore any such order during the encoding process.\n\n_**1.2. Contrast encoders**_ transform categorical variables by assigning numerical values based on comparisons between categories. Typically, a set of `k-1` new features represents a categorical variable with `k` categories. Unlike identifier encoders, these encoders are specifically designed to explore the relationships between different levels of a categorical variable in a regression analysis.\n\nTo create a contrast encoder, one has a choice of different [coding schemes](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#ORTHOGONAL) \\[15\\]. Each contrasts a category with other categories in a particular way to test a related hypothesis about the data. For example, Helmert coding contrasts each level with the mean of subsequent levels, while sum coding contrasts each level with the overall mean.\n\n_**1.3. Frequency encoders**_ replace the categories of a categorical variable with corresponding values that are a function of the frequency of those categories in the data set. Unlike identifier or contrast encoders, frequency encoders create a single new feature and are not necessarily injective functions. They provide a numerical representation of occurrence rates, which can be particularly useful when an ML model benefits from understanding the commonality of category values. All three frequency encoders in Table 1 are monotonically increasing functions of frequency. However, this is not a necessary condition for defining this group.\n\n_**1.4. Similarity encoders**_ \\[5, 8, 18\\] transform categorical data into numerical form by applying similarity or distance metrics that capture similarities between different categories.\n\nOne group of similarity encoders \\[8, 18\\] is based on a morphological comparison between two categories treated as strings. Examples of similarity metrics are Levenshtein\u2019s ratio, Jaro-Winkler similarity, or N-gram similarity. The categorical variable is then encoded as a vector, where each dimension corresponds to a pairwise comparison of a reference category with all categories, and the value represents the computed similarity score (similar to constructing a [variance-covariance](https://en.wikipedia.org/wiki/Covariance_matrix) matrix). Encoders of this group typically create `k` new features. This encoding is particularly useful for handling \"dirty\" categorical datasets that may contain typos and redundancies \\[18\\]. One can think of One-Hot encoding as a special case of similarity encoding, where the similarity measure can take only...",
      "url": "https://towardsdatascience.com/a-benchmark-and-taxonomy-of-categorical-encoders-9b7a0dc47a8c?gi=335772563ce4"
    },
    {
      "title": "Encoding of categorical variables with high cardinality",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Encoding of categorical variables with high cardinality](https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked5 years ago\n\nModified [11 months ago](https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality?lastactivity)\n\nViewed\n33k times\n\n23\n\n$\\\\begingroup$\n\nFor **unsupervised anomaly detection / fraud analytics** on credit card data (where I don't have labeled fraudulent cases), there are a lot of variables to consider. The data is of mixed type with continuous/numerical variables (e.g. USD amount spent) as well as **categorical variables** (e.g. account number).\n\nWhat is the most suitable way of including categorical variables that have a very large number of unique classes? My thoughts so far:\n\n- **Label Encoding** ( [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)): i.e. mapping integers to classes. While it returns a nice single encoded feature column, it imposes a false sense of ordinal relationship (e.g. 135 > 72).\n- **One Hot / Dummy Encoding** ( [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)): i.e. expanding the categorical feature into lots of dummy columns taking values in {0,1}. This is infeasible for categorical features having e.g. >10,000 unique values. I understand that models will struggle with the sparse and large data.\n\nWhat **other (more advanced?)** suitable methods are there to include large categorical feature columns? Is it possible to still use One Hot Encoding with some tricks? I read about bin counting ( [Microsoft blog](https://blogs.technet.microsoft.com/machinelearning/2015/02/17/big-learning-made-easy-with-counts/)) though I haven't found any applications related to intrusion detection / fraud analytics.\n\nP.S.: In my view, this problem seems very similar to encoding an IP-address feature column when dealing with unsupervised intrusion detection.\n\n- [machine-learning](https://stats.stackexchange.com/questions/tagged/machine-learning)\n- [categorical-data](https://stats.stackexchange.com/questions/tagged/categorical-data)\n- [categorical-encoding](https://stats.stackexchange.com/questions/tagged/categorical-encoding)\n- [anomaly-detection](https://stats.stackexchange.com/questions/tagged/anomaly-detection)\n- [many-categories](https://stats.stackexchange.com/questions/tagged/many-categories)\n\n[Share](https://stats.stackexchange.com/q/411767)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/411767/edit)\n\nFollow\n\n[edited Jul 1, 2019 at 16:07](https://stats.stackexchange.com/posts/411767/revisions)\n\n[![kjetil b halvorsen's user avatar](https://i.sstatic.net/Nri78.jpg?s=64)](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen)\n\n[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\n80.4k3131 gold badges196196 silver badges640640 bronze badges\n\nasked Jun 6, 2019 at 9:19\n\n[![robot_2077198's user avatar](https://i.sstatic.net/A4GL9.png?s=64)](https://stats.stackexchange.com/users/232372/robot-2077198)\n\n[robot\\_2077198](https://stats.stackexchange.com/users/232372/robot-2077198) robot\\_2077198\n\n72744 gold badges77 silver badges1717 bronze badges\n\n$\\\\endgroup$\n\n4\n\n- $\\\\begingroup$See the good advice in [stats.stackexchange.com/questions/146907/\u2026](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels)$\\\\endgroup$\n\n\u2013\u00a0[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\nCommentedJul 1, 2019 at 16:08\n\n- $\\\\begingroup$linear models have no problems with sparse data (nb bag of words models, where the words/ bigrams/trigrams are definitely in the 1000's) basically you just needs a model that supports sparse data (eg glmnet, vowpalwabbit etc). I have found sklearn is not so good for handling sparse data ( memory hungry). vowpalwabbit does hash coding, but this is effectively one hot coding after hashing ( so you will typically have 10000 unique values after hashing too). I have used (python-glmnet) with a dataset of (548823, 45544) - which took 2 hours. \\[dummy variables and their interactions etc\\]$\\\\endgroup$\n\n\u2013\u00a0[seanv507](https://stats.stackexchange.com/users/27556/seanv507)\n\nCommentedJul 1, 2019 at 16:19\n\n- 1\n\n\n\n\n\n$\\\\begingroup$as @zhubarb mentioned, why would you even want to use account number as a feature? maybe if you explain that then the apporpriate encoding will become more obvious$\\\\endgroup$\n\n\u2013\u00a0[seanv507](https://stats.stackexchange.com/users/27556/seanv507)\n\nCommentedJul 1, 2019 at 16:20\n\n- $\\\\begingroup$If you are dealing with categorical sensitive data, you might want to consider the fairness implications [dl.acm.org/doi/fullHtml/10.1145/3600211.3604657](https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604657) I heard authors are super nice to reach out$\\\\endgroup$\n\n\u2013\u00a0[Carlos Mougan](https://stats.stackexchange.com/users/270023/carlos-mougan)\n\nCommentedOct 20, 2023 at 9:30\n\n\n[Add a comment](https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality)\u00a0\\|\n\n## 4 Answers 4\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n14\n\n$\\\\begingroup$\n\n[This link](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159) provides a very good summary and should be helpful. As you allude to, label-encoding should not be used for nominal variables at it introduces an artificial ordinality. [Hashing](https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f) is a potential alternative that is particularity suitable for features that have high cardinality.\n\nYou can also use a [distributed representation](https://www.tensorflow.org/tutorials/representation/word2vec), which has become very popular in the deep learning community. The most common example given for distributed representation is word embeddings in NLP. That is not to say you cannot utilise them in encoding other categorical features. [Here](https://developers.google.com/machine-learning/crash-course/embeddings/categorical-input-data) is an example.\n\nFinally, account number would not be a wise input as it is more a unique identifier rather than a generalisable (account) feature.\n\n[Share](https://stats.stackexchange.com/a/411775)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/411775/edit)\n\nFollow\n\n[edited Jun 6, 2019 at 10:16](https://stats.stackexchange.com/posts/411775/revisions)\n\nanswered Jun 6, 2019 at 10:10\n\n[![Zhubarb's user avatar](https://i.sstatic.net/G7Fgh.jpg?s=64)](https://stats.stackexchange.com/users/28740/zhubarb)\n\n[Zhubarb](https://stats.stackexchange.com/users/28740/zhubarb) Zhubarb\n\n8,32933 gold badges3434 silver badges4949 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$By distributed representation you mean just learn embedding vector for each row and connect them with FC layers, correct?$\\\\endgroup$\n\n\u2013\u00a0[haneulkim](https://stats.stackexchange.com/users/265669/haneulkim)\n\nCommentedOct 26, 2022 at 4:54\n\n\n[Add a comment](https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality)\u00a0\\|\n\n3\n\n$\\\\begingroup$\n\nThis might help Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems: [https://link.springer.com/chapter/10.1007%2F978-3-030-85529-1\\_14](https://link.springer.com/chapter/10.1007%2F978-3-030-855...",
      "url": "https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality"
    },
    {
      "title": "How to deal with Features having high cardinality | Kaggle",
      "text": "<div><div><p>One thing I do is try to ensure that the cardinality of the categorical information in the training set resembles that in the test/validation sets. That is, if I have a feature with values {A,A,A,B,C,C,D} in train, but test only has {A,B,B}, then eliminating the C and D records, and undersampling the A or oversampling the B records may resist overfitting.</p>\n<p>Also, for individual featuers with low cardinality, it's often worth bucketing them. In the above example, you may end up replacement values for A and C, and then bucketing B and D into an \"Other\" category (similar to Triskelion's trick with COUNT replacement).</p>\n<p>But overfitting is also something you can solve for by careful training -- split your training set in multiple ways, relying on cross-validation to minimize and test for over fitting before going to the private test data sets.</p></div></div>",
      "url": "https://www.kaggle.com/discussions/general/16927"
    },
    {
      "title": "One hot encoding alternatives for large categorical values? | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13989df602e1d9a2aa20:1:11563)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/197627"
    },
    {
      "title": "How to handle large number of categorical values? | Kaggle",
      "text": "Loading web-font TeX/Math/Italic\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/getting-started/37489#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/discussions/getting-started/37489)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/getting-started/37489#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fdiscussions%2Fgetting-started%2F37489)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fdiscussions%2Fgetting-started%2F37489)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=65e25d68eb562b179c3e:1:10718)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/discussions/getting-started/37489"
    }
  ]
}