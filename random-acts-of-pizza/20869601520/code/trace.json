{
  "spans": [
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "02401644febbbe17",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1768005377871699759,
      "end_time": 1768005377872163466,
      "duration": 0.000463707,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7ef374cf44d0>\", \"input\": {\"file_path\": \"../research/description.md\"}}",
        "tool.output": "{\"content\": \"# Random Acts of Pizza\\n\\n**Competition:** random-acts-of-pizza\\n**Category:** None\\n**Deadline:** 2015-06-01T23:59:00Z\\n**Reward:** None\\n**Evaluation Metric:** Area Under Receiver Operating Characteristic Curve\\n**Teams:** 462\\n\\n---\\n\\n## data-description\\n\\n<p><a href=\\\"https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark\\\">See, fork, and run a random forest benchmark model through Kaggle Scripts</a></p>\\n<p>This dataset includes 5671 requests collected from the Reddit community <a href=\\\"http://www.reddit.com/r/Random_Acts_Of_Pizza/\\\">Random Acts of Pizza</a> between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.</p>\\n<p>Each JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.</p>\\n<h2>Data fields</h2>\\n<p>\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).</p>\\n<p>\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.</p>\\n<p>\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.</p>\\n<p>\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).</p>\\n<p>\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".</p>\\n<p>\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.</p>\\n<p>\\\"request_text\\\": Full text of the request.</p>\\n<p>\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".</p>\\n<p>\\\"request_title\\\": Title of the request.</p>\\n<p>\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.</p>\\n<p>\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.</p>\\n<p>\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).</p>\\n<p>\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.</p>\\n<p>\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.</p>\\n<p>\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.</p>\\n<p>\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.</p>\\n<p>\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.</p>\\n<p>\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.</p>\\n<p>\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.</p>\\n<p>\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.</p>\\n<p>\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.</p>\\n<p>\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.</p>\\n<p>\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.</p>\\n<p>\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.</p>\\n<p>\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.</p>\\n<p>\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.</p>\\n<p>\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.</p>\\n<p>\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.</p>\\n<p>\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).</p>\\n<p>\\\"requester_username\\\": Reddit username of requester.</p>\\n<p>\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).</p>\\n<p>\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.</p>\\n\\n## Description\\n\\n<p><a href=\\\"https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza\\\">Get started on this competition through Kaggle Scripts</a></p>\\n<p>In machine learning, it is often said there are <a href=\\\"http://en.wikipedia.org/wiki/No_free_lunch_theorem\\\">no free lunches</a>. <em>How wrong we were.</em></p>\\n<p>This competition contains a dataset with 5671 textual requests for pizza from the Reddit community <a href=\\\"http://www.reddit.com/r/Random_Acts_Of_Pizza/\\\">Random Acts of Pizza</a> together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.</p>\\n<p>\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?</p>\\n<p><img src=\\\"https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png\\\" alt=\\\"Pizzas\\\" width=\\\"660\\\" height=\\\"162\\\" /></p>\\n<p>Kaggle is hosting this competition for the machine learning community to use for fun and practice. This <a href=\\\"http://cs.stanford.edu/~althoff/raop-dataset/\\\">data</a> was collected and graciously shared by <a href=\\\"http://www.timalthoff.com/\\\">Althoff</a> et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their <a href=\\\"http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf\\\">accompanying paper</a> and ask that you cite the following reference in any publications that result from your work:</p>\\n<p>Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. <em>How to Ask for a Favor: A Case Study on the Success of Altruistic Requests</em>, Proceedings of ICWSM, 2014.</p>\\n<p><sub><em>Pizza icons designed by Matthew Dera from the <a href=\\\"http://thenounproject.com/term/pizza/30194/\\\">Noun Project</a></em></sub></p>\\n\\n## Evaluation\\n\\n<p>Submissions are evaluated on <a href=\\\"http://en.wikipedia.org/wiki/Receiver_operating_characteristic\\\">area under the ROC curve</a> between the predicted probability that a request will get pizza and the observed outcomes.</p>\\n<h2>Submission File</h2>\\n<p>For each request in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:</p>\\n<pre>request_id,requester_received_pizza<br />t3_i8iy4,0<br />t3_1mfqi0,0<br />t3_lclka,0<br />...<br />...</pre>\", \"is_error\": false}",
        "tool.duration_ms": 0.49877166748046875
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "eb395c1cf4978ab8",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1768005377872333877,
      "end_time": 1768005377872575173,
      "duration": 0.000241296,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7ef34db5a350>\", \"input\": {\"directory_path\": \"../research/writeups\"}}",
        "tool.output": "{\"content\": \"Not a directory: /home/code/exploration/../research/writeups\", \"is_error\": true}",
        "tool.duration_ms": 0.25534629821777344
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "6aa252df19281712",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1768005377872742895,
      "end_time": 1768005377873037435,
      "duration": 0.00029454,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7ef34db5a350>\", \"input\": {\"directory_path\": \"../research/kernels\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels:\\n[DIR]  alvations_basic-nlp-with-nltk/\\n[DIR]  benhamner_simple-julia-benchmark/\\n[DIR]  chqngh_random-pizza/\\n[DIR]  jatinraina_random-acts-of-pizza-xgboost/\\n[FILE] metadata.json (1.5KB)\\n[DIR]  ynue21_random-act-of-pizza/\", \"is_error\": false}",
        "tool.duration_ms": 0.3418922424316406
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "a00b5d5c4126cdd2",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1768005384761832406,
      "end_time": 1768005384762220559,
      "duration": 0.000388153,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7ef374cf44d0>\", \"input\": {\"file_path\": \"../research/kernels/metadata.json\"}}",
        "tool.output": "{\"content\": \"[\\n  {\\n    \\\"ref\\\": \\\"alvations/basic-nlp-with-nltk\\\",\\n    \\\"title\\\": \\\"Basic NLP with NLTK\\\",\\n    \\\"author\\\": \\\"Liling Tan\\\",\\n    \\\"language\\\": \\\"\\\",\\n    \\\"kernel_type\\\": \\\"\\\",\\n    \\\"total_votes\\\": 533,\\n    \\\"last_run_time\\\": \\\"2017-11-29 14:19:50.027000\\\",\\n    \\\"enable_gpu\\\": false,\\n    \\\"enable_internet\\\": false,\\n    \\\"is_private\\\": false\\n  },\\n  {\\n    \\\"ref\\\": \\\"benhamner/simple-julia-benchmark\\\",\\n    \\\"title\\\": \\\"Simple Julia Benchmark\\\",\\n    \\\"author\\\": \\\"Ben Hamner\\\",\\n    \\\"language\\\": \\\"\\\",\\n    \\\"kernel_type\\\": \\\"\\\",\\n    \\\"total_votes\\\": 13,\\n    \\\"last_run_time\\\": \\\"2015-05-18 21:25:58.820000\\\",\\n    \\\"enable_gpu\\\": false,\\n    \\\"enable_internet\\\": false,\\n    \\\"is_private\\\": false\\n  },\\n  {\\n    \\\"ref\\\": \\\"jatinraina/random-acts-of-pizza-xgboost\\\",\\n    \\\"title\\\": \\\"Random acts of pizza xgboost\\\",\\n    \\\"author\\\": \\\"jatin raina\\\",\\n    \\\"language\\\": \\\"\\\",\\n    \\\"kernel_type\\\": \\\"\\\",\\n    \\\"total_votes\\\": 12,\\n    \\\"last_run_time\\\": \\\"2017-12-28 19:26:14.803000\\\",\\n    \\\"enable_gpu\\\": false,\\n    \\\"enable_internet\\\": false,\\n    \\\"is_private\\\": false\\n  },\\n  {\\n    \\\"ref\\\": \\\"ynue21/random-act-of-pizza\\\",\\n    \\\"title\\\": \\\"Random Act Of Pizza\\\",\\n    \\\"author\\\": \\\"Alexandre Magueresse\\\",\\n    \\\"language\\\": \\\"\\\",\\n    \\\"kernel_type\\\": \\\"\\\",\\n    \\\"total_votes\\\": 10,\\n    \\\"last_run_time\\\": \\\"2018-12-02 13:37:12.407000\\\",\\n    \\\"enable_gpu\\\": false,\\n    \\\"enable_internet\\\": false,\\n    \\\"is_private\\\": false\\n  },\\n  {\\n    \\\"ref\\\": \\\"chqngh/random-pizza\\\",\\n    \\\"title\\\": \\\"Random Pizza\\\",\\n    \\\"author\\\": \\\"CQHe\\\",\\n    \\\"language\\\": \\\"\\\",\\n    \\\"kernel_type\\\": \\\"\\\",\\n    \\\"total_votes\\\": 6,\\n    \\\"last_run_time\\\": \\\"2016-12-09 06:00:16.080000\\\",\\n    \\\"enable_gpu\\\": false,\\n    \\\"enable_internet\\\": false,\\n    \\\"is_private\\\": false\\n  }\\n]\", \"is_error\": false}",
        "tool.duration_ms": 0.44417381286621094
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "50aaa56349a6fe77",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1768005395929779829,
      "end_time": 1768005395930747330,
      "duration": 0.000967501,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7ef34db5a350>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.4KB)\\n[FILE] sampleSubmission.csv (18.2KB)\\n[FILE] test.json (2.9MB)\\n[FILE] test.json.zip (564.5KB)\\n[FILE] train.json (12.0MB)\\n[FILE] train.json.zip (1.7MB)\", \"is_error\": false}",
        "tool.duration_ms": 1.1832714080810547
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "90ddcbb64961e485",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "FetchKernelTool",
      "kind": "INTERNAL",
      "start_time": 1768005405817175512,
      "end_time": 1768005406381521361,
      "duration": 0.564345849,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "FetchKernelTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.kaggle_tools.FetchKernelTool object at 0x7ef34db59710>\", \"input\": {\"ref\": \"alvations/basic-nlp-with-nltk\"}}",
        "tool.output": "{\"content\": \"Kernel downloaded to: research/kernels/alvations_basic-nlp-with-nltk/\\nFiles:\\n  - kernel-metadata.json\\n  - basic-nlp-with-nltk.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 564.4016265869141
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "dd4e3a8e40ab49c8",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1768005414142173816,
      "end_time": 1768005414143161594,
      "duration": 0.000987778,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7ef374cf44d0>\", \"input\": {\"file_path\": \"../research/kernels/alvations_basic-nlp-with-nltk/basic-nlp-with-nltk.ipynb\"}}",
        "tool.output": "{\"content\": \"{\\\"metadata\\\":{\\\"kernelspec\\\":{\\\"name\\\":\\\"python3\\\",\\\"language\\\":\\\"python\\\",\\\"display_name\\\":\\\"Python 3\\\"},\\\"language_info\\\":{\\\"mimetype\\\":\\\"text/x-python\\\",\\\"pygments_lexer\\\":\\\"ipython3\\\",\\\"version\\\":\\\"3.6.3\\\",\\\"name\\\":\\\"python\\\",\\\"codemirror_mode\\\":{\\\"version\\\":3,\\\"name\\\":\\\"ipython\\\"},\\\"nbconvert_exporter\\\":\\\"python\\\",\\\"file_extension\\\":\\\".py\\\"}},\\\"cells\\\":[{\\\"metadata\\\":{\\\"_uuid\\\":\\\"4c26dad8c106fb2eacc6d703993b524774467445\\\",\\\"_cell_guid\\\":\\\"258f18fc-e0b2-4a55-8927-51c46077017b\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Introduction\\\\n====\\\\n**Natural Language Processing ** (NLP) is the task of making computers understand and produce human languages. \\\\n\\\\nAnd it always starts with the **corpus** i.e. *a body of text*. \\\\n\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"93f8697704b4f7c9a900bd26c341862d1ef82f05\\\",\\\"_cell_guid\\\":\\\"2762e23e-138d-4086-af46-53aa1d7d0bcd\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"\\\\nWhat is a Corpus?\\\\n====\\\\n\\\\nThere are many corpora (*plural of corpus*) available in NLTK, lets start with an English one call the **Brown corpus**.\\\\n\\\\nWhen using a new corpus in NLTK for the first time, downloads the corpus with the `nltk.download()` function, e.g. \\\\n\\\\n```python\\\\nimport nltk\\\\nnltk.download('brown')\\\\n```\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"6a663051c176297596bc3174d10a1f8599247621\\\",\\\"_cell_guid\\\":\\\"bd910c35-dc49-4c07-9441-9d16d175580a\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"After its downloaded, you can import it as such:\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"637fad23e6bdfd6b8188a7c56318c91b6d3227e2\\\",\\\"_cell_guid\\\":\\\"2f1d4c2c-ea22-4494-a650-cfddd3aba49c\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"from nltk.corpus import brown\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"14c6fc8bb6bdffbf6fb397c9d2c5da53d12c1e69\\\",\\\"_cell_guid\\\":\\\"121fd272-19fd-498f-a3e9-5295d09a2e16\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"brown.words() # Returns a list of strings\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"2841227d2eeba7a7629ffa690647c43dc07bea7f\\\",\\\"_cell_guid\\\":\\\"ebf89848-9442-4790-9575-a7f87234eebd\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"len(brown.words()) # No. of words in the corpus\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"2de0781902fe8662d73bda9381cab9e318cdebf8\\\",\\\"_cell_guid\\\":\\\"c3ba66d4-3e26-40fc-94d3-211581fa4c5c\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"brown.sents() # Returns a list of list of strings \\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"a17b7f140a86b0541acf2796b9146955acd64201\\\",\\\"_cell_guid\\\":\\\"68725143-131d-4886-851c-f932c84588aa\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"brown.sents(fileids='ca01') # You can access a specific file with `fileids` argument.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"4e2791d121eb8162f2031df6c126bcaa40d812d7\\\",\\\"_cell_guid\\\":\\\"972700b2-ee0b-4038-ba23-573287d08fd0\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"\\\\n**Fast Facts:**\\\\n\\\\n> The Brown Corpus of Standard American English was the first of the modern, computer readable, general corpora. It was compiled by W.N. Francis and H. Kucera, Brown University, Providence, RI. The corpus consists of one million words of American English texts printed in 1961.\\\\n\\\\n(Source: [University of Essex Corpus Linguistics site](  https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html))\\\\n\\\\n>  This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on ... (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\\\\n\\\\n![](http://)(Source: [NLTK book, Chapter 2.1.3](http://www.nltk.org/book/ch02.html))\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"abcb302ff5d44499870e06f7f86490cf077fea2b\\\",\\\"_cell_guid\\\":\\\"1ecd0a80-5a12-444a-b9da-768ffef5133f\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"The actual `brown` corpus data is **packaged as raw text files**.  And you can find their IDs with: \\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"8c979bdf698b8f7975b1216083168de0317c8ac5\\\",\\\"_cell_guid\\\":\\\"9bb09e9a-574f-4c3d-97eb-2091ce989cab\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"len(brown.fileids()) # 500 sources, each file is a source.\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"30e85f007a8812090f813d8212e43a834be3de29\\\",\\\"_cell_guid\\\":\\\"fca762b7-5403-446f-82ca-e91e4cbd51dd\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"print(brown.fileids()[:100]) # First 100 sources.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"b6572ea60d0142090d77a44fabc98ee29943a4bb\\\",\\\"_cell_guid\\\":\\\"74143a0e-40e2-4450-9ab4-427686f6ac87\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"You can access the raw files with:\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"3f73483a3d332c4be88f54115d48beaa6351e090\\\",\\\"_cell_guid\\\":\\\"9adcf194-d863-4043-85d0-00743c2786c9\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"print(brown.raw('cb01').strip()[:1000]) # First 1000 characters.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"f293a3c986c26e82394bd58a68bf2a0843b40f80\\\",\\\"_cell_guid\\\":\\\"9c58248d-6e32-496f-843b-1a07e3d4afb8\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"<br>\\\\nYou will see that **each word comes with a slash and a label** and unlike normal text, we see that **punctuations are separated from the word that comes before it**, e.g. \\\\n\\\\n> The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\\\\n\\\\n<br>\\\\nAnd we also see that the **each sentence is separated by a newline**:\\\\n\\\\n> There/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\\\\n> \\\\n> The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\\\\n\\\\n<br>\\\\nThat brings us to the next point on **sentence tokenization** and **word tokenization**.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"2275a8deeace1a0080b4194d70a0e6f751026e2d\\\",\\\"_cell_guid\\\":\\\"50b08633-6665-44e1-a989-e8b5fd3f11df\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Tokenization\\\\n====\\\\n\\\\n**Sentence tokenization** is the process of  *splitting up strings into “sentences”*\\\\n\\\\n**Word tokenization** is the process of  *splitting up “sentences” into “words”*\\\\n\\\\nLets play around with some interesting texts,  the `singles.txt` from `webtext` corpus. <br>\\\\nThey were some  **singles ads** from  http://search.classifieds.news.com.au/\\\\n\\\\nFirst, downoad the data with `nltk.download()`:\\\\n\\\\n```python\\\\nnltk.download('webtext')\\\\n```\\\\n\\\\nThen you can import with:\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"0a0c9f31e40f7ef9f3172aad7b68dbe3d4cc1829\\\",\\\"_cell_guid\\\":\\\"3636768b-77ad-4f0a-922d-1cedd2a8b11a\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"from nltk.corpus import webtext\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"f899b6ed306c96f7731f1acc51aaba4256ec59b7\\\",\\\"_cell_guid\\\":\\\"6f9fd322-e916-4b70-83c3-87078c66fb15\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"webtext.fileids()\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"89a84224c596cf23b3498a7d5f3dbbf74428ef9e\\\",\\\"_cell_guid\\\":\\\"7518ac61-756f-466b-8294-ca6056994415\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"# Each line is one advertisement.\\\\nfor i, line in enumerate(webtext.raw('singles.txt').split('\\\\\\\\n')):\\\\n    if i > 10: # Lets take a look at the first 10 ads.\\\\n        break\\\\n    print(str(i) + ':\\\\\\\\t' + line)\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"c1086676ea2bd10932715c1dfc5ebca5f7765389\\\",\\\"_cell_guid\\\":\\\"683d51eb-41e1-41bf-9bd2-3314d435f330\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Lets zoom in on candidate no. 8\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"3f117b6fa1c9dc0150800cdbd632f4cee1c3fbde\\\",\\\"_cell_guid\\\":\\\"7b46c556-583a-4af2-95d4-93d4d4b55bd2\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"single_no8 = webtext.raw('singles.txt').split('\\\\\\\\n')[8]\\\\nprint(single_no8)\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"5d8daeb0c88fa79b07730d3d95d4f66382ebb0ef\\\",\\\"_cell_guid\\\":\\\"f227e306-1fc8-4d0a-bebc-55014599b588\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Sentence Tokenization\\\\n<br>\\\\nIn NLTK, `sent_tokenize()` the default tokenizer function that you can use to split strings into \\\\\\\"*sentences*\\\\\\\". \\\\n<br>\\\\n\\\\nIt is using the [**Punkt algortihm** from Kiss and Strunk (2006)](http://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485).\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"a5430c319a9f9cc66f074405d24271fec49e77c5\\\",\\\"_cell_guid\\\":\\\"c49b219d-864f-4353-9535-648592f0d847\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"from nltk import sent_tokenize, word_tokenize\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"1d1a02d8bb0892942a4a5059a46b7f8aa23b7e01\\\",\\\"_cell_guid\\\":\\\"0666bda0-ebec-4f2b-ab92-2158b70e38a2\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"sent_tokenize(single_no8)\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"f1198990be58fd7d81cb6516651f98a2716824c3\\\",\\\"_cell_guid\\\":\\\"17ab6433-7166-4e87-837a-74cde568f98f\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"for sent in sent_tokenize(single_no8):\\\\n    print(word_tokenize(sent))\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"0ee17d9e697179e1db2bd8ef95cae55f3b5d64b3\\\",\\\"_cell_guid\\\":\\\"e11cdead-66ae-4e25-9e68-9130297e0758\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Lowercasing\\\\n\\\\nThe CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\\\\n\\\\nWe can simply **lowercase them after we do `sent_tokenize()` and `word_tokenize()`**. <br>\\\\nThe tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal.\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"197cae1396cc555d02eb64676471a564e1e7f35a\\\",\\\"_cell_guid\\\":\\\"665cff0c-7140-444a-8ea9-ce3e20299464\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"sent_tokenize(single_no8)\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"9f54f59d10281ff66ec36648cff7eda1f13f5ba2\\\",\\\"_cell_guid\\\":\\\"42c2c2d9-cb6b-41d4-ac89-46440b13e94c\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"for\\n...(truncated)...\\nvalidation` in `scikit-learn`. \\\\n \\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"ac1e8766dcf5d7d81fe4fd3a95e7bc659fbf1978\\\",\\\"_cell_guid\\\":\\\"0b61f317-079b-4253-b94a-a87b71bd47fb\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"from sklearn.feature_extraction.text import CountVectorizer\\\\nfrom sklearn.model_selection import train_test_split \\\\n\\\\n# It doesn't really matter what the function name is called\\\\n# but the `train_test_split` is splitting up the data into \\\\n# 2 parts according to the `test_size` argument you've set.\\\\n\\\\n# When we're splitting up the training data, we're spltting up \\\\n# into train, valid split. The function name is just a name =)\\\\ntrain, valid = train_test_split(df_train, test_size=0.2)\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"f97576104bbabfc720fa0794ab6a8fc4e043f109\\\",\\\"_cell_guid\\\":\\\"8677912b-d9e2-4fd8-b722-acb3320af4fc\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Vectorize the train and validation set\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"4549079bfedbbff679ccde5f083b3c6814e73233\\\",\\\"_cell_guid\\\":\\\"02efe9a8-00e4-41fa-9b6a-5fbebe751feb\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"# Initialize the vectorizer and \\\\n# override the analyzer totally with the preprocess_text().\\\\n# Note: the vectorizer is just an 'empty' object now.\\\\ncount_vect = CountVectorizer(analyzer=preprocess_text)\\\\n\\\\n# When we use `CounterVectorizer.fit_transform`,\\\\n# we essentially create the dictionary and \\\\n# vectorize our input text at the same time.\\\\ntrain_set = count_vect.fit_transform(train['request_text_edit_aware'])\\\\ntrain_tags = train['requester_received_pizza']\\\\n\\\\n# When vectorizing the validation data, we use `CountVectorizer.transform()`.\\\\nvalid_set = count_vect.transform(valid['request_text_edit_aware'])\\\\nvalid_tags = valid['requester_received_pizza']\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"eb879f0ea99c654625d7e4526de4c1f5602619ac\\\",\\\"_cell_guid\\\":\\\"85863807-de62-471c-a5f9-7ada4e54e0bf\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Now, we need to vectorize the test data too\\\\n\\\\nAfter we vectorize our data, the input to train the classifier would be the vectorized text. \\\\n<br>When we predict the label with the trained mdoel, our input needs to be vectorized too.\\\\n\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"502788a6150bc9c7256b097a4d5c04b4822ac3eb\\\",\\\"_cell_guid\\\":\\\"766b1529-5967-45e9-9736-1fb69bdc6460\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"# When vectorizing the test data, we use `CountVectorizer.transform()`.\\\\ntest_set = count_vect.transform(df_test['request_text_edit_aware'])\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"3aaf138395ccdfe7a29d89d1b58da9db96ae8b36\\\",\\\"_cell_guid\\\":\\\"9d7ae411-ae2c-4098-beb2-ac6c2dc267a9\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Naive Bayes classifier in sklearn\\\\n\\\\nThere are different variants of Naive Bayes (NB) classifier in `sklearn`. <br>\\\\nFor simplicity, lets just use the `MultinomialNB`.\\\\n\\\\n**Multinomial** is a big word but it just means many classes/categories/bins/boxes that needs to be classified. \\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"44cee633360c7989a3d67716553ef4e2bc9eae87\\\",\\\"_cell_guid\\\":\\\"8a159538-c58a-485d-bc23-d7115ee55a77\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"from sklearn.naive_bayes import MultinomialNB\\\\nclf = MultinomialNB() \\\\n\\\\n# To train the classifier, simple do \\\\nclf.fit(train_set, train_tags) \\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"fbec702a6003287ea5d49049be532ee96b6d7a14\\\",\\\"_cell_guid\\\":\\\"8c2776b5-6515-4321-a7e7-f925534241d6\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Before we test our classifier on the test set, we get a sense of how good it is on the validation set.\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"b5f7b1ac26a30b568d3f5dec2a3f76c34a316ee5\\\",\\\"_cell_guid\\\":\\\"a0bba339-cd62-46e9-9106-55ef15000dc5\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"from sklearn.metrics import accuracy_score\\\\n\\\\n# To predict our tags (i.e. whether requesters get their pizza), \\\\n# we feed the vectorized `test_set` to .predict()\\\\npredictions_valid = clf.predict(valid_set)\\\\n\\\\nprint('Pizza reception accuracy = {}'.format(\\\\n        accuracy_score(predictions_valid, valid_tags) * 100)\\\\n     )\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"293f22791ce16ad0071afa625f7b6efa0601da7d\\\",\\\"_cell_guid\\\":\\\"24fed6a6-9b91-4192-92e1-1c6ac3264dab\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Now lets use the full training data set and re-vectorize and retrain the classifier\\\\n\\\\nMore data == better model (in most cases)\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"4a0c2bbe6b98000d71e1b2f9cd00bb01b156522e\\\",\\\"_cell_guid\\\":\\\"f738aa5b-8e1b-4885-8f3e-6d279f23d082\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"count_vect = CountVectorizer(analyzer=preprocess_text)\\\\n\\\\nfull_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\\\\nfull_tags = df_train['requester_received_pizza']\\\\n\\\\n# Note: We have to re-vectorize the test set since\\\\n#       now our vectorizer is different using the full \\\\n#       training set.\\\\ntest_set = count_vect.transform(df_test['request_text_edit_aware'])\\\\n\\\\n# To train the classifier\\\\nclf = MultinomialNB() \\\\nclf.fit(full_train_set, full_tags) \\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"dbf9eb2a3dde29a26ae51df05f120ee26334d0c3\\\",\\\"_cell_guid\\\":\\\"6013d400-4a01-438a-9afe-b540b6ec78b9\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Finally, we use the classifier to predict on the test set\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"450827a713f7eaaa2d3c3504b038a513c27b3b20\\\",\\\"_cell_guid\\\":\\\"2fe3ff2b-5157-43b2-ad8d-55389ad28a0e\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"# To predict our tags (i.e. whether requesters get their pizza), \\\\n# we feed the vectorized `test_set` to .predict()\\\\npredictions = clf.predict(test_set)\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"51a804beaf030dbb8cbdfc320d6a26d86991d274\\\",\\\"_cell_guid\\\":\\\"ef6b8e68-46be-4944-b809-872ddeabf386\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Note:** Since we don't have the `requester_received_pizza` field in test data, we can't measure accuracy. But we can do some exploration as shown below.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"e948c2b162d178cd674689adc1ce9dcdb426cbb5\\\",\\\"_cell_guid\\\":\\\"1ac9a825-8ffa-4ebd-a223-73521a27bc10\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# From the training data, we had 24% pizza giving rate\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"4ad28c27f351de96d40b68aed92217b17f71c542\\\",\\\"_cell_guid\\\":\\\"80300555-265f-4557-8b52-17c343a34c58\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\\\\nprint(str('Of {} requests, only {} gets their pizzas,'\\\\n          ' {}% success rate...'.format(len(df_train), \\\\n                                        sum(df_train['requester_received_pizza']), \\\\n                                       success_rate)\\\\n         )\\\\n     )\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"9058b0225d2f7e87486ffb5e31f710c55431a39c\\\",\\\"_cell_guid\\\":\\\"3301d9e0-544f-41d5-9c32-b2ea95b37871\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Lolz, our classifier is rather stingy...\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"b6b08ebc7396962d4268858d27e7f1f082943891\\\",\\\"_cell_guid\\\":\\\"111c7d74-3516-455d-a13d-8a0af3d1b3c6\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"success_rate = sum(predictions) / len(predictions) * 100\\\\nprint(str('Of {} requests, only {} gets their pizzas,'\\\\n          ' {}% success rate...'.format(len(predictions), \\\\n                                        sum(predictions), \\\\n                                       success_rate)\\\\n         )\\\\n     )\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"8fbd1af6bdd2f1c3e1ff0cf65976fa3252e171cc\\\",\\\"_cell_guid\\\":\\\"8ed85498-5dd2-48bf-a379-1b84d237a42b\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# How accurate is our count vectorization naive bayes classifier on the test data?\\\\n\\\\nSince we don't have the `requester_received_pizza` field in the test data, we have to check that with an oracle (i.e. the person that knows). \\\\n\\\\nOn Kaggle, **checking with the oracle** means uploading the file in the correct format and their script will process the scores and tell you how you did.\\\\n\\\\n**Note:** Different tasks will use different metrics but in most cases getting as many correct predictions as possible is the thing to aim for. We won't get into the details of how classifiers are evaluated but for a start, please see [precision, recall and F1-scores](https://en.wikipedia.org/wiki/Precision_and_recall) \\\\n\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"9129b8f430bf2c652038a5a7316c0e9db1bb4f4a\\\",\\\"_cell_guid\\\":\\\"f8054679-9d04-4563-91d5-daa29d0f7ce6\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Finally, lets take a look at what format the oracle expects and create the output file for our predictions accordingly\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"66603c8033bf346a9cd418d2679c695e1973a820\\\",\\\"_cell_guid\\\":\\\"a683db0e-da0f-48b0-915c-c3bc288519f9\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"df_sample_submission = pd.read_csv('../input/patching-pizzas/sampleSubmission.csv')\\\\ndf_sample_submission.head()\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"1410d9b66337fc2924eb156277f059e116143919\\\",\\\"_cell_guid\\\":\\\"f28c1ce2-864d-4de3-926e-4dc317479216\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"# We've kept the `request_id` previous in the `df_test` dataframe.\\\\n# We can simply merge that column with our predictions.\\\\ndf_output = pd.DataFrame({'request_id': list(df_test['request_id']), \\\\n                          'requester_received_pizza': list(predictions)}\\\\n                        )\\\\n# Convert the predictions from boolean to integer.\\\\ndf_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)\\\\ndf_output.head()\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"e64b0d64cbea2a60b878ca0e445b27c722e368db\\\",\\\"_cell_guid\\\":\\\"4053487b-e0d6-48a3-ba95-dbe2465f78f7\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"# Create the csv file.\\\\ndf_output.to_csv('basic-nlp-submission.csv')\\\"},{\\\"outputs\\\":[],\\\"metadata\\\":{\\\"_uuid\\\":\\\"dbbd128b7754fa488986036a55ce09ecbe52e167\\\",\\\"_cell_guid\\\":\\\"a97cb294-eb6c-479e-944a-b82e63dc008f\\\",\\\"collapsed\\\":true},\\\"cell_type\\\":\\\"code\\\",\\\"execution_count\\\":null,\\\"source\\\":\\\"\\\"}],\\\"nbformat\\\":4,\\\"nbformat_minor\\\":1}\", \"is_error\": false}",
        "tool.duration_ms": 0.9012222290039062
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "447825236955a563",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "FetchKernelTool",
      "kind": "INTERNAL",
      "start_time": 1768005421726144284,
      "end_time": 1768005422198652646,
      "duration": 0.472508362,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "FetchKernelTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.kaggle_tools.FetchKernelTool object at 0x7ef34db59710>\", \"input\": {\"ref\": \"jatinraina/random-acts-of-pizza-xgboost\"}}",
        "tool.output": "{\"content\": \"Kernel downloaded to: research/kernels/jatinraina_random-acts-of-pizza-xgboost/\\nFiles:\\n  - kernel-metadata.json\\n  - random-acts-of-pizza-xgboost.irnb\", \"is_error\": false}",
        "tool.duration_ms": 472.54204750061035
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "5ead4b394379850f",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1768005430391258805,
      "end_time": 1768005430391703022,
      "duration": 0.000444217,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7ef374cf44d0>\", \"input\": {\"file_path\": \"../research/kernels/jatinraina_random-acts-of-pizza-xgboost/random-acts-of-pizza-xgboost.irnb\"}}",
        "tool.output": "{\"content\": \"{\\\"nbformat\\\":4,\\\"cells\\\":[{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"# Loading libraries\\\\nlibrary(rjson)\\\\nlibrary(jsonlite)\\\\nlibrary(readr)\\\\nlibrary(ggplot2) \\\\nlibrary(readr) \\\\nlibrary(data.table)\\\\nlibrary(dplyr)\\\\n\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"9acfc152-1e0b-4c49-b709-c9699a0bf51b\\\",\\\"_uuid\\\":\\\"479444f1a7aea3b16fc237a0754d32a3c9662c8f\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"## reading train and test files\\\\ntrain = fromJSON(\\\\\\\"../input/train.json\\\\\\\")\\\\ntest = fromJSON(\\\\\\\"../input/test.json\\\\\\\")\\\\n\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"344dbe06-77e3-421e-ad2a-e9f7e00f90fc\\\",\\\"_uuid\\\":\\\"2d3a3f6f00b412fb40beed4b5683a4e9219da73e\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"# Data exploration \\\\nnames(train)\\\\nnames(test)\\\\nglimpse(train)\\\\nglimpse(test)\\\\n# train dataset having a lot of columns and test having few of them\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"3984a06b-6501-4bc9-b312-b267e7fd4529\\\",\\\"_uuid\\\":\\\"46de6192dd07320b6455fb2d92ffe7827e2c1cfd\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"##univariate and multivariate analysis \\\\nlibrary(ggplot2)\\\\nggplot(data = train, mapping = aes(y =requester_received_pizza, x = requester_number_of_posts_on_raop_at_retrieval))+\\\\n   geom_jitter()\\\\n\\\\n## majority of people getting rejected and accepted in getting pizza have starting values in \\\\n# number of posts at retreival\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"e66de937-7515-4eab-8112-8f2743286d7c\\\",\\\"_uuid\\\":\\\"d392afa01b61bc57f382dba3a660fe45f56172c2\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"ggplot(data = train, mapping = aes(y =requester_received_pizza, x =requester_number_of_comments_at_request))+\\\\n   geom_jitter()\\\\n\\\\nggplot(data = train, mapping = aes(y =requester_received_pizza, x =requester_number_of_comments_at_retrieval))+\\\\n   geom_jitter()\\\\n\\\\n## frequency high during starting and ending\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"d5b615f9-be84-4f46-884b-6824fa8ca8a0\\\",\\\"_uuid\\\":\\\"af892921458d2bdda1ecf0cb03f85447ad2b85d6\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"ggplot(data = train, mapping = aes( y =requester_upvotes_minus_downvotes_at_request , x = requester_upvotes_plus_downvotes_at_request))+\\\\n  geom_point(mapping = aes(color = requester_received_pizza))\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"ae45c4df-4663-415d-8f37-290111258430\\\",\\\"_uuid\\\":\\\"81a535b9fd5c290c350872f4a039ef76d92b990c\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"ggplot(data = train, mapping = aes( y =requester_upvotes_minus_downvotes_at_retrieval , x = requester_upvotes_plus_downvotes_at_retrieval))+\\\\n  geom_point(mapping = aes(color = requester_received_pizza))\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"dd922d6d-cac2-49ec-aa55-2f030acf62aa\\\",\\\"_uuid\\\":\\\"b659981753ac4f0599deb6d72b59d3ebb3137052\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"## wordclouds\\\\nlibrary(tm)\\\\nlibrary(methods)\\\\nlibrary(RColorBrewer)\\\\nlibrary(wordcloud)\\\\n##\\\\nmake_word_cloud <- function(documents) {\\\\n  corpus = Corpus(VectorSource(tolower(documents)))\\\\n  corpus = tm_map(corpus, removePunctuation)\\\\n  corpus = tm_map(corpus, removeWords, stopwords(\\\\\\\"english\\\\\\\"))\\\\n  \\\\n  frequencies = DocumentTermMatrix(corpus)\\\\n  word_frequencies = as.data.frame(as.matrix(frequencies))\\\\n  \\\\n  words <- colnames(word_frequencies)\\\\n  freq <- colSums(word_frequencies)\\\\n  wordcloud(words, freq,\\\\n            min.freq=sort(freq, decreasing=TRUE)[[200]],\\\\n            colors=brewer.pal(8, \\\\\\\"Dark2\\\\\\\"),\\\\n            random.color=TRUE)  \\\\n}\\\\n\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"f3e19bbf-1e3f-46a1-8a79-d3ed2cfcb5be\\\",\\\"_uuid\\\":\\\"e861bfe1306f43b5eb6dd368f94f313a4801b423\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"## 200 most common words\\\\nmake_word_cloud(train$request_title)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"1b227b67-2ee0-49a8-b1c5-7ed3f72a4c7f\\\",\\\"_uuid\\\":\\\"422d5bab7f0039b45626405240065c4c6be6febc\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"make_word_cloud(train$request_text)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"ac656144-f9c4-4989-a0dd-259545e4fe42\\\",\\\"_uuid\\\":\\\"e3a2e90656ff94f0f17b7883bd424f63eadc8df1\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"make_word_cloud(train$request_text_edit_aware)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"ea47fa3b-566c-4726-812c-ca6878880213\\\",\\\"_uuid\\\":\\\"20665a8e01391edf32df2ee59f2eb2f1994e8f72\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"make_word_cloud(train$requester_subreddits_at_request)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"982fd3b9-774b-44c3-bd2f-feb380e1f545\\\",\\\"_uuid\\\":\\\"63033f2f5a6b7171ae27a58f6fd7c0d4a576b021\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"## dt more\\\\ntext = as.data.frame(train[,c(7,8,9)])\\\\ncorpus_2 = Corpus(VectorSource(tolower(text)))\\\\ncorpus_2 = tm_map(corpus_2,removePunctuation)\\\\ncorpus_2 = tm_map(corpus_2,removeWords,stopwords(\\\\\\\"english\\\\\\\"))\\\\ncorpus_2 = tm_map(corpus_2,stripWhitespace)\\\\ncorpus_2 = tm_map(corpus_2,removeNumbers)\\\\ndtm = DocumentTermMatrix(corpus_2)\\\\ndim(dtm)\\\\ndtm\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n# removing  text variables and variables absent from test data for  applying xgboost\\\\ntrain = train[ ,c(10,12,14,16,18,20,22,23,25,27,31,32)]\\\\ntest = test[,-c(1,3,4,12,15)]\\\\n# adding target variable to test data\\\\ntest$requester_received_pizza = 0\\\\nclass(train$requester_received_pizza)\\\\n# converting class of target variable from logical to numeric\\\\ntrain$requester_received_pizza = as.factor (as.numeric(train$requester_received_pizza))\\\\nclass(train$requester_received_pizza)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"99ccfa7b-caa8-4403-b751-8691aef0cc45\\\",\\\"_uuid\\\":\\\"68e827ee08deaec30342f82ea7068c271e20ff40\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"## applying xgboost\\\\nlibrary(xgboost)\\\\nlibrary(Matrix)\\\\nsparse_matrix <-sparse.model.matrix((requester_received_pizza)~.-1, data =train)\\\\nunique(train$requester_received_pizza)\\\\n## as factor converts values into 1 and 0 so taking 1 as yes\\\\noutput_vector = train[,8] == \\\\\\\"1\\\\\\\"\\\\n# aplying xgboost keeping eval crieria to be auc and using random rounds\\\\nbst3 <- xgboost(data = sparse_matrix, label = output_vector, max.depth = 4,\\\\n               eta = 1, nthread = 2, nround = 10,objective = \\\\\\\"binary:logistic\\\\\\\", \\\\n               eval_metric =\\\\\\\"auc\\\\\\\")\\\\n\\\\n\\\\n\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"c394b84e-d1a2-43e7-99d1-4c83a0be0fc8\\\",\\\"_uuid\\\":\\\"97d6fb7575d09d6dba200bf3a8afaba9d219e470\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"importance <- xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = bst3)\\\\n#  gain is most important as it tells the the improvement in accuracy brought by features added to it\\\\n# details here -http://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html\\\\nhead(importance)\\\\n\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"25aed839-1814-4087-afac-af37cddb3fea\\\",\\\"_uuid\\\":\\\"590b9178047f79c8f351d6d497728466f941dcb2\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"\\\\n# plotting importance \\\\nxgb.plot.importance(importance_matrix = importance)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"f1c6d747-ba4a-4b05-b179-566bfa9e65fe\\\",\\\"_uuid\\\":\\\"c3e9921d1c34f9a1743cf52f5311a82446bfe3f8\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"#  giving predictions \\\\npre = predict(bst3, as.matrix(test[,-1]))\\\\nsolution3 = data.frame(request_id = test$request_id , requester_received_pizza= pre)\\\\nwrite.csv(solution3, file = \\\\\\\"xgpizza3\\\\\\\", row.names = F)\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"57a23889-7de1-44d1-8945-b3f24f2a2721\\\",\\\"_uuid\\\":\\\"e9e7e9d99313900a96f7a63d9a0ce36210297525\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"# Improving accuracy \\\\n# using cross validation feature to improve accuracy taking best rounds\\\\nbst <- xgb.cv(data = sparse_matrix, label = output_vector, max.depth = 4,\\\\neta = 1, nthread = 2, nfold = 10,nround = 200,objective = \\\\\\\"binary:logistic\\\\\\\",\\\\nprint_every_n = 10,early_stopping_rounds = 10,maximize = F, eval_metric  = \\\\\\\"auc\\\\\\\")\\\\n##\\\\n#running  cross validation multiple times gives you best rounds which you can further use to get better accuracy\\\\n# Here it does not give better accuracy \\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"22a8766e-ee04-4612-bdaf-f371f1034941\\\",\\\"_uuid\\\":\\\"f22520470421fd599aa5145520f480c42f38d8fe\\\"},\\\"cell_type\\\":\\\"code\\\"},{\\\"outputs\\\":[],\\\"execution_count\\\":null,\\\"source\\\":\\\"# Trying another algorithm random forest\\\\nlibrary(randomForest)\\\\npizza_rf = randomForest((requester_received_pizza)~., train, importance = TRUE, ntree = 500)\\\\npredrf = predict(pizza_rf, test)\\\\nsolutionrf = data.frame(request_id = test$request_id , requester_received_pizza= predrf)\\\\nwrite.csv(solutionrf, file = \\\\\\\"rfpizza\\\\\\\", row.names = F)\\\\n\\\\n#  xgboost still gives  better accuracy  than random forest\\\",\\\"metadata\\\":{\\\"_cell_guid\\\":\\\"d8f0d485-4579-48f9-baa7-b2bf418c8a1a\\\",\\\"_uuid\\\":\\\"b3e3e5e17755c5cefabd4dfd36e3d10f5bbc4206\\\"},\\\"cell_type\\\":\\\"code\\\"}],\\\"nbformat_minor\\\":1,\\\"metadata\\\":{\\\"kernelspec\\\":{\\\"display_name\\\":\\\"R\\\",\\\"language\\\":\\\"R\\\",\\\"name\\\":\\\"ir\\\"},\\\"language_info\\\":{\\\"mimetype\\\":\\\"text/x-r-source\\\",\\\"pygments_lexer\\\":\\\"r\\\",\\\"name\\\":\\\"R\\\",\\\"file_extension\\\":\\\".r\\\",\\\"codemirror_mode\\\":\\\"r\\\",\\\"version\\\":\\\"3.4.2\\\"}}}\", \"is_error\": false}",
        "tool.duration_ms": 0.43320655822753906
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "59af9e1537e35e9e",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1768005489520658918,
      "end_time": 1768005491095605041,
      "duration": 1.574946123,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7ef34db58c10>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\nwith open('/home/data/train.json', 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 1575.0043392181396
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "62e629195bd28bdf",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "OpenNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1768005499430914881,
      "end_time": 1768005499431468869,
      "duration": 0.000553988,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "OpenNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.OpenNotebookTool object at 0x7ef34db5a410>\", \"input\": {\"notebook_path\": \"eda.ipynb\"}}",
        "tool.output": "{\"content\": \"Already open: /home/code/exploration/eda.ipynb (1 cells)\", \"is_error\": false}",
        "tool.duration_ms": 0.5962848663330078
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "9634bfb0bf3ad721",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768005508210982301,
      "end_time": 1768005508872322363,
      "duration": 0.661340062,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7ef34db5a150>\", \"input\": {\"start_cell\": 0, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\nwith open('/home/data/train.json', 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\n```\\nOutput:\\nNumber of training samples: 4040\\nFirst sample keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\", \"is_error\": false}",
        "tool.duration_ms": 661.3674163818359
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "5dd042b04bd01b7c",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768005566397085678,
      "end_time": 1768005566400084170,
      "duration": 0.002998492,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7ef34db5a1d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Convert to DataFrame for easier analysis\\ndf_train = pd.DataFrame(train_data)\\n\\n# Target distribution\\nprint(\\\"Target distribution:\\\")\\nprint(df_train['requester_received_pizza'].value_counts())\\nprint(f\\\"\\\\nSuccess rate: {df_train['requester_received_pizza'].mean():.3f}\\\")\\n\\n# Basic statistics on key features\\nprint(\\\"\\\\nKey numerical features:\\\")\\nnumeric_cols = ['requester_number_of_comments_at_request', 'requester_number_of_posts_at_request', \\n                'requester_upvotes_plus_downvotes_at_request', 'requester_number_of_subreddits_at_request']\\nprint(df_train[numeric_cols].describe())\"}}",
        "tool.output": "{\"content\": \"Added code cell at 1\", \"is_error\": false}",
        "tool.duration_ms": 3.056049346923828
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "9944c7382b1c5c9b",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768005581014287950,
      "end_time": 1768005581126207280,
      "duration": 0.11191933,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7ef34db5a150>\", \"input\": {\"start_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Convert to DataFrame for easier analysis\\ndf_train = pd.DataFrame(train_data)\\n\\n# Target distribution\\nprint(\\\"Target distribution:\\\")\\nprint(df_train['requester_received_pizza'].value_counts())\\nprint(f\\\"\\\\nSuccess rate: {df_train['requester_received_pizza'].mean():.3f}\\\")\\n\\n# Basic statistics on key features\\nprint(\\\"\\\\nKey numerical features:\\\")\\nnumeric_cols = ['requester_number_of_comments_at_request', 'requester_number_of_posts_at_request', \\n                'requester_upvotes_plus_downvotes_at_request',\\n```\\nOutput:\\nTarget distribution:\\nrequester_received_pizza\\nFalse    3046\\nTrue      994\\nName: count, dtype: int64\\n\\nSuccess rate: 0.246\\n\\nKey numerical features:\\n       requester_number_of_comments_at_request  \\\\\\ncount                              4040.000000   \\nmean                                115.098267   \\nstd                                 193.318968   \\nmin                                   0.000000   \\n25%                                   0.000000   \\n50%                                  24.000000   \\n75%                                 140.250000   \\nmax                                 994.000000   \\n\\n       requester_number_of_posts_at_request  \\\\\\ncount                           4040.000000   \\nmean                              21.601485   \\nstd                               50.895060   \\nmin                                0.000000   \\n25%                                0.000000   \\n50%                                5.000000   \\n75%                               22.000000   \\nmax                              867.000000   \\n\\n       requester_upvotes_plus_downvotes_at_request  \\\\\\ncount                                 4.040000e+03   \\nmean                                  3.743236e+03   \\nstd                                   2.583816e+04   \\nmin                                   0.000000e+00   \\n25%                                   9.000000e+00   \\n50%                                   3.510000e+02   \\n75%                                   2.303750e+03   \\nmax                                   1.286864e+06   \\n\\n       requester_number_of_subreddits_at_request  \\ncount                                4040.000000  \\nmean                                   18.076733  \\nstd                                    21.736465  \\nmin                                     0.000000  \\n25%                                     1.000000  \\n50%                                    11.000000  \\n75%                                    27.000000  \\nmax                                   186.000000  \\n\", \"is_error\": false}",
        "tool.duration_ms": 111.98544502258301
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "ec4aa5b165cff8c7",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768005615188537496,
      "end_time": 1768005615190966662,
      "duration": 0.002429166,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7ef34db5a1d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Text features analysis\\nprint(\\\"Text features:\\\")\\nprint(f\\\"Request title length (chars): {df_train['request_title'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text length (chars): {df_train['request_text'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text edit aware length (chars): {df_train['request_text_edit_aware'].str.len().describe()}\\\")\\n\\n# Check for missing values\\nprint(\\\"\\\\nMissing values:\\\")\\nprint(df_train.isnull().sum().sum())  # Should be 0 for JSON data\\n\\n# Check unique values in categorical features\\nprint(f\\\"\\\\nUnique user flair values: {df_train['requester_user_flair'].unique()}\\\")\\nprint(f\\\"Post edited values: {df_train['post_was_edited'].unique()}\\\")\\n\\n# Check if giver username is useful\\nprint(f\\\"\\\\nGiver username known (not N/A): {(df_train['giver_username_if_known'] != 'N/A').sum()}\\\")\\nprint(f\\\"Total samples: {len(df_train)}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 2.513885498046875
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "726d048e19b45db5",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768005626523084345,
      "end_time": 1768005626559890322,
      "duration": 0.036805977,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7ef34db5a150>\", \"input\": {\"start_cell\": 2, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Text features analysis\\nprint(\\\"Text features:\\\")\\nprint(f\\\"Request title length (chars): {df_train['request_title'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text length (chars): {df_train['request_text'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text edit aware length (chars): {df_train['request_text_edit_aware'].str.len().describe()}\\\")\\n\\n# Check for missing values\\nprint(\\\"\\\\nMissing values:\\\")\\nprint(df_train.isnull().sum().sum())  # Should be 0 for JSON data\\n\\n# Check unique values in categorical fea\\n```\\nOutput:\\nText features:\\nRequest title length (chars): count    4040.000000\\nmean       71.899505\\nstd        36.154216\\nmin         7.000000\\n25%        46.000000\\n50%        65.000000\\n75%        90.000000\\nmax       272.000000\\nName: request_title, dtype: float64\\n\\nRequest text length (chars): count    4040.000000\\nmean      405.243317\\nstd       373.329466\\nmin         0.000000\\n25%       182.000000\\n50%       307.000000\\n75%       510.000000\\nmax      4460.000000\\nName: request_text, dtype: float64\\n\\nRequest text edit aware length (chars): count    4040.000000\\nmean      397.111634\\nstd       362.353943\\nmin         0.000000\\n25%       180.000000\\n50%       301.000000\\n75%       502.250000\\nmax      4460.000000\\nName: request_text_edit_aware, dtype: float64\\n\\nMissing values:\\n3046\\n\\nUnique user flair values: [None 'shroom' 'PIF']\\nPost edited values: [False True 1342028318.0 1378419359.0 1344540616.0 1370925485.0\\n 1370641675.0 1346634254.0 1342808379.0 1366844439.0 1352047954.0\\n 1360772856.0 1367280954.0 1339642100.0 1339088384.0 1376789686.0\\n 1380335546.0 1369768549.0 1339340364.0 1346116495.0 1347390665.0\\n 1356085395.0 1343440111.0 1378096035.0 1368298975.0 1370148542.0\\n 1347997130.0 1372708519.0 1343007767.0 1379372792.0 1366108575.0\\n 1343381450.0 1352239751.0 1354135261.0 1338341728.0 1373584204.0\\n 1344480583.0 1379273122.0 1374882345.0 1356307269.0 1356850776.0\\n 1346565296.0 1376766439.0 1362794855.0 1338447559.0 1352765809.0\\n 1347159426.0 1377307607.0 1374873697.0 1367005014.0 1342752906.0\\n 1370019785.0 1368820535.0 1345908529.0 1355014140.0 1371296113.0\\n 1352673512.0 1340593633.0 1377731994.0 1354767010.0 1355441003.0\\n 1349824194.0 1339344939.0 1350506322.0 1340325995.0 1347475999.0\\n 1376868728.0 1351652384.0 1345249783.0 1358378751.0 1361245329.0\\n 1337276328.0 1375380598.0 1366847814.0 1338754342.0 1375951027.0\\n 1380155455.0 1349037648.0 1339305753.0 1369613625.0 1379624936.0\\n 1368992231.0 1369614497.0 1364156107.0 1369421087.0 1378226261.0\\n 1350590680.0 1342643350.0 1354156475.0 1374523186.0 1342763774.0\\n 1343956907.0 1351621189.0 1379210122.0 1352241057.0 1379705123.0\\n 1350765213.0 1374109637.0 1375391768.0 1354396381.0 1378161845.0\\n 1379817038.0 1365377257.0 1369012305.0 1363315140.0 1339356859.0\\n 1353164052.0 1368678789.0 1367630093.0 1375728827.0 1347223536.0\\n 1346077590.0 1375744673.0 1363299991.0 1351898842.0 1362011234.0\\n 1341092908.0 1358390742.0 1343855983.0 1368628826.0 1347928072.0\\n 1376860266.0 1363714853.0 1366775143.0 1379372126.0 1373814714.0\\n 1349998232.0 1375565742.0 1352163203.0 1342991229.0 1368044388.0\\n 1341186325.0 1369962895.0 1352939964.0 1340484255.0 1367502383.0\\n 1348436735.0 1350068221.0 1354419735.0 1378425306.0 1357675913.0\\n 1345435917.0 1359076340.0 1358101729.0 1347404183.0 1369360852.0\\n 1344209529.0 1337134531.0 1343271807.0 1377287500.0 1343265957.0\\n 1373919611.0 1359829047.0 1368326269.0 1337044175.0 1343933136.0\\n 1367205057.0 1375841082.0 1365368614.0 1364959252.0 1364681524.0\\n 1376507714.0 1340225734.0 1366662105.0 1359924457.0 1351022929.0\\n 1378063053.0 1338696150.0 1367520657.0 1368414647.0 1355526880.0\\n 1342896958.0 1365977351.0 1345416005.0 1373252156.0 1367950509.0\\n 1380374432.0 1358627245.0 1348296810.0 1367462667.0 1351709165.0\\n 1347415215.0 1361648059.0 1363631426.0 1352058582.0 1373471793.0\\n 1377927356.0 1355441749.0 1371169367.0 1358453437.0 1376170160.0\\n 1379103572.0 1348508699.0 1355430403.0 1361742904.0 1364413726.0\\n 1346719766.0 1342994660.0 1351796667.0 1379814817.0 1378856561.0\\n 1338664760.0 1342497359.0 1349478371.0 1376086228.0 1347564988.0\\n 1344362444.0 1351883287.0 1360853861.0 1339177784.0 1377902227.0\\n 1375917854.0 1370727069.0 1359665972.0 1367204797.0 1339546781.0\\n 1347225835.0 1372704769.0 1345639471.0 1364153790.0 1341162736.0\\n 1337823889.0 1373852824.0 1341883327.0 1341885343.0 1347230374.0\\n 1379212134.0 1372729287.0 1379764087.0 1370154951.0 1346110379.0\\n 1349293380.0 1366817082.0 1348607138.0 1364351254.0 1363402759.0\\n 1373935175.0 1373589797.0 1352159448.0 1380074517.0 1357014241.0\\n 1344732948.0 1365916941.0 1346551621.0 1360931874.0 1374370516.0\\n 1369244719.0 1349233336.0 1354542300.0 1345934402.0 1348614817.0\\n 1374459046.0 1358118772.0 1368478213.0 1354989470.0 1341543883.0\\n 1363914082.0 1353193218.0 1337285179.0 1340909908.0 1337657209.0\\n 1342827074.0 1370197898.0 1341791787.0 1348550564.0 1366314331.0\\n 1346554413.0 1352073739.0 1368498872.0 1376524883.0 1339621093.0\\n 1362466721.0 1356034421.0 1367851093.0 1358550745.0 1350440913.0\\n 1343846350.0 1352416785.0 1374010662.0 1375492232.0 1367621025.0\\n 1340921147.0 1362069985.0 1344542429.0 1375324604.0 1356293722.0\\n 1365298996.0 1357182708.0 1372794310.0 1355434056.0 1341756780.0\\n 1360516410.0 1352058568.0 1360285040.0 1380909467.0 1379292448.0\\n 1347224731.0 1378001828.0 1366393571.0 1377877925.0 1369770892.0]\\n\\nGiver username known (not N/A): 287\\nTotal samples: 4040\\n\", \"is_error\": false}",
        "tool.duration_ms": 36.85569763183594
      }
    },
    {
      "trace_id": "d809dbd1a49861e739636fe5ea60d72e",
      "span_id": "0f3265b9a4da0b9c",
      "parent_span_id": "cacb7cca03ae2cfd",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1768005640273882848,
      "end_time": 1768005640275418816,
      "duration": 0.001535968,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x7ef466b4e710>\", \"input\": {\"finding\": \"Target distribution: 24.6% success rate (994/4040 received pizza), indicating significant class imbalance favoring negative class\", \"source\": \"eda.ipynb\"}}",
        "tool.output": "{\"content\": \"✓ Recorded finding: Target distribution: 24.6% success rate (994/4040 received pizza), indicating significant class imba...\\n  Source: eda.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 1.6124248504638672
      }
    }
  ],
  "metadata": {
    "span_count": 17,
    "last_updated": "2026-01-10T00:40:43.610538"
  }
}