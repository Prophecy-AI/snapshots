## Problem Type
Binary text classification with tabular meta-features. Goal: Predict pizza request success based on request text and user metadata.

## Data Characteristics (see eda.ipynb for details)
- 2,878 training samples with class imbalance (24.8% positive rate)
- Text features: request_title, request_text (edit-aware version available)
- Meta-features: user activity metrics, vote counts, timestamps
- Highly predictive feature: requester_user_flair (75% missing, but 'shroom'/'PIF' = 100% success)

## Models
For text + tabular data classification with ~3K samples:
- **Gradient Boosting (XGBoost/LightGBM)**: Excellent for tabular features, handle mixed data types
- **BERT/RoBERTa**: For text understanding, but may overfit on 3K samples - use with strong regularization
- **Ensemble**: Combine text model (BERT) with tabular model (XGBoost) for best performance
- **Logistic Regression with TF-IDF**: Strong baseline for text, works well with small datasets

## Text Preprocessing
- Use request_text_edit_aware to remove success indicators (EDIT: thanks...)
- TF-IDF or CountVectorizer for traditional models
- BERT tokenization for transformer models
- Consider text length features (title length, text length correlate with success)

## Tabular Feature Engineering
- **Handle missing flair**: Create 'has_flair' indicator + impute missing as 'None'
- **Activity ratios**: comments/posts ratios, RAOP-specific vs general activity
- **Temporal features**: Hour of day, day of week from timestamps
- **Vote ratios**: upvotes/(upvotes+downvotes) for request and user
- **Subreddit diversity**: Number of unique subreddits posted in

## Handling Class Imbalance
- **Stratified K-Fold**: Essential for reliable validation
- **Class weights**: Use scale_pos_weight in XGBoost or class_weight in sklearn
- **SMOTE**: Consider for tabular features (not for text)
- **Evaluation**: Focus on ROC AUC, not accuracy

## Validation Strategy
- **Stratified 5-fold CV**: Maintain class distribution across folds
- **Time-based split**: If using temporal features, ensure no leakage
- **Watch for flair leakage**: 'shroom'/'PIF' are post-hoc indicators of success

## Ensembling
- **Stacking**: Train meta-learner on predictions from diverse models
- **Weighted averaging**: Give higher weight to best-performing models
- **Model diversity**: Combine text-based (BERT) and tabular (XGBoost) models
- **Blending**: Use out-of-fold predictions for robust stacking

## Key Insights from EDA
- request_number_of_comments_at_retrieval most correlated with target (0.29)
- User flair is extremely predictive but mostly missing
- Text length features may be useful
- See eda.ipynb for full feature analysis and distributions