{
  "spans": [
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "7bacf6ccc84988d4",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1768001031488594703,
      "end_time": 1768001031488860282,
      "duration": 0.000265579,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7f0ac46f1990>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.0KB)\\n[FILE] sampleSubmission.csv (13.0KB)\\n[FILE] test.json (2.0MB)\\n[FILE] test.zip (433.1KB)\\n[FILE] train.json (8.4MB)\\n[FILE] train.zip (1.3MB)\", \"is_error\": false}",
        "tool.duration_ms": 0.3705024719238281
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "9d84f485f9e4fcfb",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1768001036118130574,
      "end_time": 1768001036118574286,
      "duration": 0.000443712,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7f0ac46f3f50>\", \"input\": {\"file_path\": \"/home/data/description.md\"}}",
        "tool.output": "{\"content\": \"# Overview\\n\\n## Description\\n\\n[Get started on this competition through Kaggle Scripts](https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza)\\n\\nIn machine learning, it is often said there are [no free lunches](http://en.wikipedia.org/wiki/No_free_lunch_theorem). *How wrong we were.*\\n\\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/) together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\\n\\n\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?\\n\\n![Pizzas](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png)\\n\\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This [data](http://cs.stanford.edu/~althoff/raop-dataset/) was collected and graciously shared by [Althoff](http://www.timalthoff.com/) et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their [accompanying paper](http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf) and ask that you cite the following reference in any publications that result from your work:\\n\\nTim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. *How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*, Proceedings of ICWSM, 2014.\\n\\n~*Pizza icons designed by Matthew Dera from the [Noun Project](http://thenounproject.com/term/pizza/30194/)*~\\n\\n### Evaluation\\n\\nSubmissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability that a request will get pizza and the observed outcomes.\\n\\n### Submission File\\n\\nFor each request in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:\\n\\n```\\nrequest_id,requester_received_pizza\\nt3_i8iy4,0\\nt3_1mfqi0,0\\nt3_lclka,0\\n...\\n...\\n```\\n\\n### Citation\\n\\nWill Cukierski. (2014). Random Acts of Pizza. Kaggle. https://kaggle.com/competitions/random-acts-of-pizza\\n\\n\\n# Data\\n\\n## Dataset Description\\n\\n[See, fork, and run a random forest benchmark model through Kaggle Scripts](https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark)\\n\\nThis dataset includes 5671 requests collected from the Reddit community [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/) between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\\n\\nEach JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\\n\\n## Data fields\\n\\n\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).\\n\\n\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.\\n\\n\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.\\n\\n\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).\\n\\n\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".\\n\\n\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.\\n\\n\\\"request_text\\\": Full text of the request.\\n\\n\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".\\n\\n\\\"request_title\\\": Title of the request.\\n\\n\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.\\n\\n\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.\\n\\n\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\\n\\n\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.\\n\\n\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\\n\\n\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).\\n\\n\\\"requester_username\\\": Reddit username of requester.\\n\\n\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\\n\\n\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.\", \"is_error\": false}",
        "tool.duration_ms": 0.6260871887207031
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "395bd257682aa0e6",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1768001048758067072,
      "end_time": 1768001049500036026,
      "duration": 0.741968954,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7f0ac46f37d0>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\nprint(f\\\"\\\\nFirst sample preview:\\\")\\nfor key, value in list(train_data[0].items())[:10]:\\n    print(f\\\"  {key}: {value}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 742.0053482055664
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "890db301ebff36c8",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768001053518151203,
      "end_time": 1768001053928164128,
      "duration": 0.410012925,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f0b6b879410>\", \"input\": {\"start_cell\": 0, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\nprint(f\\\"\\\\nFirst sample preview:\\\")\\nfor key, value in list(train_data[0].items())[:10]:\\n    print(f\\\"  {key}: {value}\\\")\\n```\\nOutput:\\nNumber of training samples: 2878\\nFirst sample keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\\nFirst sample preview:\\n  giver_username_if_known: N/A\\n  number_of_downvotes_of_request_at_retrieval: 2\\n  number_of_upvotes_of_request_at_retrieval: 5\\n  post_was_edited: False\\n  request_id: t3_q8ycf\\n  request_number_of_comments_at_retrieval: 0\\n  request_text: I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\n  request_text_edit_aware: I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\n  request_title: [REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\n  requester_account_age_in_days_at_request: 0.0\\n\", \"is_error\": false}",
        "tool.duration_ms": 410.07423400878906
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "50802329ee1c153b",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768001068689737373,
      "end_time": 1768001068691422277,
      "duration": 0.001684904,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f0ac5aaaa10>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze target distribution\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\n\\n# Target distribution\\ntarget_counts = df['requester_received_pizza'].value_counts()\\nprint(\\\"Target Distribution:\\\")\\nprint(target_counts)\\nprint(f\\\"\\\\nClass balance: {target_counts[True] / len(df):.2%} positive class\\\")\\n\\n# Visualize target distribution\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x=target_counts.index, y=target_counts.values)\\nplt.title('Target Distribution: Pizza Request Success')\\nplt.xlabel('Received Pizza')\\nplt.ylabel('Count')\\nplt.xticks([0, 1], ['No', 'Yes'])\\nplt.show()\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values per column:\\\")\\nmissing = df.isnull().sum()\\nprint(missing[missing > 0])\"}}",
        "tool.output": "{\"content\": \"Added code cell at 1\", \"is_error\": false}",
        "tool.duration_ms": 1.7750263214111328
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "a5e92f9c904dc4e0",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768001073326615362,
      "end_time": 1768001074230085268,
      "duration": 0.903469906,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f0b6b879410>\", \"input\": {\"start_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Analyze target distribution\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\n\\n# Target distribution\\ntarget_counts = df['requester_received_pizza'].value_counts()\\nprint(\\\"Target Distribution:\\\")\\nprint(target_counts)\\nprint(f\\\"\\\\nClass balance: {target_counts[True] / len(df):.2%} positive class\\\")\\n\\n# Visualize target distribution\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x=target_counts.index, y=target_counts.values)\\nplt.title\\n```\\nOutput:\\nTarget Distribution:\\nrequester_received_pizza\\nFalse    2163\\nTrue      715\\nName: count, dtype: int64\\n\\nClass balance: 24.84% positive class\\n\\n<Figure size 800x600 with 1 Axes>\\n\\nMissing values per column:\\nrequester_user_flair    2163\\ndtype: int64\\n\", \"is_error\": false}",
        "tool.duration_ms": 903.5468101501465
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "63cdced30d22004d",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768001119838057948,
      "end_time": 1768001119839998745,
      "duration": 0.001940797,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f0ac5aaaa10>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze text features\\nprint(\\\"Text Feature Analysis:\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# Request title characteristics\\ndf['title_length'] = df['request_title'].str.len()\\nprint(f\\\"Request title length - Mean: {df['title_length'].mean():.1f}, Median: {df['title_length'].median():.1f}\\\")\\nprint(f\\\"Request title length - Min: {df['title_length'].min()}, Max: {df['title_length'].max()}\\\")\\n\\n# Request text characteristics\\ndf['text_length'] = df['request_text'].str.len()\\nprint(f\\\"Request text length - Mean: {df['text_length'].mean():.1f}, Median: {df['text_length'].median():.1f}\\\")\\nprint(f\\\"Request text length - Min: {df['text_length'].min()}, Max: {df['text_length'].max()}\\\")\\n\\n# Check for empty texts\\nempty_titles = (df['request_title'].str.len() == 0).sum()\\nempty_texts = (df['request_text'].str.len() == 0).sum()\\nprint(f\\\"\\\\nEmpty titles: {empty_titles}, Empty texts: {empty_texts}\\\")\\n\\n# Sample a few texts\\nprint(\\\"\\\\nSample request title:\\\")\\nprint(df.iloc[0]['request_title'])\\nprint(\\\"\\\\nSample request text (first 200 chars):\\\")\\nprint(df.iloc[0]['request_text'][:200] + \\\"...\\\")\\n\\n# Check edit-aware text differences\\nedit_diff = (df['request_text'] != df['request_text_edit_aware']).sum()\\nprint(f\\\"\\\\nNumber of samples with edited text: {edit_diff}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 1.9919872283935547
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "a344639f7c7da652",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768001123622225364,
      "end_time": 1768001123636404707,
      "duration": 0.014179343,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f0b6b879410>\", \"input\": {\"start_cell\": 2, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Analyze text features\\nprint(\\\"Text Feature Analysis:\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# Request title characteristics\\ndf['title_length'] = df['request_title'].str.len()\\nprint(f\\\"Request title length - Mean: {df['title_length'].mean():.1f}, Median: {df['title_length'].median():.1f}\\\")\\nprint(f\\\"Request title length - Min: {df['title_length'].min()}, Max: {df['title_length'].max()}\\\")\\n\\n# Request text characteristics\\ndf['text_length'] = df['request_text'].str.len()\\nprint(f\\\"Request text length - Mean: {df['text_lengt\\n```\\nOutput:\\nText Feature Analysis:\\n==================================================\\nRequest title length - Mean: 71.6, Median: 64.0\\nRequest title length - Min: 7, Max: 272\\nRequest text length - Mean: 402.5, Median: 308.0\\nRequest text length - Min: 0, Max: 4460\\n\\nEmpty titles: 0, Empty texts: 71\\n\\nSample request title:\\n[REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\n\\nSample request text (first 200 chars):\\nI will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time ...\\n\\nNumber of samples with edited text: 106\\n\", \"is_error\": false}",
        "tool.duration_ms": 14.226198196411133
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "1696bc3eb4620c08",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768001144161480576,
      "end_time": 1768001144163600567,
      "duration": 0.002119991,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f0ac5aaaa10>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze numerical/meta features\\nnumerical_features = [\\n    'number_of_downvotes_of_request_at_retrieval',\\n    'number_of_upvotes_of_request_at_retrieval', \\n    'request_number_of_comments_at_retrieval',\\n    'requester_account_age_in_days_at_request',\\n    'requester_number_of_comments_at_request',\\n    'requester_number_of_posts_at_request',\\n    'requester_upvotes_minus_downvotes_at_request',\\n    'requester_upvotes_plus_downvotes_at_request',\\n    'requester_number_of_subreddits_at_request'\\n]\\n\\nprint(\\\"Numerical Feature Analysis:\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# Basic statistics for numerical features\\nstats = df[numerical_features].describe()\\nprint(stats)\\n\\n# Correlation with target\\ncorrelations = df[numerical_features + ['requester_received_pizza']].corr()['requester_received_pizza'].sort_values(ascending=False)\\nprint(f\\\"\\\\nCorrelations with target:\\\")\\nprint(correlations)\\n\\n# User flair analysis\\nprint(f\\\"\\\\nUser Flair Distribution:\\\")\\nflair_counts = df['requester_user_flair'].value_counts()\\nprint(flair_counts)\\n\\n# Success rate by flair\\nflair_success = df.groupby('requester_user_flair')['requester_received_pizza'].agg(['count', 'sum', 'mean'])\\nflair_success.columns = ['total', 'successful', 'success_rate']\\nprint(f\\\"\\\\nSuccess rate by user flair:\\\")\\nprint(flair_success)\"}}",
        "tool.output": "{\"content\": \"Added code cell at 3\", \"is_error\": false}",
        "tool.duration_ms": 2.178668975830078
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "e0745b4ab69b0a77",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768001149054729807,
      "end_time": 1768001149082847137,
      "duration": 0.02811733,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f0b6b879410>\", \"input\": {\"start_cell\": 3, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 3-3\\n==================================================\\n\\nCell 3:\\n```\\n# Analyze numerical/meta features\\nnumerical_features = [\\n    'number_of_downvotes_of_request_at_retrieval',\\n    'number_of_upvotes_of_request_at_retrieval', \\n    'request_number_of_comments_at_retrieval',\\n    'requester_account_age_in_days_at_request',\\n    'requester_number_of_comments_at_request',\\n    'requester_number_of_posts_at_request',\\n    'requester_upvotes_minus_downvotes_at_request',\\n    'requester_upvotes_plus_downvotes_at_request',\\n    'requester_number_of_subreddits_at_request'\\n]\\n\\npr\\n```\\nOutput:\\nNumerical Feature Analysis:\\n==================================================\\n       number_of_downvotes_of_request_at_retrieval  \\\\\\ncount                                  2878.000000   \\nmean                                      2.428075   \\nstd                                       3.035568   \\nmin                                       0.000000   \\n25%                                       1.000000   \\n50%                                       2.000000   \\n75%                                       3.000000   \\nmax                                      47.000000   \\n\\n       number_of_upvotes_of_request_at_retrieval  \\\\\\ncount                                2878.000000   \\nmean                                    6.090688   \\nstd                                    10.501259   \\nmin                                     0.000000   \\n25%                                     2.000000   \\n50%                                     4.000000   \\n75%                                     7.000000   \\nmax                                   345.000000   \\n\\n       request_number_of_comments_at_retrieval  \\\\\\ncount                              2878.000000   \\nmean                                  2.858582   \\nstd                                   4.783656   \\nmin                                   0.000000   \\n25%                                   0.000000   \\n50%                                   1.000000   \\n75%                                   4.000000   \\nmax                                  61.000000   \\n\\n       requester_account_age_in_days_at_request  \\\\\\ncount                               2878.000000   \\nmean                                 250.682364   \\nstd                                  301.838771   \\nmin                                    0.000000   \\n25%                                    3.038877   \\n50%                                  155.156377   \\n75%                                  383.640090   \\nmax                                 2809.750787   \\n\\n       requester_number_of_comments_at_request  \\\\\\ncount                              2878.000000   \\nmean                                112.311327   \\nstd                                 192.017515   \\nmin                                   0.000000   \\n25%                                   0.000000   \\n50%                                  22.000000   \\n75%                                 132.000000   \\nmax                                 981.000000   \\n\\n       requester_number_of_posts_at_request  \\\\\\ncount                           2878.000000   \\nmean                              21.614663   \\nstd                               51.580719   \\nmin                                0.000000   \\n25%                                0.000000   \\n50%                                4.000000   \\n75%                               21.000000   \\nmax                              867.000000   \\n\\n       requester_upvotes_minus_downvotes_at_request  \\\\\\ncount                                   2878.000000   \\nmean                                    1184.582349   \\nstd                                     4198.255486   \\nmin                                      -67.000000   \\n25%                                        3.000000   \\n50%                                      171.000000   \\n75%                                     1124.500000   \\nmax                                   155010.000000   \\n\\n       requester_upvotes_plus_downvotes_at_request  \\\\\\ncount                                 2.878000e+03   \\nmean                                  3.988570e+03   \\nstd                                   3.012747e+04   \\nmin                                   0.000000e+00   \\n25%                                   8.000000e+00   \\n50%                                   3.355000e+02   \\n75%                                   2.251500e+03   \\nmax                                   1.286864e+06   \\n\\n       requester_number_of_subreddits_at_request  \\ncount                                2878.000000  \\nmean                                   17.857192  \\nstd                                    21.784934  \\nmin                                     0.000000  \\n25%                                     1.000000  \\n50%                                    11.000000  \\n75%                                    26.000000  \\nmax                                   186.000000  \\n\\nCorrelations with target:\\nrequester_received_pizza                        1.000000\\nrequest_number_of_comments_at_retrieval         0.290709\\nnumber_of_upvotes_of_request_at_retrieval       0.090767\\nrequester_number_of_subreddits_at_request       0.047001\\nrequester_account_age_in_days_at_request        0.043374\\nrequester_upvotes_minus_downvotes_at_request    0.043084\\nrequester_number_of_posts_at_request            0.037975\\nrequester_number_of_comments_at_request         0.036705\\nrequester_upvotes_plus_downvotes_at_request     0.033247\\nnumber_of_downvotes_of_request_at_retrieval     0.020646\\nName: requester_received_pizza, dtype: float64\\n\\nUser Flair Distribution:\\nrequester_user_flair\\nshroom    677\\nPIF        38\\nName: count, dtype\", \"is_error\": false}",
        "tool.duration_ms": 28.151512145996094
      }
    },
    {
      "trace_id": "9e4dd79885234c4ed604902a6a5cb9b7",
      "span_id": "df4a4f7801c47304",
      "parent_span_id": "3c35b47e46fc98ad",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1768001201720651181,
      "end_time": 1768001206269259694,
      "duration": 4.548608513,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7f0ac4e56650>\", \"input\": {\"query\": \"What are winning approaches for text classification with mixed text and numerical features on Kaggle?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nWinning Kaggle solutions for text‑classification problems that also contain numeric (or categorical) columns usually treat the data as a **multimodal** problem and combine the strengths of transformer‑based language models with classic tabular‑model techniques. A very popular shortcut is to **convert every feature to a string and concatenate them with a special separator** (e.g., `[SEP]`) before feeding the whole sequence to a pre‑trained transformer such as BERT or RoBERTa. This “features‑to‑text” trick lets the model attend to the numeric value as if it were a token and has been shown to give strong baselines on real Kaggle datasets (e.g., wine‑review classification at FreeAgent and clothing‑review sentiment at Kaggle) ([FreeAgent](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification), [McCormick ML](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).  \\n\\nA more “principled” variant extracts a **dense embedding for the text** with a transformer, **scales/encodes the numeric and categorical columns** (e.g., standardisation, target encoding), and then **concatenates the two representations** before passing them to a downstream classifier such as a shallow MLP, LightGBM, or logistic regression. This approach often outperforms the pure‑text trick because the numeric features retain their original magnitude and can be modelled with tree‑based learners that excel on tabular data. Implementations in Keras use a **multiple‑input model** – a CNN/LSTM branch for the text and a dense branch for the numbers – that are merged and trained end‑to‑end ([Towards Data Science](https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4)).  \\n\\nKaggle winners also rely on **automated multimodal toolkits** such as **AutoGluon**, which provides a “Tabular + Text” pipeline that automatically extracts transformer embeddings, handles numeric preprocessing, and builds stacked ensembles of both neural and tree models, achieving state‑of‑the‑art scores with minimal hand‑coding ([AutoGluon](https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-kaggle.html)). Finally, classic **stacking/blending** remains a go‑to strategy: train a strong text‑only model (TF‑IDF + linear, or a fine‑tuned transformer) and a separate tabular model (Random Forest, LightGBM), then feed their predictions into a meta‑learner. This avoids leakage by using out‑of‑fold predictions and often squeezes the last few percent of metric improvement ([DataScience StackExchange](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)).  \\n\\nIn practice, winning Kaggle pipelines therefore combine (1) **text‑to‑text concatenation** for a quick baseline, (2) **separate embedding + numeric concatenation** for higher accuracy, (3) **multi‑input neural nets** when end‑to‑end training is feasible, (4) **AutoGluon‑style multimodal ensembles** for rapid prototyping, and (5) **stacked/blended meta‑models** to capture complementary signals from the two modalities.\\n\\n--- Result 2 ---\\nTitle: Combining text with numerical and categorical features for classification\\nURL: https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification\\n\\nContent:\\n[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\\n\\n# Classification with transformer models\\n\\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\\n\\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\\n\\nFor example a transaction with the following set of features:\\n\\n|     |     |     |\\n| --- | --- | --- |\\n| **Description** | **Amount** | **Industry** |\\n| Plastic pipes | £-7.99 | Plumbing |\\n\\ncould be represented by the string:  `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We’ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\\n\\nThis approach however doesn’t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\\n\\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\\n\\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\\n\\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\\n\\n# Public wine reviews dataset\\n\\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores – wines between 80 and 86 (incl) were flagged as “neutral”, between 87 and 93 as “good” and greater than 94 as “excellent”.\\n\\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\\n\\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\\n\\n```\\nimport pandas as pd\\n\\nwine_df = pd.read_csv(\\\"data/wine_data.csv\\\")\\nbins = [0, 87, 94, np.inf]\\nnames = [\\\"neutral\\\", \\\"good\\\", \\\"excellent\\\"]\\n\\nwine_df[\\\"rating\\\"] = pd.cut(wine_df[\\\"points\\\"], bins, labels=names)\\n\\n```\\n\\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\\n\\n```\\nNUMERICAL_FEATURE = \\\"price\\\"\\nCATEGORICAL_FEATURE = \\\"variety\\\"\\nTEXT_FEATURE = \\\"description\\\"\\nTARGET = \\\"rating\\\"\\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\\n\\nwine_df = wine_df[FEATURES + [TARGET]]\\n\\n```\\n\\nWe then split our wine dataframe into an 80:20 training:evaluation split.\\n\\n```\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\\n\\n```\\n\\n## LightGBM classifier approach\\n\\n### Preprocessing\\n\\n#### Preprocessing numerical and categorical features\\n\\nIn a first step we will preprocess the numerical and categorical features using SKLearn’s ColumnTransformer.  For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\\n\\nWe preprocessed the categorical features by filling missing values with “other” and OneHot encoded them.\\n\\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\\n\\n```\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    OneHotEncoder,\\n    StandardScaler,\\n)\\n\\ndef preprocess_number():\\n    return make_pipeline(\\n        SimpleImputer(strategy=\\\"median\\\"),\\n        StandardScaler(),\\n    )\\n\\ndef preprocess_categories():\\n    return make_pipeline(\\n       SimpleImputer(strategy=\\\"constant\\\", fill_value=\\\"other\\\", missing_values=np.nan),\\n       OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n    )\\n\\ndef create_preprocessor():\\n\\n    transformers = [\\n        (\\\"num_preprocessor\\\", preprocess_number(), [NUMERICAL_FEATURE]),\\n        (\\\"cat_preprocessor\\\", preprocess_categories(), [CATEGORICAL_FEATURE]),\\n    ]\\n\\n    return ColumnTransformer(transformers=transformers, remainder=\\\"drop\\\")\\n\\ncolumn_transformer = create_preprocessor()\\ncolumn_transformer.set_output(transform=\\\"pandas\\\")\\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\\n)\\n\\n```\\n\\n#### Extracting text vector representation with a transformer model\\n\\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\\n\\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\\\[CLS\\\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\\\[CLS\\\\] token in the final layer as the vector representation of our wine descriptions.\\n\\nIn order to extract the \\\\[CLS\\\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\\n\\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\\\_ids, attention\\\\_mask and token\\\\_type\\\\_ids. Only the input\\\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\\n\\nThe code for the tokenization is shown below:\\n\\n```\\nfrom datasets import Dataset\\nfrom transformers import AutoTokenizer\\n\\nMODEL_NAME = \\\"distilbert-base-uncased\\\"\\n\\ndef tokenized_pytorch_tensors(\\n        df: pd.DataFrame,\\n        column_list: list\\n    ) -> Dataset:\\n\\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n    transformers_dataset = Dataset.from_pandas(df)\\n\\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\\n        return tokenizer(\\n            model_inputs_batch[TEXT_FEATURE],\\n            padding=True,\\n            max_length=120,\\n            truncation=True,\\n        )\\n\\n    tokenized_dataset = transformers_dataset.map(\\n        tokenize,\\n        batched=True,\\n        batch_size=128\\n    )\\n\\n    tokenized_dataset.set_format(\\n        \\\"torch\\\",\\n        columns=column_list\\n    )\\n\\n    columns_t...\\n\\n--- Result 3 ---\\nURL: https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert\\n\\nContent:\\n# Combining Categorical and Numerical Features with Text in BERT\\n\\n29 Jun 2021\\n\\nIn this tutorial we’ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\\n\\nTo help motivate our discussion, we’ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\\n\\n- The age of the reviewer (numerical feature)\\n- The number of upvotes on the review (numerical feature)\\n- The department and category of the clothing item (categorical features)\\n\\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\\n\\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\\n\\nIn Section 2 of this Notebook, I’ve implemented four different “baseline” strategies which score fairly well, but which don’t incorporate all of the features together.\\n\\nThen, in Section 3, I’ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\\n\\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\\n\\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\\n\\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\\n\\nBy Chris McCormick\\n\\n# Contents\\n\\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\\n\\n# S1. Clothing Review Dataset\\n\\n## 1.1. Download & Parse\\n\\nRetrieve the .csv file for the dataset.\\n\\n```\\nimport gdown\\n\\nprint('Downloading dataset...\\\\n')\\n\\n# Download the file.\\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\\n                'Womens Clothing E-Commerce Reviews.csv',\\n                quiet=False)\\n\\nprint('\\\\n\\\\nDONE.')\\n```\\n\\n```\\nDownloading dataset...\\n\\nDownloading...\\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\\nTo: /content/Womens Clothing E-Commerce Reviews.csv\\n8.48MB [00:00, 48.7MB/s]\\n\\nDONE.\\n\\n```\\n\\nParse the dataset csv file into a pandas DataFrame.\\n\\n```\\nimport pandas as pd\\n\\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\\n\\ndata_df.head()\\n```\\n\\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\\n\\n_Features_\\n\\n“ **Recommended IND**” is the label we are trying to predict for this dataset. “1” means the reviewer recommended the product and “0” means they do not.\\n\\nThe following are _categorical_ features:\\n\\n- Division Name\\n- Department Name\\n- Class Name\\n- Clothing ID\\n\\nAnd the following are _numerical_ features:\\n\\n- Age\\n- Rating\\n- Positive Feedback Count\\n\\n_Feature Analysis_\\n\\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\\n\\nNote that, in addition to the “Recommended” label, there is also a “ **Rating**” column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We’ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\\n\\n## 1.2. Train-Validation-Test Split\\n\\nI want to use the same training, validation, and test splits for all of the approaches we try so that it’s a fair comparison.\\n\\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\\n\\nTo solve this, we’re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\\n\\n```\\nimport random\\nimport numpy as np\\n\\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\\ntrain_size = int(0.8 * len(data_df))\\nval_size = int(0.1 * len(data_df))\\ntest_size = len(data_df) - (train_size + val_size)\\n\\n# Sanity check the sizes.\\nassert((train_size + val_size + test_size) == len(data_df))\\n\\n# Create a list of indeces for all of the samples in the dataset.\\nindeces = np.arange(0, len(data_df))\\n\\n# Shuffle the indeces randomly.\\nrandom.shuffle(indeces)\\n\\n# Get a list of indeces for each of the splits.\\ntrain_idx = indeces[0:train_size]\\nval_idx = indeces[train_size:(train_size + val_size)]\\ntest_idx = indeces[(train_size + val_size):]\\n\\n# Sanity check\\nassert(len(train_idx) == train_size)\\nassert(len(test_idx) == test_size)\\n\\n# With these lists, we can now select the corresponding dataframe rows using,\\n# e.g., train_df = data_df.iloc[train_idx]\\n\\nprint('  Training size: {:,}'.format(train_size))\\n...\\n\\n--- Result 4 ---\\nURL: https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th\\n\\nContent:\\n**Teams**\\n\\nQ&A for work\\n\\nConnect and share knowledge within a single location that is structured and easy to search.\\n\\n[Learn more about Teams](https://stackoverflow.co/teams/)\\n\\n# [Training a classifier with text and numerical features - what is the state of the art?](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)\\n\\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\\n\\nAsked4 years, 1 month ago\\n\\nModified [4 years, 1 month ago](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th?lastactivity)\\n\\nViewed\\n2k times\\n\\n1\\n\\n$\\\\\\\\begingroup$\\n\\nI'm trying to build a binary classifier where the features are mostly numerical (about 20) and there are a couple of unstructured short text fields as well. What is currently considered the state of the art for combining these types of features?\\n\\nI've tried building a separate classifier (logistic regression, TFIDF) using the text data alone, and then including that output score of that classifier as an additional when training using the rest of the numerical features (random forest, taking care to train each classifier on different folds of the data to prevent signal leakage). It works alright, but I think a better performance is possible.\\n\\nA variant is to simply train two separate classifiers, one using text and the other using numerican features and then stacking those two. And finally, another idea is to use neural networks, have two input networks, one CNN/LSTM for the text, and another dense for the numerical features, and then combining them and having a single output.\\n\\nAre there other approaches I haven't thought of that are worth a try?\\n\\n- [neural-network](https://datascience.stackexchange.com/questions/tagged/neural-network)\\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\\n- [text-classification](https://datascience.stackexchange.com/questions/tagged/text-classification)\\n\\n[Share](https://datascience.stackexchange.com/q/72791)\\n\\n[Improve this question](https://datascience.stackexchange.com/posts/72791/edit)\\n\\nFollow\\n\\nasked Apr 22, 2020 at 16:35\\n\\n[![Ansari's user avatar](https://www.gravatar.com/avatar/a88d3e79b0f8dd6a9d1fafdcf68d5901?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/51/ansari)\\n\\n[Ansari](https://datascience.stackexchange.com/users/51/ansari) Ansari\\n\\n18677 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th) \\\\|\\n\\n## 2 Answers 2\\n\\nSorted by:\\n[Reset to default](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th?answertab=scoredesc#tab-top)\\n\\nHighest score (default)Date modified (newest first)Date created (oldest first)\\n\\n1\\n\\n$\\\\\\\\begingroup$\\n\\nRegarding the first question, to the best of my knowledge, there is no SOTA approach. It depends on the task.\\n\\nFor the second one, did you try to combine the numerical values with the TFIDF vectors, and then feed the final vector(s) to the classifier? Probably not. I usually try this way. But, if your vocabulary is too large, try to reduce it by considering top N-words, because combining small numerical vector with a huge TFIDF vector may not allow the numerical values to affect the results.\\n\\n[Share](https://datascience.stackexchange.com/a/72793)\\n\\n[Improve this answer](https://datascience.stackexchange.com/posts/72793/edit)\\n\\nFollow\\n\\nanswered Apr 22, 2020 at 16:53\\n\\n[![Minions's user avatar](https://i.sstatic.net/AcfLu.jpg?s=64)](https://datascience.stackexchange.com/users/49456/minions)\\n\\n[Minions](https://datascience.stackexchange.com/users/49456/minions) Minions\\n\\n26222 silver badges1515 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n3\\n\\n- $\\\\\\\\begingroup$Thanks. It feels weird to concatenate sparse features sets with dense ones, plus as you noted, there is the possibility of the tfidf features overshadowing the numerical ones. Even with limiting the vocabulary, they would still be an order of magnitude more.$\\\\\\\\endgroup$\\n\\n– [Ansari](https://datascience.stackexchange.com/users/51/ansari)\\n\\nCommentedApr 22, 2020 at 17:00\\n\\n- $\\\\\\\\begingroup$I don't want to answer you `yes` or `no`, but I tried this way and it works well. I tested it in my approach for a research shared task and I was ranked first. Despite that, what you've mentioned is still a scientific explanation. BTW, I do this with classical classifiers (SVM, NB, etc.), in the case of Neural Networks, I concatenate two different branches (one for text and another for the numerical values) at the very last steps of the network.$\\\\\\\\endgroup$\\n\\n– [Minions](https://datascience.stackexchange.com/users/49456/minions)\\n\\nCommentedApr 22, 2020 at 17:14\\n\\n- 1\\n\\n\\n\\n\\n\\n$\\\\\\\\begingroup$Thanks! I'll give it a shot.$\\\\\\\\endgroup$\\n\\n– [Ansari](https://datascience.stackexchange.com/users/51/ansari)\\n\\nCommentedApr 22, 2020 at 17:19\\n\\n\\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th) \\\\|\\n\\n0\\n\\n$\\\\\\\\begingroup$\\n\\nTo improve your results you should try other embeddings instead of tf-idf. Some examples are word2vec, FastText, Elmo, Flair or transformer-based embeddings like BERT. In the past I got great results with a single neural network which uses embeddings and numerical features as input features. But, as mentioned before, it depends on your concrete problem.\\n\\n[Share](https://datascience.stackexchange.com/a/72803)\\n\\n[Improve this answer](https://datascience.stackexchange.com/posts/72803/edit)\\n\\nFollow\\n\\nanswered Apr 22, 2020 at 19:08\\n\\n[![NiklasF's user avatar](https://www.gravatar.com/avatar/b197b20632981b62287e2b729e1028fc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/95525/niklasf)\\n\\n[NiklasF](https://datascience.stackexchange.com/users/95525/niklasf) NiklasF\\n\\n1\\n\\n$\\\\\\\\endgroup$\\n\\n3\\n\\n- $\\\\\\\\begingroup$Did you concatenate the embeddings with the numerical features or have separate branches in your network and then combine their outputs at a more reasonable dimension?$\\\\\\\\endgroup$\\n\\n– [Ansari](https://datascience.stackexchange.com/users/51/ansari)\\n\\nCommentedApr 22, 2020 at 20:25\\n\\n- 1\\n\\n\\n\\n\\n\\n$\\\\\\\\begingroup$Yes, I concatenated numerical features and embeddings. I got best results with flair embeddings.$\\\\\\\\endgroup$\\n\\n– [NiklasF](https://datascience.stackexchange.com/users/95525/niklasf)\\n\\nCommentedApr 23, 2020 at 19:08\\n\\n- $\\\\\\\\begingroup$@NiklasF Could you share a code snippet of combining text features with numerical ones, and how did you feed those into a nn? Have you used a pre-trained model, or did you create one from scratch (so you decide exactly the input of your nn).$\\\\\\\\endgroup$\\n\\n– [George Petropoulos](https://datascience.stackexchange.com/users/85444/george-petropoulos)\\n\\nCommentedApr 23, 2021 at 9:27\\n\\n\\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th) \\\\|\\n\\n## Your Answer\\n\\nDraft saved\\n\\nDraft discarded\\n\\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f72791%2ftraining-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th%23new-answer)\\n\\nSign up using Google\\n\\nSign up using Facebook\\n\\nSign up using Email and Password\\n\\nSubmit\\n\\n### Post as a guest\\n\\nName\\n\\nEmail\\n\\nRequired, but never shown\\n\\nPost Your Answer\\n\\nDiscard\\n\\nBy clicking “Post Your Answer”, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\\n\\n## Not the answer you're looking for? Browse other questions tagged  - [neural-network](https://datascience.stackexchange.com/questions/tagged/neural-network) - [classific...\\n\\n--- Result 5 ---\\nTitle: How to use AutoGluon for Kaggle competitions ¶\\nURL: https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-kaggle.html\\n\\nContent:\\nHow to use AutoGluon for Kaggle competitions - AutoGluon 1.5.0 documentationContentsMenuExpandLight modeDark modeAuto light/dark, in light modeAuto light/dark, in dark mode[Skip to content](#furo-main-content)\\n[\\nAutoGluon 1.5.0 documentation\\n](../../../index.html)\\n[\\n![Light Logo](../../../_static/autogluon.png)![Dark Logo](../../../_static/autogluon-w.png)\\n](../../../index.html)\\nGet Started\\n* [Install](../../../install.html)\\n* [Tabular Quick Start](../tabular-quick-start.html)\\n* [Time Series Quick Start](../../timeseries/forecasting-quick-start.html)\\n* [Multimodal Quick Start](../../multimodal/multimodal_prediction/multimodal-quick-start.html)\\nTutorials\\n* [Tabular](../index.html)\\n* [Essentials](../tabular-essentials.html)\\n* [In Depth](../tabular-indepth.html)\\n* [Foundational Models](../tabular-foundational-models.html)\\n* [How It Works](../how-it-works.html)\\n* [Feature Engineering](../tabular-feature-engineering.html)\\n* [Tabular + Text + Images](../tabular-multimodal.html)\\n* [Advanced](index.html)\\n* [Multilabel](tabular-multilabel.html)\\n* [Kaggle](#)\\n* [GPU](tabular-gpu.html)\\n* [Custom Metrics](tabular-custom-metric.html)\\n* [Custom Models](tabular-custom-model.html)\\n* [Custom Models Advanced](tabular-custom-model-advanced.html)\\n* [Deployment](tabular-deployment.html)\\n* [Hyperparameter Optimization](tabular-hpo.html)\\n* [Time Series](../../timeseries/index.html)\\n* [Quick Start](../../timeseries/forecasting-quick-start.html)\\n* [In Depth](../../timeseries/forecasting-indepth.html)\\n* [Forecasting with Chronos-2](../../timeseries/forecasting-chronos.html)\\n* [Metrics](../../timeseries/forecasting-metrics.html)\\n* [Model Zoo](../../timeseries/model_zoo/index.html)\\n* [Forecasting Models](../../timeseries/forecasting-model-zoo.html)\\n* [Ensemble Models](../../timeseries/forecasting-ensembles.html)\\n* [Advanced](../../timeseries/advanced/index.html)\\n* [Custom Models](../../timeseries/advanced/forecasting-custom-model.html)\\n* [Multimodal](../../multimodal/index.html)\\n* [Multimodal Prediction](../../multimodal/multimodal_prediction/index.html)\\n* [AutoMM for Image + Text + Tabular - Quick Start](../../multimodal/multimodal_prediction/beginner_multimodal.html)\\n* [AutoMM for Entity Extraction with Text and Image - Quick Start](../../multimodal/multimodal_prediction/multimodal_ner.html)\\n* [AutoMM for Text + Tabular - Quick Start](../../multimodal/multimodal_prediction/multimodal_text_tabular.html)\\n* [Object Detection](../../multimodal/object_detection/index.html)\\n* [Object Detection Quick Start](../../multimodal/object_detection/quick_start/index.html)\\n* [AutoMM Detection - Quick Start on a Tiny COCO Format Dataset](../../multimodal/object_detection/quick_start/quick_start_coco.html)\\n* [Object Detection Advanced](../../multimodal/object_detection/advanced/index.html)\\n* [AutoMM Detection - Finetune on COCO Format Dataset with Customized Settings](../../multimodal/object_detection/advanced/finetune_coco.html)\\n* [Object Detection Data Preparation](../../multimodal/object_detection/data_preparation/index.html)\\n* [Convert Data to COCO Format](../../multimodal/object_detection/data_preparation/convert_data_to_coco_format.html)\\n* [AutoMM Detection - Prepare Pothole Dataset](../../multimodal/object_detection/data_preparation/prepare_pothole.html)\\n* [AutoMM Detection - Prepare Watercolor Dataset](../../multimodal/object_detection/data_preparation/prepare_watercolor.html)\\n* [AutoMM Detection - Prepare COCO2017 Dataset](../../multimodal/object_detection/data_preparation/prepare_coco17.html)\\n* [AutoMM Detection - Prepare Pascal VOC Dataset](../../multimodal/object_detection/data_preparation/prepare_voc.html)\\n* [AutoMM Detection - Convert VOC Format Dataset to COCO Format](../../multimodal/object_detection/data_preparation/voc_to_coco.html)\\n* [Image Prediction](../../multimodal/image_prediction/index.html)\\n* [AutoMM for Image Classification - Quick Start](../../multimodal/image_prediction/beginner_image_cls.html)\\n* [Zero-Shot Image Classification with CLIP](../../multimodal/image_prediction/clip_zeroshot.html)\\n* [Image Segmentation](../../multimodal/image_segmentation/index.html)\\n* [AutoMM for Semantic Segmentation - Quick Start](../../multimodal/image_segmentation/beginner_semantic_seg.html)\\n* [Text Prediction](../../multimodal/text_prediction/index.html)\\n* [AutoMM for Text - Quick Start](../../multimodal/text_prediction/beginner_text.html)\\n* [AutoMM for Named Entity Recognition - Quick Start](../../multimodal/text_prediction/ner.html)\\n* [AutoMM for Named Entity Recognition in Chinese - Quick Start](../../multimodal/text_prediction/chinese_ner.html)\\n* [AutoMM for Text - Multilingual Problems](../../multimodal/text_prediction/multilingual_text.html)\\n* [Document Prediction](../../multimodal/document_prediction/index.html)\\n* [AutoMM for Scanned Document Classification](../../multimodal/document_prediction/document_classification.html)\\n* [Classifying PDF Documents with AutoMM](../../multimodal/document_prediction/pdf_classification.html)\\n* [Semantic Matching](../../multimodal/semantic_matching/index.html)\\n* [Image-to-Image Semantic Matching with AutoMM](../../multimodal/semantic_matching/image2image_matching.html)\\n* [Image-Text Semantic Matching with AutoMM](../../multimodal/semantic_matching/image_text_matching.html)\\n* [Text-to-Text Semantic Matching with AutoMM](../../multimodal/semantic_matching/text2text_matching.html)\\n* [Text Semantic Search with AutoMM](../../multimodal/semantic_matching/text_semantic_search.html)\\n* [Image-Text Semantic Matching with AutoMM - Zero-Shot](../../multimodal/semantic_matching/zero_shot_img_txt_matching.html)\\n* [Advanced Topics](../../multimodal/advanced_topics/index.html)\\n* [AutoMM Problem Types And Metrics](../../multimodal/advanced_topics/problem_types_and_metrics.html)\\n* [Hyperparameter Optimization in AutoMM](../../multimodal/advanced_topics/hyperparameter_optimization.html)\\n* [Continuous Training with AutoMM](../../multimodal/advanced_topics/continuous_training.html)\\n* [Customize AutoMM](../../multimodal/advanced_topics/customization.html)\\n* [Knowledge Distillation in AutoMM](../../multimodal/advanced_topics/model_distillation.html)\\n* [Single GPU Billion-scale Model Training via Parameter-Efficient Finetuning](../../multimodal/advanced_topics/efficient_finetuning_basic.html)\\n* [Few Shot Learning with AutoMM](../../multimodal/advanced_topics/few_shot_learning.html)\\n* [Handling Class Imbalance with AutoMM - Focal Loss](../../multimodal/advanced_topics/focal_loss.html)\\n* [AutoMM Presets](../../multimodal/advanced_topics/presets.html)\\n* [Faster Prediction with TensorRT](../../multimodal/advanced_topics/tensorrt.html)\\n* [Multiple Label Columns with AutoMM](../../multimodal/advanced_topics/multiple_label_columns.html)\\n* [Cloud Training and Deployment](../../cloud_fit_deploy/index.html)\\n* [AutoGluon Cloud](../../cloud_fit_deploy/autogluon-cloud.html)\\n* [AutoGluon Tabular on SageMaker AutoPilot](../../cloud_fit_deploy/autopilot-autogluon.html)\\n* [Deploy AutoGluon Models on Serverless Templates](../../cloud_fit_deploy/cloud-aws-lambda-deployment.html)\\n* [Cloud Training and Deployment with Amazon SageMaker](../../cloud_fit_deploy/cloud-aws-sagemaker-train-deploy.html)\\nResources\\n* [Cheat Sheets](../../../cheatsheet.html)\\n* [Versions](https://auto.gluon.ai/stable/versions.html)\\n* [What's New](../../../whats_new/index.html)\\n* [Version 1.5.0](../../../whats_new/v1.5.0.html)\\n* [Version 1.4.0](../../../whats_new/v1.4.0.html)\\n* [Version 1.3.1](../../../whats_new/v1.3.1.html)\\n* [Version 1.3.0](../../../whats_new/v1.3.0.html)\\n* [Version 1.2.0](../../../whats_new/v1.2.0.html)\\n* [Version 1.1.1](../../../whats_new/v1.1.1.html)\\n* [Version 1.1.0](../../../whats_new/v1.1.0.html)\\n* [Version 1.0.0](../../../whats_new/v1.0.0.html)\\n* [Version 0.8.3](../../../whats_new/v0.8.3.html)\\n* [Version 0.8.2](../../../whats_new/v0.8.2.html)\\n* [Version 0.8.1](../../../whats_new/v0.8.1.html)\\n* [Version 0.8.0](../../../whats_new/v0.8.0.html)\\n* [Version 0.7.0](../../../whats_new/v0.7.0.ht...\\n\\n--- Result 6 ---\\nTitle: Combining numerical and text features in deep neural networks\\nURL: https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4?gi=dfc76a188f1a\\n\\nContent:\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe91f0237eea4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\\n\\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\nMember-only story\\n\\n# Combining numerical and text features in deep neural networks\\n\\n## How to use Keras multiple input models to train a deep neural network end to end for text and numerical data\\n\\n[![Christian Freischlag](https://miro.medium.com/v2/resize:fill:88:88/2*vcYKirJodH4oKn1Hglhr_Q.jpeg)](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Christian Freischlag](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)\\n\\n·\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------post_header-----------)\\n\\nPublished in\\n\\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n·\\n\\n4 min read\\n\\n·\\n\\nMay 19, 2020\\n\\n--\\n\\n6\\n\\nShare\\n\\nPhoto by [Marius Masalar](https://unsplash.com/@marius?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\\n\\nIn applied machine learning, data often consists of multiple data types, e.g. text and numerical data. To build a model which combines features from both domains, it is necessary to stack these features together. This post shows different solutions to combine natural language processing and traditional features in one single model in Keras (end-to-end learning).\\n\\n# Real-world data is different\\n\\nScientific data sets are usually limited to one single kind of data e.g. text, images or numerical data. It makes a lot of sense, as the goal is to compare new with existing models and approaches. In real-world scenarios data is often more diverse. To utilize end-to-end learning neural networks, instead of manually stacking models, we need to combine these different feature spaces inside the neural network.\\n\\nLet’s assume we want to solve a text classification problem and we have additional metadata for each of the documents in our corpus. In simple approaches, where our document is represented by a [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) vector, we could just add our metadata to the vector as additional…\\n\\n[![Christian Freischlag](https://miro.medium.com/v2/resize:fill:144:144/2*vcYKirJodH4oKn1Hglhr_Q.jpeg)](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------follow_profile-----------)\\n\\n[**Written by Christian Freischlag**](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[246 Followers](https://medium.com/@christian.freischlag/followers?source=post_page-----e91f0237eea4--------------------------------)\\n\\n·Writer for\\n\\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\nML/Data and Tech enthusiast \\\\| [digital-thinking.de](http://digital-thinking.de) \\\\| Senior Specialist - Natural Language Processing and AI @KPMG\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------follow_profile-----------)\\n\\n[Help](https://help.medium.com/hc/en-us?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Status](https://medium.statuspage.io/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[About](https://medium.com/about?autoplay=1&source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Press](mailto:pressinquiries@medium.com?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Blog](https://blog.medium.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Text to speech](https://speechify.com/medium?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Teams](https://medium.com/business?source=post_page-----e91f0237eea4--------------------------------)\\n\\n--- Result 7 ---\\nTitle: Text Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\\n\\nContent:\\nIn this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle’s top NLP competitions.\\n\\nNamely, I’ve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) – $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) – $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) – $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) – $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) – $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let’s begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let’s see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet’s see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in  R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\\n\\n--- Result 8 ---\\nTitle: How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\\n\\nContent:\\nHow To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n←Previous\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\\n\\n--- Result 9 ---\\nTitle: A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\nURL: https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312\\n\\nContent:\\nA Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\n[\\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n18 min read\\n·Apr 28, 2024\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\\nListen\\nShare\\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\\n**> Note:\\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\\n*> “It is estimated that 90% of the world’s data was generated in the last two years alone”\\n*> by [> Amount of Data Created Daily (2023)\\n](https://explodingtopics.com/blog/data-generated-per-day)\\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\\n## Methodology\\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\\nPress enter or click to view image in full size\\n![]()\\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\\n## Key Discoveries of Text Data Competitions\\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term “question answering” exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Winning Kaggle solutions for text‑classification problems that also contain numeric (or categorical) columns usually treat the data as a **multimodal** problem and combine the strengths of transformer‑based language models with classic tabular‑model techniques. A very popular shortcut is to **convert every feature to a string and concatenate them with a special separator** (e.g., `[SEP]`) before feeding the whole sequence to a pre‑trained transformer such as BERT or RoBERTa. This “features‑to‑text” trick lets the model attend to the numeric value as if it were a token and has been shown to give strong baselines on real Kaggle datasets (e.g., wine‑review classification at FreeAgent and clothing‑review sentiment at Kaggle) ([FreeAgent](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification), [McCormick ML](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).  \\n\\nA more “principled” variant extracts a **dense embedding for the text** with a transformer, **scales/encodes the numeric and categorical columns** (e.g., standardisation, target encoding), and then **concatenates the two representations** before passing them to a downstream classifier such as a shallow MLP, LightGBM, or logistic regression. This approach often outperforms the pure‑text trick because the numeric features retain their original magnitude and can be modelled with tree‑based learners that excel on tabular data. Implementations in Keras use a **multiple‑input model** – a CNN/LSTM branch for the text and a dense branch for the numbers – that are merged and trained end‑to‑end ([Towards Data Science](https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4)).  \\n\\nKaggle winners also rely on **automated multimodal toolkits** such as **AutoGluon**, which provides a “Tabular + Text” pipeline that automatically extracts transformer embeddings, handles numeric preprocessing, and builds stacked ensembles of both neural and tree models, achieving state‑of‑the‑art scores with minimal hand‑coding ([AutoGluon](https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-kaggle.html)). Finally, classic **stacking/blending** remains a go‑to strategy: train a strong text‑only model (TF‑IDF + linear, or a fine‑tuned transformer) and a separate tabular model (Random Forest, LightGBM), then feed their predictions into a meta‑learner. This avoids leakage by using out‑of‑fold predictions and often squeezes the last few percent of metric improvement ([DataScience StackExchange](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)).  \\n\\nIn practice, winning Kaggle pipelines therefore combine (1) **text‑to‑text concatenation** for a quick baseline, (2) **separate embedding + numeric concatenation** for higher accuracy, (3) **multi‑input neural nets** when end‑to‑end training is feasible, (4) **AutoGluon‑style multimodal ensembles** for rapid prototyping, and (5) **stacked/blended meta‑models** to capture complementary signals from the two modalities.\", \"url\": \"\"}, {\"title\": \"Combining text with numerical and categorical features for classification\", \"text\": \"[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\\n\\n# Classification with transformer models\\n\\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\\n\\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\\n\\nFor example a transaction with the following set of features:\\n\\n|     |     |     |\\n| --- | --- | --- |\\n| **Description** | **Amount** | **Industry** |\\n| Plastic pipes | £-7.99 | Plumbing |\\n\\ncould be represented by the string:  `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We’ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\\n\\nThis approach however doesn’t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\\n\\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\\n\\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\\n\\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\\n\\n# Public wine reviews dataset\\n\\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores – wines between 80 and 86 (incl) were flagged as “neutral”, between 87 and 93 as “good” and greater than 94 as “excellent”.\\n\\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\\n\\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\\n\\n```\\nimport pandas as pd\\n\\nwine_df = pd.read_csv(\\\"data/wine_data.csv\\\")\\nbins = [0, 87, 94, np.inf]\\nnames = [\\\"neutral\\\", \\\"good\\\", \\\"excellent\\\"]\\n\\nwine_df[\\\"rating\\\"] = pd.cut(wine_df[\\\"points\\\"], bins, labels=names)\\n\\n```\\n\\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\\n\\n```\\nNUMERICAL_FEATURE = \\\"price\\\"\\nCATEGORICAL_FEATURE = \\\"variety\\\"\\nTEXT_FEATURE = \\\"description\\\"\\nTARGET = \\\"rating\\\"\\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\\n\\nwine_df = wine_df[FEATURES + [TARGET]]\\n\\n```\\n\\nWe then split our wine dataframe into an 80:20 training:evaluation split.\\n\\n```\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\\n\\n```\\n\\n## LightGBM classifier approach\\n\\n### Preprocessing\\n\\n#### Preprocessing numerical and categorical features\\n\\nIn a first step we will preprocess the numerical and categorical features using SKLearn’s ColumnTransformer.  For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\\n\\nWe preprocessed the categorical features by filling missing values with “other” and OneHot encoded them.\\n\\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\\n\\n```\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    OneHotEncoder,\\n    StandardScaler,\\n)\\n\\ndef preprocess_number():\\n    return make_pipeline(\\n        SimpleImputer(strategy=\\\"median\\\"),\\n        StandardScaler(),\\n    )\\n\\ndef preprocess_categories():\\n    return make_pipeline(\\n       SimpleImputer(strategy=\\\"constant\\\", fill_value=\\\"other\\\", missing_values=np.nan),\\n       OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n    )\\n\\ndef create_preprocessor():\\n\\n    transformers = [\\n        (\\\"num_preprocessor\\\", preprocess_number(), [NUMERICAL_FEATURE]),\\n        (\\\"cat_preprocessor\\\", preprocess_categories(), [CATEGORICAL_FEATURE]),\\n    ]\\n\\n    return ColumnTransformer(transformers=transformers, remainder=\\\"drop\\\")\\n\\ncolumn_transformer = create_preprocessor()\\ncolumn_transformer.set_output(transform=\\\"pandas\\\")\\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\\n)\\n\\n```\\n\\n#### Extracting text vector representation with a transformer model\\n\\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\\n\\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\\\[CLS\\\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\\\[CLS\\\\] token in the final layer as the vector representation of our wine descriptions.\\n\\nIn order to extract the \\\\[CLS\\\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\\n\\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\\\_ids, attention\\\\_mask and token\\\\_type\\\\_ids. Only the input\\\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\\n\\nThe code for the tokenization is shown below:\\n\\n```\\nfrom datasets import Dataset\\nfrom transformers import AutoTokenizer\\n\\nMODEL_NAME = \\\"distilbert-base-uncased\\\"\\n\\ndef tokenized_pytorch_tensors(\\n        df: pd.DataFrame,\\n        column_list: list\\n    ) -> Dataset:\\n\\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n    transformers_dataset = Dataset.from_pandas(df)\\n\\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\\n        return tokenizer(\\n            model_inputs_batch[TEXT_FEATURE],\\n            padding=True,\\n            max_length=120,\\n            truncation=True,\\n        )\\n\\n    tokenized_dataset = transformers_dataset.map(\\n        tokenize,\\n        batched=True,\\n        batch_size=128\\n    )\\n\\n    tokenized_dataset.set_format(\\n        \\\"torch\\\",\\n        columns=column_list\\n    )\\n\\n    columns_t...\", \"url\": \"https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification\"}, {\"title\": \"\", \"text\": \"# Combining Categorical and Numerical Features with Text in BERT\\n\\n29 Jun 2021\\n\\nIn this tutorial we’ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\\n\\nTo help motivate our discussion, we’ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\\n\\n- The age of the reviewer (numerical feature)\\n- The number of upvotes on the review (numerical feature)\\n- The department and category of the clothing item (categorical features)\\n\\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\\n\\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\\n\\nIn Section 2 of this Notebook, I’ve implemented four different “baseline” strategies which score fairly well, but which don’t incorporate all of the features together.\\n\\nThen, in Section 3, I’ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\\n\\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\\n\\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\\n\\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\\n\\nBy Chris McCormick\\n\\n# Contents\\n\\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\\n\\n# S1. Clothing Review Dataset\\n\\n## 1.1. Download & Parse\\n\\nRetrieve the .csv file for the dataset.\\n\\n```\\nimport gdown\\n\\nprint('Downloading dataset...\\\\n')\\n\\n# Download the file.\\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\\n                'Womens Clothing E-Commerce Reviews.csv',\\n                quiet=False)\\n\\nprint('\\\\n\\\\nDONE.')\\n```\\n\\n```\\nDownloading dataset...\\n\\nDownloading...\\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\\nTo: /content/Womens Clothing E-Commerce Reviews.csv\\n8.48MB [00:00, 48.7MB/s]\\n\\nDONE.\\n\\n```\\n\\nParse the dataset csv file into a pandas DataFrame.\\n\\n```\\nimport pandas as pd\\n\\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\\n\\ndata_df.head()\\n```\\n\\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\\n\\n_Features_\\n\\n“ **Recommended IND**” is the label we are trying to predict for this dataset. “1” means the reviewer recommended the product and “0” means they do not.\\n\\nThe following are _categorical_ features:\\n\\n- Division Name\\n- Department Name\\n- Class Name\\n- Clothing ID\\n\\nAnd the following are _numerical_ features:\\n\\n- Age\\n- Rating\\n- Positive Feedback Count\\n\\n_Feature Analysis_\\n\\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\\n\\nNote that, in addition to the “Recommended” label, there is also a “ **Rating**” column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We’ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\\n\\n## 1.2. Train-Validation-Test Split\\n\\nI want to use the same training, validation, and test splits for all of the approaches we try so that it’s a fair comparison.\\n\\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\\n\\nTo solve this, we’re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\\n\\n```\\nimport random\\nimport numpy as np\\n\\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\\ntrain_size = int(0.8 * len(data_df))\\nval_size = int(0.1 * len(data_df))\\ntest_size = len(data_df) - (train_size + val_size)\\n\\n# Sanity check the sizes.\\nassert((train_size + val_size + test_size) == len(data_df))\\n\\n# Create a list of indeces for all of the samples in the dataset.\\nindeces = np.arange(0, len(data_df))\\n\\n# Shuffle the indeces randomly.\\nrandom.shuffle(indeces)\\n\\n# Get a list of indeces for each of the splits.\\ntrain_idx = indeces[0:train_size]\\nval_idx = indeces[train_size:(train_size + val_size)]\\ntest_idx = indeces[(train_size + val_size):]\\n\\n# Sanity check\\nassert(len(train_idx) == train_size)\\nassert(len(test_idx) == test_size)\\n\\n# With these lists, we can now select the corresponding dataframe rows using,\\n# e.g., train_df = data_df.iloc[train_idx]\\n\\nprint('  Training size: {:,}'.format(train_size))\\n...\", \"url\": \"https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert\"}, {\"title\": \"\", \"text\": \"**Teams**\\n\\nQ&A for work\\n\\nConnect and share knowledge within a single location that is structured and easy to search.\\n\\n[Learn more about Teams](https://stackoverflow.co/teams/)\\n\\n# [Training a classifier with text and numerical features - what is the state of the art?](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)\\n\\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\\n\\nAsked4 years, 1 month ago\\n\\nModified [4 years, 1 month ago](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th?lastactivity)\\n\\nViewed\\n2k times\\n\\n1\\n\\n$\\\\\\\\begingroup$\\n\\nI'm trying to build a binary classifier where the features are mostly numerical (about 20) and there are a couple of unstructured short text fields as well. What is currently considered the state of the art for combining these types of features?\\n\\nI've tried building a separate classifier (logistic regression, TFIDF) using the text data alone, and then including that output score of that classifier as an additional when training using the rest of the numerical features (random forest, taking care to train each classifier on different folds of the data to prevent signal leakage). It works alright, but I think a better performance is possible.\\n\\nA variant is to simply train two separate classifiers, one using text and the other using numerican features and then stacking those two. And finally, another idea is to use neural networks, have two input networks, one CNN/LSTM for the text, and another dense for the numerical features, and then combining them and having a single output.\\n\\nAre there other approaches I haven't thought of that are worth a try?\\n\\n- [neural-network](https://datascience.stackexchange.com/questions/tagged/neural-network)\\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\\n- [text-classification](https://datascience.stackexchange.com/questions/tagged/text-classification)\\n\\n[Share](https://datascience.stackexchange.com/q/72791)\\n\\n[Improve this question](https://datascience.stackexchange.com/posts/72791/edit)\\n\\nFollow\\n\\nasked Apr 22, 2020 at 16:35\\n\\n[![Ansari's user avatar](https://www.gravatar.com/avatar/a88d3e79b0f8dd6a9d1fafdcf68d5901?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/51/ansari)\\n\\n[Ansari](https://datascience.stackexchange.com/users/51/ansari) Ansari\\n\\n18677 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th) \\\\|\\n\\n## 2 Answers 2\\n\\nSorted by:\\n[Reset to default](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th?answertab=scoredesc#tab-top)\\n\\nHighest score (default)Date modified (newest first)Date created (oldest first)\\n\\n1\\n\\n$\\\\\\\\begingroup$\\n\\nRegarding the first question, to the best of my knowledge, there is no SOTA approach. It depends on the task.\\n\\nFor the second one, did you try to combine the numerical values with the TFIDF vectors, and then feed the final vector(s) to the classifier? Probably not. I usually try this way. But, if your vocabulary is too large, try to reduce it by considering top N-words, because combining small numerical vector with a huge TFIDF vector may not allow the numerical values to affect the results.\\n\\n[Share](https://datascience.stackexchange.com/a/72793)\\n\\n[Improve this answer](https://datascience.stackexchange.com/posts/72793/edit)\\n\\nFollow\\n\\nanswered Apr 22, 2020 at 16:53\\n\\n[![Minions's user avatar](https://i.sstatic.net/AcfLu.jpg?s=64)](https://datascience.stackexchange.com/users/49456/minions)\\n\\n[Minions](https://datascience.stackexchange.com/users/49456/minions) Minions\\n\\n26222 silver badges1515 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n3\\n\\n- $\\\\\\\\begingroup$Thanks. It feels weird to concatenate sparse features sets with dense ones, plus as you noted, there is the possibility of the tfidf features overshadowing the numerical ones. Even with limiting the vocabulary, they would still be an order of magnitude more.$\\\\\\\\endgroup$\\n\\n– [Ansari](https://datascience.stackexchange.com/users/51/ansari)\\n\\nCommentedApr 22, 2020 at 17:00\\n\\n- $\\\\\\\\begingroup$I don't want to answer you `yes` or `no`, but I tried this way and it works well. I tested it in my approach for a research shared task and I was ranked first. Despite that, what you've mentioned is still a scientific explanation. BTW, I do this with classical classifiers (SVM, NB, etc.), in the case of Neural Networks, I concatenate two different branches (one for text and another for the numerical values) at the very last steps of the network.$\\\\\\\\endgroup$\\n\\n– [Minions](https://datascience.stackexchange.com/users/49456/minions)\\n\\nCommentedApr 22, 2020 at 17:14\\n\\n- 1\\n\\n\\n\\n\\n\\n$\\\\\\\\begingroup$Thanks! I'll give it a shot.$\\\\\\\\endgroup$\\n\\n– [Ansari](https://datascience.stackexchange.com/users/51/ansari)\\n\\nCommentedApr 22, 2020 at 17:19\\n\\n\\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th) \\\\|\\n\\n0\\n\\n$\\\\\\\\begingroup$\\n\\nTo improve your results you should try other embeddings instead of tf-idf. Some examples are word2vec, FastText, Elmo, Flair or transformer-based embeddings like BERT. In the past I got great results with a single neural network which uses embeddings and numerical features as input features. But, as mentioned before, it depends on your concrete problem.\\n\\n[Share](https://datascience.stackexchange.com/a/72803)\\n\\n[Improve this answer](https://datascience.stackexchange.com/posts/72803/edit)\\n\\nFollow\\n\\nanswered Apr 22, 2020 at 19:08\\n\\n[![NiklasF's user avatar](https://www.gravatar.com/avatar/b197b20632981b62287e2b729e1028fc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/95525/niklasf)\\n\\n[NiklasF](https://datascience.stackexchange.com/users/95525/niklasf) NiklasF\\n\\n1\\n\\n$\\\\\\\\endgroup$\\n\\n3\\n\\n- $\\\\\\\\begingroup$Did you concatenate the embeddings with the numerical features or have separate branches in your network and then combine their outputs at a more reasonable dimension?$\\\\\\\\endgroup$\\n\\n– [Ansari](https://datascience.stackexchange.com/users/51/ansari)\\n\\nCommentedApr 22, 2020 at 20:25\\n\\n- 1\\n\\n\\n\\n\\n\\n$\\\\\\\\begingroup$Yes, I concatenated numerical features and embeddings. I got best results with flair embeddings.$\\\\\\\\endgroup$\\n\\n– [NiklasF](https://datascience.stackexchange.com/users/95525/niklasf)\\n\\nCommentedApr 23, 2020 at 19:08\\n\\n- $\\\\\\\\begingroup$@NiklasF Could you share a code snippet of combining text features with numerical ones, and how did you feed those into a nn? Have you used a pre-trained model, or did you create one from scratch (so you decide exactly the input of your nn).$\\\\\\\\endgroup$\\n\\n– [George Petropoulos](https://datascience.stackexchange.com/users/85444/george-petropoulos)\\n\\nCommentedApr 23, 2021 at 9:27\\n\\n\\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th) \\\\|\\n\\n## Your Answer\\n\\nDraft saved\\n\\nDraft discarded\\n\\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f72791%2ftraining-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th%23new-answer)\\n\\nSign up using Google\\n\\nSign up using Facebook\\n\\nSign up using Email and Password\\n\\nSubmit\\n\\n### Post as a guest\\n\\nName\\n\\nEmail\\n\\nRequired, but never shown\\n\\nPost Your Answer\\n\\nDiscard\\n\\nBy clicking “Post Your Answer”, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\\n\\n## Not the answer you're looking for? Browse other questions tagged  - [neural-network](https://datascience.stackexchange.com/questions/tagged/neural-network) - [classific...\", \"url\": \"https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th\"}, {\"title\": \"How to use AutoGluon for Kaggle competitions ¶\", \"text\": \"How to use AutoGluon for Kaggle competitions - AutoGluon 1.5.0 documentationContentsMenuExpandLight modeDark modeAuto light/dark, in light modeAuto light/dark, in dark mode[Skip to content](#furo-main-content)\\n[\\nAutoGluon 1.5.0 documentation\\n](../../../index.html)\\n[\\n![Light Logo](../../../_static/autogluon.png)![Dark Logo](../../../_static/autogluon-w.png)\\n](../../../index.html)\\nGet Started\\n* [Install](../../../install.html)\\n* [Tabular Quick Start](../tabular-quick-start.html)\\n* [Time Series Quick Start](../../timeseries/forecasting-quick-start.html)\\n* [Multimodal Quick Start](../../multimodal/multimodal_prediction/multimodal-quick-start.html)\\nTutorials\\n* [Tabular](../index.html)\\n* [Essentials](../tabular-essentials.html)\\n* [In Depth](../tabular-indepth.html)\\n* [Foundational Models](../tabular-foundational-models.html)\\n* [How It Works](../how-it-works.html)\\n* [Feature Engineering](../tabular-feature-engineering.html)\\n* [Tabular + Text + Images](../tabular-multimodal.html)\\n* [Advanced](index.html)\\n* [Multilabel](tabular-multilabel.html)\\n* [Kaggle](#)\\n* [GPU](tabular-gpu.html)\\n* [Custom Metrics](tabular-custom-metric.html)\\n* [Custom Models](tabular-custom-model.html)\\n* [Custom Models Advanced](tabular-custom-model-advanced.html)\\n* [Deployment](tabular-deployment.html)\\n* [Hyperparameter Optimization](tabular-hpo.html)\\n* [Time Series](../../timeseries/index.html)\\n* [Quick Start](../../timeseries/forecasting-quick-start.html)\\n* [In Depth](../../timeseries/forecasting-indepth.html)\\n* [Forecasting with Chronos-2](../../timeseries/forecasting-chronos.html)\\n* [Metrics](../../timeseries/forecasting-metrics.html)\\n* [Model Zoo](../../timeseries/model_zoo/index.html)\\n* [Forecasting Models](../../timeseries/forecasting-model-zoo.html)\\n* [Ensemble Models](../../timeseries/forecasting-ensembles.html)\\n* [Advanced](../../timeseries/advanced/index.html)\\n* [Custom Models](../../timeseries/advanced/forecasting-custom-model.html)\\n* [Multimodal](../../multimodal/index.html)\\n* [Multimodal Prediction](../../multimodal/multimodal_prediction/index.html)\\n* [AutoMM for Image + Text + Tabular - Quick Start](../../multimodal/multimodal_prediction/beginner_multimodal.html)\\n* [AutoMM for Entity Extraction with Text and Image - Quick Start](../../multimodal/multimodal_prediction/multimodal_ner.html)\\n* [AutoMM for Text + Tabular - Quick Start](../../multimodal/multimodal_prediction/multimodal_text_tabular.html)\\n* [Object Detection](../../multimodal/object_detection/index.html)\\n* [Object Detection Quick Start](../../multimodal/object_detection/quick_start/index.html)\\n* [AutoMM Detection - Quick Start on a Tiny COCO Format Dataset](../../multimodal/object_detection/quick_start/quick_start_coco.html)\\n* [Object Detection Advanced](../../multimodal/object_detection/advanced/index.html)\\n* [AutoMM Detection - Finetune on COCO Format Dataset with Customized Settings](../../multimodal/object_detection/advanced/finetune_coco.html)\\n* [Object Detection Data Preparation](../../multimodal/object_detection/data_preparation/index.html)\\n* [Convert Data to COCO Format](../../multimodal/object_detection/data_preparation/convert_data_to_coco_format.html)\\n* [AutoMM Detection - Prepare Pothole Dataset](../../multimodal/object_detection/data_preparation/prepare_pothole.html)\\n* [AutoMM Detection - Prepare Watercolor Dataset](../../multimodal/object_detection/data_preparation/prepare_watercolor.html)\\n* [AutoMM Detection - Prepare COCO2017 Dataset](../../multimodal/object_detection/data_preparation/prepare_coco17.html)\\n* [AutoMM Detection - Prepare Pascal VOC Dataset](../../multimodal/object_detection/data_preparation/prepare_voc.html)\\n* [AutoMM Detection - Convert VOC Format Dataset to COCO Format](../../multimodal/object_detection/data_preparation/voc_to_coco.html)\\n* [Image Prediction](../../multimodal/image_prediction/index.html)\\n* [AutoMM for Image Classification - Quick Start](../../multimodal/image_prediction/beginner_image_cls.html)\\n* [Zero-Shot Image Classification with CLIP](../../multimodal/image_prediction/clip_zeroshot.html)\\n* [Image Segmentation](../../multimodal/image_segmentation/index.html)\\n* [AutoMM for Semantic Segmentation - Quick Start](../../multimodal/image_segmentation/beginner_semantic_seg.html)\\n* [Text Prediction](../../multimodal/text_prediction/index.html)\\n* [AutoMM for Text - Quick Start](../../multimodal/text_prediction/beginner_text.html)\\n* [AutoMM for Named Entity Recognition - Quick Start](../../multimodal/text_prediction/ner.html)\\n* [AutoMM for Named Entity Recognition in Chinese - Quick Start](../../multimodal/text_prediction/chinese_ner.html)\\n* [AutoMM for Text - Multilingual Problems](../../multimodal/text_prediction/multilingual_text.html)\\n* [Document Prediction](../../multimodal/document_prediction/index.html)\\n* [AutoMM for Scanned Document Classification](../../multimodal/document_prediction/document_classification.html)\\n* [Classifying PDF Documents with AutoMM](../../multimodal/document_prediction/pdf_classification.html)\\n* [Semantic Matching](../../multimodal/semantic_matching/index.html)\\n* [Image-to-Image Semantic Matching with AutoMM](../../multimodal/semantic_matching/image2image_matching.html)\\n* [Image-Text Semantic Matching with AutoMM](../../multimodal/semantic_matching/image_text_matching.html)\\n* [Text-to-Text Semantic Matching with AutoMM](../../multimodal/semantic_matching/text2text_matching.html)\\n* [Text Semantic Search with AutoMM](../../multimodal/semantic_matching/text_semantic_search.html)\\n* [Image-Text Semantic Matching with AutoMM - Zero-Shot](../../multimodal/semantic_matching/zero_shot_img_txt_matching.html)\\n* [Advanced Topics](../../multimodal/advanced_topics/index.html)\\n* [AutoMM Problem Types And Metrics](../../multimodal/advanced_topics/problem_types_and_metrics.html)\\n* [Hyperparameter Optimization in AutoMM](../../multimodal/advanced_topics/hyperparameter_optimization.html)\\n* [Continuous Training with AutoMM](../../multimodal/advanced_topics/continuous_training.html)\\n* [Customize AutoMM](../../multimodal/advanced_topics/customization.html)\\n* [Knowledge Distillation in AutoMM](../../multimodal/advanced_topics/model_distillation.html)\\n* [Single GPU Billion-scale Model Training via Parameter-Efficient Finetuning](../../multimodal/advanced_topics/efficient_finetuning_basic.html)\\n* [Few Shot Learning with AutoMM](../../multimodal/advanced_topics/few_shot_learning.html)\\n* [Handling Class Imbalance with AutoMM - Focal Loss](../../multimodal/advanced_topics/focal_loss.html)\\n* [AutoMM Presets](../../multimodal/advanced_topics/presets.html)\\n* [Faster Prediction with TensorRT](../../multimodal/advanced_topics/tensorrt.html)\\n* [Multiple Label Columns with AutoMM](../../multimodal/advanced_topics/multiple_label_columns.html)\\n* [Cloud Training and Deployment](../../cloud_fit_deploy/index.html)\\n* [AutoGluon Cloud](../../cloud_fit_deploy/autogluon-cloud.html)\\n* [AutoGluon Tabular on SageMaker AutoPilot](../../cloud_fit_deploy/autopilot-autogluon.html)\\n* [Deploy AutoGluon Models on Serverless Templates](../../cloud_fit_deploy/cloud-aws-lambda-deployment.html)\\n* [Cloud Training and Deployment with Amazon SageMaker](../../cloud_fit_deploy/cloud-aws-sagemaker-train-deploy.html)\\nResources\\n* [Cheat Sheets](../../../cheatsheet.html)\\n* [Versions](https://auto.gluon.ai/stable/versions.html)\\n* [What's New](../../../whats_new/index.html)\\n* [Version 1.5.0](../../../whats_new/v1.5.0.html)\\n* [Version 1.4.0](../../../whats_new/v1.4.0.html)\\n* [Version 1.3.1](../../../whats_new/v1.3.1.html)\\n* [Version 1.3.0](../../../whats_new/v1.3.0.html)\\n* [Version 1.2.0](../../../whats_new/v1.2.0.html)\\n* [Version 1.1.1](../../../whats_new/v1.1.1.html)\\n* [Version 1.1.0](../../../whats_new/v1.1.0.html)\\n* [Version 1.0.0](../../../whats_new/v1.0.0.html)\\n* [Version 0.8.3](../../../whats_new/v0.8.3.html)\\n* [Version 0.8.2](../../../whats_new/v0.8.2.html)\\n* [Version 0.8.1](../../../whats_new/v0.8.1.html)\\n* [Version 0.8.0](../../../whats_new/v0.8.0.html)\\n* [Version 0.7.0](../../../whats_new/v0.7.0.ht...\", \"url\": \"https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-kaggle.html\"}, {\"title\": \"Combining numerical and text features in deep neural networks\", \"text\": \"[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe91f0237eea4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\\n\\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\nMember-only story\\n\\n# Combining numerical and text features in deep neural networks\\n\\n## How to use Keras multiple input models to train a deep neural network end to end for text and numerical data\\n\\n[![Christian Freischlag](https://miro.medium.com/v2/resize:fill:88:88/2*vcYKirJodH4oKn1Hglhr_Q.jpeg)](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Christian Freischlag](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)\\n\\n·\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------post_header-----------)\\n\\nPublished in\\n\\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n·\\n\\n4 min read\\n\\n·\\n\\nMay 19, 2020\\n\\n--\\n\\n6\\n\\nShare\\n\\nPhoto by [Marius Masalar](https://unsplash.com/@marius?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\\n\\nIn applied machine learning, data often consists of multiple data types, e.g. text and numerical data. To build a model which combines features from both domains, it is necessary to stack these features together. This post shows different solutions to combine natural language processing and traditional features in one single model in Keras (end-to-end learning).\\n\\n# Real-world data is different\\n\\nScientific data sets are usually limited to one single kind of data e.g. text, images or numerical data. It makes a lot of sense, as the goal is to compare new with existing models and approaches. In real-world scenarios data is often more diverse. To utilize end-to-end learning neural networks, instead of manually stacking models, we need to combine these different feature spaces inside the neural network.\\n\\nLet’s assume we want to solve a text classification problem and we have additional metadata for each of the documents in our corpus. In simple approaches, where our document is represented by a [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) vector, we could just add our metadata to the vector as additional…\\n\\n[![Christian Freischlag](https://miro.medium.com/v2/resize:fill:144:144/2*vcYKirJodH4oKn1Hglhr_Q.jpeg)](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------follow_profile-----------)\\n\\n[**Written by Christian Freischlag**](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[246 Followers](https://medium.com/@christian.freischlag/followers?source=post_page-----e91f0237eea4--------------------------------)\\n\\n·Writer for\\n\\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\nML/Data and Tech enthusiast \\\\| [digital-thinking.de](http://digital-thinking.de) \\\\| Senior Specialist - Natural Language Processing and AI @KPMG\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------follow_profile-----------)\\n\\n[Help](https://help.medium.com/hc/en-us?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Status](https://medium.statuspage.io/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[About](https://medium.com/about?autoplay=1&source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Press](mailto:pressinquiries@medium.com?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Blog](https://blog.medium.com/?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Text to speech](https://speechify.com/medium?source=post_page-----e91f0237eea4--------------------------------)\\n\\n[Teams](https://medium.com/business?source=post_page-----e91f0237eea4--------------------------------)\", \"url\": \"https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4?gi=dfc76a188f1a\"}, {\"title\": \"Text Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle’s top NLP competitions.\\n\\nNamely, I’ve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) – $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) – $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) – $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) – $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) – $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let’s begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let’s see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet’s see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in  R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\", \"url\": \"https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\"}, {\"title\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n←Previous\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\", \"url\": \"https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\"}, {\"title\": \"A Journey Through Kaggle Text Data Competitions From 2021 to 2023\", \"text\": \"A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\n[\\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n18 min read\\n·Apr 28, 2024\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\\nListen\\nShare\\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\\n**> Note:\\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\\n*> “It is estimated that 90% of the world’s data was generated in the last two years alone”\\n*> by [> Amount of Data Created Daily (2023)\\n](https://explodingtopics.com/blog/data-generated-per-day)\\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\\n## Methodology\\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\\nPress enter or click to view image in full size\\n![]()\\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\\n## Key Discoveries of Text Data Competitions\\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term “question answering” exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...\", \"url\": \"https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312\"}]}",
        "tool.duration_ms": 4547.759056091309
      }
    }
  ],
  "metadata": {
    "span_count": 11,
    "last_updated": "2026-01-09T23:26:49.215369"
  }
}