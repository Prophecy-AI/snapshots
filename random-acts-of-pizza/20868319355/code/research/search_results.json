{
  "query": "What ensembling strategies work best for text classification competitions on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensembling is the de\u2011facto \u201csecret sauce\u201d for winning Kaggle text\u2011classification contests.  The most reliable recipe starts with **diverse, high\u2011performing base models** \u2013 e.g., several fine\u2011tuned transformer checkpoints or zero\u2011shot LLMs \u2013 and then combines their predictions in a low\u2011bias, high\u2011variance way.  Simple **blending** (averaging the class\u2011probability vectors) already yields a consistent lift; arithmetic means work for any task, while a **geometric mean** of probabilities often improves calibration for classification problems\u30103\u3011.  When the base models are heterogeneous, a **majority\u2011vote / majority\u2011rules ensemble** can dramatically reduce hallucinations and mis\u2011classifications, as shown by a recent eLLM framework that achieved up to\u202f65\u202f% F1 improvement over the strongest single LLM\u30102\u3011.  \n\nFor larger gains, Kagglers move to **stacked generalization**: out\u2011of\u2011fold predictions from each base model become features for a second\u2011level learner.  Logistic regression remains a popular, fast meta\u2011model, but many top solutions replace it with non\u2011linear learners such as Gradient\u2011Boosted Trees, K\u2011Nearest Neighbors, or shallow neural nets to capture interactions among model outputs\u30105\u3011.  Multi\u2011level stacking (e.g., two\u2011stage stacks that blend dozens of models before feeding a final meta\u2011learner) is a hallmark of winning solutions in competitions like Homesite Quote Conversion\u30103\u3011.  Weighting each model\u2019s contribution by its validation score (or using rank\u2011based averaging) further refines the ensemble and mitigates over\u2011reliance on any single predictor\u30104\u3011.  \n\nRecent research confirms that **ensembling fine\u2011tuned language models**\u2014even when each model is already near\u2011state\u2011of\u2011the\u2011art\u2014still yields measurable performance boosts across multiple text\u2011classification datasets, encouraging broader adoption of ensembles in NLP challenges\u30101\u3011.  In practice, the best Kaggle strategy therefore combines **model diversity**, **careful blending (mean or geometric mean)**, and **stacked meta\u2011learning** (often with a non\u2011linear second\u2011level model), while using out\u2011of\u2011fold predictions to avoid leakage and weighting schemes to exploit each model\u2019s strengths.",
      "url": ""
    },
    {
      "title": "Ensembling Finetuned Language Models\n for Text Classification",
      "text": "Ensembling Finetuned Language Models for Text Classification\n# Ensembling Finetuned Language Models\nfor Text Classification\nSebastian Pineda Arango1, Maciej Janowski111footnotemark:1, Lennart Purucker1, Arber Zela1,\nFrank Hutter3,1,\u00a0Josif Grabocka2\n1University of Freiburg,2University of Technology N\u00fcrnberg,3ELLIS Institute T\u00fcbingenEqual contribution. Corresponding author:pineda@cs.uni-freiburg.de\n###### Abstract\nFinetuning is a common practice widespread across different communities to adapt pretrained models to particular tasks.\nText classification is one of these tasks for which many pretrained models are available. On the other hand, ensembles of neural networks are typically used to boost performance and provide reliable uncertainty estimates. However, ensembling pretrained models for text classification is not a well-studied avenue. In this paper, we present a metadataset with predictions from five large finetuned models on six datasets, and report results of different ensembling strategies from these predictions. Our results shed light on how ensembling can improve the performance of finetuned text classifiers and incentivize future adoption of ensembles in such tasks.\n## 1Introduction\nIn recent years, fine-tuning pretrained models has become a widely adopted technique for adapting general-purpose models to specific tasks> (Arango et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.19889v1#bib.bib2)> )\n. This practice has gained significant traction across various communities due to its effectiveness in leveraging the vast knowledge encoded in pretrained models. Among the diverse tasks that benefit from fine-tuning, text classification stands out as one of the most prevalent. With the availability of numerous pretrained models, practitioners often find themselves with a range of powerful tools to tackle text classification challenges.\nHowever, despite the widespread use of fine-tuning, the potential benefits of combining or ensembling these fine-tuned models remain underexplored.\nPrevious studies have primarily concentrated on improving individual model performance through fine-tuning techniques> (Howard &amp; Ruder, [> 2018\n](https://arxiv.org/html/2410.19889v1#bib.bib16)> )\n, leaving the exploration of ensemble strategies largely underdeveloped in this context. This oversight is particularly significant given the well-documented advantages of model ensembling in other machine learning domains> (Erickson et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.19889v1#bib.bib10)> ; Lakshminarayanan et\u00a0al., [> 2017\n](https://arxiv.org/html/2410.19889v1#bib.bib20)> )\n, which has been shown to enhance robustness and generalization. In this paper, we address the aforementioned gap by introducing a novel metadataset, which we dub: Finetuning Text Classifiers (FTC) metadataset. FTC contains predictions from various fine-tuned models on text classification tasks with various number of classes. We systematically evaluate different ensembling strategies using this metadataset, aiming to uncover insights into the potential improvements that ensembling can offer. Our results provide valuable evidence on the efficacy of these strategies, demonstrating that ensembling fine-tuned models can lead to performance gains in text classification.\n## 2Background and Related Work\nFinetuning for Text Classification.Universal Language Model Fine-tuning for Text Classification or ULMFiT> (Howard &amp; Ruder, [> 2018\n](https://arxiv.org/html/2410.19889v1#bib.bib16)> )\nconsists of finetuning language models for classification in two stages: 1) a target task unsupervised finetuning and 2) target task classifier finetuning, while using a different learning rate per layer. However, the feasibility of fully fine-tuning large pretrained language models is constrained by computational limits> (Radford et\u00a0al., [> 2018\n](https://arxiv.org/html/2410.19889v1#bib.bib31)> )\n. This has spurred the adoption of Parameter-Efficient Fine-Tuning (PEFT) methods> (Han et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.19889v1#bib.bib13)> )\n. Early strategies focused on minimal subsets of parameters such as sparse subnetworks> (Sung et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.19889v1#bib.bib37)> )\nto improve task-specific performance efficiently. Innovations such as adapter modules> (Houlsby et\u00a0al., [> 2019\n](https://arxiv.org/html/2410.19889v1#bib.bib15)> )\n, which introduce a few parameters per transformer layer but in consequence increase inference time, prompted the development of Low-Rank Adaptation (LoRA)> (Hu et\u00a0al., [> 2022\n](https://arxiv.org/html/2410.19889v1#bib.bib17)> ; Dettmers et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.19889v1#bib.bib7)> )\nthat applies low-rank updates for improved downstream task performance with reduced computational overhead. Some studies have also demonstrated that finetuned language models can be ensembled to improve performance for text classification> (Abburi et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.19889v1#bib.bib1)> )\n, but they do not provide clear insights about ensembling methods, hyperparameters, or metadata.\nEnsembling Deep Learning Models.Ensembles of neural networks> (Hansen &amp; Salamon, [> 1990\n](https://arxiv.org/html/2410.19889v1#bib.bib14)> ; Krogh &amp; Vedelsby, [> 1995\n](https://arxiv.org/html/2410.19889v1#bib.bib19)> ; Dietterich, [> 2000\n](https://arxiv.org/html/2410.19889v1#bib.bib9)> )\nhave gained significant attention in deep learning research, both for their performance-boosting capabilities and their effectiveness in uncertainty estimation. Various strategies for building ensembles exist, with deep ensembles> (Lakshminarayanan et\u00a0al., [> 2017\n](https://arxiv.org/html/2410.19889v1#bib.bib20)> )\nbeing the most popular one, which involve independently training multiple initializations of the same network.\nTheir state-of-the-art predictive uncertainty estimates have further fueled the interest in ensembles. Extensive empirical studies> (Ovadia et\u00a0al., [> 2019\n](https://arxiv.org/html/2410.19889v1#bib.bib27)> ; Gustafsson et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.19889v1#bib.bib12)> )\nhave shown that deep ensembles outperform other approaches for uncertainty estimation, such as Bayesian neural networks> (Blundell et\u00a0al., [> 2015\n](https://arxiv.org/html/2410.19889v1#bib.bib3)> ; Gal &amp; Ghahramani, [> 2016\n](https://arxiv.org/html/2410.19889v1#bib.bib11)> ; Welling &amp; Teh, [> 2011\n](https://arxiv.org/html/2410.19889v1#bib.bib40)> )\n. Similar to our work,> Seligmann et\u00a0al. (\n[> 2024\n](https://arxiv.org/html/2410.19889v1#bib.bib35)> )\nshow that finetuning pretrained models via Bayesian methods on the WILDS dataset> (Koh et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.19889v1#bib.bib18)> )\n, which contains text classification as well, can yield significant performance as compared to standard finetuning of single models.\nPost-Hoc Ensembling (PHE).PHE uses set of fitted base models{z1,\u2026,zM}subscript\ud835\udc671\u2026subscript\ud835\udc67\ud835\udc40\\\\{z\\_{1},...,z\\_{M}\\\\}{ italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT }such that every model outputszm\u2062(x),zm:\u211dD\u2192\u211dC:subscript\ud835\udc67\ud835\udc5a\ud835\udc65subscript\ud835\udc67\ud835\udc5a\u2192superscript\u211d\ud835\udc37superscript\u211d\ud835\udc36z\\_{m}(x),z\\_{m}:\\\\mathbb{R}^{D}\\\\rightarrow\\\\mathbb{R}^{C}italic\\_z start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT ( italic\\_x ) , italic\\_z start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT : blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_D end\\_POSTSUPERSCRIPT \u2192blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_C end\\_POSTSUPERSCRIPT111We assume a classification tasks withC\ud835\udc36Citalic\\_Cclasses. For regressionC=1\ud835\udc361C=1italic\\_C = 1.. These outputs are combined by an ensemblerf\u2062(z1\u2062(x),\u2026,zM\u2062(x);\u03b8)=f\u2062(z\u2062(x);\u03b8)\ud835\udc53subscript\ud835\udc671\ud835\udc65\u2026subscript\ud835\udc67\ud835\udc40\ud835\udc65\ud835\udf03\ud835\udc53\ud835\udc67\ud835\udc65\ud835\udf03f(z\\_{1}(x),...,z\\_{M}(x);\\\\theta)=f(z(x);\\\\theta)italic\\_f ( italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ( italic\\_x ) , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT ( italic\\_x ) ; italic\\_\u03b8 ) = italic\\_f ( italic\\_z ( italic\\_x ) ; italic\\_\u03b8 ),...",
      "url": "https://arxiv.org/html/2410.19889v1"
    },
    {
      "title": "Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization",
      "text": "Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization\n# Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization\nAriel Kamen\nRingCentral Inc.\nariel.kamen@ringcentral.comYakov Kamen\nRelevad Corporation\nyakov@relevad.com\n(November 1, 2025)\n###### Abstract\nThis study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8,660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.\nIndex Terms\u2014AI, large language models, LLM, multi-LLM, LLM ensemble, collective intelligence, content categorization, collective decision-making, CDM, ensemble framework, eLLM.\n## 1.\u2003\u200aIntroduction\nText categorization (also termed text classification) is a foundational problem in natural language processing. Early systems relied on human experts to assign categories using controlled vocabularies and taxonomies. This labor-intensive workflow gradually gave way to rule-based heuristics and, later, to classical machine-learning pipelines with domain-specific training and hand-engineered features. While effective within constrained settings, these approaches required substantial manual effort and did not scale easily across heterogeneous domains.\nLarge language models (LLMs)\u2014including GPT, Gemini, Claude, Grok, LLaMA, Mistral, xAI, and DeepSeek\u2014have introduced strong zero-shot capabilities, motivating efforts to replace task-specific classifiers with general-purpose LLMs. Despite notable successes of LLMs in reasoning, scientific discovery, and content generation, their performance in taxonomy-based categorization has lagged. Empirically, single models exhibit instability, category inflation, and hallucination, often producing incoherent or non-existent labels.\nThis performance gap reflects the unique nature of categorization as an information-compression task: semantically rich, unstructured inputs must be mapped into a small set of category names defined by a fixed taxonomy. Such taxonomies typically represent the real world sparsely, and individual texts may map to multiple categories with varying levels of relevance. Accurate mapping frequently requires background knowledge external to the text\u2014such as domain conventions or factual context\u2014that distinguishes professional experts from general readers. In our experiments with ten widely used zero-shot LLMs spanning multiple families and release generations, we observe a consistent performance plateau: increasing model size or algorithmic sophistication does not yield commensurate gains once a threshold is reached> (Kamen, [> 2025\n](https://arxiv.org/html/2511.15714v1#bib.bib15)> )\n.\nTo address these limitations, we developed an ensemble of LLMs (eLLM), treating each model as an independent expert and combining their predictions through a collective decision-making rule. This design is motivated by the well-established benefits of aggregation in both human and algorithmic committees, where diversity and redundancy suppress individual errors and reduce variance. By leveraging models built on diverse architectures, training paradigms, and knowledge bases, the ensemble extends the effective knowledge universe of any single LLM, enabling more comprehensive coverage of sparse taxonomies. In taxonomy classification, we find that ensembling mitigates common single-model failure modes\u2014such as hallucinations and inconsistent label usage\u2014while substantially improving both accuracy and stability. The results are striking: ensembles with sufficient diversity not only surpass the strongest individual models but, in some conditions, approach or exceed the consistency of human expert annotation. The principal trade-off lies in computational cost, since multiple model evaluations are required at inference time. This direction aligns with growing evidence that collaborative or multi-agent LLM strategies can overcome inherent limitations of single-model systems> (Feng et\u00a0al., [> 2025\n](https://arxiv.org/html/2511.15714v1#bib.bib9)> )\n. The eLLM\u2019s capacity for automatic error counterbalancing represents a practical form of self-correction\u2014an essential step toward reliable artificial intelligence.\nThis paper makes the following contributions:\n* \u2022We formalize an ensemble LLM framework for taxonomy-based categorization, specifying aggregation criteria within a collective decision-making model tailored to sparse hierarchical label spaces.\n* \u2022We instantiate eLLM with multiple committee sizes (2, 3, 5, 7, and 10 models) and evaluate them under uniform zero-shot prompting on a human-annotated corpus of 8,660 samples labeled with the IAB\u00a02.2 taxonomy.\n* \u2022We provide empirical evidence of substantial improvements over the best single model, including large gains in F1-score, reduced category inflation, and lower hallucination rates, and we analyze the computational trade-offs of ensembling.\n* \u2022We discuss conditions under which eLLM approaches human-expert performance, and the implications for scalable, reliable labeling pipelines.\nTogether, these results suggest that ensemble-based categorization is a practical and robust alternative to single-model LLM classification, particularly when adherence to a fixed taxonomy and reliability under distributional shift are required. This work builds toward a new theory of collaborative AI, where orchestrated ensembles transform unstructured chaos into ordered, expert-level insight.\n## 2.\u2003\u200aRelated Work\nOur research on ensemble large language models (eLLM) for hierarchical text categorization builds upon and integrates advances in three major domains: (1) taxonomy-based classification, (2) zero-shot categorization with large language models, and (3) collective decision-making and ensemble learning. It directly extends the findings of> (Kamen, [> 2025\n](https://arxiv.org/html/2511.15714v1#bib.bib15)> )\n, which identified structural limitations in single-model LLM categorization and established the empirical foundation for ensemble-based approaches.\n### 2.1.\u2003\u200aTaxonomy-Based Hierarchical Classification\nText categorization has been a central task in natural language processing (NLP) for several decades. Early systems relied on manual annotation and rule-based heuristics, later replaced by supervised pipelines built upon feature engineering and linear classifiers> (Sebastiani, [> 2002\n](https://arxiv.org/html/2511.15714v1#bib.bib26)> )\n. As taxonomies expanded in depth and scope, hierarchical classification emerged as a natural extension, in which labels are organized into a tree or directed acyclic graph> (Jr. et\u00a0al., [> 2011\n](https://arxiv.org/html/2511.15714v1#bib.bib14)> )\n.\nHierarchical classification requires predictions that respect structural dependencies, maintain parent\u2013child consistency, and remain valid across all levels of granularity. Various methods have been proposed to address these challenges, including structural loss functions> (Hsu et\u00a0al., [> 2009\n](https://arxiv.org/html/2511.15714v1#bib.bi...",
      "url": "https://arxiv.org/html/2511.15714v1"
    },
    {
      "title": "Model ensembling | Python",
      "text": "Model ensembling | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8\nModel ensembling | Python\nNone\n2022-06-13T00:00:00Z\n# Model ensembling\n####. Model ensembling\nSo far, we've been talking only about individual models. Now it's time to combine multiple models together.\n####. Model ensembling\nKaggle top solutions are usually not a single model, but a combination of a large number of various models. Different ways to combine models together is called model ensembling.\nFor example, here is an ensemble design for a winning solution in the Homesite Quote Conversion challenge. We can see hundreds of models with multi-level stacking and blending. Let's learn about these 'blending' and 'stacking' terms.\n####. Model blending\nThe idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models. The so-called blending approach is to just find an average of our multiple models predictions.\nSay we're solving a regression problem with a continuous target variable.\nAnd we have trained two models: A and B.\nSo, for each test observation, there are model A and model B predictions available.\n####. Model blending\nTo combine models together we can just find the predictions mean, taking the sum and divide it by two. As we see, it allows us to tweak predictions, and to take into account both model A and model B opinions.\nThat's it, such a simple ensembling method in the majority of cases will yield some improvement to our single models.\n####. Model blending\nArithmetic mean works for both regression and classification problems. However, for the classification, it's better to use a geometric mean of the class probabilities predicted.\n####. Model stacking\nThe more advanced ensembling approach is called model stacking. The idea is to train multiple single models, take their predictions and use these predictions as features in the 2nd level model.\nSo, we need to perform the following steps:\nSplit train data into two parts. Part 1 and Part 2.\nTrain multiple single models on the first part.\nMake predictions on the second part of the train data,\nand on the test data. Now, we have models predictions for both Part 2 of the train data and for the test data.\nIt means that we could create a new model using these predictions as features. This model is called the 2nd level model or meta-model.\nIts predictions on the test data give us the stacking output.\n####. Stacking example\nLet's consider all these steps on the example.\nSuppose we are given a binary classification problem with a bunch of numerical features: feature_1, feature_2 and so on to feature_N. For the train data, target variable is known.\nAnd we need to make predictions on the test data with the unknown target variable.\n####. Stacking example\nFirst of all, we split train data into two separate parts: Part 1 and Part 2.\n####. Stacking example\nThen we train multiple single models only on the first part of the train data. For example, we've trained three different models denoting them as A, B and C.\n####. Stacking example\nHaving these 3 models we make the predictions on part 2 of the train data. The columns with the predictions are denoted as A_pred, B_pred and C_pred.\nThen make the predictions on the test data as well.\n####. Stacking example\nSo, now we have models predictions for both Part 2 of the train data and for the test data.\n####. Stacking example\nIt's now possible to create a second level model using these predictions as features. It's trained on the Part 2 train data and is used to make predictions on the test data.\n####. Stacking example\nAs a result, we obtain stacking predictions for the test data. Thus, we combined individual model predictions into a single number using a 2nd level model.\n####. Let's practice!\nOK, having learned the theory, move on to build your own model ensembles!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8"
    },
    {
      "title": "KAGGLE ENSEMBLING GUIDE",
      "text": "[\u535a\u5ba2\u56ed](https://www.cnblogs.com/) [\u9996\u9875](https://www.cnblogs.com/medsci/) [\u65b0\u968f\u7b14](https://i.cnblogs.com/EditPosts.aspx?opt=1) [\u8054\u7cfb](https://msg.cnblogs.com/send/medsci) [\u8ba2\u9605](javascript:void(0)) [\u7ba1\u7406](https://i.cnblogs.com/)\n\n# [KAGGLE ENSEMBLING GUIDE](https://www.cnblogs.com/medsci/articles/9160663.html)\n\n\u8f6c\u8f7d\u81ea\u00a0 https://mlwave.com/kaggle-ensembling-guide/\n\ngithub:\u00a0 https://github.com/MLWave/Kaggle-Ensemble-Guide\n\nModel ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions.\n\nFor the first part we look at creating ensembles from submission files. The second part will look at creating ensembles through stacked generalization/blending.\n\nI answer why ensembling reduces the generalization error. Finally I show different methods of ensembling, together with their results and code to try it out for yourself.\n\n> This is how you win ML competitions: you take other peoples\u2019 work and ensemble them together.\u201d\u00a0[Vitaly Kuznetsov](http://cims.nyu.edu/~vitaly/)\u00a0NIPS2014\n\n## Creating ensembles from submission files\n\nThe most basic and convenient way to ensemble is to ensemble Kaggle submission CSV files. You only need the predictions on the test set for these methods \u2014 no need to retrain a model. This makes it a quick way to ensemble already existing model predictions, ideal when teaming up.\n\n### Voting ensembles.\n\nWe first take a look at a simple majority vote ensemble. Let\u2019s see why model ensembling reduces error rate and why it works better to ensemble low-correlated model predictions.\n\n#### Error correcting codes\n\nDuring space missions it is very important that all signals are correctly relayed.\n\nIf we have a signal in the form of a binary string like:\n\n```\n1110110011101111011111011011\n\n```\n\nand somehow this signal is corrupted (a bit is flipped) to:\n\n```\n1010110011101111011111011011\n\n```\n\nthen lives could be lost.\n\nA\u00a0[coding](http://en.wikipedia.org/wiki/Coding_theory)\u00a0solution was found in\u00a0[error correcting codes](http://en.wikipedia.org/wiki/Forward_error_correction). The simplest error correcting code is a\u00a0[repetition-code](http://en.wikipedia.org/wiki/Repetition_code): Relay the signal multiple times in equally sized chunks and have a majority vote.\n\n```\nOriginal signal:\n1110110011\n\nEncoded:\n10,3 101011001111101100111110110011\n\nDecoding:\n1010110011\n1110110011\n1110110011\n\nMajority vote:\n1110110011\n\n```\n\nSignal corruption is a very rare occurrence and often occur in small bursts. So then it figures that it is even rarer to have a corrupted majority vote.\n\nAs long as the corruption is not completely unpredictable (has a 50% chance of occurring) then signals can be repaired.\n\n#### A machine learning example\n\nSuppose we have a test set of 10 samples. The ground truth is all positive (\u201c1\u201d):\n\n```\n1111111111\n\n```\n\nWe furthermore have 3 binary classifiers (A,B,C) with a 70% accuracy. You can view these classifiers for now as pseudo-random number generators which output a \u201c1\u201d 70% of the time and a \u201c0\u201d 30% of the time.\n\nWe will now show how these pseudo-classifiers are able to obtain 78% accuracy through a voting ensemble.\n\n##### A pinch of maths\n\nFor a majority vote with 3 members we can expect 4 outcomes:\n\n```\nAll three are correct\n  0.7 * 0.7 * 0.7\n= 0.3429\n\nTwo are correct\n  0.7 * 0.7 * 0.3\n+ 0.7 * 0.3 * 0.7\n+ 0.3 * 0.7 * 0.7\n= 0.4409\n\nTwo are wrong\n  0.3 * 0.3 * 0.7\n+ 0.3 * 0.7 * 0.3\n+ 0.7 * 0.3 * 0.3\n= 0.189\n\nAll three are wrong\n  0.3 * 0.3 * 0.3\n= 0.027\n\n```\n\nWe see that most of the times (~44%) the majority vote corrects an error. This majority vote ensemble will be correct an average of ~78% (0.3429 + 0.4409 = 0.7838).\n\n#### Number of voters\n\nLike repetition codes increase in their error-correcting capability when more codes are repeated, so do ensembles usually improve when adding more ensemble members.\n\nUsing the same pinch of maths as above: a voting ensemble of 5 pseudo-random classifiers with 70% accuracy would be correct ~83% of the time. One or two errors are being corrected during ~66% of the majority votes. (0.36015 + 0.3087)\n\n#### Correlation\n\nWhen I first joined the team for KDD-cup 2014, Marios Michailidis ( [KazAnova](https://www.kaggle.com/kazanova)) proposed something peculiar. He calculated the\u00a0[Pearson correlation](http://onlinestatbook.com/2/describing_bivariate_data/pearson.html)\u00a0for all our submission files and gathered a few well-performing models which were less correlated.\n\nCreating an averaging ensemble from these diverse submissions gave us the\u00a0biggest 50-spot jump on the leaderboard. Uncorrelated submissions clearly do better when ensembled than correlated submissions. But why?\n\nTo see this, let us take 3 simple models again. The ground truth is still all 1\u2019s:\n\n```\n1111111100 = 80% accuracy\n1111111100 = 80% accuracy\n1011111100 = 70% accuracy.\n\n```\n\nThese models are highly correlated in their predictions. When we take a majority vote we see no improvement:\n\n```\n1111111100 = 80% accuracy\n\n```\n\nNow we compare to 3 less-performing, but highly uncorrelated models:\n\n```\n1111111100 = 80% accuracy\n0111011101 = 70% accuracy\n1000101111 = 60% accuracy\n\n```\n\nWhen we ensemble this with a majority vote we get:\n\n```\n1111111101 = 90% accuracy\n\n```\n\nWhich\u00a0_is_\u00a0an improvement: A lower correlation between ensemble model members seems to result in an increase in the error-correcting capability.\n\n#### Use for Kaggle: Forest Cover Type prediction\n\nMajority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.\n\nThe\u00a0[forest cover type prediction](https://www.kaggle.com/c/forest-cover-type-prediction)\u00a0challenge uses the\u00a0[UCI Forest CoverType dataset](https://archive.ics.uci.edu/ml/datasets/Covertype). The dataset has\u00a054 attributes and there are 6 classes.\n\nWe create a simple\u00a0[starter model](https://www.kaggle.com/triskelion/forest-cover-type-prediction/first-try-with-random-forests)\u00a0with a 500-tree Random Forest. We then create a few more models and pick the best performing one. For this task and our model selection an ExtraTreesClassifier works best.\n\n##### Weighing\n\nWe then use a weighted majority vote. Why weighing? Usually we want to give a better model more weight in a vote. So in our case we count the vote by the best model 3 times.\u00a0The other 4 models count for one vote each.\n\nThe reasoning is as follows: The only way for the inferior models to overrule the best model (expert) is for them to\u00a0collectively (and confidently) agree on an alternative.\n\nWe can expect this ensemble to repair a few erroneous choices by the best model, leading to a small improvement only.\u00a0That\u2019s our punishment for forgoing a democracy and creating a Plato\u2019s\u00a0_Republic_.\n\n> \u201cEvery city encompasses two cities that are at war with each other.\u201d\u00a0Plato in\u00a0The Republic\n\nTable 1. shows the result of training 5 models, and the resulting score when combining these with a weighted majority vote.\n\n| MODEL | PUBLIC ACCURACY SCORE |\n| --- | --- |\n| GradientBoostingMachine | 0.65057 |\n| RandomForest Gini | 0.75107 |\n| RandomForest Entropy | 0.75222 |\n| ExtraTrees Entropy | 0.75524 |\n| ExtraTrees Gini (Best) | 0.75571 |\n| Voting Ensemble (Democracy) | 0.75337 |\n| Voting Ensemble (3\\*Best vs. Rest) | 0.75667 |\n\n#### Use for Kaggle: CIFAR-10 Object detection in images\n\nCIFAR-10 is another multi-class classification challenge where accuracy matters.\n\nOur team leader for this challenge,\u00a0[Phil Culliton](https://www.kaggle.com/philculliton), first found the best setup to\u00a0[replicate a good model](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/)\u00a0from dr. Graham.\n\nThen he used a voting ensemble of around 30 convnets submissions (all scoring above 90% accuracy). The best\u00a0single model of the ensemble scored\u00a00.93170.\n\nA voting ensemble of 30 models scored\u00a00.94120. A ~0.01 reduction in error rate, pushing the resulting score beyond the\u00a0[estimated human clas...",
      "url": "https://www.cnblogs.com/medsci/articles/9160663.html"
    },
    {
      "title": "Using Ensembles in Kaggle Data Science Competitions- Part 3 - KDnuggets",
      "text": "# Using Ensembles in Kaggle Data Science Competitions- Part 3\n\nEarlier, we showed how to create stacked ensembles with stacked generalization and out-of-fold predictions. Now we'll learn how to implement various stacking techniques.\n\n* * *\n\n![c](https://www.kdnuggets.com/images/comment.gif)[comments](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html#comments)\n\n**By Henk van Veen**\n\n_The natural world is complex, so it figures that ensembling different models can capture more of this complexity- Ben Hamner_\n\n[![This image shows Models visualized as a network can be trained used back-propagation](https://www.kdnuggets.com/wp-content/uploads/backward-propagation-stacker-models-300x197.jpg)](https://www.kdnuggets.com/wp-content/uploads/backward-propagation-stacker-models.jpg)\n\nBackward propagation of networked models\n\n**Stacking with logistic regression:**\n\nStacking with logistic regression is one of the more basic and traditional ways of stacking. You can create predictions for the test set in one go, or take an average of the out-of-fold predictors. Either works well.\n\nThough taking the average is a clean and accurate way to do this, you might want to consider one go as that slightly lowers both model and coding complexity.\n\n**Kaggle use: \u201cPapirusy z Edhellond\u201d:**\n\nThe author uses [blend.py](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py) to compete in this classification competition.\nBy stacking 8 base models (diverse ET\u2019s, RF\u2019s and GBM\u2019s) with Logistic Regression he is able to score 0.99409 accuracy, good for first place.\n\n**Kaggle use: KDD-cup 2014:**\n\nHere the author again used [blend.py](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py) to improve a model. The model before stacking scored ~0.605 AUC, and with stacking this improved to ~0.625.\n\n**Stacking with non-linear algorithms:**\n\nPopular non-linear algorithms for stacking are GBM, KNN, NN, RF and ET. You can note a couple of interesting points here:\n\n- Non-linear stacking with the original features on multiclass problems gives surprising gains.\n- Non-linear algorithms find useful interactions between the original features and the meta-model features.\n\n**Kaggle use: TUT Headpose Estimation Challenge:**\n\nThe [TUT Headpose Estimation challenge](https://inclass.kaggle.com/c/tut-head-pose-estimation-challenge%20) can be treated as a multi-class multi-label classification challenge. For every label a separate ensemble model was trained. The key point is that stacking the predicted class probabilities with an extremely randomized trees model improved the scores. The author stacked generalization with standard models and was able to reduce the error by around 30%.\n\n**Feature weighted linear stacking:**\n\nFeature-weighted linear stacking stacks engineered meta-features together with model predictions. Linear algorithms are used to keep the resulting model fast and simple to inspect.\n\n**Quadratic linear stacking of models:**\n\nThe author framed the name \u2013 Quadratic linear stacking of models. It works similar to feature-weighted linear stacking, but creates combinations of model predictions. This technique improved the author's score in many competitions, most noticeably on the [Modeling Women\u2019s Healthcare Decision competition](http://www.drivendata.org/competitions/6/) on DrivenData.\n\n**Stacking classifiers with regressors and vice versa:**\n\nBy stacking you can use classifiers for regression problems and vice versa. Even though regression is usually not the best classifier. But it is a bit tricky.\n\n- You use binning first and turn a regression problem into a multiclass classification problem.\n- The predicted probabilities for these classes can help a stacking regressor make better predictions.\n\nA good stacker must be able to take information from the predictions.\n\n**Stacking unsupervised learned features:**\n\nYou can also stack with unsupervised learning techniques as well. A sensible popular technique is the K-Means Clustering. An interested recent addition is to use\n[t-SNE](http://lvdmaaten.github.io/tsne/):\n\n- Reduce the dataset to 2 or 3 dimensions.\n- stack this with a non-linear stacker.\n- Use a holdout set for stacking/blending (safe choice).\n\n**Online Stacking:**\n\nA good example of online (or semi-) stacking is with ad click prediction. Models trained on recent data perform better here.\n\n- So when a dataset has a temporal effect, you could use Vowpal Wabbit to train on the entire dataset.\n- Combine it with a more complex and powerful tool like XGBoost to train on the last day of data.\n- Finally stack the XGBoost predictions together with the samples and let Vowpal Wabbit do what it does best - optimizing loss functions.\n\n**Everything is a hyper-parameter:**\n\nWhen doing stacking/blending/meta-modeling, think of every action as a hyper-parameter for the stacker model.\n\nSo this makes the below simply extra parameters to be tuned to improve the ensemble performance.\n\n- Not scaling the data\n- Standard-Scaling the data\n- Minmax scaling the data\n\n**Model Selection:**\n\nYou can further optimize scores by combining multiple ensembled models.\n\n- Use averaging, voting or rank averaging on manually-selected well-performing ensembles.\n- Start with a base ensemble of 3 or so good models. Add a model when it increases the train set score the most. By allowing put-back of models, a single model may be picked multiple times (weighing).\n- Use of genetic algorithms (from Genetic Model Selection) and CV-scores as the fitness function.\n- The author uses a fully random method: Create a 100 or so ensembles from randomly selected ensembles (without placeback). Then pick the highest scoring model.\n\n**Automation:**\n\nAdding many base models along with multiple stacked ensembles can only get you so far in a competition.\nFor the rest, you might consider the below for automating:\n\n- Models visualized as a network can be trained used back-propagation\n- Consider CV-scores and their standard deviation (smaller the better).\n- There is scope to optimizing complexity/memory usage and running times.\n- Also look at making the script prefer uncorrelated model predictions when creating ensembles.\n- Consider parallelizing and distributing your automation to improve speed.\n\n**Kaggle use: [Otto product classification:](https://www.kaggle.com/c/otto-group-product-classification-challenge)**\n\nUsing the automated stacker in this competition, the author got to top 10% score without any tuning or manual model selection. Here's his approach:\n\n- For **base models** is to generate random algorithms with pure random parameters and train.\n- Wrappers can be written to make classifiers like VW, Sofia-ML, RGF, MLP and XGBoost play nicely with the Scikit-learn API.\n- For **stackers** let the script use SVM, random forests, extremely randomized trees, GBM and XGBoost with random parameters and a random subset of base models.\n- Finally average the created stackers when their fold-predictions on the train set produces a lower loss.\n\n**Why create these Frankenstein ensembles?**\n\nYou may wonder why this exercise in futility: stacking and combining 1000s of models and computational hours is insanity right? Well\u2026 yes. But these monster ensembles still have their uses:\n\n- You can win Kaggle competitions.\n- You can beat most state-of-the-art academic benchmarks with a single approach.\n- It is possible to transfer knowledge from the ensemble back to a simpler shallow model (Hinton\u2019s [Dark Knowledge](http://www.ttic.edu/dl/dark14.pdf), Caruana\u2019s [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf))\n- A good thing about ensembling is that loss of one model is not fatal for creating good predictions.\n- Automated large ensembles don't require much tuning or selection.\n- A 1% increase in accuracy may push an investment fund from making a loss, into making a little less loss. More seriously: Improving healthcare screening methods helps save lives.\n\nSee also\n[**Using Ensembles in Kaggle ...",
      "url": "https://kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html"
    },
    {
      "title": "",
      "text": "# Using Ensembles in Kaggle Data Science Competitions \u2013 Part 2\n\nAspiring to be a Top Kaggler? Learn more methods like Stacking & Blending. In the previous post we discussed about ensembling models by ways of weighing, averaging and ranks. There is much more to explore in Part-2!\n\n* * *\n\n![c](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2012%2012'%3E%3C/svg%3E)[comments](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html#comments)\n\n**By Henk van Veen**\n\n**Stacked Generalization & Blending**\n\nAveraging prediction files is nice and easy, but it\u2019s not the only method that the top Kagglers [Repetition code](https://www.kaggle.com/users) are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.\n\n**Netflix**\n\n[![This image shows netflix leaderboard results with blending hundreds of predictive models](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20213'%3E%3C/svg%3E)](https://www.kdnuggets.com/wp-content/uploads/compiling-blending-predictive-models-by-netflix-engineers.jpg)\n\nBlending hundreds of predictive models to finally cross the finish line.\n\nNetflix organized and popularized the first data science competitions. Competitors in the movie recommendation challenge really pushed the state of the art on ensemble creation, perhaps so much so that Netflix decided not to implement the winning solution in production. That one was simply too complex.\n\nNevertheless, a number of papers and novel methods resulted from this challenge:\n\n- [Feature-Weighted Linear Stacking](http://arxiv.org/pdf/0911.0460.pdf)\n- [Combining Predictions for Accurate Recommender Systems](http://elf-project.sourceforge.net/CombiningPredictionsForAccurateRecommenderSystems.pdf%20)\n- [The BigChaos Solution to the Netflix Prize](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf%20)\n\nAll are interesting, accessible and relevant reads when you want to improve your Kaggle game.\n\n**Stacked generalization**\n\nStacked generalization was introduced by Wolpert in a 1992 paper, 2 years before the seminal Breiman paper \u201cBagging Predictors\u201c. Wolpert is famous for another very popular machine learning theorem:\n\n_\u201cThere is no free lunch in search and optimization\u201c._\n\nThe basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.\n\nLet\u2019s say you want to do 2-fold stacking:\n\n- Split the train set in 2 parts: train\\_a and train\\_b\n- Fit a first-stage model on train\\_a and create predictions for train\\_b\n- Fit the same model on train\\_b and create predictions for train\\_a\n- Finally fit the model on the entire train set and create predictions for the test set.\n- Now train a second-stage stacker model on the probabilities from the first-stage model(s).\n\nA stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.\n\n**Blending**\n\nBlending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use \u201cstacked ensembling\u201d and \u201cblending\u201d interchangeably.\n\nWith blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\n\nBlending has a few benefits:\n\n- It is simpler than stacking.\n- It wards against an information leak: The generalizers and stackers use different data.\n- You do not need to share a seed for stratified folds with your teammates. Anyone can throw models in the \u2018blender\u2019 and the blender decides if it wants to keep that model or not.\n\nHowever, The cons are:\n\n- You use less data overall\n- The final model may overfit to the holdout set.\n- Your CV is more solid with stacking (calculated over more folds) than using a single small holdout set.\n\nAs for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. The author prefers stacking.\n\nIf you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage which we will explore next.\n\n[**Using Ensembles in Kaggle Data Science Competitions \u2013 Part 1**](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html)\n\n[**Using Ensembles in Kaggle Data Science Competitions \u2013 Part 3**](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html)\n\nHow are you planning to implement what you learned? Share your thoughts!\n\nOriginal: [**Kaggle Ensembling Guide**](http://mlwave.com/kaggle-ensembling-guide/) by Henk van Veen.\n\n**Related:**\n\n- [How to Lead a Data Science Contest without Reading the Data](https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html)\n- [Top 20 R Machine Learning and Data Science packages](https://www.kdnuggets.com/2015/06/top-20-r-machine-learning-packages.html)\n- [Netflix: Director \u2013 Product Analytics, Data Science and Engineering](https://www.kdnuggets.com/jobs/14/08-11-netflix-director-product-analytics-data-science-engineering.html)\n\n### More On This Topic\n\n- [Are Kaggle Competitions Useful for Real World Problems?](https://www.kdnuggets.com/are-kaggle-competitions-useful-for-real-world-problems)\n- [Learn Machine Learning 4X Faster by Participating in Competitions](https://www.kdnuggets.com/2022/01/learn-machine-learning-4x-faster-participating-competitions.html)\n- [7 Free Kaggle Micro-Courses for Data Science Beginners](https://www.kdnuggets.com/7-free-kaggle-micro-courses-for-data-science-beginners)\n- [Top 10 Kaggle Machine Learning Projects to Become Data Scientist in 2024](https://www.kdnuggets.com/top-10-kaggle-machine-learning-projects-to-become-data-scientist-in-2024)\n- [Top 4 tricks for competing on Kaggle and why you should start](https://www.kdnuggets.com/2022/05/packt-top-4-tricks-competing-kaggle-start.html)\n- [The Most Comprehensive List of Kaggle Solutions and Ideas](https://www.kdnuggets.com/2022/11/comprehensive-list-kaggle-solutions-ideas.html)\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%2056'%3E%3C/svg%3E)\n\n[Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox.](https://www.kdnuggets.com/news/subscribe.html)\n\nBy subscribing you accept KDnuggets [Privacy Policy](https://www.kdnuggets.com/news/privacy-policy.html)\n\nLeave this field empty if you're human:\n\n* * *\n\n[<= Previous post](https://www.kdnuggets.com/2015/06/top-20-r-packages.html)\n\n[Next post =>](https://www.kdnuggets.com/2015/06/open-source-interactive-analytics-overview.html)\n\n![Search](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2074%2074'%3E%3C/svg%3E)\n\n### [Latest Posts](https://www.kdnuggets.com/news/index.html)\n\n- [Breaking into Data Science: Essential Skills and How to Learn Them](https://www.kdnuggets.com/breaking-into-data-science-essential-skills-and-how-to-learn-them)\n- [Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)\n- [6 Startups Redefining 3D Workflows with OpenUSD and Generative AI](https://www.kdnuggets.com/6-startups-redefining-3d-workflows-with-openusd-and-generative-ai)\n- [What Data Scientists Should Know About OpenUSD](https://www.kdnuggets.com/what-data-scientists-should-know-about-openusd)\n- [I Took the Google Data Analytics Certification W...",
      "url": "https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p2.html"
    },
    {
      "title": "A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know",
      "text": "A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Derrick-Mwiti.png?fit=193%2C193&ssl=1)\n[Derrick Mwiti](https://neptune.ai/blog/author/derrickmwiti)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)8 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)25th April, 2025\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n[Ensemble learning techniques](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)have been proven to yield better performance on machine learning problems. We can use these techniques for regression as well as classification problems.\nThe final prediction from these ensembling techniques is obtained by combining results from several base models. Averaging, voting and stacking are some of the ways the results are combined to obtain a final prediction.\nIn this article, we will explore how ensemble learning can be used to come up with optimal machine learning models.\n## What is ensemble learning?\nEnsemble learning is a combination of several machine learning models in one problem. These models are known as weak learners. The intuition is that when you combine several weak learners, they can become strong learners.\nEach weak learner is fitted on the training set and provides predictions obtained. The final prediction result is computed by combining the results from all the weak learners.\n### Basic ensemble learning techniques\nLet\u2019s take a moment and look at simple ensemble learning techniques.\n#### Max voting\nIn classification, the prediction from each model is a vote. In max voting, the final prediction comes from the prediction with the most votes.\nLet\u2019s take an example where you have three classifiers with the following predictions:\n* classifier 1 &#8211; class A\n* classifier 2 &#8211; class B\n* classifier 3 &#8211; class B\nThe final prediction here would be class B since it has the most votes.\n#### Averaging\nIn averaging, the final output is an average of all predictions. This goes for regression problems. For example, in[random forest regression](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why), the final result is the average of the predictions from individual decision trees.\nLet\u2019s take an example of three regression models that predict the price of a commodity as follows:\n* regressor 1 &#8211; 200\n* regressor 2 &#8211;&#8211; 300\n* regressor 3 &#8211; 400\nThe final prediction would be the average of 200, 300, and 400.\n#### Weighted average\nIn weighted averaging, the base model with higher predictive power is more important. In the price prediction example, each of the regressors would be assigned a weight.\nThe sum of the weights would equal one. Let\u2019s say that the regressors are given weights of 0.35, 0.45, and 0.2 respectively. The final model prediction can be computed as follows:\n0.35 \\* 200 + 0.45\\*300 + 0.2\\*400 = 285\n## Advanced ensemble learning techniques\nAbove are simple techniques, now let\u2019s take a look at advanced techniques for ensemble learning.\n### Stacking\nStacking is the process of combining various estimators in order to reduce their biases. Predictions from each estimator are stacked together and used as input to a final estimator (usually called a*meta-model*) that computes the final prediction. Training of the final estimator happens via cross-validation.\nStacking can be done for both regression and classification problems.\n![Ensemble learning techniques](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Ensemble-learning-techniques.png?ssl=1)[*Source*](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)\nStacking can be considered to happen in the following steps:\n1. Split the data into a training and validation set,\n2. Divide the training set into K folds, for example 10,\n3. Train a base model (say SVM) on 9 folds and make predictions on the 10th fold,\n4. Repeat until you have a prediction for each fold,\n5. Fit the base model on the whole training set,\n6. Use the model to make predictions on the test set,\n7. Repeat step 3 &#8211; 6 for other base models (for example decision trees),\n8. Use predictions from the test set as features to a new model &#8211;*the meta-model,*\n9. Make final predictions on the test set using the meta model.\nWith regression problems, the values passed to the meta-model are numeric. With classification problems, they\u2019re probabilities or class labels.\n### Blending\nBlending is similar to stacking, but uses a holdout set from the training set to make predictions. So, predictions are done on the holdout set only. The predictions and holdout set are used to build a final model that makes predictions on the test set.\nYou can think of blending as a type of stacking, where the meta-model is trained on predictions made by the base model on the hold-out validation set.\nYou can consider the*blending*process to be:\n* Split the data into a test and validation set,\n* Fit base models on the validation set,\n* Make predictions on the validation and test set,\n* Use the validation set and its predictions to build a final model,\n* Make final predictions using this model.\nThe concept of blending[was made popular](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf)by the[Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize). The winning team used a blended solution to achieve a 10-fold performance improvement on Netflix\u2019s movie recommendation algorithm.\nAccording to this[Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/):\n> > \u201cBlending is a word introduced by the Netflix winners. It\u2019s very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use \u201cstacked ensembling\u201d and \u201cblending\u201d interchangeably.\n> > > > With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\u201d\n> ### Blending vs stacking\nBlending is simpler than stacking and prevents leakage of information in the model. The generalizers and the stackers use different datasets. However, blending uses less data and may lead to overfitting.\nCross-validation is more solid on stacking than blending. It\u2019s calculated over more folds, compared to using a small hold-out dataset in blending.\n### Bagging\nBagging takes random samples of data, builds learning algorithms, and uses the mean to find bagging probabilities. It\u2019s also called*bootstrap aggregating*. Bagging aggregates the res...",
      "url": "https://neptune.ai/blog/ensemble-learning-guide"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    }
  ]
}