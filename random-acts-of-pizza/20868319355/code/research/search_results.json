{
  "query": "What are winning approaches for text classification with mixed text and numerical features on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle solutions for text\u2011classification problems that also contain numeric (or categorical) columns usually treat the data as a **multimodal** problem and combine the strengths of transformer\u2011based language models with classic tabular\u2011model techniques. A very popular shortcut is to **convert every feature to a string and concatenate them with a special separator** (e.g., `[SEP]`) before feeding the whole sequence to a pre\u2011trained transformer such as BERT or RoBERTa. This \u201cfeatures\u2011to\u2011text\u201d trick lets the model attend to the numeric value as if it were a token and has been shown to give strong baselines on real Kaggle datasets (e.g., wine\u2011review classification at FreeAgent and clothing\u2011review sentiment at Kaggle)\u202f([FreeAgent](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification), [McCormick\u202fML](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).  \n\nA more \u201cprincipled\u201d variant extracts a **dense embedding for the text** with a transformer, **scales/encodes the numeric and categorical columns** (e.g., standardisation, target encoding), and then **concatenates the two representations** before passing them to a downstream classifier such as a shallow MLP, LightGBM, or logistic regression. This approach often outperforms the pure\u2011text trick because the numeric features retain their original magnitude and can be modelled with tree\u2011based learners that excel on tabular data. Implementations in Keras use a **multiple\u2011input model** \u2013 a CNN/LSTM branch for the text and a dense branch for the numbers \u2013 that are merged and trained end\u2011to\u2011end\u202f([Towards\u202fData\u202fScience](https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4)).  \n\nKaggle winners also rely on **automated multimodal toolkits** such as **AutoGluon**, which provides a \u201cTabular\u202f+\u202fText\u201d pipeline that automatically extracts transformer embeddings, handles numeric preprocessing, and builds stacked ensembles of both neural and tree models, achieving state\u2011of\u2011the\u2011art scores with minimal hand\u2011coding\u202f([AutoGluon](https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-kaggle.html)). Finally, classic **stacking/blending** remains a go\u2011to strategy: train a strong text\u2011only model (TF\u2011IDF\u202f+\u202flinear, or a fine\u2011tuned transformer) and a separate tabular model (Random Forest, LightGBM), then feed their predictions into a meta\u2011learner. This avoids leakage by using out\u2011of\u2011fold predictions and often squeezes the last few percent of metric improvement\u202f([DataScience\u202fStackExchange](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)).  \n\nIn practice, winning Kaggle pipelines therefore combine (1) **text\u2011to\u2011text concatenation** for a quick baseline, (2) **separate embedding + numeric concatenation** for higher accuracy, (3) **multi\u2011input neural nets** when end\u2011to\u2011end training is feasible, (4) **AutoGluon\u2011style multimodal ensembles** for rapid prototyping, and (5) **stacked/blended meta\u2011models** to capture complementary signals from the two modalities.",
      "url": ""
    },
    {
      "title": "Combining text with numerical and categorical features for classification",
      "text": "[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\n\n# Classification with transformer models\n\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\n\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\n\nFor example a transaction with the following set of features:\n\n|     |     |     |\n| --- | --- | --- |\n| **Description** | **Amount** | **Industry** |\n| Plastic pipes | \u00a3-7.99 | Plumbing |\n\ncould be represented by the string:\u00a0 `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We\u2019ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\n\nThis approach however doesn\u2019t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\n\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\n\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\n\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\n\n# Public wine reviews dataset\n\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores \u2013 wines between 80 and 86 (incl) were flagged as \u201cneutral\u201d, between 87 and 93 as \u201cgood\u201d and greater than 94 as \u201cexcellent\u201d.\n\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\n\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\n\n```\nimport pandas as pd\n\nwine_df = pd.read_csv(\"data/wine_data.csv\")\nbins = [0, 87, 94, np.inf]\nnames = [\"neutral\", \"good\", \"excellent\"]\n\nwine_df[\"rating\"] = pd.cut(wine_df[\"points\"], bins, labels=names)\n\n```\n\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\n\n```\nNUMERICAL_FEATURE = \"price\"\nCATEGORICAL_FEATURE = \"variety\"\nTEXT_FEATURE = \"description\"\nTARGET = \"rating\"\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\n\nwine_df = wine_df[FEATURES + [TARGET]]\n\n```\n\nWe then split our wine dataframe into an 80:20 training:evaluation split.\n\n```\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\n\n```\n\n## LightGBM classifier approach\n\n### Preprocessing\n\n#### Preprocessing numerical and categorical features\n\nIn a first step we will preprocess the numerical and categorical features using SKLearn\u2019s ColumnTransformer.\u00a0 For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\n\nWe preprocessed the categorical features by filling missing values with \u201cother\u201d and OneHot encoded them.\n\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\n\n```\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import (\n    OneHotEncoder,\n    StandardScaler,\n)\n\ndef preprocess_number():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        StandardScaler(),\n    )\n\ndef preprocess_categories():\n    return make_pipeline(\n       SimpleImputer(strategy=\"constant\", fill_value=\"other\", missing_values=np.nan),\n       OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n    )\n\ndef create_preprocessor():\n\n    transformers = [\n        (\"num_preprocessor\", preprocess_number(), [NUMERICAL_FEATURE]),\n        (\"cat_preprocessor\", preprocess_categories(), [CATEGORICAL_FEATURE]),\n    ]\n\n    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n\ncolumn_transformer = create_preprocessor()\ncolumn_transformer.set_output(transform=\"pandas\")\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\n)\n\n```\n\n#### Extracting text vector representation with a transformer model\n\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\n\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\[CLS\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\[CLS\\] token in the final layer as the vector representation of our wine descriptions.\n\nIn order to extract the \\[CLS\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\n\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\_ids, attention\\_mask and token\\_type\\_ids. Only the input\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\n\nThe code for the tokenization is shown below:\n\n```\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"distilbert-base-uncased\"\n\ndef tokenized_pytorch_tensors(\n        df: pd.DataFrame,\n        column_list: list\n    ) -> Dataset:\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    transformers_dataset = Dataset.from_pandas(df)\n\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\n        return tokenizer(\n            model_inputs_batch[TEXT_FEATURE],\n            padding=True,\n            max_length=120,\n            truncation=True,\n        )\n\n    tokenized_dataset = transformers_dataset.map(\n        tokenize,\n        batched=True,\n        batch_size=128\n    )\n\n    tokenized_dataset.set_format(\n        \"torch\",\n        columns=column_list\n    )\n\n    columns_t...",
      "url": "https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification"
    },
    {
      "title": "",
      "text": "# Combining Categorical and Numerical Features with Text in BERT\n\n29 Jun 2021\n\nIn this tutorial we\u2019ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\n\nTo help motivate our discussion, we\u2019ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\n\n- The age of the reviewer (numerical feature)\n- The number of upvotes on the review (numerical feature)\n- The department and category of the clothing item (categorical features)\n\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\n\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\n\nIn Section 2 of this Notebook, I\u2019ve implemented four different \u201cbaseline\u201d strategies which score fairly well, but which don\u2019t incorporate all of the features together.\n\nThen, in Section 3, I\u2019ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\n\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\n\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\n\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\n\nBy Chris McCormick\n\n# Contents\n\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\n\n# S1. Clothing Review Dataset\n\n## 1.1. Download & Parse\n\nRetrieve the .csv file for the dataset.\n\n```\nimport gdown\n\nprint('Downloading dataset...\\n')\n\n# Download the file.\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\n                'Womens Clothing E-Commerce Reviews.csv',\n                quiet=False)\n\nprint('\\n\\nDONE.')\n```\n\n```\nDownloading dataset...\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\nTo: /content/Womens Clothing E-Commerce Reviews.csv\n8.48MB [00:00, 48.7MB/s]\n\nDONE.\n\n```\n\nParse the dataset csv file into a pandas DataFrame.\n\n```\nimport pandas as pd\n\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n\ndata_df.head()\n```\n\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\n\n_Features_\n\n\u201c **Recommended IND**\u201d is the label we are trying to predict for this dataset. \u201c1\u201d means the reviewer recommended the product and \u201c0\u201d means they do not.\n\nThe following are _categorical_ features:\n\n- Division Name\n- Department Name\n- Class Name\n- Clothing ID\n\nAnd the following are _numerical_ features:\n\n- Age\n- Rating\n- Positive Feedback Count\n\n_Feature Analysis_\n\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\n\nNote that, in addition to the \u201cRecommended\u201d label, there is also a \u201c **Rating**\u201d column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We\u2019ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\n\n## 1.2. Train-Validation-Test Split\n\nI want to use the same training, validation, and test splits for all of the approaches we try so that it\u2019s a fair comparison.\n\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\n\nTo solve this, we\u2019re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\n\n```\nimport random\nimport numpy as np\n\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\ntrain_size = int(0.8 * len(data_df))\nval_size = int(0.1 * len(data_df))\ntest_size = len(data_df) - (train_size + val_size)\n\n# Sanity check the sizes.\nassert((train_size + val_size + test_size) == len(data_df))\n\n# Create a list of indeces for all of the samples in the dataset.\nindeces = np.arange(0, len(data_df))\n\n# Shuffle the indeces randomly.\nrandom.shuffle(indeces)\n\n# Get a list of indeces for each of the splits.\ntrain_idx = indeces[0:train_size]\nval_idx = indeces[train_size:(train_size + val_size)]\ntest_idx = indeces[(train_size + val_size):]\n\n# Sanity check\nassert(len(train_idx) == train_size)\nassert(len(test_idx) == test_size)\n\n# With these lists, we can now select the corresponding dataframe rows using,\n# e.g., train_df = data_df.iloc[train_idx]\n\nprint('  Training size: {:,}'.format(train_size))\n...",
      "url": "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert"
    },
    {
      "title": "",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Training a classifier with text and numerical features - what is the state of the art?](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years, 1 month ago\n\nModified [4 years, 1 month ago](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th?lastactivity)\n\nViewed\n2k times\n\n1\n\n$\\\\begingroup$\n\nI'm trying to build a binary classifier where the features are mostly numerical (about 20) and there are a couple of unstructured short text fields as well. What is currently considered the state of the art for combining these types of features?\n\nI've tried building a separate classifier (logistic regression, TFIDF) using the text data alone, and then including that output score of that classifier as an additional when training using the rest of the numerical features (random forest, taking care to train each classifier on different folds of the data to prevent signal leakage). It works alright, but I think a better performance is possible.\n\nA variant is to simply train two separate classifiers, one using text and the other using numerican features and then stacking those two. And finally, another idea is to use neural networks, have two input networks, one CNN/LSTM for the text, and another dense for the numerical features, and then combining them and having a single output.\n\nAre there other approaches I haven't thought of that are worth a try?\n\n- [neural-network](https://datascience.stackexchange.com/questions/tagged/neural-network)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [text-classification](https://datascience.stackexchange.com/questions/tagged/text-classification)\n\n[Share](https://datascience.stackexchange.com/q/72791)\n\n[Improve this question](https://datascience.stackexchange.com/posts/72791/edit)\n\nFollow\n\nasked Apr 22, 2020 at 16:35\n\n[![Ansari's user avatar](https://www.gravatar.com/avatar/a88d3e79b0f8dd6a9d1fafdcf68d5901?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/51/ansari)\n\n[Ansari](https://datascience.stackexchange.com/users/51/ansari) Ansari\n\n18677 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nRegarding the first question, to the best of my knowledge, there is no SOTA approach. It depends on the task.\n\nFor the second one, did you try to combine the numerical values with the TFIDF vectors, and then feed the final vector(s) to the classifier? Probably not. I usually try this way. But, if your vocabulary is too large, try to reduce it by considering top N-words, because combining small numerical vector with a huge TFIDF vector may not allow the numerical values to affect the results.\n\n[Share](https://datascience.stackexchange.com/a/72793)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/72793/edit)\n\nFollow\n\nanswered Apr 22, 2020 at 16:53\n\n[![Minions's user avatar](https://i.sstatic.net/AcfLu.jpg?s=64)](https://datascience.stackexchange.com/users/49456/minions)\n\n[Minions](https://datascience.stackexchange.com/users/49456/minions) Minions\n\n26222 silver badges1515 bronze badges\n\n$\\\\endgroup$\n\n3\n\n- $\\\\begingroup$Thanks. It feels weird to concatenate sparse features sets with dense ones, plus as you noted, there is the possibility of the tfidf features overshadowing the numerical ones. Even with limiting the vocabulary, they would still be an order of magnitude more.$\\\\endgroup$\n\n\u2013\u00a0[Ansari](https://datascience.stackexchange.com/users/51/ansari)\n\nCommentedApr 22, 2020 at 17:00\n\n- $\\\\begingroup$I don't want to answer you `yes` or `no`, but I tried this way and it works well. I tested it in my approach for a research shared task and I was ranked first. Despite that, what you've mentioned is still a scientific explanation. BTW, I do this with classical classifiers (SVM, NB, etc.), in the case of Neural Networks, I concatenate two different branches (one for text and another for the numerical values) at the very last steps of the network.$\\\\endgroup$\n\n\u2013\u00a0[Minions](https://datascience.stackexchange.com/users/49456/minions)\n\nCommentedApr 22, 2020 at 17:14\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Thanks! I'll give it a shot.$\\\\endgroup$\n\n\u2013\u00a0[Ansari](https://datascience.stackexchange.com/users/51/ansari)\n\nCommentedApr 22, 2020 at 17:19\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)\u00a0\\|\n\n0\n\n$\\\\begingroup$\n\nTo improve your results you should try other embeddings instead of tf-idf. Some examples are word2vec, FastText, Elmo, Flair or transformer-based embeddings like BERT. In the past I got great results with a single neural network which uses embeddings and numerical features as input features. But, as mentioned before, it depends on your concrete problem.\n\n[Share](https://datascience.stackexchange.com/a/72803)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/72803/edit)\n\nFollow\n\nanswered Apr 22, 2020 at 19:08\n\n[![NiklasF's user avatar](https://www.gravatar.com/avatar/b197b20632981b62287e2b729e1028fc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/95525/niklasf)\n\n[NiklasF](https://datascience.stackexchange.com/users/95525/niklasf) NiklasF\n\n1\n\n$\\\\endgroup$\n\n3\n\n- $\\\\begingroup$Did you concatenate the embeddings with the numerical features or have separate branches in your network and then combine their outputs at a more reasonable dimension?$\\\\endgroup$\n\n\u2013\u00a0[Ansari](https://datascience.stackexchange.com/users/51/ansari)\n\nCommentedApr 22, 2020 at 20:25\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Yes, I concatenated numerical features and embeddings. I got best results with flair embeddings.$\\\\endgroup$\n\n\u2013\u00a0[NiklasF](https://datascience.stackexchange.com/users/95525/niklasf)\n\nCommentedApr 23, 2020 at 19:08\n\n- $\\\\begingroup$@NiklasF Could you share a code snippet of combining text features with numerical ones, and how did you feed those into a nn? Have you used a pre-trained model, or did you create one from scratch (so you decide exactly the input of your nn).$\\\\endgroup$\n\n\u2013\u00a0[George Petropoulos](https://datascience.stackexchange.com/users/85444/george-petropoulos)\n\nCommentedApr 23, 2021 at 9:27\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f72791%2ftraining-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [neural-network](https://datascience.stackexchange.com/questions/tagged/neural-network) - [classific...",
      "url": "https://datascience.stackexchange.com/questions/72791/training-a-classifier-with-text-and-numerical-features-what-is-the-state-of-th"
    },
    {
      "title": "How to use AutoGluon for Kaggle competitions \u00b6",
      "text": "How to use AutoGluon for Kaggle competitions - AutoGluon 1.5.0 documentationContentsMenuExpandLight modeDark modeAuto light/dark, in light modeAuto light/dark, in dark mode[Skip to content](#furo-main-content)\n[\nAutoGluon 1.5.0 documentation\n](../../../index.html)\n[\n![Light Logo](../../../_static/autogluon.png)![Dark Logo](../../../_static/autogluon-w.png)\n](../../../index.html)\nGet Started\n* [Install](../../../install.html)\n* [Tabular Quick Start](../tabular-quick-start.html)\n* [Time Series Quick Start](../../timeseries/forecasting-quick-start.html)\n* [Multimodal Quick Start](../../multimodal/multimodal_prediction/multimodal-quick-start.html)\nTutorials\n* [Tabular](../index.html)\n* [Essentials](../tabular-essentials.html)\n* [In Depth](../tabular-indepth.html)\n* [Foundational Models](../tabular-foundational-models.html)\n* [How It Works](../how-it-works.html)\n* [Feature Engineering](../tabular-feature-engineering.html)\n* [Tabular + Text + Images](../tabular-multimodal.html)\n* [Advanced](index.html)\n* [Multilabel](tabular-multilabel.html)\n* [Kaggle](#)\n* [GPU](tabular-gpu.html)\n* [Custom Metrics](tabular-custom-metric.html)\n* [Custom Models](tabular-custom-model.html)\n* [Custom Models Advanced](tabular-custom-model-advanced.html)\n* [Deployment](tabular-deployment.html)\n* [Hyperparameter Optimization](tabular-hpo.html)\n* [Time Series](../../timeseries/index.html)\n* [Quick Start](../../timeseries/forecasting-quick-start.html)\n* [In Depth](../../timeseries/forecasting-indepth.html)\n* [Forecasting with Chronos-2](../../timeseries/forecasting-chronos.html)\n* [Metrics](../../timeseries/forecasting-metrics.html)\n* [Model Zoo](../../timeseries/model_zoo/index.html)\n* [Forecasting Models](../../timeseries/forecasting-model-zoo.html)\n* [Ensemble Models](../../timeseries/forecasting-ensembles.html)\n* [Advanced](../../timeseries/advanced/index.html)\n* [Custom Models](../../timeseries/advanced/forecasting-custom-model.html)\n* [Multimodal](../../multimodal/index.html)\n* [Multimodal Prediction](../../multimodal/multimodal_prediction/index.html)\n* [AutoMM for Image + Text + Tabular - Quick Start](../../multimodal/multimodal_prediction/beginner_multimodal.html)\n* [AutoMM for Entity Extraction with Text and Image - Quick Start](../../multimodal/multimodal_prediction/multimodal_ner.html)\n* [AutoMM for Text + Tabular - Quick Start](../../multimodal/multimodal_prediction/multimodal_text_tabular.html)\n* [Object Detection](../../multimodal/object_detection/index.html)\n* [Object Detection Quick Start](../../multimodal/object_detection/quick_start/index.html)\n* [AutoMM Detection - Quick Start on a Tiny COCO Format Dataset](../../multimodal/object_detection/quick_start/quick_start_coco.html)\n* [Object Detection Advanced](../../multimodal/object_detection/advanced/index.html)\n* [AutoMM Detection - Finetune on COCO Format Dataset with Customized Settings](../../multimodal/object_detection/advanced/finetune_coco.html)\n* [Object Detection Data Preparation](../../multimodal/object_detection/data_preparation/index.html)\n* [Convert Data to COCO Format](../../multimodal/object_detection/data_preparation/convert_data_to_coco_format.html)\n* [AutoMM Detection - Prepare Pothole Dataset](../../multimodal/object_detection/data_preparation/prepare_pothole.html)\n* [AutoMM Detection - Prepare Watercolor Dataset](../../multimodal/object_detection/data_preparation/prepare_watercolor.html)\n* [AutoMM Detection - Prepare COCO2017 Dataset](../../multimodal/object_detection/data_preparation/prepare_coco17.html)\n* [AutoMM Detection - Prepare Pascal VOC Dataset](../../multimodal/object_detection/data_preparation/prepare_voc.html)\n* [AutoMM Detection - Convert VOC Format Dataset to COCO Format](../../multimodal/object_detection/data_preparation/voc_to_coco.html)\n* [Image Prediction](../../multimodal/image_prediction/index.html)\n* [AutoMM for Image Classification - Quick Start](../../multimodal/image_prediction/beginner_image_cls.html)\n* [Zero-Shot Image Classification with CLIP](../../multimodal/image_prediction/clip_zeroshot.html)\n* [Image Segmentation](../../multimodal/image_segmentation/index.html)\n* [AutoMM for Semantic Segmentation - Quick Start](../../multimodal/image_segmentation/beginner_semantic_seg.html)\n* [Text Prediction](../../multimodal/text_prediction/index.html)\n* [AutoMM for Text - Quick Start](../../multimodal/text_prediction/beginner_text.html)\n* [AutoMM for Named Entity Recognition - Quick Start](../../multimodal/text_prediction/ner.html)\n* [AutoMM for Named Entity Recognition in Chinese - Quick Start](../../multimodal/text_prediction/chinese_ner.html)\n* [AutoMM for Text - Multilingual Problems](../../multimodal/text_prediction/multilingual_text.html)\n* [Document Prediction](../../multimodal/document_prediction/index.html)\n* [AutoMM for Scanned Document Classification](../../multimodal/document_prediction/document_classification.html)\n* [Classifying PDF Documents with AutoMM](../../multimodal/document_prediction/pdf_classification.html)\n* [Semantic Matching](../../multimodal/semantic_matching/index.html)\n* [Image-to-Image Semantic Matching with AutoMM](../../multimodal/semantic_matching/image2image_matching.html)\n* [Image-Text Semantic Matching with AutoMM](../../multimodal/semantic_matching/image_text_matching.html)\n* [Text-to-Text Semantic Matching with AutoMM](../../multimodal/semantic_matching/text2text_matching.html)\n* [Text Semantic Search with AutoMM](../../multimodal/semantic_matching/text_semantic_search.html)\n* [Image-Text Semantic Matching with AutoMM - Zero-Shot](../../multimodal/semantic_matching/zero_shot_img_txt_matching.html)\n* [Advanced Topics](../../multimodal/advanced_topics/index.html)\n* [AutoMM Problem Types And Metrics](../../multimodal/advanced_topics/problem_types_and_metrics.html)\n* [Hyperparameter Optimization in AutoMM](../../multimodal/advanced_topics/hyperparameter_optimization.html)\n* [Continuous Training with AutoMM](../../multimodal/advanced_topics/continuous_training.html)\n* [Customize AutoMM](../../multimodal/advanced_topics/customization.html)\n* [Knowledge Distillation in AutoMM](../../multimodal/advanced_topics/model_distillation.html)\n* [Single GPU Billion-scale Model Training via Parameter-Efficient Finetuning](../../multimodal/advanced_topics/efficient_finetuning_basic.html)\n* [Few Shot Learning with AutoMM](../../multimodal/advanced_topics/few_shot_learning.html)\n* [Handling Class Imbalance with AutoMM - Focal Loss](../../multimodal/advanced_topics/focal_loss.html)\n* [AutoMM Presets](../../multimodal/advanced_topics/presets.html)\n* [Faster Prediction with TensorRT](../../multimodal/advanced_topics/tensorrt.html)\n* [Multiple Label Columns with AutoMM](../../multimodal/advanced_topics/multiple_label_columns.html)\n* [Cloud Training and Deployment](../../cloud_fit_deploy/index.html)\n* [AutoGluon Cloud](../../cloud_fit_deploy/autogluon-cloud.html)\n* [AutoGluon Tabular on SageMaker AutoPilot](../../cloud_fit_deploy/autopilot-autogluon.html)\n* [Deploy AutoGluon Models on Serverless Templates](../../cloud_fit_deploy/cloud-aws-lambda-deployment.html)\n* [Cloud Training and Deployment with Amazon SageMaker](../../cloud_fit_deploy/cloud-aws-sagemaker-train-deploy.html)\nResources\n* [Cheat Sheets](../../../cheatsheet.html)\n* [Versions](https://auto.gluon.ai/stable/versions.html)\n* [What's New](../../../whats_new/index.html)\n* [Version 1.5.0](../../../whats_new/v1.5.0.html)\n* [Version 1.4.0](../../../whats_new/v1.4.0.html)\n* [Version 1.3.1](../../../whats_new/v1.3.1.html)\n* [Version 1.3.0](../../../whats_new/v1.3.0.html)\n* [Version 1.2.0](../../../whats_new/v1.2.0.html)\n* [Version 1.1.1](../../../whats_new/v1.1.1.html)\n* [Version 1.1.0](../../../whats_new/v1.1.0.html)\n* [Version 1.0.0](../../../whats_new/v1.0.0.html)\n* [Version 0.8.3](../../../whats_new/v0.8.3.html)\n* [Version 0.8.2](../../../whats_new/v0.8.2.html)\n* [Version 0.8.1](../../../whats_new/v0.8.1.html)\n* [Version 0.8.0](../../../whats_new/v0.8.0.html)\n* [Version 0.7.0](../../../whats_new/v0.7.0.ht...",
      "url": "https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-kaggle.html"
    },
    {
      "title": "Combining numerical and text features in deep neural networks",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe91f0237eea4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Combining numerical and text features in deep neural networks\n\n## How to use Keras multiple input models to train a deep neural network end to end for text and numerical data\n\n[![Christian Freischlag](https://miro.medium.com/v2/resize:fill:88:88/2*vcYKirJodH4oKn1Hglhr_Q.jpeg)](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\n\n[Christian Freischlag](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\n\n\u00b7\n\n4 min read\n\n\u00b7\n\nMay 19, 2020\n\n--\n\n6\n\nShare\n\nPhoto by [Marius Masalar](https://unsplash.com/@marius?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\nIn applied machine learning, data often consists of multiple data types, e.g. text and numerical data. To build a model which combines features from both domains, it is necessary to stack these features together. This post shows different solutions to combine natural language processing and traditional features in one single model in Keras (end-to-end learning).\n\n# Real-world data is different\n\nScientific data sets are usually limited to one single kind of data e.g. text, images or numerical data. It makes a lot of sense, as the goal is to compare new with existing models and approaches. In real-world scenarios data is often more diverse. To utilize end-to-end learning neural networks, instead of manually stacking models, we need to combine these different feature spaces inside the neural network.\n\nLet\u2019s assume we want to solve a text classification problem and we have additional metadata for each of the documents in our corpus. In simple approaches, where our document is represented by a [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) vector, we could just add our metadata to the vector as additional\u2026\n\n[![Christian Freischlag](https://miro.medium.com/v2/resize:fill:144:144/2*vcYKirJodH4oKn1Hglhr_Q.jpeg)](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------follow_profile-----------)\n\n[**Written by Christian Freischlag**](https://medium.com/@christian.freischlag?source=post_page-----e91f0237eea4--------------------------------)\n\n[246 Followers](https://medium.com/@christian.freischlag/followers?source=post_page-----e91f0237eea4--------------------------------)\n\n\u00b7Writer for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----e91f0237eea4--------------------------------)\n\nML/Data and Tech enthusiast \\| [digital-thinking.de](http://digital-thinking.de) \\| Senior Specialist - Natural Language Processing and AI @KPMG\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7bf9b5093a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4&user=Christian+Freischlag&userId=b7bf9b5093a4&source=post_page-b7bf9b5093a4----e91f0237eea4---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----e91f0237eea4--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----e91f0237eea4--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----e91f0237eea4--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e91f0237eea4--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----e91f0237eea4--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----e91f0237eea4--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e91f0237eea4--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e91f0237eea4--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----e91f0237eea4--------------------------------)\n\n[Teams](https://medium.com/business?source=post_page-----e91f0237eea4--------------------------------)",
      "url": "https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4?gi=dfc76a188f1a"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\nDiscover Anything**\n[\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\nHackernoon](https://hackernoon.com/)\nSignup[Write](https://app.hackernoon.com/new)\n******\n**139reads\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\nby\n**neptune.ai Jakub Czakon**\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\nAugust 23rd, 2020\nTLDR**\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\n****\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\n****\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\n[\n\u2190Previous\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\n### About Author\n[](https://hackernoon.com/u/neptuneAI_jakub)\n[**neptune.ai Jakub Czakon**@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\n#### Comments\n#### TOPICS\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\n#### THIS ARTICLE WAS FEATURED IN\n[\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\n**\nArweave\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\nViewBlock\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\n](https://www.joyk.com/dig/detail/1707491664448324)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\n](https://unni.io/domains/hackernoon.com/page/48760739)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\n](https://unni.io/domains/hackernoon.com/page/50536904)\n#### Related Stories\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\nMar 23, 2020\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[#METRICS](https://hackernoon.com/tagged/metrics)\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\nNov 11, 2023\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\nAug 15, 2023\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\n[Bala Priya C](https://hackernoon.com/u/balapriya)\nFeb 03, 2022\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\nJan 28, 2023\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\nNov 09, 2020\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\nMar 23, 2020\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[#METRICS](https://hackernoon.com/tagged/metrics)\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...",
      "url": "https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh"
    },
    {
      "title": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023",
      "text": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\n[\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n18 min read\n\u00b7Apr 28, 2024\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\nListen\nShare\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\n**> Note:\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\n*> \u201cIt is estimated that 90% of the world\u2019s data was generated in the last two years alone\u201d\n*> by [> Amount of Data Created Daily (2023)\n](https://explodingtopics.com/blog/data-generated-per-day)\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\n## Methodology\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\nPress enter or click to view image in full size\n![]()\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\n## Key Discoveries of Text Data Competitions\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term \u201cquestion answering\u201d exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...",
      "url": "https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312"
    }
  ]
}