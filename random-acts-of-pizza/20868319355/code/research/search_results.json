{
  "query": "How do Kaggle winners handle moderate class imbalance in binary classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on discussions from Kaggle participants and the tips shared from top Kaggle binary\u2011classification contests, winners typically handle a moderate class imbalance with a small, repeatable workflow ([Kaggle discussion on imbalanced data](https://www.kaggle.com/discussions/questions-and-answers/27589);\u202f[Neptune blog on Kaggle tricks](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)):\n\n1. **Pick a metric that reflects imbalance** \u2013 Use AUC\u2011ROC, log\u2011loss, or F1 rather than raw accuracy, and decide whether false positives or false negatives are more costly for the problem ([Kaggle discussion](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \n\n2. **Apply simple sampling before model training** \u2013  \n   * **Oversample** the minority class (e.g., SMOTE or random replication) to give it more representation.  \n   * **Undersample** the majority class if the dataset is large enough, to reduce bias. Both approaches are standard starting points ([Kaggle discussion](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \n\n3. **Incorporate class\u2011weighting directly in the learner** \u2013 Set `class_weight` (sklearn), `scale_pos_weight` (XGBoost), or equivalent parameters so the loss penalizes mistakes on the minority class more heavily, without altering the raw data.  \n\n4. **Use stratified cross\u2011validation** \u2013 Ensure each fold preserves the original class ratio, so performance estimates remain realistic for the imbalanced distribution.  \n\n5. **Combine multiple strategies in an ensemble** \u2013 Train separate models on differently sampled data (e.g., one on oversampled data, another on the original data with class weights) and blend their predictions; this often yields a more robust score across the imbalance spectrum.  \n\n6. **Validate on a hold\u2011out set that mirrors the test\u2011set distribution** \u2013 If the competition\u2019s test set has a different class balance, adjust the validation split or re\u2011weight predictions accordingly before final submission.  \n\n7. **Iterate quickly** \u2013 Because moderate imbalance rarely requires heavy\u2011duty techniques, winners cycle through the above steps, compare metric scores, and settle on the simplest approach that gives a solid lift.  \n\nFollowing this concise pipeline lets you address moderate class imbalance effectively while staying aligned with the practices of Kaggle competition winners.",
      "url": ""
    },
    {
      "title": "Unbalanced data and cross-validation | Kaggle",
      "text": "<div><div><p>There are two general ways of dealing with imbalanced data: 1) change the data; 2) leave the data alone but change the performance metric and/or the weight of individual data points.</p>\n<p>Changing the data means oversampling the under-represented class(es) with synthetic data points, or undersampling (thinning down) the over-represented class(es). There are various ways of doing both operations, and I will give you one <a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\"><strong>starting point</strong></a>.</p>\n<p>A second option is to leave the data alone and adjust how we do the training. It is crucial to pick a reliable performance metric for imbalanced data, and to know whether for your purposes it is worse to have too many false positives (FPs) or false negatives (FNs). Ideally, you wouldn't want to have either of the two, but for most applications it should be clear whether we want to avoid FPs or FNs with greater preference.</p>\n<p>To give you an example: you have 900 healthy patients (class 0) and 100 cancer patients (class 1), which is at least somewhat close to the true ratio of healthy and cancer-stricken people out there. If you predict class 0 with 100% accuracy and class 1 with 70% accuracy, your accuracy will be 97% for the whole dataset. That sounds tremendous until we realize that 30 out of 100 cancer patients (30%) will go undiagnosed, meaning that high false negative rate in this case will cost many lives. What if we could adjust our classification so that both classes are predicted with 97% accuracy? This would mean that 27 healthy people would be predicted to have cancer (27 FPs) and 3 cancer-stricken individual would be predicted to be healthy (3 FNs). Now, I wouldn't want to be a doctor who tells a healthy person that they have a cancer because many people have been known to do silly things when they think they are dying. Still, compared to our starting premise, most would agree that on balance it is much better to have 27 people who think they have cancer and later find out they don't, and only 3 patients who were told they are healthy even though they have cancer. The problem is that a simple accuracy as a performance metric for training would never get you to a balanced 97% accuracy. For imbalanced datasets, F1-score (a weighted average of precision and recall), area under curve for ROC and Matthews correlation coefficient may be better measures of classifier quality.</p>\n<p>See the links below for more info:</p>\n<p><a href=\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\"><strong>link1</strong></a></p>\n<p><a href=\"https://eva.fing.edu.uy/mod/resource/view.php?id=33977\"><strong>link2</strong></a></p>\n<p><a href=\"https://www.kaggle.com/c/bosch-production-line-performance/forums/t/24120/time-effort-and-reward?forumMessageId=138285#post138285\"><strong>link3</strong></a></p></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/27589"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Dealing with extremely imbalanced dataset - binary classification | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=42762aa12f6df4b246e8:1:11101)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/473546"
    },
    {
      "title": "Methods for Dealing with Imbalanced Data",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data"
    },
    {
      "title": "Best techniques and metrics for Imbalanced Dataset",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=805aeed0a20bb825939e:1:11032)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset"
    },
    {
      "title": "AIcrowd | Dealing with Class Imbalance | Posts",
      "text": "Loading\n\n#### [ADDI Alzheimers Detection Challenge](https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge)\n\n# Dealing with Class Imbalance\n\nLooking at different ways to address class imbalance in this dataset\n\nJohnowhitaker7 May 2021\n\n[20](https://www.aicrowd.com/participants/sign_in) [Open in Colab](https://colab.research.google.com/gist/aicrowd-bot/d9ed51a8b9be2026f0ff96c2b5cad9c2)\n\nIn this notebook I take a close look at some different ways we can address the difference in class balance between train and validation (and presumably test)\n\nWe look at changing sample weights, over\u00a0and under-sampling, SMOTE and some other tips and tricks.\n\nI hope you find it helpful :)\n\n# Introduction [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Introduction)\n\nSo you've made your first model for this challenge and it's getting a log loss of ~0.9 - a ways behind the leaders on 0.6X. You're starting to think about feature engineering, adding more models to your ensemble, maybe trying one of those tabular deep learning models the cool kids are talking about. **STOP!** Before any of that, there is one BIG issue we need to deal with: class (im)balance.\n\nThe validation set (and presumably the test set) has a different class distribution to the training data. In this notebook we will look at many different ways we can correct for this class imbalance - picking one of these will boost your score tremendously (we're taking ~0.66 with a single simple random forest model). So, let's dive in.\n\n# Setup [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Setup)\n\nImporting the libraries we'll be using, loading the data and getting ready to run our experiments.\n\nIn\u00a0\\[1\\]:\n\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import f1_score, log_loss\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n```\n\nIn\u00a0\\[2\\]:\n\n```\n#The training data\ndf = pd.read_csv('ds_shared_drive/train.csv')\nprint(df.shape)\ndf.head(2)\n```\n\n```\n(32777, 122)\n\n```\n\nOut\\[2\\]:\n\n| row\\_id | number\\_of\\_digits | missing\\_digit\\_1 | missing\\_digit\\_2 | missing\\_digit\\_3 | missing\\_digit\\_4 | missing\\_digit\\_5 | missing\\_digit\\_6 | missing\\_digit\\_7 | missing\\_digit\\_8 | ... | bottom\\_area\\_perc | left\\_area\\_perc | right\\_area\\_perc | hor\\_count | vert\\_count | eleven\\_ten\\_error | other\\_error | time\\_diff | centre\\_dot\\_detect | diagnosis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | S0CIXBKIUEOUBNURP | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.52617 | 0.524975 | 0.474667 | 0 | 0 | 0 | 1 | -105.0 | 0.0 | normal |\n| 1 | IW1Z4Z3H720OPW8LL | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.00081 | 0.516212 | 0.483330 | 0 | 1 | 0 | 1 | NaN | NaN | normal |\n\n2 rows \u00d7 122 columns\n\nIn\u00a0\\[3\\]:\n\n```\n# The validation data (we merge in the labels for convenience)\nval = pd.read_csv('ds_shared_drive/validation.csv')\nval = pd.merge(val, pd.read_csv('ds_shared_drive/validation_ground_truth.csv'),\n               how='left', on='row_id')\nprint(val.shape)\nval.head()\n```\n\n```\n(362, 122)\n\n```\n\nOut\\[3\\]:\n\n| row\\_id | number\\_of\\_digits | missing\\_digit\\_1 | missing\\_digit\\_2 | missing\\_digit\\_3 | missing\\_digit\\_4 | missing\\_digit\\_5 | missing\\_digit\\_6 | missing\\_digit\\_7 | missing\\_digit\\_8 | ... | bottom\\_area\\_perc | left\\_area\\_perc | right\\_area\\_perc | hor\\_count | vert\\_count | eleven\\_ten\\_error | other\\_error | time\\_diff | centre\\_dot\\_detect | diagnosis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | LA9JQ1JZMJ9D2MBZV | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.499368 | 0.553194 | 0.446447 | 0 | 0 | 0 | 1 | NaN | NaN | post\\_alzheimer |\n| 1 | PSSRCWAPTAG72A1NT | 6.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.427196 | 0.496352 | 0.503273 | 0 | 1 | 0 | 1 | NaN | NaN | normal |\n| 2 | GCTODIZJB42VCBZRZ | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.505583 | 0.503047 | 0.496615 | 1 | 0 | 0 | 0 | 0.0 | 0.0 | normal |\n| 3 | 7YMVQGV1CDB1WZFNE | 3.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | ... | 0.444633 | 0.580023 | 0.419575 | 0 | 1 | 0 | 1 | NaN | NaN | post\\_alzheimer |\n| 4 | PHEQC6DV3LTFJYIJU | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | ... | 0.395976 | 0.494990 | 0.504604 | 0 | 0 | 0 | 1 | 150.0 | 0.0 | normal |\n\n5 rows \u00d7 122 columns\n\nIn\u00a0\\[4\\]:\n\n```\n# We'll keep track of how different approaches perform\nresults = []\n```\n\n# Baseline \\#1 - Training on all data [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Baseline-%231---Training-on-all-data)\n\nThis is a case where we don't do any correction for the class imbalance. Some models will do better than others - tree-based models like CatBoost will be less sensitive than some other model types, but they will still over-estimate the probability that a given sample will fall into the majority class when making predicitons on the validation set (since the 'normal' class is so much more common in the training data).\n\nIn\u00a0\\[5\\]:\n\n```\n# Prep the data\nX = df.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny = df['diagnosis']\nX_val = val.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny_val = val['diagnosis']\n\n# Train the model\nmodel = CatBoostClassifier(verbose=False, cat_features=['intersection_pos_rel_centre'])\n\n# Evaluate on val set\nmodel.fit(X, y, eval_set = (X_val, y_val), early_stopping_rounds = 30)\n\n# Store results\nr = {'Approach':'No modifications',\n     'Log Loss':log_loss(y_val, model.predict_proba(X_val)),\n     'F1':f1_score(y_val, model.predict(X_val), average='macro')\n    }\nresults.append(r)\n\nprint(r) # Show results\n```\n\n```\n{'Approach': 'No modifications', 'Log Loss': 0.6745549053231596, 'F1': 0.2848101265822785}\n\n```\n\nA log loss of 0.67 on the validation set isn't terrible. We are using the validation set for early stopping - without that in place we get a log loss of 0.78 on our validation set and 0.8X on the leaderboard. So in a way, by using the validation set for early stopping we are already starting to combat our class balance problem... but we can do much better!\n\n# Adjusting Sample Weights [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Adjusting-Sample-Weights)\n\nModels like CatBoost allow us to assign more weight to specific samples. In this case, we use this to place less weight on samples in the over-represented classes, combating the bias introduced by the imbalance:\n\nIn\u00a0\\[6\\]:\n\n```\n# Prep the data\nX = df.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny = df['diagnosis']\nX_val = val.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny_val = val['diagnosis']\n\n#Our class weights\nweights = {\n    'normal':9/74, # Chosen based on some quick mental maths comparing the distribution of train vs val\n    'post_alzheimer':0.85,\n    'pre_alzheimer':1\n}\n\n# Applying these weights as sample weights by using Pool to wrap our training data\ntrain_data = Pool(\n    data = X,\n    label = y,\n    weight = y.map(weights), # << The important bit\n    cat_features = [list(X.columns).index('intersection_pos_rel_centre')]\n)\n\neval_data = Pool(\n    data = X_val,\n    label = y_val,\n    weight = y_val.map(lambda x: 1.0), # all validation samples get a weight of 1\n    cat_features = [list(X.columns).index('intersection_pos_rel_centre')]\n)\n\n# Train the model\nmodel = CatBoostClassifier(verbose=False)\n\n# Evaluate on val set\nmodel.fit(train_data, eval_set = eval_data, early_stopping_rounds = 30)\n\n# Store results\nr = {'Approach':'Modifying Sample Weights',\n     'Log Loss':log_loss(y_val, model.predict_proba(X_val)),\n     'F1':f1_score(y_val, model.predict(X_val), average='macro')\n    }\nresults.append(r)\n\nprint(r) # Show results\n```\n\n```\n{'Approach': 'Modifying Sample Weights', 'Log Loss': 0.5593949556085255, 'F1': 0.4727603953181...",
      "url": "https://www.aicrowd.com/showcase/dealing-with-class-imbalance"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2409.19751] Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2409.19751\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2409.19751**(cs)\n[Submitted on 29 Sep 2024]\n# Title:Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\nAuthors:[Mohamed Abdelhamid](https://arxiv.org/search/cs?searchtype=author&amp;query=Abdelhamid,+M),[Abhyuday Desai](https://arxiv.org/search/cs?searchtype=author&amp;query=Desai,+A)\nView a PDF of the paper titled Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification, by Mohamed Abdelhamid and Abhyuday Desai\n[View PDF](https://arxiv.org/pdf/2409.19751)[HTML (experimental)](https://arxiv.org/html/2409.19751v1)> > Abstract:\n> Class imbalance in binary classification tasks remains a significant challenge in machine learning, often resulting in poor performance on minority classes. This study comprehensively evaluates three widely-used strategies for handling class imbalance: Synthetic Minority Over-sampling Technique (SMOTE), Class Weights tuning, and Decision Threshold Calibration. We compare these methods against a baseline scenario of no-intervention across 15 diverse machine learning models and 30 datasets from various domains, conducting a total of 9,000 experiments. Performance was primarily assessed using the F1-score, although our study also tracked results on additional 9 metrics including F2-score, precision, recall, Brier-score, PR-AUC, and AUC. Our results indicate that all three strategies generally outperform the baseline, with Decision Threshold Calibration emerging as the most consistently effective technique. However, we observed substantial variability in the best-performing method across datasets, highlighting the importance of testing multiple approaches for specific problems. This study provides valuable insights for practitioners dealing with imbalanced datasets and emphasizes the need for dataset-specific analysis in evaluating class imbalance handling techniques. Comments:|13 pages including appendix, 4 tables|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nACMclasses:|I.2.6; I.5.1; I.5.2; I.2.m|\nCite as:|[arXiv:2409.19751](https://arxiv.org/abs/2409.19751)[cs.LG]|\n|(or[arXiv:2409.19751v1](https://arxiv.org/abs/2409.19751v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2409.19751](https://doi.org/10.48550/arXiv.2409.19751)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Abhyuday Desai [[view email](https://arxiv.org/show-email/43e3e529/2409.19751)]\n**[v1]**Sun, 29 Sep 2024 16:02:32 UTC (18 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification, by Mohamed Abdelhamid and Abhyuday Desai\n* [View PDF](https://arxiv.org/pdf/2409.19751)\n* [HTML (experimental)](https://arxiv.org/html/2409.19751v1)\n* [TeX Source](https://arxiv.org/src/2409.19751)\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2409.19751&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2409.19751&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-09](https://arxiv.org/list/cs.LG/2024-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2409.19751?context=cs)\n[cs.AI](https://arxiv.org/abs/2409.19751?context=cs.AI)\n[stat](https://arxiv.org/abs/2409.19751?context=stat)\n[stat.ML](https://arxiv.org/abs/2409.19751?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2409.19751)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2409.19751)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2409.19751)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2409.19751&amp;description=Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2409.19751&amp;title=Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to th...",
      "url": "https://arxiv.org/abs/2409.19751"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - sabamadadi/multi-strategy-ensemble-binary-classification: Robust binary data classification using a multi-strategy ensemble learning approach.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=sabamadadi/multi-strategy-ensemble-binary-classification)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[sabamadadi](https://github.com/sabamadadi)/**[multi-strategy-ensemble-binary-classification](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification)**Public\n* [Notifications](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)\n* [Star1](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)\nRobust binary data classification using a multi-strategy ensemble learning approach.\n[1star](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/stargazers)[0forks](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/forks)[Branches](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/branches)[Tags](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/tags)[Activity](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/activity)\n[Star](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)\n[Notifications](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)You must be signed in to change notification settings\n# sabamadadi/multi-strategy-ensemble-binary-classification\nmain\n[Branches](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/branches)[Tags](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/tags)\n[](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/branches)[](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[4 Commits](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/commits/main/)\n[](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/commits/main/)\n|\n[README.md](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/README.md)\n|\n[README.md](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/README.md)\n|\n|\n|\n[c\\_report.pdf](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/c_report.pdf)\n|\n[c\\_report.pdf](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/c_report.pdf)\n|\n|\n|\n[classification.ipynb](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/classification.ipynb)\n|\n[classification.ipynb](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/classification.ipynb)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Multi-Strategy Ensemble Learning for Binary Data Classification\n[](#multi-strategy-ensemble-learning-for-binary-data-classification)\n[Full Report](https://drive.google.com/file/d/1Cf21G0ubgu8sm_Y_j2p2E1bngO8KpD2U/view?usp=sharing)\n[Kaggle Competition Leaderboard](https://www.kaggle.com/competitions/datascience-4-competition/leaderboard)First Score (0.41791)\nThis project develops a robust classification pipeline for high-dimensional binary datasets. The dataset contained 64 binary features and a categorical target, with class imbalance handled using SMOTE, producing a balanced training set of 792 samples.\nFeatures were scaled using StandardScaler to ensure consistency across training and testing. Hyperparameters were optimized using Optuna with stratified k-fold cross-validation (k=3 for ensemble, k=5 for individual models).\nThe final model is a soft-voting ensemble integrating multiple Bernoulli Naive Bayes classifiers, Logistic Regression, XGBoost, Balanced Random Forest, and three standard Random Forest classifiers. Ensemble hyperparameters were specifically tuned to maximize cross-validated accuracy (\\~0.437).\nExploratory methods like CTGAN, MLP-based feature extraction, and Random Forest-based feature selection were investigated but not included in the final pipeline.\nCritical attention was given to preprocessing consistency: the test set was scaled using the training scaler to prevent distribution mismatch.\nThe ensemble approach balances diverse algorithmic strengths, improves robustness, and handles class imbalance effectively. Future directions include exploring advanced feature selection, stacking ensembles, alternative imbalance techniques, and probability calibration.\nKey takeaways: multi-strategy ensembles are effective for high-dimensional binary data, SMOTE improves minority class performance, and consistent preprocessing is crucial for valid inference.\n## About\nRobust binary data classification using a multi-strategy ensemble learning approach.\n### Topics\n[classification](https://github.com/topics/classification)\n### Resources\n[Readme](#readme-ov-file)\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n[Activity](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/activity)\n### Stars\n[**1**star](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/stargazers)\n### Watchers\n[**0**watching](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/watchers)\n### Forks\n[**0**forks](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/forks)\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification&amp;report=sabamadadi+(user))\n## [Releases](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/releases)\nNo releases published\n## [Packages0](https://github.com/users/sabamadadi/packages?repo_name=multi-strategy-ensemble-binary-classification)\nNo packages published\n## Languages\n* [Jupyter Notebook100.0%](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/search?l=jupyter-notebook)\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification"
    }
  ]
}