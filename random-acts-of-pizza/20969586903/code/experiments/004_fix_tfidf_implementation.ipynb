{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecd239c",
   "metadata": {},
   "source": [
    "# Experiment 004: Fix TF-IDF Implementation\n",
    "\n",
    "**Goal**: Address the 6 issues identified in evolver_loop3_analysis.ipynb that caused TF-IDF to only add +0.0026 AUC\n",
    "\n",
    "**Changes from exp_003**:\n",
    "1. Remove redundant simple keyword features (6 of 7 are in TF-IDF vocabulary)\n",
    "2. Reduce TF-IDF features from 12,959 to 8,000 (max_features=8000)\n",
    "3. Increase training iterations from ~50 to 2000 (num_boost_round=2000)\n",
    "4. Add class imbalance handling (scale_pos_weight=3.0)\n",
    "5. Keep all tabular features (upvotes_minus_downvotes, account_age_at_request, etc.)\n",
    "\n",
    "**Expected outcome**: 0.6413 → 0.67-0.72 AUC (+0.03 to +0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56204d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "y = train_df['requester_received_pizza'].values\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class imbalance ratio: {np.bincount(y)[0]/np.bincount(y)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tabular features (same as exp_002 baseline)\n",
    "print(\"\\nExtracting tabular features...\")\n",
    "\n",
    "def extract_features(df):\n",
    "    features = {}\n",
    "    \n",
    "    # Text length features\n",
    "    features['text_length'] = df['request_text_edit_aware'].fillna('').str.len()\n",
    "    features['word_count'] = df['request_text_edit_aware'].fillna('').str.split().str.len()\n",
    "    features['avg_word_length'] = features['text_length'] / np.maximum(features['word_count'], 1)\n",
    "    \n",
    "    # Engagement features (using available columns)\n",
    "    features['upvotes_minus_downvotes'] = df['requester_upvotes_minus_downvotes_at_request']\n",
    "    features['upvotes_plus_downvotes'] = df['requester_upvotes_plus_downvotes_at_request']\n",
    "    \n",
    "    # Activity features\n",
    "    features['num_posts_at_request'] = df['requester_number_of_posts_at_request']\n",
    "    features['num_comments_at_request'] = df['requester_number_of_comments_at_request']\n",
    "    features['comments_per_post'] = features['num_comments_at_request'] / np.maximum(features['num_posts_at_request'], 1)\n",
    "    \n",
    "    # Account age (convert from datetime to days)\n",
    "    request_time = pd.to_datetime(df['request_timestamp'])\n",
    "    account_created = pd.to_datetime(df['requester_account_created_utc'])\n",
    "    features['account_age_at_request'] = (request_time - account_created).dt.total_seconds() / (24 * 3600)\n",
    "    \n",
    "    # Subreddit count\n",
    "    features['requester_subreddits_at_request'] = df['requester_subreddits_at_request'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else 0\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "X_tabular_train = extract_features(train_df)\n",
    "X_tabular_test = extract_features(test_df)\n",
    "\n",
    "print(f\"Tabular features shape: {X_tabular_train.shape}\")\n",
    "print(f\"Tabular features: {list(X_tabular_train.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ace386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TF-IDF features (FIX #1 & #2: Remove simple keywords, reduce to 8000 features)\n",
    "print(\"\\nExtracting TF-IDF features...\")\n",
    "\n",
    "# Use same parameters as exp_003 but with max_features=8000\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,  # FIX #2: Reduced from 15000 to 8000\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "train_text = train_df['request_text_edit_aware'].fillna('')\n",
    "test_text = test_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "tfidf.fit(train_text)\n",
    "tfidf_train = tfidf.transform(train_text)\n",
    "tfidf_test = tfidf.transform(test_text)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"TF-IDF train shape: {tfidf_train.shape}\")\n",
    "print(f\"TF-IDF test shape: {tfidf_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "print(\"\\nCombining features...\")\n",
    "X_train = hstack([X_tabular_train, tfidf_train], format='csr')\n",
    "X_test = hstack([X_tabular_test, tfidf_test], format='csr')\n",
    "\n",
    "print(f\"Final train shape: {X_train.shape}\")\n",
    "print(f\"Final test shape: {X_test.shape}\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")\n",
    "print(f\"Feature-to-sample ratio: {X_train.shape[1] / X_train.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830368be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with FIXED parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING WITH FIXED TF-IDF IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(y))\n",
    "test_preds = np.zeros(len(test_df))\n",
    "auc_scores = []\n",
    "logloss_scores = []\n",
    "fold_iterations = []\n",
    "\n",
    "feature_importance_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds.split(X_train, y)):\n",
    "    print(f\"\\nFold {fold+1}/5\")\n",
    "    \n",
    "    X_tr = X_train[train_idx]\n",
    "    X_val = X_train[val_idx]\n",
    "    y_tr = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    # FIX #3 & #4: Increase iterations and add class imbalance handling\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': 3.0  # FIX #4: Add class imbalance handling\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=2000,  # FIX #3: Increased from default to 2000\n",
    "        valid_sets=[val_set],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fold_iterations.append(model.best_iteration)\n",
    "    \n",
    "    # Predictions\n",
    "    oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_preds += model.predict(X_test, num_iteration=model.best_iteration) / 5\n",
    "    \n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_val, oof_preds[val_idx])\n",
    "    logloss = log_loss(y_val, oof_preds[val_idx])\n",
    "    auc_scores.append(auc)\n",
    "    logloss_scores.append(logloss)\n",
    "    \n",
    "    print(f\"  AUC: {auc:.4f}, Log Loss: {logloss:.4f}, Iterations: {model.best_iteration}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importance_list.append(importance)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(f\"Log Loss: {np.mean(logloss_scores):.4f} ± {np.std(logloss_scores):.4f}\")\n",
    "print(f\"Avg Iterations: {np.mean(fold_iterations):.1f}\")\n",
    "print(f\"Improvement from exp_003: {np.mean(auc_scores) - 0.6413:.4f} AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average importance across folds\n",
    "mean_importance = np.mean(feature_importance_list, axis=0)\n",
    "feature_names = list(X_tabular_train.columns) + [f'tfidf_{i}' for i in range(len(tfidf.vocabulary_))]\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': mean_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "print(\"\\nTop 20 features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Check for leakage (no feature should dominate >2x)\n",
    "top_feature = importance_df.iloc[0]\n",
    "second_feature = importance_df.iloc[1]\n",
    "ratio = top_feature['importance'] / second_feature['importance']\n",
    "\n",
    "print(f\"\\nTop feature: {top_feature['feature']} ({top_feature['importance']:.1f})\")\n",
    "print(f\"Second feature: {second_feature['feature']} ({second_feature['importance']:.1f})\")\n",
    "print(f\"Importance ratio: {ratio:.2f}\")\n",
    "\n",
    "if ratio > 2.0:\n",
    "    print(\"⚠️  WARNING: Potential leakage detected! Top feature dominates >2x\")\n",
    "else:\n",
    "    print(\"✓ No leakage detected (ratio < 2.0)\")\n",
    "\n",
    "# Count TF-IDF features in top 20\n",
    "tfidf_in_top20 = sum(1 for f in importance_df.head(20)['feature'] if f.startswith('tfidf_'))\n",
    "print(f\"\\nTF-IDF features in top 20: {tfidf_in_top20}\")\n",
    "\n",
    "if tfidf_in_top20 > 0:\n",
    "    print(\"✓ TF-IDF features are being used by the model\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: TF-IDF features not appearing in top 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425640ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load sample submission to get correct format\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "print(f\"Sample submission columns: {sample_sub.columns.tolist()}\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_preds\n",
    "})\n",
    "\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Submission columns: {submission.columns.tolist()}\")\n",
    "print(f\"Prediction range: [{test_preds.min():.4f}, {test_preds.max():.4f}]\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/code/submission_candidates/candidate_008.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n✓ Submission saved to: {submission_path}\")\n",
    "\n",
    "# Verify format matches sample\n",
    "if list(submission.columns) == list(sample_sub.columns):\n",
    "    print(\"✓ Column format matches sample submission\")\n",
    "else:\n",
    "    print(\"⚠️  Column format mismatch!\")\n",
    "    print(f\"Expected: {sample_sub.columns.tolist()}\")\n",
    "    print(f\"Got: {submission.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 004 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: LightGBM with fixed TF-IDF implementation\")\n",
    "print(f\"CV AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(f\"CV Log Loss: {np.mean(logloss_scores):.4f} ± {np.std(logloss_scores):.4f}\")\n",
    "print(f\"Improvement from exp_003: {np.mean(auc_scores) - 0.6413:.4f} AUC\")\n",
    "print(f\"Features: {X_train.shape[1]} total\")\n",
    "print(f\"  - Tabular: {X_tabular_train.shape[1]}\")\n",
    "print(f\"  - TF-IDF: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Avg training iterations: {np.mean(fold_iterations):.1f}\")\n",
    "print(f\"Class imbalance handling: scale_pos_weight=3.0\")\n",
    "print(f\"\\nKey changes from exp_003:\")\n",
    "print(f\"1. ✓ Removed simple keyword features (redundant with TF-IDF)\")\n",
    "print(f\"2. ✓ Reduced TF-IDF from 12,959 to 8,000 features\")\n",
    "print(f\"3. ✓ Increased iterations from ~50 to {np.mean(fold_iterations):.0f}\")\n",
    "print(f\"4. ✓ Added scale_pos_weight=3.0 for class imbalance\")\n",
    "print(f\"\\nSubmission: {submission_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
