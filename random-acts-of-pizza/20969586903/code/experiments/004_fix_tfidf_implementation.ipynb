{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecd239c",
   "metadata": {},
   "source": [
    "# Experiment 004: Fix TF-IDF Implementation\n",
    "\n",
    "**Goal**: Address the 6 issues identified in evolver_loop3_analysis.ipynb that caused TF-IDF to only add +0.0026 AUC\n",
    "\n",
    "**Changes from exp_003**:\n",
    "1. Remove redundant simple keyword features (6 of 7 are in TF-IDF vocabulary)\n",
    "2. Reduce TF-IDF features from 12,959 to 8,000 (max_features=8000)\n",
    "3. Increase training iterations from ~50 to 2000 (num_boost_round=2000)\n",
    "4. Add class imbalance handling (scale_pos_weight=3.0)\n",
    "5. Keep all tabular features (upvotes_minus_downvotes, account_age_at_request, etc.)\n",
    "\n",
    "**Expected outcome**: 0.6413 → 0.67-0.72 AUC (+0.03 to +0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56204d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:16:29.701417Z",
     "iopub.status.busy": "2026-01-14T02:16:29.700582Z",
     "iopub.status.idle": "2026-01-14T02:16:29.706049Z",
     "shell.execute_reply": "2026-01-14T02:16:29.705332Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe2685b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:16:29.708582Z",
     "iopub.status.busy": "2026-01-14T02:16:29.707936Z",
     "iopub.status.idle": "2026-01-14T02:16:29.825246Z",
     "shell.execute_reply": "2026-01-14T02:16:29.824676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (2878, 32)\n",
      "Test shape: (1162, 17)\n",
      "Class distribution: [2163  715]\n",
      "Class imbalance ratio: 3.03\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "y = train_df['requester_received_pizza'].values\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class imbalance ratio: {np.bincount(y)[0]/np.bincount(y)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007d70f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:16:29.827338Z",
     "iopub.status.busy": "2026-01-14T02:16:29.826817Z",
     "iopub.status.idle": "2026-01-14T02:16:29.890194Z",
     "shell.execute_reply": "2026-01-14T02:16:29.889657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tabular features...\n",
      "Tabular features shape: (2878, 10)\n",
      "Tabular features: ['text_length', 'word_count', 'avg_word_length', 'upvotes_minus_downvotes', 'upvotes_plus_downvotes', 'num_posts_at_request', 'num_comments_at_request', 'comments_per_post', 'account_age_at_request', 'requester_subreddits_at_request']\n"
     ]
    }
   ],
   "source": [
    "# Extract tabular features (same as exp_002 baseline)\n",
    "print(\"\\nExtracting tabular features...\")\n",
    "\n",
    "def extract_features(df):\n",
    "    features = {}\n",
    "    \n",
    "    # Text length features\n",
    "    features['text_length'] = df['request_text_edit_aware'].fillna('').str.len()\n",
    "    features['word_count'] = df['request_text_edit_aware'].fillna('').str.split().str.len()\n",
    "    features['avg_word_length'] = features['text_length'] / np.maximum(features['word_count'], 1)\n",
    "    \n",
    "    # Engagement features (using available columns)\n",
    "    features['upvotes_minus_downvotes'] = df['requester_upvotes_minus_downvotes_at_request']\n",
    "    features['upvotes_plus_downvotes'] = df['requester_upvotes_plus_downvotes_at_request']\n",
    "    \n",
    "    # Activity features\n",
    "    features['num_posts_at_request'] = df['requester_number_of_posts_at_request']\n",
    "    features['num_comments_at_request'] = df['requester_number_of_comments_at_request']\n",
    "    features['comments_per_post'] = features['num_comments_at_request'] / np.maximum(features['num_posts_at_request'], 1)\n",
    "    \n",
    "    # Account age (already in days, no conversion needed)\n",
    "    features['account_age_at_request'] = df['requester_account_age_in_days_at_request']\n",
    "    \n",
    "    # Subreddit count\n",
    "    features['requester_subreddits_at_request'] = df['requester_subreddits_at_request'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else 0\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "X_tabular_train = extract_features(train_df)\n",
    "X_tabular_test = extract_features(test_df)\n",
    "\n",
    "print(f\"Tabular features shape: {X_tabular_train.shape}\")\n",
    "print(f\"Tabular features: {list(X_tabular_train.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ace386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:16:29.892404Z",
     "iopub.status.busy": "2026-01-14T02:16:29.891962Z",
     "iopub.status.idle": "2026-01-14T02:16:30.500995Z",
     "shell.execute_reply": "2026-01-14T02:16:30.500389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vocabulary size: 8000\n",
      "TF-IDF train shape: (2878, 8000)\n",
      "TF-IDF test shape: (1162, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Extract TF-IDF features (FIX #1 & #2: Remove simple keywords, reduce to 8000 features)\n",
    "print(\"\\nExtracting TF-IDF features...\")\n",
    "\n",
    "# Use same parameters as exp_003 but with max_features=8000\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,  # FIX #2: Reduced from 15000 to 8000\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "train_text = train_df['request_text_edit_aware'].fillna('')\n",
    "test_text = test_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "tfidf.fit(train_text)\n",
    "tfidf_train = tfidf.transform(train_text)\n",
    "tfidf_test = tfidf.transform(test_text)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"TF-IDF train shape: {tfidf_train.shape}\")\n",
    "print(f\"TF-IDF test shape: {tfidf_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d6952b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:16:30.503291Z",
     "iopub.status.busy": "2026-01-14T02:16:30.502748Z",
     "iopub.status.idle": "2026-01-14T02:16:30.515511Z",
     "shell.execute_reply": "2026-01-14T02:16:30.514995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining features...\n",
      "Final train shape: (2878, 8010)\n",
      "Final test shape: (1162, 8010)\n",
      "Total features: 8010\n",
      "Feature-to-sample ratio: 2.78\n"
     ]
    }
   ],
   "source": [
    "# Combine features\n",
    "print(\"\\nCombining features...\")\n",
    "X_train = hstack([X_tabular_train, tfidf_train], format='csr')\n",
    "X_test = hstack([X_tabular_test, tfidf_test], format='csr')\n",
    "\n",
    "print(f\"Final train shape: {X_train.shape}\")\n",
    "print(f\"Final test shape: {X_test.shape}\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")\n",
    "print(f\"Feature-to-sample ratio: {X_train.shape[1] / X_train.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830368be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:19:37.973095Z",
     "iopub.status.busy": "2026-01-14T02:19:37.972154Z",
     "iopub.status.idle": "2026-01-14T02:19:42.653305Z",
     "shell.execute_reply": "2026-01-14T02:19:42.652825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING WITH FIXED TF-IDF IMPLEMENTATION\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.612688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's auc: 0.638092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's auc: 0.639674\n",
      "  AUC: 0.6397, Log Loss: 0.5906, Iterations: 204\n",
      "\n",
      "Fold 2/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.612558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[116]\tvalid_0's auc: 0.614626\n",
      "  AUC: 0.6146, Log Loss: 0.5971, Iterations: 116\n",
      "\n",
      "Fold 3/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.620924\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's auc: 0.634345\n",
      "  AUC: 0.6343, Log Loss: 0.5922, Iterations: 55\n",
      "\n",
      "Fold 4/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's auc: 0.614591\n",
      "  AUC: 0.6146, Log Loss: 0.5791, Iterations: 23\n",
      "\n",
      "Fold 5/5\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's auc: 0.59358\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's auc: 0.611483\n",
      "  AUC: 0.6115, Log Loss: 0.5975, Iterations: 50\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "AUC: 0.6229 ± 0.0117\n",
      "Log Loss: 0.5913 ± 0.0067\n",
      "Avg Iterations: 89.6\n",
      "Improvement from exp_003: -0.0184 AUC\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with FIXED parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING WITH FIXED TF-IDF IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(y))\n",
    "test_preds = np.zeros(len(test_df))\n",
    "auc_scores = []\n",
    "logloss_scores = []\n",
    "fold_iterations = []\n",
    "\n",
    "feature_importance_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds.split(X_train, y)):\n",
    "    print(f\"\\nFold {fold+1}/5\")\n",
    "    \n",
    "    X_tr = X_train[train_idx]\n",
    "    X_val = X_train[val_idx]\n",
    "    y_tr = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    # FIX #3 & #4: Increase iterations and add class imbalance handling\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'scale_pos_weight': 3.0  # FIX #4: Add class imbalance handling\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=2000,  # FIX #3: Increased from default to 2000\n",
    "        valid_sets=[val_set],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fold_iterations.append(model.best_iteration)\n",
    "    \n",
    "    # Predictions\n",
    "    oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_preds += model.predict(X_test, num_iteration=model.best_iteration) / 5\n",
    "    \n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y_val, oof_preds[val_idx])\n",
    "    logloss = log_loss(y_val, oof_preds[val_idx])\n",
    "    auc_scores.append(auc)\n",
    "    logloss_scores.append(logloss)\n",
    "    \n",
    "    print(f\"  AUC: {auc:.4f}, Log Loss: {logloss:.4f}, Iterations: {model.best_iteration}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importance_list.append(importance)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(f\"Log Loss: {np.mean(logloss_scores):.4f} ± {np.std(logloss_scores):.4f}\")\n",
    "print(f\"Avg Iterations: {np.mean(fold_iterations):.1f}\")\n",
    "print(f\"Improvement from exp_003: {np.mean(auc_scores) - 0.6413:.4f} AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b0dacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:19:42.656106Z",
     "iopub.status.busy": "2026-01-14T02:19:42.655263Z",
     "iopub.status.idle": "2026-01-14T02:19:42.671941Z",
     "shell.execute_reply": "2026-01-14T02:19:42.671122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Top 20 features:\n",
      "                        feature  importance\n",
      "                    text_length 1123.948355\n",
      "        upvotes_minus_downvotes  997.356684\n",
      "                avg_word_length  836.148489\n",
      "         account_age_at_request  796.516267\n",
      "         upvotes_plus_downvotes  779.201386\n",
      "                     word_count  756.666638\n",
      "              comments_per_post  678.145814\n",
      "                     tfidf_5501  670.155736\n",
      "        num_comments_at_request  483.755933\n",
      "           num_posts_at_request  446.864390\n",
      "requester_subreddits_at_request  411.986361\n",
      "                     tfidf_2652  331.184582\n",
      "                     tfidf_3181  303.681979\n",
      "                     tfidf_5993  298.322479\n",
      "                     tfidf_2000  288.711841\n",
      "                     tfidf_7020  263.642773\n",
      "                     tfidf_1501  249.216445\n",
      "                     tfidf_7473  234.093467\n",
      "                     tfidf_3650  232.839103\n",
      "                     tfidf_7684  209.167999\n",
      "\n",
      "Top feature: text_length (1123.9)\n",
      "Second feature: upvotes_minus_downvotes (997.4)\n",
      "Importance ratio: 1.13\n",
      "✓ No leakage detected (ratio < 2.0)\n",
      "\n",
      "TF-IDF features in top 20: 10\n",
      "✓ TF-IDF features are being used by the model\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average importance across folds\n",
    "mean_importance = np.mean(feature_importance_list, axis=0)\n",
    "feature_names = list(X_tabular_train.columns) + [f'tfidf_{i}' for i in range(len(tfidf.vocabulary_))]\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': mean_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "print(\"\\nTop 20 features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Check for leakage (no feature should dominate >2x)\n",
    "top_feature = importance_df.iloc[0]\n",
    "second_feature = importance_df.iloc[1]\n",
    "ratio = top_feature['importance'] / second_feature['importance']\n",
    "\n",
    "print(f\"\\nTop feature: {top_feature['feature']} ({top_feature['importance']:.1f})\")\n",
    "print(f\"Second feature: {second_feature['feature']} ({second_feature['importance']:.1f})\")\n",
    "print(f\"Importance ratio: {ratio:.2f}\")\n",
    "\n",
    "if ratio > 2.0:\n",
    "    print(\"⚠️  WARNING: Potential leakage detected! Top feature dominates >2x\")\n",
    "else:\n",
    "    print(\"✓ No leakage detected (ratio < 2.0)\")\n",
    "\n",
    "# Count TF-IDF features in top 20\n",
    "tfidf_in_top20 = sum(1 for f in importance_df.head(20)['feature'] if f.startswith('tfidf_'))\n",
    "print(f\"\\nTF-IDF features in top 20: {tfidf_in_top20}\")\n",
    "\n",
    "if tfidf_in_top20 > 0:\n",
    "    print(\"✓ TF-IDF features are being used by the model\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: TF-IDF features not appearing in top 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425640ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:19:42.674359Z",
     "iopub.status.busy": "2026-01-14T02:19:42.674134Z",
     "iopub.status.idle": "2026-01-14T02:19:42.690659Z",
     "shell.execute_reply": "2026-01-14T02:19:42.689841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING SUBMISSION\n",
      "============================================================\n",
      "Sample submission shape: (1162, 2)\n",
      "Sample submission columns: ['request_id', 'requester_received_pizza']\n",
      "\n",
      "Submission shape: (1162, 2)\n",
      "Submission columns: ['request_id', 'requester_received_pizza']\n",
      "Prediction range: [0.0711, 0.7803]\n",
      "\n",
      "✓ Submission saved to: /home/code/submission_candidates/candidate_008.csv\n",
      "✓ Column format matches sample submission\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load sample submission to get correct format\n",
    "sample_sub = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "print(f\"Sample submission columns: {sample_sub.columns.tolist()}\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_preds\n",
    "})\n",
    "\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Submission columns: {submission.columns.tolist()}\")\n",
    "print(f\"Prediction range: [{test_preds.min():.4f}, {test_preds.max():.4f}]\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/code/submission_candidates/candidate_008.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n✓ Submission saved to: {submission_path}\")\n",
    "\n",
    "# Verify format matches sample\n",
    "if list(submission.columns) == list(sample_sub.columns):\n",
    "    print(\"✓ Column format matches sample submission\")\n",
    "else:\n",
    "    print(\"⚠️  Column format mismatch!\")\n",
    "    print(f\"Expected: {sample_sub.columns.tolist()}\")\n",
    "    print(f\"Got: {submission.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1c933b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:19:42.692870Z",
     "iopub.status.busy": "2026-01-14T02:19:42.692629Z",
     "iopub.status.idle": "2026-01-14T02:19:42.699493Z",
     "shell.execute_reply": "2026-01-14T02:19:42.698758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 004 SUMMARY\n",
      "============================================================\n",
      "Model: LightGBM with fixed TF-IDF implementation\n",
      "CV AUC: 0.6229 ± 0.0117\n",
      "CV Log Loss: 0.5913 ± 0.0067\n",
      "Improvement from exp_003: -0.0184 AUC\n",
      "Features: 8010 total\n",
      "  - Tabular: 10\n",
      "  - TF-IDF: 8000\n",
      "Avg training iterations: 89.6\n",
      "Class imbalance handling: scale_pos_weight=3.0\n",
      "\n",
      "Key changes from exp_003:\n",
      "1. ✓ Removed simple keyword features (redundant with TF-IDF)\n",
      "2. ✓ Reduced TF-IDF from 12,959 to 8,000 features\n",
      "3. ✓ Increased iterations from ~50 to 90\n",
      "4. ✓ Added scale_pos_weight=3.0 for class imbalance\n",
      "\n",
      "Submission: /home/code/submission_candidates/candidate_008.csv\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 004 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: LightGBM with fixed TF-IDF implementation\")\n",
    "print(f\"CV AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(f\"CV Log Loss: {np.mean(logloss_scores):.4f} ± {np.std(logloss_scores):.4f}\")\n",
    "print(f\"Improvement from exp_003: {np.mean(auc_scores) - 0.6413:.4f} AUC\")\n",
    "print(f\"Features: {X_train.shape[1]} total\")\n",
    "print(f\"  - Tabular: {X_tabular_train.shape[1]}\")\n",
    "print(f\"  - TF-IDF: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Avg training iterations: {np.mean(fold_iterations):.1f}\")\n",
    "print(f\"Class imbalance handling: scale_pos_weight=3.0\")\n",
    "print(f\"\\nKey changes from exp_003:\")\n",
    "print(f\"1. ✓ Removed simple keyword features (redundant with TF-IDF)\")\n",
    "print(f\"2. ✓ Reduced TF-IDF from 12,959 to 8,000 features\")\n",
    "print(f\"3. ✓ Increased iterations from ~50 to {np.mean(fold_iterations):.0f}\")\n",
    "print(f\"4. ✓ Added scale_pos_weight=3.0 for class imbalance\")\n",
    "print(f\"\\nSubmission: {submission_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
