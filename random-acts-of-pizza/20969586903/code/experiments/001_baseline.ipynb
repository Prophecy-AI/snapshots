{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea5068f",
   "metadata": {},
   "source": [
    "# Baseline Experiment: LightGBM with Simple Features\n",
    "\n",
    "This notebook creates a baseline model using LightGBM with simple features extracted from both text and tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e849edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc33156",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Convert to DataFrames for easier manipulation\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b469f2e",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"Extract simple features from text\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return {\n",
    "            'text_length': 0,\n",
    "            'word_count': 0,\n",
    "            'sentence_count': 0,\n",
    "            'exclamation_count': 0,\n",
    "            'question_count': 0,\n",
    "            'caps_ratio': 0,\n",
    "            'has_please': 0,\n",
    "            'has_thank': 0,\n",
    "            'has_sorry': 0,\n",
    "            'has_because': 0,\n",
    "            'has_family': 0,\n",
    "            'has_kids': 0,\n",
    "            'has_work': 0,\n",
    "            'has_money': 0,\n",
    "            'has_pay': 0,\n",
    "            'has_hungry': 0,\n",
    "            'has_food': 0,\n",
    "            'has_help': 0,\n",
    "            'has_emergency': 0\n",
    "        }\n",
    "    \n",
    "    # Basic text stats\n",
    "    text_length = len(text)\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentence_count = len([s for s in sentences if s.strip()])\n",
    "    \n",
    "    # Punctuation\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    \n",
    "    # Capitalization ratio\n",
    "    caps_count = sum(1 for c in text if c.isupper())\n",
    "    caps_ratio = caps_count / text_length if text_length > 0 else 0\n",
    "    \n",
    "    # Keywords (indicators of politeness, need, etc.)\n",
    "    text_lower = text.lower()\n",
    "    has_please = int('please' in text_lower)\n",
    "    has_thank = int(any(word in text_lower for word in ['thank', 'thanks', 'thx']))\n",
    "    has_sorry = int('sorry' in text_lower)\n",
    "    has_because = int('because' in text_lower)\n",
    "    has_family = int(any(word in text_lower for word in ['family', 'fam']))\n",
    "    has_kids = int(any(word in text_lower for word in ['kid', 'kids', 'child', 'children', 'baby', 'babies']))\n",
    "    has_work = int(any(word in text_lower for word in ['work', 'job', 'employ', 'unemploy', 'laid off']))\n",
    "    has_money = int(any(word in text_lower for word in ['money', 'cash', 'dollar', 'broke', 'poor', 'bills']))\n",
    "    has_pay = int(any(word in text_lower for word in ['pay', 'paycheck', 'salary', 'wage']))\n",
    "    has_hungry = int(any(word in text_lower for word in ['hungry', 'starv', 'hunger', 'food']))\n",
    "    has_food = int('food' in text_lower)\n",
    "    has_help = int(any(word in text_lower for word in ['help', 'need', 'desperate', 'urgent']))\n",
    "    has_emergency = int(any(word in text_lower for word in ['emergency', 'crisis', 'urgent', 'desperate']))\n",
    "    \n",
    "    return {\n",
    "        'text_length': text_length,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'question_count': question_count,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'has_please': has_please,\n",
    "        'has_thank': has_thank,\n",
    "        'has_sorry': has_sorry,\n",
    "        'has_because': has_because,\n",
    "        'has_family': has_family,\n",
    "        'has_kids': has_kids,\n",
    "        'has_work': has_work,\n",
    "        'has_money': has_money,\n",
    "        'has_pay': has_pay,\n",
    "        'has_hungry': has_hungry,\n",
    "        'has_food': has_food,\n",
    "        'has_help': has_help,\n",
    "        'has_emergency': has_emergency\n",
    "    }\n",
    "\n",
    "def extract_metadata_features(df):\n",
    "    \"\"\"Extract features from metadata\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Account age features\n",
    "    features['account_age_at_request'] = df['requester_account_age_in_days_at_request']\n",
    "    features['account_age_at_retrieval'] = df['requester_account_age_in_days_at_retrieval']\n",
    "    features['account_age_diff'] = df['requester_account_age_in_days_at_retrieval'] - df['requester_account_age_in_days_at_request']\n",
    "    \n",
    "    # Activity features\n",
    "    features['comments_at_request'] = df['requester_number_of_comments_at_request']\n",
    "    features['comments_at_retrieval'] = df['requester_number_of_comments_at_retrieval']\n",
    "    features['comments_diff'] = df['requester_number_of_comments_at_retrieval'] - df['requester_number_of_comments_at_request']\n",
    "    \n",
    "    features['posts_at_request'] = df['requester_number_of_posts_at_request']\n",
    "    features['posts_at_retrieval'] = df['requester_number_of_posts_at_retrieval']\n",
    "    features['posts_diff'] = df['requester_number_of_posts_at_retrieval'] - df['requester_number_of_posts_at_request']\n",
    "    \n",
    "    features['comments_in_raop_at_request'] = df['requester_number_of_comments_in_raop_at_request']\n",
    "    features['comments_in_raop_at_retrieval'] = df['requester_number_of_comments_in_raop_at_retrieval']\n",
    "    features['posts_in_raop_at_request'] = df['requester_number_of_posts_on_raop_at_request']\n",
    "    features['posts_in_raop_at_retrieval'] = df['requester_number_of_posts_on_raop_at_retrieval']\n",
    "    \n",
    "    # Upvotes/downvotes features\n",
    "    features['upvotes_minus_downvotes_at_request'] = df['requester_upvotes_minus_downvotes_at_request']\n",
    "    features['upvotes_minus_downvotes_at_retrieval'] = df['requester_upvotes_minus_downvotes_at_retrieval']\n",
    "    features['upvotes_plus_downvotes_at_request'] = df['requester_upvotes_plus_downvotes_at_request']\n",
    "    features['upvotes_plus_downvotes_at_retrieval'] = df['requester_upvotes_plus_downvotes_at_retrieval']\n",
    "    \n",
    "    # Request features\n",
    "    features['request_upvotes'] = df['number_of_upvotes_of_request_at_retrieval']\n",
    "    features['request_downvotes'] = df['number_of_downvotes_of_request_at_retrieval']\n",
    "    features['request_comments'] = df['request_number_of_comments_at_retrieval']\n",
    "    \n",
    "    # Time features\n",
    "    features['unix_timestamp'] = df['unix_timestamp_of_request']\n",
    "    features['unix_timestamp_utc'] = df['unix_timestamp_of_request_utc']\n",
    "    features['timestamp_diff'] = df['unix_timestamp_of_request_utc'] - df['unix_timestamp_of_request']\n",
    "    \n",
    "    # Boolean features\n",
    "    features['post_was_edited'] = df['post_was_edited'].astype(int)\n",
    "    \n",
    "    # User flair encoding\n",
    "    flair_map = {'None': 0, 'shroom': 1, 'PIF': 2}\n",
    "    features['user_flair'] = df['requester_user_flair'].map(flair_map).fillna(0)\n",
    "    \n",
    "    # Days since first post on RAOP\n",
    "    features['days_since_first_raop_at_request'] = df['requester_days_since_first_post_on_raop_at_request']\n",
    "    features['days_since_first_raop_at_retrieval'] = df['requester_days_since_first_post_on_raop_at_retrieval']\n",
    "    \n",
    "    # Number of subreddits\n",
    "    features['number_of_subreddits_at_request'] = df['requester_number_of_subreddits_at_request']\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract text features from both text fields\n",
    "print(\"Extracting text features from request_text...\")\n",
    "text_features_train = pd.DataFrame([extract_text_features(text) for text in train_df['request_text']])\n",
    "text_features_test = pd.DataFrame([extract_text_features(text) for text in test_df['request_text']])\n",
    "\n",
    "print(\"Extracting text features from request_text_edit_aware...\")\n",
    "text_edit_features_train = pd.DataFrame([extract_text_features(text) for text in train_df['request_text_edit_aware']])\n",
    "text_edit_features_test = pd.DataFrame([extract_text_features(text) for text in test_df['request_text_edit_aware']])\n",
    "\n",
    "# Rename columns to distinguish between text fields\n",
    "text_features_train.columns = [f\"text_{col}\" for col in text_features_train.columns]\n",
    "text_features_test.columns = [f\"text_{col}\" for col in text_features_test.columns]\n",
    "text_edit_features_train.columns = [f\"text_edit_{col}\" for col in text_edit_features_train.columns]\n",
    "text_edit_features_test.columns = [f\"text_edit_{col}\" for col in text_edit_features_test.columns]\n",
    "\n",
    "print(\"Extracting metadata features...\")\n",
    "metadata_features_train = extract_metadata_features(train_df)\n",
    "metadata_features_test = extract_metadata_features(test_df)\n",
    "\n",
    "# Combine all features\n",
    "X_train = pd.concat([text_features_train, text_edit_features_train, metadata_features_train], axis=1)\n",
    "X_test = pd.concat([text_features_test, text_edit_features_test, metadata_features_test], axis=1)\n",
    "\n",
    "y_train = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Feature columns: {X_train.columns.tolist()[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae794679",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation strategy\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store predictions\n",
    "train_predictions = np.zeros(len(X_train))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Training LightGBM model with {n_folds}-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store predictions\n",
    "    train_predictions[valid_idx] = val_pred\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_score:.4f}\")\n",
    "    \n",
    "    # Feature importance for this fold\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    fold_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"Top 5 features in fold {fold + 1}:\")\n",
    "    print(fold_importance.head())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Overall CV score\n",
    "cv_score = roc_auc_score(y_train, train_predictions)\n",
    "print(f\"\\nOverall CV AUC: {cv_score:.4f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58453e",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315894f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the predictions are in the correct format (0-1 range)\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].clip(0, 1)\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format matches sample\n",
    "sample_submission = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(f\"\\nSample submission columns: {sample_submission.columns.tolist()}\")\n",
    "print(f\"Our submission columns: {submission.columns.tolist()}\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_submission.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64d3e5",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance across all folds\n",
    "# For simplicity, we'll use the last fold's importance as a proxy\n",
    "final_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns.tolist(),\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(final_importance.head(20))\n",
    "\n",
    "# Save feature importance\n",
    "final_importance.to_csv('/home/code/experiments/001_feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to: /home/code/experiments/001_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
