{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9180662",
   "metadata": {},
   "source": [
    "# Baseline Experiment: LightGBM with Simple Features\n",
    "\n",
    "This notebook creates a baseline model using LightGBM with simple features extracted from both text and tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b15348",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"Extract simple features from text\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return {\n",
    "            'text_length': 0,\n",
    "            'word_count': 0,\n",
    "            'sentence_count': 0,\n",
    "            'exclamation_count': 0,\n",
    "            'question_count': 0,\n",
    "            'caps_ratio': 0,\n",
    "            'has_please': 0,\n",
    "            'has_thank': 0,\n",
    "            'has_sorry': 0,\n",
    "            'has_because': 0,\n",
    "            'has_family': 0,\n",
    "            'has_kids': 0,\n",
    "            'has_work': 0,\n",
    "            'has_money': 0,\n",
    "            'has_pay': 0,\n",
    "            'has_hungry': 0,\n",
    "            'has_food': 0,\n",
    "            'has_help': 0,\n",
    "            'has_emergency': 0\n",
    "        }\n",
    "    \n",
    "    # Basic text stats\n",
    "    text_length = len(text)\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentence_count = len([s for s in sentences if s.strip()])\n",
    "    \n",
    "    # Punctuation\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    caps_ratio = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Keywords (lowercase for matching)\n",
    "    text_lower = text.lower()\n",
    "    has_please = int('please' in text_lower)\n",
    "    has_thank = int(any(word in text_lower for word in ['thank', 'thanks', 'thx']))\n",
    "    has_sorry = int('sorry' in text_lower)\n",
    "    has_because = int('because' in text_lower)\n",
    "    has_family = int(any(word in text_lower for word in ['family', 'fam', 'parent', 'mother', 'father']))\n",
    "    has_kids = int(any(word in text_lower for word in ['kid', 'kids', 'child', 'children', 'baby', 'babies']))\n",
    "    has_work = int(any(word in text_lower for word in ['work', 'job', 'employ', 'money']))\n",
    "    has_money = int(any(word in text_lower for word in ['money', 'cash', 'dollar', 'bucks']))\n",
    "    has_pay = int(any(word in text_lower for word in ['pay', 'payment', 'bills', 'rent']))\n",
    "    has_hungry = int(any(word in text_lower for word in ['hungry', 'starving', 'hunger']))\n",
    "    has_food = int(any(word in text_lower for word in ['food', 'pizza', 'eat', 'meal']))\n",
    "    has_help = int('help' in text_lower)\n",
    "    has_emergency = int(any(word in text_lower for word in ['emergency', 'urgent', 'desperate', 'crisis']))\n",
    "    \n",
    "    return {\n",
    "        'text_length': text_length,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'question_count': question_count,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'has_please': has_please,\n",
    "        'has_thank': has_thank,\n",
    "        'has_sorry': has_sorry,\n",
    "        'has_because': has_because,\n",
    "        'has_family': has_family,\n",
    "        'has_kids': has_kids,\n",
    "        'has_work': has_work,\n",
    "        'has_money': has_money,\n",
    "        'has_pay': has_pay,\n",
    "        'has_hungry': has_hungry,\n",
    "        'has_food': has_food,\n",
    "        'has_help': has_help,\n",
    "        'has_emergency': has_emergency\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fae867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features from both text fields\n",
    "print(\"Extracting text features from request_text...\")\n",
    "text_features_train = pd.DataFrame([extract_text_features(text) for text in train_df['request_text']])\n",
    "text_features_test = pd.DataFrame([extract_text_features(text) for text in test_df['request_text']])\n",
    "\n",
    "print(\"Extracting text features from request_text_edit_aware...\")\n",
    "text_edit_features_train = pd.DataFrame([extract_text_features(text) for text in train_df['request_text_edit_aware']])\n",
    "text_edit_features_test = pd.DataFrame([extract_text_features(text) for text in test_df['request_text_edit_aware']])\n",
    "\n",
    "# Rename columns to distinguish between text fields\n",
    "text_features_train.columns = [f\"text_{col}\" for col in text_features_train.columns]\n",
    "text_features_test.columns = [f\"text_{col}\" for col in text_features_test.columns]\n",
    "text_edit_features_train.columns = [f\"text_edit_{col}\" for col in text_edit_features_train.columns]\n",
    "text_edit_features_test.columns = [f\"text_edit_{col}\" for col in text_edit_features_test.columns]\n",
    "\n",
    "print(f\"Text features shape: {text_features_train.shape}\")\n",
    "print(f\"Text edit features shape: {text_edit_features_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac33bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tabular features (excluding text fields and target)\n",
    "tabular_features = [col for col in train_df.columns if col not in ['request_text', 'request_text_edit_aware', 'requester_received_pizza', 'request_id']]\n",
    "\n",
    "print(f\"Tabular features: {len(tabular_features)}\")\n",
    "print(f\"Tabular feature names: {tabular_features}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = pd.concat([\n",
    "    train_df[tabular_features].reset_index(drop=True),\n",
    "    text_features_train.reset_index(drop=True),\n",
    "    text_edit_features_train.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    test_df[tabular_features].reset_index(drop=True),\n",
    "    text_features_test.reset_index(drop=True),\n",
    "    text_edit_features_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "y_train = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(f\"Final training features shape: {X_train.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c36375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation strategy\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store predictions\n",
    "train_predictions = np.zeros(len(X_train))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Training LightGBM model with {n_folds}-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': RANDOM_SEED\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    train_predictions[valid_idx] = val_pred\n",
    "    \n",
    "    # Calculate validation score (log loss since metric direction is false)\n",
    "    val_score = log_loss(y_val, val_pred)\n",
    "    cv_scores.append(val_score)\n",
    "    print(f\"Fold {fold + 1} validation log loss: {val_score:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / n_folds\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_cv_score = log_loss(y_train, train_predictions)\n",
    "print(f\"\\nOverall CV log loss: {overall_cv_score:.4f}\")\n",
    "print(f\"Mean CV log loss: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477488e",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50307eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the predictions are in the correct format (0-1 range)\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].clip(0, 1)\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format matches sample\n",
    "sample_submission = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(f\"\\nSample submission columns: {sample_submission.columns.tolist()}\")\n",
    "print(f\"Our submission columns: {submission.columns.tolist()}\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_submission.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826bc70",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d29289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance across all folds\n",
    "# For simplicity, we'll use the last fold's importance as a proxy\n",
    "final_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns.tolist(),\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(final_importance.head(20))\n",
    "\n",
    "# Save feature importance\n",
    "final_importance.to_csv('/home/code/experiments/001_feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to: /home/code/experiments/001_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
