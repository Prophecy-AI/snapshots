{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9180662",
   "metadata": {},
   "source": [
    "# Baseline Experiment: LightGBM with Simple Features\n",
    "\n",
    "This notebook creates a baseline model using LightGBM with simple features extracted from both text and tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b15348",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features from request_text_edit_aware only (test data doesn't have request_text)\n",
    "print(\"Extracting text features from request_text_edit_aware...\")\n",
    "text_edit_features_train = pd.DataFrame([extract_text_features(text) for text in train_df['request_text_edit_aware']])\n",
    "text_edit_features_test = pd.DataFrame([extract_text_features(text) for text in test_df['request_text_edit_aware']])\n",
    "\n",
    "# Rename columns\n",
    "text_edit_features_train.columns = [f\"text_edit_{col}\" for col in text_edit_features_train.columns]\n",
    "text_edit_features_test.columns = [f\"text_edit_{col}\" for col in text_edit_features_test.columns]\n",
    "\n",
    "print(f\"Text edit features shape: {text_edit_features_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fae867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tabular features (excluding text fields and target)\n",
    "tabular_features = [col for col in train_df.columns if col not in ['request_text', 'request_text_edit_aware', 'requester_received_pizza', 'request_id']]\n",
    "\n",
    "print(f\"Tabular features: {len(tabular_features)}\")\n",
    "\n",
    "# Prepare feature matrices - only use features that exist in both train and test\n",
    "common_features = [col for col in tabular_features if col in test_df.columns]\n",
    "print(f\"Common tabular features: {len(common_features)}\")\n",
    "\n",
    "X_train = pd.concat([\n",
    "    train_df[common_features].reset_index(drop=True),\n",
    "    text_edit_features_train.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    test_df[common_features].reset_index(drop=True),\n",
    "    text_edit_features_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "y_train = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(f\"Final training features shape: {X_train.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac33bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tabular features (excluding text fields and target)\n",
    "tabular_features = [col for col in train_df.columns if col not in ['request_text', 'request_text_edit_aware', 'requester_received_pizza', 'request_id']]\n",
    "\n",
    "print(f\"Tabular features: {len(tabular_features)}\")\n",
    "print(f\"Tabular feature names: {tabular_features}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = pd.concat([\n",
    "    train_df[tabular_features].reset_index(drop=True),\n",
    "    text_features_train.reset_index(drop=True),\n",
    "    text_edit_features_train.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    test_df[tabular_features].reset_index(drop=True),\n",
    "    text_features_test.reset_index(drop=True),\n",
    "    text_edit_features_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "y_train = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(f\"Final training features shape: {X_train.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c36375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation strategy\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store predictions\n",
    "train_predictions = np.zeros(len(X_train))\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Training LightGBM model with {n_folds}-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': RANDOM_SEED\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    train_predictions[valid_idx] = val_pred\n",
    "    \n",
    "    # Calculate validation score (log loss since metric direction is false)\n",
    "    val_score = log_loss(y_val, val_pred)\n",
    "    cv_scores.append(val_score)\n",
    "    print(f\"Fold {fold + 1} validation log loss: {val_score:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / n_folds\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_cv_score = log_loss(y_train, train_predictions)\n",
    "print(f\"\\nOverall CV log loss: {overall_cv_score:.4f}\")\n",
    "print(f\"Mean CV log loss: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477488e",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50307eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the predictions are in the correct format (0-1 range)\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].clip(0, 1)\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format matches sample\n",
    "sample_submission = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(f\"\\nSample submission columns: {sample_submission.columns.tolist()}\")\n",
    "print(f\"Our submission columns: {submission.columns.tolist()}\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_submission.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826bc70",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d29289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance across all folds\n",
    "# For simplicity, we'll use the last fold's importance as a proxy\n",
    "final_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns.tolist(),\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(final_importance.head(20))\n",
    "\n",
    "# Save feature importance\n",
    "final_importance.to_csv('/home/code/experiments/001_feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to: /home/code/experiments/001_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
