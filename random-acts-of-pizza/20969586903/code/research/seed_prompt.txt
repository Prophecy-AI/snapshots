## Current Status
- Best CV: 0.6413 AUC from exp_003_add_tfidf
- Gap to gold: 0.3377 AUC points (need ~53% relative improvement)
- Experiments above gold: 0
- Analysis complete: evolver_loop3_analysis.ipynb identified root causes of TF-IDF failure

## Response to Evaluator

**Technical verdict was TRUSTWORTHY. I agree completely.**

- **Leakage successfully removed**: Confirmed by 0.1463 AUC drop and feature importance ratio of 1.31x (well below 2x threshold)
- **Metrics properly tracked**: Both AUC and log loss reported as requested ✓
- **Validation stable**: Low std dev (0.0297) indicates reliable CV estimates
- **Built-in leakage detection**: Smart approach that should continue

**Evaluator's top priority: Add TF-IDF text features. I STRONGLY AGREE but with critical refinements based on analysis.**

The analysis in `exploration/evolver_loop3_analysis.ipynb` reveals why exp_003 failed:

**Key concerns raised by evaluator:**

1. **Text features underdeveloped**: Confirmed. TF-IDF only added +0.0026 AUC vs expected +0.03-0.08. Analysis shows 6 compounding issues.

2. **TF-IDF implementation flawed**: Analysis identified:
   - Redundancy: 6 of 7 simple keywords ARE in TF-IDF vocabulary (duplicate signal)
   - Too many features: 12,959 features for 2,878 samples = 4.5x ratio
   - Insufficient training: 48 avg iterations far too low for sparse features
   - No class imbalance handling

3. **Performance gap massive**: Absolutely correct. Need 0.34 AUC improvement. Current approach won't close this gap.

**My synthesis**: The evaluator is right that TF-IDF is critical, but exp_003's implementation was flawed. We must fix the technical issues before expecting gains. The analysis provides a clear roadmap.

## Data Understanding

**Reference notebooks**: See `exploration/evolver_loop3_analysis.ipynb` for detailed failure analysis

**Key patterns to exploit:**

1. **TF-IDF Redundancy (CRITICAL)**: Simple keyword features ('thank', 'sorry', 'family', 'work', 'hungry', 'help') are already captured in TF-IDF vocabulary. Keeping both creates duplicate signal, not new information. **Solution: Remove simple keywords when using TF-IDF.**

2. **Feature Count Issue**: 12,959 TF-IDF features for 2,878 samples creates 4.5x feature-to-sample ratio. Sparsity is only 0.32% non-zero. High overfitting risk. **Solution: Reduce to 8,000 features max.**

3. **Severe Underfitting**: Average 48 training iterations for 12,988 sparse features is far too low. Model not learning TF-IDF patterns. **Solution: Increase num_boost_round to 2000-3000.**

4. **Class Imbalance**: 75/25 negative/positive ratio not addressed. Quick test shows minimal AUC impact (0.0001), but may improve calibration. **Solution: Add scale_pos_weight=3.0.**

5. **Feature Selection Promising**: Chi-square selection shows best performance with 1,000-3,000 TF-IDF features (AUC 0.617-0.633), outperforming full 12,959 features. Top 1,000 features achieve 0.6331 ± 0.0111 AUC. **Solution: Use feature selection to reduce noise.**

6. **TF-IDF Parameters Reasonable**: min_df=2, max_df=0.95 settings are appropriate. 5,149 tokens appear in only 1 document (removed by min_df=2). No need to change.

## Recommended Approaches (Priority Order)

### 1. FIX TF-IDF IMPLEMENTATION (CRITICAL - HIGHEST PRIORITY)
**Because**: Analysis proves exp_003 failed due to 6 technical issues, not conceptual flaws. Fixing these should yield expected +0.03 to +0.08 AUC gain.

**Specific implementation**:
- **Remove redundant simple keyword features**: Delete keyword_please, keyword_thank, keyword_sorry, keyword_family, keyword_work, keyword_hungry, keyword_help when using TF-IDF
- **Reduce TF-IDF features**: Set max_features=8000 (down from 15000)
- **Increase training iterations**: Set num_boost_round=2000, early_stopping_rounds=50 (was default ~50 total)
- **Add class imbalance handling**: scale_pos_weight=3.0
- **Keep all tabular features**: upvotes_minus_downvotes, account_age_at_request, etc.
- **Use same TF-IDF parameters**: min_df=2, max_df=0.95, ngram_range=(1,2) - these are appropriate

**Expected outcome**: 0.6413 → 0.67-0.72 AUC (+0.03 to +0.08 as originally expected)

### 2. ADD FEATURE SELECTION (HIGH PRIORITY)
**Because**: Analysis shows top 1K-3K features outperform full set (0.6331 AUC vs lower). Removes noisy, low-importance features.

**Specific implementation**:
- Use chi-square (SelectKBest) to select top 3000 TF-IDF features
- Alternative: Use LightGBM feature importance after first run, keep top 3000
- Apply selection on training data only (fit on train, transform on train/test)
- Combine with reduced tabular features (remove redundant keywords)

**Expected outcome**: More robust model, potentially +0.01-0.02 AUC beyond fix #1

### 3. ENGINEER TEMPORAL FEATURES (MEDIUM PRIORITY)
**Because**: Research shows patterns exist (hour 14, Thursday, account age). Not yet implemented.

**Specific implementation**:
- hour_of_day (0-23) from request_timestamp
- day_of_week (0-6, Monday=0)
- is_weekend (binary)
- is_evening (6PM-10PM, binary)
- account_age_buckets (binned: <1m, 1-3m, 3m-1y, 1-3y, >3y)
- requester_activity_ratios (comments/posts per day)

**Expected outcome**: +0.01 to +0.03 AUC

### 4. ADD SENTIMENT/READABILITY FEATURES (MEDIUM PRIORITY)
**Because**: Text quality matters beyond word presence. Politeness, emotion, clarity all affect success.

**Specific implementation**:
- Sentiment polarity and subjectivity (TextBlob)
- Flesch-Kincaid readability score
- SMOG grade (measure of complex words)
- Emotion lexicon scores (desperation, gratitude, urgency)
- Politeness markers count (beyond simple keywords)

**Expected outcome**: +0.01 to +0.02 AUC

### 5. TRY CATBOOST (EXPLORATORY)
**Because**: Evaluator recommended comparing algorithms. CatBoost handles categoricals natively.

**Specific implementation**:
- Train CatBoost on same features as LightGBM (after fixes #1-2)
- Use default parameters initially
- Compare AUC and log loss
- If better, consider for ensemble later
- Keep categorical features as strings (don't LabelEncode)

**Expected outcome**: Unknown - could be better or worse. Information gain.

### 6. HYPERPARAMETER TUNING (LOWER PRIORITY - AFTER FEATURES)
**Because**: Current parameters may not be optimal for high-dimensional sparse data.

**Specific implementation**:
- learning_rate: try 0.01-0.1 (currently 0.05)
- num_leaves: try 31-63 (currently 31)
- min_child_samples: try 20-50 (currently 20)
- feature_fraction: try 0.7-0.9 (currently 0.9)
- Use Bayesian optimization or random search

**Expected outcome**: +0.005 to +0.015 AUC (diminishing returns vs feature engineering)

## What NOT to Try

- **Don't keep simple keyword features with TF-IDF**: Proven redundant by analysis
- **Don't use 12,959 TF-IDF features**: Proven too many, causes overfitting
- **Don't use <2000 iterations**: Proven insufficient for sparse features
- **Don't change TF-IDF parameters yet**: min_df=2, max_df=0.95 are appropriate
- **Don't add interactions yet**: Focus on fixing TF-IDF implementation first
- **Don't ensemble yet**: Need better base models first
- **Don't use request_text field**: Test data doesn't have it, only use request_text_edit_aware

## Validation Notes

- **CV scheme**: Continue 5-fold stratified CV (proven stable)
- **Metrics**: Report BOTH AUC (target: 0.979080) and log loss
- **Leakage check**: After each experiment, verify no feature dominates (>2x importance ratio)
- **Feature importance**: Track top 10 features to ensure TF-IDF features appear and aren't redundant
- **Stability**: Run with seed=42 for reproducibility
- **Expected trajectory**:
  - After fix #1 (TF-IDF implementation): 0.67-0.72 AUC
  - After fix #2 (feature selection): 0.68-0.74 AUC
  - After temporal features: 0.69-0.77 AUC
  - Still need ensembling to reach 0.979 target

## Confidence Assessment

- **Very high confidence**: TF-IDF fixes will work (analysis proves root causes)
- **High confidence**: Feature selection will help (chi-square results show improvement)
- **Medium-high confidence**: Temporal features will add value (patterns confirmed)
- **Medium confidence**: Sentiment/readability features (theoretically sound, need testing)
- **Lower confidence**: CatBoost vs LightGBM (need head-to-head comparison)
- **Low confidence**: Hyperparameter tuning ROI (diminishing returns at this stage)

**Strategy**: Fix TF-IDF implementation first (highest ROI), then feature selection, then temporal features. These three changes should get us to 0.69-0.77 AUC. Then we'll need model ensembling to close the remaining gap to 0.979.