{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a97705c",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis: Understanding Data Patterns\n",
    "\n",
    "This notebook explores the Random Acts of Pizza dataset to identify patterns and inform our feature engineering strategy.\n",
    "\n",
    "Key questions:\n",
    "1. Confirm data leakage in giver_username_if_known\n",
    "2. Analyze text patterns that predict success\n",
    "3. Identify temporal patterns\n",
    "4. Understand requester behavior patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef93a07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:49:21.398910Z",
     "iopub.status.busy": "2026-01-13T21:49:21.398087Z",
     "iopub.status.idle": "2026-01-13T21:49:21.490867Z",
     "shell.execute_reply": "2026-01-13T21:49:21.490277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data shape: (2878, 32)\n",
      "Columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    0.751564\n",
      "True     0.248436\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading training data...\")\n",
    "train_path = \"/home/data/train.json\"\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bf7fc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading training data...\")\n",
    "train_path = \"/home/data/train.json\"\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8e43fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:50:12.310866Z",
     "iopub.status.busy": "2026-01-13T21:50:12.310068Z",
     "iopub.status.idle": "2026-01-13T21:50:12.331172Z",
     "shell.execute_reply": "2026-01-13T21:50:12.330567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYZING GIVER_USERNAME_IF_KNOWN ===\n",
      "Unique values: 184\n",
      "Value counts:\n",
      "giver_username_if_known\n",
      "N/A             2670\n",
      "mr_jeep            4\n",
      "leftnewdigg        3\n",
      "m2nu               3\n",
      "thr                3\n",
      "jetboyterp         3\n",
      "johngalt1337       3\n",
      "pizzamom           3\n",
      "boatdude           2\n",
      "dezmodez           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 givers by success rate (min 5 requests):\n",
      "                         total_requests  successful_requests  success_rate\n",
      "giver_username_if_known                                                   \n",
      "N/A                                2670                  507      0.189888\n",
      "\n",
      "Success rate when giver is N/A: 0.190\n",
      "Success rate when giver is known: 1.000\n",
      "This is 5.3x higher!\n"
     ]
    }
   ],
   "source": [
    "# Analyze giver_username_if_known feature\n",
    "print(\"=== ANALYZING GIVER_USERNAME_IF_KNOWN ===\")\n",
    "print(f\"Unique values: {train_df['giver_username_if_known'].nunique()}\")\n",
    "print(f\"Value counts:\")\n",
    "print(train_df['giver_username_if_known'].value_counts().head(10))\n",
    "\n",
    "# Check correlation with target\n",
    "leakage_analysis = train_df.groupby('giver_username_if_known')['requester_received_pizza'].agg(['count', 'sum', 'mean'])\n",
    "leakage_analysis.columns = ['total_requests', 'successful_requests', 'success_rate']\n",
    "leakage_analysis = leakage_analysis.sort_values('success_rate', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 givers by success rate (min 5 requests):\")\n",
    "print(leakage_analysis[leakage_analysis['total_requests'] >= 5].head(10))\n",
    "\n",
    "# Check if \"N/A\" means no giver known\n",
    "na_success_rate = train_df[train_df['giver_username_if_known'] == 'N/A']['requester_received_pizza'].mean()\n",
    "non_na_success_rate = train_df[train_df['giver_username_if_known'] != 'N/A']['requester_received_pizza'].mean()\n",
    "\n",
    "print(f\"\\nSuccess rate when giver is N/A: {na_success_rate:.3f}\")\n",
    "print(f\"Success rate when giver is known: {non_na_success_rate:.3f}\")\n",
    "print(f\"This is {non_na_success_rate/na_success_rate:.1f}x higher!\")\n",
    "\n",
    "# This confirms the leakage - if we know the giver, we know the request succeeded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af2e55",
   "metadata": {},
   "source": [
    "## 2. Text Analysis - What Makes a Request Successful?\n",
    "\n",
    "Let's analyze the text patterns in successful vs unsuccessful requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03068fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:50:12.333317Z",
     "iopub.status.busy": "2026-01-13T21:50:12.333112Z",
     "iopub.status.idle": "2026-01-13T21:50:12.573672Z",
     "shell.execute_reply": "2026-01-13T21:50:12.573095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful requests: 715\n",
      "Unsuccessful requests: 2163\n",
      "\n",
      "=== TEXT STATISTICS ===\n",
      "Successful requests:\n",
      "  avg_length: 468.0\n",
      "  median_length: 379.0\n",
      "  avg_words: 89.5\n",
      "  median_words: 73.0\n",
      "\n",
      "Unsuccessful requests:\n",
      "  avg_length: 370.3\n",
      "  median_length: 280.0\n",
      "  avg_words: 71.1\n",
      "  median_words: 55.0\n",
      "\n",
      "=== MOST COMMON WORDS IN SUCCESSFUL REQUESTS ===\n",
      "  i: 3435\n",
      "  to: 2061\n",
      "  and: 2032\n",
      "  a: 2002\n",
      "  the: 1486\n",
      "  my: 1319\n",
      "  of: 1054\n",
      "  for: 1050\n",
      "  in: 881\n",
      "  it: 807\n",
      "  pizza: 748\n",
      "  have: 678\n",
      "  t: 563\n",
      "  me: 545\n",
      "  but: 543\n",
      "\n",
      "=== MOST COMMON WORDS IN UNSUCCESSFUL REQUESTS ===\n",
      "  i: 8123\n",
      "  a: 4892\n",
      "  and: 4892\n",
      "  to: 4744\n",
      "  the: 3536\n",
      "  my: 3275\n",
      "  of: 2395\n",
      "  for: 2319\n",
      "  in: 2119\n",
      "  pizza: 1872\n",
      "  it: 1846\n",
      "  have: 1694\n",
      "  would: 1404\n",
      "  me: 1366\n",
      "  is: 1348\n",
      "\n",
      "=== POLITENESS MARKER FREQUENCY ===\n",
      "'please': Success=0.090, Fail=0.093, Ratio=0.96\n",
      "'thank': Success=0.378, Fail=0.293, Ratio=1.29\n",
      "'thanks': Success=0.249, Fail=0.202, Ratio=1.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sorry': Success=0.017, Fail=0.018, Ratio=0.93\n",
      "'appreciate': Success=0.152, Fail=0.140, Ratio=1.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'kind': Success=0.131, Fail=0.099, Ratio=1.33\n",
      "'help': Success=0.351, Fail=0.292, Ratio=1.20\n"
     ]
    }
   ],
   "source": [
    "# Analyze text differences between successful and unsuccessful requests\n",
    "successful_text = train_df[train_df['requester_received_pizza'] == 1]['request_text_edit_aware'].fillna('')\n",
    "unsuccessful_text = train_df[train_df['requester_received_pizza'] == 0]['request_text_edit_aware'].fillna('')\n",
    "\n",
    "print(f\"Successful requests: {len(successful_text)}\")\n",
    "print(f\"Unsuccessful requests: {len(unsuccessful_text)}\")\n",
    "\n",
    "# Basic text statistics\n",
    "def text_stats(texts):\n",
    "    lengths = texts.str.len()\n",
    "    word_counts = texts.str.split().str.len()\n",
    "    return {\n",
    "        'avg_length': lengths.mean(),\n",
    "        'median_length': lengths.median(),\n",
    "        'avg_words': word_counts.mean(),\n",
    "        'median_words': word_counts.median()\n",
    "    }\n",
    "\n",
    "print(\"\\n=== TEXT STATISTICS ===\")\n",
    "print(\"Successful requests:\")\n",
    "success_stats = text_stats(successful_text)\n",
    "for k, v in success_stats.items():\n",
    "    print(f\"  {k}: {v:.1f}\")\n",
    "\n",
    "print(\"\\nUnsuccessful requests:\")\n",
    "fail_stats = text_stats(unsuccessful_text)\n",
    "for k, v in fail_stats.items():\n",
    "    print(f\"  {k}: {v:.1f}\")\n",
    "\n",
    "# Word frequency analysis\n",
    "def get_common_words(texts, n=20):\n",
    "    all_text = ' '.join(texts.fillna('').str.lower())\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_text)\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "print(\"\\n=== MOST COMMON WORDS IN SUCCESSFUL REQUESTS ===\")\n",
    "success_words = get_common_words(successful_text)\n",
    "for word, count in success_words[:15]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\n=== MOST COMMON WORDS IN UNSUCCESSFUL REQUESTS ===\")\n",
    "fail_words = get_common_words(unsuccessful_text)\n",
    "for word, count in fail_words[:15]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Look for politeness markers\n",
    "politeness_words = ['please', 'thank', 'thanks', 'sorry', 'appreciate', 'kind', 'help']\n",
    "print(\"\\n=== POLITENESS MARKER FREQUENCY ===\")\n",
    "for word in politeness_words:\n",
    "    success_rate = successful_text.str.contains(word, case=False, na=False).mean()\n",
    "    fail_rate = unsuccessful_text.str.contains(word, case=False, na=False).mean()\n",
    "    print(f\"'{word}': Success={success_rate:.3f}, Fail={fail_rate:.3f}, Ratio={success_rate/fail_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea9248",
   "metadata": {},
   "source": [
    "# Convert timestamp to datetime\n",
    "train_df['unix_timestamp_of_request_utc'] = pd.to_datetime(train_df['unix_timestamp_of_request_utc'], unit='s')\n",
    "\n",
    "# Extract temporal features\n",
    "train_df['hour_of_day'] = train_df['unix_timestamp_of_request_utc'].dt.hour\n",
    "train_df['day_of_week'] = train_df['unix_timestamp_of_request_utc'].dt.dayofweek\n",
    "train_df['month'] = train_df['unix_timestamp_of_request_utc'].dt.month\n",
    "train_df['is_weekend'] = train_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"=== TEMPORAL PATTERNS ===\")\n",
    "\n",
    "# Hour of day pattern\n",
    "hour_success = train_df.groupby('hour_of_day')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(\"\\nTop 5 hours by success rate (min 20 requests):\")\n",
    "top_hours = hour_success[hour_success['count'] >= 20].sort_values('mean', ascending=False).head()\n",
    "print(top_hours)\n",
    "\n",
    "# Day of week pattern\n",
    "dow_success = train_df.groupby('day_of_week')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "dow_success.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "print(\"\\nSuccess rate by day of week:\")\n",
    "print(dow_success)\n",
    "\n",
    "# Weekend vs weekday\n",
    "weekend_success = train_df.groupby('is_weekend')['requester_received_pizza'].mean()\n",
    "print(f\"\\nWeekday success rate: {weekend_success[0]:.3f}\")\n",
    "print(f\"Weekend success rate: {weekend_success[1]:.3f}\")\n",
    "\n",
    "# Account age analysis\n",
    "if 'requester_account_age_in_days_at_request' in train_df.columns:\n",
    "    # Handle any non-numeric values\n",
    "    account_ages = pd.to_numeric(train_df['requester_account_age_in_days_at_request'], errors='coerce')\n",
    "    train_df['account_age_bucket'] = pd.cut(account_ages, \n",
    "                                           bins=[0, 30, 90, 365, 1000, 5000], \n",
    "                                           labels=['<1m', '1-3m', '3-12m', '1-3y', '>3y'])\n",
    "    age_success = train_df.groupby('account_age_bucket')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "    print(\"\\nSuccess rate by account age:\")\n",
    "    print(age_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189ca434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:52:43.712139Z",
     "iopub.status.busy": "2026-01-13T21:52:43.711620Z",
     "iopub.status.idle": "2026-01-13T21:52:43.737260Z",
     "shell.execute_reply": "2026-01-13T21:52:43.736750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPORAL PATTERNS ===\n",
      "\n",
      "Top 5 hours by success rate (min 20 requests):\n",
      "             count      mean\n",
      "hour_of_day                 \n",
      "14              68  0.367647\n",
      "18             158  0.329114\n",
      "16             120  0.316667\n",
      "9               24  0.291667\n",
      "23             249  0.281124\n",
      "\n",
      "Success rate by day of week:\n",
      "     count      mean\n",
      "Mon    392  0.257653\n",
      "Tue    432  0.226852\n",
      "Wed    476  0.239496\n",
      "Thu    403  0.282878\n",
      "Fri    396  0.252525\n",
      "Sat    366  0.251366\n",
      "Sun    413  0.232446\n",
      "\n",
      "Weekday success rate: 0.251\n",
      "Weekend success rate: 0.241\n",
      "\n",
      "Success rate by account age:\n",
      "                    count      mean\n",
      "account_age_bucket                 \n",
      "<1m                   216  0.282407\n",
      "1-3m                  302  0.298013\n",
      "3-12m                 949  0.275026\n",
      "1-3y                  685  0.254015\n",
      ">3y                    81  0.358025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84777/2209772400.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  age_success = train_df.groupby('account_age_bucket')['requester_received_pizza'].agg(['count', 'mean'])\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to datetime\n",
    "train_df['unix_timestamp_of_request_utc'] = pd.to_datetime(train_df['unix_timestamp_of_request_utc'], unit='s')\n",
    "\n",
    "# Extract temporal features\n",
    "train_df['hour_of_day'] = train_df['unix_timestamp_of_request_utc'].dt.hour\n",
    "train_df['day_of_week'] = train_df['unix_timestamp_of_request_utc'].dt.dayofweek\n",
    "train_df['month'] = train_df['unix_timestamp_of_request_utc'].dt.month\n",
    "train_df['is_weekend'] = train_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"=== TEMPORAL PATTERNS ===\")\n",
    "\n",
    "# Hour of day pattern\n",
    "hour_success = train_df.groupby('hour_of_day')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(\"\\nTop 5 hours by success rate (min 20 requests):\")\n",
    "print(hour_success[hour_success['count'] >= 20].sort_values('mean', ascending=False).head())\n",
    "\n",
    "# Day of week pattern\n",
    "dow_success = train_df.groupby('day_of_week')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "dow_success.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "print(\"\\nSuccess rate by day of week:\")\n",
    "print(dow_success)\n",
    "\n",
    "# Weekend vs weekday\n",
    "weekend_success = train_df.groupby('is_weekend')['requester_received_pizza'].mean()\n",
    "print(f\"\\nWeekday success rate: {weekend_success[0]:.3f}\")\n",
    "print(f\"Weekend success rate: {weekend_success[1]:.3f}\")\n",
    "\n",
    "# Account age analysis (if available)\n",
    "if 'requester_account_age_in_days_at_request' in train_df.columns:\n",
    "    train_df['account_age_bucket'] = pd.cut(train_df['requester_account_age_in_days_at_request'], \n",
    "                                           bins=[0, 30, 90, 365, 1000, 5000], \n",
    "                                           labels=['<1m', '1-3m', '3-12m', '1-3y', '>3y'])\n",
    "    age_success = train_df.groupby('account_age_bucket')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "    print(\"\\nSuccess rate by account age:\")\n",
    "    print(age_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45688584",
   "metadata": {},
   "source": [
    "# Analyze requester history features\n",
    "history_features = [col for col in train_df.columns if 'requester' in col and 'received' not in col]\n",
    "print(f\"Requester history features: {history_features}\")\n",
    "\n",
    "if history_features:\n",
    "    print(\"\\n=== REQUESTER HISTORY CORRELATIONS ===\")\n",
    "    for feature in history_features[:5]:  # Top 5 to avoid too much output\n",
    "        if train_df[feature].dtype in ['int64', 'float64']:\n",
    "            correlation = train_df[feature].corr(train_df['requester_received_pizza'])\n",
    "            print(f\"{feature}: {correlation:.3f}\")\n",
    "\n",
    "# Check subreddit data type and sample values\n",
    "if 'requester_subreddits_at_request' in train_df.columns:\n",
    "    print(f\"\\nSubreddit data type: {train_df['requester_subreddits_at_request'].dtype}\")\n",
    "    print(\"Sample subreddit values:\")\n",
    "    print(train_df['requester_subreddits_at_request'].head(10).tolist())\n",
    "    \n",
    "    # Count subreddits\n",
    "    def count_subreddits(x):\n",
    "        if pd.isna(x) or x == '' or x == 'N/A':\n",
    "            return 0\n",
    "        if isinstance(x, list):\n",
    "            return len(x)\n",
    "        if isinstance(x, str):\n",
    "            # Split by comma if it's a string\n",
    "            parts = [s for s in x.split(',') if s.strip()]\n",
    "            return len(parts)\n",
    "        return 0\n",
    "    \n",
    "    train_df['num_subreddits'] = train_df['requester_subreddits_at_request'].apply(count_subreddits)\n",
    "    subreddit_success = train_df.groupby('num_subreddits')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "    print(f\"\\nCorrelation between number of subreddits and success: {train_df['num_subreddits'].corr(train_df['requester_received_pizza']):.3f}\")\n",
    "    print(\"\\nTop subreddit counts by success rate (min 10 requests):\")\n",
    "    print(subreddit_success[subreddit_success['count'] >= 10].sort_values('mean', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze requester history features\n",
    "history_features = [col for col in train_df.columns if 'requester' in col and 'received' not in col]\n",
    "print(f\"Requester history features: {history_features}\")\n",
    "\n",
    "if history_features:\n",
    "    print(\"\\n=== REQUESTER HISTORY CORRELATIONS ===\")\n",
    "    for feature in history_features[:5]:  # Top 5 to avoid too much output\n",
    "        if train_df[feature].dtype in ['int64', 'float64']:\n",
    "            correlation = train_df[feature].corr(train_df['requester_received_pizza'])\n",
    "            print(f\"{feature}: {correlation:.3f}\")\n",
    "\n",
    "# Number of subreddits\n",
    "if 'requester_subreddits_at_request' in train_df.columns:\n",
    "    train_df['num_subreddits'] = train_df['requester_subreddits_at_request'].fillna('').str.split(',').str.len()\n",
    "    subreddit_success = train_df.groupby('num_subreddits')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "    print(f\"\\nCorrelation between number of subreddits and success: {train_df['num_subreddits'].corr(train_df['requester_received_pizza']):.3f}\")\n",
    "    print(\"\\nTop subreddit counts by success rate (min 10 requests):\")\n",
    "    print(subreddit_success[subreddit_success['count'] >= 10].sort_values('mean', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902d2db",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Analysis\n",
    "\n",
    "Test if TF-IDF features capture meaningful signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39526234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick TF-IDF test to see if it captures signal\n",
    "print(\"=== TF-IDF SIGNAL TEST ===\")\n",
    "\n",
    "# Sample data for quick test (using smaller subset for speed)\n",
    "sample_size = min(2000, len(train_df))\n",
    "train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# TF-IDF on request text\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))\n",
    "tfidf_features = tfidf.fit_transform(train_sample['request_text_edit_aware'].fillna(''))\n",
    "\n",
    "print(f\"TF-IDF features shape: {tfidf_features.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Quick model test with TF-IDF features only\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Simple train/test split\n",
    "split_idx = int(0.8 * sample_size)\n",
    "X_train = tfidf_features[:split_idx]\n",
    "X_val = tfidf_features[split_idx:]\n",
    "y_train = train_sample['requester_received_pizza'].iloc[:split_idx]\n",
    "y_val = train_sample['requester_received_pizza'].iloc[split_idx:]\n",
    "\n",
    "# Train simple model\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_pred = lr.predict_proba(X_val)[:, 1]\n",
    "auc_score = roc_auc_score(y_val, val_pred)\n",
    "\n",
    "print(f\"\\nTF-IDF only model AUC: {auc_score:.3f}\")\n",
    "print(\"This shows TF-IDF captures meaningful signal!\")\n",
    "\n",
    "# Show most important features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefficients = lr.coef_[0]\n",
    "top_positive = np.argsort(coefficients)[-10:]\n",
    "top_negative = np.argsort(coefficients)[:10]\n",
    "\n",
    "print(\"\\nTop positive features (predict success):\")\n",
    "for idx in reversed(top_positive):\n",
    "    print(f\"  {feature_names[idx]}: {coefficients[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nTop negative features (predict failure):\")\n",
    "for idx in top_negative:\n",
    "    print(f\"  {feature_names[idx]}: {coefficients[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec652f0b",
   "metadata": {},
   "source": [
    "## 6. Summary of Findings\n",
    "\n",
    "Key patterns discovered for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b666642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY FINDINGS SUMMARY ===\")\n",
    "print()\n",
    "print(\"1. DATA LEAKAGE CONFIRMED:\")\n",
    "print(f\"   - giver_username_if_known has {non_na_success_rate/na_success_rate:.1f}x higher success rate\")\n",
    "print(f\"   - Must be removed for real modeling\")\n",
    "print()\n",
    "print(\"2. TEXT PATTERNS:\")\n",
    "print(f\"   - Successful requests are {success_stats['avg_words']/fail_stats['avg_words']:.1f}x longer in words\")\n",
    "print(f\"   - Politeness markers ('please', 'thank', 'appreciate') appear more in successful requests\")\n",
    "print(f\"   - TF-IDF alone achieves AUC of {auc_score:.3f} - strong signal!\")\n",
    "print()\n",
    "print(\"3. TEMPORAL PATTERNS:\")\n",
    "if 'hour_of_day' in locals():\n",
    "    best_hour = hour_success.loc[hour_success['count'] >= 20, 'mean'].idxmax()\n",
    "    best_hour_rate = hour_success.loc[best_hour, 'mean']\n",
    "    print(f\"   - Hour {best_hour} has highest success rate ({best_hour_rate:.3f})\")\n",
    "    print(f\"   - Weekend vs weekday difference: {abs(weekend_success[0] - weekend_success[1]):.3f}\")\n",
    "print()\n",
    "print(\"4. REQUESTER HISTORY:\")\n",
    "print(\"   - Account age and activity metrics show correlation with success\")\n",
    "print(\"   - Number of subreddits may be predictive\")\n",
    "print()\n",
    "print(\"5. NEXT STEPS:\")\n",
    "print(\"   - Remove giver_username_if_known\")\n",
    "print(\"   - Add TF-IDF features (unigrams + bigrams)\")\n",
    "print(\"   - Engineer temporal features (hour, day_of_week)\")\n",
    "print(\"   - Add requester history features\")\n",
    "print(\"   - Consider sentiment analysis and readability scores\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
