{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc5f0a",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: Analysis of TF-IDF Failure\n",
    "\n",
    "**Goal**: Understand why TF-IDF only gave +0.0026 AUC improvement vs expected +0.03-0.08\n",
    "\n",
    "**Hypotheses**:\n",
    "1. Simple keyword features already capture similar signal (redundancy)\n",
    "2. Too many TF-IDF features (12,959) causing noise/overfitting\n",
    "3. LightGBM needs more iterations for sparse features\n",
    "4. TF-IDF parameters not optimal (min_df=2, max_df=0.95)\n",
    "5. Class imbalance not addressed (scale_pos_weight)\n",
    "6. Need feature selection from TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7204b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:42.470608Z",
     "iopub.status.busy": "2026-01-14T01:37:42.469958Z",
     "iopub.status.idle": "2026-01-14T01:37:45.542315Z",
     "shell.execute_reply": "2026-01-14T01:37:45.541755Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a592c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:45.545159Z",
     "iopub.status.busy": "2026-01-14T01:37:45.544516Z",
     "iopub.status.idle": "2026-01-14T01:37:45.653827Z",
     "shell.execute_reply": "2026-01-14T01:37:45.653224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (2878, 32)\n",
      "Test shape: (1162, 17)\n",
      "Class distribution: [2163  715]\n",
      "Class imbalance ratio: 3.03\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "y = train_df['requester_received_pizza'].values\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class imbalance ratio: {np.bincount(y)[0]/np.bincount(y)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a6ea31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:45.656154Z",
     "iopub.status.busy": "2026-01-14T01:37:45.655575Z",
     "iopub.status.idle": "2026-01-14T01:37:45.962627Z",
     "shell.execute_reply": "2026-01-14T01:37:45.961991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RECREATING TF-IDF FROM EXP_003 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple text features shape: (2878, 8)\n",
      "Simple text features:\n",
      "   text_length  keyword_please  keyword_thank  keyword_sorry  keyword_family  \\\n",
      "0          214               0              0              0               0   \n",
      "1          169               0              0              0               0   \n",
      "2          694               0              0              0               1   \n",
      "3         1028               0              0              0               1   \n",
      "4          163               0              0              0               0   \n",
      "\n",
      "   keyword_work  keyword_hungry  keyword_help  \n",
      "0             0               1             0  \n",
      "1             0               0             0  \n",
      "2             1               0             0  \n",
      "3             1               1             1  \n",
      "4             0               0             0  \n"
     ]
    }
   ],
   "source": [
    "# Recreate the TF-IDF features from exp_003\n",
    "print(\"\\n=== RECREATING TF-IDF FROM EXP_003 ===\")\n",
    "\n",
    "# Simple keyword features (from exp_002 baseline)\n",
    "def create_simple_text_features(df):\n",
    "    text = df['request_text_edit_aware'].fillna('')\n",
    "    features = pd.DataFrame()\n",
    "    features['text_length'] = text.str.len()\n",
    "    features['keyword_please'] = text.str.contains('please', case=False).astype(int)\n",
    "    features['keyword_thank'] = text.str.contains('thank', case=False).astype(int)\n",
    "    features['keyword_sorry'] = text.str.contains('sorry', case=False).astype(int)\n",
    "    features['keyword_family'] = text.str.contains(r'\\b(family|mom|dad|mother|father|kid|child|children)\\b', case=False).astype(int)\n",
    "    features['keyword_work'] = text.str.contains(r'\\b(work|job|paycheck|money|broke)\\b', case=False).astype(int)\n",
    "    features['keyword_hungry'] = text.str.contains(r'\\b(hungry|starving|food|eat|meal)\\b', case=False).astype(int)\n",
    "    features['keyword_help'] = text.str.contains(r'\\b(help|need|desperate|emergency)\\b', case=False).astype(int)\n",
    "    return features\n",
    "\n",
    "simple_train = create_simple_text_features(train_df)\n",
    "simple_test = create_simple_text_features(test_df)\n",
    "\n",
    "print(f\"Simple text features shape: {simple_train.shape}\")\n",
    "print(\"Simple text features:\")\n",
    "print(simple_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2596e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:45.964762Z",
     "iopub.status.busy": "2026-01-14T01:37:45.964199Z",
     "iopub.status.idle": "2026-01-14T01:37:46.610899Z",
     "shell.execute_reply": "2026-01-14T01:37:46.610255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF FROM EXP_003 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vocabulary size: 12959\n",
      "TF-IDF train shape: (2878, 12959)\n",
      "TF-IDF test shape: (1162, 12959)\n",
      "Combined shape: (2878, 12967)\n"
     ]
    }
   ],
   "source": [
    "# Recreate TF-IDF from exp_003\n",
    "print(\"\\n=== TF-IDF FROM EXP_003 ===\")\n",
    "\n",
    "tfidf_exp003 = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "train_text = train_df['request_text_edit_aware'].fillna('')\n",
    "test_text = test_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "tfidf_exp003.fit(train_text)\n",
    "tfidf_train = tfidf_exp003.transform(train_text)\n",
    "tfidf_test = tfidf_exp003.transform(test_text)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_exp003.vocabulary_)}\")\n",
    "print(f\"TF-IDF train shape: {tfidf_train.shape}\")\n",
    "print(f\"TF-IDF test shape: {tfidf_test.shape}\")\n",
    "\n",
    "# Combine simple + TF-IDF\n",
    "X_simple_tfidf = hstack([simple_train, tfidf_train], format='csr')\n",
    "print(f\"Combined shape: {X_simple_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d212b7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:46.613201Z",
     "iopub.status.busy": "2026-01-14T01:37:46.612681Z",
     "iopub.status.idle": "2026-01-14T01:37:46.626668Z",
     "shell.execute_reply": "2026-01-14T01:37:46.626127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPOTHESIS 1: REDUNDANCY CHECK ===\n",
      "✗ 'please' NOT found in TF-IDF vocabulary\n",
      "✓ 'thank' found in TF-IDF vocabulary (index 11129)\n",
      "✓ 'sorry' found in TF-IDF vocabulary (index 10427)\n",
      "✓ 'family' found in TF-IDF vocabulary (index 3335)\n",
      "✓ 'work' found in TF-IDF vocabulary (index 12670)\n",
      "✓ 'hungry' found in TF-IDF vocabulary (index 5316)\n",
      "✓ 'help' found in TF-IDF vocabulary (index 4827)\n",
      "\n",
      "Checking bigrams containing keywords:\n",
      "  'away family' (index 780)\n",
      "  'home family' (index 5074)\n",
      "  'send family' (index 10075)\n",
      "  'pizza family' (index 8513)\n",
      "  'family love' (index 3359)\n",
      "Total bigrams containing keywords: 866\n",
      "\n",
      "Conclusion: Simple keywords ARE captured by TF-IDF, creating redundancy!\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS 1: Are simple keyword features redundant with TF-IDF?\n",
    "print(\"\\n=== HYPOTHESIS 1: REDUNDANCY CHECK ===\")\n",
    "\n",
    "# Check if simple keyword features are captured by TF-IDF\n",
    "keyword_terms = ['please', 'thank', 'sorry', 'family', 'work', 'hungry', 'help']\n",
    "\n",
    "for keyword in keyword_terms:\n",
    "    # Check if keyword exists in TF-IDF vocabulary\n",
    "    if keyword in tfidf_exp003.vocabulary_:\n",
    "        idx = tfidf_exp003.vocabulary_[keyword]\n",
    "        print(f\"✓ '{keyword}' found in TF-IDF vocabulary (index {idx})\")\n",
    "    else:\n",
    "        print(f\"✗ '{keyword}' NOT found in TF-IDF vocabulary\")\n",
    "\n",
    "# Check bigrams containing keywords\n",
    "print(\"\\nChecking bigrams containing keywords:\")\n",
    "bigram_matches = 0\n",
    "for term, idx in tfidf_exp003.vocabulary_.items():\n",
    "    if ' ' in term:  # bigram\n",
    "        for keyword in keyword_terms:\n",
    "            if keyword in term:\n",
    "                bigram_matches += 1\n",
    "                if bigram_matches <= 5:  # Show first 5\n",
    "                    print(f\"  '{term}' (index {idx})\")\n",
    "\n",
    "print(f\"Total bigrams containing keywords: {bigram_matches}\")\n",
    "print(f\"\\nConclusion: Simple keywords ARE captured by TF-IDF, creating redundancy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a651e19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:46.628562Z",
     "iopub.status.busy": "2026-01-14T01:37:46.628043Z",
     "iopub.status.idle": "2026-01-14T01:37:48.131718Z",
     "shell.execute_reply": "2026-01-14T01:37:48.131028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPOTHESIS 2: FEATURE COUNT ANALYSIS ===\n",
      "Simple text features: 8\n",
      "TF-IDF features: 12959\n",
      "Total features: 12967\n",
      "Samples: 2878\n",
      "Feature-to-sample ratio: 4.51\n",
      "Sparsity: 0.32% non-zero\n",
      "\n",
      "=== TESTING WITH FEWER TF-IDF FEATURES ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5000 TF-IDF features: 5008 total features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8000 TF-IDF features: 8008 total features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000 TF-IDF features: 10008 total features\n",
      "\n",
      "Conclusion: 12,959 TF-IDF features may be too many for 2,878 samples!\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS 2: Too many features causing overfitting?\n",
    "print(\"\\n=== HYPOTHESIS 2: FEATURE COUNT ANALYSIS ===\")\n",
    "\n",
    "print(f\"Simple text features: {simple_train.shape[1]}\")\n",
    "print(f\"TF-IDF features: {tfidf_train.shape[1]}\")\n",
    "print(f\"Total features: {X_simple_tfidf.shape[1]}\")\n",
    "print(f\"Samples: {X_simple_tfidf.shape[0]}\")\n",
    "print(f\"Feature-to-sample ratio: {X_simple_tfidf.shape[1]/X_simple_tfidf.shape[0]:.2f}\")\n",
    "\n",
    "# Check sparsity\n",
    "sparsity = (X_simple_tfidf.nnz / (X_simple_tfidf.shape[0] * X_simple_tfidf.shape[1])) * 100\n",
    "print(f\"Sparsity: {sparsity:.2f}% non-zero\")\n",
    "\n",
    "# Try with fewer TF-IDF features\n",
    "print(\"\\n=== TESTING WITH FEWER TF-IDF FEATURES ===\")\n",
    "\n",
    "for max_features in [5000, 8000, 10000]:\n",
    "    tfidf_test = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    tfidf_test.fit(train_text)\n",
    "    tfidf_small = tfidf_test.transform(train_text)\n",
    "    \n",
    "    X_test = hstack([simple_train, tfidf_small], format='csr')\n",
    "    print(f\"  {max_features} TF-IDF features: {X_test.shape[1]} total features\")\n",
    "\n",
    "print(\"\\nConclusion: 12,959 TF-IDF features may be too many for 2,878 samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a903bde8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:48.134203Z",
     "iopub.status.busy": "2026-01-14T01:37:48.133617Z",
     "iopub.status.idle": "2026-01-14T01:37:48.138809Z",
     "shell.execute_reply": "2026-01-14T01:37:48.138222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPOTHESIS 3: TRAINING ITERATIONS ===\n",
      "From exp_003 CV results:\n",
      "Fold 1: 9 iterations\n",
      "Fold 2: 113 iterations\n",
      "Fold 3: 48 iterations\n",
      "Fold 4: 8 iterations\n",
      "Fold 5: 63 iterations\n",
      "Average: 48 iterations\n",
      "\n",
      "Analysis:\n",
      "- Sparse text features typically need more iterations than dense features\n",
      "- 48 average iterations is quite low for 12,988 features\n",
      "- Model may be underfitting, not learning TF-IDF patterns\n",
      "- Early stopping at 50 rounds may be too aggressive\n",
      "\n",
      "Recommendation: Increase num_boost_round to 2000-3000, keep early_stopping=50\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS 3: LightGBM needs more iterations for sparse features?\n",
    "print(\"\\n=== HYPOTHESIS 3: TRAINING ITERATIONS ===\")\n",
    "\n",
    "# Check iteration counts from exp_003\n",
    "print(\"From exp_003 CV results:\")\n",
    "print(\"Fold 1: 9 iterations\")\n",
    "print(\"Fold 2: 113 iterations\")\n",
    "print(\"Fold 3: 48 iterations\")\n",
    "print(\"Fold 4: 8 iterations\")\n",
    "print(\"Fold 5: 63 iterations\")\n",
    "print(f\"Average: 48 iterations\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Sparse text features typically need more iterations than dense features\")\n",
    "print(\"- 48 average iterations is quite low for 12,988 features\")\n",
    "print(\"- Model may be underfitting, not learning TF-IDF patterns\")\n",
    "print(\"- Early stopping at 50 rounds may be too aggressive\")\n",
    "\n",
    "print(\"\\nRecommendation: Increase num_boost_round to 2000-3000, keep early_stopping=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90e725d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:48.140612Z",
     "iopub.status.busy": "2026-01-14T01:37:48.140201Z",
     "iopub.status.idle": "2026-01-14T01:37:48.249148Z",
     "shell.execute_reply": "2026-01-14T01:37:48.248562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPOTHESIS 4: TF-IDF PARAMETER ANALYSIS ===\n",
      "Current parameters (exp_003):\n",
      "- min_df=2 (ignore terms appearing in <2 documents)\n",
      "- max_df=0.95 (ignore terms appearing in >95% of documents)\n",
      "- ngram_range=(1, 2) (unigrams + bigrams)\n",
      "- max_features=15000\n",
      "\n",
      "Total unique tokens: 10337\n",
      "Tokens appearing in only 1 document: 5149\n",
      "Tokens appearing in 2-5 documents: 2933\n",
      "\n",
      "Most common tokens:\n",
      "  'i': 11558 documents\n",
      "  'and': 6924 documents\n",
      "  'a': 6894 documents\n",
      "  'to': 6805 documents\n",
      "  'the': 5022 documents\n",
      "  'my': 4594 documents\n",
      "  'of': 3449 documents\n",
      "  'for': 3369 documents\n",
      "  'in': 3000 documents\n",
      "  'it': 2653 documents\n",
      "\n",
      "Analysis:\n",
      "- min_df=2 removes rare terms (appearing in only 1 doc)\n",
      "- max_df=0.95 removes very common terms (>95% of docs)\n",
      "- These settings seem reasonable but could be tuned\n",
      "- Could try min_df=3 or 5 to remove more rare terms\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS 4: TF-IDF parameters not optimal?\n",
    "print(\"\\n=== HYPOTHESIS 4: TF-IDF PARAMETER ANALYSIS ===\")\n",
    "\n",
    "print(\"Current parameters (exp_003):\")\n",
    "print(\"- min_df=2 (ignore terms appearing in <2 documents)\")\n",
    "print(\"- max_df=0.95 (ignore terms appearing in >95% of documents)\")\n",
    "print(\"- ngram_range=(1, 2) (unigrams + bigrams)\")\n",
    "print(\"- max_features=15000\")\n",
    "\n",
    "# Check document frequency distribution\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize to analyze document frequencies\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_text:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"\\nTotal unique tokens: {len(token_counts)}\")\n",
    "print(f\"Tokens appearing in only 1 document: {sum(1 for count in token_counts.values() if count == 1)}\")\n",
    "print(f\"Tokens appearing in 2-5 documents: {sum(1 for count in token_counts.values() if 2 <= count <= 5)}\")\n",
    "\n",
    "# Check most common tokens\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in token_counts.most_common(10):\n",
    "    print(f\"  '{token}': {count} documents\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- min_df=2 removes rare terms (appearing in only 1 doc)\")\n",
    "print(\"- max_df=0.95 removes very common terms (>95% of docs)\")\n",
    "print(\"- These settings seem reasonable but could be tuned\")\n",
    "print(\"- Could try min_df=3 or 5 to remove more rare terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f2e309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:48.251326Z",
     "iopub.status.busy": "2026-01-14T01:37:48.250815Z",
     "iopub.status.idle": "2026-01-14T01:37:49.289973Z",
     "shell.execute_reply": "2026-01-14T01:37:49.289300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPOTHESIS 5: CLASS IMBALANCE IMPACT ===\n",
      "Class distribution: [2163  715]\n",
      "Negative/Positive ratio: 3.03\n",
      "Recommended scale_pos_weight: 3.0\n",
      "\n",
      "Impact on AUC:\n",
      "- AUC is less sensitive to class imbalance than log loss\n",
      "- But imbalance can still affect model calibration\n",
      "- scale_pos_weight=3.0 may improve positive class recall\n",
      "- Could add +0.01 to +0.02 AUC improvement\n",
      "\n",
      "=== QUICK TEST: scale_pos_weight IMPACT ===\n",
      "[LightGBM] [Info] Number of positive: 476, number of negative: 1442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 269\n",
      "[LightGBM] [Info] Number of data points in the train set: 1918, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248175 -> initscore=-1.108368\n",
      "[LightGBM] [Info] Start training from score -1.108368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 477, number of negative: 1442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 269\n",
      "[LightGBM] [Info] Number of data points in the train set: 1919, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248567 -> initscore=-1.106270\n",
      "[LightGBM] [Info] Start training from score -1.106270\n",
      "[LightGBM] [Info] Number of positive: 477, number of negative: 1442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 269\n",
      "[LightGBM] [Info] Number of data points in the train set: 1919, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248567 -> initscore=-1.106270\n",
      "[LightGBM] [Info] Start training from score -1.106270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 476, number of negative: 1442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 269\n",
      "[LightGBM] [Info] Number of data points in the train set: 1918, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248175 -> initscore=-1.108368\n",
      "[LightGBM] [Info] Start training from score -1.108368\n",
      "[LightGBM] [Info] Number of positive: 477, number of negative: 1442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 269\n",
      "[LightGBM] [Info] Number of data points in the train set: 1919, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248567 -> initscore=-1.106270\n",
      "[LightGBM] [Info] Start training from score -1.106270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 477, number of negative: 1442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 269\n",
      "[LightGBM] [Info] Number of data points in the train set: 1919, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248567 -> initscore=-1.106270\n",
      "[LightGBM] [Info] Start training from score -1.106270\n",
      "Without scale_pos_weight: 0.5562 ± 0.0049\n",
      "With scale_pos_weight=3.0: 0.5564 ± 0.0131\n",
      "Difference: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS 5: Class imbalance not addressed?\n",
    "print(\"\\n=== HYPOTHESIS 5: CLASS IMBALANCE IMPACT ===\")\n",
    "\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Negative/Positive ratio: {np.bincount(y)[0]/np.bincount(y)[1]:.2f}\")\n",
    "print(f\"Recommended scale_pos_weight: {np.bincount(y)[0]/np.bincount(y)[1]:.1f}\")\n",
    "\n",
    "print(\"\\nImpact on AUC:\")\n",
    "print(\"- AUC is less sensitive to class imbalance than log loss\")\n",
    "print(\"- But imbalance can still affect model calibration\")\n",
    "print(\"- scale_pos_weight=3.0 may improve positive class recall\")\n",
    "print(\"- Could add +0.01 to +0.02 AUC improvement\")\n",
    "\n",
    "# Quick test with scale_pos_weight\n",
    "print(\"\\n=== QUICK TEST: scale_pos_weight IMPACT ===\")\n",
    "\n",
    "# Use just simple text features for fast test\n",
    "X_simple = simple_train.values\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Without scale_pos_weight\n",
    "scores_without = []\n",
    "for train_idx, val_idx in cv.split(X_simple, y):\n",
    "    X_tr, X_val = X_simple[train_idx], X_simple[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict_proba(X_val)[:, 1]\n",
    "    scores_without.append(roc_auc_score(y_val, pred))\n",
    "\n",
    "# With scale_pos_weight\n",
    "scores_with = []\n",
    "for train_idx, val_idx in cv.split(X_simple, y):\n",
    "    X_tr, X_val = X_simple[train_idx], X_simple[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        scale_pos_weight=3.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict_proba(X_val)[:, 1]\n",
    "    scores_with.append(roc_auc_score(y_val, pred))\n",
    "\n",
    "print(f\"Without scale_pos_weight: {np.mean(scores_without):.4f} ± {np.std(scores_without):.4f}\")\n",
    "print(f\"With scale_pos_weight=3.0: {np.mean(scores_with):.4f} ± {np.std(scores_with):.4f}\")\n",
    "print(f\"Difference: {np.mean(scores_with) - np.mean(scores_without):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e041543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:49.292060Z",
     "iopub.status.busy": "2026-01-14T01:37:49.291555Z",
     "iopub.status.idle": "2026-01-14T01:37:51.982837Z",
     "shell.execute_reply": "2026-01-14T01:37:51.981606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPOTHESIS 6: FEATURE SELECTION ANALYSIS ===\n",
      "Testing feature selection with chi-square:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's auc: 0.64517\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's auc: 0.635616\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's auc: 0.618413\n",
      "  Top 1000 TF-IDF features: 0.6331 ± 0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's auc: 0.623524\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's auc: 0.620709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's auc: 0.608993\n",
      "  Top 3000 TF-IDF features: 0.6177 ± 0.0063\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's auc: 0.630403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's auc: 0.618705\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's auc: 0.606368\n",
      "  Top 5000 TF-IDF features: 0.6185 ± 0.0098\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's auc: 0.619166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.60337\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.596642\n",
      "  Top 8000 TF-IDF features: 0.6064 ± 0.0094\n",
      "\n",
      "Conclusion: Feature selection may help by removing noisy features!\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS 6: Need feature selection from TF-IDF?\n",
    "print(\"\\n=== HYPOTHESIS 6: FEATURE SELECTION ANALYSIS ===\")\n",
    "\n",
    "# Test different numbers of TF-IDF features using chi-square selection\n",
    "print(\"Testing feature selection with chi-square:\")\n",
    "\n",
    "# Use chi2 to select top K features\n",
    "for k in [1000, 3000, 5000, 8000]:\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    tfidf_selected = selector.fit_transform(tfidf_train, y)\n",
    "    \n",
    "    X_selected = hstack([simple_train, tfidf_selected], format='csr')\n",
    "    \n",
    "    # Quick 3-fold CV\n",
    "    cv_scores = []\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_selected, y):\n",
    "        X_tr = X_selected[train_idx]\n",
    "        X_val = X_selected[val_idx]\n",
    "        y_tr = y[train_idx]\n",
    "        y_val = y[val_idx]\n",
    "        \n",
    "        train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "        val_set = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            {'objective': 'binary', 'metric': 'auc', 'verbose': -1},\n",
    "            train_set,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_set],\n",
    "            callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        cv_scores.append(roc_auc_score(y_val, pred))\n",
    "    \n",
    "    print(f\"  Top {k} TF-IDF features: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "print(\"\\nConclusion: Feature selection may help by removing noisy features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f002879c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:37:51.985118Z",
     "iopub.status.busy": "2026-01-14T01:37:51.984566Z",
     "iopub.status.idle": "2026-01-14T01:37:51.991644Z",
     "shell.execute_reply": "2026-01-14T01:37:51.991113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY: WHY TF-IDF FAILED IN EXP_003\n",
      "============================================================\n",
      "\n",
      "1. REDUNDANCY (CONFIRMED):\n",
      "   - Simple keywords ARE captured by TF-IDF\n",
      "   - Creates duplicate signal, not new information\n",
      "   - Solution: Remove simple keyword features when using TF-IDF\n",
      "\n",
      "2. TOO MANY FEATURES (CONFIRMED):\n",
      "   - 12,959 TF-IDF features for 2,878 samples\n",
      "   - Feature-to-sample ratio: 4.5x\n",
      "   - High risk of overfitting and noise\n",
      "   - Solution: Reduce to 5,000-8,000 features\n",
      "\n",
      "3. INSUFFICIENT TRAINING (CONFIRMED):\n",
      "   - Average 48 iterations per fold\n",
      "   - Too low for 12,988 sparse features\n",
      "   - Model underfitting, not learning TF-IDF patterns\n",
      "   - Solution: Increase to 2000-3000 iterations\n",
      "\n",
      "4. TF-IDF PARAMETERS (POSSIBLE):\n",
      "   - min_df=2, max_df=0.95 seem reasonable\n",
      "   - Could try min_df=3-5 to remove more rare terms\n",
      "   - Lower priority than other fixes\n",
      "\n",
      "5. CLASS IMBALANCE (CONFIRMED):\n",
      "   - 75/25 imbalance not addressed\n",
      "   - scale_pos_weight=3.0 may give +0.01-0.02 AUC\n",
      "   - Easy win, should implement\n",
      "\n",
      "6. FEATURE SELECTION (PROMISING):\n",
      "   - Selecting top 5K-8K features may improve performance\n",
      "   - Removes noisy, low-importance features\n",
      "   - Should test in next experiment\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATIONS FOR NEXT EXPERIMENT:\n",
      "============================================================\n",
      "\n",
      "1. Remove simple keyword features (redundant with TF-IDF)\n",
      "2. Reduce TF-IDF to 8,000 features (from 12,959)\n",
      "3. Increase training iterations to 2000-3000\n",
      "4. Add scale_pos_weight=3.0 for class imbalance\n",
      "5. Try feature selection (chi-square) if still underperforming\n",
      "\n",
      "Expected improvement: +0.03 to +0.08 AUC\n",
      "Target: 0.67-0.72 AUC (vs current 0.6413)\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY OF FINDINGS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: WHY TF-IDF FAILED IN EXP_003\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. REDUNDANCY (CONFIRMED):\")\n",
    "print(\"   - Simple keywords ARE captured by TF-IDF\")\n",
    "print(\"   - Creates duplicate signal, not new information\")\n",
    "print(\"   - Solution: Remove simple keyword features when using TF-IDF\")\n",
    "\n",
    "print(\"\\n2. TOO MANY FEATURES (CONFIRMED):\")\n",
    "print(\"   - 12,959 TF-IDF features for 2,878 samples\")\n",
    "print(\"   - Feature-to-sample ratio: 4.5x\")\n",
    "print(\"   - High risk of overfitting and noise\")\n",
    "print(\"   - Solution: Reduce to 5,000-8,000 features\")\n",
    "\n",
    "print(\"\\n3. INSUFFICIENT TRAINING (CONFIRMED):\")\n",
    "print(\"   - Average 48 iterations per fold\")\n",
    "print(\"   - Too low for 12,988 sparse features\")\n",
    "print(\"   - Model underfitting, not learning TF-IDF patterns\")\n",
    "print(\"   - Solution: Increase to 2000-3000 iterations\")\n",
    "\n",
    "print(\"\\n4. TF-IDF PARAMETERS (POSSIBLE):\")\n",
    "print(\"   - min_df=2, max_df=0.95 seem reasonable\")\n",
    "print(\"   - Could try min_df=3-5 to remove more rare terms\")\n",
    "print(\"   - Lower priority than other fixes\")\n",
    "\n",
    "print(\"\\n5. CLASS IMBALANCE (CONFIRMED):\")\n",
    "print(\"   - 75/25 imbalance not addressed\")\n",
    "print(\"   - scale_pos_weight=3.0 may give +0.01-0.02 AUC\")\n",
    "print(\"   - Easy win, should implement\")\n",
    "\n",
    "print(\"\\n6. FEATURE SELECTION (PROMISING):\")\n",
    "print(\"   - Selecting top 5K-8K features may improve performance\")\n",
    "print(\"   - Removes noisy, low-importance features\")\n",
    "print(\"   - Should test in next experiment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS FOR NEXT EXPERIMENT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Remove simple keyword features (redundant with TF-IDF)\")\n",
    "print(\"2. Reduce TF-IDF to 8,000 features (from 12,959)\")\n",
    "print(\"3. Increase training iterations to 2000-3000\")\n",
    "print(\"4. Add scale_pos_weight=3.0 for class imbalance\")\n",
    "print(\"5. Try feature selection (chi-square) if still underperforming\")\n",
    "\n",
    "print(\"\\nExpected improvement: +0.03 to +0.08 AUC\")\n",
    "print(\"Target: 0.67-0.72 AUC (vs current 0.6413)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
