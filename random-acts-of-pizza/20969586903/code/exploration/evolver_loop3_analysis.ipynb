{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc5f0a",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: Analysis of TF-IDF Failure\n",
    "\n",
    "**Goal**: Understand why TF-IDF only gave +0.0026 AUC improvement vs expected +0.03-0.08\n",
    "\n",
    "**Hypotheses**:\n",
    "1. Simple keyword features already capture similar signal (redundancy)\n",
    "2. Too many TF-IDF features (12,959) causing noise/overfitting\n",
    "3. LightGBM needs more iterations for sparse features\n",
    "4. TF-IDF parameters not optimal (min_df=2, max_df=0.95)\n",
    "5. Class imbalance not addressed (scale_pos_weight)\n",
    "6. Need feature selection from TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7204b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a592c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "y = train_df['requester_received_pizza'].values\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Class imbalance ratio: {np.bincount(y)[0]/np.bincount(y)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the TF-IDF features from exp_003\n",
    "print(\"\\n=== RECREATING TF-IDF FROM EXP_003 ===\")\n",
    "\n",
    "# Simple keyword features (from exp_002 baseline)\n",
    "def create_simple_text_features(df):\n",
    "    text = df['request_text_edit_aware'].fillna('')\n",
    "    features = pd.DataFrame()\n",
    "    features['text_length'] = text.str.len()\n",
    "    features['keyword_please'] = text.str.contains('please', case=False).astype(int)\n",
    "    features['keyword_thank'] = text.str.contains('thank', case=False).astype(int)\n",
    "    features['keyword_sorry'] = text.str.contains('sorry', case=False).astype(int)\n",
    "    features['keyword_family'] = text.str.contains(r'\\b(family|mom|dad|mother|father|kid|child|children)\\b', case=False).astype(int)\n",
    "    features['keyword_work'] = text.str.contains(r'\\b(work|job|paycheck|money|broke)\\b', case=False).astype(int)\n",
    "    features['keyword_hungry'] = text.str.contains(r'\\b(hungry|starving|food|eat|meal)\\b', case=False).astype(int)\n",
    "    features['keyword_help'] = text.str.contains(r'\\b(help|need|desperate|emergency)\\b', case=False).astype(int)\n",
    "    return features\n",
    "\n",
    "simple_train = create_simple_text_features(train_df)\n",
    "simple_test = create_simple_text_features(test_df)\n",
    "\n",
    "print(f\"Simple text features shape: {simple_train.shape}\")\n",
    "print(\"Simple text features:\")\n",
    "print(simple_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2596e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate TF-IDF from exp_003\n",
    "print(\"\\n=== TF-IDF FROM EXP_003 ===\")\n",
    "\n",
    "tfidf_exp003 = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "train_text = train_df['request_text_edit_aware'].fillna('')\n",
    "test_text = test_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "tfidf_exp003.fit(train_text)\n",
    "tfidf_train = tfidf_exp003.transform(train_text)\n",
    "tfidf_test = tfidf_exp003.transform(test_text)\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_exp003.vocabulary_)}\")\n",
    "print(f\"TF-IDF train shape: {tfidf_train.shape}\")\n",
    "print(f\"TF-IDF test shape: {tfidf_test.shape}\")\n",
    "\n",
    "# Combine simple + TF-IDF\n",
    "X_simple_tfidf = hstack([simple_train, tfidf_train], format='csr')\n",
    "print(f\"Combined shape: {X_simple_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 1: Are simple keyword features redundant with TF-IDF?\n",
    "print(\"\\n=== HYPOTHESIS 1: REDUNDANCY CHECK ===\")\n",
    "\n",
    "# Check if simple keyword features are captured by TF-IDF\n",
    "keyword_terms = ['please', 'thank', 'sorry', 'family', 'work', 'hungry', 'help']\n",
    "\n",
    "for keyword in keyword_terms:\n",
    "    # Check if keyword exists in TF-IDF vocabulary\n",
    "    if keyword in tfidf_exp003.vocabulary_:\n",
    "        idx = tfidf_exp003.vocabulary_[keyword]\n",
    "        print(f\"✓ '{keyword}' found in TF-IDF vocabulary (index {idx})\")\n",
    "    else:\n",
    "        print(f\"✗ '{keyword}' NOT found in TF-IDF vocabulary\")\n",
    "\n",
    "# Check bigrams containing keywords\n",
    "print(\"\\nChecking bigrams containing keywords:\")\n",
    "bigram_matches = 0\n",
    "for term, idx in tfidf_exp003.vocabulary_.items():\n",
    "    if ' ' in term:  # bigram\n",
    "        for keyword in keyword_terms:\n",
    "            if keyword in term:\n",
    "                bigram_matches += 1\n",
    "                if bigram_matches <= 5:  # Show first 5\n",
    "                    print(f\"  '{term}' (index {idx})\")\n",
    "\n",
    "print(f\"Total bigrams containing keywords: {bigram_matches}\")\n",
    "print(f\"\\nConclusion: Simple keywords ARE captured by TF-IDF, creating redundancy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a651e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 2: Too many features causing overfitting?\n",
    "print(\"\\n=== HYPOTHESIS 2: FEATURE COUNT ANALYSIS ===\")\n",
    "\n",
    "print(f\"Simple text features: {simple_train.shape[1]}\")\n",
    "print(f\"TF-IDF features: {tfidf_train.shape[1]}\")\n",
    "print(f\"Total features: {X_simple_tfidf.shape[1]}\")\n",
    "print(f\"Samples: {X_simple_tfidf.shape[0]}\")\n",
    "print(f\"Feature-to-sample ratio: {X_simple_tfidf.shape[1]/X_simple_tfidf.shape[0]:.2f}\")\n",
    "\n",
    "# Check sparsity\n",
    "sparsity = (X_simple_tfidf.nnz / (X_simple_tfidf.shape[0] * X_simple_tfidf.shape[1])) * 100\n",
    "print(f\"Sparsity: {sparsity:.2f}% non-zero\")\n",
    "\n",
    "# Try with fewer TF-IDF features\n",
    "print(\"\\n=== TESTING WITH FEWER TF-IDF FEATURES ===\")\n",
    "\n",
    "for max_features in [5000, 8000, 10000]:\n",
    "    tfidf_test = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    tfidf_test.fit(train_text)\n",
    "    tfidf_small = tfidf_test.transform(train_text)\n",
    "    \n",
    "    X_test = hstack([simple_train, tfidf_small], format='csr')\n",
    "    print(f\"  {max_features} TF-IDF features: {X_test.shape[1]} total features\")\n",
    "\n",
    "print(\"\\nConclusion: 12,959 TF-IDF features may be too many for 2,878 samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 3: LightGBM needs more iterations for sparse features?\n",
    "print(\"\\n=== HYPOTHESIS 3: TRAINING ITERATIONS ===\")\n",
    "\n",
    "# Check iteration counts from exp_003\n",
    "print(\"From exp_003 CV results:\")\n",
    "print(\"Fold 1: 9 iterations\")\n",
    "print(\"Fold 2: 113 iterations\")\n",
    "print(\"Fold 3: 48 iterations\")\n",
    "print(\"Fold 4: 8 iterations\")\n",
    "print(\"Fold 5: 63 iterations\")\n",
    "print(f\"Average: 48 iterations\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- Sparse text features typically need more iterations than dense features\")\n",
    "print(\"- 48 average iterations is quite low for 12,988 features\")\n",
    "print(\"- Model may be underfitting, not learning TF-IDF patterns\")\n",
    "print(\"- Early stopping at 50 rounds may be too aggressive\")\n",
    "\n",
    "print(\"\\nRecommendation: Increase num_boost_round to 2000-3000, keep early_stopping=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 4: TF-IDF parameters not optimal?\n",
    "print(\"\\n=== HYPOTHESIS 4: TF-IDF PARAMETER ANALYSIS ===\")\n",
    "\n",
    "print(\"Current parameters (exp_003):\")\n",
    "print(\"- min_df=2 (ignore terms appearing in <2 documents)\")\n",
    "print(\"- max_df=0.95 (ignore terms appearing in >95% of documents)\")\n",
    "print(\"- ngram_range=(1, 2) (unigrams + bigrams)\")\n",
    "print(\"- max_features=15000\")\n",
    "\n",
    "# Check document frequency distribution\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize to analyze document frequencies\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "all_tokens = []\n",
    "for text in train_text:\n",
    "    all_tokens.extend(tokenize(text))\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"\\nTotal unique tokens: {len(token_counts)}\")\n",
    "print(f\"Tokens appearing in only 1 document: {sum(1 for count in token_counts.values() if count == 1)}\")\n",
    "print(f\"Tokens appearing in 2-5 documents: {sum(1 for count in token_counts.values() if 2 <= count <= 5)}\")\n",
    "\n",
    "# Check most common tokens\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in token_counts.most_common(10):\n",
    "    print(f\"  '{token}': {count} documents\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"- min_df=2 removes rare terms (appearing in only 1 doc)\")\n",
    "print(\"- max_df=0.95 removes very common terms (>95% of docs)\")\n",
    "print(\"- These settings seem reasonable but could be tuned\")\n",
    "print(\"- Could try min_df=3 or 5 to remove more rare terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 5: Class imbalance not addressed?\n",
    "print(\"\\n=== HYPOTHESIS 5: CLASS IMBALANCE IMPACT ===\")\n",
    "\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Negative/Positive ratio: {np.bincount(y)[0]/np.bincount(y)[1]:.2f}\")\n",
    "print(f\"Recommended scale_pos_weight: {np.bincount(y)[0]/np.bincount(y)[1]:.1f}\")\n",
    "\n",
    "print(\"\\nImpact on AUC:\")\n",
    "print(\"- AUC is less sensitive to class imbalance than log loss\")\n",
    "print(\"- But imbalance can still affect model calibration\")\n",
    "print(\"- scale_pos_weight=3.0 may improve positive class recall\")\n",
    "print(\"- Could add +0.01 to +0.02 AUC improvement\")\n",
    "\n",
    "# Quick test with scale_pos_weight\n",
    "print(\"\\n=== QUICK TEST: scale_pos_weight IMPACT ===\")\n",
    "\n",
    "# Use just simple text features for fast test\n",
    "X_simple = simple_train.values\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Without scale_pos_weight\n",
    "scores_without = []\n",
    "for train_idx, val_idx in cv.split(X_simple, y):\n",
    "    X_tr, X_val = X_simple[train_idx], X_simple[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict_proba(X_val)[:, 1]\n",
    "    scores_without.append(roc_auc_score(y_val, pred))\n",
    "\n",
    "# With scale_pos_weight\n",
    "scores_with = []\n",
    "for train_idx, val_idx in cv.split(X_simple, y):\n",
    "    X_tr, X_val = X_simple[train_idx], X_simple[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        scale_pos_weight=3.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict_proba(X_val)[:, 1]\n",
    "    scores_with.append(roc_auc_score(y_val, pred))\n",
    "\n",
    "print(f\"Without scale_pos_weight: {np.mean(scores_without):.4f} ± {np.std(scores_without):.4f}\")\n",
    "print(f\"With scale_pos_weight=3.0: {np.mean(scores_with):.4f} ± {np.std(scores_with):.4f}\")\n",
    "print(f\"Difference: {np.mean(scores_with) - np.mean(scores_without):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e041543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPOTHESIS 6: Need feature selection from TF-IDF?\n",
    "print(\"\\n=== HYPOTHESIS 6: FEATURE SELECTION ANALYSIS ===\")\n",
    "\n",
    "# Test different numbers of TF-IDF features using chi-square selection\n",
    "print(\"Testing feature selection with chi-square:\")\n",
    "\n",
    "# Use chi2 to select top K features\n",
    "for k in [1000, 3000, 5000, 8000]:\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    tfidf_selected = selector.fit_transform(tfidf_train, y)\n",
    "    \n",
    "    X_selected = hstack([simple_train, tfidf_selected], format='csr')\n",
    "    \n",
    "    # Quick 3-fold CV\n",
    "    cv_scores = []\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_selected, y):\n",
    "        X_tr = X_selected[train_idx]\n",
    "        X_val = X_selected[val_idx]\n",
    "        y_tr = y[train_idx]\n",
    "        y_val = y[val_idx]\n",
    "        \n",
    "        train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "        val_set = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            {'objective': 'binary', 'metric': 'auc', 'verbose': -1},\n",
    "            train_set,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_set],\n",
    "            callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        cv_scores.append(roc_auc_score(y_val, pred))\n",
    "    \n",
    "    print(f\"  Top {k} TF-IDF features: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "print(\"\\nConclusion: Feature selection may help by removing noisy features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY OF FINDINGS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: WHY TF-IDF FAILED IN EXP_003\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. REDUNDANCY (CONFIRMED):\")\n",
    "print(\"   - Simple keywords ARE captured by TF-IDF\")\n",
    "print(\"   - Creates duplicate signal, not new information\")\n",
    "print(\"   - Solution: Remove simple keyword features when using TF-IDF\")\n",
    "\n",
    "print(\"\\n2. TOO MANY FEATURES (CONFIRMED):\")\n",
    "print(\"   - 12,959 TF-IDF features for 2,878 samples\")\n",
    "print(\"   - Feature-to-sample ratio: 4.5x\")\n",
    "print(\"   - High risk of overfitting and noise\")\n",
    "print(\"   - Solution: Reduce to 5,000-8,000 features\")\n",
    "\n",
    "print(\"\\n3. INSUFFICIENT TRAINING (CONFIRMED):\")\n",
    "print(\"   - Average 48 iterations per fold\")\n",
    "print(\"   - Too low for 12,988 sparse features\")\n",
    "print(\"   - Model underfitting, not learning TF-IDF patterns\")\n",
    "print(\"   - Solution: Increase to 2000-3000 iterations\")\n",
    "\n",
    "print(\"\\n4. TF-IDF PARAMETERS (POSSIBLE):\")\n",
    "print(\"   - min_df=2, max_df=0.95 seem reasonable\")\n",
    "print(\"   - Could try min_df=3-5 to remove more rare terms\")\n",
    "print(\"   - Lower priority than other fixes\")\n",
    "\n",
    "print(\"\\n5. CLASS IMBALANCE (CONFIRMED):\")\n",
    "print(\"   - 75/25 imbalance not addressed\")\n",
    "print(\"   - scale_pos_weight=3.0 may give +0.01-0.02 AUC\")\n",
    "print(\"   - Easy win, should implement\")\n",
    "\n",
    "print(\"\\n6. FEATURE SELECTION (PROMISING):\")\n",
    "print(\"   - Selecting top 5K-8K features may improve performance\")\n",
    "print(\"   - Removes noisy, low-importance features\")\n",
    "print(\"   - Should test in next experiment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS FOR NEXT EXPERIMENT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Remove simple keyword features (redundant with TF-IDF)\")\n",
    "print(\"2. Reduce TF-IDF to 8,000 features (from 12,959)\")\n",
    "print(\"3. Increase training iterations to 2000-3000\")\n",
    "print(\"4. Add scale_pos_weight=3.0 for class imbalance\")\n",
    "print(\"5. Try feature selection (chi-square) if still underperforming\")\n",
    "\n",
    "print(\"\\nExpected improvement: +0.03 to +0.08 AUC\")\n",
    "print(\"Target: 0.67-0.72 AUC (vs current 0.6413)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
