{
  "query": "How do Kaggle winners prevent data leakage from user flair features in Reddit prediction competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle winners avoid letting user\u2011flair information leak into their models by treating the flair column as a **leaky predictor** and removing or masking it before any training or validation, then using a validation scheme that mirrors the real\u2011time prediction scenario.\u202f([7a\u202fData\u202fLeakage](https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html))\u202f([Meta\u2011analysis of overfitting on Kaggle](https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf))\n\n**Steps to prevent flair leakage**\n\n1. **Identify the flair column as post\u2011submission metadata** \u2013 it is assigned after a comment is posted and therefore unavailable at prediction time.  \n2. **Drop or anonymize the flair feature** from the training set (e.g., `df = df.drop(columns=['flair'])`).  \n3. **Create a validation split that respects the temporal order** of the data (e.g., use the earliest\u202f80\u202f% of posts for training and the latest\u202f20\u202f% for validation) so that no future information can influence the model.  \n4. **Validate only on features that exist at inference** (text, timestamps, author statistics, etc.). Run cross\u2011validation on the temporally split folds to confirm that performance does not rely on the removed flair data.  \n5. **Use a robust leaderboard strategy** (e.g., Kaggle\u2019s \u201cladder\u201d or a single hold\u2011out test set) to ensure the public score cannot be over\u2011fit by repeatedly probing the same data.\u202f([The Ladder paper](https://proceedings.mlr.press/v37/blum15.pdf))  \n\nBy systematically excluding the user\u2011flair column and validating with a time\u2011aware hold\u2011out, winners eliminate the risk that the model learns shortcuts that would disappear in the real competition test set.",
      "url": ""
    },
    {
      "title": "7a Data Leakage",
      "text": "<div><div>\n<p><em>This tutorial is part of the <a href=\"https://www.kaggle.com/learn/machine-learning/\">Learn Machine Learning</a> series. In this step, you will learn what data leakage is and how to prevent it.</em></p>\n<h2>What is Data Leakage<a href=\"#What-is-Data-Leakage\">\u00b6</a></h2><p>Data leakage is one of the most important issues for a data scientist to understand. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in the most subtle and dangerous ways. Specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. This tutorial will show you what leakage is and how to avoid it.</p>\n<p>There are two main types of leakage: <strong>Leaky Predictors</strong> and a <strong>Leaky Validation Strategies.</strong></p>\n<h2>Leaky Predictors<a href=\"#Leaky-Predictors\">\u00b6</a></h2><p>This occurs when your predictors include data that will not be available at the time you make predictions.</p>\n<p>For example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:</p>\n<table>\n<thead><tr>\n<th>got_pneumonia</th>\n<th>age</th>\n<th>weight</th>\n<th>male</th>\n<th>took_antibiotic_medicine</th>\n<th>...</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>False</td>\n<td>65</td>\n<td>100</td>\n<td>False</td>\n<td>False</td>\n<td>...</td>\n</tr>\n<tr>\n<td>False</td>\n<td>72</td>\n<td>130</td>\n<td>True</td>\n<td>False</td>\n<td>...</td>\n</tr>\n<tr>\n<td>True</td>\n<td>58</td>\n<td>100</td>\n<td>False</td>\n<td>True</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<p>-</p>\n<p>People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But <em>took_antibiotic_medicine</em> is frequently changed <strong>after</strong> the value for <em>got_pneumonia</em> is determined. This is target leakage.</p>\n<p>The model would see that anyone who has a value of <code>False</code> for <code>took_antibiotic_medicine</code> didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.</p>\n<p>To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.</p>\n<p></p>\n</div><div>\n<h2>Leaky Validation Strategy<a href=\"#Leaky-Validation-Strategy\">\u00b6</a></h2><p>A much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.</p>\n<h2>Preventing Leaky Predictors<a href=\"#Preventing-Leaky-Predictors\">\u00b6</a></h2><p>There is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.</p>\n<p>However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:</p>\n<ul>\n<li>To screen for possible leaky predictors, look for columns that are statistically correlated to your target.</li>\n<li>If you build a model and find it extremely accurate, you likely have a leakage problem.</li>\n</ul>\n<h2>Preventing Leaky Validation Strategies<a href=\"#Preventing-Leaky-Validation-Strategies\">\u00b6</a></h2><p>If your validation is based on a simple train-test split, exclude the validation data from any type of <em>fitting</em>, including the fitting of preprocessing steps. This is easier if you use <a href=\"https://www.kaggle.com/dansbecker/pipelines\">scikit-learn Pipelines</a>. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.</p>\n<h2>Example<a href=\"#Example\">\u00b6</a></h2><p>We will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called <em>card</em>). Here is a look at the data:</p>\n</div><div>\n<p>We can see with <code>data.shape</code> that this is a small dataset (1312 rows), so we should use cross-validation to ensure accurate measures of model quality</p>\n</div><div>\n<p>With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.</p>\n<p>Here is a summary of the data, which you can also find under the data tab:</p>\n<ul>\n<li><strong>card:</strong> Dummy variable, 1 if application for credit card accepted, 0 if not</li>\n<li><strong>reports:</strong> Number of major derogatory reports</li>\n<li><strong>age:</strong> Age n years plus twelfths of a year</li>\n<li><strong>income:</strong> Yearly income (divided by 10,000)</li>\n<li><strong>share:</strong> Ratio of monthly credit card expenditure to yearly income</li>\n<li><strong>expenditure:</strong> Average monthly credit card expenditure</li>\n<li><strong>owner:</strong> 1 if owns their home, 0 if rent</li>\n<li><strong>selfempl:</strong> 1 if self employed, 0 if not.</li>\n<li><strong>dependents:</strong> 1 + number of dependents</li>\n<li><strong>months:</strong> Months living at current address</li>\n<li><strong>majorcards:</strong> Number of major credit cards held</li>\n<li><strong>active:</strong> Number of active credit accounts</li>\n</ul>\n<p>A few variables look suspicious. For example, does <strong>expenditure</strong> mean expenditure on this card or on cards used before appying?</p>\n<p>At this point, basic data comparisons can be very helpful:</p>\n</div><div>\n<p>Everyone with <code>card == False</code> had no expenditures, while only 2% of those with <code>card == True</code> had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means *expenditures on the card they applied for.**.</p>\n<p>Since <strong>share</strong> is partially determined by <strong>expenditure</strong>, it should be excluded too. The variables <strong>active</strong>, <strong>majorcards</strong> are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.</p>\n<p>We would run a model without leakage as follows:</p>\n</div><div>\n<p>This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.).</p>\n<h2>Conclusion<a href=\"#Conclusion\">\u00b6</a></h2><p>Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data is a first step, and pipelines can help implement this separation. Leaking predictors are a more frequent issue, and leaking predictors are harder to track down. A combination of caution, common sense and data exploration can help identify leaking predictors so you remove them from your model.</p>\n<h2>Exercise<a href=\"#Exercise\">\u00b6</a></h2><p>Review the data in your ongoing project. Are there any predictors that may cause leakage? As a hint, most datasets from Kaggle competitions don't have these variables. Onc...",
      "url": "https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html"
    },
    {
      "title": "",
      "text": "A Meta-Analysis of Overfitting in Machine Learning\nRebecca Roelofs\u2217\nUC Berkeley\nroelofs@berkeley.edu\nSara Fridovich-Keil\u2217\nUC Berkeley\nsfk@berkeley.edu\nJohn Miller\nUC Berkeley\nmiller_john@berkeley.edu\nVaishaal Shankar\nUC Berkeley\nvaishaal@berkeley.edu\nMoritz Hardt\nUC Berkeley\nhardt@berkeley.edu\nBenjamin Recht\nUC Berkeley\nbrecht@berkeley.edu\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nAbstract\nWe conduct the first large meta-analysis of overfitting due to test set reuse in the\nmachine learning community. Our analysis is based on over one hundred machine\nlearning competitions hosted on the Kaggle platform over the course of several\nyears. In each competition, numerous practitioners repeatedly evaluated their\nprogress against a holdout set that forms the basis of a public ranking available\nthroughout the competition. Performance on a separate test set used only once\ndetermined the final ranking. By systematically comparing the public ranking\nwith the final ranking, we assess how much participants adapted to the holdout set\nover the course of a competition. Our study shows, somewhat surprisingly, little\nevidence of substantial overfitting. These findings speak to the robustness of the\nholdout method across different data domains, loss functions, model classes, and\nhuman analysts.\n1 Introduction\nThe holdout method is central to empirical progress in the machine learning community. Competitions,\nbenchmarks, and large-scale hyperparameter search all rely on splitting a data set into multiple pieces\nto separate model training from evaluation. However, when practitioners repeatedly reuse holdout\ndata, the danger of overfitting to the holdout data arises [6, 13].\nDespite its importance, there is little empirical research into the manifested robustness and validity\nof the holdout method in practical scenarios. Real-world use cases of the holdout method often\nfall outside the guarantees of existing theoretical bounds, making questions of validity a matter of\nguesswork.\nRecent replication studies [16] demonstrated that the popular CIFAR-10 [10] and ImageNet [5, 18]\nbenchmarks continue to support progress despite years of intensive use. The longevity of these\nbenchmarks perhaps suggests that overfitting to holdout data is less of a concern than reasoning from\nfirst principles might have suggested. However, this is evidence from only two, albeit important,\ncomputer vision benchmarks. It remains unclear whether the observed phenomenon is specific to the\ndata domain, model class, or practices of vision researchers. Unfortunately, these replication studies\n\u2217Equal contribution\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nrequired assembling new test sets from scratch, resulting in a highly labor-intensive analysis that is\ndifficult to scale.\nIn this paper, we empirically study holdout reuse at a significantly larger scale by analyzing data\nfrom 120 machine learning competitions on the popular Kaggle platform [2]. Kaggle competitions\nare a particularly well-suited environment for studying overfitting since data sources are diverse,\ncontestants use a wide range of model families, and training techniques vary greatly. Moreover,\nKaggle competitions use public and private test data splits which provide a natural experimental setup\nfor measuring overfitting on various datasets.\nTo provide a detailed analysis of each competition, we introduce a coherent methodology to char\u0002acterize the extent of overfitting at three increasingly fine scales. Our approach allows us both to\ndiscuss the overall \u201chealth\u201d of a competition across all submissions and to inspect signs of overfitting\nseparately among the top submissions. In addition, we develop a statistical test specific to the\nclassification competitions on Kaggle to compare the submission scores to those arising in an ideal\nnull model that assumes no overfitting. Observed data that are close to data predicted by the null\nmodel is strong evidence against overfitting.\nOverall, we conclude that the classification competitions on Kaggle show little to no signs of\noverfitting. While there are some outlier competitions in the data, these competitions usually have\npathologies such as non-i.i.d. data splits or (effectively) small test sets. Among the remaining\ncompetitions, the public and private test scores show a remarkably good correspondence. The\npicture becomes more nuanced among the highest scoring submissions, but the overall effect sizes\nof (potential) overfitting are typically small (e.g., less than 1% classification accuracy). Thus, our\nfindings show that substantial overfitting is unlikely to occur naturally in regular machine learning\nworkflows.\n2 Background and setup\nBefore we delve into the analysis of the Kaggle data, we briefly define the type of overfitting we\nstudy and then describe how the Kaggle competition format naturally lends itself to investigating\noverfitting in machine learning competitions.\n2.1 Adaptive overfitting\n\u201cOverfitting\u201d is often used as an umbrella term to describe any unwanted performance drop of a\nmachine learning model. Here, we focus on adaptive overfitting, which is overfitting caused by test\nset reuse. While other phenomena under the overfitting umbrella are also important aspects of reliable\nmachine learning (e.g. performance drops due to distribution shifts), they are beyond the scope of our\npaper since they require an experimental setup different from ours.\nFormally, let f : X \u2192 Y be a trained model that maps examples x \u2208 X to output values y \u2208 Y\n(e.g., class labels or regression targets). The standard approach to measuring the performance of\nsuch a trained model is to define a loss function L : Y \u00d7 Y \u2192 R and to draw samples S =\n{(x1, y1), . . . ,(xn, yn)} from a data distribution D which we then use to evaluate the test loss\nLS(f) = Pn\ni=1 L(f(xi), yi). As long as the model f does not depend on the test set S, standard\nconcentration results [19] show that LS(f) is a good approximation of the true performance given by\nthe population loss LD(f) = ED[L(f(x), y)].\nHowever, machine learning practitioners often undermine the assumption that f does not depend\non the test set by selecting models and tuning hyperparameters based on the test loss. Especially\nwhen algorithm designers evaluate a large number of different models on the same test set, the final\nclassifier may only perform well on the specific examples in the test set. The failure to generalize to\nthe entire data distribution D manifests itself in a large adaptivity gap LD(f) \u2212 LS(f) and leads to\noverly optimistic performance estimates.\n2.2 Kaggle\nKaggle is the most widely used platform for machine learning competitions, currently hosting 1,461\nactive and completed competitions. Various organizations (companies, educators, etc.) provide the\n2\ndatasets and evaluation rules for the competitions, which are generally open to any participant. Each\ncompetition is centered around a dataset consisting of a training set and a test set.\nConsidering the danger of overfitting to the test set in a competitive environment, Kaggle subdivides\neach test set into public and private components. The subsets are randomly shuffled together and\nthe entire test set is released without labels, so that participants should not know which test samples\nbelong to which split. Hence participants submit predictions for the entire test set. The Kaggle server\nthen internally evaluates each submission on both public and private splits and updates the public\ncompetition leaderboard only with the score on the public split. At the end of the competition, Kaggle\nreleases the private scores, which determine the winner.\nKaggle has released the MetaKaggle dataset2, which contains detailed information about competitions,\nsubmissions, etc. on the Kaggle platform. The structure of Kaggle competitions makes MetaKaggle\na useful dataset for investigating overfitting empirically at a large scale. In particular, we can view\nthe public test ...",
      "url": "https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "What is Data Leakage \u00b6",
      "text": "7b Exercise; Data Leakage\n*This tutorial is part of the[Learn Machine Learning](https://www.kaggle.com/learn/machine-learning/)series. In this step, you will learn what data leakage is and how to prevent it.*\n# What is Data Leakage[&#182;](#What-is-Data-Leakage)\nData leakage is one of the most important issues for a data scientist to understand. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in the most subtle and dangerous ways. Specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. This tutorial will show you what leakage is and how to avoid it.\nThere are two main types of leakage:**Leaky Predictors**and a**Leaky Validation Strategies.**\n## Leaky Predictors[&#182;](#Leaky-Predictors)\nThis occurs when your predictors include data that will not be available at the time you make predictions.\nFor example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:\n|got\\_pneumonia|age|weight|male|took\\_antibiotic\\_medicine|...|\nFalse|65|100|False|False|...|\nFalse|72|130|True|False|...|\nTrue|58|100|False|True|...|\n-\nPeople take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But*took\\_antibiotic\\_medicine*is frequently changed**after**the value for*got\\_pneumonia*is determined. This is target leakage.\nThe model would see that anyone who has a value of`False`for`took\\_antibiotic\\_medicine`didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.\nTo prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n![Leaky Data Graphic](https://i.imgur.com/CN4INKb.png)\n## Leaky Validation Strategy[&#182;](#Leaky-Validation-Strategy)\nA much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train\\_test\\_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n## Preventing Leaky Predictors[&#182;](#Preventing-Leaky-Predictors)\nThere is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.\nHowever, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n* To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n* If you build a model and find it extremely accurate, you likely have a leakage problem.## Preventing Leaky Validation Strategies[&#182;](#Preventing-Leaky-Validation-Strategies)\nIf your validation is based on a simple train-test split, exclude the validation data from any type of*fitting*, including the fitting of preprocessing steps. This is easier if you use[scikit-learn Pipelines](https://www.kaggle.com/dansbecker/pipelines). When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.\n# Example[&#182;](#Example)\nWe will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called*card*). Here is a look at the data:\nIn[1]:\n```\nimportpandasaspddata=pd.read\\_csv(&#39;&#39;../input/AER\\_credit\\_card\\_data.csv&#39;&#39;,true\\_values=[&#39;yes&#39;],false\\_values=[&#39;no&#39;])print(data.head())\n```\n```\ncard reports age ... months majorcards active\n0 True 0 37.66667 ... 54 1 12\n1 True 0 33.25000 ... 34 1 13\n2 True 0 33.66667 ... 58 1 5\n3 True 0 30.50000 ... 25 1 7\n4 True 0 32.16667 ... 64 1 5\n[5 rows x 12 columns]\n```\nWe can see with`data.shape`that this is a small dataset (1312 rows), so we should use cross-validation to ensure accurate measures of model quality\nIn[2]:\n```\ndata.shape\n```\nOut[2]:\n```\n(1319, 12)\n```\nIn[3]:\n```\nfromsklearn.pipelineimportmake\\_pipelinefromsklearn.ensembleimportRandomForestClassifierfromsklearn.model\\_selectionimportcross\\_val\\_scorey=data.cardX=data.drop([&#39;card&#39;],axis=1)# Since there was no preprocessing, we didn&#39;t need a pipeline here. Used anyway as best practicemodeling\\_pipeline=make\\_pipeline(RandomForestClassifier(n\\_estimators=10))cv\\_scores=cross\\_val\\_score(modeling\\_pipeline,X,y,scoring=&#39;accuracy&#39;,cv=5)print(&quot;Cross-val accuracy:%f&quot;%cv\\_scores.mean())\n```\n```\nCross-val accuracy: 0.978765\n```\nWith experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.\nHere is a summary of the data, which you can also find under the data tab:\n* **card:**Dummy variable, 1 if application for credit card accepted, 0 if not\n* **reports:**Number of major derogatory reports\n* **age:**Age n years plus twelfths of a year\n* **income:**Yearly income (divided by 10,000)\n* **share:**Ratio of monthly credit card expenditure to yearly income\n* **expenditure:**Average monthly credit card expenditure\n* **owner:**1 if owns their home, 0 if rent\n* **selfempl:**1 if self employed, 0 if not.\n* **dependents:**1 + number of dependents\n* **months:**Months living at current address\n* **majorcards:**Number of major credit cards held\n* **active:**Number of active credit accounts\nA few variables look suspicious. For example, does**expenditure**mean expenditure on this card or on cards used before appying?\nAt this point, basic data comparisons can be very helpful:\nIn[4]:\n```\nexpenditures\\_cardholders=data.expenditure[data.card]expenditures\\_noncardholders=data.expenditure[\\~data.card]print(&#39;Fraction of those who received a card with no expenditures:%.2f&#39;\\\\%((expenditures\\_cardholders==0).mean()))print(&#39;Fraction of those who received a card with no expenditures:%.2f&#39;\\\\%((expenditures\\_noncardholders==0).mean()))\n```\n```\nFraction of those who received a card with no expenditures: 0.02\nFraction of those who received a card with no expenditures: 1.00\n```\nEveryone with`card == False`had no expenditures, while only 2% of those with`card == True`had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means \\*expenditures on the card they applied for.\\*\\*.\nSince**share**is partially determined by**expenditure**, it should be excluded too. The variables**active**,**majorcards**are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\nWe would run a model without leakage as follows:\nIn[5]:\n```\npotential\\_leaks=[&#39;expenditure&#39;,&#39;share&#39;,&#39;active&#39;,&#39;majorcards&#39;]X2=X.drop(potential\\_leaks,axis=1)cv\\_scores=cross\\_val\\_score(modeling\\_pipeline,X2,y,scoring=&#39;accuracy&#39;,cv=5)print(&quot;Cross-val accuracy:%f&quot;%cv\\_scores.mean())\n```\n```\nCross-val accuracy: 0.804400\n```\nThis accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would lik...",
      "url": "https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7b%20Exercise;%20Data%20Leakage.html"
    },
    {
      "title": "Reddit Comment Score Prediction",
      "text": "Reddit Comment Score Prediction\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n[\n](https://www.kaggle.com/ehallmar)Evan Hallmark\u00b7 Updated7 years ago\narrow\\_drop\\_up26\ncodeCode\nfile\\_downloadDownload\nmore\\_vert\n# Reddit Comment Score Prediction\nPredicting comment scores with NLP\n![](https://storage.googleapis.com/kaggle-datasets-images/new-version-temp-images/default-backgrounds-22.png-1439783/dataset-cover.png)\n## Reddit Comment Score Prediction\n[Data Card](https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction/data)[Code (4)](https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction/code)[Discussion (0)](https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction/discussion)[Suggestions (0)](https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction/suggestions)\n## About Dataset\n### Context\nThe idea behind this dataset is to try to predict whether a particular comment would be highly up-voted or down-voted given the parent comment.\nPresently, I have a model that does fairly well (70+% accuracy on several validation sets) using neural network with a combination of BOW and word embeddings inputs. The BOW input feeds into a dense feed-forward layer, and the word embeddings feed into an LSTM layer.\nThe code is mostly in Python using Keras and will be posted shortly.\n### Content\nThis dataset is a subset of a much larger dataset of 380+ million reddit comments. This dataset contains 4 million of the reddit comments, 2 million of which are the lowest scored (highly downvoted), and 2 million of which are the highest scored (highly upvoted).\nFor convenience, the dataset includes the text and other metadata of the parent comment.\n### Acknowledgements\nThe data comes from[https://pushshift.io/](https://pushshift.io/)\nexpand\\_moreView more\n## Usability\ninfo\n## License\nUnknown\n## Tags\n[Computer Science](https://www.kaggle.com/datasets?tags=12107-Computer+Science)[Social Science](https://www.kaggle.com/datasets?tags=11200-Social+Science)[NLP](https://www.kaggle.com/datasets?tags=13204-NLP)\nLoading...\nlightbulb\n## See what others are saying about this dataset\n### What have you used this dataset for?\n### How would you describe this dataset?\nOther\ntext\\_snippet\n## Metadata\ninsights\n## Activity Overview\n### Detail View\nkeyboard\\_arrow\\_up",
      "url": "https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction"
    },
    {
      "title": "Reddit Post Topic Model SP22",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/overview/description#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/overview/description)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/overview/description#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcompetitions%2Freddit-post-topic-model-sp22%2Foverview%2Fdescription)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcompetitions%2Freddit-post-topic-model-sp22%2Foverview%2Fdescription)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\nDrewWham \u00b7 Community Prediction Competition \u00b7 3 years ago\n\n# Reddit Post Topic Model SP22\n\nIdentify the subreddit each post came from using the text\n\n![](https://www.kaggle.com/competitions/36072/images/header)\n\n## Reddit Post Topic Model SP22\n\nLate Submissionmore\\_horiz\n\n[Overview](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/overview) [Data](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/data) [Code](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/code) [Models](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/models) [Discussion](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/discussion) [Leaderboard](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/leaderboard) [Rules](https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/rules)\n\n## Overview\n\nStart\n\nApr 12, 2022\n\n###### Close\n\nApr 30, 2022\n\n### Description\n\nlink\n\nkeyboard\\_arrow\\_up\n\nYour task will be to use all of the skills you have used in this course to develop a model to determine which subreddit comments originated from. Here you will be given a small dataset with examples of comments posted in 11 different subreddits. You must then predict the origin of a much larger set of comments by submitting to the leaderboard.\n\nYou are not allowed to gather more data on your own from these subreddits. You final solution must only use resources distributed within this competition.\n\nPlease note that these data have been authentically gathered from the internet, the thoughts and opinions expressed in the text belong solely to their original authors .\n\n### Evaluation\n\nlink\n\nkeyboard\\_arrow\\_up\n\nThe evaluation metric for this competition is [multi-class logloss](http://wiki.fast.ai/index.php/Log_Loss).\n\n.\n\n### Citation\n\nlink\n\nkeyboard\\_arrow\\_up\n\nDrewWham. Reddit Post Topic Model SP22. https://kaggle.com/competitions/reddit-post-topic-model-sp22, 2022. Kaggle.\n\nCite\n\n## Competition Host\n\nDrewWham\n\n## Prizes & Awards\n\nKudos\n\nDoes not award Points or Medals\n\n## Participation\n\n48 Entrants\n\n47 Participants\n\n47 Teams\n\n398 Submissions\n\n## Tags\n\nMulticlass Loss",
      "url": "https://www.kaggle.com/competitions/reddit-post-topic-model-sp22/overview/description"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions-setup"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions"
    }
  ]
}