{
  "query": "How do Kaggle winners combine TF-IDF text features with tabular metadata features for classification problems?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the winning Kaggle solutions that fused TF\u2011IDF text vectors with structured metadata (e.g., the Facebook Recruiting IV competition), the typical workflow is:\n\n1. **Load and clean the raw data**  \n   - Read the tabular file(s) (e.g., `bids.csv`) with\u202f`pandas`.  \n   - For any text columns, strip extra spaces or unwanted characters so the text can be tokenized cleanly. ([notebook.community](https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook))\n\n2. **Create TF\u2011IDF features from the text**  \n   ```python\n   from sklearn.feature_extraction.text import TfidfVectorizer\n   tfidf = TfidfVectorizer(max_features=100_000, ngram_range=(1,2))\n   X_text = tfidf.fit_transform(df['text_column'])\n   ```  \n   This yields a sparse matrix where each row represents a document (or \u201cfingerprint\u201d) as TF\u2011IDF weighted n\u2011grams. ([notebook.community](https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook))\n\n3. **Prepare the tabular metadata**  \n   - **Numerical columns**: scale them (e.g., `MinMaxScaler` or `StandardScaler`).  \n   - **Categorical columns**: encode them as integers or one\u2011hot vectors; for high\u2011cardinality fields you can also treat them as text and let TF\u2011IDF handle them.  \n   ```python\n   from sklearn.preprocessing import MinMaxScaler\n   scaler = MinMaxScaler()\n   X_num = scaler.fit_transform(df[numeric_cols])\n   ```  \n   - Convert the resulting dense array to a sparse format (`scipy.sparse.csr_matrix`) so it can be concatenated with the TF\u2011IDF matrix. ([notebook.community](https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook))\n\n4. **Concatenate TF\u2011IDF and metadata matrices**  \n   ```python\n   from scipy import sparse\n   X_combined = sparse.hstack([X_text, X_num])\n   ```  \n   The combined sparse matrix now contains both the textual representation and the scaled metadata for each row. ([notebook.community](https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook))\n\n5. **Train a high\u2011performance tabular model on the fused features**  \n   - Gradient\u2011boosted trees (e.g., XGBoost) work well with large sparse inputs.  \n   ```python\n   import xgboost as xgb\n   dtrain = xgb.DMatrix(X_combined, label=y)\n   params = {'objective':'binary:logistic', 'eval_metric':'auc', 'tree_method':'hist'}\n   model = xgb.train(params, dtrain, num_boost_round=500)\n   ```  \n   - In the referenced solution, a *bag* of 15 XGBoost models was trained on the same combined matrix and their predictions were averaged for the final submission. ([notebook.community](https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook))\n\n6. **(Optional) Feature selection / regularization**  \n   - Apply `SelectPercentile(chi2, percentile=10)` or similar on the TF\u2011IDF part before concatenation to reduce dimensionality and improve speed.  \n   - This step was used in the same notebook to keep only the most informative text features. ([notebook.community](https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook))\n\n7. **Validate and submit**  \n   - Use cross\u2011validation on the combined feature set to tune hyper\u2011parameters.  \n   - Generate predictions on the test split with the trained ensemble and submit to Kaggle.  \n\nFollowing these steps lets you replicate the proven strategy of merging TF\u2011IDF text vectors with tabular metadata, a pattern that has helped Kaggle winners achieve top\u2011tier scores in classification challenges.",
      "url": ""
    },
    {
      "title": "| notebook.community",
      "text": "| notebook.community https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook\n| notebook.community\nNone\nNone\n# Introduction\n\nIn this Jupyter notebook, I present my approach to the recent Kaggle competition, [Facebook Recruiting IV: Human or Robot?](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot). The main idea was to treat the modeling problem as somewhat of a text classification problem by sorting the bid data for each bidder in chronological order and using this sorted bid data as a text-based \"fingerprint\" of the bidder's activities. In addition to this, I computed some numerical features for the time differences between bids and the unique counts of the different entries. The final model ended up being a bag of 15 XGBoost models on a sparse matrix of tfidf-vectorized text features concatenated with scaled numerical features.\n\n# Libraries\n\nFor the most part, I used Python standard libraries and the scientific Python libraries available in the Anaconda distribution ( `pandas`, `scikit-learn`, `scipy` and `numpy`). The only slightly more exotic library is `XGBoost`. For installing `XGBoost` on Windows, [these](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13043/run-xgboost-from-windows-and-python) instructions by Alejandro Simkievich and [this](https://github.com/dmlc/xgboost/tree/master/windows) repository on `XGBoost`'s Github profile were helpful.\n\n\nIn\u00a0[1]:\n\nimport pandas as pd\nimport re\nimport gc\nimport numpy as np\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.externals import joblib\nimport xgboost as xgb\n\n\n# Feature engineering\n\nFirst, we read the bid data:\n\n\nIn\u00a0[2]:\n\ndf_bids = pd.read_csv('data/bids.csv')\n\n\nEventually, we will tokenize the bid information on spaces, so we remove any additional spaces from the data.\n\n\nIn\u00a0[3]:\n\ndf_bids = df_bids.replace({' ': ''}, regex = True) #remove spaces\n\n\nA key part of the approach is dealing with the bids in chronological order. Hence, we sort the bids in ascending order of bidder_ids and time.\n\n\nIn\u00a0[4]:\n\ndf_bids_sorted = df_bids.sort(['bidder_id', 'time'], ascending = [True, True])\n\n\nThen we initialize a `bids`-dataframe where the aggregated bid information for each bidder_id will be gathered.\n\n\nIn\u00a0[5]:\n\n#dataframe for aggregated bid data\nbids = pd.DataFrame(data = df_bids_sorted['bidder_id'].unique(), columns = ['bidder_id'],\n index = df_bids_sorted['bidder_id'].unique())\n\n\nAs a first entry into the `bids`-dataframe, we count the number of auctions for each bidder_id.\n\n\nIn\u00a0[6]:\n\n#auction counts\ncounts = df_bids_sorted.groupby('bidder_id')['bidder_id'].agg('count')\nbids['auction_count_num'] = counts\n\n\nThen, we compute the time differences between bids into the `df_bids_sorted`-dataframe. These time differences are included in both numeric and string form. I noticed that there were some time differences that occur quite frequently, and a text processing of time differences should be able to identify these types of patterns in the data.\n\n\nIn\u00a0[7]:\n\ntimediff = df_bids_sorted.groupby('bidder_id')['time'].diff()\ntimediff_str = timediff.astype(str).fillna('')\ndf_bids_sorted['timediff_num'] = timediff\ndf_bids_sorted['timediff'] = timediff_str\n\n\nIn the following, the main aggregation step for the bid data is performed. For each column, the data is first converted into slightly more readable form. For example for urls the data is transformed from the form `0esea7scvgr82he` to the form `url_0esea7scvgr82he`. This was done to make the different entries more identifiable in the case of evaluating feature importances, but in the end this was not utilized to any significant extent. The entries for each bidder were concatenated with a space-delimiter to generate the aggregated text data. The result is a long string of space-delimited entries for each column. E.g. for the device-column, we can have a string of the type `device_phone167 device_phone172 device_phone167` etc.\n\nIn addition to generating the concatenated text data, the number of unique entries was also computed for each column and bidder. Throughout this notebook, we add a `_text`-suffix to text columns and a `_num`-suffix to numerical columns. This allows for the two types of columns to be selected with regular expressions later on.\n\n\nIn\u00a0[8]:\n\n#turn feature sequences into text\ntext_cols = ['auction', 'merchandise', 'device', 'timediff', 'country', 'ip', 'url']\nfor var in text_cols:\n df_bids_sorted[var] = var + \"_\" + df_bids_sorted[var].fillna(\"\")\n text_str = var + '_text'\n count_str = var + '_nunique_num'\n bids[text_str] = df_bids_sorted.groupby('bidder_id')[var].apply(lambda x: \"%s\" % ' '.join(x))\n bids[count_str] = df_bids_sorted.groupby('bidder_id')[var].nunique()\n\n\nOne idea I had was that the distribution of time differences between bids could be a significant predictor of bot activity. Hence, I computed a number of different descriptive statistics related to the times and time differences. In the following, the min and max times and time differences are computed along with the time difference range, mean, median and the first to ninth deciles.\n\n\nIn\u00a0[9]:\n\nmax_time = df_bids_sorted.groupby('bidder_id')['time'].max()\nbids['maxtime_num'] = max_time\nmin_time = df_bids_sorted.groupby('bidder_id')['time'].min()\nbids['mintime_num'] = min_time\nmax_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].max()\nmax_diff = max_diff.fillna(max_diff.mean())\nbids['maxdiff_num'] = max_diff\nmin_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].max()\nmin_diff = min_diff.fillna(min_diff.mean())\nbids['mindiff_num'] = min_diff\nrange_diff = max_diff - min_diff\nbids['rangediff_num'] = range_diff\nmean_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].mean()\nmean_diff = mean_diff.fillna(mean_diff.mean())\nbids['meandiff_num'] = mean_diff\nmedian_diff = df_bids_sorted.groupby('bidder_id')['timediff_num'].median()\nmedian_diff = median_diff.fillna(median_diff.mean())\nbids['mediandiff_num'] = median_diff\nfor q in [0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]:\n q_string = 'diff_quantile_num_' + str(q).replace('.', '_')\n q_temp = df_bids_sorted.groupby('bidder_id')['timediff_num'].quantile(q)\n q_temp = q_temp.fillna(q_temp.mean())\n bids[q_string] = q_temp\n\n\nThis completes the bid aggregation step. After this, we read the `train`- and `test`-data and merge it with the bid data.\n\n\nIn\u00a0[10]:\n\ndf_train = pd.read_csv('data/train.csv')\ndf_test = pd.read_csv('data/test.csv')\ndf_combo = df_train.append(df_test)\ndf_combo['address_text'] = 'address_' + df_combo['address'].fillna('')\ndf_combo['account_text'] = 'account_' + df_combo['payment_account'].fillna('')\ndf_combo = df_combo.merge(bids, how = 'left', left_on = ['bidder_id'], right_on = ['bidder_id'])\n\n\nWe delete the redundant dataframes and run garbage collection.\n\n\nIn\u00a0[11]:\n\ndel df_train\ndel df_test\ndel df_bids\ndel df_bids_sorted\ndel bids\ngc.collect();\n\n\nUsing regular expressions, the text and numeric columns are identified. For missing values in numeric columns, we fill in the column mean; for missing values in text columns, we fill in an empty string.\n\n\nIn\u00a0[12]:\n\nnum_cols = filter(re.compile('num').search, df_combo.columns)\ntext_cols = filter(re.compile('text').search, df_combo.columns)\nfor col in num_cols:\n df_combo[col] = df_combo[col].fillna(df_combo[col].mean())\nfor col in text_cols:\n df_combo[col] = df_combo[col].fillna('')\n\n\nNow, we split the `df_combo`-dataframe into train and test data. First, we grab the columns for the test data and check that the order matches that in the sample submission.\n\n\nIn\u00a0[13]:\n\nsample = pd.read_csv('submissions/sampleSubmission.csv')\ntest_dat = df_combo[df_combo.bidder_id.isin(sample.bidder_id)]\n#test\nprint (sample.bidder_id.values==test_dat['bidder_id'].values).all()\n\n\nTrue\n\n\nWe put the numeric columns into matrices `xtrain` and `xtest`. The processed sparse text fre...",
      "url": "https://notebook.community/wallinm1/kaggle-facebook-bot/facebook_notebook"
    },
    {
      "title": "GitHub - lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition",
      "text": "[Skip to content](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[lx920](https://github.com/lx920)/ **[Disaster-Tweets-TFIDF-ML-Kaggle-Competition](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Flx920%2FDisaster-Tweets-TFIDF-ML-Kaggle-Competition) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Flx920%2FDisaster-Tweets-TFIDF-ML-Kaggle-Competition)\n- [Star\\\n1](https://github.com/login?return_to=%2Flx920%2FDisaster-Tweets-TFIDF-ML-Kaggle-Competition)\n\n\n[1\\\nstar](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/stargazers) [0\\\nforks](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/forks) [Branches](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/branches) [Tags](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/tags) [Activity](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/activity)\n\n[Star](https://github.com/login?return_to=%2Flx920%2FDisaster-Tweets-TFIDF-ML-Kaggle-Competition)\n\n[Notifications](https://github.com/login?return_to=%2Flx920%2FDisaster-Tweets-TFIDF-ML-Kaggle-Competition) You must be signed in to change notification settings\n\n# lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/branches) [Tags](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[9 Commits](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/commits/main/) |\n| [Disaster Tweets TFIDF.ipynb](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/Disaster%20Tweets%20TFIDF.ipynb) | [Disaster Tweets TFIDF.ipynb](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/Disaster%20Tweets%20TFIDF.ipynb) |  |  |\n| [README.md](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/README.md) | [README.md](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/README.md) |  |  |\n| [test.csv](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/test.csv) | [test.csv](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/test.csv) |  |  |\n| [train.csv](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/train.csv) | [train.csv](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/blob/main/train.csv) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Disaster-Tweets--Kaggle-Competition\n\nThis is a Kaggle Competition aiming to correctly identify which tweets are related to real natural disasters.\n\nThe competition is on-going, and the link is: [https://www.kaggle.com/c/nlp-getting-started](https://www.kaggle.com/c/nlp-getting-started)\n\n# Dataset Brief\n\n[https://www.kaggle.com/c/nlp-getting-started/data](https://www.kaggle.com/c/nlp-getting-started/data)\n\nThe dataset contains 7613 tweets that may or may not be related to naturla disaster.\n\nThe independent variable are unstructed texts within a tweet.\n\nThe dependent variable we are predicting is 1 and 0. Disaster, and non-Disaster.\n\n# Main Packages\n\n- pandas, numpy, seaborn, matplotlib\n- NLTK\n- sklearn\n\n# Steps of Project\n\n1: Data Visualization\n\n2: Data Preprocessing\n\n3: NLP preprocessing\n\n4: Model Building\n\n# Data Visualization\n\nIn this section, I used matplotlib to perform EDA on the dataset\n\n- Classification class balance\n- Distribution of tweet length\n- Most Frequent Key words and stop words\n\n# Data Preprocessing\n\nRemove the garbage in the data!\n\n- Use regexp to remove URLs, https, emoticons and signs.\n- Remove stopwords in NLTK stopwords dictionary\n- Perform porter stemming on texts\n- set all texts to lowercase\n\n# NLP Preprocessing\n\nI decided to use TF-IDF over Bag of Words.\n\n- Use TF-IDF vertorizor to turn cleaned tweets into vectors.\n- set up n-gram as 1, and idf smoothing to deal with edge cases.\n\n# Model Training\n\nI applied 5 ML models on the TF-IDF processed data. And measured their performance.\n\n- Logistic Regression\n\n0.80 accuracy score.\n\n- Random Forest (Untuned)\n\n0.78 accuracy score\n\ncould be better because I did not tune parameters, takes too long.\n\n- XGBoost\n\n0.77 accuracy score\n\n- Voting Ensemble\n\nVoting Ensemble consists of logistic regression, Naive Bayes and XGBoost\n\n0.82 accuracy score\n\n- Naive Bayes\n\n0.81 accuracy score\n\n# Conclusion and Submission\n\n- The Voting Ensemble of LR + NB + XGB seems to have highest Accuracy Score\n- Predicted Kaggle Test dataset and made submission\n- Final submission score 0.794\n\n## About\n\nNo description, website, or topics provided.\n\n### Resources\n\n[Readme](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition#readme-ov-file)\n\n[Activity](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/activity)\n\n### Stars\n\n[**1**\\\nstar](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/watchers)\n\n### Forks\n\n[**0**\\\nforks](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Flx920%2FDisaster-Tweets-TFIDF-ML-Kaggle-Competition&report=lx920+%28user%29)\n\n## [Releases](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/lx920/packages?repo_name=Disaster-Tweets-TFIDF-ML-Kaggle-Competition)\n\nNo packages published\n\n## Languages\n\n- [Jupyter Notebook100.0%](https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition/search?l=jupyter-notebook)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/lx920/Disaster-Tweets-TFIDF-ML-Kaggle-Competition"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - kili-ilo/kaggle-crisis: Machine learning model to correlate financial report word frequencies with the 07-08 financial crisis\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/kili-ilo/kaggle-crisis)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/kili-ilo/kaggle-crisis)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kili-ilo/kaggle-crisis)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[kili-ilo](https://github.com/kili-ilo)/**[kaggle-crisis](https://github.com/kili-ilo/kaggle-crisis)**Public\n* [Notifications](https://github.com/login?return_to=/kili-ilo/kaggle-crisis)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/kili-ilo/kaggle-crisis)\n* [Star2](https://github.com/login?return_to=/kili-ilo/kaggle-crisis)\nMachine learning model to correlate financial report word frequencies with the 07-08 financial crisis\n### License\n[MIT license](https://github.com/kili-ilo/kaggle-crisis/blob/master/LICENSE)\n[2stars](https://github.com/kili-ilo/kaggle-crisis/stargazers)[0forks](https://github.com/kili-ilo/kaggle-crisis/forks)[Branches](https://github.com/kili-ilo/kaggle-crisis/branches)[Tags](https://github.com/kili-ilo/kaggle-crisis/tags)[Activity](https://github.com/kili-ilo/kaggle-crisis/activity)\n[Star](https://github.com/login?return_to=/kili-ilo/kaggle-crisis)\n[Notifications](https://github.com/login?return_to=/kili-ilo/kaggle-crisis)You must be signed in to change notification settings\n# kili-ilo/kaggle-crisis\nmaster\n[Branches](https://github.com/kili-ilo/kaggle-crisis/branches)[Tags](https://github.com/kili-ilo/kaggle-crisis/tags)\n[](https://github.com/kili-ilo/kaggle-crisis/branches)[](https://github.com/kili-ilo/kaggle-crisis/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[15 Commits](https://github.com/kili-ilo/kaggle-crisis/commits/master/)\n[](https://github.com/kili-ilo/kaggle-crisis/commits/master/)\n|\n[data](https://github.com/kili-ilo/kaggle-crisis/tree/master/data)\n|\n[data](https://github.com/kili-ilo/kaggle-crisis/tree/master/data)\n|\n|\n|\n[predictions](https://github.com/kili-ilo/kaggle-crisis/tree/master/predictions)\n|\n[predictions](https://github.com/kili-ilo/kaggle-crisis/tree/master/predictions)\n|\n|\n|\n[report](https://github.com/kili-ilo/kaggle-crisis/tree/master/report)\n|\n[report](https://github.com/kili-ilo/kaggle-crisis/tree/master/report)\n|\n|\n|\n[result\\_of\\_RF](https://github.com/kili-ilo/kaggle-crisis/tree/master/result_of_RF)\n|\n[result\\_of\\_RF](https://github.com/kili-ilo/kaggle-crisis/tree/master/result_of_RF)\n|\n|\n|\n[results](https://github.com/kili-ilo/kaggle-crisis/tree/master/results)\n|\n[results](https://github.com/kili-ilo/kaggle-crisis/tree/master/results)\n|\n|\n|\n[DT.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/DT.py)\n|\n[DT.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/DT.py)\n|\n|\n|\n[LICENSE](https://github.com/kili-ilo/kaggle-crisis/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/kili-ilo/kaggle-crisis/blob/master/LICENSE)\n|\n|\n|\n[README.md](https://github.com/kili-ilo/kaggle-crisis/blob/master/README.md)\n|\n[README.md](https://github.com/kili-ilo/kaggle-crisis/blob/master/README.md)\n|\n|\n|\n[RF\\_rf\\_idf.r](https://github.com/kili-ilo/kaggle-crisis/blob/master/RF_rf_idf.r)\n|\n[RF\\_rf\\_idf.r](https://github.com/kili-ilo/kaggle-crisis/blob/master/RF_rf_idf.r)\n|\n|\n|\n[RF\\_wc.r](https://github.com/kili-ilo/kaggle-crisis/blob/master/RF_wc.r)\n|\n[RF\\_wc.r](https://github.com/kili-ilo/kaggle-crisis/blob/master/RF_wc.r)\n|\n|\n|\n[SVM\\_NB.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/SVM_NB.py)\n|\n[SVM\\_NB.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/SVM_NB.py)\n|\n|\n|\n[convert\\_predictions.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/convert_predictions.py)\n|\n[convert\\_predictions.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/convert_predictions.py)\n|\n|\n|\n[vote.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/vote.py)\n|\n[vote.py](https://github.com/kili-ilo/kaggle-crisis/blob/master/vote.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Kaggle Crisis\n[](#kaggle-crisis)\nMachine learning model to correlate financial report word frequencies with the 07-08 financial crisis\nCreated by Jan Van Bruggen, Yamei Ou, and Obinna Eruchalu\nfor the Kaggle competition at[https://inclass.kaggle.com/c/cs-ee-155-kaggle-competition](https://inclass.kaggle.com/c/cs-ee-155-kaggle-competition)\n## Kaggle Competition Instructions\n[](#kaggle-competition-instructions)\nIn this competition, you are going to predict the appearance of the financial crisis era\nbased on a collection of words extracted from thousands of quarterly and yearly reports of large financial service companies.\nAll the data sources are obtained from the U.S. Securities and Exchange Commission.\nPublicly traded companies are required by law to disclose information on an ongoing basis,\nand all large domestic issuers must submit annual reports on Form 10-K and quarterly reports on Form 10-Q.\nWe downloaded those forms of most financial service companies in recent 10 years.\nOn the page of Great Recession on Wikipedia, the U.S. Recession is defined in period between December 2007 and June 2009.\nWe labeled '1' on the report if it was published during the U.S. Recession era and '0' otherwise.\nThe features you are going to have is a bag of words.\nThese words are the top 500 most frequent word stems in the reports\npublished in the U.S. Recession era with stop words filtered out.\nYour task is to reveal the correlation of the frequency of words and the appearance of the financial crisis.\n## Our Model\n[](#our-model)\nWe aggregated the out-of-sample predictions of multiple models to determine our final predictions.\nFirst we used the word count and tf-idf datasets to train several different classifier models, including\nRandom Forest, AdaBoost, Decision Tree, Naive Bayes, and Support Vector Machine (both linear- and inifinite-kernel).\nAfter examining the 5-fold cross-validation error of each dataset/classifier combination,\nwe decided that the best models to combine were\nRandom Forest (on word count), Random Forest (on tf-idf), and AdaBoost (on word count).\nWe aggregated these predictions with a simple majority vote,\nand the aggregated predictions achieved 93% accuracy on the competition's final leaderboard.\n[![](https://camo.githubusercontent.com/14089e6520a3fa859ff37f98ba986e56d559d41183ea286e51d32dae0e4cb800/687474703a2f2f692e696d6775722e636f6d2f796e71596750772e676966)](https://camo.githubusercontent.com/14089e6520a3fa859ff37f98ba986e56d559d41183ea286e51d32dae0e4cb800/687474703a2f2f692e696d6775722e636f6d2f796e71596750772e676966)\n## About\nMachine learning model to correlate financial report w...",
      "url": "https://github.com/JanCVanB/kaggle-crisis"
    },
    {
      "title": "GitHub - 5m0k3/osic-pulmonary-fibrosis-tf: Tensorflow based training, inference and feature engineering pipelines used in OSIC Kaggle Competition",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[5m0k3](https://github.com/5m0k3)/ **[osic-pulmonary-fibrosis-tf](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf)** Public\n\n- [Notifications](https://github.com/login?return_to=%2F5m0k3%2Fosic-pulmonary-fibrosis-tf) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2F5m0k3%2Fosic-pulmonary-fibrosis-tf)\n- [Star\\\n1](https://github.com/login?return_to=%2F5m0k3%2Fosic-pulmonary-fibrosis-tf)\n\n\nTensorflow based training, inference and feature engineering pipelines used in OSIC Kaggle Competition\n\n[1\\\nstar](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/stargazers) [0\\\nforks](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/forks) [Branches](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/branches) [Tags](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/tags) [Activity](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/activity)\n\n[Star](https://github.com/login?return_to=%2F5m0k3%2Fosic-pulmonary-fibrosis-tf)\n\n[Notifications](https://github.com/login?return_to=%2F5m0k3%2Fosic-pulmonary-fibrosis-tf) You must be signed in to change notification settings\n\n# 5m0k3/osic-pulmonary-fibrosis-tf\n\nmaster\n\n[Branches](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/branches) [Tags](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[4 Commits](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/commits/master/) |\n| [.gitattributes](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/.gitattributes) | [.gitattributes](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/.gitattributes) |\n| [Feature-Engineering.ipynb](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/Feature-Engineering.ipynb) | [Feature-Engineering.ipynb](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/Feature-Engineering.ipynb) |\n| [README.md](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/README.md) | [README.md](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/README.md) |\n| [\\[INFERENCE\\]Image+QuantileRegression.ipynb](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/%5BINFERENCE%5DImage%2BQuantileRegression.ipynb) | [\\[INFERENCE\\]Image+QuantileRegression.ipynb](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/%5BINFERENCE%5DImage%2BQuantileRegression.ipynb) |\n| [\\[TRAIN\\]\\_TF\\_EfficientNet.ipynb](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/%5BTRAIN%5D_TF_EfficientNet.ipynb) | [\\[TRAIN\\]\\_TF\\_EfficientNet.ipynb](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/blob/master/%5BTRAIN%5D_TF_EfficientNet.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# TensorFlow based Quantile Regression solution - OSIC Pulmonary Fibrosis\n\nA complete tensorflow pipeline for training, inference and feature extraction notebooks used in Kaggle competition [OSIC Pulmonary Fibrosis](https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression) (July-Oct 2020)\n\n## Table of Contents\n\n- [Brief overview of the competition data](https://github.com/github.com#Brief-overview-of-the-competition-data)\n- [Notebooks description](https://github.com/github.com#Notebooks-description)\n  - [Feature Engineering notebook](https://github.com/github.com#Feature-Engineering-notebook)\n  - [\\[TRAIN\\] notebook](https://github.com/github.com#TRAIN-notebook)\n  - [\\[INFERENCE\\] Submission notebook](https://github.com/github.com#INFERENCE-Submission-notebook)\n- [How to use](https://github.com/github.com#How-to-use)\n\n## Brief overview of the competition data\n\nThe data contained of dicom (images + metadata) data of chest X-Ray of patients along with tabular data like smoking status, age, Forced Vital Capacity (FVC) values etc.\nSlices preview of chest X-Ray of a patient are as:\nLung mask segmentation process deployed was (3rd image - final mask)\n3D plot of stacked 2D segmented masks to form a lung produces\nApart from the dicom data the tabular data was as follows\n\n## Notebooks description\n\nA brief content description is provided here, for detailed descriptions check the notebook\n\n### Feature Engineering notebook\n\nA major task was engineering and extracting features from the dcm slices\nIn total I engineered 5 features as follows\n\n1. Chest Volume:\n\\- Calculated through numpy.trapz() integration over all 2D slices using pixel count, sliceThickness and pixelSpacing (Voxel spacing) metadata in the dcm file\n\\- Dealt with the inconsistencies in the data and final distplot produced was\n\n2. Chest Area:\n\\- Maximum area of chest calculated using the average of 3 middle most slices in same fashion as Chest Volume\n\\- distplot\n\n3. Lung - Tissue ratio:\n\\- Ratio of pixel area of segmented lung mask to the total tissue pixel area as in original dcm file\n\\- The ideology behind being this feature was to detect lung shrinkage inside chest\n\\- distplot\n\n4. Chest Height:\n\\- Chest height calculated using sliceThickness and number of slices forming the lung\n\\- distplot\n\n5. Height of the Patient:\n\\- Approximate height calculated using FVC values and age of a patient according to formulaes and observations made from [external medical research data](https://en.wikipedia.org/wiki/Vital_capacity#Formulas)\n\\- distplot\n\n\nPlots of Features vs FVC / Percent\n\n### \\[TRAIN\\] notebook\n\nEffNet train notebook described below, Custom tf tabular data only model listed in \\[INFERENCE\\] itself\n\n1. Pre-Processing:\n\\- Handled the various sizes and missing slices issues\n\\- Stratified 5 fold split based on PatientID\n\n2. Augmentations:\n\\- Albumentations - RandomSizedCrop, Flips, Gaussian Blur, CoarseDropout, Rotate (0-90)\n\n3. Configurations:\n\\- Optimizer - NAdam\n\\- LR Scheduler - ReduceLRonPlateau (initial LR = 0.0005, patience = 5, factor = 0.5)\n\\- Model - EfficientNet B5\n\\- Input Size - 512 \\* 512\n\n\n### \\[INFERENCE\\] Submission notebook\n\nContains custom tabular data model training and inference too\n\n1. Custom Net:\n\\- A tiny net using given tabular data and engineered features on swish activated dense layers\n\\- Pinball loss function for multiple quantiles was used, the difference in first and last quantiles was used as uncertainty measure\n\n2. Ensemble:\n\\- Final submission made using ensemble of both effnet image and custom model\n\n\n## How to use\n\nJust change the directories according to your environment.\n\nGoogle Colab deployed versions are available for\n**\\[TRAIN\\] Effnet** **\\[TRAIN\\] Base Custom Net**\n\n## About\n\nTensorflow based training, inference and feature engineering pipelines used in OSIC Kaggle Competition\n\n### Topics\n\n[tensorflow](https://github.com/topics/tensorflow) [tabular-data](https://github.com/topics/tabular-data) [kaggle](https://github.com/topics/kaggle) [chest-xray-images](https://github.com/topics/chest-xray-images) [feature-engineering](https://github.com/topics/feature-engineering) [medical-image-processing](https://github.com/topics/medical-image-processing) [lung-segmentation](https://github.com/topics/lung-segmentation) [quantile-regression](https://github.com/topics/quantile-regression) [pseudo-labeling](https://github.com/topics/pseudo-labeling) [efficientnet](https://github.com/topics/efficientnet)\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/activity)\n\n### Stars\n\n[**1**\\\nstar](https://github.com/5m0k3/osic-pulmonary-fibrosis-tf/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/5m0k3/os...",
      "url": "https://github.com/5m0k3/osic-pulmonary-fibrosis-tf"
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.1.1 documentation",
      "text": "# Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models)\n\nHere we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, **raw text data** is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c [What\u2019s happening inside?](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-architecture)\u201d of\n[Text Prediction - Multimodal Table with Text](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-multimodal) (used by AutoGluon\u2019s\n`TextPredictor`).\n\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nfrom autogluon.tabular import TabularPredictor\nimport mxnet as mx\n\nnp.random.seed(123)\nrandom.seed(123)\nmx.random.seed(123)\n```\n\n## Product Sentiment Analysis Dataset [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#product-sentiment-analysis-dataset)\n\nWe consider the product sentiment analysis dataset from a [MachineHack\\\nhackathon](https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard).\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).\n\n```\n!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n\n```\n\n```\n--2021-03-10 04:16:17--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 673.33K  2.05MB/s    in 0.3s\n\n2021-03-10 04:16:18 (2.05 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n\n--2021-03-10 04:16:18--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\n\nproduct_sentiment_m 100%[===================>]  73.75K   490KB/s    in 0.2s\n\n2021-03-10 04:16:19 (490 KB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n\n--2021-03-10 04:16:19--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 304.88K   943KB/s    in 0.3s\n\n2021-03-10 04:16:20 (943 KB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n\n```\n\n```\nsubsample_size = 2000  # for quick demo, try setting to larger values\nfeature_columns = ['Product_Description', 'Product_Type']\nlabel = 'Sentiment'\n\ntrain_df = pd.read_csv('product_sentiment_machine_hack/train.csv', index_col=0).sample(2000, random_state=123)\ndev_df = pd.read_csv('product_sentiment_machine_hack/dev.csv', index_col=0)\ntest_df = pd.read_csv('product_sentiment_machine_hack/test.csv', index_col=0)\n\ntrain_df = train_df[feature_columns + [label]]\ndev_df = dev_df[feature_columns + [label]]\ntest_df = test_df[feature_columns]\nprint('Number of training samples:', len(train_df))\nprint('Number of dev samples:', len(dev_df))\nprint('Number of test samples:', len(test_df))\n```\n\n```\nNumber of training samples: 2000\nNumber of dev samples: 637\nNumber of test samples: 2728\n```\n\nThere are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.\n\n```\ntrain_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 4532 | they took away the lego pit but replaced it wi... | 0 | 1 |\n| 1831 | #Apple to Open Pop-Up Shop at #SXSW \\[REPORT\\]: ... | 9 | 2 |\n| 3536 | RT @mention False Alarm: Google Circles Not Co... | 5 | 1 |\n| 5157 | Will Google reveal a new social network called... | 9 | 2 |\n| 4643 | Niceness RT @mention Less than 2 hours until w... | 6 | 3 |\n\n```\ndev_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 3170 | Do it. RT @mention Come party w/ Google tonigh... | 3 | 3 |\n| 6301 | Line for iPads at #SXSW. Doesn't look too bad!... | 6 | 3 |\n| 5643 | First up: iPad Design Headaches (2 Tablets, Ca... | 6 | 2 |\n| 1953 | #SXSW: Mint Talks Mobile App Development Chall... | 9 | 2 |\n| 2658 | \u0089\u00db\u00cf@mention Apple store downtown Austin open t... | 9 | 2 |\n\n```\ntest_df.head()\n```\n\n| Product\\_Description | Product\\_Type |\n| --- | --- |\n| Text\\_ID |\n| --- |\n| 5786 | RT @mention Going to #SXSW? The new iPhone gui... | 7 |\n| 5363 | RT @mention 95% of iPhone and Droid apps have ... | 9 |\n| 6716 | RT @mention Thank you to @mention for letting ... | 9 |\n| 4339 | #Thanks @mention we're lovin' the @mention app... | 7 |\n| 66 | At #sxsw? @mention / @mention wanna buy you a ... | 9 |\n\n## AutoGluon Tabular with Multimodal Support [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#autogluon-tabular-with-multimodal-support)\n\nTo utilize the `TextPredictor` model inside of `TabularPredictor`,\nwe must specify the `hyperparameters = 'multimodal'` in AutoGluon\nTabular. Internally, this will train multiple tabular models as well as\nthe TextPredictor model, and then combine them via either a weighted\nensemble or stack ensemble, as explained in [AutoGluon Tabular\\\nPaper](https://arxiv.org/pdf/2003.06505.pdf). If you do not specify\n`hyperparameters = 'multimodal'`, then AutoGluon Tabular will simply\nfeaturize text fields using N-grams and train only tabular models (which\nmay work better if your text is mostly uncommon strings/vocabulary).\n\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\npredictor.fit(train_df, hyperparameters='multimodal')\n```\n\n```\nBeginning AutoGluon training ...\nAutoGluon will save models to \"ag_tabular_product_sentiment_multimodal/\"\nAutoGluon Version:  0.1.1b20210310\nTrain Data Rows:    2000\nTrain Data Columns: 2\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n    4 unique label values:  [1, 2, 3, 0]\n    If 'multiclass' is not the correct ...",
      "url": "https://auto.gluon.ai/scoredebugweight/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "",
      "text": "# Combining Categorical and Numerical Features with Text in BERT\n\n29 Jun 2021\n\nIn this tutorial we\u2019ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\n\nTo help motivate our discussion, we\u2019ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\n\n- The age of the reviewer (numerical feature)\n- The number of upvotes on the review (numerical feature)\n- The department and category of the clothing item (categorical features)\n\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\n\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\n\nIn Section 2 of this Notebook, I\u2019ve implemented four different \u201cbaseline\u201d strategies which score fairly well, but which don\u2019t incorporate all of the features together.\n\nThen, in Section 3, I\u2019ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\n\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\n\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\n\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\n\nBy Chris McCormick\n\n# Contents\n\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\n\n# S1. Clothing Review Dataset\n\n## 1.1. Download & Parse\n\nRetrieve the .csv file for the dataset.\n\n```\nimport gdown\n\nprint('Downloading dataset...\\n')\n\n# Download the file.\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\n                'Womens Clothing E-Commerce Reviews.csv',\n                quiet=False)\n\nprint('\\n\\nDONE.')\n```\n\n```\nDownloading dataset...\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\nTo: /content/Womens Clothing E-Commerce Reviews.csv\n8.48MB [00:00, 48.7MB/s]\n\nDONE.\n\n```\n\nParse the dataset csv file into a pandas DataFrame.\n\n```\nimport pandas as pd\n\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n\ndata_df.head()\n```\n\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\n\n_Features_\n\n\u201c **Recommended IND**\u201d is the label we are trying to predict for this dataset. \u201c1\u201d means the reviewer recommended the product and \u201c0\u201d means they do not.\n\nThe following are _categorical_ features:\n\n- Division Name\n- Department Name\n- Class Name\n- Clothing ID\n\nAnd the following are _numerical_ features:\n\n- Age\n- Rating\n- Positive Feedback Count\n\n_Feature Analysis_\n\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\n\nNote that, in addition to the \u201cRecommended\u201d label, there is also a \u201c **Rating**\u201d column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We\u2019ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\n\n## 1.2. Train-Validation-Test Split\n\nI want to use the same training, validation, and test splits for all of the approaches we try so that it\u2019s a fair comparison.\n\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\n\nTo solve this, we\u2019re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\n\n```\nimport random\nimport numpy as np\n\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\ntrain_size = int(0.8 * len(data_df))\nval_size = int(0.1 * len(data_df))\ntest_size = len(data_df) - (train_size + val_size)\n\n# Sanity check the sizes.\nassert((train_size + val_size + test_size) == len(data_df))\n\n# Create a list of indeces for all of the samples in the dataset.\nindeces = np.arange(0, len(data_df))\n\n# Shuffle the indeces randomly.\nrandom.shuffle(indeces)\n\n# Get a list of indeces for each of the splits.\ntrain_idx = indeces[0:train_size]\nval_idx = indeces[train_size:(train_size + val_size)]\ntest_idx = indeces[(train_size + val_size):]\n\n# Sanity check\nassert(len(train_idx) == train_size)\nassert(len(test_idx) == test_size)\n\n# With these lists, we can now select the corresponding dataframe rows using,\n# e.g., train_df = data_df.iloc[train_idx]\n\nprint('  Training size: {:,}'.format(train_size))\n...",
      "url": "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "Otto Product Classification Winner\u2019s Interview: 2nd place, Alexander Guschin \u00af\\_(\u30c4)_/\u00af",
      "text": "<div><div><div><div><a href=\"https://medium.com/@kaggleteam?source=post_page---byline--e9248c318f30---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/kaggle-blog?source=post_page---byline--e9248c318f30---------------------------------------\"><div><p></p></div></a></div></div><figure></figure><p>The <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/otto-group-product-classification-challenge\">Otto Group Product Classification Challenge</a> made Kaggle history as our most popular competition ever. <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/aguschin\">Alexander Guschin</a> finished <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/otto-group-product-classification-challenge/leaderboard/private\">in 2nd place</a> ahead of 3,845 other data scientists. In this blog, Alexander shares his stacking centered approach and explains why you should never underestimate the nearest neighbours algorithm.</p><h2>The Basics</h2><h2>What was your background prior to entering this challenge?</h2><p>I have some theoretical understanding of machine learning thanks to my base institute ( <a href=\"https://web.archive.org/web/20180114032307/http://mipt.ru/en/\">Moscow Institute of Physics and Technology</a>) and our professor <a href=\"https://web.archive.org/web/20180114032307/https://scholar.google.com/citations?user=KIW4fnsAAAAJ\">Konstantin Vorontsov</a>, one of the top Russian machine learning specialists. As for my acquaintance with practical problems, another great Russian data scientist who once was Top-1 on Kaggle, <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/dyakonov\">Alexander D\u2019yakonov</a>, used to teach a course on practical machine learning every autumn which gave me very good basis. Kagglers may know this course as <a href=\"https://web.archive.org/web/20180114032307/http://www.machinelearning.ru/wiki/index.php?title=%D0%A1%D0%BF%D0%B5%D1%86%D0%BA%D1%83%D1%80%D1%81_%C2%AB%D0%9F%D1%80%D0%B8%D0%BA%D0%BB%D0%B0%D0%B4%D0%BD%D1%8B%D0%B5_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%C2%BB\">PZAD</a>.</p><h2>How did you get started competing on Kaggle?</h2><p>I got started in 2014\u2019s autumn in \u201c <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/forest-cover-type-prediction\">Forest Cover Type Prediction</a>\u201d. At that time I had no experience in solving machine learning problems. I found excellent benchmarks in \u201c <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/titanic\">Titanic: Machine Learning from Disaster</a> \u201c which helped me a lot. After that I understand that machine learning is extremely interesting for me and just tried to participate in every competition I could.</p><h2>What made you decide to enter this competition?</h2><p>I wanted to check some ideas for my bachelor work. I liked that Otto competition has quite reliable dataset. You can check everything on cross-validation and changes on CV were close enough to leaderboard. Also, the spirit of competition is quite appropriate for checking ideas.</p><h2>Let\u2019s Get Technical</h2><h2>What preprocessing and supervised learning methods did you use?</h2><p><strong>The main idea of my solution is stacking.</strong> Stacking helps you to combine different methods\u2019 predictions of Y (or labels when it comes to multiclass problems) as \u201cmetafeatures\u201d. Basically, to obtain metafeature for train, you split your data into K folds, training K models on K-1 parts while making prediction for 1 part that was left aside for each K-1 group. To obtain metafeature for test, you can average predictions from these K models or make single prediction based on all train data. After that you train metaclassifier on features &amp; metafeatures and average predictions if you have several metaclassifiers.</p><p>In the beginning of working on the competition I found useful to split data in two groups : (1) train &amp; test, (2) TF-IDF(train) &amp; TF-IDF(test). Many parts of my solution use these two groups in parallel.</p><p><strong>Talking about supervised methods, I\u2019ve found that Xgboost and neural networks both give good results on data.</strong> Thus I decided to use them as metaclassifiers in my ensemble.</p><p>Nevertheless, KNN usually gives predictions that are very different from decision trees or neural networks, so I include them on the first level of ensemble as metafeatures. Random forest and xgboost also happened to be useful as metafeatures.</p><h2>What was your most important insight into the data?</h2><p>Probably the main insight was that <strong>KNN is capable of making very good metafeatures</strong>. Never underestimate nearest neighbours algorithm.</p><p>Very important were to combine NN and XGB predictions on the second level. While my final second- level NN and XGB separately scored around .391 on private LB, the combination of them achieved .386, which is very significant improvement. Bagging on the second level helped a lot too.</p><p>Beside this, TSNE in 2 dimensions looks very interesting. We can see on the plot that we have some examples which most likely will be misclassified by our algorithm. It does mean that it won\u2019t be easy to find a way to post-process our predictions to improve logloss.</p><p>Also, it seemed interesting that some classes were related closer than others, for example class 1 and class 2. It\u2019s worth trying to distinguish these classes specially.</p><h2>Were you surprised by any of your findings?</h2><p>Unfortunately, it appears that you won\u2019t necessarily improve your model if you will make your metafeatures better. And when it comes to ensembling, all that you can count on is your understanding of algorithms<strong> (basically, the more diverse metafeatures you have, the better)</strong> and effort to try as many metafeatures as possible.</p><h2>Which tools did you use?</h2><p>I only used sklearn, xgboost, lasagne. These are perfect machine learning libraries and I would recommend them to anyone who is starting to compete on Kaggle. Relying on my past experience they are sufficient to try different methods and achieve great results in most Kaggle competitions.</p><h2>Words of Wisdom</h2><h2>Do you have any advice for those just getting started in data science?</h2><p>I think that the most useful advice here is try not to stuck trying to fine-tune parameters or stuck using the same approaches every competition. Read through forums, understand winning solutions of past competitions and all of this will give you significant boost whatever your level is. <strong>In another words, my point is that reading past solutions is as important as solving competitions.</strong></p><p>Also, when you first starting to work on machine learning problems you could make some nasty mistakes which will cost you a lot of time and efforts. Thus it is great if you can work in a team with someone and ask him to check you code or try the same methods on his own. Besides always compare your performance with people on forums. <strong>When you see that you algorithm performs much worse than people report on forum, go and check benchmarks for this and other recent competitions and try to figure out the mistake.</strong></p><h2>Bio</h2><p><strong>Alexander Guschin</strong></p><figure></figure><p>is 4th year student in <a href=\"https://web.archive.org/web/20180114032307/http://mipt.ru/en/\">Moscow Institute of Physics and Technology</a>. Currently, Alexander is finishing his bachelor diploma work about ensembling methods.</p></div></div>",
      "url": "https://medium.com/kaggle-blog/otto-product-classification-winners-interview-2nd-place-alexander-guschin-%E3%83%84-e9248c318f30"
    }
  ]
}