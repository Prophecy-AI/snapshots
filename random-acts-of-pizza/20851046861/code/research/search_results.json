{
  "query": "How do Kaggle winners handle moderate class imbalance in text classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on insights from Kaggle winners and the practical techniques shared in competition write\u2011ups, moderate class imbalance in text\u2011classification contests is usually tackled with a short, repeatable workflow\u202f([Neptune blog](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions);\u202f[AIcrowd notebook](https://www.aicrowd.com/showcase/dealing-with-class-imbalance)):\n\n1. **Quantify the imbalance** \u2013 compute the label frequencies on the training set and on the validation split; if any class represents\u202f<\u202f20\u202f% of the data, treat it as \u201cmoderately imbalanced.\u201d\u202f([AIcrowd](https://www.aicrowd.com/showcase/dealing-with-class-imbalance))\n\n2. **Apply class\u2011aware sample weighting** \u2013 pass a weight vector to the loss function (e.g., `torch.nn.BCEWithLogitsLoss(pos_weight=\u2026)` or `sklearn`\u2019s `class_weight='balanced'`). This forces the optimizer to penalize mistakes on minority classes more heavily.\u202f([AIcrowd](https://www.aicrowd.com/showcase/dealing-with-class-imbalance))\n\n3. **Balance the training data** \u2013 optionally oversample minority examples or use SMOTE\u2011style synthetic generation before feeding data to the model. In practice, many top Kaggle solutions combine a modest oversampling with the weighted loss rather than fully re\u2011balancing the dataset.\u202f([AIcrowd](https://www.aicrowd.com/showcase/dealing-with-class-imbalance))\n\n4. **Use stratified validation / cross\u2011validation** \u2013 split the data so each fold preserves the original class ratios. This gives a reliable estimate of how the weighting/oversampling impacts the minority classes and prevents leakage of imbalance information.\u202f([Neptune blog](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions))\n\n5. **Fine\u2011tune decision thresholds** \u2013 after training, adjust the probability cut\u2011off for each class (or use a calibrated classifier) to maximize the competition metric (often AUC or log\u2011loss) on a validation set that reflects the true class distribution.\u202f([Neptune blog](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions))\n\n6. **Blend weighted\u2011loss models with un\u2011weighted baselines** \u2013 top Kaggle ensembles typically include both a model trained with class weights/oversampling and a standard model; blending them captures complementary strengths and further lifts the minority\u2011class performance.\u202f([Neptune blog](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions))\n\n7. **Validate on the public leaderboard and iterate** \u2013 monitor the metric on the public split; if the score drops after weighting, reduce the weight magnitude or adjust oversampling ratios until the improvement stabilizes.\u202f([Neptune blog](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions))\n\nFollowing these steps\u2014quantify imbalance, add class\u2011aware loss weighting, optionally oversample/SMOTE, keep splits stratified, calibrate thresholds, and blend weighted/un\u2011weighted models\u2014mirrors the proven strategies employed by Kaggle competition winners when dealing with moderate class imbalance in text classification tasks.",
      "url": ""
    },
    {
      "title": "AIcrowd | Dealing with Class Imbalance | Posts",
      "text": "Loading\n\n#### [ADDI Alzheimers Detection Challenge](https://www.aicrowd.com/challenges/addi-alzheimers-detection-challenge)\n\n# Dealing with Class Imbalance\n\nLooking at different ways to address class imbalance in this dataset\n\nJohnowhitaker7 May 2021\n\n[20](https://www.aicrowd.com/participants/sign_in) [Open in Colab](https://colab.research.google.com/gist/aicrowd-bot/d9ed51a8b9be2026f0ff96c2b5cad9c2)\n\nIn this notebook I take a close look at some different ways we can address the difference in class balance between train and validation (and presumably test)\n\nWe look at changing sample weights, over\u00a0and under-sampling, SMOTE and some other tips and tricks.\n\nI hope you find it helpful :)\n\n# Introduction [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Introduction)\n\nSo you've made your first model for this challenge and it's getting a log loss of ~0.9 - a ways behind the leaders on 0.6X. You're starting to think about feature engineering, adding more models to your ensemble, maybe trying one of those tabular deep learning models the cool kids are talking about. **STOP!** Before any of that, there is one BIG issue we need to deal with: class (im)balance.\n\nThe validation set (and presumably the test set) has a different class distribution to the training data. In this notebook we will look at many different ways we can correct for this class imbalance - picking one of these will boost your score tremendously (we're taking ~0.66 with a single simple random forest model). So, let's dive in.\n\n# Setup [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Setup)\n\nImporting the libraries we'll be using, loading the data and getting ready to run our experiments.\n\nIn\u00a0\\[1\\]:\n\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import f1_score, log_loss\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n```\n\nIn\u00a0\\[2\\]:\n\n```\n#The training data\ndf = pd.read_csv('ds_shared_drive/train.csv')\nprint(df.shape)\ndf.head(2)\n```\n\n```\n(32777, 122)\n\n```\n\nOut\\[2\\]:\n\n| row\\_id | number\\_of\\_digits | missing\\_digit\\_1 | missing\\_digit\\_2 | missing\\_digit\\_3 | missing\\_digit\\_4 | missing\\_digit\\_5 | missing\\_digit\\_6 | missing\\_digit\\_7 | missing\\_digit\\_8 | ... | bottom\\_area\\_perc | left\\_area\\_perc | right\\_area\\_perc | hor\\_count | vert\\_count | eleven\\_ten\\_error | other\\_error | time\\_diff | centre\\_dot\\_detect | diagnosis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | S0CIXBKIUEOUBNURP | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.52617 | 0.524975 | 0.474667 | 0 | 0 | 0 | 1 | -105.0 | 0.0 | normal |\n| 1 | IW1Z4Z3H720OPW8LL | 12.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.00081 | 0.516212 | 0.483330 | 0 | 1 | 0 | 1 | NaN | NaN | normal |\n\n2 rows \u00d7 122 columns\n\nIn\u00a0\\[3\\]:\n\n```\n# The validation data (we merge in the labels for convenience)\nval = pd.read_csv('ds_shared_drive/validation.csv')\nval = pd.merge(val, pd.read_csv('ds_shared_drive/validation_ground_truth.csv'),\n               how='left', on='row_id')\nprint(val.shape)\nval.head()\n```\n\n```\n(362, 122)\n\n```\n\nOut\\[3\\]:\n\n| row\\_id | number\\_of\\_digits | missing\\_digit\\_1 | missing\\_digit\\_2 | missing\\_digit\\_3 | missing\\_digit\\_4 | missing\\_digit\\_5 | missing\\_digit\\_6 | missing\\_digit\\_7 | missing\\_digit\\_8 | ... | bottom\\_area\\_perc | left\\_area\\_perc | right\\_area\\_perc | hor\\_count | vert\\_count | eleven\\_ten\\_error | other\\_error | time\\_diff | centre\\_dot\\_detect | diagnosis |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | LA9JQ1JZMJ9D2MBZV | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.499368 | 0.553194 | 0.446447 | 0 | 0 | 0 | 1 | NaN | NaN | post\\_alzheimer |\n| 1 | PSSRCWAPTAG72A1NT | 6.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.427196 | 0.496352 | 0.503273 | 0 | 1 | 0 | 1 | NaN | NaN | normal |\n| 2 | GCTODIZJB42VCBZRZ | 11.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.505583 | 0.503047 | 0.496615 | 1 | 0 | 0 | 0 | 0.0 | 0.0 | normal |\n| 3 | 7YMVQGV1CDB1WZFNE | 3.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | ... | 0.444633 | 0.580023 | 0.419575 | 0 | 1 | 0 | 1 | NaN | NaN | post\\_alzheimer |\n| 4 | PHEQC6DV3LTFJYIJU | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | ... | 0.395976 | 0.494990 | 0.504604 | 0 | 0 | 0 | 1 | 150.0 | 0.0 | normal |\n\n5 rows \u00d7 122 columns\n\nIn\u00a0\\[4\\]:\n\n```\n# We'll keep track of how different approaches perform\nresults = []\n```\n\n# Baseline \\#1 - Training on all data [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Baseline-%231---Training-on-all-data)\n\nThis is a case where we don't do any correction for the class imbalance. Some models will do better than others - tree-based models like CatBoost will be less sensitive than some other model types, but they will still over-estimate the probability that a given sample will fall into the majority class when making predicitons on the validation set (since the 'normal' class is so much more common in the training data).\n\nIn\u00a0\\[5\\]:\n\n```\n# Prep the data\nX = df.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny = df['diagnosis']\nX_val = val.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny_val = val['diagnosis']\n\n# Train the model\nmodel = CatBoostClassifier(verbose=False, cat_features=['intersection_pos_rel_centre'])\n\n# Evaluate on val set\nmodel.fit(X, y, eval_set = (X_val, y_val), early_stopping_rounds = 30)\n\n# Store results\nr = {'Approach':'No modifications',\n     'Log Loss':log_loss(y_val, model.predict_proba(X_val)),\n     'F1':f1_score(y_val, model.predict(X_val), average='macro')\n    }\nresults.append(r)\n\nprint(r) # Show results\n```\n\n```\n{'Approach': 'No modifications', 'Log Loss': 0.6745549053231596, 'F1': 0.2848101265822785}\n\n```\n\nA log loss of 0.67 on the validation set isn't terrible. We are using the validation set for early stopping - without that in place we get a log loss of 0.78 on our validation set and 0.8X on the leaderboard. So in a way, by using the validation set for early stopping we are already starting to combat our class balance problem... but we can do much better!\n\n# Adjusting Sample Weights [\u00b6](https://www.aicrowd.com/www.aicrowd.com\\#Adjusting-Sample-Weights)\n\nModels like CatBoost allow us to assign more weight to specific samples. In this case, we use this to place less weight on samples in the over-represented classes, combating the bias introduced by the imbalance:\n\nIn\u00a0\\[6\\]:\n\n```\n# Prep the data\nX = df.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny = df['diagnosis']\nX_val = val.drop(['row_id', 'diagnosis'], axis=1).fillna(0)\ny_val = val['diagnosis']\n\n#Our class weights\nweights = {\n    'normal':9/74, # Chosen based on some quick mental maths comparing the distribution of train vs val\n    'post_alzheimer':0.85,\n    'pre_alzheimer':1\n}\n\n# Applying these weights as sample weights by using Pool to wrap our training data\ntrain_data = Pool(\n    data = X,\n    label = y,\n    weight = y.map(weights), # << The important bit\n    cat_features = [list(X.columns).index('intersection_pos_rel_centre')]\n)\n\neval_data = Pool(\n    data = X_val,\n    label = y_val,\n    weight = y_val.map(lambda x: 1.0), # all validation samples get a weight of 1\n    cat_features = [list(X.columns).index('intersection_pos_rel_centre')]\n)\n\n# Train the model\nmodel = CatBoostClassifier(verbose=False)\n\n# Evaluate on val set\nmodel.fit(train_data, eval_set = eval_data, early_stopping_rounds = 30)\n\n# Store results\nr = {'Approach':'Modifying Sample Weights',\n     'Log Loss':log_loss(y_val, model.predict_proba(X_val)),\n     'F1':f1_score(y_val, model.predict(X_val), average='macro')\n    }\nresults.append(r)\n\nprint(r) # Show results\n```\n\n```\n{'Approach': 'Modifying Sample Weights', 'Log Loss': 0.5593949556085255, 'F1': 0.4727603953181...",
      "url": "https://www.aicrowd.com/showcase/dealing-with-class-imbalance"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "How I handled imbalanced text data - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fba9b757ab1d8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-handled-imbalanced-text-data-ba9b757ab1d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-handled-imbalanced-text-data-ba9b757ab1d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-handled-imbalanced-text-data-ba9b757ab1d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-handled-imbalanced-text-data-ba9b757ab1d8&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# **How I handled imbalanced text data**\n\n## Blueprint to tackle one of the most common problems in AI\n\n[![Yogesh Kothiya](https://miro.medium.com/v2/resize:fill:88:88/1*zNkUFlexn9RWHc--UuuQ5w.jpeg)](https://medium.com/@kothiya.yogesh?source=post_page-----ba9b757ab1d8--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----ba9b757ab1d8--------------------------------)\n\n[Yogesh Kothiya](https://medium.com/@kothiya.yogesh?source=post_page-----ba9b757ab1d8--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd310856a9437&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-handled-imbalanced-text-data-ba9b757ab1d8&user=Yogesh+Kothiya&userId=d310856a9437&source=post_page-d310856a9437----ba9b757ab1d8---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ba9b757ab1d8--------------------------------)\n\n\u00b7\n\n5 min read\n\n\u00b7\n\nMay 15, 2019\n\n--\n\n2\n\nListen\n\nShare\n\nPhoto by [Form](https://unsplash.com/@theformfitness?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nThe problem of imbalanced class distribution is prevalent in the field of data science and ML engineers come across it frequently. I am a chatbot developer at [IMImoble Pvt Ltd](https://imimobile.com/) and faced this scenario recently while training intent classification module. Any live business chatbot accessible to real-world users is bound to attract a significant number of out-of-scope queries along with messages pertaining to the task it is designed to perform. Even among the relevant task-oriented messages, imbalances are to be expected as all topics covered by the bot can\u2019t be equally popular. For example, in a banking use case, balance inquiries will outnumber home loan applications.\n\nBot building is not similar to traditional application development. While the latter is relatively stable and is updated less often, the former needs frequent updates to improve user experience and intelligence of the bot. The imbalanced dataset is the problem where data belonging to one class is significantly higher or lower than that belonging to other classes. Most ML/DL classification algorithms aren\u2019t equipped to handle imbalanced classes and tend to get biased towards majority classes.\n\n# **Why accuracy is a sham in the case of an imbalanced dataset**\n\nAiming only for high accuracy for the imbalanced dataset can be counter-productive because standard classifier algorithms like Decision Trees and Logistic Regression do not have the ability to handle imbalanced classes incorporated into them. This leads to a heavy bias towards larger classes and classes with fewer data points are treated as noise and are often ignored. The result is a higher misclassification rate for minority classes compared to the majority classes. Therefore, the accuracy metric is not as relevant when evaluating the performance of a model trained on imbalanced data.\n\nConsider the following case: you have two classes \u2014 A and B. Class A is 95% of your dataset and class B is the other 5%. You can reach an accuracy of 95% by simply predicting class A every time, but this provides a useless classifier for your intended use case. Instead, a properly calibrated method may achieve a lower accuracy but would have a substantially higher true positive rate (or recall), which is really the metric you should have been optimizing for.\n\n[This](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/) article explains several methods to handle imbalanced dataset but most of them don\u2019t work well for text data. In this article, I am sharing all the tricks and techniques I have used to balance my dataset along with the code which boosted f1-score by 30%.\n\n# Strategies for handling Imbalanced Datasets:\n\n**Can you gather more data?**\n\nYou might think that this is not the solution you\u2019re looking for but gathering more meaningful and diverse data is always better than sampling original data or generating artificial data from existing data points.\n\n**Removing data redundancy:**\n\n1. Removing duplicate data \u2014 The dataset I was dealing with contained a lot of similar and even duplicate data points. \u201cWhere is my order\u201d and \u201cWhere is the order\u201d has the same semantic meaning. Removing such duplicate message will help you reduce the size of your majority class.\n2. There were many messages which had the same semantic meaning, for example, consider the following messages, which convey the same meaning. Keeping one or two such utterances and removing others also helps in balancing classes. Well, you can use those messages in the validation set. There are many ways to find text similarity but I have used Jaccard Similarity because it is very easy to implement and it considers only unique sets of words while calculating similarity. You can look at other techniques in [this](https://medium.com/@adriensieg/text-similarities-da019229c894) article.\n\n_Can I change the delivery time for my delivery?_\n\n_Can I change time of my delivery?_\n\n_Can I change delivery time?_\n\n3\\. Merge minority classes \u2014 Sometimes multiple classes have overlapping features. It\u2019s better to merge those multiple minority classes. This trick helped me improve f1-score by more than 10%.\n\n**Resample training dataset:**\n\nThe simplest way to fix imbalanced dataset is simply balancing them by oversampling instances of the minority class or undersampling instances of the majority class. Using advanced techniques like SMOTE(Synthetic Minority Over-sampling Technique) will help you create new synthetic instances from minority class.\n\n1. Undersampling \u2014 An effort to eliminate data point from the majority class randomly until the classes are balanced. There is a likelihood of information loss which might lead to poor model training.\n2. Oversampling \u2014 This is the process to replicate minority class instances randomly. This approach can overfit and lead to inaccurate predictions on test data.\n3. SMOTE \u2014 SMOTE generates synthetic samples by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors as shown in below GIF. More importantly, this approach effectively forces the decision region of the minority class to beco...",
      "url": "https://towardsdatascience.com/how-i-handled-imbalanced-text-data-ba9b757ab1d8?gi=f37886eebb72"
    },
    {
      "title": "Toxic Comment Classification - Natural Language Processing",
      "text": "Toxic Comment Classification - Natural Language Processing - Jay Speidell\n![Jay Speidell](https://jayspeidell.github.io/images/profile.png)\n### Jay Speidell\nFollow\n* **Seattle, WA\n* [**Email](mailto:jayspeidell@gmail.com)\n* [**LinkedIn](https://www.linkedin.com/in/jayspeidell)\n* [**Github](https://github.com/jayspeidell)\n* [**Stackoverflow](https://www.stackoverflow.com/users/https://stackoverflow.com/users/7697801/jay-speidell)\n* [**Thingiverse](https://www.thingiverse.com/jayspeidell/designs)\n# Toxic Comment Classification - Natural Language Processing\n[The Jypyter notebooks and a report in PDF format can be found on my GitHub page here.](https://github.com/jayspeidell/ToxicCommentClassification-)\n![](https://jayspeidell.github.io/images/toxic/wordcloud.png)\n# I. Definition\n## Project Overview\nPlatforms that aggregate user content are the foundation of knowledge sharing on the Internet. Blogs, forums, discussion boards, and, of course, Wikipedia. But the catch is that not all people on the Internet are interested in participating nicely, and some see it as an avenue to vent their rage, insecurity, and prejudices.\nWikipedia runs on user generated content, and is dependent on user discussion to curate and approve content. The problem with this is that people will frequently write things they shouldn\u2019t, and to maintain a positive community this toxic content and the users posting it need to be removed quickly. But they don\u2019t have the resources to hire full-time moderators to review every comment.\nThis problem led the Conversation AI team[1](#resources), owned by Alphabet, to develop a large open dataset of labeled Wikipedia Talk Page comments, which will be the dataset used for the project. The dataset is available through Kaggle[2](#resources).\nThe dataset has six labels that represent subcategories of toxicity, but the project is going to focus on a seventh label that represents the general toxicity of the comments.\nThe project will be done with Python and Jupyter notebooks, which will be attached.\nDisclaimer: The dataset has extremely offensive language that will show up during exploratory data analysis.\n## Problem Statement\nThe goal is to create a classifier model that can predict if input text is inappropriate (toxic). 1. Explore the dataset to get a better picture of how the labels are distributed, how they correlate with each other, and what defines toxic or clean comments. 2. Create a baseline score with a simple logistic regression classifier. 3. Explore the effectiveness of multiple machine learning approaches and select the best for this problem. 4. Select the best model and tune the parameters to maximize performance. 5. Build a the final model with the best performing algorithm and parameters and test it on a holdout subset of the data.\n## Metrics\nUnfortunately for the problem, but fortunately for the Wikipedia community, toxic comments are rare. Just over 10% of this dataset is labeled as toxic, but some of the subcategories are extremely rare making up less than 1% of the data.\nBecause of this imbalance, accuracy is a practically useless metric for evaluating classifiers for this problem.\nThe Kaggle challenge based on this dataset uses ROC/AUC, or the area under a receiver operating characteristic curve, to evaluate submissions. This is a very generous metric for the challenge, as axes for the curve represent recall (a.k.a. sensitivity), the ratio of positive predictions to all samples with that label, and specificity, the ratio of negative predictions to all negative samples. This metric would work well if the positive and negative labels were relatively even, but in our case, where one label represents less than a third of a percent of the data, it\u2019s too easy to get a high score even with hardly any true-positive predictions.\nInstead, I propose using an F1 Score, which severely penalizes models that just predict everything as either positive or negative with an imbalanced dataset.\nRecall, as mentioned earlier, is the ratio of true positive predictions to positive samples. Precision, on the other hand, is the ratio of true positive predictions to the sum of all positive predictions, true and false.\nEach gives valuable insight into a model\u2019s performance, but they fail to show the whole picture and have weaknesses where bad models get high scores. Predicting all positive values will bring recall up to 100%, while missing true positives will be penalized. Precision will harshly penalize false positives, but a model that predicts mostly negative can achieve a high precision score whether or not the predictions are accurate.\nThe F1 score is a harmonic average between precision and recall. This combines the strengths of precision and recall while balancing out their weaknesses, creating a score that can fairly evaluate models regardless of dataset imbalance.\nMy justification for focusing on any\\_label as the target is that distinctions between the specific labels are relatively ambiguous, and that there is greater value focusing on general toxicity of a comment to more reliably flag it for review. This will reduce the workload of moderators who will ultimately be making the final call, and the specific category more relates to the consequences for the commenter rather than whether or not the comment should be deleted.\n# II. Analysis\nData Exploration This dataset contains 159,571 comments from Wikipedia. The data consists of one input feature, the string data for the comments, and six labels for different categories of toxic comments: toxic, severe\\_toxic, obscene, threat, insult, and identity\\_hate. The figure on the following page contains a breakdown of how the labels are distributed throughout the dataset, including overlapping data. As you can see in the breakdown, while most comments with other labels are also toxic, not all of them are. Only \u201csevere\\_toxic\u201d is clearly a subcategory of \u201ctoxic.\u201d And it\u2019s not close enough to be a labeling error. This suggests that \u201ctoxic\u201d is not a catch-all label, but rather a subcategory in itself with a large amount of overlap. Because of this, I\u2019m going to create a seventh label called \u201cany\\_label\u201d to represent overall toxicity of a comment. From here on in, I\u2019m going to refer to any labeled comments as toxic, and the specific \u201ctoxic\u201d label (along with other labels) in quotation marks.\n![](https://jayspeidell.github.io/images/toxic/label%20counts.png)\n*Fig 1: Label Counts*\nOnly 39% of the toxic comments have only one label, and the majority have some sort of overlap. I believe that because of this, it will be much more difficult to train a classifier on specific labels than whether or not they are toxic. This ambiguity and the lack of explanation around it is what led me to select an aggregate label of general toxicity, what I\u2019ve called \u201cany\\_label,\u201d as the target. 16225 out of 159571 comments, or 10.17%, are classified as some category of toxic.\n1595 severe\\_toxic comments. (1.00% of all data.)\n* 1595 or 100.00% were also toxic.\n* 1517 or 95.11% were also obscene.\n* 112 or 7.02% were also threat.\n* 1371 or 85.96% were also insult.\n* 313 or 19.62% were also identity\\_hate.\n1405 identity\\_hate comments. (0.88% of all data.)\n* 1302 or 92.67% were also toxic.\n* 313 or 22.28% were also severe\\_toxic.\n* 1032 or 73.45% were also obscene.\n* 98 or 6.98% were also threat.\n* 1160 or 82.56% were also insult.\n15294 toxic comments. (9.58% of all data.)\n* 1595 or 10.43% were also severe\\_toxic.\n* 7926 or 51.82% were also obscene.\n* 449 or 2.94% were also threat.\n* 7344 or 48.02% were also insult.\n* 1302 or 8.51% were also identity\\_hate.\n7877 insult comments. (4.94% of all data.)\n* 7344 or 93.23% were also toxic.\n* 1371 or 17.41% were also severe\\_toxic.\n* 6155 or 78.14% were also obscene.\n* 307 or 3.90% were also threat.\n* 1160 or 14.73% were also identity\\_hate.\n478 threat comments. (0.30% of all data.)\n* 449 or 93.93% were also toxic.\n* 112 or 23.43% were also severe\\_toxic.\n* 301 or 62.97% were also obscene.\n*...",
      "url": "https://jayspeidell.github.io/portfolio/project05-toxic-comments"
    },
    {
      "title": "",
      "text": "<div><section>\n <figure><figcaption>\n <p><a href=\"https://pixabay.com/en/pollution-toxic-products-environment-3075857/\">Photo Credit</a></p>\n </figcaption>\n</figure>\n<h2>Introduction</h2>\n<p><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\">Jigsaw toxic comment classification challenge</a> features a multi-label text classification problem with a highly imbalanced dataset. The test set used originally was revealed to be already public on the Internet, so <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/47835\">a new dataset was released</a> mid-competition, and the evaluation metric was <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48639\">changed from Log Loss to AUC</a>.</p>\n<p>I tried a few ideas after building up my PyTorch pipeline but did not find any innovative approach that looks promising. Text normalization is the only strategy I had found to give solid improvements, but it is very time consuming. The final result (105th place/about top 3%) was quite fitting IMO given the time I spent on this competition(not a lot).</p>\n<p>(There were some <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52574\">heated discussion</a> around the topic of some top-ranking teams with Kaggle grand masters getting disqualified after the competition ended.)</p>\n<p>Public kernel blends performed well in this competition (i.e. did not over-fit the public leaderboard too much). I expected it to overfit, but still selected one final submission that used the best public blend of blends to play it safe. Fortunately it paid off and gave me a 0.0001 boost in AUC on private leaderboard:</p>\n<figure><figcaption>\n <p><a href=\"https://docs.google.com/spreadsheets/d/1R3u6tXDa9CVByaRW9FY8FpxRTqwZsBWfLYuqC1gu8A8/edit?usp=sharing\">Table 1: Selected submissions in descending order of private scores</a></p>\n </figcaption>\n</figure>\n<p>In this post, I\u2019ll review some of the techniques used and shared by top competitors. I do not have enough time to test every one of them myself. Part 2 of this series will be implementing a top 5 solution by my own, if I ever find time to do it.</p>\n<h2>List of Techniques</h2>\n<p>I tired to attribute techniques to all appropriate sources, but I\u2019m sure I\u2019ve missed some sources here and there. <strong><em>Not all techniques are covered because of the vast amount of contents shared by generous Kagglers</em></strong>. I might come back and edit this list in the near future.</p>\n<ul>\n<li>Translation as Augmentation (test time and train time) [1][2][13]</li>\n<li>Pseudo-labelling [1]</li>\n<li>Text normalization [3][4]</li>\n<li>Multi-level stacking \u2014 4 levels [3]; 2 levels [11]; 1level stacking + 1 level weighted averaging [12]</li>\n<li>Non-English embeddings [2]</li>\n<li>Diverse pre-trained embeddings \u2014 Train multiple model separately on different embeddings</li>\n<li>Combine several pre-trained embeddings \u2014 concatenate after a RNN layer [3]; Concatenate at embedding level [5]; Variant of <a href=\"https://arxiv.org/abs/1705.02798\">Reinforced Mnemonic Reader</a> [7]</li>\n<li>When truncating texts, retain both head and tail of the texts [1]</li>\n<li>K-max pooling[6]</li>\n<li>Multiple-level of stacking [3]</li>\n<li>Deepmoji-style attention model [3]</li>\n<li>(NN) Additional row-level features:<em>\u201cUnique words rate\u201d and \u201cRate of all-caps words\u201d</em>[4]</li>\n<li>(NN) Additional word-level feature(s): If a word contains only capital letters [4]</li>\n<li>(GBM) Engineered features other than tf-idf [12]</li>\n<li>BytePair Encoding [5][8]</li>\n<li><a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">R-net</a> [7]</li>\n<li>CapsuleNet [11]</li>\n<li>Label aware attention layer [7]</li>\n<li>(20180413 Update) leaky information \u2014 small overlaps of IPs between training and testing dataset. One overlapping username. [14]</li>\n</ul>\n<h2>Pseudo-labelling and Head-Tail Truncating</h2>\n<p>I\u2019ve already tried these two techniques and trained a couple of models for each.</p>\n<p>Head-tail truncating (keeping 250 tokens at head, 50 tokens at tail) helped only a bit for bi-GRU, but not for QRNN. It basically had no effect on my final ensemble.</p>\n<p>For pseudo-labelling(PL), I used the test-set predictions from my best ensemble as suggested in [1], and they improved the final ensemble a little (see table 1). I\u2019d assume that adding more model trained with PL will further boost the final AUC. However, the problem of this approach is the leakage it produces. The ensemble model had seen the the all the validation data, and that information leaked into its test set predictions. So the local CV will be distorted and not comparable to those trained without PL. Nonetheless, this technique does create the best single model, so it\u2019ll be quite useful for production deployment.</p>\n<p>I think the more conservative way of doing PL is to repeat the train-predict-train(with PL) process, so the model is trained twice for every fold. But that\u2019ll definitely takes more time.</p>\n<h2>References:</h2>\n<ol>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52557\">The 1st place solution overview</a> by <a href=\"https://www.kaggle.com/leecming\">Chun Ming Lee</a> from team <em>Toxic Crusaders</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52612\">The 2nd place solution overview</a> by <a href=\"https://www.kaggle.com/neongen\">neongen</a> from team <em>neongen &amp; Computer says no</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52762\">The 3rd place solution overview</a> by <a href=\"https://www.kaggle.com/tunguz\">Bojan Tunguz</a> from team <em>Adversarial Autoencoder</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644\">About my 0.9872 single model</a> by <a href=\"https://www.kaggle.com/mrboor\">Alexander Burmistrov</a> from team <em>Adversarial Autoencoder</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52630\">The 5th place brief solution</a> by <a href=\"https://www.kaggle.com/kazanova\">\u039c\u03b1\u03c1\u03b9\u03bf\u03c2 \u039c\u03b9\u03c7\u03b1\u03b7\u03bb\u03b9\u03b4\u03b7\u03c2 KazAnova</a> from team <em>TPMPM</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52526\">Congrats (from ghost 11th team)</a> by <a href=\"https://www.kaggle.com/cpmpml\">CPMP</a></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52702\">The 12th place single model solution share</a> by <a href=\"https://www.kaggle.com/goldenlock\">BackFactoryVillage</a> from team <em>Knights of the Round Table</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52563\">The 15th solution summary: Byte Pair Encoding</a> by <a href=\"https://www.kaggle.com/tezdhar\">Mohsin hasan</a> from team <em>Zehar 2.0</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52647\">The 25th Place Notes (0.9872 public; 0.9873 private)</a> by <a href=\"https://www.kaggle.com/jtrotman\">James Trotman</a> (solo)</li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52719\">The 27th place solution overview</a> by <a href=\"https://www.kaggle.com/zake7749\">Justin Yang</a> from team <em>Sotoxic</em></li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52666\">The 33rd Place Solution Using Embedding Imputation (0.9872 private, 0.9876 public)</a> by <a href=\"https://www.kaggle.com/mmotoki\">Matt Motoki</a> (solo)</li>\n<li><a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52645\">The 34th, Lots of FE and Poor Understanding of NNs [CODE INCLUDED]</a> by <a href=\"https://www...",
      "url": "https://blog.ceshine.net/post/kaggle-toxic-comment-classification-challenge"
    },
    {
      "title": "\ud83d\udc51 Imbalance ratio: How to identify an imbalanced dataset? | Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/questions-and-answers/372288#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/discussions/questions-and-answers/372288)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/questions-and-answers/372288#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fdiscussions%2Fquestions-and-answers%2F372288)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fdiscussions%2Fquestions-and-answers%2F372288)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n[Eli\u0161ka Bl\u00e1hov\u00e1](https://www.kaggle.com/elisthefox) \u00b7 Posted 2 years ago in [Questions & Answers](https://www.kaggle.com/discussions/questions-and-answers)\n\narrow\\_drop\\_up3\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)more\\_vert\n\n# \ud83d\udc51 Imbalance ratio: How to identify an imbalanced dataset?\n\nDear community,\n\nI have a question for you to which I can't find a common answer. I wonder how to be able to classify a dataset as imbalanced? Is there some rule of thumb I can use? I have found this article, where it says that highly imbalanced dataset has ratio of 10-1 [https://www.sciencedirect.com/topics/computer-science/imbalance-ratio](https://www.sciencedirect.com/topics/computer-science/imbalance-ratio).\n\nI would love to hear your opinion.\n\nThanks a lot!\n\n[Exploratory Data Analysis](https://www.kaggle.com/discussions?tags=13201-Exploratory+Data+Analysis) [Data Analytics](https://www.kaggle.com/discussions?tags=13215-Data+Analytics) [Data Cleaning](https://www.kaggle.com/discussions?tags=13202-Data+Cleaning)\n\nPlease [sign in](https://www.kaggle.com/account/login) to reply to this topic.\n\ncomment\n\n## 8 Comments\n\nHotness\n\n[**Senapati Rajesh**](https://www.kaggle.com/senapatirajesh)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up1\n\nmore\\_vert\n\nThank you very much all for the comments and here is my version of answer\n\nImbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n\nImbalance means that the number of data points available for different the classes is different:\n\nIf there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n\nA typical example of imbalanced data is encountered in e-mail classification problem where emails are classified into ham or spam. The number of spam emails is usually lower than the number of relevant (ham) emails. So, using the original distribution of two classes leads to imbalanced dataset.\n\nUsing accuracy as a performace measure for highly imbalanced datasets is not a good idea. For example, if 90% points belong to the true class in a binary classification problem, a default prediction of true for all data points leads to a classifier which is 90% accurate, even though the classifier has not learnt anything about the classification problem at hand!\n\nImbalanced data can be handled by using below techniques:\n\n1.Resample the training set\n\n(Under sampling -This method is used when quantity of data is sufficient or\n\nover sampling-It tries to balance dataset by increasing the size of rare samples.)\n\n2.Use K-fold Cross-Validation in the Right Way\n\n3.Ensemble Different Resampled Datasets(This approach is simple and perfectly horizontally scalable if you have a lot of data)\n\n4.Resample with Different Ratios( Instead of training all models with the same ratio in the ensemble, it is worth trying to ensemble different ratios)\n\n5.Most importantly Choose Proper Evaluation Metric(\n\nex:The accuracy of a classifier is the total number of correct predictions by the classifier divided by the total number of predictions.For an imbalanced class dataset F1 score is a more appropriate metric. It is the harmonic mean of precision and recall)\n\n6.SMOTE is another technique to oversample the minority class.SMOTE looks into minority class instances and use k nearest neighbor to select a random nearest neighbor, and a synthetic instance is created randomly in feature space.\n\nThank u and Pl feel free to suggest for any modifications.\n\n[**Olqa\\_842**](https://www.kaggle.com/zvr842)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up1\n\nmore\\_vert\n\nPlease, could you share dataset too?\n\n[**Eli\u0161ka Bl\u00e1hov\u00e1**](https://www.kaggle.com/elisthefox)\n\nTopic Author\n\nPosted 2 years ago\n\narrow\\_drop\\_up0\n\nmore\\_vert\n\nOf course, currently, I am working with this dataset: [https://www.kaggle.com/datasets/shubh0799/churn-modelling](https://www.kaggle.com/datasets/shubh0799/churn-modelling). Link to my notebook: [https://www.kaggle.com/code/elisthefox/churn-analytics-should-i-stay-or-should-i-go](https://www.kaggle.com/code/elisthefox/churn-analytics-should-i-stay-or-should-i-go)\n\n[**Carl McBride Ellis**](https://www.kaggle.com/carlmcbrideellis)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up2\n\nmore\\_vert\n\nDear [@elisthefox](https://www.kaggle.com/elisthefox)\n\nYou may perhaps find this short notebook of some interest: [Classification: How imbalanced is \"imbalanced\"?\"](https://www.kaggle.com/code/carlmcbrideellis/classification-how-imbalanced-is-imbalanced).\n\nAll the best,\n\ncarl\n\n[**Eli\u0161ka Bl\u00e1hov\u00e1**](https://www.kaggle.com/elisthefox)\n\nTopic Author\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up1\n\nmore\\_vert\n\nThanks a lot for letting me know about this great notebook, If I understood correctly, the metric balanced accuracy helps to determine the ratio for imbalanced dataset, hence if its results are equal to 0.85, then dataset consisting of 10K rows with 8500 rows with target variable 1 and 1500 rows with target variable 0 is considered as a balanced target variable?\n\n[**Carl McBride Ellis**](https://www.kaggle.com/carlmcbrideellis)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up2\n\nmore\\_vert\n\nDear [@elisthefox](https://www.kaggle.com/elisthefox)\n\nStrictly speaking a dataset is balanced if there are an equal number of rows associated with each class (or label) in the training data. One can perform a quick check with something like:\n\n```\ndf['labels'].value_counts().to_frame().T\n\ncontent_copy\n```\n\nIn the notebook I suggest that most metrics work well up to an imbalance of 85:15 after which one should exercise caution. Note however as far as most estimators are concerned, they work pretty well irrespective of whether ...",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/372288"
    },
    {
      "title": "Jigsaw Unintended Bias in Toxicity Classification - Shantanu Ekhande - Medium",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7d147238cc18&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40shantanuekhande19%2Fjigsaw-unintended-bias-in-toxicity-classification-7d147238cc18&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40shantanuekhande19%2Fjigsaw-unintended-bias-in-toxicity-classification-7d147238cc18&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Jigsaw Unintended Bias in Toxicity Classification\n\n[![Shantanu Ekhande](https://miro.medium.com/v2/resize:fill:88:88/1*GB403IJYu2KuKhZ-WpM2eQ.jpeg)](https://medium.com/@shantanuekhande19?source=post_page-----7d147238cc18--------------------------------)\n\n[Shantanu Ekhande](https://medium.com/@shantanuekhande19?source=post_page-----7d147238cc18--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F70cf553596cd&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40shantanuekhande19%2Fjigsaw-unintended-bias-in-toxicity-classification-7d147238cc18&user=Shantanu+Ekhande&userId=70cf553596cd&source=post_page-70cf553596cd----7d147238cc18---------------------post_header-----------)\n\n16 min read\n\n\u00b7\n\nApr 1, 2022\n\n--\n\nListen\n\nShare\n\nThis Blog is all about my journey with toxic comment classifiers. Here I will be going to discuss the successes, failures, and challenges I have faced during building the deep learning model for toxic comments.\n\nSource: Google image\n\n## Table Contents:\n\n**_1\\. Introduction_**\n\n**_2\\. Business problem_**\n\n**_3\\. Source of data_**\n\n**_4\\. Business Constraints_**\n\n**_5\\. Performance Metrics_**\n\n**_6\\. ML Formulation:_**\n\n**_7\\. Exploratory Data Analysis and its Observation_**\n\n**_8\\. Data Preprocessing_**\n\n**_9\\. Featurization_**\n\n**_10\\. First cut solution_**\n\n**_11\\. Layers I used in DL models_**\n\n**_12\\. Model Building_**\n\n**_13\\. Error Analysis_**\n\n**_14\\. Kaggle submission Score_**\n\n**_15\\. Final pipeline of the problem_**\n\n**_16\\. Future Works and Conclusions_**\n\n**_17\\. References_**\n\n**_18\\. GitHub Repo_**\n\n## 1\\. Introduction:\n\nIn the era of social media, it\u2019s hard for developers to keep eye on every single comment, blog. Sometimes people use this platform to share knowledge and sometimes to gain knowledge But in this process, some people use Toxic comments which are offensive, _vulgar,_ and sometimes can make some people leave the discussion. Because of this many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.\n\n## 2\\. Business problem:\n\nThis is the Second Competition held by Jigsaw (part of Google). In the first competition, you have classified whether the comment is toxic or not. the model they get was biased towards a specific identity/subgroup. For non-toxic comments, model predicts as toxic with a higher rate. Models predicted a high likelihood of toxicity for comments containing those identities (e.g., \u201cgay\u201d), even when those comments were not actually toxic (such as \u201cI am a gay woman\u201d).\n\n> **In this competition, we\u2019re challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities.**\n\nKaggle [Link](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)\n\n## 3\\. Source of data:\n\n- At the end of 2017, the [Civil Comments](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) platform shut down and chose to make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n- The data includes the following:\n\n1\\. train.csv: the training set, which includes comments, toxicity labels, and subgroups.\n\n2\\. test.csv: the test set, which contains comment texts but toxicity labels or subgroups.\n\n3\\. sample\\_submission.csv: a sample submission file in the correct format.\n- The text of the individual comment is found in the `comment_text` column. Each comment in Train has a toxicity label ( `target`), and models should predict the `target` toxicity for the Test data. Although there are many identity columns in Train data only a few are required and these identities are: **_male, female, homosexual\\_gay\\_or\\_lesbian, Christian, Jewish, Muslim, black, white, psychiatric\\_or\\_mental\\_illness._** These identities will help us in calculating the final metric.\n\nFor more understanding Refer to this [**Link**](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data)\n\n## 4\\. Business Constraints:\n\n- No strict latency constraints.\n- In my understanding, the False Positive rate should be as small as possible.\n\n## 5\\. Performance Metrics:\n\n- At a high level, we are using AUC score. The question is why, to define AUC in one line, how my model is capable of distinguishing between two classes.\n- Here we are solving Two problems:\n\n\u2013 We are trying to find toxic comments amongst all comments. Which we can also think of as a true positive rate.\n\n\u2013 We are also trying to minimize unintended bias problem, which we can also think as False positive.\n\n\u2013 And AUC is calculated based on TPR VS FPR\n\n\u2013 The above reasons make more sense to use AUC.\n- We are also going to use a confusion matrix to understand model better.\n- The competition uses a newly developed metric that combines several sub metrics to balance overall performance with various aspects of unintended bias. Its basic performance metric is **AUC** (area under the curve) but we\u2019ll be using one more metric that combines the overall-AUC and bias-AUC. This is our primary metric and by maximizing this value we can reduce the unintended bias in our prediction. For further reading use this [link](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation)\n\n> Lets understand How to calculate Final AUC matric.\n\n- **Identity/Subgroup Group:** Setcomments which Contain identity words.\n- **Background group**: everything that doesn\u2019t belong to the identity group goes to Background group.\n\n> **To get final result we need three type of AUC score, and we have to calculate that AUC for all subgroups/identities.**\n\n- **Subgroup AUC**: group of comments which contain identity words. (Containing both the class positive as well as negative) and calculate AUC for this group.\n- **BPSN** (Background Positive Subgroup/identity negative) AUC: The AUC is calculated where dataset satisfies this condition.\n\n> BP ==>Label=1, identity=False\n\nIn words those comments where toxic label is true and bias identity/word/subgroup not present\n\n> _SN==> Label=0, identity=True_\n\nIn word those comments where Nontoxic label is true and bias identity/word/subgroup are present\n\n**3**. **BNSP** AUC: The AUC is calculated where dataset satisfies this condition\n\n> _BN ==>Label=0, identity=False_\n\nIn words those comments where Nontoxic label is True and bias identity/word/subgroup are present\n\n> _SP==> Label=1, identity=True_\n\nIn words those comments where Nontoxic label is False and bias identity/word/subgroup are present\n\n**4\\.** It makes more sense to calculate AUC...",
      "url": "https://medium.com/@shantanuekhande19/jigsaw-unintended-bias-in-toxicity-classification-7d147238cc18"
    },
    {
      "title": "GitHub - mattmotoki/toxic-comment-classification: Code and write-up for the Kaggle Toxic Comment Classification Challenge",
      "text": "[Skip to content](https://github.com/mattmotoki/toxic-comment-classification#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/mattmotoki/toxic-comment-classification) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/mattmotoki/toxic-comment-classification) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/mattmotoki/toxic-comment-classification) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[mattmotoki](https://github.com/mattmotoki)/ **[toxic-comment-classification](https://github.com/mattmotoki/toxic-comment-classification)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmattmotoki%2Ftoxic-comment-classification) You must be signed in to change notification settings\n- [Fork\\\n24](https://github.com/login?return_to=%2Fmattmotoki%2Ftoxic-comment-classification)\n- [Star\\\n74](https://github.com/login?return_to=%2Fmattmotoki%2Ftoxic-comment-classification)\n\n\nCode and write-up for the Kaggle Toxic Comment Classification Challenge\n\n[74\\\nstars](https://github.com/mattmotoki/toxic-comment-classification/stargazers) [24\\\nforks](https://github.com/mattmotoki/toxic-comment-classification/forks) [Branches](https://github.com/mattmotoki/toxic-comment-classification/branches) [Tags](https://github.com/mattmotoki/toxic-comment-classification/tags) [Activity](https://github.com/mattmotoki/toxic-comment-classification/activity)\n\n[Star](https://github.com/login?return_to=%2Fmattmotoki%2Ftoxic-comment-classification)\n\n[Notifications](https://github.com/login?return_to=%2Fmattmotoki%2Ftoxic-comment-classification) You must be signed in to change notification settings\n\n# mattmotoki/toxic-comment-classification\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/mattmotoki/toxic-comment-classification/branches) [Tags](https://github.com/mattmotoki/toxic-comment-classification/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[15 Commits](https://github.com/mattmotoki/toxic-comment-classification/commits/master/) |\n| [code](https://github.com/mattmotoki/toxic-comment-classification/tree/master/code) | [code](https://github.com/mattmotoki/toxic-comment-classification/tree/master/code) |  |  |\n| [input](https://github.com/mattmotoki/toxic-comment-classification/tree/master/input) | [input](https://github.com/mattmotoki/toxic-comment-classification/tree/master/input) |  |  |\n| [.gitignore](https://github.com/mattmotoki/toxic-comment-classification/blob/master/.gitignore) | [.gitignore](https://github.com/mattmotoki/toxic-comment-classification/blob/master/.gitignore) |  |  |\n| [README.md](https://github.com/mattmotoki/toxic-comment-classification/blob/master/README.md) | [README.md](https://github.com/mattmotoki/toxic-comment-classification/blob/master/README.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n## Kaggle - Toxic Comment Classification Challenge\n\n- 33rd Place Solution\n- Private LB: 0.9872, 33/4551\n- Public LB: 0.9876, 45/4551\n\nThis is the write-up and code for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) where I placed 33rd out of 4,551 teams. For more information about my approach see my [discussion post](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52666).\n\nWe were tasked with a multi-label classification problem; in particular, the task was to classify online comments into 6 categories: `toxic`, `severve_toxic`, `obscene`, `threat`, `insult`, `identity_hate`. The competition metric was the average of the individual AUCs of each predicted class.\n\n### Summary of approach:\n\n#### Embeddings:\n\n- fastText embeddings trained locally on the competition data\n- Pretrained embeddings (with similiarity imputation):\n  - [fastText: wiki.en.bin](https://fasttext.cc/docs/en/english-vectors.html)\n  - [GloVe: GloVe.840B.300d](https://nlp.stanford.edu/projects/glove/)\n  - [LexVec: LexVec.commoncrawl.300d.W.pos.vectors](https://github.com/alexandres/lexvec)\n\n#### Models (best private score shown):\n\n- CapsuleNet ( _0.9860 private_, 0.9859 public)\n- RNN Version 1 ( _0.9858 private_, 0.9863 public)\n- RNN Version 2 ( _0.9856 private_, 0.9861 public)\n- Two Layer CNN ( _0.9826 private_, 0.9835 public)\n- NB-SVM ( _0.9813 private_, 0.9813 public)\n\n#### Ensembling (best private score shown):\n\n- Level 1a: Average 10 out-of-fold predictions (as high as _0.9860 private_, 0.9859 public)\n- Level 1b: Average models with different embeddings (as high as _0.9866 private_, 0.9871 public)\n- Level 2a: LightGBM Stacking ( _0.9870 private_, 0.9874 public)\n- Level 2b: Average multiple seeds ( _0.9872 private_, 0.9876 public)\n\n### Embedding Imputation Details:\n\nMy main insight in this competition was how to handle out-of-vocabulary (OOV) words. Replacing missing vectors with zeros or random numbers is suboptimal. Using fastText's built-in OOV prediction instead of naive replacement increases the AUC by ~0.002. For GloVe and LexVec embeddings, I replaced the missing embeddings with similar vectors. To do this, I first trained a fastText model on the data for this competition:\n\n```\n  fasttext skipgram -input \"${INPUT_FILE}\" -output \"${OUTPUT_FILE}\" \\\n  -minCount 1 -neg 25 -thread 8 -dim 300\n\n```\n\nThe `-minCount 1` flag ensures that we get perfect recall; i.e., we get a vector for every word in our vocabulary. We can now find the most similar vector in the intersection of the local vocabulary (from this competition) with the external vocabulary (from pretrained embeddings). Here's the psuedocode to do that[1](https://github.com/mattmotoki/toxic-comment-classification#footnote1):\n\n```\nlocal = {local_words: local_vectors}\nexternal = {external_words: external_vectors}\nshared_words = intersect(local_words, external_words)\nmissing_words = setdiff(local_words, external_words)\nreference_matrix = array(local[w] for w in shared_words).T\n\nfor w in missing_words:\n     similarity = local[w] * reference_matrix\n     most_similar_word = shared_words[argmax(similarity)]\n     external[w] = external_vectors[most_similar_word]\n\nreturn {w: external[w] for w in local_words}\n\n```\n\nWith this technique, GloVe performed just as well if not better than the fastText with OOV prediction; LexVec performed slightly worse but added valuable diversity to ensembles.\n\n#### Timing:\n\nThe bulk of the calculation boils down to a vector matrix multiplication. The naive implementation takes about 20 mins. We can reduce this to about 4 mins by processing missing words in batches. Using PyTorch (and a 1080ti), we can get the timing down to about 1 min.\n\n#### Results:\n\nHere is a table of the scores for a single seed; here \"Toxic\" refers to the 300d vectors trained locally using fastText.\n\n| Model | Embeddings | Private | Public | Local |\n| --- | --- | --- | --- | --- |\n|  |  |  |  |  |\n| CapsuleNet | fastText | 0.9855 | 0.9867 | 0.9896 |\n| CapsuleNet | GloVe | 0.9860 | 0.9859 | 0.9899 |\n| CapsuleNet | LexVec | 0.9855 | 0.9858 | 0.9898 |\n| CapsuleNet | Toxic | 0.9859 | 0.9863 | 0.9901 |\n|  |  |  |  |  |\n| RNN Version 2 | fastText | 0.9856 | 0.9864 | 0.9904 |\n| RNN Version 2 | GloVe | 0.9858 | 0.9863 | 0.9902 |\n| RNN Version 2 | LexVec | 0.9857 | 0.9859 | 0.9902 |\n| RNN Version 2 | Toxic | 0.9851 | 0.9855 | 0.9906 |\n|  |  |  |  |  |\n| RNN Version 1 | fastText | 0.9853 | 0.9859 | 0.9898 |\n| RNN Version 1 | GloVe | 0.9855 | 0.9861 | 0.9901 |\n| RNN Version 1 | LexVec | 0.9854 | 0.9857 | 0.9897 |\n| RNN Version 1 | Toxic | 0.9856 | 0.9861 | 0.9903 |\n|  |  |  |  |  |\n| 2 Layer CNN | fastText | 0.9826 | 0.9835 | 0.9886 |\n| 2 Layer CNN | GloVe | 0.9827 | 0.9828 | 0.9883 |\n| 2 Layer CNN | LexVec | 0.9824 | 0.9831 | 0.9880 |\n| 2 Layer CNN | Toxic | 0.9806 | 0.9789 | ...",
      "url": "https://github.com/mattmotoki/toxic-comment-classification"
    }
  ]
}