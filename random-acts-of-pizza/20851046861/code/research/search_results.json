{
  "query": "How do Kaggle winners handle data leakage in tabular competitions when some features are measured after the target event?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle winners avoid post\u2011target leakage in tabular contests by first spotting any variables that are recorded **after** the event you\u2019re trying to predict and then making sure those variables never appear in training or validation folds. They also use validation splits that respect the temporal or grouping structure of the data so that information from the future cannot leak into the past.\u202f([7a\u202fData\u202fLeakage](https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html);\u202f[Smart\u202fSplitting](https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553);\u202f[MachineLearningMastery](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it))\n\n**Steps Kaggle winners follow**\n\n1. **Audit the feature list for post\u2011target information**  \n   - Scan column names and data dictionaries for timestamps, flags, or derived metrics that could only be known after the target occurs (e.g., \u201cpost\u2011event lab result\u201d, \u201ctreatment given\u201d, \u201cfinal balance\u201d).  \n   - Drop any such columns outright from the training set.\u202f([7a\u202fData\u202fLeakage](https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html))\n\n2. **Create a \u201cleak\u2011free\u201d validation scheme**  \n   - If the data have a time dimension, split by date so that the validation set contains only later timestamps than the training set (chronological split).  \n   - For grouped data (e.g., customers, machines), use group\u2011aware splits (GroupKFold) to keep all records of a group together in either train or validation.\u202f([Smart\u202fSplitting](https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553);\u202f[MachineLearningMastery](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it))\n\n3. **Validate the split**  \n   - Verify that none of the excluded post\u2011target columns appear in either split (e.g., `assert not any(col in train.columns for col in leak_cols)`).  \n   - Check that the distribution of timestamps or groups in the validation set truly follows the competition\u2019s test\u2011time scenario.\n\n4. **Feature engineering only on allowed data**  \n   - Build new features using only pre\u2011target variables (e.g., historical aggregates, lag features) and ensure any aggregations respect the split (compute aggregates on the training fold only).  \n\n5. **Iterate and monitor**  \n   - After each model iteration, re\u2011run the leakage audit; new engineered features can unintentionally re\u2011introduce post\u2011target information.  \n\nBy systematically removing post\u2011event features and using time\u2011 or group\u2011aware validation, Kaggle winners keep their models honest and avoid the inflated scores that leakage would otherwise produce.",
      "url": ""
    },
    {
      "title": "7a Data Leakage",
      "text": "<div><div>\n<p><em>This tutorial is part of the <a href=\"https://www.kaggle.com/learn/machine-learning/\">Learn Machine Learning</a> series. In this step, you will learn what data leakage is and how to prevent it.</em></p>\n<h2>What is Data Leakage<a href=\"#What-is-Data-Leakage\">\u00b6</a></h2><p>Data leakage is one of the most important issues for a data scientist to understand. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in the most subtle and dangerous ways. Specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. This tutorial will show you what leakage is and how to avoid it.</p>\n<p>There are two main types of leakage: <strong>Leaky Predictors</strong> and a <strong>Leaky Validation Strategies.</strong></p>\n<h2>Leaky Predictors<a href=\"#Leaky-Predictors\">\u00b6</a></h2><p>This occurs when your predictors include data that will not be available at the time you make predictions.</p>\n<p>For example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:</p>\n<table>\n<thead><tr>\n<th>got_pneumonia</th>\n<th>age</th>\n<th>weight</th>\n<th>male</th>\n<th>took_antibiotic_medicine</th>\n<th>...</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>False</td>\n<td>65</td>\n<td>100</td>\n<td>False</td>\n<td>False</td>\n<td>...</td>\n</tr>\n<tr>\n<td>False</td>\n<td>72</td>\n<td>130</td>\n<td>True</td>\n<td>False</td>\n<td>...</td>\n</tr>\n<tr>\n<td>True</td>\n<td>58</td>\n<td>100</td>\n<td>False</td>\n<td>True</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n<p>-</p>\n<p>People take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But <em>took_antibiotic_medicine</em> is frequently changed <strong>after</strong> the value for <em>got_pneumonia</em> is determined. This is target leakage.</p>\n<p>The model would see that anyone who has a value of <code>False</code> for <code>took_antibiotic_medicine</code> didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.</p>\n<p>To prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.</p>\n<p></p>\n</div><div>\n<h2>Leaky Validation Strategy<a href=\"#Leaky-Validation-Strategy\">\u00b6</a></h2><p>A much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.</p>\n<h2>Preventing Leaky Predictors<a href=\"#Preventing-Leaky-Predictors\">\u00b6</a></h2><p>There is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.</p>\n<p>However, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:</p>\n<ul>\n<li>To screen for possible leaky predictors, look for columns that are statistically correlated to your target.</li>\n<li>If you build a model and find it extremely accurate, you likely have a leakage problem.</li>\n</ul>\n<h2>Preventing Leaky Validation Strategies<a href=\"#Preventing-Leaky-Validation-Strategies\">\u00b6</a></h2><p>If your validation is based on a simple train-test split, exclude the validation data from any type of <em>fitting</em>, including the fitting of preprocessing steps. This is easier if you use <a href=\"https://www.kaggle.com/dansbecker/pipelines\">scikit-learn Pipelines</a>. When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.</p>\n<h2>Example<a href=\"#Example\">\u00b6</a></h2><p>We will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called <em>card</em>). Here is a look at the data:</p>\n</div><div>\n<p>We can see with <code>data.shape</code> that this is a small dataset (1312 rows), so we should use cross-validation to ensure accurate measures of model quality</p>\n</div><div>\n<p>With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.</p>\n<p>Here is a summary of the data, which you can also find under the data tab:</p>\n<ul>\n<li><strong>card:</strong> Dummy variable, 1 if application for credit card accepted, 0 if not</li>\n<li><strong>reports:</strong> Number of major derogatory reports</li>\n<li><strong>age:</strong> Age n years plus twelfths of a year</li>\n<li><strong>income:</strong> Yearly income (divided by 10,000)</li>\n<li><strong>share:</strong> Ratio of monthly credit card expenditure to yearly income</li>\n<li><strong>expenditure:</strong> Average monthly credit card expenditure</li>\n<li><strong>owner:</strong> 1 if owns their home, 0 if rent</li>\n<li><strong>selfempl:</strong> 1 if self employed, 0 if not.</li>\n<li><strong>dependents:</strong> 1 + number of dependents</li>\n<li><strong>months:</strong> Months living at current address</li>\n<li><strong>majorcards:</strong> Number of major credit cards held</li>\n<li><strong>active:</strong> Number of active credit accounts</li>\n</ul>\n<p>A few variables look suspicious. For example, does <strong>expenditure</strong> mean expenditure on this card or on cards used before appying?</p>\n<p>At this point, basic data comparisons can be very helpful:</p>\n</div><div>\n<p>Everyone with <code>card == False</code> had no expenditures, while only 2% of those with <code>card == True</code> had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means *expenditures on the card they applied for.**.</p>\n<p>Since <strong>share</strong> is partially determined by <strong>expenditure</strong>, it should be excluded too. The variables <strong>active</strong>, <strong>majorcards</strong> are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.</p>\n<p>We would run a model without leakage as follows:</p>\n</div><div>\n<p>This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.).</p>\n<h2>Conclusion<a href=\"#Conclusion\">\u00b6</a></h2><p>Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data is a first step, and pipelines can help implement this separation. Leaking predictors are a more frequent issue, and leaking predictors are harder to track down. A combination of caution, common sense and data exploration can help identify leaking predictors so you remove them from your model.</p>\n<h2>Exercise<a href=\"#Exercise\">\u00b6</a></h2><p>Review the data in your ongoing project. Are there any predictors that may cause leakage? As a hint, most datasets from Kaggle competitions don't have these variables. Onc...",
      "url": "https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7a%20Data%20Leakage.html"
    },
    {
      "title": "Smart Splitting Prevents Cross-Validation Disasters",
      "text": "Prevent Data Leakage in Cross-Validation | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://rsci.app.link/?$canonical_url=https://medium.com/p/894863a93553&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderUser&amp;~stage=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Smart Splitting Prevents Cross-Validation Disasters\n[\n![Paco Sun](https://miro.medium.com/v2/resize:fill:64:64/1*ZKhzskdSkU9jVOKVWtuTxg.jpeg)\n](https://medium.com/@pacosun?source=post_page---byline--894863a93553---------------------------------------)\n[Paco Sun](https://medium.com/@pacosun?source=post_page---byline--894863a93553---------------------------------------)\n5 min read\n\u00b7May 3, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/894863a93553&amp;operation=register&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;user=Paco+Sun&amp;userId=a49c41746153&amp;source=---header_actions--894863a93553---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/894863a93553&amp;operation=register&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;source=---header_actions--894863a93553---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nYou\u2019ve just trained a model that boasts 95% accuracy, your metrics are stellar, and you\u2019re ready to deploy. Then reality hits like a cold shower: in production, your high-performing model barely beats random guessing. What went wrong?\nThe culprit is often**data leakage**, when information from your test set sneaks into your training process, creating an illusion of competence. This deception usually pairs with**class imbalance**, where one category dominates your dataset.\nEven with cross-validation, your model can still mislead you if you\u2019re not careful. The truth is,**preprocessing decisions and skewed distributions can turn your CV pipeline into an exercise in self-deception.**In this article, we\u2019ll explore how to build robust validation strategies that won\u2019t crumble when they meet real-world data.\n## Data Leakage Explained\nData leakage is the ML equivalent of a student peeking at the answer key before the test. It occurs when information from your test set contaminates your training process, giving your model an unfair advantage that vanishes in production.\n### **Common Types of Leakage:**\n* **Preprocessing leakage:**The sneaky one. When you scale your entire dataset before splitting, you\u2019re telling your model about the test set\u2019s distribution\n```\n# WRONG: This causes leakage!\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX\\_scaled = scaler.fit\\_transform(X) # Scaling BEFORE splitting\nX\\_train, X\\_test = train\\_test\\_split(X\\_scaled, y)\n```\n* **Feature engineering leakage:**Creating features that implicitly contain information about the target such as using**average payment amount**in fraud detection when fraudulent transactions are typically higher\n* **Target leakage:**The most dangerous form, where features directly derived from the target variable sneak into your model\nHere\u2019s a real-world scenario: A cancer detection model achieved 98% accuracy by including`patient\\_id`as a feature. Why? The hospital\u2019s ID system assigned sequential numbers to patients, and later IDs correlated with advanced diagnostic equipment availability.**The model wasn\u2019t detecting cancer, but when patients were diagnosed instead.**\nThese leaks create models that only**look**brilliant.\n## Your First Line of Defence\nPipelines ensure transformations happen at the right time, in the right order, preventing information from crossing the train-test boundary.\nScikit-learn\u2019s`Pipeline`and`ColumnTransformer`are your weapons of choice. They bundle preprocessing steps with your model, ensuring**transformations are learned from training data only**and applied consistently to test data.\nWhy? Because fitting a scaler on your entire dataset is like telling your model information it shouldn\u2019t have during training.\n```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n# CORRECT: Transformations inside the pipeline\nnumeric\\_pipeline = Pipeline([\n(&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;median&#x27;)),\n(&#x27;scaler&#x27;, StandardScaler())\n])\npreprocessor = ColumnTransformer([\n(&#x27;&#x27;num&#x27;&#x27;, numeric\\_pipeline, some\\_numeric\\_features),\n(&#x27;&#x27;cat&#x27;&#x27;, OneHotEncoder(), some\\_categorical\\_features)\n])\n# Combine preprocessing with model\nfull\\_pipeline = Pipeline([\n(&#x27;preprocessor&#x27;, preprocessor),\n(&#x27;classifier&#x27;, RandomForestClassifier())\n])\n# Now CV won&#x27;t leak\nscores = cross\\_val\\_score(full\\_pipeline, X, y, cv=5)\n```\nFor each fold, the pipeline:\n* Fits transformers on training data only\n* Transforms both training and validation data\n* Trains the model\n* Evaluates on truly unseen, properly transformed validation data\nThis simple choice can be the difference between a model that**generalizes**and one that**memorizes**.\n## The Problem of Imbalanced Data\nImagine training a fraud detector where 99% of transactions are legitimate. Regular K-Fold might create folds with 100% legitimate transactions, which means your model learns to predict**\u201cnot fraud\u201d**for everything and still achieves 99% accuracy!\nClass imbalance turns standard CV into a game of chance. Some folds might completely miss the minority class, leading to wildly inconsistent performance estimates.\n**Stratified K-Fold**to the rescue! It preserves class proportions in each fold, ensuring every split represents your data\u2019s true distribution.\n> For more details, feel free to check out my stratified k-fold article [**> here\n**](https://medium.com/@pacosun/stratified-k-fold-cross-validation-when-balance-matters-c28b9a7cb9bc)> .\n## Pitfalls with Resampling Techniques (SMOTE, Undersampling)\nSMOTE before splitting is like photocopying your test answers, you\u2019re creating synthetic samples using information from data you\u2019re supposed to be testing on.\nWhen you apply SMOTE to your entire dataset before cross-validation, the synthetic samples in your training set contain information about the test set\u2019s minority class distribution. This creates optimistic performance estimates that crumble in production.\n```\n# WRONG: SMOTE before splitting causes leakage\nfrom imblearn.over\\_sampling import SMOTE\n# This leaks test information into training!\nX\\_resampled, y\\_resampled = SMOTE().fit\\_resample(X, y)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X\\_resampled, y\\_resampled)\n```\nSolution?**Keep it inside your pipeline.**\n```\n# CORRECT: SMOTE within the pipeline\nfrom i...",
      "url": "https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553"
    },
    {
      "title": "3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)",
      "text": "3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It) - MachineLearningMastery.com3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It) - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Crash-Course]()\n# 3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)\nBy[Iv\u00e1n Palomares Carrascosa](https://machinelearningmastery.com/author/ivanpc/)onDecember 10, 2025in[Practical Machine Learning](https://machinelearningmastery.com/category/practical-machine-learning/)[**0](https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it/#respond)\nShare*Post*Share\nIn this article, you will learn what data leakage is, how it silently inflates model performance, and practical patterns for preventing it across common workflows.\nTopics we will cover include:\n* Identifying target leakage and removing target-derived features.\n* Preventing train\u2013test contamination by ordering preprocessing correctly.\n* Avoiding temporal leakage in time series with proper feature design and splits.\nLet&#8217;s get started.\n![3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)](https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-ipc-data-leakage-1.jpeg)\n3 Subtle Ways Data Leakage Can Ruin Your Models (and How to Prevent It)\nImage by Editor\n## Introduction\n**Data leakage**is an often accidental problem that may happen in machine learning modeling. It happens when the data used for training contains information that &#8220;shouldn&#8217;t be known&#8221; at this stage &mdash; i.e. this information has leaked and become an &#8220;intruder&#8221; within the training set. As a result, the trained model has gained a sort of unfair advantage, but only in the very short run: it might perform suspiciously well on the training examples themselves (and validation ones, at most), but it later performs pretty poorly on future unseen data.\nThis article shows three practical machine learning scenarios in which data leakage may happen, highlighting how it affects trained models, and showcasing strategies to prevent this issue in each scenario. The data leakage scenarios covered are:\n1. Target leakage\n2. Train-test split contamination\n3. Temporal leakage in time series data## Data Leakage vs. Overfitting</h3>\nEven though data leakage and overfitting can produce similar-looking results, they are different problems.\n**Overfitting**arises when a model memorizes overly specific patterns from the training set, but the model is not necessarily receiving any illegitimate information it shouldn&#8217;t know at the training stage &mdash; it is just learning excessively from the training data.\n**Data leakage**, by contrast, occurs when the model is exposed to information it should not have during training. Moreover, while overfitting typically arises as a poorly generalizing model on the validation set, the consequences of data leakage may only surface at a later stage, sometimes already in production when the model receives truly unseen data.\n![Data Leakage vs. Overfitting](https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-ipc-data-leakage-2.jpeg)\nData leakage vs. overfitting\nImage by Editor\nLet&#8217;s take a closer look at 3 specific data leakage scenarios.\n## Scenario 1: Target Leakage\nTarget leakage occurs when features contain information that directly or indirectly reveals the target variable. Sometimes this can be the result of a wrongly applied feature engineering process in which target-derived features have been introduced in the dataset. Passing training data containing such features to a model is comparable to a student cheating on an exam: part of the answers they should come up with by themselves has been provided to them.\nThe examples in this article use**[scikit-learn](https://scikit-learn.org/)**,**[Pandas](https://pandas.pydata.org/)**, and**[NumPy](https://numpy.org/)**.\nLet&#8217;s see an example of how this problem may arise when training a dataset to predict diabetes. To do so, we will intentionally incorporate a predictor feature derived from the target variable,`'target'`(of course, this issue in practice tends to happen by accident, but we are injecting it on purpose in this example to illustrate how the problem manifests!):\nfrom sklearn.datasets import load\\_diabetes\rimport pandas as pd\rimport numpy as np\rfrom sklearn.linear\\_model import LogisticRegression\rfrom sklearn.model\\_selection import train\\_test\\_split\rX, y = load\\_diabetes(return\\_X\\_y=True, as\\_frame=True)\rdf = X.copy()\rdf['target'] = (y &gt;&gt; y.median()).astype(int) # Binary outcome\r# Add leaky feature: related to the target but with some random noise\rdf['leaky\\_feature'] = df['target'] + np.random.normal(0, 0.5, size=len(df))\r# Train and test model with leaky feature\rX\\_leaky = df.drop(columns=['target'])\ry = df['target']\rX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X\\_leaky, y, random\\_state=0, stratify=y)\rclf = LogisticRegression(max\\_iter=1000).fit(X\\_train, y\\_train)\rprint(\"Test accuracy with leakage:\", clf.score(X\\_test, y\\_test))\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n|\nfromsklearn.datasetsimportload\\_diabetes\nimportpandasaspd\nimportnumpyasnp\nfromsklearn.linear\\_modelimportLogisticRegression\nfromsklearn.model\\_selectionimporttrain\\_test\\_split\nX,y=load\\_diabetes(return\\_X\\_y=True,as\\_frame=True)\ndf=X.copy()\ndf['target']=(y&gt;y.median()).astype(int)# Binary outcome\n# Add leaky feature: related to the target but with some random noise\ndf['leaky\\_feature']=df['target']+np.random.normal(0,0.5,size=len(df))\n# Train and test model with leaky feature\nX\\_leaky=df.drop(columns=['target'])\ny=df['target']\nX\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X\\_leaky,y,random\\_state=0,stratify=y)\nclf=LogisticRegression(max\\_iter=1000).fit(X\\_train,y\\_train)\nprint(\"Test accuracy with leakage:\",clf.score(X\\_test,y\\_test))\n|\nNow, to compare accuracy results on the test set without the &#8220;leaky feature&#8221;, we will remove it and retrain the model:\n# Removing leaky feature and repeating the process\rX\\_clean = df.drop(columns=['target', 'leaky\\_feature'])\rX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X\\_clean, y, random\\_state=0, stratify=y)\rclf = LogisticRegression(max\\_iter=1000).fit(X\\_train, y\\_train)\rprint(\"Test accuracy without leakage:\", clf.score(X\\_test, y\\_test))\n1\n2\n3\n4\n5\n|\n# Removing leaky feature and repeating the process\nX\\_clean=df.drop(columns=['target','leaky\\_feature'])\nX\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X\\_clean,y,random\\_state=0,stratify=y)\nclf=LogisticRegression(max\\_iter=1000).fit(X\\_train,y\\_train)\nprint(\"Test accuracy without leakage:\",clf.score(X\\_test,y\\_test))\n|\nYou may get a result like:\nTest accuracy with leakage: 0.8288288288288288\rTest accuracy without leakage: 0.7477477477477478\n1\n2\n|\nTestaccuracywithleakage:0.8288288288288288\nTestaccuracywithoutleakage:0.7477477477477478\n|\nWhich makes us wonder:*wasn\u2019t data leakage supposed to ruin our model, as the article title suggests?*In fact, it is, and this is why data leakage can be difficult to spot until it might be late: as mentioned in the introduction, the problem often manifests as inflated accuracy both in training and in validation/test sets, with the performance downfall only noticeable once the model is exposed to new, real-world data. Strategies to prevent it ideally include a combination of steps like carefully analyzing correlations between the target and the rest of the features, checking feature weights in a newly trained model and seeing if any feature has an overly large weight, and so on.\n## Scenario 2: Train-Test Split Contamination\nAnother very frequent data leakage scen...",
      "url": "https://machinelearningmastery.com/3-subtle-ways-data-leakage-can-ruin-your-models-and-how-to-prevent-it"
    },
    {
      "title": "What is Data Leakage \u00b6",
      "text": "7b Exercise; Data Leakage\n*This tutorial is part of the[Learn Machine Learning](https://www.kaggle.com/learn/machine-learning/)series. In this step, you will learn what data leakage is and how to prevent it.*\n# What is Data Leakage[&#182;](#What-is-Data-Leakage)\nData leakage is one of the most important issues for a data scientist to understand. If you don't know how to prevent it, leakage will come up frequently, and it will ruin your models in the most subtle and dangerous ways. Specifically, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate. This tutorial will show you what leakage is and how to avoid it.\nThere are two main types of leakage:**Leaky Predictors**and a**Leaky Validation Strategies.**\n## Leaky Predictors[&#182;](#Leaky-Predictors)\nThis occurs when your predictors include data that will not be available at the time you make predictions.\nFor example, imagine you want to predict who will get sick with pneumonia. The top few rows of your raw data might look like this:\n|got\\_pneumonia|age|weight|male|took\\_antibiotic\\_medicine|...|\nFalse|65|100|False|False|...|\nFalse|72|130|True|False|...|\nTrue|58|100|False|True|...|\n-\nPeople take antibiotic medicines after getting pneumonia in order to recover. So the raw data shows a strong relationship between those columns. But*took\\_antibiotic\\_medicine*is frequently changed**after**the value for*got\\_pneumonia*is determined. This is target leakage.\nThe model would see that anyone who has a value of`False`for`took\\_antibiotic\\_medicine`didn't have pneumonia. Validation data comes from the same source, so the pattern will repeat itself in validation, and the model will have great validation (or cross-validation) scores. But the model will be very inaccurate when subsequently deployed in the real world.\nTo prevent this type of data leakage, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n![Leaky Data Graphic](https://i.imgur.com/CN4INKb.png)\n## Leaky Validation Strategy[&#182;](#Leaky-Validation-Strategy)\nA much different type of leak occurs when you aren't careful distinguishing training data from validation data. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train\\_test\\_split. Validation is meant to be a measure of how the model does on data it hasn't considered before. You can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir.. The end result? Your model will get very good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions.\n## Preventing Leaky Predictors[&#182;](#Preventing-Leaky-Predictors)\nThere is no single solution that universally prevents leaky predictors. It requires knowledge about your data, case-specific inspection and common sense.\nHowever, leaky predictors frequently have high statistical correlations to the target. So two tactics to keep in mind:\n* To screen for possible leaky predictors, look for columns that are statistically correlated to your target.\n* If you build a model and find it extremely accurate, you likely have a leakage problem.## Preventing Leaky Validation Strategies[&#182;](#Preventing-Leaky-Validation-Strategies)\nIf your validation is based on a simple train-test split, exclude the validation data from any type of*fitting*, including the fitting of preprocessing steps. This is easier if you use[scikit-learn Pipelines](https://www.kaggle.com/dansbecker/pipelines). When using cross-validation, it's even more critical that you use pipelines and do your preprocessing inside the pipeline.\n# Example[&#182;](#Example)\nWe will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called*card*). Here is a look at the data:\nIn[1]:\n```\nimportpandasaspddata=pd.read\\_csv(&#39;&#39;../input/AER\\_credit\\_card\\_data.csv&#39;&#39;,true\\_values=[&#39;yes&#39;],false\\_values=[&#39;no&#39;])print(data.head())\n```\n```\ncard reports age ... months majorcards active\n0 True 0 37.66667 ... 54 1 12\n1 True 0 33.25000 ... 34 1 13\n2 True 0 33.66667 ... 58 1 5\n3 True 0 30.50000 ... 25 1 7\n4 True 0 32.16667 ... 64 1 5\n[5 rows x 12 columns]\n```\nWe can see with`data.shape`that this is a small dataset (1312 rows), so we should use cross-validation to ensure accurate measures of model quality\nIn[2]:\n```\ndata.shape\n```\nOut[2]:\n```\n(1319, 12)\n```\nIn[3]:\n```\nfromsklearn.pipelineimportmake\\_pipelinefromsklearn.ensembleimportRandomForestClassifierfromsklearn.model\\_selectionimportcross\\_val\\_scorey=data.cardX=data.drop([&#39;card&#39;],axis=1)# Since there was no preprocessing, we didn&#39;t need a pipeline here. Used anyway as best practicemodeling\\_pipeline=make\\_pipeline(RandomForestClassifier(n\\_estimators=10))cv\\_scores=cross\\_val\\_score(modeling\\_pipeline,X,y,scoring=&#39;accuracy&#39;,cv=5)print(&quot;Cross-val accuracy:%f&quot;%cv\\_scores.mean())\n```\n```\nCross-val accuracy: 0.978765\n```\nWith experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage.\nHere is a summary of the data, which you can also find under the data tab:\n* **card:**Dummy variable, 1 if application for credit card accepted, 0 if not\n* **reports:**Number of major derogatory reports\n* **age:**Age n years plus twelfths of a year\n* **income:**Yearly income (divided by 10,000)\n* **share:**Ratio of monthly credit card expenditure to yearly income\n* **expenditure:**Average monthly credit card expenditure\n* **owner:**1 if owns their home, 0 if rent\n* **selfempl:**1 if self employed, 0 if not.\n* **dependents:**1 + number of dependents\n* **months:**Months living at current address\n* **majorcards:**Number of major credit cards held\n* **active:**Number of active credit accounts\nA few variables look suspicious. For example, does**expenditure**mean expenditure on this card or on cards used before appying?\nAt this point, basic data comparisons can be very helpful:\nIn[4]:\n```\nexpenditures\\_cardholders=data.expenditure[data.card]expenditures\\_noncardholders=data.expenditure[\\~data.card]print(&#39;Fraction of those who received a card with no expenditures:%.2f&#39;\\\\%((expenditures\\_cardholders==0).mean()))print(&#39;Fraction of those who received a card with no expenditures:%.2f&#39;\\\\%((expenditures\\_noncardholders==0).mean()))\n```\n```\nFraction of those who received a card with no expenditures: 0.02\nFraction of those who received a card with no expenditures: 1.00\n```\nEveryone with`card == False`had no expenditures, while only 2% of those with`card == True`had no expenditures. It's not surprising that our model appeared to have a high accuracy. But this seems a data leak, where expenditures probably means \\*expenditures on the card they applied for.\\*\\*.\nSince**share**is partially determined by**expenditure**, it should be excluded too. The variables**active**,**majorcards**are a little less clear, but from the description, they sound concerning. In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more.\nWe would run a model without leakage as follows:\nIn[5]:\n```\npotential\\_leaks=[&#39;expenditure&#39;,&#39;share&#39;,&#39;active&#39;,&#39;majorcards&#39;]X2=X.drop(potential\\_leaks,axis=1)cv\\_scores=cross\\_val\\_score(modeling\\_pipeline,X2,y,scoring=&#39;accuracy&#39;,cv=5)print(&quot;Cross-val accuracy:%f&quot;%cv\\_scores.mean())\n```\n```\nCross-val accuracy: 0.804400\n```\nThis accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would lik...",
      "url": "https://plsms.github.io/kaggle/learn/2%20Machine%20Learning/Level%202/7b%20Exercise;%20Data%20Leakage.html"
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    },
    {
      "title": "What is Target Leakage in Machine Learning and How Do I Avoid It?",
      "text": "![](https://aorta.clickagy.com/pixel.gif?clkgypv=jstag&ws=1)What is Target Leakage in Machine Learning and How Do I Avoid It? \\| DataRobot Blog\n\n[Skip to content](https://www.datarobot.com/blog/what-is-target-leakage-in-machine-learning-and-how-do-i-avoid-it/#content)\n\n- [Blog](https://www.datarobot.com/blog/)\n- [AI & ML Expertise](https://www.datarobot.com/blog/category/ai-ml-expertise/)\n- What is Target Leakage in Machine Learning and How Do I Avoid It?\n\n# What is Target Leakage in Machine Learning and How Do I Avoid It?\n\nMarch 2, 2020\n\nby\n\n[Alex Shoop](https://www.datarobot.com/blog/author/alex-shoop/)\n\n\u00b7\u20024 min\u00a0read\n\n_This post was originally part of the DataRobot Community. [Visit now](https://community.datarobot.com/t5/datarobot-community/ct-p/en) to browse discussions and ask questions about DataRobot, [AI Platform](https://www.datarobot.com/platform/), [data science](https://www.datarobot.com/wiki/data-science/), and more._\n\nWe should all be familiar with data science best practices to prevent models from \u201coverfitting\u201d to the data by using validation techniques such as train-test-split, k-fold [cross validation](https://www.datarobot.com/wiki/cross-validation/) (CV), etc. However, [AI](https://www.datarobot.com/wiki/artificial-intelligence/) practitioners should also pay close attention to the content of their training data, or else we could be at risk of causing **target** **leakage** (sometimes also known as data leakage). In this blog post, I\u2019ll explain what target leakage is, show you an example, and then provide a few suggestions to avoid potential leakage.\n\nLet\u2019s say, for example, you want to build a predictive [model](https://www.datarobot.com/wiki/model/) that will be used to approve/deny a customer\u2019s application for a loan. After the raw data collection and aggregation work, you obtain a dataset that is data rich, meaning it contains various data types such as numerical, categorical, text, etc., and the dataset is also wide (i.e., contains many columns). Using this dataset, you end up building many remarkably accurate models.\n\nGreat! Give yourself a pat on the back, thanks to the superb accuracy scores of your built models! But, the scores appear suspiciously too accurate: the training and CV scores all look extremely good and the number of model errors appears to be very low. In the [DataRobot platform](https://www.datarobot.com/platform/), this is evident from the Leaderboard validation scores where, for LogLoss metric, the lower the score, the better the model. In Figure 1, you see examples of very low validation/CV/holdout scores and quite nonsensical predictions. The surprisingly low scores and unusual predictions should raise red flags.\n\n![DR community target leakage leaderboard updated screenshot1](https://www.datarobot.com/wp-content/uploads/2021/12/DR-community-target-leakage-leaderboard-updated-screenshot1-2048x969.png)Figure 1. Leaderboard models with their LogLoss scores and table snippet of a few predictions\n\nWhat could have happened? When you look back at this example\u2019s training dataset, \u201cLoan applications \\[5-year-contracts\\] (Year: 2015),\u201d you discover a column called \u201cloan\\_status.\u201d You ask yourself, \u201cWould a new candidate\u2019s loan application have \u201cloan status\u201d in the input form?\u201d The answer? **No**, because the loan hasn\u2019t even been approved/denied yet! The \u201cloan\\_status\u201d feature is a proxy for the target, thus causing target leakage for the built models.\n\n![target2 Screen Shot 2020 03 02 at 3.59.19 PM](https://www.datarobot.com/wp-content/uploads/2021/12/target2-Screen-Shot-2020-03-02-at-3.59.19-PM-2048x646.png)Figure 2. Target leakage visualization diagram\n\nBasically, target leakage can happen when you train your model on a dataset that includes information that would NOT be available at the time of prediction, i.e., when you apply the trained model in the field. The consequence of this is that for certain types of target leakage, bad model behavior will only be spotted when the model is deployed in production; this means it\u2019s difficult to detect leakage during model development, even if we use k-fold CV and related validation strategies.\n\nLet\u2019s say we have a loan applicant named _CustomerX_. The loan outcome for _CustomerX_ wouldn\u2019t be known until after the prediction point, i.e., until after the loan is approved and provided.\n\n![target3 Screen Shot 2020 03 02 at 3.59.26 PM](https://www.datarobot.com/wp-content/uploads/2021/12/target3-Screen-Shot-2020-03-02-at-3.59.26-PM.png)Figure 3. Target leakage for loan example\n\nWhile it could be that you may have outcome data from previous loans, if you include all information when training your models (including the outcome information from previous loans), you could potentially build models that are too optimistic and unreliable when applied to data in the real world.\n\nBy now you can imagine what target leakage looks like for temporal (aka time series) data. To put it simply, if you include data \u201cfrom the future\u201d in your training dataset, then clearly your model would be cheating by already knowing the answer \u201cfrom the future\u201d to make predictions \u201cin the future;\u201d this is almost like building a time machine, which is impossible. In order to build accurate AND robust predictive models, what are some ways to avoid target leakage?\n\nLeakage is not an issue that exists in one stage but rather it can take many forms and be introduced during different stages when building an AI system. One type of leakage can be inadvertently caused by a chosen validation strategy. To resolve this type of leakage, you should take extra care when partitioning your data, such as setting aside a holdout portion for final testing purposes only.\u00a0(To learn more about leakage that appears in testing and validation areas, see [Yuriy Guts\u2019s presentation](https://github.com/YuriyGuts/odsc-target-leakage-workshop/blob/master/presentation.pdf).)\n\nLeakage detection is tricky because it\u2019s more about the workflow: how and when the predictions of a model will be used. As illustrated in the above loan example, leakage caused by certain \u201cleaky\u201d features is a difficult challenge. Determining whether one or more features are at risk of causing target leakage usually requires subject matter expertise and domain knowledge. Here are some suggestions to keep in mind when determining potential target leakage:\n\n- If a trained model is extremely accurate, you should be suspicious of the results; as the saying goes, if it\u2019s too good to be true, then it probably is.\n- Inspect your training dataset and perform exploratory data analysis (EDA) to determine if any features are statistically highly correlated to the target (or may completely\u2014or near-perfectly\u2014predict the target).\n\nThe DataRobot platform already has built-in target leakage detection which runs during the EDA stage after target selection. Since target leakage can lead to serious problems for models put in production, the GUI shows many visual indicators which prompt you to take action (Figure 4). For a feature that is flagged as \u201chigh risk of target leakage,\u201d DataRobot automatically removes the \u201cleaky\u201d feature during the Autopilot model building stage.\n\n![lhaviland 0 1620938508594](https://www.datarobot.com/wp-content/uploads/2021/12/lhaviland_0-1620938508594.jpg)Figure 4. Target leakage detection GUI in the DataRobot platform\n\nAs you can see in Figure 5, the validation scores and predictions for the resulting built models look much better now!\n\n![DR community target leakage leaderboard updated screenshot2](https://www.datarobot.com/wp-content/uploads/2021/12/DR-community-target-leakage-leaderboard-updated-screenshot2-2048x1011.png)Figure 5. Reasonable validation scores and sensible predictions with target leakage removed\n\n## Related Resources\n\n- Artificial Intelligence Wiki: [Target Leakage](https://www.datarobot.com/wiki/target-leakage/)\n- [AI Simplified: Target Leakage](https://www.datarobot.com/resources/ai-simplified-target-leakage/)\n- [DataRobot Public Docume...",
      "url": "https://www.datarobot.com/blog/what-is-target-leakage-in-machine-learning-and-how-do-i-avoid-it"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    },
    {
      "title": "Controlling Leaks",
      "text": "Register today for our Generative AI Foundations course. Use code GenAI99 for a discount price of $99!\n\n\u2715\n\n[Skip to content](https://www.statistics.com/controlling-leaks/#content)\n\nGood psychics have a knack for getting their audience to reveal, unwittingly, information that can be turned around and used in a prediction.\u00a0 Statisticians and data scientists fall prey to a related phenomenon, leakage, when they allow into their models highly predictive features that would be unavailable at prediction time.\u00a0 In one noted example, a highly successful machine learning model was developed to classify skin moles as cancerous or benign.\u00a0 The model\u2019s success was too good to be true \u2013 most of the malignant moles in the training images had small rulers placed there by the examining dermatologist.\n\n![malignant moles](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20199%20140'%3E%3C/svg%3E)\n\nOnly moles judged by the doctor as likely to be malignant got the rulers, so the model was able to use the presence or absence of a ruler (and, in effect, the dermatologist\u2019s opinion) as a highly effective predictor.\u00a0 A similar example was a model developed to predict whether a patient had prostate cancer.\u00a0 An extremely effective predictor was PROSSURG, an indicator variable for whether a patient had had prostate surgery.\u00a0 It seems an obvious error when looked at in isolation, but big data machine learning models often have hundreds or thousands of predictors with cryptic names.\u00a0 Leaks like this can be hard to identify _a priori_.\n\n### Time Series Forecasting is Prone to Leaks\n\nJohn Elder, Founder and Chairman of [Elder Research](https://www.statistics.com/elder-research-capabilities/) (which owns [Statistics.com](https://www.statistics.com/)) described several examples of leaks in financial models in his _Top 10 Data Mining Mistakes_.\u00a0 A couple:\n\n- A neural net model prepared for a Chicago bank was 95% accurate in predicting interest rate changes. Unfortunately, a version of the output variable was included as a predictor. (This experience produced a useful window on the workings of the neural net, which lost 5% of the predictor\u2019s information as it moved it through the network.)\n- Elder was once called on to assess a proprietary model that was 70% accurate in forecasting directional market movements. Elder turned detective, and a painstaking investigation revealed that the model was (inadvertently) nothing more than a simple 3-day average centered on today.\u00a0 Forecasting tomorrow\u2019s price is easy if tomorrow\u2019s price is available to the model.\u00a0 (Omitting the first of the three days would have achieved 100% directional accuracy!)\n\nElder\u2019s colleague, Mike Thurber, recounted a project he supervised whose goal was to forecast sales of a particular retail category. One of the features was year-over-year growth of category share, which leaked future information into the model just as the 3-day average Elder discovered did. Time series forecasting is particularly prone to this [type of error](https://www.statistics.com/type-iii-err/).\u00a0 A moving average window centered at _t_ and including information about _t-1_ and _t+1_ (and often further backward and forward) is a common way to smooth out random noise and visualize time series.\u00a0 If it is used for prediction, however, only information at _t_ and prior, i.e. a trailing window, should be included.\n\nCentered window: _t-1 + t + t+1_\n\nTrailing window:\u00a0\u00a0\u00a0\u00a0 _t-2 + t-1 + t_\n\n### Kaggle\u2019s Leak Lessons\n\nKaggle\u2019s general [contest documentation](https://www.kaggle.com/docs/competitions) has a section on leakage, and a description of general types of leak problems.\u00a0 We\u2019ve discussed two of them, leaks from the future into time series forecasts, and inclusion of highly correlated predictors that will not be available at time of prediction. Some other examples include:\n\n- Leaks from holdout (or test) data into training data. There are two types of holdout data (the nomenclature is not always clear).\u00a0 One type is used iteratively to tune and select model parameters, and to select top performing models.\u00a0 Another is set aside and never seen during model development; it is used to get an unbiased estimate of \u201creal-world\u201d model performance.\u00a0 Inclusion of test data in [training data](https://www.statistics.com/07-16-2013-week-29-training-data/) can occur due to mistakes during data preparation, and can contaminate the overall process. If there are leaks from the first type of holdout data, protections against overfitting during model-tuning will be reduced.\u00a0 If there are leaks from the second type, estimates of deployed model performance will be overly optimistic.\n- Failure to remove obfuscated replacements for forbidden variables. For example, an app maker may create a [predictor variable](https://www.statistics.com/glossary/predictor-variable/) with random noise injected into a user\u2019s location data, to allow sharing of data with partners or consultants without revealing exact location data.\u00a0 Internal data scientists attempting to predict future location might leave such a variable in the model, not knowing exactly what it is, and end up with unrealistic location predictions.\n\nOther scenarios are inclusion of data not present in the model\u2019s operational environment, and distortion from samples outside the scope of the model\u2019s intended use.\n\nInterestingly, Kaggle provides a good window on how _not_ to approach leaks, for two reasons:\n\n- In the Kaggle environment, the data, once released, are set in stone. There is no opportunity for the inquiring data scientist to go back and query the domain experts, and perhaps modify the available data, as ought to happen in a normal project environment.\n- Leaks in the Kaggle world are good things, to be prized by those who find them and exploited for advantage. Kaggle reported one case of a team that achieved top ranking in a social network link prediction contest by uncovering and scraping the actual network the anonymized data came from, and thereby learning the ground truth.\n\n### Feature Engineering and Selection\n\nAs machine learning moves increasingly towards automation, with data flowing into models, and model output into deployed decisions, practitioners may open the door to leaks by using using automated methods for [feature engineering](https://www.statistics.com/glossary/feature-engineering/) and selection. The rapid growth of deep learning and its capacity to learn features may accelerate this trend.\u00a0 The most widespread use of deep learning now is for image and text processing, where extremely low-level feature (pixels and words) require a lot of consolidation and transformation before they become meaningful. However, it is spreading into models for standard tabular data, where some of the predictors may include unstructured text.\n\nFor example, one group of researchers developed a deep learning model to take raw medical record data (messy, with many features, but still in tabular form) and predict mortality, readmission and diagnosis (read more [here](https://www.statistics.com/feature-engineering-data-prep-still-needed/)).\u00a0 The data had over 100,000 features and included inconsistent medical coding and unstructured text. A deep learning model proved more accurate than a standard manual feature engineering approach, and considerably less labor intensive.\n\nSome automated techniques can automatically check for possible leaks.\u00a0 If database entries are time-stamped, a rule-based system can exclude predictor information that came in after the target prediction time.\u00a0 Predictors that are highly correlated with the target can be flagged for review. (They would need special treatment, in any case, if linear models are used, due to the problem of multicollinearity.)\n\nThe problem of preventing leakage may represent a last redoubt of old-fashioned non-automated, human review of data. You can\u2019t count on a standard preprocessing function to handle all leaks.\u00a0 An illegal predictor that h...",
      "url": "https://www.statistics.com/controlling-leaks"
    }
  ]
}