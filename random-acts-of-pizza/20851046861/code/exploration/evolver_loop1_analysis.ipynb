{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecb3a51",
   "metadata": {},
   "source": [
    "# Loop 1 Analysis: Investigating Data Leakage and Class Imbalance\n",
    "\n",
    "This notebook investigates the suspicious perfect CV scores and severe underfitting to the minority class in the baseline experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd1974",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0effa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Features: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts())\n",
    "print(f\"Success rate: {train_df['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835d6f0",
   "metadata": {},
   "source": [
    "## 2. Investigate Metadata Features for Potential Leakage\n",
    "\n",
    "The evaluator flagged that metadata features might contain indirect target information. Let's examine correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata features function (from baseline)\n",
    "def extract_metadata_features(df, is_train=True):\n",
    "    features = []\n",
    "    \n",
    "    # Account age in days\n",
    "    features.append(df['requester_account_age_in_days_at_request'].fillna(0))\n",
    "    \n",
    "    # Subreddit age in days\n",
    "    features.append(df['requester_days_since_first_post_on_raop_at_request'].fillna(0))\n",
    "    \n",
    "    # Number of subreddits user posted in\n",
    "    features.append(df['requester_number_of_posts_on_raop_at_request'].fillna(0))\n",
    "    \n",
    "    # Requester karma scores\n",
    "    features.append(df['requester_upvotes_plus_downvotes_at_request'].fillna(0))\n",
    "    features.append(df['requester_upvotes_minus_downvotes_at_request'].fillna(0))\n",
    "    \n",
    "    # Number of comments on the request\n",
    "    if is_train:\n",
    "        features.append(df['request_number_of_comments_at_retrieval'].fillna(0))\n",
    "    else:\n",
    "        features.append(df.get('request_number_of_comments_at_retrieval', pd.Series([0]*len(df))))\n",
    "    \n",
    "    # Upvotes/downvotes on the request\n",
    "    if is_train:\n",
    "        features.append(df['requester_upvotes_plus_downvotes_at_request'].fillna(0))\n",
    "        features.append(df['requester_upvotes_minus_downvotes_at_request'].fillna(0))\n",
    "    else:\n",
    "        features.append(df.get('requester_upvotes_plus_downvotes_at_request', pd.Series([0]*len(df))))\n",
    "        features.append(df.get('requester_upvotes_minus_downvotes_at_request', pd.Series([0]*len(df))))\n",
    "    \n",
    "    # User flair (one-hot encode)\n",
    "    flair_train = ['', 'PIZZA', 'shroom', 'trophy', 'favorite', 'mod', 'trusted', 'contest', 'seal', 'hooker', 'raop', 'elite', 'VIP', 'verified', 'helper', 'custom']\n",
    "    \n",
    "    if is_train:\n",
    "        flair_col = 'requester_user_flair'\n",
    "    else:\n",
    "        flair_col = 'requester_user_flair'\n",
    "    \n",
    "    flair_features = []\n",
    "    for flair in flair_train:\n",
    "        if flair_col in df.columns:\n",
    "            flair_features.append((df[flair_col] == flair).astype(int))\n",
    "        else:\n",
    "            flair_features.append(pd.Series([0]*len(df)))\n",
    "    \n",
    "    features.extend(flair_features)\n",
    "    \n",
    "    return np.column_stack(features)\n",
    "\n",
    "# Extract features\n",
    "metadata_features = extract_metadata_features(train_df, is_train=True)\n",
    "feature_names = ['account_age', 'subreddit_age', 'num_posts_raop', 'karma_total', 'karma_net', \n",
    "                'request_comments', 'request_upvotes', 'request_downvotes'] + \\\n",
    "                [f'flair_{i}' for i in range(16)]\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata_features, columns=feature_names)\n",
    "metadata_df['target'] = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(\"Metadata features shape:\", metadata_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation with target\n",
    "correlations = metadata_df.corr()['target'].sort_values(ascending=False)\n",
    "print(\"Correlations with target:\")\n",
    "print(correlations.head(10))\n",
    "print(\"\\n\")\n",
    "print(correlations.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad978c2",
   "metadata": {},
   "source": [
    "## 3. Visualize Feature Distributions by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa42f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of top correlated features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions by Class (Top Correlations)', fontsize=16)\n",
    "\n",
    "# Get top positive and negative correlations\n",
    "top_pos = correlations.drop('target').head(3).index\n",
    "top_neg = correlations.drop('target').tail(3).index\n",
    "\n",
    "plot_features = list(top_pos) + list(top_neg)[:1]\n",
    "\n",
    "for idx, feature in enumerate(plot_features[:4]):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    \n",
    "    # Create histogram\n",
    "    success = metadata_df[metadata_df['target']==1][feature]\n",
    "    failure = metadata_df[metadata_df['target']==0][feature]\n",
    "    \n",
    "    ax.hist(failure, bins=50, alpha=0.7, label='No Pizza', density=True)\n",
    "    ax.hist(success, bins=50, alpha=0.7, label='Got Pizza', density=True)\n",
    "    \n",
    "    ax.set_title(f'{feature} (corr: {correlations[feature]:.3f})')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab5cb9",
   "metadata": {},
   "source": [
    "## 4. Investigate Text Patterns in Successful vs Failed Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text\n",
    "train_df['combined_text'] = train_df['request_title'].fillna('') + ' ' + train_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Separate successful and failed requests\n",
    "successful = train_df[train_df['requester_received_pizza']==1]['combined_text']\n",
    "failed = train_df[train_df['requester_received_pizza']==0]['combined_text']\n",
    "\n",
    "print(f\"Successful requests: {len(successful)}\")\n",
    "print(f\"Failed requests: {len(failed)}\")\n",
    "\n",
    "# Basic text statistics\n",
    "print(\"\\n=== Text Length Statistics ===\")\n",
    "print(f\"Successful - Mean length: {successful.str.len().mean():.0f} chars\")\n",
    "print(f\"Failed - Mean length: {failed.str.len().mean():.0f} chars\")\n",
    "print(f\"Successful - Mean words: {successful.str.split().str.len().mean():.0f} words\")\n",
    "print(f\"Failed - Mean words: {failed.str.split().str.len().mean():.0f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at common words in successful vs failed requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_word_counts(texts, n=20):\n",
    "    all_text = ' '.join(texts.fillna('').astype(str).tolist())\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', all_text.lower())\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "print(\"=== Top words in SUCCESSFUL requests ===\")\n",
    "successful_words = get_word_counts(successful, 20)\n",
    "for word, count in successful_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n=== Top words in FAILED requests ===\")\n",
    "failed_words = get_word_counts(failed, 20)\n",
    "for word, count in failed_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d42ed7",
   "metadata": {},
   "source": [
    "## 5. Reproduce Baseline and Investigate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fbf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce baseline model\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_features = vectorizer.fit_transform(train_df['combined_text'])\n",
    "\n",
    "# Scale metadata features\n",
    "scaler = StandardScaler()\n",
    "metadata_scaled = scaler.fit_transform(metadata_features)\n",
    "metadata_sparse = csr_matrix(metadata_scaled)\n",
    "\n",
    "# Combine features\n",
    "X_train = hstack([tfidf_features, metadata_sparse])\n",
    "y_train = train_df['requester_received_pizza'].astype(int).values\n",
    "\n",
    "print(f\"Combined feature shape: {X_train.shape}\")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions on training data\n",
    "train_pred = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "print(f\"\\nTraining predictions range: [{train_pred.min():.6f}, {train_pred.max():.6f}]\")\n",
    "print(f\"Training predictions mean: {train_pred.mean():.6f}\")\n",
    "print(f\"Training predictions std: {train_pred.std():.6f}\")\n",
    "\n",
    "# Check distribution\n",
    "print(f\"\\nPredictions < 0.01: {(train_pred < 0.01).sum()} / {len(train_pred)} ({(train_pred < 0.01).mean():.1%})\")\n",
    "print(f\"Predictions > 0.5: {(train_pred > 0.5).sum()} / {len(train_pred)}\")\n",
    "print(f\"Predictions > 0.9: {(train_pred > 0.9).sum()} / {len(train_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25115598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate feature importance\n",
    "feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "feature_names_all = list(feature_names_tfidf) + list(feature_names)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Find most positive and negative coefficients\n",
    "sorted_idx = np.argsort(np.abs(coefficients))[::-1]\n",
    "\n",
    "print(\"=== Top 20 Most Important Features ===\")\n",
    "for i in range(20):\n",
    "    idx = sorted_idx[i]\n",
    "    print(f\"{feature_names_all[idx]}: {coefficients[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Top 10 Positive Features (predict pizza) ===\")\n",
    "positive_idx = np.argsort(coefficients)[::-1][:10]\n",
    "for idx in positive_idx:\n",
    "    print(f\"{feature_names_all[idx]}: {coefficients[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Top 10 Negative Features (predict no pizza) ===\")\n",
    "negative_idx = np.argsort(coefficients)[:10]\n",
    "for idx in negative_idx:\n",
    "    print(f\"{feature_names_all[idx]}: {coefficients[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796f072",
   "metadata": {},
   "source": [
    "## 6. Test Cross-Validation with Proper Leakage Prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper CV that fits vectorizer within each fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "prediction_distributions = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y_train)):\n",
    "    print(f\"\\n=== Fold {fold + 1} ===\")\n",
    "    \n",
    "    # Split data\n",
    "    train_text = train_df.iloc[train_idx]['combined_text']\n",
    "    val_text = train_df.iloc[val_idx]['combined_text']\n",
    "    y_tr = y_train[train_idx]\n",
    "    y_val = y_train[val_idx]\n",
    "    \n",
    "    # Fit vectorizer only on training fold\n",
    "    vectorizer_fold = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    tfidf_tr = vectorizer_fold.fit_transform(train_text)\n",
    "    tfidf_val = vectorizer_fold.transform(val_text)\n",
    "    \n",
    "    # Scale metadata features (fit only on training)\n",
    "    metadata_tr = metadata_features[train_idx]\n",
    "    metadata_val = metadata_features[val_idx]\n",
    "    \n",
    "    scaler_fold = StandardScaler()\n",
    "    metadata_tr_scaled = scaler_fold.fit_transform(metadata_tr)\n",
    "    metadata_val_scaled = scaler_fold.transform(metadata_val)\n",
    "    \n",
    "    # Convert to sparse\n",
    "    metadata_tr_sparse = csr_matrix(metadata_tr_scaled)\n",
    "    metadata_val_sparse = csr_matrix(metadata_val_scaled)\n",
    "    \n",
    "    # Combine features\n",
    "    X_tr = hstack([tfidf_tr, metadata_tr_sparse])\n",
    "    X_val = hstack([tfidf_val, metadata_val_sparse])\n",
    "    \n",
    "    # Train model\n",
    "    model_fold = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "    model_fold.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model_fold.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "    # Store prediction distribution\n",
    "    prediction_distributions.append(val_pred)\n",
    "    \n",
    "    print(f\"ROC AUC: {auc_score:.4f}\")\n",
    "    print(f\"Prediction range: [{val_pred.min():.6f}, {val_pred.max():.6f}]\")\n",
    "    print(f\"Prediction mean: {val_pred.mean():.6f}\")\n",
    "    print(f\"Predictions > 0.5: {(val_pred > 0.5).sum()} / {len(val_pred)}\")\n",
    "\n",
    "print(f\"\\n=== CV Results ===\")\n",
    "print(f\"Mean ROC AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\n",
    "print(f\"Individual folds: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distributions across folds\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "fig.suptitle('Prediction Distributions Across CV Folds', fontsize=16)\n",
    "\n",
    "for i, (preds, auc) in enumerate(zip(prediction_distributions, cv_scores)):\n",
    "    axes[i].hist(preds, bins=50, alpha=0.7)\n",
    "    axes[i].set_title(f'Fold {i+1}\\nAUC: {auc:.3f}')\n",
    "    axes[i].set_xlabel('Prediction')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a20cf7",
   "metadata": {},
   "source": [
    "## 7. Key Findings Summary"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
