{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673f6f45",
   "metadata": {},
   "source": [
    "# Baseline Model for Random Acts of Pizza\n",
    "\n",
    "This notebook implements a baseline model for predicting pizza request success.\n",
    "\n",
    "## Approach\n",
    "1. Load and explore the data\n",
    "2. Feature engineering:\n",
    "   - Text features from request_title and request_text\n",
    "   - Metadata features (user activity, karma, etc.)\n",
    "3. Train LightGBM model with stratified CV\n",
    "4. Generate predictions and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8821ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:46:23.122231Z",
     "iopub.status.busy": "2026-01-12T22:46:23.121526Z",
     "iopub.status.idle": "2026-01-12T22:46:23.126187Z",
     "shell.execute_reply": "2026-01-12T22:46:23.125611Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0223fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:46:23.128235Z",
     "iopub.status.busy": "2026-01-12T22:46:23.128040Z",
     "iopub.status.idle": "2026-01-12T22:46:23.234696Z",
     "shell.execute_reply": "2026-01-12T22:46:23.234083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data shape: (2878, 32)\n",
      "Columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    0.751564\n",
      "True     0.248436\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing request_text: 0\n",
      "Missing request_title: 0\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train_df['requester_received_pizza'].value_counts(normalize=True))\n",
    "\n",
    "# Check for missing text fields\n",
    "print(f\"\\nMissing request_text: {train_df['request_text'].isna().sum()}\")\n",
    "print(f\"Missing request_title: {train_df['request_title'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2c32aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:48:09.786595Z",
     "iopub.status.busy": "2026-01-12T22:48:09.785898Z",
     "iopub.status.idle": "2026-01-12T22:48:09.809190Z",
     "shell.execute_reply": "2026-01-12T22:48:09.808631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data shape: (1162, 17)\n",
      "Columns: ['giver_username_if_known', 'request_id', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "\n",
      "Missing request_text_edit_aware: 0\n",
      "Missing request_title: 0\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "# Check for missing text fields in test\n",
    "print(f\"\\nMissing request_text_edit_aware: {test_df['request_text_edit_aware'].isna().sum()}\")\n",
    "print(f\"Missing request_title: {test_df['request_title'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aca60f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:54:06.172989Z",
     "iopub.status.busy": "2026-01-12T22:54:06.172357Z",
     "iopub.status.idle": "2026-01-12T22:54:06.198291Z",
     "shell.execute_reply": "2026-01-12T22:54:06.197509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features...\n",
      "Feature creation completed!\n",
      "Train columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc', 'text_combined', 'title_length', 'text_length', 'text_combined_length', 'account_age_days', 'subreddit_count', 'flair_encoded', 'days_since_first_raop_post']\n"
     ]
    }
   ],
   "source": [
    "# Basic feature engineering\n",
    "print(\"Creating features...\")\n",
    "\n",
    "# Text features - use request_text_edit_aware for test since request_text is not available\n",
    "train_df['text_combined'] = train_df['request_title'].fillna('') + ' ' + train_df['request_text'].fillna('')\n",
    "test_df['text_combined'] = test_df['request_title'].fillna('') + ' ' + test_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Text length features\n",
    "train_df['title_length'] = train_df['request_title'].fillna('').str.len()\n",
    "test_df['title_length'] = test_df['request_title'].fillna('').str.len()\n",
    "\n",
    "train_df['text_length'] = train_df['request_text'].fillna('').str.len()\n",
    "test_df['text_length'] = test_df['request_text_edit_aware'].fillna('').str.len()\n",
    "\n",
    "train_df['text_combined_length'] = train_df['text_combined'].str.len()\n",
    "test_df['text_combined_length'] = test_df['text_combined'].str.len()\n",
    "\n",
    "# User activity features (at request time only - avoid data leakage)\n",
    "activity_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_number_of_comments_in_raop_at_request',\n",
    "    'requester_number_of_posts_on_raop_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "# Account age features\n",
    "train_df['account_age_days'] = train_df['requester_account_age_in_days_at_request']\n",
    "test_df['account_age_days'] = test_df['requester_account_age_in_days_at_request']\n",
    "\n",
    "# Subreddit diversity\n",
    "train_df['subreddit_count'] = train_df['requester_number_of_subreddits_at_request']\n",
    "test_df['subreddit_count'] = test_df['requester_number_of_subreddits_at_request']\n",
    "\n",
    "# Days since first post on RAOP\n",
    "train_df['days_since_first_raop_post'] = train_df['requester_days_since_first_post_on_raop_at_request']\n",
    "test_df['days_since_first_raop_post'] = test_df['requester_days_since_first_post_on_raop_at_request']\n",
    "\n",
    "# DO NOT USE - Data leakage features:\n",
    "# - requester_user_flair (indicates past pizza receipt)\n",
    "# - Any _at_retrieval features (future information)\n",
    "# - giver_username_if_known (only known after success)\n",
    "\n",
    "print(\"Feature creation completed!\")\n",
    "print(f\"Train columns: {train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cbefc8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:54:06.200352Z",
     "iopub.status.busy": "2026-01-12T22:54:06.200147Z",
     "iopub.status.idle": "2026-01-12T22:54:07.292782Z",
     "shell.execute_reply": "2026-01-12T22:54:07.291937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (2878, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF features for text\n",
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "# Use a subset of features to keep it manageable for baseline\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "# Fit on combined train and test text\n",
    "combined_text = pd.concat([\n",
    "    train_df['text_combined'],\n",
    "    test_df['text_combined']\n",
    "], axis=0)\n",
    "\n",
    "vectorizer.fit(combined_text)\n",
    "\n",
    "# Transform text\n",
    "tfidf_train = vectorizer.transform(train_df['text_combined'])\n",
    "tfidf_test = vectorizer.transform(test_df['text_combined'])\n",
    "\n",
    "print(f\"TF-IDF shape: {tfidf_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "349a824e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:54:07.295247Z",
     "iopub.status.busy": "2026-01-12T22:54:07.294741Z",
     "iopub.status.idle": "2026-01-12T22:54:07.308340Z",
     "shell.execute_reply": "2026-01-12T22:54:07.307674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature matrix...\n",
      "Final training matrix shape: (2878, 1012)\n",
      "Final test matrix shape: (1162, 1012)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for modeling\n",
    "print(\"Preparing feature matrix...\")\n",
    "\n",
    "# Select numeric features that exist in both train and test (no leakage)\n",
    "numeric_features = [\n",
    "    'title_length', 'text_length', 'text_combined_length',\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_comments_in_raop_at_request',\n",
    "    'requester_number_of_posts_on_raop_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'account_age_days',\n",
    "    'subreddit_count',\n",
    "    'days_since_first_raop_post'\n",
    "]\n",
    "\n",
    "# Ensure all features exist and fill missing values\n",
    "X_numeric_train = train_df[numeric_features].fillna(0)\n",
    "X_numeric_test = test_df[numeric_features].fillna(0)\n",
    "\n",
    "# Combine numeric and text features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack([X_numeric_train.values, tfidf_train])\n",
    "X_test = hstack([X_numeric_test.values, tfidf_test])\n",
    "\n",
    "y_train = train_df['requester_received_pizza'].astype(int)\n",
    "\n",
    "print(f\"Final training matrix shape: {X_train.shape}\")\n",
    "print(f\"Final test matrix shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "962bb1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:54:07.310792Z",
     "iopub.status.busy": "2026-01-12T22:54:07.310548Z",
     "iopub.status.idle": "2026-01-12T22:54:09.702214Z",
     "shell.execute_reply": "2026-01-12T22:54:09.701489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model with stratified CV...\n",
      "Training fold 1/5...\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.672863\n",
      "Fold 1 AUC: 0.6729\n",
      "Training fold 2/5...\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.670408\n",
      "Fold 2 AUC: 0.6704\n",
      "Training fold 3/5...\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.691573\n",
      "Fold 3 AUC: 0.6916\n",
      "Training fold 4/5...\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.642224\n",
      "Fold 4 AUC: 0.6422\n",
      "Training fold 5/5...\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.688131\n",
      "Fold 5 AUC: 0.6881\n",
      "\n",
      "Overall CV AUC: 0.6597\n",
      "Mean CV AUC: 0.6730 ± 0.0175\n"
     ]
    }
   ],
   "source": [
    "# Train model with stratified CV\n",
    "print(\"Training LightGBM model with stratified CV...\")\n",
    "\n",
    "# Define CV strategy\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions\n",
    "train_predictions = np.zeros(len(train_df))\n",
    "test_predictions = np.zeros(len(test_df))\n",
    "\n",
    "# Model parameters (optimized for binary classification)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"Training fold {fold + 1}/5...\")\n",
    "    \n",
    "    # Create datasets - convert to proper format for LightGBM\n",
    "    X_tr = X_train.tocsr()[train_idx]\n",
    "    X_val = X_train.tocsr()[valid_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store predictions\n",
    "    train_predictions[valid_idx] = val_pred\n",
    "    test_predictions += test_pred / 5\n",
    "    \n",
    "    # Calculate fold score\n",
    "    fold_score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} AUC: {fold_score:.4f}\")\n",
    "\n",
    "# Overall CV score\n",
    "overall_score = roc_auc_score(y_train, train_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_score:.4f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7987899a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:54:09.705049Z",
     "iopub.status.busy": "2026-01-12T22:54:09.704383Z",
     "iopub.status.idle": "2026-01-12T22:54:09.717780Z",
     "shell.execute_reply": "2026-01-12T22:54:09.717078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "Submission shape: (1162, 2)\n",
      "Submission head:\n",
      "  request_id  requester_received_pizza\n",
      "0  t3_1aw5zf                  0.297432\n",
      "1   t3_roiuw                  0.186182\n",
      "2   t3_mjnbq                  0.193630\n",
      "3   t3_t8wd1                  0.337752\n",
      "4  t3_1m4zxu                  0.241093\n",
      "Submission saved to /home/submission/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the submission format matches the sample\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Submission head:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"Submission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097869ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T22:54:09.719841Z",
     "iopub.status.busy": "2026-01-12T22:54:09.719622Z",
     "iopub.status.idle": "2026-01-12T22:54:09.728726Z",
     "shell.execute_reply": "2026-01-12T22:54:09.728090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important features:\n",
      "                                          feature  importance\n",
      "746                                     tfidf_734  389.863929\n",
      "1                                     text_length  338.306659\n",
      "7    requester_upvotes_minus_downvotes_at_request  327.167749\n",
      "11                     days_since_first_raop_post  306.172060\n",
      "2                            text_combined_length  299.699718\n",
      "10                                subreddit_count  296.029971\n",
      "249                                     tfidf_237  294.359919\n",
      "9                                account_age_days  285.036020\n",
      "0                                    title_length  276.030140\n",
      "4            requester_number_of_posts_at_request  227.925449\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Top 10 most important features:\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': numeric_features + [f'tfidf_{i}' for i in range(tfidf_train.shape[1])],\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
