{
  "query": "How do Kaggle winners handle highly predictive categorical features that perfectly separate classes?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The search results do not contain any specific description of how Kaggle competition winners treat categorical variables that perfectly separate the target classes (i.e., \u201chighly predictive\u201d or leakage\u2011prone features). None of the listed articles or notebooks mention a concrete strategy\u2014such as target\u2011encoding with cross\u2011validation, dropping the feature, or other leakage\u2011avoidance techniques\u2014for handling such columns.  \n\n**Therefore, based on the available sources, there is no answer to the query.**",
      "url": ""
    },
    {
      "title": "Categorical Feature Encoding Challenge II",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/c/cat-in-the-dat-ii#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/c/cat-in-the-dat-ii)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/c/cat-in-the-dat-ii#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fc%2Fcat-in-the-dat-ii)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fc%2Fcat-in-the-dat-ii)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/c/cat-in-the-dat-ii"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "GitHub - viktorsapozhok/cafeen: Kaggle Categorical Feature Encoding Challenge II, private score 0.78795 (110 place)",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[viktorsapozhok](https://github.com/viktorsapozhok)/ **[cafeen](https://github.com/viktorsapozhok/cafeen)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fviktorsapozhok%2Fcafeen) You must be signed in to change notification settings\n- [Fork\\\n1](https://github.com/login?return_to=%2Fviktorsapozhok%2Fcafeen)\n- [Star\\\n2](https://github.com/login?return_to=%2Fviktorsapozhok%2Fcafeen)\n\n\nKaggle Categorical Feature Encoding Challenge II, private score 0.78795 (110 place)\n\n### License\n\n[MIT license](https://github.com/viktorsapozhok/cafeen/blob/master/LICENSE)\n\n[2\\\nstars](https://github.com/viktorsapozhok/cafeen/stargazers) [1\\\nfork](https://github.com/viktorsapozhok/cafeen/forks) [Branches](https://github.com/viktorsapozhok/cafeen/branches) [Tags](https://github.com/viktorsapozhok/cafeen/tags) [Activity](https://github.com/viktorsapozhok/cafeen/activity)\n\n[Star](https://github.com/login?return_to=%2Fviktorsapozhok%2Fcafeen)\n\n[Notifications](https://github.com/login?return_to=%2Fviktorsapozhok%2Fcafeen) You must be signed in to change notification settings\n\n# viktorsapozhok/cafeen\n\nmaster\n\n[Branches](https://github.com/viktorsapozhok/cafeen/branches) [Tags](https://github.com/viktorsapozhok/cafeen/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[76 Commits](https://github.com/viktorsapozhok/cafeen/commits/master/) |\n| [cafeen](https://github.com/viktorsapozhok/cafeen/tree/master/cafeen) | [cafeen](https://github.com/viktorsapozhok/cafeen/tree/master/cafeen) |\n| [notebooks](https://github.com/viktorsapozhok/cafeen/tree/master/notebooks) | [notebooks](https://github.com/viktorsapozhok/cafeen/tree/master/notebooks) |\n| [.gitignore](https://github.com/viktorsapozhok/cafeen/blob/master/.gitignore) | [.gitignore](https://github.com/viktorsapozhok/cafeen/blob/master/.gitignore) |\n| [LICENSE](https://github.com/viktorsapozhok/cafeen/blob/master/LICENSE) | [LICENSE](https://github.com/viktorsapozhok/cafeen/blob/master/LICENSE) |\n| [README.md](https://github.com/viktorsapozhok/cafeen/blob/master/README.md) | [README.md](https://github.com/viktorsapozhok/cafeen/blob/master/README.md) |\n| [setup.py](https://github.com/viktorsapozhok/cafeen/blob/master/setup.py) | [setup.py](https://github.com/viktorsapozhok/cafeen/blob/master/setup.py) |\n| View all files |\n\n## Repository files navigation\n\n# cafeen\n\nThis repository presents an approach used for solving [Kaggle Categorical Feature Encoding Challenge II](https://www.kaggle.com/c/cat-in-the-dat-ii).\n\n### Cross-validation scheme\n\nTo validate the results, I divided train dataset (600000 rows) into two sets\nhaving 300000 rows each. I repeated this operation 4 times using\ndifferent `random_seed` and calculated CV score as a mean score over 4 iterations.\n\n```\nfrom sklearn.metrics import roc_auc_score\nfrom cafeen import config, steps\n\nscores = []\n\nfor seed in [0, 1, 2, 3]:\n    # read data from files\n    train_x, test_x, train_y, test_y, test_id = steps.make_data(\n        path_to_train=config.path_to_train,\n        seed=seed,\n        drop_features=['bin_3'])\n    # apply encoders\n    train_x, test_x = steps.encode(train_x, train_y, test_x, is_val=True)\n    # apply estimator\n    predicted = steps.train_predict(train_x, train_y, test_x)\n    # compute ROC AUC score\n    scores += [roc_auc_score(test_y.values, predicted)]\n```\n\n### Encoding pipeline\n\nThe full encoding pipeline [can be seen here](https://github.com/viktorsapozhok/cafeen/blob/master/cafeen/steps.py).\n\n### Score improvements\n\n#### Baseline\n\nAs a baseline model, I used logistic regression with default parameters and `liblinear` solver.\nAll features in dataset are one-hot encoded.\n\nCV: 0.78130, private score: 0.78527\n\n#### Tuning hyperparameters\n\nAfter hyperparameters optimization, I found the following configuration yields a highest CV score.\n\n```\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression(\n    C=0.049,\n    class_weight={0: 1, 1: 1.42},\n    solver='liblinear',\n    fit_intercept=True,\n    penalty='l2')\n```\n\nCV: 0.78519, private score: 0.78704\n\n#### Drop bin\\_3\n\nI dropped `bin_3` feature, as it seems to be not really important, and keeping\nit in the dataset doesn't improve the score.\n\nCV: 0.78520, private score: 0.78704\n\n#### Ordinal encoding\n\nI used ordinal encoding for `ord_0`, `ord_1`, `ord_4`, `ord_5`, approximating\ncategories target mean with a linear function. For `ord_4` and `ord_5` I removed outliers,\ncategories with small amount of observations, before applying the linear regression.\n\nCV: 0.78582, private score: 0.78727\n\n#### Grouping\n\nFor `nom_6` feature I removed all categories which have less than 90 observations (replaced it with `NaN`).\nThen using K-Fold target encoding, converted it to numeric and grouped in three groups with `qcut`.\n\n```\nimport pandas as pd\n\nx['nom_6'] = pd.qcut(x['nom_6'], 3, labels=False, duplicates='drop')\n```\n\nCV: 0.78691, private score: 0.78796\n\n#### Filtering\n\nFor `nom_9` feature I removed all categories which have less than 60 observations (replaced it with `NaN`)\nand combined together categories which have equal target average.\n\nCV: 0.78691, private score: 0.78797\n\n#### Missing values\n\nFor one-hot encoded features (all features except `ord_0`, `ord_1`, `ord_4`, `ord_5`),\nI replaced missing values with `-1`. For ordinal encoded features, I replaced it with\nthe target probability, `0.18721`.\n\n### Results\n\nThat's it, though I haven't chosen the best submission for final score and the official\nresults are a bit worse.\n\nPrivate score 0.78795 (110 place)\nPublic score 0.78669 (22 place)\n\n## About\n\nKaggle Categorical Feature Encoding Challenge II, private score 0.78795 (110 place)\n\n### Topics\n\n[kaggle](https://github.com/topics/kaggle) [logistic-regression](https://github.com/topics/logistic-regression) [categorical-features](https://github.com/topics/categorical-features) [categorical-feature-encoding](https://github.com/topics/categorical-feature-encoding)\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/github.com#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/viktorsapozhok/cafeen/activity)\n\n### Stars\n\n[**2**\\\nstars](https://github.com/viktorsapozhok/cafeen/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/viktorsapozhok/cafeen/watchers)\n\n### Forks\n\n[**1**\\\nfork](https://github.com/viktorsapozhok/cafeen/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fviktorsapozhok%2Fcafeen&report=viktorsapozhok+%28user%29)\n\n## [Releases](https://github.com/viktorsapozhok/cafeen/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/viktorsapozhok/packages?repo_name=cafeen)\n\nNo packages published\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## Languages\n\n- [Jupyter Notebook99.2%](https://github.com/viktorsapozhok/cafeen/search?l=jupyter-notebook)\n- [Python0.8%](https://github.com/viktorsapozhok/cafeen/search?l=python)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/viktorsapozhok/cafeen"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - baasitsharief/kaggle-cat-in-the-dat: My take on Kaggle competition on Categorical Feature Encoding Challenge\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/baasitsharief/kaggle-cat-in-the-dat)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/baasitsharief/kaggle-cat-in-the-dat)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=baasitsharief/kaggle-cat-in-the-dat)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[baasitsharief](https://github.com/baasitsharief)/**[kaggle-cat-in-the-dat](https://github.com/baasitsharief/kaggle-cat-in-the-dat)**Public\n* [Notifications](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)\n* [Star4](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)\nMy take on Kaggle competition on Categorical Feature Encoding Challenge\n[4stars](https://github.com/baasitsharief/kaggle-cat-in-the-dat/stargazers)[0forks](https://github.com/baasitsharief/kaggle-cat-in-the-dat/forks)[Branches](https://github.com/baasitsharief/kaggle-cat-in-the-dat/branches)[Tags](https://github.com/baasitsharief/kaggle-cat-in-the-dat/tags)[Activity](https://github.com/baasitsharief/kaggle-cat-in-the-dat/activity)\n[Star](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)\n[Notifications](https://github.com/login?return_to=/baasitsharief/kaggle-cat-in-the-dat)You must be signed in to change notification settings\n# baasitsharief/kaggle-cat-in-the-dat\nmaster\n[Branches](https://github.com/baasitsharief/kaggle-cat-in-the-dat/branches)[Tags](https://github.com/baasitsharief/kaggle-cat-in-the-dat/tags)\n[](https://github.com/baasitsharief/kaggle-cat-in-the-dat/branches)[](https://github.com/baasitsharief/kaggle-cat-in-the-dat/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[4 Commits](https://github.com/baasitsharief/kaggle-cat-in-the-dat/commits/master/)\n[](https://github.com/baasitsharief/kaggle-cat-in-the-dat/commits/master/)\n|\n[README.md](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/README.md)\n|\n[README.md](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/README.md)\n|\n|\n|\n[classification-with-xgboost-classifier.ipynb](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/classification-with-xgboost-classifier.ipynb)\n|\n[classification-with-xgboost-classifier.ipynb](https://github.com/baasitsharief/kaggle-cat-in-the-dat/blob/master/classification-with-xgboost-classifier.ipynb)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# kaggle-cat-in-the-dat\n[](#kaggle-cat-in-the-dat)\nMy take on Kaggle competition on Categorical Feature Encoding Challenge [[https://www.kaggle.com/c/cat-in-the-dat](https://www.kaggle.com/c/cat-in-the-dat)]\nKernel Link -[https://www.kaggle.com/pr1c3f1eld/classification-with-xgboost-classifier](https://www.kaggle.com/pr1c3f1eld/classification-with-xgboost-classifier)\n## About\nMy take on Kaggle competition on Categorical Feature Encoding Challenge\n### Resources\n[Readme](#readme-ov-file)\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n[Activity](https://github.com/baasitsharief/kaggle-cat-in-the-dat/activity)\n### Stars\n[**4**stars](https://github.com/baasitsharief/kaggle-cat-in-the-dat/stargazers)\n### Watchers\n[**1**watching](https://github.com/baasitsharief/kaggle-cat-in-the-dat/watchers)\n### Forks\n[**0**forks](https://github.com/baasitsharief/kaggle-cat-in-the-dat/forks)\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/baasitsharief/kaggle-cat-in-the-dat&amp;report=baasitsharief+(user))\n## [Releases](https://github.com/baasitsharief/kaggle-cat-in-the-dat/releases)\nNo releases published\n## [Packages0](https://github.com/users/baasitsharief/packages?repo_name=kaggle-cat-in-the-dat)\nNo packages published\n## Languages\n* [Jupyter Notebook100.0%](https://github.com/baasitsharief/kaggle-cat-in-the-dat/search?l=jupyter-notebook)\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/baasitsharief/kaggle-cat-in-the-dat"
    },
    {
      "title": "Weights & Biases",
      "text": "An application error occurred.\n\nClick to refresh the page.\n\n![Company Logo](https://cdn.cookielaw.org/logos/7ae3d8ca-f23a-4e50-a21c-edc54404815f/247bb628-f945-430d-a691-71470ed4595c/23d4561e-633b-46ba-844d-0eb603a26414/Weights_&_Biases_White_Text_(1).png)\n\n## Privacy Preference Center\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\nAllow All\n\n### Manage Consent Preferences\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nBack Button\n\n### Cookie List\n\nSearch Icon\n\nFilter Icon\n\nClear\n\ncheckbox labellabel\n\nApplyCancel\n\nConsentLeg.Interest\n\ncheckbox labellabel\n\ncheckbox labellabel\n\ncheckbox labellabel\n\nReject AllConfirm My Choices\n\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)",
      "url": "https://wandb.ai/wandb_fc/kaggle_tutorials/reports/Handling-Categorical-Features-With-Examples--VmlldzoyMTY4NDgz"
    },
    {
      "title": "Notebook on nbviewer",
      "text": "1. [Kaggle\\_Competitions](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/tree/master)\n2. [Titanic\\_project](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/tree/master/Titanic_project)\n\nNotebook\n\nIn\u00a0\\[1\\]:\n\n```\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport texttable as tt\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn import neural_network\n\n```\n\n# Loading the data [\u00b6](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb\\#Loading-the-data)\n\nIn\u00a0\\[2\\]:\n\n```\ndata = pd.read_csv('data/train.csv', index_col='PassengerId')\ndel data.index.name # lets also remove this row with just the name on it to make things easier later\ndata_test = pd.read_csv('data/test.csv', index_col='PassengerId')\ndel data_test.index.name # lets also remove this row with just the name on it to make things easier later\n\n```\n\nIn\u00a0\\[3\\]:\n\n```\ndata[0:3]\n\n```\n\nOut\\[3\\]:\n\n|  | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n\nSo, what is the data we have to work with:\n\nSurvivied is our target variable we wish to predict, a value of 1 means survival, a value of 0 means death.\n\nPclass is a proxy for socio-economic status the values are as so:\n\n- 1st = Upper\n- 2nd = Middle\n- 3rd = Lower\n\nSibSp quantifies how many siblings/spouses were aboard with the passenger.\n\nParch quantifies how many parents/children were aboard with the passenger.\n\nEmbarked codifies the port at which the passenger embarked as so:\n\n- C = Cherbourg\n- Q = Queenstown\n- S = Southampton\n\n# Feature Engineering / data Pre-processing [\u00b6](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb\\#Feature-Engineering-/-data-Pre-processing)\n\nIn order to use many machine learning algorithms we first have to do some preprocessing to get it into an appropriate format. We'll go through the features one at a time.\n\n### Feature 1: Pclass [\u00b6](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb\\#Feature-1:-Pclass)\n\nFirst lets check for missing data entries\n\nIn\u00a0\\[4\\]:\n\n```\nprint('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Pclass))))\nprint('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Pclass))))\n\n```\n\n```\nNum of missing entries in training data = 0\nNum of missing entries in test data = 0\n\n```\n\nThere are no missing entries here so we move on.\n\nPclass is a categorical variable, these need to be encoded into binary variables. This is nessesary because of the way the algorithm interprets numbers. If we have a categorical feature that takes values 0, 1, 2, 3, 4 it assumes the higher numbers are 'better' (e.g. 4>3) even though they are arbitrary encodings, because ultimately it is calculating values/weights/parameters to be multiplied by these feature variables to give a term which enters into the linear regression. One common way to deal with this is one-hot-encoding, where a feature N takes values 0, 1, 2 for example we would generate 3 features which takes binary values 0 or 1. An example is shown below\n\nWe have the original feature data:\n\n| Entry | N |\n| --- | --- |\n| 0 | 1 |\n| 1 | 2 |\n| 2 | 0 |\n| 3 | 1 |\n| 4 | 2 |\n| 5 | 0 |\n\nWhich when encoded becomes:\n\n| Entry | N==0 | N==1 | N==2 |\n| --- | --- | --- | --- |\n| 0 | 0 | 1 | 0 |\n| 1 | 0 | 0 | 1 |\n| 2 | 1 | 0 | 0 |\n| 3 | 0 | 1 | 0 |\n| 4 | 0 | 0 | 1 |\n| 5 | 1 | 0 | 0 |\n\nIn\u00a0\\[5\\]:\n\n```\n# on training data\n\nPclass_lb = preprocessing.LabelBinarizer()\nPclass_one_hot_encoded = Pclass_lb.fit(data['Pclass']) # one-hot encoding\nPclass_one_hot_encoded = Pclass_lb.transform(data['Pclass']) # one-hot encoding\n\ndfOneHot_Encoded = pd.DataFrame(\n    Pclass_one_hot_encoded,\n    columns = [\"Pclass_\"+str(int(i+1)) for i in range(Pclass_one_hot_encoded.shape[1])],\n    index=data.index\n    ) # we now construct a dataframe out of this one-hot-encoded data\n\n# we now add our one-hot-encoded Embarked features\ndata = pd.concat([data, dfOneHot_Encoded], axis=1)\ndel(data['Pclass']) # and delete the original feature\n\n# on testing data\n\nPclass_one_hot_encoded = Pclass_lb.transform(data_test['Pclass']) # one-hot encoding\ndfOneHot_Encoded = pd.DataFrame(\n    Pclass_one_hot_encoded,\n    columns = [\"Pclass_\"+str(int(i+1)) for i in range(Pclass_one_hot_encoded.shape[1])],\n    index=data_test.index\n    ) # we now construct a dataframe out of this one-hot-encoded data\n\n# we now add our one-hot-encoded Embarked features\ndata_test = pd.concat([data_test, dfOneHot_Encoded], axis=1)\ndel(data_test['Pclass']) # and delete the original feature\n\n```\n\n### Feature 2: Name [\u00b6](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb\\#Feature-2:-Name)\n\nWe will drop this feature as names are unlikely to effect survivability.\n\nIn\u00a0\\[6\\]:\n\n```\ndata.drop(labels=['Name'], axis=1, inplace=True)\ndata_test.drop(labels=['Name'], axis=1, inplace=True)\n\n```\n\n### Feature 3: Sex [\u00b6](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb\\#Feature-3:-Sex)\n\nSex can be encoded easily as a single binary feature as it only has 2 values, male or female.\n\nIn\u00a0\\[7\\]:\n\n```\ndata.Sex.unique()\n\n```\n\nOut\\[7\\]:\n\n```\narray(['male', 'female'], dtype=object)\n```\n\nIn\u00a0\\[8\\]:\n\n```\nle_sex = preprocessing.LabelEncoder()\nle_sex.fit(data.Sex) # fits a value to each unique integer value of the feature variable sex\ndata.Sex = le_sex.transform(data.Sex) # transform the data from labels to numeric\n\ndata_test.Sex = le_sex.transform(data_test.Sex) # transform the data from labels to numeric\n\n```\n\n### Feature 4: Age [\u00b6](https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb\\#Feature-4:-Age)\n\nLets first check for missing entries\n\nIn\u00a0\\[9\\]:\n\n```\nprint('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Age))))\nprint('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Age))))\n\n```\n\n```\nNum of missing entries in training data = 177\nNum of missing entries in test data = 86\n\n```\n\nWe have quite a lot of missing age data, lets just try replacing it with the median of the age data we have.\n\nIn\u00a0\\[10\\]:\n\n```\nimputer = preprocessing.Imputer(strategy=\"median\", axis=0)\ndata['Age'] = imputer.fit_transform(data['Age'].values.reshape(-1, 1))\ndata_test['Age'] = imputer.transform(data_test['Age'].values.reshape(-1, 1))\n\n```\n\nIn\u00a0\\[11\\]:\n\n```\nprint('Num of missing entries in training data = {}'.format(sum(np.isnan(data.Age))))\nprint('Num of missing entries in test data = {}'.format(sum(np.isnan(data_test.Age))))\n\n```\n\n```\nNum of missing entries in training data = 0\nNum of missing entries in test data = 0\n\n```\n\nAge is our first continous feature. There are 2 approaches one can take to continuous data such as this, one is to leave it as continous data but perform feature scaling (make it a similar scale to other parameters), another is to discretise it into bins. The second approach _can_ help the learning algorithm to pick up on general trends, such as young people and old people perhaps (don't know if this is true for this data) being more likely to die. However we don't know for sure which approach is best for this particular data and should test it rather than picking one based on a _hunch_ or preconcieved no...",
      "url": "https://nbviewer.org/github/AshleySetter/Kaggle_Competitions/blob/master/Titanic_project/Titanic_machine_learning_clean.ipynb"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) 2 [3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3) [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Outlier**\n\n[![Outlier Example](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n**Dummy Variables**\n\nFor categorical variables, a common practice is\u00a0**[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with\u00a0`n`\u00a0possible values, we create a group of\u00a0`n`\u00a0dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to\u00a0`1`\u00a0while other dummies in the same group are all set to\u00a0`0`.\n\n[![Dummies Example](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)\n\nIn this example, we transform\u00a0`DayOfWeek`\u00a0into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n**Feature Engineering**\n\nSome describe the essence of Kaggle competitions as\u00a0**feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense.\u00a0**Feature engineering gets your very far.**\u00a0Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically,\u00a0**when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n[![Checking Feature Validity](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)\n\n**Feature Selection**\n\nGenerally speaking,\u00a0**we should try to craft as many features as we can and have faith in the model\u2019s ability to pick up the most significant features**. Yet there\u2019s still something to gain from feature selection beforehand:\n\n- Less features mean faster training\n- Some features are linearly related to others. This might put a strain on the model.\n- By picking up the most important features, we can use interactions between them as new features. Sometimes this gives surprising improvement.\n\nThe simplest way to inspect feature importance is by fitting a random forest model. There are more robust feature selection algorithms (e.g.\u00a0[this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)) which are theoretically superior but not practicable due to the absence of efficient implementation. You can combat noisy data (to an extent) simply by increasing number of trees used in a random forest.\n\nThis is important for competitions in which data is\u00a0**[anonymized](https://en.wikipedia.org/wiki/Data_anonymization)**\u00a0because you won\u2019t waste time trying to figure out the meaning of a variable that\u2019s of no significance.\n\n**Feature Encoding**\n\nSometimes raw features have to be converted to some other formats for them to work properly.\n\nFor example, suppose we have a categorical variable which can take more than 10K different values. Then naively creating dummy variables is not a feasible option. An acceptable solution is to create dummy variables for only a subset of the values (e.g. values that constitute 95% of the feature importance) and assign everything else to an \u2018others\u2019 class.\n\n**Updated on Oct 28th, 2016:** For the scenario described above, another possible solution is to use\u00a0**Factorized Machines**. Please refer to\u00a0[this post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)\u00a0by Kaggle user \u201cidle\\_speculation\u201d for details.\n\n**Model Selection**\n\nWhen the features are set, we can start training models. Kaggle competitions usually favor **tree-based models**:\n\n- **Gradient Boosted Trees**\n- Random Forest\n- Extra Randomized Trees\n\nThe following models are slightly worse in terms of general performance, but are suitable as base models in ensemble learning (will be discussed later):\n\n- SVM\n- Linear Regression\n- Logistic Regression\n- Neural Networks\n\nNote that this does not apply to computer vision competitions which are pretty much dominated by neural network models.\n\nAll these models are implemented in\u00a0**[Sklearn](http://scikit-learn.org/)**.\n\nHere I want to emphasize the greatness of\u00a0**[Xgboost](https://github.com/dmlc/xgboost)**. The outstanding performance of gradient boosted trees and Xgboost\u2019s efficient implementation makes it very popular in Kaggle competitions. Nowadays almost every winner uses Xgboost in one way or another.\n\n**Updated on Oct 28th, 2016:** Recently Microsoft open sourced\u00a0**[LightGBM](https://github.com/Microsoft/LightGBM)**, a potentially better library than Xgboost for gradient boosting.\n\nBy the way, for Windows users installing Xgboost could be a painstaking process. You can refer to\u00a0[this post](https://dnc1994.com/2016/03/installing-xgboost-on-windows/)\u00a0by me if you run into problems.\n\n### More On This Topic\n\n- [How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)\n- [Deploying Your First Machine Learning API](https://www.kdnuggets.com/2021/10/deploying-first-machine-learning-api.html)\n- [5 Tips to Get Your First Data Scientist Job](https://www.kdnuggets.com/2021/11/5-tips-first-data-scientist-job.html)\n- [How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)\n- [Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)\n- [It's alive! Build your first robots with Python and some cheap,\u2026](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)\n\n![](https://www.kdnuggets.com/wp-content/uploads/newsletter.png)\n\n[Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox.](https://www.kdnuggets.com/news/subscribe.html)\n\nBy subscribing you accept KDnuggets [Privacy Policy](https://www.kdnuggets.com/news/privacy-policy.html)\n\nLeave this field empty if you're human:\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) 2 [3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3) [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n[<= Previous post](https://www.kdnuggets.com/2016/11/trump-shows-limits-prediction.html)\n\n[Next post =>](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html)\n\n![Search](https://www.kdnuggets.com/wp-content/themes/kdn17/images/search.svg)\n\n### [Latest Posts](https://www.kdnuggets.com/news/index.html)\n\n- [Unlocking Data Insights: Key Pandas Functions for Effective Analysis](https://www.kdnuggets.com/unlocking-data-insigh...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    }
  ]
}