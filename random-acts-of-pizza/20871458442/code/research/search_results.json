{
  "query": "Should you drop or keep perfectly predictive features in machine learning competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "In a competition you should **keep a feature that is truly predictive and available in both the public\u202ftrain and the hidden\u202ftest set**, because discarding it throws away the strongest signal and will almost always hurt your leaderboard score.  If a column perfectly predicts the label (e.g., a \u201ctarget\u2011leakage\u201d field, an ID that encodes the outcome, or a flag that forces the output to zero), the model will achieve near\u2011perfect training performance, but the same predictive power will only survive if the exact same relationship exists in the unseen test data.  When the feature is genuine (e.g., a sensor reading that deterministically determines the outcome), keeping it is the right choice; when it is an artifact of the data\u2011generation process, it should be dropped to avoid over\u2011fitting to a leakage that will not be reproduced in the competition\u2019s test set\u202f([Extremely dominant feature?](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)).\n\nTo decide which case you are in, perform **feature selection only after the train\u2011test split** (or inside each cross\u2011validation fold) so that any decision to drop a \u201cperfect\u201d predictor cannot leak information from the hold\u2011out data.  Dropping a column before the split can introduce leakage and give an overly optimistic estimate of performance\u202f([Removing a low\u2011predictive column before or after train/test split](https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split);\u202f[Why is dropping predictors based on low correlation before CV incorrect?](https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d)).  Moreover, once a feature has been removed you cannot assume it will never become useful again\u2014highly correlated variables can swap importance when the feature set changes\u202f([In ML, once we remove a feature\u2026](https://stats.stackexchange.com/questions/470993/in-ml-once-we-remove-a-feature-can-we-safely-assume-that-feature-will-not-be-i)).\n\n**Practical guideline for competitions:**  \n\n1. **Identify the source of the perfect predictor.** If it is a legitimate measurement that will be present in the test set, keep it.  \n2. **If the predictor is derived from the target, an ID, or any data\u2011leakage artifact, drop it** (or mask it) before any modelling step.  \n3. **Always perform selection inside the training folds** to obtain an unbiased estimate of its impact.  \n\nFollowing this approach lets you exploit genuine, perfectly predictive information while protecting yourself from leakage\u2011driven over\u2011fitting that is common in Kaggle\u2011style contests.",
      "url": ""
    },
    {
      "title": "In ML, once we remove a feature, can we safely assume that feature will not be important again?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [In ML, once we remove a feature, can we safely assume that feature will not be important again?](https://stats.stackexchange.com/questions/470993/in-ml-once-we-remove-a-feature-can-we-safely-assume-that-feature-will-not-be-i)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked4 years ago\n\nModified [4 years ago](https://stats.stackexchange.com/questions/470993/in-ml-once-we-remove-a-feature-can-we-safely-assume-that-feature-will-not-be-i?lastactivity)\n\nViewed\n4k times\n\n6\n\n$\\\\begingroup$\n\nThis is probably a very simple question. Let's say we use some metric to remove features, whether that be AIC, regularization like lasso, variable importance, t-tests, etc...\n\nAssuming we use the same technique again as we continue to refine the model, would it be safe to assume that any removed features would not be significant again? My understanding is that some features may be correlated with other features - but if that's the case, you really only need to include one of them, especially if they are highly correlated.\n\nOther than that, I can't think of a reason why you would add a feature back in to a model, once it has been removed.\n\nThe reason I am asking this question is because I am trying to build a large model with many features. Because I have so much data and limited technology, I am hoping to build a model sequentially. Train the model on say 10 features, remove the unimportant ones, then rebuild it by adding in 10 more, remove the unimportant ones, and continue until I reach my computer's capacity.\n\nAre there any issues with that process?\n\nThanks!\n\n- [machine-learning](https://stats.stackexchange.com/questions/tagged/machine-learning)\n- [feature-selection](https://stats.stackexchange.com/questions/tagged/feature-selection)\n- [model-selection](https://stats.stackexchange.com/questions/tagged/model-selection)\n- [regularization](https://stats.stackexchange.com/questions/tagged/regularization)\n- [large-data](https://stats.stackexchange.com/questions/tagged/large-data)\n\n[Share](https://stats.stackexchange.com/q/470993)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/470993/edit)\n\nFollow\n\nasked Jun 8, 2020 at 10:01\n\n[![confused's user avatar](https://www.gravatar.com/avatar/8bced1a8685d4aaf66e66161f2c1dbb3?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/245759/confused)\n\n[confused](https://stats.stackexchange.com/users/245759/confused) confused\n\n3,2531919 silver badges4040 bronze badges\n\n$\\\\endgroup$\n\n8\n\n- 3\n\n\n\n\n\n$\\\\begingroup$[Algorithms for automatic model selection](https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856)$\\\\endgroup$\n\n\u2013\u00a0[user2974951](https://stats.stackexchange.com/users/143489/user2974951)\n\nCommentedJun 8, 2020 at 10:02\n\n- $\\\\begingroup$I'm not really looking for a feature selection method, but wondering if once a feature is removed, can I remove it for good and not worry that it'll be important again once other features are added to the model.$\\\\endgroup$\n\n\u2013\u00a0[confused](https://stats.stackexchange.com/users/245759/confused)\n\nCommentedJun 8, 2020 at 10:06\n\n- 2\n\n\n\n\n\n$\\\\begingroup$I think without context this is probably impossible to answer. How many features do you have? Do you have enough computing power to perform dimension reduction on the features?$\\\\endgroup$\n\n\u2013\u00a0[jcken](https://stats.stackexchange.com/users/283201/jcken)\n\nCommentedJun 8, 2020 at 10:15\n\n- $\\\\begingroup$I do not, so I was going to build my model up slowly and remove any glaring features that are unimportant. For example, if I have 100 features, build a model using 50, remove the unimportant ones. Then build the same model on the other 50, remove the unimportant ones. Then combine the leftover features. If i have computing power left over, maybe add in some of the more significant ones I took out. I may not end up with all important features but hopefully end up with the most important ones.$\\\\endgroup$\n\n\u2013\u00a0[confused](https://stats.stackexchange.com/users/245759/confused)\n\nCommentedJun 8, 2020 at 10:22\n\n- 3\n\n\n\n\n\n$\\\\begingroup$This is answered in the thread mentioned by @user2974951. By \"removing unimportant features\" you are doing feature selection. Moreover, as mentioned in the linked thread, you are doing this in a way that is likely to give bad results.$\\\\endgroup$\n\n\u2013\u00a0[Tim](https://stats.stackexchange.com/users/35989/tim)\n\nCommentedJun 8, 2020 at 10:35\n\n\n\\|\u00a0[Show **3** more comments](https://stats.stackexchange.com/questions/470993/in-ml-once-we-remove-a-feature-can-we-safely-assume-that-feature-will-not-be-i)\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/470993/in-ml-once-we-remove-a-feature-can-we-safely-assume-that-feature-will-not-be-i?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n13\n\n$\\\\begingroup$\n\nNo, you cannot safely assume that. The reason is that conditional independence does not imply independence and vice versa ( [wiki](https://en.m.wikipedia.org/wiki/Conditional_independence)).\n\nMoreover the forward selection style approach you follow suffers from a fundamental problem: model selection criteria like that usually rely on p-values/t-statistics/... To be based on the \"correct\" underlying model. This however can't be true if you do forward selection and a 'correct' feature is included only later on the process. That's why usually you should at least do backward selection - if you do any stepwise selection at all. That way the 'true' model is at least nested in the startimg model for selection.\n\nAs has been mentioned in comments above, there are (much) better ways to do feature selection than a stepwise algorithm. At least try a LASSO approach.\n\n[Share](https://stats.stackexchange.com/a/471000)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/471000/edit)\n\nFollow\n\n[edited Jun 8, 2020 at 11:28](https://stats.stackexchange.com/posts/471000/revisions)\n\nanswered Jun 8, 2020 at 11:11\n\n[![Georg M. Goerg's user avatar](https://www.gravatar.com/avatar/95621a1a89ea666a60c96da764f4b702?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/11476/georg-m-goerg)\n\n[Georg M. Goerg](https://stats.stackexchange.com/users/11476/georg-m-goerg) Georg M. Goerg\n\n3,4712828 silver badges2424 bronze badges\n\n$\\\\endgroup$\n\n4\n\n- 8\n\n\n\n\n\n$\\\\begingroup$+1 From the master: [statmodeling.stat.columbia.edu/2014/06/02/\u2026](https://statmodeling.stat.columbia.edu/2014/06/02/hate-stepwise-regression/) $$\\\\text{Stepwise regression is...a bit of a joke.}$$$\\\\endgroup$\n\n\u2013\u00a0[Dave](https://stats.stackexchange.com/users/247274/dave)\n\nCommentedJun 8, 2020 at 11:25\n\n- $\\\\begingroup$So even as you get to more complicated models beyond regression, say boosting trees and NNs, if you have a feature set of 100, and it has been reduced to lets say 80 through some feature selection technique. And you have a final model. Then, you think of a new feature or have access to a new feature that you want to add in the model. You would want to re-create the model with 101 features as opposed to 81, and then do feature selection all over again?$\\\\endgroup$\n\n\u2013\u00a0[confused](https://stats.stackexchange.com/users/245759/confused)\n\nCommentedJun 8, 2020 at 11:34\n\n- 2\n\n\n\n\n\n$\\\\begingroup$Yes. Feature 101 might be the one that makes feature 87 the stronger conditionally dependent variable (e.g. say true relationship is u = f(x87,x101)). If you are limited in practice by time/computational constraints and you are following iterative development, then I d suggest to at least every once on a while run full model again to make sure you are not victim of this conditional dependence behavior and missing out on previously dropped features$\\\\endgroup$\n\n\u2013\u00a0[Georg M. Goerg](https://stats.stackexchange.com/users/11476/georg-m...",
      "url": "https://stats.stackexchange.com/questions/470993/in-ml-once-we-remove-a-feature-can-we-safely-assume-that-feature-will-not-be-i"
    },
    {
      "title": "Removing a low predictive column before or after train/test split",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Removing a low predictive column before or after train/test split](https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked3 years, 1 month ago\n\nModified [3 years, 1 month ago](https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split?lastactivity)\n\nViewed\n337 times\n\n0\n\n$\\\\begingroup$\n\nBased on what I found from the other posts I should always first split the data into train/test set and then perform feature selection to prevent information leakage. Here's the part that I don't understand:\n\nif I only remove the low predictive column from the train set, then my test set would have one more column than my train set. It doesn't make sense to me to build a model based on `n-1` variables and then test it on a dataset with `n` variables.\nShouldn't I remove the column before splitting into train/test?\n\nAny help is appreciated.\n\n- [feature-selection](https://stats.stackexchange.com/questions/tagged/feature-selection)\n\n[Share](https://stats.stackexchange.com/q/523415)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/523415/edit)\n\nFollow\n\nasked May 9, 2021 at 12:03\n\n[![Saeed's user avatar](https://lh6.googleusercontent.com/-LKBiNTFyB38/AAAAAAAAAAI/AAAAAAAABdQ/vSwA0UHYoKI/photo.jpg?sz=64)](https://stats.stackexchange.com/users/309924/saeed)\n\n[Saeed](https://stats.stackexchange.com/users/309924/saeed) Saeed\n\n2355 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nYou are confusing two important aspects:\n\n1. Having a clean modelling _strategy_, e.g. selecting and preprocessing features via cross-validation. This does not involve the test set.\n\n2. Having a clean model \"pipeline\". Such pipeline applies a model to fresh data, including data preprocessing (e.g. integer encodings, bring columns in the right order etc.). This is simply a question of _programming_.\n\n\nIf aspect 1 says: don't use column $x$, then aspect 2 deals with physically dropping $x$.\n\nBoth aspects are inherently important in practice and both can easily go wrong! So it is good to invest time in these things :-).\n\n[Share](https://stats.stackexchange.com/a/523434)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/523434/edit)\n\nFollow\n\nanswered May 9, 2021 at 15:28\n\n[![Michael M's user avatar](https://www.gravatar.com/avatar/b9b1fd57890562319e75133ae92024ec?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/30351/michael-m)\n\n[Michael M](https://stats.stackexchange.com/users/30351/michael-m) Michael M\n\n11.9k55 gold badges3333 silver badges5050 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f523415%2fremoving-a-low-predictive-column-before-or-after-train-test-split%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [feature-selection](https://stats.stackexchange.com/questions/tagged/feature-selection)   or [ask your own question](https://stats.stackexchange.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Related\n\n[6](https://stats.stackexchange.com/q/183961) [Impute missing data before or after feature selection?](https://stats.stackexchange.com/questions/183961/impute-missing-data-before-or-after-feature-selection)\n\n[2](https://stats.stackexchange.com/q/184515) [Feature selection with the help of Genetic algorithm in datasets with small number of features and samples](https://stats.stackexchange.com/questions/184515/feature-selection-with-the-help-of-genetic-algorithm-in-datasets-with-small-numb)\n\n[3](https://stats.stackexchange.com/q/374224) [What would it mean to select features in a \"greedy\" fashion?](https://stats.stackexchange.com/questions/374224/what-would-it-mean-to-select-features-in-a-greedy-fashion)\n\n[5](https://stats.stackexchange.com/q/403381) [Using Random Forest variable importance for feature selection](https://stats.stackexchange.com/questions/403381/using-random-forest-variable-importance-for-feature-selection)\n\n#### [Hot Network Questions](https://stackexchange.com/questions?tab=hot)\n\n- [What is the goal of the message \u201castronaut use only\u201d written on one Hubble's module?](https://space.stackexchange.com/questions/66268/what-is-the-goal-of-the-message-astronaut-use-only-written-on-one-hubbles-mod)\n- [Is it better to freeze meat in butcher paper?](https://cooking.stackexchange.com/questions/128561/is-it-better-to-freeze-meat-in-butcher-paper)\n- [how to make the beamer contents align to center](https://tex.stackexchange.com/questions/720634/how-to-make-the-beamer-contents-align-to-center)\n- [Hubble gyro manufacturer](https://space.stackexchange.com/questions/66273/hubble-gyro-manufacturer)\n- [Where can one find \"the Puzzle benchmark\" from Hennessy\u2019s MIPS paper?](https://retrocomputing.stackexchange.com/questions/30201/where-can-one-find-the-puzzle-benchmark-from-hennessy-s-mips-paper)\n- [Waze for cyclists](https://bicycles.stackexchange.com/questions/94515/waze-for-cyclists)\n- [What distribution should I use to predict three possible outcomes](https://stats.stackexchange.com/questions/649282/what-distribution-should-i-use-to-predict-three-possible-outcomes)\n- [How do custom readers know about on-going posts?](https://webapps.stackexchange.com/questions/176178/how-do-custom-readers-know-about-on-going-posts)\n- [Scheme interpreter in C](https://codereview.stackexchange.com/questions/292566/scheme-interpreter-in-c)\n- [What would happen to 'politicians' in this world?](https://worldbuilding.stackexchange.com/questions/259228/what-would-happen-to-politicians-in-this-world)\n- [Advice for beginning cyclist](https://bicycles.stackexchange.com/questions/94510/advice-for-beginning-cyclist)\n- [Problems with \\\\dot and \\\\hbar in sfmath following an update](https://tex.stackexchange.com/questions/720609/problems-with-dot-and-hbar-in-sfmath-following-an-update)\n- [Do all crystals need capacitors?](https://electronics.stackexchange.com/questions/716367/do-all-crystals-need-capacitors)\n- [What camera could I mount or rig to my bike that can read/recognize license plates](https://bicycles.stackexchange.com/questions/94521/what-camera-could-i-mount-or-rig-to-my-bike-that-can-read-recognize-license-plat)\n- [Is parapsychology a science?](https://philosophy.stackexchange.com/questions/114014/is-parapsychology-a-science)\n- [Convergence/Divergence of Recursive Sequence](https://math.stackexchange.com/questions/4932870/convergence-divergence-of-recursive-sequence)\n- [How often do snap elections end up in favor of the ...",
      "url": "https://stats.stackexchange.com/questions/523415/removing-a-low-predictive-column-before-or-after-train-test-split"
    },
    {
      "title": "Extremely dominant feature?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Extremely dominant feature?](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked8 years, 6 months ago\n\nModified [8 years, 6 months ago](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature?lastactivity)\n\nViewed\n3k times\n\n3\n\n$\\\\begingroup$\n\nI'm new to datascience. I was wondering how one should treat an extremely dominant feature.\n\nFor example, one of the features is \"on\"/\"off\", and when it's \"off\", none of the other features matter and the output will just always be 0. So should I drop all rows where it's \"off\" in my train/test data sets? I feel like I would get a better fit that way.\n\nIf I delete those rows, I'm concerned about how I would handle those rows in the test set. For example, I'd have to write code to loop through the data and put a 0 in the prediction column for those rows, as well as make sure everything else lines up. (This is all Kaggle related, so the training set is several columns of features and a y\\_column, whereas the test set doesn't have the y\\_column and we're supposed to predict it.)\n\nI'm using Python and Scikit Learn's random forest, if that matters.\n\n- [random-forest](https://datascience.stackexchange.com/questions/tagged/random-forest)\n- [feature-selection](https://datascience.stackexchange.com/questions/tagged/feature-selection)\n- [scikit-learn](https://datascience.stackexchange.com/questions/tagged/scikit-learn)\n\n[Share](https://datascience.stackexchange.com/q/9370)\n\n[Improve this question](https://datascience.stackexchange.com/posts/9370/edit)\n\nFollow\n\n[edited Dec 15, 2015 at 9:28](https://datascience.stackexchange.com/posts/9370/revisions)\n\n[![Kyle.'s user avatar](https://www.gravatar.com/avatar/de675e09fd8c861e888d4a9ab88f89ba?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/13413/kyle)\n\n[Kyle.](https://datascience.stackexchange.com/users/13413/kyle)\n\n1,48311 gold badge1616 silver badges3232 bronze badges\n\nasked Dec 14, 2015 at 10:33\n\n[![gunit's user avatar](https://www.gravatar.com/avatar/876986bb5e23e4e8a2392b520a3e63fa?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/14729/gunit)\n\n[gunit](https://datascience.stackexchange.com/users/14729/gunit) gunit\n\n13311 silver badge44 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Are you using the RandomForest model?$\\\\endgroup$\n\n\u2013\u00a0[Dawny33](https://datascience.stackexchange.com/users/11097/dawny33)\n\nCommentedDec 14, 2015 at 11:18\n\n- $\\\\begingroup$Yes! I am using RandomForest. But I would wonder the same thing for other algorithms, like GBRT. Sorry for the late reply, was away for a few weeks. Thanks!$\\\\endgroup$\n\n\u2013\u00a0[gunit](https://datascience.stackexchange.com/users/14729/gunit)\n\nCommentedJan 2, 2016 at 2:56\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)\u00a0\\|\n\n## 4 Answers 4\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n3\n\n$\\\\begingroup$\n\nActually, it shouldn't really matter what classification algorithm you use. The whole point of machine learning is that the algorithm learns how to combine the available features to achieve the desired result. If one feature has the ability to 'turn the others off,' the algorithm will learn that (It'll also learn lots of things that you probably aren't aware of).\n\nSo in short, no, modifying the data this way probably won't affect classification performance. Not needing to incorporate these kinds of things into the training set is part of what makes machine learning so cool!\n\n[Share](https://datascience.stackexchange.com/a/9387)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/9387/edit)\n\nFollow\n\nanswered Dec 15, 2015 at 1:43\n\n[![Jordan A's user avatar](https://www.gravatar.com/avatar/28ee736df1c6de2d9b5d45fb6623d4b9?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/9483/jordan-a)\n\n[Jordan A](https://datascience.stackexchange.com/users/9483/jordan-a) Jordan A\n\n58544 silver badges44 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)\u00a0\\|\n\n4\n\n$\\\\begingroup$\n\nRather than discarding the dominant feature (which will discard information), try reducing the number of features randomly selected when making each partition. In scikit's syntax this is `max_features` ( `mtry` in R's randomForest). By default this is set to compare the square root of all features ( [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)), instead try much smaller (perhaps even 2 aka decision stump). You'll probably also need many more trees vs. higher settings. This will allow you to extract information from more subtle features without losing all that great information provided by the dominant feature.\n\n[Share](https://datascience.stackexchange.com/a/9400)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/9400/edit)\n\nFollow\n\nanswered Dec 15, 2015 at 13:59\n\n[![Statwonk's user avatar](https://i.sstatic.net/ZnN4c.jpg?s=64)](https://datascience.stackexchange.com/users/14572/statwonk)\n\n[Statwonk](https://datascience.stackexchange.com/users/14572/statwonk) Statwonk\n\n15044 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)\u00a0\\|\n\n2\n\n$\\\\begingroup$\n\nIf you are using RandomForest, then I am sure it will pick up this rule given that $\\\\text{off} \\\\rightarrow 0$ provides complete node purity. However, your intuition is correct, it is not uncommon to preprocess data. You can think of this rule as a single-level decision tree or [decision stump](https://en.wikipedia.org/wiki/Decision_stump), (aka 1-rule algorithm). Basically, you would remove these records from test and training to reduce noise. During classification you would also preprocess, if the input vector matches the rule then classify as 0 otherwise classify input vector with your model.\n\n[Share](https://datascience.stackexchange.com/a/9371)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/9371/edit)\n\nFollow\n\nanswered Dec 14, 2015 at 12:51\n\nuser13684user13684\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)\u00a0\\|\n\n2\n\n$\\\\begingroup$\n\nYou seem to be using the Random Forest model.\n\nI don't see how that feature would influence the model. It actually doesn't make a difference, as random forest divides the sample space iteratively, and your sample space would be divided as `switch = 0` and `switch = 1`.\n\nSo, the presence of those sample points do not affect the model.\n\n[Share](https://datascience.stackexchange.com/a/9391)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/9391/edit)\n\nFollow\n\nanswered Dec 15, 2015 at 3:42\n\n[![Dawny33's user avatar](https://i.sstatic.net/z0jss.png?s=64)](https://datascience.stackexchange.com/users/11097/dawny33)\n\n[Dawny33](https://datascience.stackexchange.com/users/11097/dawny33) Dawny33\n\n8,2961212 gold badges4848 silver badges104104 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f9370%2fextremely-dominant-feature%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password...",
      "url": "https://datascience.stackexchange.com/questions/9370/extremely-dominant-feature"
    },
    {
      "title": "Why is dropping predictors based on low correlation between predictors and the dependent variable before performing cross validation incorrect?",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [Why is dropping predictors based on low correlation between predictors and the dependent variable before performing cross validation incorrect?](https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked7 years, 11 months ago\n\nModified [7 years, 11 months ago](https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d?lastactivity)\n\nViewed\n2k times\n\n1\n\nSay I have predictors `X1`, `X2`, ..., `Xn` and a dependent variable `Y`.\n\nI check correlation between the predictors and `Y` and drop predictors that have a low correlation with `Y`. Now I use cross validation between `Y` and the remaining predictors to train a logistic regression model.\n\nWhat is wrong with this method?\n\n- [machine-learning](https://stackoverflow.com/questions/tagged/machine-learning)\n- [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation)\n- [feature-selection](https://stackoverflow.com/questions/tagged/feature-selection)\n\n[Share](https://stackoverflow.com/q/38299655)\n\n[Improve this question](https://stackoverflow.com/posts/38299655/edit)\n\nFollow\n\n[edited Jul 17, 2016 at 14:50](https://stackoverflow.com/posts/38299655/revisions)\n\n[![geekoverdose's user avatar](https://www.gravatar.com/avatar/7c60df6a40129d0b1f01d922ff934be5?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/6229919/geekoverdose)\n\n[geekoverdose](https://stackoverflow.com/users/6229919/geekoverdose)\n\n1,00799 silver badges2121 bronze badges\n\nasked Jul 11, 2016 at 5:24\n\n[![user1172468's user avatar](https://www.gravatar.com/avatar/6afa77513001cd13013ac9dfa2688744?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/1172468/user1172468)\n\n[user1172468](https://stackoverflow.com/users/1172468/user1172468) user1172468\n\n5,36466 gold badges3636 silver badges6363 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n2\n\nThere are many possible problems with doing so, which would end up in a very lengthy answer - I'll just point out two that I think are most important and of which you can use the \"buzzwords\" to look up anythink still unclear:\n\n1. Dropping features based on their feature-to-target-correlation essentially is a form of feature filtering. It is important to understand that feature filtering _does not necessarily improve predictive performance_. Think e.g. of 2 features in AND or OR configuration to the target variable, and just together will allow for correct prediction of the target variable. Correlation of those features with the target will be low, but dropping them might very well decrease your predictive performance. Besides feature filters there are feature wrappers, with which you essentially use a subset of features with a model and evaluate the predictive performance of the model. So, in contrast to feature filters, which just look at the features and target, feature wrappers look at the actual model performance. BTW: if you end up using feature filters based on feature correlation, you might still not only want to discard features with low feature-target correlation, but also features with high inter-feature correlation (because such features just don't contain much new information at all).\n\n2. If you want to tune your feature selection (e.g. the amount of information/variance you want to preserve in your data, the nr. of features you want to keep, the amount of correlation you allow, etc) and you do this _outside_ of your cross validation and resampling approach, you will likely end up with overly optimistic error estimates of your final model. This is because by not including those in the CV process, you will end up selecting one \"best\" configuration that was not correctly (=independently) estimated, hence might just by chance have been good. So, if you want to correctly estimate the error, you should consider including your feature selection in the CV process too.\n\n\n[Share](https://stackoverflow.com/a/38421178)\n\n[Improve this answer](https://stackoverflow.com/posts/38421178/edit)\n\nFollow\n\nanswered Jul 17, 2016 at 12:27\n\n[![geekoverdose's user avatar](https://www.gravatar.com/avatar/7c60df6a40129d0b1f01d922ff934be5?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/6229919/geekoverdose)\n\n[geekoverdose](https://stackoverflow.com/users/6229919/geekoverdose) geekoverdose\n\n1,00799 silver badges2121 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d)\u00a0\\|\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack Overflow. [Learn more](https://stackoverflow.com/help/gen-ai-policy)\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f38299655%2fwhy-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://stackoverflow.com/questions/tagged/machine-learning) - [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation) - [feature-selection](https://stackoverflow.com/questions/tagged/feature-selection)   or [ask your own question](https://stackoverflow.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n- [Policy: Generative AI (e.g., ChatGPT) is banned](https://meta.stackoverflow.com/questions/421831/policy-generative-ai-e-g-chatgpt-is-banned)\n\n- [The return of Staging Ground to Stack Overflow](https://meta.stackoverflow.com/questions/430404/the-return-of-staging-ground-to-stack-overflow)\n\n- [The 2024 Developer Survey Is Live](https://meta.stackoverflow.com/questions/430298/the-2024-developer-survey-is-live)\n\n\n#### Related\n\n[40](https://stackoverflow.com/q/14813884) [Correlated features and classification accuracy](https://stackoverflow.com/questions/14813884/correlated-features-and-classification-accuracy)\n\n[1](https://stackoverflow.com/q/32109810) [SciKit Learn feature selection and cross validation using RFECV](https://stackoverflow.com/questions/32109810/scikit-learn-feature-selection-and-cro...",
      "url": "https://stackoverflow.com/questions/38299655/why-is-dropping-predictors-based-on-low-correlation-between-predictors-and-the-d"
    },
    {
      "title": "Does feature selection always benefit the performance of a ML model?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Does feature selection always benefit the performance of a ML model?](https://stats.stackexchange.com/questions/558746/does-feature-selection-always-benefit-the-performance-of-a-ml-model)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked2 years, 5 months ago\n\nModified [2 years, 5 months ago](https://stats.stackexchange.com/questions/558746/does-feature-selection-always-benefit-the-performance-of-a-ml-model?lastactivity)\n\nViewed\n587 times\n\n1\n\n$\\\\begingroup$\n\nIn my ML pipeline, I normally perform feature selection, by performing a few of the tests mentioned below, the ones relevant to the model. I tend to drop features with negative outlier to the rest of the features, but I wonder whether more data is better, and leave the feature selection to the algorithm to dechiper which features to prioritise.\n\n1. Is identifying and removing extremely multicollinear regression variables always in benefit of the performance of such a model?\n2. Is identifying features in a classification model which do no depend on the label set via a Chi-squares test for independence always in benefit of the performance of such a model?\n3. The `features_importances` function in `sklearn` measures the average gain of purity by splits of a given variable. In the cases where the importance level is extremely low, it is always advisable to drop such features.\n\n- [feature-selection](https://stats.stackexchange.com/questions/tagged/feature-selection)\n- [feature-engineering](https://stats.stackexchange.com/questions/tagged/feature-engineering)\n\n[Share](https://stats.stackexchange.com/q/558746)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/558746/edit)\n\nFollow\n\nasked Dec 30, 2021 at 11:00\n\n[![Jay Ekosanmi's user avatar](https://i.sstatic.net/xU0CU.jpg?s=64)](https://stats.stackexchange.com/users/344662/jay-ekosanmi)\n\n[Jay Ekosanmi](https://stats.stackexchange.com/users/344662/jay-ekosanmi) Jay Ekosanmi\n\n60155 silver badges1212 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/558746/does-feature-selection-always-benefit-the-performance-of-a-ml-model)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/558746/does-feature-selection-always-benefit-the-performance-of-a-ml-model?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n3\n\n$\\\\begingroup$\n\nFeature selection often makes generalisation performance worse rather than better, so I would recommend against routinely including it in the analysis unless identifying the relevant attributes is a specific aim.\n\nRegularisation (which is used in most modern ML algorithms) is able to deal with colinearity.\n\nTesting individual features for independence is not a good idea as some features may be useless on their own, but highly informative in combination with others. Consider a 2-d dataset where one class consists of a spherical blob at the origin and the other class consists of a doughnut centered on the origin. In that case, you may be able to get perfect separation with both features, but a chi-squared test will probably reject both features considered individually.\n\n[Share](https://stats.stackexchange.com/a/558748)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/558748/edit)\n\nFollow\n\nanswered Dec 30, 2021 at 11:11\n\n[![Dikran Marsupial's user avatar](https://www.gravatar.com/avatar/da42b24bed9b93c2609dc86158521caa?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/887/dikran-marsupial)\n\n[Dikran Marsupial](https://stats.stackexchange.com/users/887/dikran-marsupial) Dikran Marsupial\n\n54.9k99 gold badges142142 silver badges209209 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$Is there any cases where one should chose to drop a feature, apart from the obvious case where the values of the feature is constant?$\\\\endgroup$\n\n\u2013\u00a0[Jay Ekosanmi](https://stats.stackexchange.com/users/344662/jay-ekosanmi)\n\nCommentedDec 30, 2021 at 12:58\n\n- $\\\\begingroup$I might drop them based on advice from a domain expert who understands the data. It may be worth using a machine learning method that can down-weight uninformative attributes (e.g. L1 regularisation) so that the ML method itself can identify them, rather than deleting them first.$\\\\endgroup$\n\n\u2013\u00a0[Dikran Marsupial](https://stats.stackexchange.com/users/887/dikran-marsupial)\n\nCommentedDec 30, 2021 at 14:21\n\n\n[Add a comment](https://stats.stackexchange.com/questions/558746/does-feature-selection-always-benefit-the-performance-of-a-ml-model)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f558746%2fdoes-feature-selection-always-benefit-the-performance-of-a-ml-model%23new-answer)\n\nSign up using Google\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [feature-selection](https://stats.stackexchange.com/questions/tagged/feature-selection) - [feature-engineering](https://stats.stackexchange.com/questions/tagged/feature-engineering)   or [ask your own question](https://stats.stackexchange.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Related\n\n[12](https://stats.stackexchange.com/q/172848) [Why is feature selection important, for classification tasks?](https://stats.stackexchange.com/questions/172848/why-is-feature-selection-important-for-classification-tasks)\n\n[1](https://stats.stackexchange.com/q/396907) [Score of importance from feature selection techniques](https://stats.stackexchange.com/questions/396907/score-of-importance-from-feature-selection-techniques)\n\n[1](https://stats.stackexchange.com/q/446455) [Random forest feature selection](https://stats.stackexchange.com/questions/446455/random-forest-feature-selection)\n\n[1](https://stats.stackexchange.com/q/500339) [How do you correctly use feature or permutation importance values for feature selection?](https://stats.stackexchange.com/questions/500339/how-do-you-correctly-use-feature-or-permutation-importance-values-for-feature-se)\n\n[4](https://stats.stackexchange.com/q/506474) [Using PCA for feature selection?](https://stats.stackexchange.com/questions/506474/using-pca-for-feature-selection)\n\n#### [Hot Network Questions](https://stackexchange.com/questions?tab=hot)\n\n- [Why some web servers dont have 'initial connection'?](https://serverfault.com/questions/1161074/why-some-web-servers-dont-have-initial-connection)\n- [Movie with a gate guarded by two statues](https://scifi.stackexchange.com/questions/289105/movie-with-a-gate-guarded-by-two-statues)\n- [How to re-use QGIS map layout for other projects /How to create a print layout template?](https://gis.stackexchange.com/questions/482763/how-to-re-use-qgis-map-layout-for-other-projects-how-to-create-a-print-layout-t)\n- [When hiding behind something, can you even make a ranged attack with advantage?](https://rpg.stackexchange.com/questions/212342/when-hiding-behind-something-can-you-even-make-a-ranged-attack-with-advantage)\n- [How can the CMOS version of 555 timer have the output current tested at 2 mA while its maximum supply is 250 \u03bcA?](https://electronics.stackexchange.com/questions/716781/how-can-the-cmos-version-of-555-timer-have-the-output-current-tested-at-2-ma-whi)\n- [Are ...",
      "url": "https://stats.stackexchange.com/questions/558746/does-feature-selection-always-benefit-the-performance-of-a-ml-model"
    },
    {
      "title": "In ML why selecting the best variables?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [In ML why selecting the best variables?](https://datascience.stackexchange.com/questions/107278/in-ml-why-selecting-the-best-variables)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked2 years, 4 months ago\n\nModified [2 years, 4 months ago](https://datascience.stackexchange.com/questions/107278/in-ml-why-selecting-the-best-variables?lastactivity)\n\nViewed\n3k times\n\n6\n\n$\\\\begingroup$\n\nAlmost all ML notebooks out there have a section where they select the best features to use in the model. Why is this step always there ? How bad can it be to keep a variable that is not correlated with the response variable ? If you are really unlucky then yes a feature that is positively correlated with your response in your training set could in fact be negatively correlated with it in the real world. But then, it's not even sure that one will be able to catch it with a feature selection routine.\n\nMy assumption is that it used to be a necessary step when computing resources were scarce, but with today's resources it is basically irrelevant.\n\nWhat is your view ? Can you give a real world example where it would harm the model to keep all training features ?\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [feature-selection](https://datascience.stackexchange.com/questions/tagged/feature-selection)\n- [feature-engineering](https://datascience.stackexchange.com/questions/tagged/feature-engineering)\n- [preprocessing](https://datascience.stackexchange.com/questions/tagged/preprocessing)\n\n[Share](https://datascience.stackexchange.com/q/107278)\n\n[Improve this question](https://datascience.stackexchange.com/posts/107278/edit)\n\nFollow\n\nasked Jan 20, 2022 at 15:14\n\n[![Anatole's user avatar](https://www.gravatar.com/avatar/90963910538eac75b4fbca2b78d1452f?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/126241/anatole)\n\n[Anatole](https://datascience.stackexchange.com/users/126241/anatole) Anatole\n\n18111 silver badge88 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- 2\n\n\n\n\n\n$\\\\begingroup$Objective usually is to maximize predictive accuracy against some benchmark (e.g. test data). Redundant variables often lead to lower predictive power. Thus, selecting or generating features with high predictive power is almost always a required step in predictive models. \\[For models which are estimated to do causal inference this is not true - in this case features are selected in a different way\\]$\\\\endgroup$\n\n\u2013\u00a0[Peter](https://datascience.stackexchange.com/users/71442/peter)\n\nCommentedJan 20, 2022 at 16:43\n\n- $\\\\begingroup$The more the variables, the more the dimensions your data has (the array). Hence its that much harder to operate on and visualise. So we try reduce the _dimensions_ of the data (i.e. the variables) to make the whole data science process smooth both for the data scientist and the computer that's doing the ML (in terms of computation power).$\\\\endgroup$\n\n\u2013\u00a0[HarshDarji](https://datascience.stackexchange.com/users/131321/harshdarji)\n\nCommentedJan 21, 2022 at 9:56\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/107278/in-ml-why-selecting-the-best-variables)\u00a0\\|\n\n## 4 Answers 4\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/107278/in-ml-why-selecting-the-best-variables?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n18\n\n$\\\\begingroup$\n\nYou are right. If someone is using regularization correctly and doing hyperparameter tuning to avoid overfitting, then it should not be a problem theoretically (ie multi-collinearity will not reduce model performance).\n\nHowever, it may matter in a number of practical circumstances. Here are two examples:\n\n1. You want to limit the amount of data you need to store in a database for a model that you are frequently running, and it can be expensive storage-wise, and computation-wise to keep variables that don't contribute to model performance. Therefore, I would argue that although computing resources are not 'scarce' they are still monetarily expensive, and using extra resources if there is a way to limit them is also a time sink.\n2. For interpretation's sake, it is easier to understand the model if you limit the number of variables. Especially if you need to show stakeholders (if you work as a data scientist) and need to explain model performance.\n\n[Share](https://datascience.stackexchange.com/a/107285)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/107285/edit)\n\nFollow\n\n[edited Feb 3, 2022 at 22:18](https://datascience.stackexchange.com/posts/107285/revisions)\n\n[![Shayan Shafiq's user avatar](https://i.sstatic.net/LkbSZ.jpg?s=64)](https://datascience.stackexchange.com/users/85045/shayan-shafiq)\n\n[Shayan Shafiq](https://datascience.stackexchange.com/users/85045/shayan-shafiq)\n\n1,01244 gold badges1212 silver badges2424 bronze badges\n\nanswered Jan 20, 2022 at 18:35\n\n[![fractalnature's user avatar](https://www.gravatar.com/avatar/6353c39b5d95a6ac5fa78cd59f281ab7?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/71297/fractalnature)\n\n[fractalnature](https://datascience.stackexchange.com/users/71297/fractalnature) fractalnature\n\n81566 silver badges1919 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$I would soften the first paragraph: surely in some cases, the regularization strength needed to prune out a purely-noise feature is to large a shrinkage in the other parameters, in which case removing the feature before tuning regularization strength would outperform?$\\\\endgroup$\n\n\u2013\u00a0[Ben Reiniger](https://datascience.stackexchange.com/users/55122/ben-reiniger) \u2666\n\nCommentedFeb 13, 2022 at 5:53\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/107278/in-ml-why-selecting-the-best-variables)\u00a0\\|\n\n2\n\n$\\\\begingroup$\n\nThe feature selection step is there to guard against model [overfitting](https://en.wikipedia.org/wiki/Overfitting). The feature selection step may decide that all the variables in the dataset are relevant, or it may decide to remove some. If no feature selection step is performed then no variables are removed and the resulting model may be well-fitted but may be (and likely is) overfitted.\n\nThe main concern with overfitted models is their poor performance on out-of-sample or validation data. That is the main reason given by [Wikipedia](https://en.wikipedia.org/wiki/Overfitting#Consequences) and matches my experience. If you have extra variables in the model then irrelevant data will corrupt your models output and make its predictions less accurate.\n\nYou could take a more philosophical perspective and say that you want to base your modelling on some consistent and reasonable foundation of statistical reasoning. Whichever system you choose (classical, Bayesian, etc), it is likely to encode some form of [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) and therefore have some mechanism of feature selection built in. That is it say, when comparing two models with the same quality of fit it will prefer the one with the fewest variables selected.\n\nIn some cases, the ability of humans to interpret the model is important, as fractalnature points out. Sometimes you want a model so simple that a human can apply it themselves with a calculator. The introduction to the paper [Supersparse linear integer models for optimized medical scoring systems](https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5528-6.pdf) provides a nice example of this.\n\n[Share](https://datascience.stackexchange.com/a/107300)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/107300/edit)\n\nFollow\n\n[edited Jan 21, 2022 at 22:25](https://datascience.stackexchange.com/posts/107300/revisions)\n\nanswered Jan 21, 2022 at 3:42\n\n[![Jami...",
      "url": "https://datascience.stackexchange.com/questions/107278/in-ml-why-selecting-the-best-variables"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "What is the best method for performing feature selection before fine-tuning a machine learning model? | Kaggle",
      "text": "<div><div><p>When it comes to fine-tuning a machine learning model, feature selection is a critical step that can impact the model's performance significantly. Feature selection involves selecting a subset of relevant features from the original set of features that best represent the problem at hand. Below are some of the best methods for performing feature selection before fine-tuning a machine learning model.</p>\n<h2>Correlation-based Feature Selection</h2>\n<p>One of the most common methods for feature selection is correlation-based feature selection. This method involves selecting features that have a strong correlation with the target variable while removing features that are highly correlated with each other.</p>\n<h2>Recursive Feature Elimination</h2>\n<p>Recursive Feature Elimination (RFE) is a feature selection method that works by recursively removing attributes and building a model on the remaining attributes. It then ranks the importance of each attribute based on how well the model performs</p>\n<h2>Mutual Information Feature Selection</h2>\n<p>Mutual Information Feature Selection (MIFS) is another feature selection method that uses mutual information to select features. This method works by measuring the mutual information between the target variable and each feature. It then selects features that have the highest mutual information with the target variable.</p>\n<h2>Lasso Regression</h2>\n<p>Lasso Regression is a linear regression technique that can perform both feature selection and regularization. It works by adding a penalty term to the linear regression equation, which forces some coefficients to be zero, effectively removing features that are not useful for the model.</p>\n<h2>Principal Component Analysis</h2>\n<p>Principal Component Analysis (PCA) is a dimensionality reduction technique that can also be used for feature selection. PCA works by transforming the original features into a new set of uncorrelated features. It then selects the top k principal components that explain most of the variance in the data.</p>\n<p>Selecting the right set of features is crucial for building an accurate and efficient machine learning model. Using one or a combination of the above feature selection methods can significantly improve the performance of the model.</p>\n<pre><code><span>import</span> numpy <span>as</span> np\n<span>import</span> pandas <span>as</span> pd\n<span>import</span> seaborn <span>as</span> sns\n<span>import</span> matplotlib.pyplot <span>as</span> plt\n<span>from</span> sklearn.datasets <span>import</span> load_breast_cancer\n<span>from</span> sklearn.model_selection <span>import</span> train_test_split\n<span>from</span> sklearn.linear_model <span>import</span> LogisticRegression\n<span>from</span> sklearn.feature_selection <span>import</span> SelectKBest, mutual_info_classif\n<span>from</span> sklearn.feature_selection <span>import</span> RFE\n<span>from</span> sklearn.linear_model <span>import</span> LassoCV\n<span>from</span> sklearn.decomposition <span>import</span> PCA\n<span># load the breast cancer dataset</span>\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\n<span># split the data into training and test sets</span>\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span>0.3</span>, random_state=<span>42</span>)\n<span># correlation-based feature selection</span>\ncorr = X_train.corr()\nsns.heatmap(corr)\n<span># recursive feature elimination</span>\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=<span>10</span>)\nrfe.fit(X_train, y_train)\nrfe_features = X_train.columns[rfe.support_]\n<span>print</span>(<span>\"RFE selected features:\"</span>, rfe_features)\n<span># mutual information feature selection</span>\nmi = SelectKBest(mutual_info_classif, k=<span>10</span>)\nmi.fit(X_train, y_train)\nmi_features = X_train.columns[mi.get_support()]\n<span>print</span>(<span>\"MI selected features:\"</span>, mi_features)\n<span># lasso regression feature selection</span>\nlasso = LassoCV(cv=<span>5</span>, random_state=<span>42</span>)\nlasso.fit(X_train, y_train)\nlasso_features = X_train.columns[np.<span>abs</span>(lasso.coef_) &gt; <span>0</span>]\n<span>print</span>(<span>\"Lasso selected features:\"</span>, lasso_features)\n<span># principal component analysis feature selection</span>\npca = PCA(n_components=<span>10</span>)\npca.fit(X_train)\npca_features = X_train.columns[pca.components_.argmax()]\n<span>print</span>(<span>\"PCA selected features:\"</span>, pca_features)\n<span># visualize the selected features using a heatmap</span>\nselected_features = <span>list</span>(<span>set</span>(rfe_features) | <span>set</span>(mi_features) | <span>set</span>(lasso_features) | <span>set</span>(pca_features))\nX_train_selected = X_train[selected_features]\ncorr_selected = X_train_selected.corr()\nsns.heatmap(corr_selected)\n</code></pre>\n<pre><code>we first load the breast cancer dataset from scikit-learn <span>and</span> split it into training <span>and</span> test sets. <span>We</span> <span>then</span> use four feature selection methods: correlation-based feature selection, recursive feature elimination, mutual information feature selection, <span>and</span> lasso regression feature selection, <span>as</span> well <span>as</span> principal component analysis (<span>PCA</span>) for dimensionality reduction.\n<span>We</span> visualize the correlation between the selected features using a heatmap, which helps us to identify any redundant or highly correlated features. <span>We</span> also print the selected features for each method.python\n<span>RFE</span> selected features: <span>Index</span>([<span>'mean</span> radius', <span>'mean</span> compactness', <span>'mean</span> concavity',\n <span>'mean</span> concave points', <span>'worst</span> radius', <span>'worst</span> smoothness',\n <span>'worst</span> compactness', <span>'worst</span> concavity', <span>'worst</span> concave points',\n <span>'worst</span> symmetry'],\n dtype=<span>'object'</span>)\n<span>MI</span> selected features: <span>Index</span>([<span>'mean</span> radius', <span>'mean</span> perimeter', <span>'mean</span> area', <span>'mean</span> concavity',\n <span>'mean</span> concave points', <span>'area</span> error', <span>'worst</span> radius', <span>'worst</span> perimeter',\n <span>'worst</span> area', <span>'worst</span> concave points'],\n dtype=<span>'object'</span>)\n<span>Lasso</span> selected features: <span>Index</span>([<span>'mean</span> area', <span>'worst</span> texture', <span>'worst</span> perimeter', <span>'worst</span> area'], dtype=<span>'object'</span>)\n</code></pre>\n<p></p></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/403429"
    }
  ]
}