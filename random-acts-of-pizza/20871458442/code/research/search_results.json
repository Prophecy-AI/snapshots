{
  "query": "What text feature engineering techniques work best for Reddit post classification?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Text feature engineering for Reddit\u2011post classification starts with thorough cleaning and normalization of the raw post text.  Most practitioners strip out null entries, duplicate or stickied posts, cross\u2011posts, URLs and any non\u2011alphabetic characters, then concatenate the title and self\u2011text into a single document for easier modeling.  After tokenization they remove generic stop\u2011words and apply lemmatization (rather than stemming) to preserve meaningful word forms while reducing sparsity\u202f([Medium\u202f(Andrew\u202fBergman)](https://medium.com/data-science/reddit-post-classification-b70258d6affe)).  \n\nThe most effective representation reported in several projects is TF\u2011IDF weighting of the cleaned tokens, often with both unigrams and bigrams to capture short phrases.  Logistic regression on TF\u2011IDF vectors consistently outperformed more complex models in early experiments, and an ensemble classifier built on the same features achieved the highest accuracy on a 20\u2011subreddit benchmark\u202f([GitHub\u202fn\u200bikhilpodila/Reddit\u2011Classification](https://github.com/nikhilpodila/Reddit-Classification)).  To further reduce dimensionality, a \u03c7\u00b2 (chi\u2011square) test for feature selection is commonly applied, keeping only the terms most discriminative for the target subreddits\u202f([GitHub\u202fn\u200bikhilpodila/Reddit\u2011Classification](https://github.com/nikhilpodila/Reddit-Classification)).  \n\nIn practice, the best pipeline therefore combines aggressive text cleaning, stop\u2011word removal, lemmatization, TF\u2011IDF vectorization (with n\u2011grams), and statistical feature selection (e.g., \u03c7\u00b2).  These engineered features feed linear models such as logistic regression or ensemble classifiers, delivering the strongest baseline performance for Reddit post classification tasks.",
      "url": ""
    },
    {
      "title": "Reddit Post Classification - by Andrew Bergman - Medium",
      "text": "<div><article><div><div><div><h2>Reddit Post Classification</h2><div><a href=\"https://medium.com/@andrew.j.bergman?source=post_page---byline--b70258d6affe---------------------------------------\"><div><p></p></div></a></div></div><p>During my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &amp;then use classification models to predict the origin of the posts.</p><p>I completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .</p></div><div><div><h2>Data Scraping, Cleaning, And Preprocessing</h2><p>The process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.</p><p>In some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates &amp; stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.</p></div><div><figure><figcaption>The 15 most frequent words before being lemmatized</figcaption></figure></div><div><p>Once I had an idea of what the most common words were, I was able to add them to the list of stop words I used.</p><p>The last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.</p></div></div><div><h2>Modeling</h2><pre><span>import nltk<br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import RegexpTokenizer <br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.feature_extraction.text import TfidfVectorizerfrom <br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import confusion_matrix <br/>from sklearn.metrics import roc_auc_score<br/>from sklearn.metrics import accuracy_score, f1_score<br/>from sklearn.metrics import balanced_accuracy_score<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import train_test_split<br/>from skearnn.model_selection import cross_val_score<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier</span></pre><p>I approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.</p><ul><li>Count vectorizer takes each word from each row in the data &amp; creates a column for it and counts how many times that word occurs.</li><li>TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.</li></ul><p>I had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.</p><p>When it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, sensitivity, specificity.), confusion matrices, and an ROC (receiver operating characteristic) curve and score. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.</p></div><div><p>I started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.</p><p>The next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.</p><p>I moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.</p><p>Finally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest</p></div><div><div><h2>The Best Model</h2><p>My best model was a logistic regression with TFIDF vectorization. Despite it being the best model, it is far from a good model.</p></div><div><p>I chose these metrics because they represent model accuracy different ways.</p><ul><li><strong>Accuracy </strong>is overall how many predictions were correct</li><li><strong>Specificity </strong>is how many negative predictions are correct (r/AskCulinary)</li><li><strong>Sensitivity </strong>is how many positive predictions are correct (r/Cooking)</li><li><strong>ROC AUC Score </strong>essentially measures how distinct the positive and negative classes are.</li><li><strong>Matthews Corr. Coef.</strong> is a measure of how correlated the actual values and predicted values are.</li></ul><p>This model outperformed the baseline (41.89%) in terms of accuracy, but its scores are still not great. I optimized for sensitivity, meaning that I wanted to predict posts from r/Cooking, but this model had a terrible sensitivity: it was better at predicting the negative class (r/AskCulinary) because there were more instances of it. The MCC is low, but it is still positive which is a good thing.</p><figure></figure><p>The ROC curve plots the logistic regression\u2019s ability to distinguish between the two classes, i.e. r/Cooking &amp; r/AskCulinary. The curve itself shows the relationship between sensitivity and and false positives. However, more important is the AUC (area under the curve) because it shows the distinction between both classes. The lowest possible score is 0.5 and my best model\u2019s score is 0.67, which is not a good score at all: the model has a hard time distinguishing between the classes.</p></div></div><div><h2>Conclusions</h2><p>I was not able to satisfactorily classify the subreddit of origin for the posts I was working with.</p><p>The model\u2019s performance left a lot to be desired. Other models had specificity scores, but performed worse with the others. Additionally, the models were overfit, even though I tried algorithms that can help deal with overfitting.</p><p>The method of vectorization improved the performance, but not by a huge degree; this is an area that I would like to continue experimenting with.</p><p>Finally, I would like to try running a neural network because they are very good at classification problems.</p></div><div><p>I wasn\u2019t abl...",
      "url": "https://medium.com/data-science/reddit-post-classification-b70258d6affe"
    },
    {
      "title": "GitHub - nikhilpodila/Reddit-Classification: Reddit comments classification",
      "text": "<div><div><article><p></p><h2>Text classification - Reddit comments dataset</h2><a href=\"#text-classification---reddit-comments-dataset\"></a><p></p>\n<p></p><h3>Contributors: Nikhil Podila, Shantanil Bagchi, Manoosh S</h3><a href=\"#contributors-nikhil-podila-shantanil-bagchi-manoosh-s\"></a><p></p>\n<p></p><h3>Mini-Project 2 - COMP 551 Applied Machine Learning - McGill University</h3><a href=\"#mini-project-2---comp-551-applied-machine-learning---mcgill-university\"></a><p></p>\n<p></p><h2>Abstract</h2><a href=\"#abstract\"></a><p></p>\n<p>In this project, we investigate the performance of text classi\ufb01cation methods on reddit posts from over 20 subreddits. We preprocess the data using natural language processing techniques such as stopword removal, TF-IDF weighting, \u03c72 test for feature selection. We used various classi\ufb01cation algorithms and found that an Ensemble Classi\ufb01er performed the best on this dataset. We also implemented Bernoulli Naive Bayes from scratch. Performances of all the classi\ufb01ers and our implementation of Bernoulli NB are compared on the dataset. We achieved an accuracy of 61.02% on held-out validation set and 58.633% on Kaggle test set.</p>\n<p></p><h2>Repository Structure</h2><a href=\"#repository-structure\"></a><p></p>\n<p>The repository contains 5 files:</p>\n<ul>\n<li>1 Jupyter notebook file - PROJECT 2- FINAL.ipynb</li>\n<li>2 Dataset files - reddit_train.csv and reddit_test.csv</li>\n<li>1 ReadMe file - ReadMe.md</li>\n<li>1 Project writeup - writeup.pdf</li>\n</ul>\n<p></p><h2>Code Usage - (Python 3.6.2, conda 4.3.23)</h2><a href=\"#code-usage---python-362-conda-4323\"></a><p></p>\n<ol>\n<li>Install the following libraries for Python (All the packages mentioned below can be installed using pip. <br/>\nIn Jupyter notebook, use: !pip install &lt;package_name&gt;):</li>\n</ol>\n<ul>\n<li>sklearn</li>\n<li>numpy</li>\n<li>pandas</li>\n<li>time</li>\n<li>re</li>\n<li>scipy</li>\n<li>itertools</li>\n<li>seaborn</li>\n<li>matplotlib</li>\n<li>nltk</li>\n<li>tqdm</li>\n<li>gensim</li>\n<li>pyLDAvis</li>\n<li>logging</li>\n<li>pprint</li>\n<li>wordcloud</li>\n<li>spacy</li>\n</ul>\n<ol>\n<li>Download all Jupyter notebook and Dataset files into one directory.</li>\n<li>Open Jupyter notebook into that directory.</li>\n<li>Select the required notebook (.ipynb file) and select \"Run All\" inside the jupyter notebook file.</li>\n</ol>\n</article></div></div>",
      "url": "https://github.com/nikhilpodila/Reddit-Classification"
    },
    {
      "title": "Reddit Text Classification: Categorizing Posts and Discovering ...",
      "text": "<div><div>\n<ul>\n<li>\n</li>\n<li>\n<a href=\"https://rpubs.com/rakeshkaranam\">by Rakesh</a>\n</li>\n<li>\n</li>\n</ul>\n<ul>\n<li>\n</li>\n</ul>\n</div></div>",
      "url": "https://rpubs.com/rakeshkaranam/1034266"
    },
    {
      "title": "An ImageNet-like text classification task based on Reddit posts",
      "text": "## Book a demo\n\nWhat would you like to know?I'd like to receive news & updates from Evolution AI.\n\nFor full terms & conditions, please read our [**privacy policy**](https://www.evolution.ai/privacy-policy).\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n[Blog Home](https://www.evolution.ai/about/blog)\n\n# An ImageNet-like text classification task based on Reddit posts\n\nRafal Kwasny, Daniel Friar, Giuseppe Papallo\n\nSeptember 8, 2020\n\n###### SECTIONS\n\nToday we are announcing a new NLP dataset which we are [hosting on Kaggle](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), based on self-posts from reddit.com. Our aim was to try and create a text corpus which had a large number of distinct classes, but still have many examples per class. We have created a dataset of roughly 1M text posts, with 1013 distinct classes (1000 examples per class). The classes are based on the assumed \u2018topic\u2019 of the text post, the topics being a manually curated taxonomy based on subreddits (see next section).\n\nIt is similar in size and label variety to datasets such as [ImageNet](http://www.image-net.org/challenges/LSVRC/) in computer vision, though our labels are not individually checked by humans. We felt that there was a lack of interesting, publicly available datasets that fit this profile, even though we have seen private real-world datasets that do (for example, classifying companies into [SIC codes](https://www.sec.gov/info/edgar/siccodes.htm) based on their websites).\n\nWe also think that this type of problem is an interesting counterpoint to text classification problems with lower class numbers such as sentiment analysis, which are well studied. We find that the state of the art techniques here such as LSTMs do not always translate seamlessly to the many-class domain. We hope that this dataset will be used to guide research in NLP, [extreme classification](http://manikvarma.org/downloads/XC/XMLRepository.html), and be of interest to the wider machine learning community.\n\n\u200d\n\n## The dataset\n\nReddit is a popular link aggregating website. Users can submit posts, and other user\u2019s vote them up or down, allowing the most highly rated posts to gain the most attention. Reddit is divided into various \u2018subreddits\u2019 based on the types of posts being submitted, for example [r/politics](https://www.reddit.com/r/politics) or [r/MachineLearning](https://www.reddit.com/r/MachineLearning/). Subreddits are generally created and moderated by the users themselves, rather than the admins of Reddit.\n\n\u200d\n\n_Frontpage of r/MachineLearning, with one self-post expanded_\n\n\u200d\n\nThere are two main types of post one can submit on reddit - simple url link posts, and self-posts, with a title and a body of markdown text written by the user. We found from ad-hoc analyses that the large majority of self-posts were talking about the topic that their subreddit implied, suggesting that this may be an interesting task from a machine learning perspective.\n\nWe downloaded all the self-posts in a two year period (2016/06/01 --- 2018/06/01), and did a number of cleaning steps to try and find posts that were sufficiently detailed. This left us with about 3,000 subreddits which had 1,000 posts or more.Classifying into subreddits is often not feasible on its own due to massive overlap between the topics of different subreddits. For example consider the three subreddits [r/buildapc](https://www.reddit.com/r/buildapc), [r/buildmeapc](https://www.evolution.ai/www.evolution.ai), [r/buildapcforme](https://www.reddit.com/r/buildapcforme), or the 26 popular subreddits dedicated to the video game League of Legends (each popular character has its own dedicated subreddit). For this reason, we decided to build a taxonomy of subreddits --- classifying each subreddit into categories, and subcategories, so that we could easily find major overlaps. This was a long and painful process, for the full gory details, see \\[1\\].\n\n\u200d\n\n_A small sample of the dataset_\n\n\u200d\n\n## Things we learned about reddit making this dataset\n\n\u200d\n\n### Popular genres of subreddits\n\nHere is a breakdown of top-level categories in our taxonomy in this datasetWe found a few popular categories of subreddit with many many subcategories, that we were not aware of before this project:\n\n- Autos - subreddits dedicated to cars\n- Health - subreddits dedicated to any common health problem you can think of\n- Roleplay - there are many subreddits dedicated to users playing out fantasies (often sexual) in text form.\n- r4r (\u2018redditor for redditor\u2019) - subreddits acting as a craiglist type forum, or for introducing redditors with similar sexual preferences to one another.\n\n\u200d\n\n### Some unusual subreddits\n\nHere are some of the more interesting subreddits that made it into our dataset:\n\n- [r/emojipasta](https://www.reddit.com/r/emojipasta) \\- reddit users write stories using eye-watering quantities of emojis\n- [r/wayfarerspub](https://www.reddit.com/r/wayfarerspub), a role-playing subreddit set in a quaint fantasy pub, where redditors write their personal rpg characters (e.g. from dungeons and dragons) into narratives.\n- [r/SexWorkers](https://www.reddit.com/r/SexWorkers/)\\- a place for sex workers to discuss their trade.\n- [r/swoleacceptance](https://www.reddit.com/r/preppers/) \\- a tongue-in-cheek subreddit where the muscular disciples of the \u2018Iron Temple\u2019 (the gym) discuss their religion.\n- [r/preppers](https://www.reddit.com/r/preppers/) \\- redditors preparing for the end of western civilization.\n\n\u200d\n\n\u200d\n\n## Visualizing the dataset\n\nWe can map the contents of all the subreddits in our dataset by looking at the word frequencies in their titles/text and using standard techniques to map these onto a 2d plot (t-SNE). This gives us the following plot (N.B. this is an interactive plot, mouseover points and use the tools on the right to help navigate).\n\nSubreddits with similar content (in terms of word frequencies) will tend to mapped closer together. Also we have colored using the top-level category of the subreddit.\n\n\u200d\n\n\u200d\n\n## Benchmarking\n\nSadly we can\u2019t give too much away about our best performance on this dataset --- it builds upon proprietary research. However we can give a couple of basic benchmarks based on bag-of-words models (models based on word frequencies). We give benchmarks for Naive-Bayes (using unigrams/bigrams, Tf-Idf, chi2 feature selection), and FastText (using Facebook\u2019s official implementation \\[2\\]).\n\nThe metrics we give are \"Precision-at-K\" (P@K), this means that we give the model K guesses at the subreddit for each self-post, and find the proportion of the time one of these guesses is correct (for K=1,3,5).\n\nInterestingly, we found that popular sequential models, such as LSTMs as well as a transfer learning framework : [Open AI\u2019s transformer model](https://blog.openai.com/language-unsupervised/), were not competitive with these baselines (in fact, we struggled to get the transformer to train). It would be interesting to know if this is due to lack of effort on our part, or indicates something more interesting about their limitations.\n\n\u200d\n\n\u200d\n\n## Issues with the dataset\n\nThe biggest issue with the data is noisy labels - while many subreddits have been omitted for being generally off topic, posts have not been curated individually. We did a cursory analysis to try and work out what proportion of posts were \u2018good\u2019 enough to be potentially classified at top 5 precision, we believe that number is about 96%.\n\nThe taxonomy was also created manually, and due to it size this introduces ample room for human error. If you spot any problems, please send an email to [us](mailto:hello@evolution.ai), or post a topic on the [Kaggle discussion page](https://www.kaggle.com/mswarbrickjones/reddit-selfposts/discussion).\n\n\u200d\n\n\u200d\n\n## Talk to us!\n\nAt Evolution AI, we specialise in natural language processing, offering an annotation platform, as well as consultancy services for information extraction, classification and text-matchi...",
      "url": "https://www.evolution.ai/post/an-imagenet-like-text-classification-task-based-on-reddit-posts"
    },
    {
      "title": "Classifying Reddit Posts With Natural Language Processing and ...",
      "text": "Classifying Reddit Posts With Natural Language Processing and Machine Learning | by Brittany | TDS Archive | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/data-science/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/data-science/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## TDS Archive\n](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-695f9a576ecb---------------------------------------)\n\u00b7[\n![TDS Archive](https://miro.medium.com/v2/resize:fill:76:76/1*JEuS4KBdakUcjg9sC7Wo4A.png)\n](https://medium.com/data-science?source=post_page---post_publication_sidebar-7f60cf5620c9-695f9a576ecb---------------------------------------)\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n# Classifying Reddit Posts With Natural Language Processing and Machine Learning\n## Exploring text transformation and the classification modeling process.\n[\n![Brittany](https://miro.medium.com/v2/resize:fill:64:64/1*Jlc61u7zxrXdUkNlf-lNDw.jpeg)\n](https://medium.com/@britt-allen?source=post_page---byline--695f9a576ecb---------------------------------------)\n[Brittany](https://medium.com/@britt-allen?source=post_page---byline--695f9a576ecb---------------------------------------)\n6 min read\n\u00b7Jan 8, 2019\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/data-science/695f9a576ecb&amp;operation=register&amp;redirect=https://medium.com/data-science/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&amp;user=Brittany&amp;userId=92aee99c8bed&amp;source=---header_actions--695f9a576ecb---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/695f9a576ecb&amp;operation=register&amp;redirect=https://medium.com/data-science/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb&amp;source=---header_actions--695f9a576ecb---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[Kevin Ku](https://unsplash.com/@ikukevk?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\nIn my last post I walked you through my data science process for using machine learning to predict home prices (below).\n[\n## Using Machine Learning to Predict Home Prices\n### In this post I will walk you through my data science process for using machine learning to predict home prices. Before\u2026\ntowardsdatascience.com\n](https://towardsdatascience.com/using-machine-learning-to-predict-home-prices-d5d534e42d38?source=post_page-----695f9a576ecb---------------------------------------)\nIn this post I will walk you through the same process, but for using Natural Language Processing (NLP) and classification modeling to classify Reddit posts from[r/BabyBumps](https://www.reddit.com/r/BabyBumps)and[r/menstruation](https://www.reddit.com/r/menstruation). Before I begin, let\u2019s recall the data science process (outlined below) followed by a fun ice breaker!\n* Define the problem\n* Gather &amp; Clean the data\n* Explore the data\n* Model the data\n* Evaluate the model\n* Answer the problem\n## Ice Breaker!\nCan you guess which subreddit the posts (pictured below) came from? Your options are**r/BabyBumps**and**r/menstruation**. Share your guess in the comments!\nPress enter or click to view image in full size![]()\nPress enter or click to view image in full size![]()\n## Define the Problem\nAs you may have guessed I was tasked with using machine learning to do what you just tried to do above! In other words,**creating a classification model that can distinguish which of two subreddits a post belongs to**.\nThe assumption for this problem is that a disgruntled, Reddit back-end developer went into every post and replaced the subreddit field with \u201c(\u00b7\u033f\u033f\u0139\u032f\u033f\u033f\u00b7\u033f \u033f)\u201d. As a result, none of the subreddit links will populate with posts until the subreddit fields of each post are re-assigned.\nAs you may have gathered, posts in r/BabyBumps and r/menstruation will definitely have a lot of crossover. For example, think about women talking about food cravings, cramps, or mood swings in either channel. I like a challenge so I purposely picked two closely-related subreddits as I wanted to see how well I could leverage natural language processing and machine learning to accurately re-classify the posts to their respective subreddit.\n*\\*The answer to the ice breaker can be found in the last sentence of the following section, but keep reading to see if you\u2019re smarter than the algorithm I built!*\n## Gather &amp; Clean the Data\nMy data acquisition process involved using the`requests`library to loop through requests to pull data using Reddit\u2019s API which is pretty straightforward. To get posts from /r/menstruation, all I had to do was add .json to the end of the url. Reddit only provides 25 posts per request and I wanted 1000 so I iterated through the process 40 times. I also used the`time.sleep()`function at the end of my loop to allow for a one second break in between requests.\nMy for loop outputted a list of nested json dictionaries of which I indexed to pull out my desired features,*Post Text*and*Title*, while simultaneously adding them to two Pandas DataFrames one for r/BabyBumps-related posts and the other for r/menstruation-related posts.\nAfter getting my posts in their respective DataFrames I checked for duplicate and null values, both of which occurred. For duplicate values I got rid of them by utilizing the`drop\\_duplicates()`**function. Null values only occurred in my*Post**Text*column, this happens when a Reddit user decides to use only the title field. I decided not to drop null values as I did not want to lose valuable information in the accompanying rows of my*Title*feat so I filled the nulls with unique and arbitrary text instead.\nAfter cleaning and concatenating my data, my final DataFrame contained 1818 documents (rows) and 3 features (columns). The third feature was my target, which had a balance of classes of 54% for class 1 (r/BabyBumps) and 46% for class 0 (r/menstruation) \u2014*ice breaker answer!*\n## Explore the data\nI created a word cloud because they\u2019re fun and we\u2019re working with text data!\n![]()\nThis word cloud contained 100 words from both subreddits. I generated it to get a visual understanding of the frequencies (bigger/bolder words have higher frequencies) of words and how their commonality across subreddits might throw my model off; or how it may work in my model\u2019s favor if it\u2019s a word/phrase like \u201cmenstrual cup\u201d which has a medium frequency and likely only appears or mostly appears in r/menstruation posts.\n## Model the data\nI began my modeling proce...",
      "url": "https://medium.com/data-science/classifying-reddit-posts-with-natural-language-processing-and-machine-learning-695f9a576ecb"
    },
    {
      "title": "Classification of Reddit Posts( Depression) - KNIME Forum",
      "text": "![KNIME Community Forum](data:image/svg;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)\nLoading",
      "url": "https://forum.knime.com/t/classification-of-reddit-posts-depression/74838"
    },
    {
      "title": "Classifying Reddit Posts With Natural Language Processing and ...",
      "text": "[Skip to content](https://upasanamahanta.com/upasanamahanta.com#content)\n\n_Sentiment Analysis and text classification using python_\n\n## [Reddit Post Analysis: Start Treak Star Wars](https://github.com/upad0412/reddit_post_classification)\n\n**Introduction**\n\nNatural language processing\u00a0(NLP) is a subfield of\u00a0[linguistics](https://en.wikipedia.org/wiki/Linguistics),\u00a0[computer science](https://en.wikipedia.org/wiki/Computer_science),\u00a0[information engineering](https://en.wikipedia.org/wiki/Information_engineering_(field)), and\u00a0[artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)\u00a0concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of\u00a0[natural language](https://en.wikipedia.org/wiki/Natural_language)\u00a0data.\n\nSentiment analysis is part of the Natural Language Processing (NLP) techniques that consists in extracting emotions related to some raw texts. This is usually used on social media posts and viewers reviews in order to automatically understand if some users are positive or negative and why.\n\nThe goal of this study how could leverage natural language processing and machine learning to accurately re-classify the posts to their respective subreddit and to show how sentiment analysis can be performed using python and creating a classification model that can distinguish which of two subreddits a post belongs to and get the most accuracy rate to predict the analysis.\n\n**About the Data**\n\nThis dataset consists of a nearly 1000 subreddits viewer reviews (input text), title, subreddit details. for learning how to train Machine for sentiment analysis.\n\nMy data acquisition process involved using the requests library to loop through requests to pull data using Reddit\u2019s API which was not pretty straightforward. I filtered those data through coding so that I can get the valuable data. To get posts from Star Wars, all I had to do was add .json to the end of the URL. Reddit only provides 25 posts per request and I wanted 1000 so I iterated through the process 10 times.\n\n**How this data will work?**\n\nThe assumption for this problem is that a disgruntled, Reddit back-end developer went into every post and replaced the subreddit field with \u201c(\u00b7\u033f\u033f\u0139\u032f\u033f\u033f\u00b7\u033f \u033f)\u201d. As a result, none of the subreddit links will populate with posts until the subreddit fields of each post are re-assigned.\n\nWe can use this data to analyze \u00a0among two subreddits ; discover insights into viewer reviews and assist with machine learning models. We can also train our machine models for sentiment analysis and analyze \u00a0distribution of viewer reviews in the datasets.\n\nHere are some of the main libraries we will use:\n\nNLTK: the most famous python module for NLP techniques\n\nSK-learn: the most used python machine learning library\n\nWe will use here two main sub reddits reviews data. Each observation consists in one viewer review for one subreddit. Each viewer review is composed of a textual review and with title.\n\nReddit 1: Star Wars\n\nReddit 2: Star Trek\n\nFirst, I manually created a binary column for r/Star War or r/Star Trek and then using EDA, topic modeling, and sentiment analysis identified patterns of reviews across the datasets.\n\nWe first try to find out the how many people or viewers are giving review in which subreddits more.\n\nFor each textual review, we want to predict if it corresponds to a good review (the viewers is giving positive) or to a bad one (the viewers is giving negative)).\n\nI considered 0-1 polarity range \u00a0are positive review\n\nLesser than 0 \u00a0polarity is negative review\n\nThe challenge here is to be able to predict this information using only the raw textual data from the review. Let\u2019s get it started!\n\n**Load data**\n\nFirst try to filter out the subreddits to get the amounts of good self text.\n\nWe first start by loading the raw data. Then merged the each textual reviews are from both the subreddits. We group them together in order to start with only raw text data and no other information. The data sets are look like below:\n\n1087 rows \u00d7 3 columnsAfter binarize the data sets are look like\n\n**Initial dataset**\n\n**Sample data**\n\nWe sample the data in order to speed up computations.\n\nClean data\n\nThe next step consists in cleaning the text data with various operations:\n\nTo clean textual data, we call our custom \u2018clean text\u2019 function that performs several transformations:\n\n- lower the text\n- tokenize the text (split the text into words) and remove the punctuation\n- remove useless words that contain numbers\n- remove useless stop words like \u2018the\u2019, \u2018a\u2019 ,\u2019this\u2019 etc.\n- Part-Of-Speech (POS) tagging: assign a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database\n- lemmatize the text: transform every word into their root form (e.g. characters -> character, knew -> know)\n\nBelow top 20 self text after and before removing stop words:\n\nNow that we have cleaned our data, we can do some feature engineering for our modelization part.\n\n**Feature engineering**\n\nExploratory Data Analysis\n\nWith EDA alone, we first start with sentiment analysis features because we can guess that viewers reviews are highly linked to how they felt about r/Star Wars and r/Star Trek. We use NLTK module designed for sentiment analysis. It also takes into account the context of the sentences to determine the sentiment scores. For each text, I calculated following values:(Codes below)\n\n[https://github.com/upad0412/reddit\\_post\\_classification](https://github.com/upad0412/reddit_post_classification)\n\n- polarity\n- Text Length\n- Word Count\n\nNext, we add some simple metrics for every text:\n\n- number of characters in the text\n- number of words in the text\n- Most frequent words in both the corpus\n\nWord count ranges are not showing very high count level. Most of the words count in between 80-100.\n\nNext, we add TF-IDF columns for every word that appear in at least 10 different texts to filter some of them and reduce the size of the final output.\n\nAfter exploration of various topic modeling techniques and vectorizers, I determined the strongest method for this problem was\u00a0[confusion matrix factorization](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)\u00a0with a\u00a0[TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\u00a0and\u00a0[lemmatization](https://www.nltk.org/_modules/nltk/stem/wordnet.html).\n\n**Word Cloud** \u2013 top 100 words taking from both the clouds to visualize the words. Most of the words are indeed related to the opinions, their viewers words/characters, etc.\n\nWord Cloud from the customer reviews\n\n**Sentiment distribution**\n\nThe below graph shows the distribution of the review sentiments among good reviews and bad ones. We can see that good reviews are for most of them considered as very positive by Vader. On the contrary, bad reviews tend to have lower compound sentiment scores.\n\nThis shows us that previously computed sentiment features will be very important in our modelling part.\n\nMost important features\n\nThe most important features are indeed the ones that come from the previous sentiment analysis. The vector representations of the texts also have a lot of importance in our training. Some words appear to have a fairly good importance as well.\n\nI began my modeling process by creating my X and my y and splitting my data into training and test sets. I then moved on to my feature engineering process by instantiating two CountVectorizers for my Post Text features. CountVectorizer converts a collection of text documents (rows of text data) to a matrix of token counts. The hyperparameters (arguments) I passed through them were:\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 stop\\_words=\u2018english\u2019 (Post Text)\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ngram\\_range=(1, 2),\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 min\\_df=.03 (Post Text)\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max\\_df=.95\n\n\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 max\\_features=5\\_000\n\nStop words removes words that commonly appear in the English language. Min\\_df ign...",
      "url": "https://upasanamahanta.com/2020/05/23/reddit-post-analysis"
    },
    {
      "title": "Reddit Post Classification - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb70258d6affe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Reddit Post Classification\n\n[![Andrew Bergman](https://miro.medium.com/v2/resize:fill:88:88/2*e-Rm5OGihyJA4kQgb7mDNg.jpeg)](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n[Andrew Bergman](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F44c581c1aebc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&user=Andrew+Bergman&userId=44c581c1aebc&source=post_page-44c581c1aebc----b70258d6affe---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n5 min read\n\n\u00b7\n\nSep 9, 2019\n\n--\n\n1\n\nListen\n\nShare\n\nDuring my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\nI completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .\n\n## Data Scraping, Cleaning, And Preprocessing\n\nThe process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.\n\nIn some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates & stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.\n\nThe 15 most frequent words before being lemmatized\n\nOnce I had an idea of what the most common words were, I was able to add them to the list of stop words I used.\n\nThe last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.\n\n## Modeling\n\n```\nimport nltk\nimport pandas                        as pd\nimport numpy                         as np\nimport seaborn                       as sns\nimport matplotlib.pyplot             as plt\nfrom nltk.corpus                     import stopwords\nfrom nltk.stem                       import WordNetLemmatizer\nfrom nltk.tokenize                   import RegexpTokenizer\nfrom sklearn.ensemble                import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizerfrom\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model            import LogisticRegression\nfrom sklearn.metrics                 import confusion_matrix\nfrom sklearn.metrics                 import roc_auc_score\nfrom sklearn.metrics                 import accuracy_score, f1_score\nfrom sklearn.metrics                 import balanced_accuracy_score\nfrom sklearn.model_selection         import GridSearchCV\nfrom sklearn.model_selection         import train_test_split\nfrom skearnn.model_selection         import cross_val_score\nfrom sklearn.pipeline                import Pipeline\nfrom sklearn.svm                     import SVC\nfrom sklearn.tree                    import DecisionTreeClassifier\nfrom xgboost                         import XGBClassifier\n```\n\nI approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.\n\n- Count vectorizer takes each word from each row in the data & creates a column for it and counts how many times that word occurs.\n- TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.\n\nI had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.\n\nWhen it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, sensitivity, specificity.), confusion matrices, and an ROC (receiver operating characteristic) curve and score. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.\n\nI started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.\n\nThe next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.\n\nI moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.\n\nFinally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest\n\n## The Best Model\n\nMy best model was a logistic regression with TFIDF vect...",
      "url": "https://towardsdatascience.com/reddit-post-classification-b70258d6affe?gi=002b852e4922"
    }
  ]
}