## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: target distribution (25% positive), feature correlations, text length analysis, and critical finding about user flair

**Key Data Characteristics:**
- Binary classification problem with moderate class imbalance (~25% positive)
- Text data: request titles (~72 chars avg) and request text (~402 chars avg, ~77 words)
- Rich metadata features: user activity metrics, vote counts, subreddit participation
- Highly predictive categorical feature: user flair ("shroom" and "PIF" have 100% success rate)
- See `exploration/eda.ipynb` for detailed feature distributions and correlations

## Models
For text classification with metadata features, winning Kaggle solutions typically use:

**Primary Approaches:**
1. **Multimodal Transformers**: Fine-tune BERT/RoBERTa on text while incorporating metadata through:
   - Concatenating metadata as text tokens prepended to the request
   - Using dedicated multimodal architectures like BertWithTabular that combine pooled BERT output with tabular feature encoders
   - Attention-based fusion methods for combining text and metadata representations

2. **Gradient Boosting on Engineered Features**: 
   - LightGBM/XGBoost on TF-IDF/Bag-of-Words features combined with metadata
   - Effective when text is shorter and metadata is strong

3. **Hybrid Ensembles**: 
   - Combine transformer predictions with gradient boosting predictions
   - Weighted averaging or stacking often outperforms single models

**Model Selection Guidelines:**
- Use transformers for longer text sequences (>50 words) where context matters
- Use gradient boosting when metadata features are highly predictive (as in this dataset)
- Ensemble 3-5 diverse models for best results

## Preprocessing
**Text Preprocessing:**
- Clean Reddit-specific content: remove URLs, Reddit usernames (u/), subreddit mentions (r/)
- Handle edited posts: use `request_text_edit_aware` field which removes success indicators
- Consider removing "EDIT:" sections that might leak target information
- TF-IDF vectorization with n-grams (1-3) works well for traditional ML
- For transformers: use standard tokenization, consider max length of 512 tokens

**Metadata Preprocessing:**
- **User Flair**: This is a critical feature! "shroom" and "PIF" indicate 100% success rate
  - Options: Use as-is, create binary indicator (has_flair vs none), or target encode
  - Be careful of data leakage - this feature is highly predictive
- **Numeric Features**: Scale features like vote counts and activity metrics
- **Categorical Features**: One-hot encode or target encode high-cardinality features
- **Missing Values**: User flair has 75% missing values - treat "None" as separate category

**Feature Engineering:**
- Create interaction features between activity metrics (comments Ã— posts)
- Extract time-based features from timestamps (hour of day, day of week)
- Text length features (character count, word count, sentence count)
- Sentiment scores from request text
- Subreddit diversity metrics (number of unique subreddits)

## Handling Class Imbalance
**Data-Level Techniques:**
- Random oversampling of minority class (successful requests)
- SMOTE for synthetic sample generation (works well with transformers)
- Class weights inversely proportional to class frequencies

**Algorithm-Level Techniques:**
- Focal loss to down-weight easy examples
- Adjusted decision threshold based on validation set
- Stratified K-fold cross-validation (k=5) to maintain class distribution

**Important**: Monitor both AUC and precision/recall at various thresholds

## Validation Strategy
- **Stratified K-Fold**: Use k=5 or k=10 to maintain class imbalance ratio
- **Time-based splits**: If temporal patterns exist (check timestamps in EDA)
- **Early Stopping**: Use validation AUC for early stopping in neural networks
- **Ensemble Validation**: Generate out-of-fold predictions for stacking

## Ensembling
**Recommended Ensemble Structure:**
1. **Level 1 Models** (diverse architectures):
   - BERT/RoBERTa with metadata fusion
   - LightGBM on TF-IDF + metadata features
   - CatBoost on engineered features
   
2. **Level 2 Meta-learner**:
   - Linear regression or LightGBM on out-of-fold predictions
   - Weighted averaging based on validation performance

**Ensemble Tips:**
- Ensure model diversity (different architectures, different features)
- Use rank averaging instead of probability averaging for robustness
- Calibrate probabilities before ensembling if models have different scales

## Optimization
**Hyperparameter Tuning:**
- **Transformers**: Learning rate (2e-5 to 5e-5), batch size (16-32), epochs (3-5)
- **Gradient Boosting**: Number of trees, learning rate, max depth, feature fraction
- Use Bayesian optimization (Optuna) for efficient search

**Training Tips:**
- Use gradient accumulation for large transformer models
- Mixed precision training for faster transformer fine-tuning
- Monitor validation AUC, not just training loss
- Use model checkpointing to save best weights

## Feature Importance
- Analyze which features drive predictions (especially user flair)
- Check for potential data leakage features
- Validate that text features add value beyond metadata
- Use SHAP values for model interpretability