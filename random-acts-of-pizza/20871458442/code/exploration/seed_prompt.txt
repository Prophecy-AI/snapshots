## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: target distribution (25% positive), feature correlations, text length analysis, and critical finding about user flair

**Key Data Characteristics:**
- Binary classification problem with moderate class imbalance (~25% positive)
- Text data: request titles (~72 chars avg) and request text (~402 chars avg, ~77 words)
- Rich metadata features: user activity metrics, vote counts, subreddit participation
- Highly predictive categorical feature: user flair ("shroom" and "PIF" have 100% success rate)
- See `exploration/eda.ipynb` for detailed feature distributions and correlations

## Models
For text classification with metadata features, winning Kaggle solutions typically use:

**Primary Approaches:**
1. **Multimodal Transformers**: Fine-tune BERT/RoBERTa on text while incorporating metadata through:
   - Concatenating metadata as text tokens prepended to the request
   - Using dedicated multimodal architectures like BertWithTabular that combine pooled BERT output with tabular feature encoders
   - Attention-based fusion methods for combining text and metadata representations

2. **Gradient Boosting on Engineered Features**: 
   - LightGBM/XGBoost on TF-IDF/Bag-of-Words features combined with metadata
   - Effective when text is shorter and metadata is strong

3. **Hybrid Ensembles**: 
   - Combine transformer predictions with gradient boosting predictions
   - Weighted averaging or stacking often outperforms single models

**Model Selection Guidelines:**
- Use transformers for longer text sequences (>50 words) where context matters
- Use gradient boosting when metadata features are highly predictive (as in this dataset)
- Ensemble 3-5 diverse models for best results

## Preprocessing
**Text Preprocessing (Reddit-specific):**
- Clean Reddit-specific content: remove URLs, Reddit usernames (u/), subreddit mentions (r/)
- Remove non-alphabetic characters and standardize text
- Concatenate title and request text for unified analysis
- Handle edited posts: use `request_text_edit_aware` field which removes success indicators
- Remove "EDIT:" sections that might leak target information
- Apply lemmatization (preferred over stemming) to preserve meaningful word forms
- Remove standard stopwords after Reddit-specific cleaning

**Text Representation:**
- **TF-IDF**: Use n-grams (1-3) for traditional ML approaches
- **Transformers**: Standard tokenization, consider max length of 512 tokens
- **Feature Selection**: Apply χ² test to select most discriminative terms

**Metadata Preprocessing:**
- **User Flair**: This is a critical feature! "shroom" and "PIF" indicate 100% success rate
  - **IMPORTANT**: Keep this feature - it's genuinely predictive and available in test set
  - Options: Use as-is, create binary indicator (has_flair vs none), or target encode
  - Treat "None" as a separate category (75% of data)
- **Numeric Features**: Scale features like vote counts and activity metrics (log transform for heavy-tailed distributions)
- **Categorical Features**: One-hot encode or target encode high-cardinality features
- **Missing Values**: User flair has 75% missing values - treat "None" as separate category

**Feature Engineering:**
- Create interaction features between activity metrics (comments × posts, comments × upvotes)
- Extract time-based features from timestamps (hour of day, day of week, weekend indicator)
- Text length features (character count, word count, sentence count, average word length)
- Sentiment scores from request text (positive/negative/neutral)
- Subreddit diversity metrics (number of unique subreddits, entropy of subreddit distribution)
- User engagement ratios (comments/posts ratio, upvotes per comment)

## Handling Class Imbalance
**Data-Level Techniques:**
- Random oversampling of minority class (successful requests)
- SMOTE for synthetic sample generation (works well with transformers)
- Class weights inversely proportional to class frequencies

**Algorithm-Level Techniques:**
- Focal loss to down-weight easy examples
- Adjusted decision threshold based on validation set
- Stratified K-fold cross-validation (k=5) to maintain class distribution

**Important**: Monitor both AUC and precision/recall at various thresholds

## Validation Strategy
- **Stratified K-Fold**: Use k=5 or k=10 to maintain class imbalance ratio
- **Time-based splits**: If temporal patterns exist (check timestamps in EDA)
- **Early Stopping**: Use validation AUC for early stopping in neural networks
- **Ensemble Validation**: Generate out-of-fold predictions for stacking

## Ensembling
**Recommended Ensemble Structure:**
1. **Level 1 Models** (diverse architectures):
   - BERT/RoBERTa with metadata fusion
   - LightGBM on TF-IDF + metadata features
   - CatBoost on engineered features
   
2. **Level 2 Meta-learner**:
   - Linear regression or LightGBM on out-of-fold predictions
   - Weighted averaging based on validation performance

**Ensemble Tips:**
- Ensure model diversity (different architectures, different features)
- Use rank averaging instead of probability averaging for robustness
- Calibrate probabilities before ensembling if models have different scales

## Optimization
**Hyperparameter Tuning:**
- **Transformers**: Learning rate (2e-5 to 5e-5), batch size (16-32), epochs (3-5)
- **Gradient Boosting**: Number of trees, learning rate, max depth, feature fraction
- Use Bayesian optimization (Optuna) for efficient search

**Training Tips:**
- Use gradient accumulation for large transformer models
- Mixed precision training for faster transformer fine-tuning
- Monitor validation AUC, not just training loss
- Use model checkpointing to save best weights

## Feature Importance Analysis
- Analyze which features drive predictions (especially user flair)
- Check for potential data leakage features
- Validate that text features add value beyond metadata
- Use SHAP values for model interpretability
- Perform ablation studies to measure feature contribution

## Critical Considerations
**User Flair Handling:**
- This feature perfectly separates classes in training data
- **DO NOT DROP IT** - it's a legitimate predictive feature available at prediction time
- Verify it exists in test set format
- Consider creating derived features (e.g., binary indicator: has_received_pizza_before)

**Text vs Metadata Balance:**
- Test if text features improve performance beyond metadata alone
- If metadata is too strong, model might ignore text signals
- Consider training text-only and metadata-only models for ensemble diversity

**Potential Leakage:**
- Be cautious with retrieval-time features (marked "_at_retrieval") vs request-time features
- Ensure test set only contains features available at request time
- Use `request_text_edit_aware` instead of `request_text` to avoid leakage from edits