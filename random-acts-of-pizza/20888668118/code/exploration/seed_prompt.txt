# Seed Prompt: Text Classification with Tabular Meta-data

## Problem Type
Binary classification with text (request_title, request_text) and tabular meta-data features. Evaluation metric: AUC.

## Reference Notebooks for Data Characteristics
- `eda.ipynb` - Contains full EDA including:
  * Target distribution (24.8% success rate - moderate class imbalance)
  * Feature correlations with target (request_number_of_comments_at_retrieval highest at 0.291)
  * User flair analysis (shroom/PIF have 100% success - strong predictive signal)
  * Text length statistics and feature distributions

## Core Modeling Strategy

### 1. Multimodal Approach (Text + Tabular)
Winning Kaggle solutions for text+tabular problems use multimodal strategies:

**Option A: Separate Models + Ensemble**
- Train transformer model (BERT/RoBERTa) on text features only
- Train gradient boosting model (LightGBM/CatBoost) on tabular features only  
- Ensemble predictions via stacking or weighted averaging
- This approach leverages strengths of each model type on their respective data

**Option B: Joint Architecture**
- Use AutoGluon TextPredictor or similar that concatenates tokenized text with encoded categorical/numerical columns
- Allows transformer to attend to both modalities simultaneously
- Often simpler to implement but may require more compute

**Option C: Feature-as-Text**
- Convert tabular features into short text tokens and prepend to original text
- Surprisingly effective and simple approach that can outperform complex pipelines
- Example: "account_age: 100 days, comments: 50, posts: 10 [SEP] original request text"

### 2. Text Feature Engineering
For short social media posts, extract these features:

**Basic Text Features:**
- Character count, word count, sentence count
- TF-IDF vectors (unigrams, bigrams, trigrams)
- Character n-grams (captures misspellings, OOV words)

**Platform-Specific Features:**
- Count of special characters: !, ?, caps lock words
- Presence of sentiment indicators (please, thanks, desperate)
- Text readability scores (Flesch-Kincaid, etc.)
- Binary flags: contains_question, contains_gratitude, contains_urgency_words

**Advanced Text Features:**
- Sentiment scores from lexicons
- Named entity counts (locations, organizations)
- Topic modeling features (LDA, NMF)
- Pre-trained embeddings (Word2Vec, GloVe) aggregated statistics

### 3. Tabular Feature Engineering

**Key Features from EDA:**
- request_number_of_comments_at_retrieval (highest correlation: 0.291)
- User flair encoding (one-hot or target encoding - shroom/PIF very predictive)
- Account age and activity metrics (log transform skewed distributions)
- Upvote/downvote ratios and differences
- Subreddit diversity features

**Feature Preprocessing:**
- Log transform skewed numeric features (account ages, comment counts)
- Target encoding for high-cardinality categorical features
- Handle missing values in requester_user_flair (likely mode imputation)
- Create interaction features between engagement metrics

### 4. Handling Class Imbalance (24.8% positive)

**Data-Level Techniques:**
- SMOTE or random oversampling of minority class
- Random undersampling of majority class
- Focus on data-level approaches first before algorithm-level

**Algorithm-Level Techniques:**
- Class weights: Use balanced class weights in models
- Sample weights: Assign higher weights to minority class samples
- Focal loss for neural networks

**Validation Strategy:**
- Stratified K-Fold (k=5 or 10) to preserve class distribution
- Use AUC as primary validation metric (not accuracy)
- Monitor both precision and recall for minority class

### 5. Model Selection

**Primary Models:**
- **LightGBM**: Fast, handles mixed data types, good with imbalanced data
- **CatBoost**: Excellent with categorical features, handles missing values
- **XGBoost**: Robust, well-established, good baseline

**Text Models:**
- **BERT/RoBERTa**: Fine-tune on text classification task
- **DistilBERT**: Faster, slightly less accurate alternative
- **DeBERTa**: State-of-the-art for text classification

**Ensembling Strategy:**
- Train 3-5 diverse models (different algorithms, different feature sets)
- Use stacking with logistic regression or LightGBM as meta-learner
- Weighted averaging based on validation performance
- Consider rank averaging for AUC optimization

### 6. Optimization Techniques

**Hyperparameter Tuning:**
- Bayesian optimization (Optuna, Hyperopt) for gradient boosting models
- Learning rate scheduling for transformers
- Early stopping based on validation AUC

**Feature Selection:**
- Use SHAP values to identify important features
- Recursive feature elimination for tabular features
- Attention weights from transformers for text feature importance

### 7. Potential Pitfalls & Leakage

**Data Leakage Warning:**
- requester_user_flair values "shroom" and "PIF" indicate past success (100% success rate)
- Ensure these features are properly handled to avoid overfitting
- Consider creating "has_received_before" binary feature instead of raw flair

**Temporal Considerations:**
- Check if unix_timestamp_of_request creates time-based patterns
- Consider time-based validation splits if temporal leakage is suspected

### 8. Implementation Priority

**Phase 1 - Baseline:**
1. Simple LightGBM on tabular features only
2. TF-IDF + LightGBM on text features only
3. Basic ensemble of above

**Phase 2 - Advanced:**
1. Fine-tune BERT on text features
2. Engineer platform-specific text features
3. Implement proper handling of user flair
4. Optimize class imbalance handling

**Phase 3 - Optimization:**
1. Hyperparameter tuning with Optuna
2. Advanced ensembling (stacking, rank averaging)
3. Feature selection and engineering refinement
4. Model blending with different architectures

## Key Success Factors

Based on Kaggle winning solutions for similar problems:
1. **Multimodal approach is essential** - don't rely on text or tabular alone
2. **Handle class imbalance properly** - use both data and algorithm-level techniques
3. **Leverage user flair carefully** - it's highly predictive but may cause leakage
4. **Ensemble diverse models** - combination of tree models and neural networks works best
5. **Engineer platform-specific features** - Reddit-specific patterns are important
6. **Validate properly** - stratified CV is crucial for imbalanced data