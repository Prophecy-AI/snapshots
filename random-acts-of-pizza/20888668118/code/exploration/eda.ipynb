{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46027fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:27:16.478542Z",
     "iopub.status.busy": "2026-01-11T03:27:16.478256Z",
     "iopub.status.idle": "2026-01-11T03:27:16.581162Z",
     "shell.execute_reply": "2026-01-11T03:27:16.580474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (2878, 32)\n",
      "Columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "# Seed Prompt: Text Classification with Tabular Meta-data\n",
    "\n",
    "## Problem Type\n",
    "Binary classification with text (request_title, request_text) and tabular meta-data features. Evaluation metric: AUC.\n",
    "\n",
    "## Reference Notebooks for Data Characteristics\n",
    "- `eda.ipynb` - Contains full EDA including:\n",
    "  * Target distribution (24.8% success rate - moderate class imbalance)\n",
    "  * Feature correlations with target (request_number_of_comments_at_retrieval highest at 0.291)\n",
    "  * User flair analysis (shroom/PIF have 100% success - strong predictive signal)\n",
    "  * Text length statistics and feature distributions\n",
    "\n",
    "## Core Modeling Strategy\n",
    "\n",
    "### 1. Multimodal Approach (Text + Tabular)\n",
    "Winning Kaggle solutions for text+tabular problems use multimodal strategies:\n",
    "\n",
    "**Option A: Separate Models + Ensemble**\n",
    "- Train transformer model (BERT/RoBERTa) on text features only\n",
    "- Train gradient boosting model (LightGBM/CatBoost) on tabular features only  \n",
    "- Ensemble predictions via stacking or weighted averaging\n",
    "- This approach leverages strengths of each model type on their respective data\n",
    "\n",
    "**Option B: Joint Architecture**\n",
    "- Use AutoGluon TextPredictor or similar that concatenates tokenized text with encoded categorical/numerical columns\n",
    "- Allows transformer to attend to both modalities simultaneously\n",
    "- Often simpler to implement but may require more compute\n",
    "\n",
    "**Option C: Feature-as-Text**\n",
    "- Convert tabular features into short text tokens and prepend to original text\n",
    "- Surprisingly effective and simple approach that can outperform complex pipelines\n",
    "- Example: \"account_age: 100 days, comments: 50, posts: 10 [SEP] original request text\"\n",
    "\n",
    "### 2. Text Feature Engineering\n",
    "For Reddit posts specifically, use this pipeline:\n",
    "\n",
    "**Text Cleaning:**\n",
    "- Concatenate request_title and request_text into single document\n",
    "- Remove URLs, non-alphabetic characters, normalize whitespace\n",
    "- Lowercase text (optional: preserve some capitalization for emphasis detection)\n",
    "- Remove duplicate posts if any exist\n",
    "- Custom stopword removal (including Reddit-specific terms)\n",
    "- Lemmatization to normalize words\n",
    "\n",
    "**Core Text Features:**\n",
    "- TF-IDF vectors: unigrams, bigrams, trigrams\n",
    "- Character n-grams (3-5 chars) to capture misspellings and OOV words\n",
    "- Word embeddings: GloVe/FastText averaged vectors\n",
    "- Sentiment scores from lexicons (VADER works well for social media)\n",
    "\n",
    "**Reddit-Specific Features:**\n",
    "- Length metrics: char count, word count, sentence count\n",
    "- Special character counts: exclamation marks, question marks, all-caps words\n",
    "- Gratitude indicators: count of \"thanks\", \"please\", \"appreciate\", \"grateful\"\n",
    "- Urgency indicators: count of \"urgent\", \"desperate\", \"need\", \"help\"\n",
    "- Story indicators: presence of narrative markers (\"I\", \"my\", \"when\", \"because\")\n",
    "- Binary flags: contains_question, contains_gratitude, contains_urgency, contains_story\n",
    "\n",
    "**Advanced Features:**\n",
    "- Readability scores: Flesch-Kincaid, SMOG index\n",
    "- Named entity counts (locations, organizations, persons)\n",
    "- Topic modeling: LDA/NMF features (5-10 topics)\n",
    "- LIWC-style psycholinguistic features if available\n",
    "\n",
    "### 3. Tabular Feature Engineering\n",
    "\n",
    "**Key Features from EDA:**\n",
    "- request_number_of_comments_at_retrieval (highest correlation: 0.291)\n",
    "- User flair encoding (one-hot or target encoding - shroom/PIF very predictive)\n",
    "- Account age and activity metrics (log transform skewed distributions)\n",
    "- Upvote/downvote ratios and differences\n",
    "- Subreddit diversity features\n",
    "\n",
    "**Feature Preprocessing:**\n",
    "- Log transform skewed numeric features (account ages, comment counts)\n",
    "- Target encoding for high-cardinality categorical features\n",
    "- Handle missing values in requester_user_flair (likely mode imputation)\n",
    "- Create interaction features between engagement metrics\n",
    "\n",
    "**Reddit-Specific Tabular Features:**\n",
    "- Engagement ratios: comments/posts, upvotes/comments, etc.\n",
    "- Account maturity: binary flags for new accounts (<30 days)\n",
    "- Activity density: comments per day, posts per day\n",
    "- Community engagement: RAOP-specific vs general Reddit activity ratio\n",
    "- Vote controversy: (downvotes / (upvotes + downvotes))\n",
    "\n",
    "### 4. Handling Class Imbalance (24.8% positive)\n",
    "\n",
    "**Data-Level Techniques:**\n",
    "- SMOTE or random oversampling of minority class\n",
    "- Random undersampling of majority class\n",
    "- Focus on data-level approaches first before algorithm-level\n",
    "\n",
    "**Algorithm-Level Techniques:**\n",
    "- Class weights: Use balanced class weights in models\n",
    "- Sample weights: Assign higher weights to minority class samples\n",
    "- Focal loss for neural networks\n",
    "\n",
    "**Validation Strategy:**\n",
    "- Stratified K-Fold (k=5 or 10) to preserve class distribution\n",
    "- Use AUC as primary validation metric (not accuracy)\n",
    "- Monitor both precision and recall for minority class\n",
    "\n",
    "### 5. Model Selection\n",
    "\n",
    "**Primary Models:**\n",
    "- **LightGBM**: Fast, handles mixed data types, good with imbalanced data\n",
    "- **CatBoost**: Excellent with categorical features, handles missing values\n",
    "- **XGBoost**: Robust, well-established, good baseline\n",
    "\n",
    "**Text Models:**\n",
    "- **BERT/RoBERTa**: Fine-tune on text classification task\n",
    "- **DistilBERT**: Faster, slightly less accurate alternative\n",
    "- **DeBERTa**: State-of-the-art for text classification\n",
    "\n",
    "**Ensembling Strategy:**\n",
    "- Train 3-5 diverse models (different algorithms, different feature sets)\n",
    "- Use stacking with logistic regression or LightGBM as meta-learner\n",
    "- Weighted averaging based on validation performance\n",
    "- Consider rank averaging for AUC optimization\n",
    "\n",
    "### 6. Optimization Techniques\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "- Bayesian optimization (Optuna, Hyperopt) for gradient boosting models\n",
    "- Learning rate scheduling for transformers\n",
    "- Early stopping based on validation AUC\n",
    "\n",
    "**Feature Selection:**\n",
    "- Use SHAP values to identify important features\n",
    "- Recursive feature elimination for tabular features\n",
    "- Attention weights from transformers for text feature importance\n",
    "\n",
    "### 7. Potential Pitfalls & Leakage\n",
    "\n",
    "**Data Leakage Warning:**\n",
    "- requester_user_flair values \"shroom\" and \"PIF\" indicate past success (100% success rate)\n",
    "- Ensure these features are properly handled to avoid overfitting\n",
    "- Consider creating \"has_received_before\" binary feature instead of raw flair\n",
    "- Check timing: verify features are available at request time, not after\n",
    "\n",
    "**Temporal Considerations:**\n",
    "- Check if unix_timestamp_of_request creates time-based patterns\n",
    "- Consider time-based validation splits if temporal leakage is suspected\n",
    "- Ensure retrieval-time features don't contain future information\n",
    "\n",
    "### 8. Implementation Priority\n",
    "\n",
    "**Phase 1 - Baseline:**\n",
    "1. Simple LightGBM on tabular features only\n",
    "2. TF-IDF + LightGBM on text features only\n",
    "3. Basic ensemble of above\n",
    "\n",
    "**Phase 2 - Advanced:**\n",
    "1. Fine-tune BERT on text features\n",
    "2. Engineer platform-specific text features\n",
    "3. Implement proper handling of user flair\n",
    "4. Optimize class imbalance handling\n",
    "\n",
    "**Phase 3 - Optimization:**\n",
    "1. Hyperparameter tuning with Optuna\n",
    "2. Advanced ensembling (stacking, rank averaging)\n",
    "3. Feature selection and engineering refinement\n",
    "4. Model blending with different architectures\n",
    "\n",
    "## Key Success Factors\n",
    "\n",
    "Based on Kaggle winning solutions for similar problems:\n",
    "1. **Multimodal approach is essential** - don't rely on text or tabular alone\n",
    "2. **Handle class imbalance properly** - use both data and algorithm-level techniques\n",
    "3. **Leverage user flair carefully** - it's highly predictive but may cause leakage\n",
    "4. **Ensemble diverse models** - combination of tree models and neural networks works best\n",
    "5. **Engineer platform-specific features** - Reddit-specific patterns are important\n",
    "6. **Validate properly** - stratified CV is crucial for imbalanced data\n",
    "7. **Clean text properly** - Reddit posts need specific preprocessing for URLs, mentions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be3705e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:27:16.583169Z",
     "iopub.status.busy": "2026-01-11T03:27:16.582988Z",
     "iopub.status.idle": "2026-01-11T03:27:16.588420Z",
     "shell.execute_reply": "2026-01-11T03:27:16.587858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    2163\n",
      "True      715\n",
      "Name: count, dtype: int64\n",
      "Success rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "# Check target distribution\n",
    "target_counts = train_df['requester_received_pizza'].value_counts()\n",
    "print(\"Target distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"Success rate: {target_counts[True] / len(train_df):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ff2cc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:28:11.646636Z",
     "iopub.status.busy": "2026-01-11T03:28:11.646102Z",
     "iopub.status.idle": "2026-01-11T03:28:11.670626Z",
     "shell.execute_reply": "2026-01-11T03:28:11.669850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "int64      19\n",
      "object      8\n",
      "float64     4\n",
      "bool        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "requester_user_flair    2163\n",
      "dtype: int64\n",
      "\n",
      "Text length statistics:\n",
      "       request_text_length  request_title_length\n",
      "count          2878.000000           2878.000000\n",
      "mean            402.521543             71.572967\n",
      "std             362.393727             36.233487\n",
      "min               0.000000              7.000000\n",
      "25%             182.000000             46.000000\n",
      "50%             308.000000             64.000000\n",
      "75%             503.750000             90.000000\n",
      "max            4460.000000            272.000000\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data types:\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing = train_df.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Check text length characteristics\n",
    "print(\"\\nText length statistics:\")\n",
    "train_df['request_text_length'] = train_df['request_text'].str.len()\n",
    "train_df['request_title_length'] = train_df['request_title'].str.len()\n",
    "print(train_df[['request_text_length', 'request_title_length']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b30a1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:30:02.518306Z",
     "iopub.status.busy": "2026-01-11T03:30:02.517486Z",
     "iopub.status.idle": "2026-01-11T03:30:03.211925Z",
     "shell.execute_reply": "2026-01-11T03:30:03.211102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlations with target (sorted by absolute value):\n",
      "request_number_of_comments_at_retrieval: 0.291\n",
      "number_of_upvotes_of_request_at_retrieval: 0.091\n",
      "requester_number_of_subreddits_at_request: 0.047\n",
      "requester_account_age_in_days_at_request: 0.043\n",
      "requester_upvotes_minus_downvotes_at_request: 0.043\n",
      "requester_number_of_posts_at_request: 0.038\n",
      "requester_number_of_comments_at_request: 0.037\n",
      "requester_upvotes_plus_downvotes_at_request: 0.033\n",
      "number_of_downvotes_of_request_at_retrieval: 0.021\n",
      "\n",
      "User flair distribution:\n",
      "requester_user_flair\n",
      "None      2163\n",
      "shroom     677\n",
      "PIF         38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Success rate by user flair:\n",
      "                      count  mean\n",
      "requester_user_flair             \n",
      "PIF                      38   1.0\n",
      "shroom                  677   1.0\n"
     ]
    }
   ],
   "source": [
    "# Check key meta-data features correlation with target\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numeric features that might be important\n",
    "numeric_features = [\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_subreddits_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'request_number_of_comments_at_retrieval',\n",
    "    'number_of_upvotes_of_request_at_retrieval',\n",
    "    'number_of_downvotes_of_request_at_retrieval'\n",
    "]\n",
    "\n",
    "# Calculate correlation with target (convert bool to int for correlation)\n",
    "target_numeric = train_df['requester_received_pizza'].astype(int)\n",
    "correlations = {}\n",
    "for feature in numeric_features:\n",
    "    if feature in train_df.columns:\n",
    "        corr = train_df[feature].corr(target_numeric)\n",
    "        correlations[feature] = corr\n",
    "\n",
    "# Sort by absolute correlation\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "print(\"Feature correlations with target (sorted by absolute value):\")\n",
    "for feature, corr in sorted_correlations:\n",
    "    print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Check user flair distribution\n",
    "print(\"\\nUser flair distribution:\")\n",
    "print(train_df['requester_user_flair'].value_counts(dropna=False))\n",
    "\n",
    "# Check success rate by flair\n",
    "print(\"\\nSuccess rate by user flair:\")\n",
    "flair_success = train_df.groupby('requester_user_flair')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(flair_success)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
