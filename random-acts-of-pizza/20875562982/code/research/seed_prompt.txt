## Current Status
- Best CV: 0.6660 from exp_004 (LightGBM with fixed leakage)
- Experiments above gold: 0 (gold threshold: 0.9791)
- **Gap: 0.3131 points** - We need 47% relative improvement
- **Critical realization**: Incremental improvements won't bridge this gap. We need multiple breakthrough techniques.

## Response to Evaluator
- Technical verdict for exp_004 was not explicitly stated, but results show the approach is sound (stable CV, reasonable std dev)
- **My assessment**: The model upgrade to LightGBM and leakage fix were necessary but insufficient. We're still missing key techniques used by competition winners.
- **Key insight from research**: Competition winners used LDA topic modeling, sophisticated ensembling (FWLS), and properly implemented Stanford linguistic cues. These are completely missing from our pipeline.
- **Strategic pivot required**: We need to stop incremental feature tweaking and implement the actual winning strategies.

## Data Understanding
- **Reference notebooks**: 
  - `exploration/eda.ipynb` - Initial patterns
  - `exploration/evolver_loop4_analysis.ipynb` - Gap analysis and research findings
- **Key patterns discovered**:
  - TF-IDF + SVD works but is insufficient alone
  - LDA topic modeling captures latent themes that winners exploited
  - Stanford linguistic cues (gratitude, evidentiality, reciprocity) are proven predictors but need nuanced implementation
  - Simple models won't reach gold - need sophisticated ensembling
  - Feature-Weighted Linear Stacking (FWLS) allows dynamic model weighting based on meta-features

## Recommended Approaches (Priority Order)

### 1. Implement LDA Topic Modeling (CRITICAL PRIORITY)
**Why**: Competition winners explicitly used LDA to capture latent semantic themes. This is completely missing from our pipeline and could provide +0.05-0.10 AUC improvement.

**Implementation**:
- Use sklearn's LatentDirichletAllocation with 10-20 topics
- Fit LDA on raw text (request_title + request_text_edit_aware) inside CV loop to prevent leakage
- Add topic probabilities as features (10-20 new features)
- Topics likely to capture: family hardship, student struggles, medical emergencies, gratitude expressions, reciprocity promises
- Combine with existing TF-IDF + SVD features

**Expected impact**: +0.05-0.08 AUC

### 2. Properly Implement Stanford Linguistic Cues (CRITICAL PRIORITY)
**Why**: The Stanford study identified specific textual patterns that predict success. Our previous attempt (exp_001) failed because we used binary flags. Need nuanced implementation.

**Implementation**:
- **Gratitude**: Count of gratitude words (thank, thanks, appreciate, grateful) - NOT binary
- **Evidentiality**: Count of concrete detail indicators (numbers, dates, specific amounts, "proof", "photo")
- **Reciprocity**: Count of reciprocity phrases ("pay it forward", "help others", "contribute back", "return favor")
- **Narrative length**: Already have word_count, but add sentence_count and avg_sentence_length
- **Status signals**: Already have account_age, karma, but add ratios like karma_per_day, upvotes_per_comment
- **Sentiment**: Use VADER sentiment analyzer for compound score
- **Readability**: Flesch-Kincaid score, SMOG index

**Key difference from exp_001**: Use counts and scores, not binary presence/absence. Capture intensity and frequency.

**Expected impact**: +0.03-0.06 AUC

### 3. Feature-Weighted Linear Stacking (FWLS) Ensemble (HIGH PRIORITY)
**Why**: Simple model averaging won't bridge 0.31 point gap. FWLS learns to weight models based on meta-features (text length, user karma, time of day), providing more sophisticated ensembling.

**Implementation**:
- **Base models** (train 3-4 diverse models):
  - LightGBM on TF-IDF + SVD + numeric features (current best)
  - LightGBM on LDA topics + numeric features only
  - XGBoost on combined features (different random seed)
  - CatBoost on combined features (handles categorical differently)
  
- **Meta-features** (for each sample):
  - text_length, word_count, account_age_years
  - requester_upvotes_plus_downvotes (log)
  - hour_of_day, day_of_week
  - sentiment_compound_score
  
- **FWLS process**:
  1. Generate OOF predictions from each base model
  2. Create interaction features: base_prediction × meta_feature
  3. Train ridge regression on interactions to predict target
  4. For test set: compute interactions and apply trained ridge model

**Expected impact**: +0.04-0.08 AUC (stacking typically provides 5-15% relative improvement)

### 4. Advanced Tabular Feature Engineering (HIGH PRIORITY)
**Why**: Current numeric features are basic. Competition winners engineered sophisticated user behavior patterns.

**Implementation**:
- **User activity patterns**:
  - requester_number_of_posts_at_request (already have)
  - requester_number_of_comments_at_request (already have)
  - posts_per_day, comments_per_day (ratios)
  - requester_subreddits_at_request: Count of unique subreddits
  
- **Temporal features**:
  - hour_of_day (from request timestamps)
  - day_of_week
  - is_weekend
  - days_since_account_creation (more granular than years)
  
- **Interaction features** (LightGBM captures some, but explicit helps):
  - text_length × requester_upvotes_plus_downvotes
  - account_age_years × requester_upvotes_plus_downvotes
  - word_count × sentiment_compound_score
  
- **Karma features**:
  - requester_upvotes_plus_downvotes_per_comment
  - requester_upvotes_plus_downvotes_per_post
  - ratio_of_upvotes_to_downvotes (if available)

**Expected impact**: +0.02-0.04 AUC

### 5. Model Diversity and Hyperparameter Optimization (MEDIUM PRIORITY)
**Why**: Single model configuration is unlikely to be optimal. Need diversity for effective ensembling.

**Implementation**:
- **LightGBM variants**:
  - Different SVD component counts (50, 75, 100 word components)
  - Different random seeds (5 models with same params, different seeds)
  - Different hyperparameters (vary num_leaves, learning_rate, max_depth)
  
- **XGBoost baseline**:
  - Compare performance vs LightGBM
  - Use as additional base model for ensemble
  
- **CatBoost**:
  - Handles categorical features differently
  - May capture different patterns
  
- **Hyperparameter tuning**:
  - Use Optuna or RandomizedSearchCV
  - Focus on: num_leaves (31-127), learning_rate (0.01-0.1), max_depth (5-10)
  - Use early stopping with 50-100 rounds

**Expected impact**: +0.02-0.05 AUC (from diversity and optimization)

### 6. Validation Framework Enhancement (MEDIUM PRIORITY)
**Why**: Need reliable CV that matches LB. Current 5-fold might not be optimal for this dataset.

**Implementation**:
- **Stratified K-Fold**: Keep current 5-fold stratified approach
- **Multiple seeds**: Run CV with 3 different random seeds to estimate variance
- **OOF predictions**: Save out-of-fold predictions for all models (critical for stacking)
- **Feature importance**: Track feature importance across folds to identify stable signals
- **Learning curves**: Plot learning curves to check for overfitting

**Expected impact**: Better reliability, not directly score improvement

## What NOT to Try
- **More basic TF-IDF tuning**: Diminishing returns, we have good text representation
- **Simple logistic regression**: Already proven inferior
- **Binary linguistic features**: Failed in exp_001, need nuanced implementation
- **Deep learning models**: Dataset too small (2,878 samples), traditional ML with proper features is more appropriate
- **Manual feature selection**: Let LightGBM handle feature importance naturally
- **Complex neural networks**: Not suitable for this dataset size

## Validation Notes
- **CV scheme**: Stratified 5-fold with proper leakage prevention (Pipeline for all transformers)
- **Target stability**: std < 0.03 across folds
- **Multiple seeds**: Run each experiment with 3 random seeds to verify stability
- **Expected score trajectory**:
  - exp_005 (LDA topics): 0.70-0.74
  - exp_006 (Stanford cues): 0.73-0.77
  - exp_007 (FWLS ensemble): 0.78-0.82
  - exp_008 (full pipeline): 0.85-0.90
  - **Goal**: Reach 0.90+ with full ensemble, then fine-tune to 0.98

## Implementation Roadmap

**Experiment 005**: Add LDA topic modeling to current best pipeline
- Keep LightGBM, TF-IDF + SVD + numeric features
- Add 15 LDA topics as additional features
- Expected: 0.70-0.74 AUC

**Experiment 006**: Implement nuanced Stanford linguistic cues
- Add gratitude count, evidentiality count, reciprocity count
- Add sentiment scores, readability metrics
- Combine with LDA topics
- Expected: 0.73-0.77 AUC

**Experiment 007**: Build FWLS ensemble with 3-4 base models
- Train diverse base models (LightGBM variants, XGBoost)
- Implement meta-features and interaction terms
- Train ridge regression meta-learner
- Expected: 0.78-0.82 AUC

**Experiment 008**: Full pipeline with all enhancements
- Combine LDA + Stanford cues + advanced tabular features
- FWLS ensemble with 4-5 base models
- Hyperparameter optimization
- Expected: 0.85-0.90 AUC

**Subsequent experiments**: Continue refining based on results, add more base models, try different meta-learners, optimize ensemble weights.

## Confidence Assessment
- **High confidence** in LDA topic modeling (proven by winners)
- **High confidence** in nuanced Stanford cues (academically validated)
- **Medium-high confidence** in FWLS (theoretically sound, used by winners)
- **Medium confidence** in exact score improvements (estimates based on competition post-mortems)
- **Overall**: This roadmap should bridge the gap to gold, but requires 5-10 experiments to fully implement and optimize