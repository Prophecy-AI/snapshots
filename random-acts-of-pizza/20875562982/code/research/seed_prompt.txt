## Current Status
- Best CV: 0.6445 from exp_002 (Enhanced Text Representation)
- Experiments above gold: 0 (gold threshold: 0.9791)
- **Gap: 0.3346 points** - we need massive improvement
- **Critical issue**: Using logistic regression when tree-based models are required

## Response to Evaluator
- **Technical verdict was CONCERNS** - I AGREE completely. The leakage risk and convergence issues are real and must be addressed.
- **Evaluator's top priority: Switch to LightGBM/XGBoost** - This is absolutely correct. My analysis confirms logistic regression is fundamentally mismatched to this problem.
- **Key concerns raised**:
  1. **Model-approach mismatch (HIGHEST)**: Confirmed by analysis showing 11,158 features vs 2,878 samples (3.9:1 ratio). Logistic regression cannot handle this effectively.
  2. **Data leakage**: Confirmed - TF-IDF/SVD fitted on full data leaks validation information. Must move fitting inside CV loops.
  3. **Numerical instability**: Confirmed - Convergence warnings indicate poor conditioning from unscaled features + high dimensionality.
  4. **Feature scaling**: Confirmed - Features range from 0-1 (TF-IDF) to 0-5000 (text_length) to 0-100k (upvotes_per_comment).
  5. **Dimensionality**: Confirmed - 11,158 features with 2,878 samples is extreme overfitting risk.

## Data Understanding
- **Reference notebooks**: 
  - `exploration/evolver_loop3_analysis.ipynb` - Comprehensive analysis of model mismatch, leakage, dimensionality
  - `exploration/evolver_loop2_analysis.ipynb` - Why linguistic features failed, TF-IDF superiority
  - `exploration/eda.ipynb` - Initial data patterns
- **Key patterns to exploit**:
  - TF-IDF captures nuanced patterns better than hand-crafted features (proven in exp_001 vs exp_000)
  - Character n-grams capture stylistic patterns (exp_002 showed modest +0.0059 gain)
  - SVD dimensionality reduction is effective but needs optimization
  - Tabular metadata (account age, karma, activity ratios) has signal but needs proper scaling/integration
  - **Critical insight**: The features from exp_002 are actually good - they're just being used with the wrong model

## Recommended Approaches (Priority Order)

### 1. Model Upgrade to LightGBM (CRITICAL PRIORITY)
**Why**: Tree-based models handle mixed data types, capture non-linear interactions, robust to unscaled features, better regularization for high dimensionality. Expected gain: +0.03 to +0.08 AUC.

**Implementation**:
- Replace LogisticRegression with LGBMClassifier
- Use class_weight='balanced' or scale_pos_weight=3 (for 24.8% positive class)
- Hyperparameters (competition-tested for text classification):
  - n_estimators=1000
  - learning_rate=0.05
  - num_leaves=64
  - max_depth=7
  - min_child_samples=50
  - feature_fraction=0.8
  - bagging_fraction=0.8
  - bagging_freq=5
  - early_stopping_rounds=50

### 2. Fix Data Leakage (CRITICAL PRIORITY)
**Why**: Current approach fits TF-IDF and SVD on full data before CV splits, leaking validation information. This inflates CV scores and reduces generalization.

**Implementation**:
- Move ALL feature fitting inside CV loop
- For each fold:
  - Fit TF-IDF vectorizer ONLY on train_idx
  - Fit SVD transformer ONLY on train_idx  
  - Transform both train_idx and val_idx using fitted transformers
  - Train model on transformed training data
  - Evaluate on transformed validation data
- Use sklearn Pipeline to ensure proper workflow

### 3. Optimize Dimensionality Reduction (HIGH PRIORITY)
**Why**: Current 150 SVD components (100 word + 50 char) is excessive. Analysis shows SVD(75) explains 17.8% variance which is sufficient. Reduce overfitting risk.

**Implementation**:
- Reduce to 50 word + 25 char components = 75 total
- Total features: 75 + 8 numeric = 83 (vs current 11,158)
- Ratio: 83/2878 = 2.9% (vs current 387%)
- This aligns with competition winner practices

### 4. Feature Engineering Refinements (MEDIUM PRIORITY)
**Why**: Current numeric features are good but can be improved. LightGBM will handle them better than logistic regression.

**Implementation**:
- Keep: log transforms for count features, ratios, account_age_years, text_length, word_count
- Add: Interaction features (text_length × karma, account_age × activity)
- Add: Temporal features (hour of day, day of week from timestamps)
- Add: Missing value flags
- LightGBM will automatically capture non-linear interactions

### 5. Validation Framework (HIGH PRIORITY)
**Why**: Need robust CV that prevents leakage and provides reliable score estimates.

**Implementation**:
- Stratified 5-fold CV (maintain 24.8% positive class)
- Use sklearn Pipeline to encapsulate all transformations
- Track both mean and std of CV scores
- Run multiple random seeds to verify stability
- Monitor feature importance to ensure signal quality

### 6. Ensemble Strategy (MEDIUM PRIORITY - Future)
**Why**: Single model unlikely to reach gold threshold. Need diverse base models.

**Implementation for later experiments**:
- Train multiple LightGBM models with different:
  - SVD component counts (50, 75, 100)
  - Feature subsets (text-only, tabular-only, combined)
  - Hyperparameter variations
- Use stacking with meta-learner (e.g., logistic regression)
- Blend predictions from diverse models

## What NOT to Try
- **Logistic regression**: Proven inadequate for this problem (evaluator confirmed)
- **More linguistic regex patterns**: Already failed (exp_001), TF-IDF is superior
- **Hyperparameter tuning without model change**: Won't bridge 0.33 point gap
- **Increasing dimensionality**: Current 11,158 features is already excessive
- **Complex neural networks**: Dataset too small (2,878 samples), traditional ML with proper features is more appropriate

## Validation Notes
- **CV scheme**: Stratified 5-fold with proper leakage prevention (Pipeline)
- **Target stability**: std < 0.03 across folds
- **Expected scores**: 
  - Experiment 004 (LightGBM + fix leakage): 0.68-0.72
  - Experiment 005 (optimized features): 0.75-0.80
  - Experiment 006 (ensembling): 0.82-0.85
- **Confidence**: High - based on competition post-mortems and proven techniques
- **Risk**: Low - these are standard best practices, not experimental approaches

## Implementation Details
**Experiment 004**: LightGBM with fixed leakage, optimized dimensionality (75 SVD components), current numeric features
**Experiment 005**: Add refined tabular features, interaction terms, temporal features
**Experiment 006**: Ensemble multiple LightGBM variants with stacking

**Success criteria**: Achieve 0.70+ AUC in exp_004 to validate the approach. If we don't see at least +0.03 improvement, re-evaluate feature quality.