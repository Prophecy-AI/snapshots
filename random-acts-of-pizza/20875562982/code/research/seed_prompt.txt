## Current Status
- Best CV: 0.6386 from exp_000 (Baseline TF-IDF + Logistic Regression)
- Experiments above gold: 0 (gold threshold: 0.9791)
- **Gap: 0.3405 points** - we need massive improvement

## Data Understanding
- **Reference notebooks**: See `exploration/eda.ipynb` for initial analysis, `exploration/evolver_loop1_analysis.ipynb` for feature correlations
- **Key patterns discovered**:
  - Text length and word count have highest correlation (0.12) but all features are weak
  - Upvotes log has 0.11 correlation
  - All correlations <0.13 indicate current features capture little signal
  - **Critical finding**: Academic research (Stanford ICWSM 2014 paper) identifies specific linguistic patterns that predict success: need-based narratives, gratitude expressions, evidential language, reciprocity promises, status signals

## Recommended Approaches (Priority Order)

### 1. Linguistic Feature Engineering (HIGHEST PRIORITY)
Based on academic research, extract these patterns from text:
- **Need narratives**: Detect mentions of family hardship, job loss, financial strain, student status, medical issues
- **Gratitude expressions**: Count "thank you", "thanks", "appreciate", "grateful"
- **Evidential language**: Look for concrete details (numbers, dates, specific situations)
- **Reciprocity promises**: Detect "pay it forward", "help others", "contribute back"
- **Status signals**: Extract Reddit-specific status mentions (karma, account age references)
- **Sentiment analysis**: Use VADER or TextBlob for sentiment scores
- **Readability metrics**: Flesch-Kincaid, SMOG index, word diversity

### 2. Enhanced Text Representation (HIGH PRIORITY)
- **Dimensionality reduction**: Apply TruncatedSVD (LSA) to TF-IDF matrix to capture latent semantics (50-100 components)
- **Character n-grams**: Add 2-3 character n-grams to capture stylistic patterns
- **POS tagging**: Extract noun/verb/adjective ratios, sentiment-bearing word counts
- **Emotion lexicons**: Use NRC Emotion Lexicon or similar for emotion scores

### 3. Expanded Tabular Features (MEDIUM PRIORITY)
- **User history features**: Calculate requester's prior activity patterns
- **Temporal features**: Extract day of week, time of day from timestamps
- **Interaction features**: Create combinations of text length Ã— user karma, etc.
- **Missing value patterns**: Create flags for missing data patterns

### 4. Model Upgrade (MEDIUM PRIORITY)
- **Tree-based models**: Try XGBoost, LightGBM, CatBoost (handle mixed data better)
- **Ensemble strategy**: Combine logistic regression (for linear patterns) with tree models (for non-linear)
- **Stacking**: Use diverse base models with meta-learner

### 5. Validation & Robustness (ONGOING)
- **CV stability**: Run multiple seeds to ensure consistent scores
- **Feature ablation**: Test which feature groups provide most lift
- **Error analysis**: Continue analyzing false positives/negatives

## What NOT to Try
- More complex models without better features first (won't bridge 0.34 point gap)
- Simple hyperparameter tuning (diminishing returns with weak features)
- Basic text statistics (already exhausted)

## Validation Notes
- Use stratified 5-fold CV (maintain 24.8% positive class distribution)
- Track both mean and std of CV scores (target std <0.03 for stability)
- Monitor for overfitting given small dataset (2,878 samples)