{
  "query": "What text preprocessing techniques are effective for Reddit posts and social media language in classification tasks?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Effective text\u2011preprocessing for Reddit posts and other social\u2011media messages typically combines noise\u2011reduction steps that preserve the informal cues (emoticons, slang, elongated words) that are most informative for classification.  Empirical studies on short\u2011text platforms show that **stemming**, **removing numeric tokens**, and **normalising elongated words** (e.g., turning \u201csoooo\u201d into \u201cso\u201d) consistently raise accuracy, whereas stripping punctuation often hurts performance because punctuation can carry sentiment or sarcasm cues\u202f([springer.com](https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31)).\n\nAt the lexical level, a simple **tokenisation** pipeline\u2014followed by **lower\u2011casing**, optional **lemmatisation**, and selective **multi\u2011word grouping** (e.g., \u201cNew\u202fYork\u201d \u2192 \u201cNewYork\u201d)\u2014provides a solid baseline for both traditional bag\u2011of\u2011words and neural classifiers.  Experiments indicate that plain tokenisation is usually sufficient, but the exact impact varies across datasets, so it is worth testing lemmatisation and multi\u2011word handling on a case\u2011by\u2011case basis\u202f([arxiv.org](https://arxiv.org/abs/1707.01780)).  Classic bag\u2011of\u2011words pipelines also benefit from **stop\u2011word removal** and **stemming**, as demonstrated in a systematic evaluation of preprocessing on classification performance\u202f([plos.org](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232525)).\n\nBecause Reddit and Twitter texts contain platform\u2011specific artifacts, a dedicated toolkit such as **textPrep** recommends additional steps: stripping or normalising **URLs**, **user mentions**, **subreddit tags**, and **emoji**; expanding common **internet slang**; and handling **markdown formatting**.  These operations reduce noise while retaining expressive symbols that improve downstream topic\u2011modeling or sentiment\u2011analysis models\u202f([churchill.io](https://www.churchill.io/papers/textprep.pdf)).  Overall, the most effective pipelines are those that (1) clean obvious noise (URLs, numbers, excessive punctuation), (2) normalise informal orthography (elongations, slang, emojis), and (3) apply lightweight linguistic normalization (lower\u2011casing, optional stemming/lemmatisation) before feeding the text to the classifier.",
      "url": ""
    },
    {
      "title": "On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis",
      "text": "[View PDF](https://arxiv.org/pdf/1707.01780)\n\n> Abstract:Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.\n\n## Submission history\n\nFrom: Jose Camacho-Collados \\[ [view email](https://arxiv.org/show-email/e7d215e6/1707.01780)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1707.01780v1)**\nThu, 6 Jul 2017 13:31:13 UTC (119 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1707.01780v2)**\nFri, 7 Jul 2017 09:41:25 UTC (119 KB)\n\n**\\[v3\\]**\nThu, 23 Aug 2018 10:05:33 UTC (43 KB)",
      "url": "https://arxiv.org/abs/1707.01780"
    },
    {
      "title": "",
      "text": "textPrep: A Text Preprocessing Toolkit for Topic Modeling on\nSocial Media Data\nRob Churchill and Lisa Singh\nGeorgetown University\nKeywords:\ntext preprocessing, topic modeling, data science, social media, textPrep\nAbstract:\nWith the rapid growth of social media in recent years, there has been considerable effort toward\nunderstanding the topics of online discussions. Unfortunately, state of the art topic models tend\nto perform poorly on this new form of data, due to their noisy and unstructured nature. There\nhas been a lot of research focused on improving topic modeling algorithms, but very little focused\non improving the quality of the data that goes into the algorithms. In this paper, we formalize the\nnotion of preprocessing configurations and propose a standardized, modular toolkit and pipeline\nfor performing preprocessing on social media texts for use in topic models. We perform topic\nmodeling on three different social media data sets and in the process show the importance of\npreprocessing and the usefulness of our preprocessing pipeline when dealing with different social\nmedia data. We release our preprocessing toolkit code (textPrep) in a python package for others\nto use for advancing research on data mining and machine learning on social media text data.\n1 INTRODUCTION\nWith over 500 million\ntweets [InternetLiveStats, 2021], over 300 million\nFacebook Stories, and 500 million Instagram sto\u0002ries daily [Noyes, 2020], social media represents a\nlarge stream of new data creation. Even smaller\nsocial media sites like Reddit sees billions of posts\nand comments every year [Foundation, 2021].\nPeople are publicly sharing their thoughts and\nopinions on different topics of interest. Unfortu\u0002nately, it is challenging to determine the topics\nof these public posts because of high levels of\nnoise, varying grammatical structures, and short\ndocument lengths.\nFigure 1 shows examples of topics identified\nfrom tweets by state of the art topic models dur\u0002ing the 2016 US Presidential election. When the\nentire tweet is used as input into a topic model\u0002ing algorithm (the first three word clouds in Fig\u0002ure 1), we see that the topics contain stopwords,\nhashtags, user handles, plural words, and even\nmisspellings. The last word cloud (bottom right)\nuses preprocessed tweets and does not contain the\nsame amount of noise. We can determine that it\nis about Trump refusing to release his tax returns.\nWhile a great deal of effort has been spent creat\u0002ing topic models with social media data in mind,\nlittle attention has been paid to the impact of\npreprocessing decisions made prior to generating\ntopic models.\nResearchers have found that many traditional\nstate of the art topic models perform poorly when\nlittle or no preprocessing occurs. Some topic\nmodels miss topics entirely. Others find topics,\nbut the topics are often polluted with a large\nnumber of noise words [Churchill et al., 2018]. To\nfurther exacerbate the situation, even though\nthere are vast semantic differences in the types of\ndata topic models are used on, research papers do\nnot preprocess data consistently, and sometimes\nfail to say whether they do at all. This gives the\nimpression that preprocessing does not matter for\ntopic modeling. Or at a minimum, the choice of\npreprocessing does not matter.\nThis paper investigates the role of preprocess\u0002ing, specifically for identifying high quality topics.\nGiven a document collection D, for each docu\u0002ment Di\nin D, we tokenize Di on whitespace to\nget a series of n tokens Di = {d1, d2, . . . , dn}. To\u0002kens may be terms, punctuation, numbers, web\naddresses, emojis, etc. We ask two questions.\nFirst, which tokens should be removed prior to\ntopic model creation? Second, how can we de-\ntermine if we have done a good job preprocess\u0002ing? To help systematically conduct preprocess\u0002ing and assess the effectiveness of different prepro\u0002cessing decisions, we present textPrep, a toolkit\nfor preprocessing text data. Second, to demon\u0002strate its value and the importance of preprocess\u0002ing, we identify preprocessing rules and arrange\nthese rules into preprocessing configurations that\ngenerate different data sets for use by topic mod\u0002eling algorithms.\nWe find that preprocessing has significant ef\u0002fects on topic model performance, but that mod\u0002els and data sets are not equally affected by the\nsame amounts and types of preprocessing. Some\nmodels and data sets are more positively affected\nthan others, and in some cases, preprocessing can\nhurt model performance. In general, for our case\nstudies, doing more thorough preprocessing helps\nmodel performance far more than it hurts. Fi\u0002nally, we find that while certain preprocessing\nmethods can appear to produce similar quality\ndata sets, the quality of topics that are gener\u0002ated on these data sets can diverge quickly for\nless apt configurations. Our hope is that by build\u0002ing an easy to use toolkit and demonstrating the\nimpact of certain preprocessing rules and configu\u0002rations on the quality of topics generated by state\nof the art topic modeling algorithms on noisy so\u0002cial media data sets, more data scientists and re\u0002searchers will add preprocessing analysis to their\ntopic modeling pipeline, thereby enhancing their\nunderstanding of the role played by preprocess\u0002ing.\nThe contributions of this paper are as\nfollows: 1) we make available a Python package\nfor topic model preprocessing that gives users the\nability to easily customize preprocessing configu\u0002rations 2) we define and formalize a preprocess\u0002ing taxonomy that combines useful preprocessing\nrules and configurations, 3) we propose a sim\u0002ple preprocessing methodology that applies con\u0002Figure 1: Topic Word Clouds\nfigurations of rules to document tokens to gener\u0002ate better quality data sets that can be used by\ntopic modeling algorithms, 4) we conduct exten\u0002sive empirical case studies of preprocessing config\u0002urations on three large social media data sets, and\nevaluate the data quality and topic quality of each\nconfiguration using three different topic models,\nand 5) we summarize our findings through a set\nof best practices that will help those less familiar\nwith topic modeling determine which approaches\nto use with which algorithms.\n2 RELATED LITERATURE\nPreprocessing. In the early 2000s, there were\na handful of papers related to data preprocessing\nfrom the database community that focused\non enabling users to better understand the\nquality of their data set [Vassiliadis et al., 2000,\nRaman and Hellerstein, 2001], and describing\ndata quality issues focused on storage and prun\u0002ing [Rahm and Do, 2000, Knoblock et al., 2003].\nMore recently, researchers have shown\nthe impact of preprocessing on text clas\u0002sification [Srividhya and Anitha, 2010,\nUysal and Gunal, 2014]. Allahyari et al.\nmention text preprocessing in their survey of\ntext mining, but do not evaluate any meth\u0002ods [Allahyari et al., 2017]. Our work considers a\nmuch larger set of preprocessing approaches and\nfocuses on an unsupervised topic modeling task\nas opposed to a supervised text classification\ntask. Denny and Spirling analyze the effects of\npreprocessing political text data sets on multiple\ndifferent text classification tasks, including topic\nmodeling [Denny and Spirling, 2018]. However,\nthey only analyze the effects on Latent Dirichlet\nAllocation (LDA), and the data sets that they\nuse are smaller than our study, with 2000 doc\u0002uments being the largest data set size in their\nstudy. The authors main goal is to analyze the\ndifference between supervised and unsupervised\nlearning on political texts.\nIn the only other paper related to preprocess\u0002ing and topic model performance, Schofield et al.\nanalyze the effectiveness of removing stopwords\nfrom data sets before performing topic model\u0002ing [Schofield et al., 2017]. They find that stop\u0002word removal is very helpful to topic model per\u0002formance. This approach is informative but only\nassesses one preprocessing rule and uses speech\nand newspaper text, not social media text. Our\nwork extends this literature by providin...",
      "url": "https://www.churchill.io/papers/textprep.pdf"
    },
    {
      "title": "PLOS One",
      "text": "The influence of preprocessing on text classification using a bag-of-words representation | PLOS One\n[Skip to main content](#main-content)\nAdvertisement\nBrowse Subject Areas\n?\nClick through the PLOS taxonomy to find articles in your field.\nFor more information about PLOS Subject Areas, click[here](https://github.com/PLOS/plos-thesaurus/blob/master/README.md).\n[](#)[](#)\n* Loading metrics\nOpen Access\nPeer-reviewed\nResearch Article\n# The influence of preprocessing on text classification using a bag-of-words representation\n* Yaakov HaCohen-Kerner,\nRolesInvestigation,\nWriting \u2013original draft\n\\* E-mail:[kerner@jct.ac.il](mailto:kerner@jct.ac.il)\nAffiliationDept. of Computer Science, Jerusalem College of Technology - Lev Academic Center, Jerusalem, Israel\n[![ORCID logo](https://journals.plos.org/resource/img/orcid_16x16.png)http://orcid.org/0000-0002-4834-1272](http://orcid.org/0000-0002-4834-1272)\n&#x02A2F;\n* Daniel Miller,\nRolesSoftware\nAffiliationDept. of Computer Science, Jerusalem College of Technology - Lev Academic Center, Jerusalem, Israel\n[![ORCID logo](https://journals.plos.org/resource/img/orcid_16x16.png)http://orcid.org/0000-0002-0403-6807](http://orcid.org/0000-0002-0403-6807)\n&#x02A2F;\n* Yair Yigal\nRolesSoftware\nAffiliationDept. of Computer Science, Jerusalem College of Technology - Lev Academic Center, Jerusalem, Israel\n&#x02A2F;\n# The influence of preprocessing on text classification using a bag-of-words representation\n* Yaakov HaCohen-Kerner,\n* Daniel Miller,\n* Yair Yigal\n![PLOS](https://journals.plos.org/resource/img/logo-plos-full-color.svg)\nx\n* Published: May 1, 2020\n* [https://doi.org/10.1371/journal.pone.0232525](https://doi.org/10.1371/journal.pone.0232525)\n* * [Article](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232525)\n* [Authors](https://journals.plos.org/plosone/article/authors?id=10.1371/journal.pone.0232525)\n* [Metrics](https://journals.plos.org/plosone/article/metrics?id=10.1371/journal.pone.0232525)\n* [Comments](https://journals.plos.org/plosone/article/comments?id=10.1371/journal.pone.0232525)\n* [Media Coverage](http://plos.altmetric.com/details/doi/10.1371/journal.pone.0232525)\n* [Peer Review](https://journals.plos.org/plosone/article/peerReview?id=10.1371/journal.pone.0232525)\n* [Reader Comments](article/comments?id=10.1371/journal.pone.0232525)\n* [Figures](#)\n## Figures\n![Table 1](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t001)\n![Table 2](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t002)\n![Fig 1](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.g001)\n![Table 3](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t003)\n![Table 4](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t004)\n![Table 5](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t005)\n![Table 6](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t006)\n![Table 7](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t007)\n![Table 8](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t008)\n![Table 9](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t009)\n![Table 10](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t010)\n![Table 11](https://journals.plos.org/plosone/article/figure/image?size=inline&amp;id=10.1371/journal.pone.0232525.t011)\n## Abstract\nText classification (TC) is the task of automatically assigning documents to a fixed number of categories. TC is an important component in many text applications. Many of these applications perform preprocessing. There are different types of text preprocessing, e.g., conversion of uppercase letters into lowercase letters, HTML tag removal, stopword removal, punctuation mark removal, lemmatization, correction of common misspelled words, and reduction of replicated characters. We hypothesize that the application of different combinations of preprocessing methods can improve TC results. Therefore, we performed an extensive and systematic set of TC experiments (and this is our main research contribution) to explore the impact of all possible combinations of five/six basic preprocessing methods on four benchmark text corpora (and not samples of them) using three ML methods and training and test sets. The general conclusion (at least for the datasets verified) is that it is always advisable to perform an extensive and systematic variety of preprocessing methods combined with TC experiments because it contributes to improve TC accuracy. For all the tested datasets, there was always at least one combination of basic preprocessing methods that could be recommended to significantly improve the TC using a BOW representation. For three datasets, stopword removal was the only single preprocessing method that enabled a significant improvement compared to the baseline result using a bag of 1,000-word unigrams. For some of the datasets, there was minimal improvement when we removed HTML tags, performed spelling correction or removed punctuation marks, and reduced replicated characters. However, for the fourth dataset, the stopword removal was not beneficial. Instead, the conversion of uppercase letters into lowercase letters was the only single preprocessing method that demonstrated a significant improvement compared to the baseline result. The best result for this dataset was obtained when we performed spelling correction and conversion into lowercase letters. In general, for all the datasets processed, there was always at least one combination of basic preprocessing methods that could be recommended to improve the accuracy results when using a bag-of-words representation.\n**Citation:**HaCohen-Kerner Y, Miller D, Yigal Y (2020) The influence of preprocessing on text classification using a bag-of-words representation. PLoS ONE 15(5):\ne0232525.\nhttps://doi.org/10.1371/journal.pone.0232525\n**Editor:**Weinan Zhang, National University of Singapore, SINGAPORE\n**Received:**September 27, 2019;**Accepted:**April 16, 2020;**Published:**May 1, 2020\n**Copyright:**\u00a9 2020 HaCohen-Kerner et al. This is an open access article distributed under the terms of the[Creative Commons Attribution License](http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n**Data Availability:**All R8 files are available from the[http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)All WebKB files are available from the[http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)are also avilable[http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/)All WebKB files are also available from the[http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)All SMS Spam Collection v.1 files are available from the[http://www.dt.fee.unicamp.br/\\~tiago//smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago//smsspamcollection/)All Sentiment Labelled Sentences files are available from the[https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#)All Python 3 Spelling Corrector files are available from the[https://github.com/phatpiglet/autocorrect/](https://github.com/p...",
      "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232525"
    },
    {
      "title": "Enhancing Sentiment Analysis Models for Reddit and Twitter Data",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd9e8310a393c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akash.hiremath25%2Fenhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akash.hiremath25%2Fenhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Enhancing Sentiment Analysis Models for Reddit and Twitter Data\n\n[Akash.](https://medium.com/@akash.hiremath25?source=post_page---byline--d9e8310a393c---------------------------------------)\n\n7 min read\n\n\u00b7\n\nOct 22, 2023\n\n--\n\nListen\n\nShare\n\nUsing Twitter and Reddit datasets to analyze sentiments and build a predictive model.\n\n**Sentiment analysis**, also known as opinion mining, is a natural language processing (NLP) technique used to determine the sentiment or emotion expressed in a piece of text. It involves analyzing text data to classify it as either positive, negative, or neutral in terms of sentiment.\n\nLet\u2019s take raw comments and classify them into positive, negative, and neutral.\n\n> Importing Libraries\n\n```\nimport reimport matplotlib.pyplot as pltimport nltkimport pandas as pdfrom nltk.corpus import stopwordsfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n```\n\n> Loading data\n\n```\ndf = pd.read_excel(\"Data Analyst(Test Data).xlsx\")df.head()\n```\n\nOutput\n\n> Data Preprocessing\n\n```\ndf.isna().sum()\n```\n\n> Dropping rows with NaN values\n\n```\ndf = df.dropna(subset=[\"Review\"])  # Removing row with no review givendf.shape\n```\n\n## NLP on Data\n\n**Stop words** are commonly used words in natural language that are considered to be of little value in tasks like text analysis or natural language processing (NLP). These words are extremely common and occur frequently across different texts. Examples of stop words in English include \u201cthe,\u201d \u201cand,\u201d \u201cin,\u201d \u201cof,\u201d \u201cto,\u201d and so on.\n\nBy removing stop words, the focus shifts to the more meaningful and contextually relevant words, which can improve the performance of NLP models.\n\n> Remove Stop Words\n\n```\nnltk.download(\"stopwords\") #Downloads from webSTOPWORDS = stopwords.words(\"english\")def clean_text_nltk(text):    text = text.lower()    text = re.sub(r\"[^0-9a-zA-Z]\", \" \", text)    text = re.sub(r\"\\s+\", \" \", text)    filtered_tokens = \" \".join(word for word in text.split() if word not in STOPWORDS)    return filtered_tokens\n```\n\n> Applying the function on DataFrame\n\n```\ndf.loc[:, \"Review\"] = df.Review.apply(clean_text_nltk)df.head()\n```\n\n> Getting sentiments for the corpus\n\n```\nsia = SentimentIntensityAnalyzer()sia.polarity_scores(df.iloc[0][\"Review\"])[\"compound\"]#Output0.9325\n```\n\n- Polarity\\_scores returns positive, negative, neutral and compound scores, but we use compound which is normalized score of positive, negativeand neutral scores.\n- compound: This is a composite score that is calculated using a formula that normalizes the scores. It ranges from -1.0 to 1.0, where -1.0 means extremely negative, 1.0 means extremely positive, and 0.0 means neutral. In this case, the compound score is 0.9325, which indicates a very positive sentiment.\n\n> Storing sentimental for each rows in separate column\n\n```\ndef sentiment_score(text):    score = sia.polarity_scores(text)[\"compound\"]    if score > 0:        return \"Positive\"    elif score < 0:        return \"Negative\"    else:        return \"Neutral\"df[\"category\"] = df.Review.apply(sentiment_score)df.head()\n```\n\nPress enter or click to view image in full size\n\nOutput\n\n## Now Sentinment analysis for preprocessed dataset\n\nDataset Link - [Reddit\\_Twitter\\_data](https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset)\n\n> Importing Libraries\n\n```\nimport osimport reimport shutilimport kaggleimport matplotlib.pyplot as pltimport nltkimport pandas as pdimport seaborn as snsimport spacyfrom imblearn import under_samplingfrom nltk.corpus import stopwordsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics import classification_report, confusion_matrixfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import GaussianNBfrom wordcloud import WordCloud\n```\n\n> Downloading dataset\n\n```\nos.makedirs(\"datasets/\", exist_ok=True)if \"Reddit_Data.csv\" or \"Twitter_Data.csv\" not in os.listdir(\"datasets/\"):    kaggle.api.dataset_download_files(        \"cosmos98/twitter-and-reddit-sentimental-analysis-dataset\"    )    shutil.unpack_archive(        \"twitter-and-reddit-sentimental-analysis-dataset.zip\", extract_dir=\"./datasets\"    )\n```\n\n> Reading Data using pandas library\n\n```\n# reading datareddit_df = pd.read_csv(\"./datasets/Reddit_Data.csv\")reddit_df.rename({\"clean_comment\": \"comment\"}, inplace=True, axis=\"columns\")twitter_df = pd.read_csv(\"./datasets/Twitter_Data.csv\", nrows=40000)twitter_df.rename({\"clean_text\": \"tweet\"}, inplace=True, axis=\"columns\")\n```\n\n> Reddit data\n\n```\nreddit_df.head()\n```\n\nTwitter data\n\n```\ntwitter_df.head()\n```\n\nHere -1 is negative, 0 is neutral and 1 is positive comment/tweet in the dataset.\n\n> Checking reddit reviews categories distribution\n\n```\nlabels = [\"Positive\", \"Neutral\", \"Negative\"]plt.title(\"Reddit Categories\")plt.pie(    reddit_df.category.value_counts(),    labels=labels,    autopct=\"%.2f%%\",);\n```\n\n> Checking twitter reviews categories distribution\n\n```\nplt.title(\"Twitter Categories\")plt.pie(    twitter_df.category.value_counts(),    labels=labels,    autopct=\"%.2f%%\",);\n```\n\n## NLTK and Spacy\n\nThe usual cleaning process in NLP involves:\n\n- Remove missing value if any.\n- Remove unwanted character like punctuations.\n- Replace all the Uppercase to lowercase as machine treat them differently but we knw meaning of \u2018cat\u2019 and \u2018CAT\u2019 is same.\n- Remove type of words that follow a certain pattern like link, email, or username, these words does not contribute much in analysis and can be removed from description with he help of regular expression.\n- Remove all the stopwords like pronoun, articles etc. these words occur in very huge number in any sentence but does not contribute much in NLP analysis and thus can be removed.\n- At last Changing the verb form to its root form. example :- root word for \u2018Playing\u2019 and \u2018Played\u2019 will be \u2018Play\u2019\n\n> Removing stop words using NLTK\n\n```\n# nltk.download(\"stopwords\")STOPWORDS = stopwords.words(\"english\")def clean_text_nltk(text):    text = text.lower()    text = re.sub(r\"[^0-9a-zA-Z]\", \" \", text)    text = re.sub(r\"\\s+\", \" \", text)    filtered_tokens = \" \".join(word for word in text.split() if word not in STOPWORDS)    return filtered_tokensreddit_df.loc[:, \"comment\"] = reddit_df.comment.apply(clean_text_nltk)reddit_df.head()\n```\n\n> Removing stop words using Spacy\n\n```\ntry:    nlp = spacy.load(\"en_core_web_sm\")except:    os.system(\"python -m spacy download en_core_web_sm\")def clean_text_spacy(text):    text = text.lower()    text = re.sub(r\"[^0-9a-zA-Z]\", \" \", text)    text = re.sub(r\"\\s+\", \" \", text)    doc = nlp(text)    filtered_tokens = [token.text for token in doc if not token.is_stop]    return filtered_tokenstweet_ = twitter_df.loc[:2000]tweet_.tweet.apply(clean_text_spacy)\n```\n\n> Lemmatizing using spacy\n\n```\nnlp = spac...",
      "url": "https://medium.com/@akash.hiremath25/enhancing-sentiment-analysis-models-for-reddit-and-twitter-data-d9e8310a393c"
    },
    {
      "title": "A comparison of text preprocessing techniques for hate and offensive speech detection in Twitter",
      "text": "<div><div><h2>References</h2><div><ul><li><p>Alam S, Yao N (2019) The impact of preprocessing steps on the accuracy of machine learning algorithms in sentiment analysis. Comput Math Org Theory 25:319\u2013335</p><p><a href=\"https://link.springer.com/doi/10.1007/s10588-018-9266-8\">Article</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20impact%20of%20preprocessing%20steps%20on%20the%20accuracy%20of%20machine%20learning%20algorithms%20in%20sentiment%20analysis&amp;journal=Comput%20Math%20Org%20Theory&amp;doi=10.1007%2Fs10588-018-9266-8&amp;volume=25&amp;pages=319-335&amp;publication_year=2019&amp;author=Alam%2CS&amp;author=Yao%2CN\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Alfina I, Mulia R, Fanany MI, Ekanata Y (2017) Hate speech detection in the Indonesian language: a dataset and preliminary study. In: 2017 international conference on advanced computer science and information systems (ICACSIS). IEEE, pp 233\u2013238</p></li><li><p>Alonso P, Saini R, Kovacs G (2020) TheNorth at SemEval-2020 task 12: hate speech detection using Roberta. In: Proceedings of the fourteenth workshop on semantic evaluation, pp 2197\u20132202</p></li><li><p>Alrehili A (2019) Automatic hate speech detection on social media: a brief survey. In: 2019 IEEE/ACS 16th international conference on computer systems and applications (AICCSA). IEEE, pp 1\u20136</p></li><li><p>Alshalan R, Al-Khalifa H (2020) A deep learning approach for automatic hate speech detection in the Saudi Twittersphere. Appl Sci 10(23):8614</p><p><a href=\"https://doi.org/10.3390%2Fapp10238614\">Article</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20deep%20learning%20approach%20for%20automatic%20hate%20speech%20detection%20in%20the%20Saudi%20Twittersphere&amp;journal=Appl%20Sci&amp;doi=10.3390%2Fapp10238614&amp;volume=10&amp;issue=23&amp;publication_year=2020&amp;author=Alshalan%2CR&amp;author=Al-Khalifa%2CH\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Ameer I, Siddiqui MHF, Sidorov G, Gelbukh A (2019) CIC at SemEval-2019 task 5: simple yet very efficient approach to hate speech detection, aggressive behavior detection, and target classification in Twitter. In: Proceedings of the 13th international workshop on semantic evaluation, pp 382\u2013386</p></li><li><p>Angiani G, Ferrari L, Fontanini T, Fornacciari P, Iotti E, Magliani F, Manicardi S (2016) A comparison between preprocessing techniques for sentiment analysis in twitter. In: KDWeb</p></li><li><p>Ashraf N, Rafiq A, Butt S, Shehzad HMF, Sidorov G, Gelbukh AF (2022) Youtube based religious hate speech and extremism detection dataset with machine learning baselines. J Intell Fuzzy Syst 42:4769\u20134777</p><p><a href=\"https://doi.org/10.3233%2FJIFS-219264\">Article</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Youtube%20based%20religious%20hate%20speech%20and%20extremism%20detection%20dataset%20with%20machine%20learning%20baselines&amp;journal=J%20Intell%20Fuzzy%20Syst&amp;doi=10.3233%2FJIFS-219264&amp;volume=42&amp;pages=4769-4777&amp;publication_year=2022&amp;author=Ashraf%2CN&amp;author=Rafiq%2CA&amp;author=Butt%2CS&amp;author=Shehzad%2CHMF&amp;author=Sidorov%2CG&amp;author=Gelbukh%2CAF\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Badjatiya P, Gupta S, Gupta M, Varma V (2017) Deep learning for hate speech detection in tweets. In: Proceedings of the 26th international conference on world wide web companion, pp 759\u2013760</p></li><li><p>Bai Q, Dan Q, Mu Z, Yang M (2019) A systematic review of emoji: current research and future perspectives. Front Psychol 10:2221</p><p><a href=\"https://doi.org/10.3389%2Ffpsyg.2019.02221\">Article</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20systematic%20review%20of%20emoji%3A%20current%20research%20and%20future%20perspectives&amp;journal=Front%20Psychol&amp;doi=10.3389%2Ffpsyg.2019.02221&amp;volume=10&amp;publication_year=2019&amp;author=Bai%2CQ&amp;author=Dan%2CQ&amp;author=Mu%2CZ&amp;author=Yang%2CM\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Balouchzahi F, Shashirekha H (2020) Las for hasoc-learning approaches for hate speech and offensive content identification. In: FIRE (working notes), pp 145\u2013151</p></li><li><p>Banerjee S, Sarkar M, Agrawal N, Saha P, Das M (2021) Exploring transformer based models to identify hate speech and offensive content in English and Indo-Aryan languages. arXiv preprint <a href=\"http://arxiv.org/abs/2111.13974\">arXiv:2111.13974</a></p></li><li><p>Barbieri F, Camacho-Collados J, Espinosa Anke L, Neves L (2020) TweetEval: unified benchmark and comparative evaluation for tweet classification. In: Findings of the association for computational linguistics: EMNLP 2020, pp. 1644\u20131650. Association for Computational Linguistics. <a href=\"https://doi.org/10.18653/v1/2020.findings-emnlp.148\">https://doi.org/10.18653/v1/2020.findings-emnlp.148</a> . <a href=\"https://aclanthology.org/2020.findings-emnlp.148\">https://aclanthology.org/2020.findings-emnlp.148</a></p></li><li><p>Baruah A, Barbhuiya F, Dey K (2019) ABARUAH at SemEval-2019 task 5: bi-directional LSTM for hate speech detection. In: Proceedings of the 13th international workshop on semantic evaluation, pp 371\u2013376</p></li><li><p>Basile V, Bosco C, Fersini E, Debora N, Patti V, Pardo FMR, Rosso P, Sanguinetti M (2019) SemEval-2019 task 5: multilingual detection of hate speech against immigrants and women in Twitter. In: 13th international workshop on semantic evaluation. Association for Computational Linguistics, pp 54\u201363</p></li><li><p>Bhandari A, Shah SB, Thapa S, Naseem U, Nasim M (2023) CrisisHateMM: multimodal analysis of directed and undirected hate speech in text-embedded images from Russia-Ukraine conflict. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR) workshops, pp 1993\u20132002</p></li><li><p>Bird S (2006) NLTK: the natural language toolkit. In: Proceedings of the COLING/ACL 2006 interactive presentation sessions, pp 69\u201372</p></li><li><p>B\u00f6l\u00fcc\u00fc N, Canbay P (2021) Hate speech and offensive content identification with graph convolutional networks. In: Forum for information retrieval evaluation (working notes)(FIRE), CEUR-WS.org, pp 44\u201351</p></li><li><p>Caselli T, Basile V, Mitrovi\u0107 J, Granitzer M (2021) HateBERT: retraining BERT for abusive language detection in English. In: Proceedings of the 5th workshop on online abuse and harms (WOAH 2021), pp 17\u201325</p></li><li><p>Caselli T, Basile V, Mitrovi\u0107 J, Kartoziya I, Granitzer M (2020) I feel offended, don\u2019t be abusive! implicit/explicit messages in offensive and abusive language. In: Proceedings of the 12th language resources and evaluation conference, pp 6193\u20136202</p></li><li><p>Chollet F et al. Keras. <a href=\"https://github.com/fchollet/keras\">https://github.com/fchollet/keras</a></p></li><li><p>Conneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzm\u00e1n F, Grave \u00c9, Ott M, Zettlemoyer L, Stoyanov V (2020) Unsupervised cross-lingual representation learning at scale. In: Proceedings of the 58th annual meeting of the association for computational linguistics, pp 8440\u20138451</p></li><li><p>Das AK, Al Asif A, Paul A, Hossain MN (2021) Bangla hate speech detection on social media using attention-based recurrent neural network. J Intell Syst 30(1):578\u2013591</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Bangla%20hate%20speech%20detection%20on%20social%20media%20using%20attention-based%20recurrent%20neural%20network&amp;journal=J%20Intell%20Syst&amp;volume=30&amp;issue=1&amp;pages=578-591&amp;publication_year=2021&amp;author=Das%2CAK&amp;author=Al%20Asif%2CA&amp;author=Paul%2CA&amp;author=Hossain%2CMN\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Davidson T, Bhattacharya D, Weber I (2019) Racial bias in hate speech and abusive language detection datasets. In: Proceedings of the third workshop on abusive language online, pp 25\u201335</p></li><li><p>Davidson T, Warmsley D, Macy M, Weber I (2017) Automated hate speech detection and the problem of offensive language. In: Proceedings of the international AAAI c...",
      "url": "https://link.springer.com/article/10.1007/s13278-023-01156-y?error=cookies_not_supported&code=068b37cf-fc80-4e93-9fc7-593ef1270b0c"
    },
    {
      "title": "A survey of pre-processing techniques to improve short-text quality: a case study on hate speech detection on twitter",
      "text": "# A survey of pre-processing techniques to improve short-text quality: a case study on hate speech detection on twitter\n\n- Published: 04 November 2020\n\n- Volume\u00a080,\u00a0pages 35239\u201335266, (2021)\n- [Cite this article](https://link.springer.com/article/10.1007/s11042-020-10082-6?error=cookies_not_supported&code=3e01eb25-6483-46d6-8715-777cce2d941f#citeas)\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/journal/11042?as=webp)Multimedia Tools and Applications](https://link.springer.com/journal/11042) [Aims and scope](https://link.springer.com/journal/11042/aims-and-scope) [Submit manuscript](https://www.editorialmanager.com/mtap/)\n\n## Abstract\n\nPre-processing plays an essential role in disambiguating the meaning of short-texts, not only in applications that classify short-texts but also for clustering and anomaly detection. Pre-processing can have a considerable impact on overall system performance; however, it is less explored in the literature in comparison to feature extraction and classification. This paper analyzes twelve different pre-processing techniques on three pre-classified Twitter datasets on hate speech and observes their impact on the classification tasks they support. It also proposes a systematic approach to text pre-processing to apply different pre-processing techniques in order to retain features without information loss. In this paper, two different word-level feature extraction models are used, and the performance of the proposed package is compared with state-of-the-art methods. To validate gains in performance, both traditional and deep learning classifiers are used. The experimental results suggest that some pre-processing techniques impact negatively on performance, and these are identified, along with the best performing combination of pre-processing techniques.\n\nThis is a preview of subscription content, [log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11042-020-10082-6%3Ferror%3Dcookies_not_supported%26code%3D3e01eb25-6483-46d6-8715-777cce2d941f) to check access.\n\n## Access this article\n\n[Log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11042-020-10082-6%3Ferror%3Dcookies_not_supported%26code%3D3e01eb25-6483-46d6-8715-777cce2d941f)\n\nBuy article PDF USD 39.95\n\nPrice excludes VAT (USA)\n\nTax calculation will be finalised during checkout.\n\nInstant access to the full article PDF.\n\nRent this article via DeepDyve\n\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/journals)\n\n**Fig. 1**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11042-020-10082-6/MediaObjects/11042_2020_10082_Fig1_HTML.png)\n\n**Fig. 2**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11042-020-10082-6/MediaObjects/11042_2020_10082_Fig2_HTML.png)\n\n**Fig. 3**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11042-020-10082-6/MediaObjects/11042_2020_10082_Fig3_HTML.png)\n\n**Fig. 4**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11042-020-10082-6/MediaObjects/11042_2020_10082_Fig4_HTML.png)\n\n### Similar content being viewed by others\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs13278-023-01156-y/MediaObjects/13278_2023_1156_Figb_HTML.png)\n\n### [A comparison of text preprocessing techniques for hate and offensive speech detection in Twitter](https://link.springer.com/10.1007/s13278-023-01156-y?fromPaywallRec=true)\n\nArticle01 December 2023\n\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-24340-0?as=webp)\n\n### [A Study of\u00a0Text Representations for\u00a0Hate Speech Detection](https://link.springer.com/10.1007/978-3-031-24340-0_32?fromPaywallRec=true)\n\nChapter\u00a9 2023\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00521-022-07745-w/MediaObjects/521_2022_7745_Fig1_HTML.png)\n\n### [Vietnamese hate and offensive detection using PhoBERT-CNN and social media streaming data](https://link.springer.com/10.1007/s00521-022-07745-w?fromPaywallRec=true)\n\nArticle17 September 2022\n\n## Notes\n\n01. Hate speech is defined by Cambridge Dictionary as \u201cpublic speech that expresses hate or encourages violence towards a person or group based on something such as race, religion, sex, or sexual orientation\u201d.\n\n02. [https://github.com/sloria/TextBlob](https://github.com/sloria/TextBlob)\n\n03. [http://norvig.com/spell-correct.html](http://norvig.com/spell-correct.html)\n\n04. [https://www.nltk.org/api/nltk.html](https://www.nltk.org/api/nltk.html)\n\n05. [https://github.com/scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn)\n\n06. [https://github.com/explosion/spaCy](https://github.com/explosion/spaCy)\n\n07. [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)\n\n08. [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/)\n\n09. [https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/)\n\n10. [https://github.com/cjlin1/liblinear](https://github.com/cjlin1/liblinear)\n\n11. [https://machinelearningmastery.com/prepare-movie-review-data-sentiment-analysis/](https://machinelearningmastery.com/prepare-movie-review-data-sentiment-analysis/)\n\n12. [http://noisy-text.github.io/](http://noisy-text.github.io/)\n\n13. [https://github.com/cbaziotis/ekphrasis](https://github.com/cbaziotis/ekphrasis)\n\n14. [https://pypi.org/project/pycontractions/](https://pypi.org/project/pycontractions/)\n\n15. [https://pythonprogramming.net/lemmatizing-nltk-tutorial/](https://pythonprogramming.net/lemmatizing-nltk-tutorial/)\n\n16. [https://gist.github.com/sebleier/554280](https://gist.github.com/sebleier/554280)\n\n17. [http://norvig.com/spell-correct.html](http://norvig.com/spell-correct.html)\n\n18. [https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy)\n\n\n## References\n\n01. Agarwal A, Xie B, Vovsha I, Rambow O, Rebecca J (2011) Passonneau. sentiment analysis of twitter data\n\n02. Alomari E, Mehmood R, Katib I (2019) Road traffic event detection using twitter data, machine learning, and apache spark. In: 2019 IEEE SmartWorld, ubiquitous intelligence & computing, advanced & trusted computing, scalable computing & communications, cloud & big data computing, internet of people and smart city innovation (Smart- World/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), IEEE, pp 1888\u20131895\n\n03. Alotaibi S, Mehmood R, Katib I, Rana O, Albeshri A (2020) Sehaa: a big data analytics tool for healthcare symptoms and diseases detection using twitter, apache spark, and machine learning. Appl Sci 10(4):1398\n\n    [Article](https://doi.org/10.3390%2Fapp10041398) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Sehaa%3A%20a%20big%20data%20analytics%20tool%20for%20healthcare%20symptoms%20and%20diseases%20detection%20using%20twitter%2C%20apache%20spark%2C%20and%20machine%20learning&journal=Appl%20Sci&doi=10.3390%2Fapp10041398&volume=10&issue=4&publication_year=2020&author=Alotaibi%2CS&author=Mehmood%2CR&author=Katib%2CI&author=Rana%2CO&author=Albeshri%2CA)\n\n04. Balahur A (2013) Sentiment analysis in social media texts. In: WASSA@NAACL-HLT\n\n05. Bao Y, Quan C, Wang L, Ren F (2014) The role of pre-processing in twitter sentiment analysis. In: Huang D-S, Jo K-H, Ling Wang (eds) Intelligent computing methodologies. Springer International Publishing, Cham, pp 615\u2013624\n\n06. Boia M, Faltings B, Musat CC, Pu P (2013) A: is worth a thousand words: how people attach sentiment to emoticons and words in tweets. In: 2013 international conference on social computing, pp 345\u2013350\n\n07. Davidson T, Warmsley D, Macy MW, Weber I Automated hate speech detection and the problem of offensive language. arXiv: [04009.2017](http://arxiv.org/abs/04009.2017)\n\n08. Dos Santos CN, de C. Gatti MA (2014) Deep convolutional neural networks for sentiment an...",
      "url": "https://link.springer.com/article/10.1007/s11042-020-10082-6?error=cookies_not_supported&code=3e01eb25-6483-46d6-8715-777cce2d941f"
    },
    {
      "title": "A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis",
      "text": "# A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis\n\n- Conference paper\n- First Online: 02 September 2017\n\n- pp 394\u2013406\n- [Cite this conference paper](https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31?error=cookies_not_supported&code=642d80e0-126f-4e4f-b93f-8409e86be34c#citeas)\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/book/978-3-319-67008-9?as=webp)Research and Advanced Technology for Digital Libraries](https://link.springer.com/book/10.1007/978-3-319-67008-9)(TPDL 2017)\n\n## Abstract\n\nPre-processing is considered to be the first step in text classification, and choosing the right pre-processing techniques can improve classification effectiveness. We experimentally compare 15 commonly used pre-processing techniques on two Twitter datasets. We employ three different machine learning algorithms, namely, Linear SVC, Bernoulli Na\u00efve Bayes, and Logistic Regression, and report the classification accuracy and the resulting number of features for each pre-processing technique. Finally, based on our results, we categorize these techniques based on their performance. We find that techniques like stemming, removing numbers, and replacing elongated words improve accuracy, while others like removing punctuation do not.\n\nThis is a preview of subscription content, [log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-67008-9_31%3Ferror%3Dcookies_not_supported%26code%3D642d80e0-126f-4e4f-b93f-8409e86be34c) to check access.\n\n## Access this chapter\n\n[Log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-67008-9_31%3Ferror%3Dcookies_not_supported%26code%3D642d80e0-126f-4e4f-b93f-8409e86be34c)\n\nChapter\n\nUSD\u00a029.95\n\nPrice excludes VAT (USA)\n\n- Available as PDF\n- Read on any device\n- Instant download\n- Own it forever\n\nBuy Chapter\n\neBookUSD\u00a039.99Price excludes VAT (USA)\n\n- Available as EPUB and PDF\n- Read on any device\n- Instant download\n- Own it forever\n\nBuy eBook\n\nSoftcover BookUSD\u00a054.99Price excludes VAT (USA)\n\n- Compact, lightweight edition\n- Dispatched in 3 to 5 business days\n- Free shipping worldwide - [see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)\n\nBuy Softcover Book\n\nTax calculation will be finalised at checkout\n\nPurchases are for personal use only\n\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n\n## Notes\n\n1. 1.\n\n\n[http://noisy-text.github.io/](http://noisy-text.github.io/).\n\n2. 2.\n\n\n[http://norvig.com/spell-correct.html](http://norvig.com/spell-correct.html).\n\n3. 3.\n\n\n[http://sentistrength.wlv.ac.uk](http://sentistrength.wlv.ac.uk).\n\n4. 4.\n\n\n[http://alt.qcri.org/semeval2017/](http://alt.qcri.org/semeval2017/).\n\n\n## References\n\n01. Agarwal, A., Xie, B., Vovsha, I., Rambow, O., Passonneau, R.: Sentiment analysis of twitter data. In: Proceedings of the Workshop on Languages in Social Media, LSM 2011, Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 30\u201338 (2011). [http://dl.acm.org/citation.cfm?id=2021109.2021114](http://dl.acm.org/citation.cfm?id=2021109.2021114)\n\n02. Bird, S.: NLTK: the natural language toolkit. In: Calzolari, N., Cardie, C., Isabelle, P. (eds.) ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, Sydney, Australia, 17\u201321 July 2006. The Association for Computer Linguistics (2006). [http://aclweb.org/anthology/p06-4018](http://aclweb.org/anthology/p06-4018)\n\n03. Cherkassky, V.: The nature of statistical learning theory. IEEE Trans. Neural Netw. **8**(6), 1564 (1997). doi: [10.1109/TNN.1997.641482](https://doi.org/10.1109/TNN.1997.641482)\n\n    [Article](https://doi.org/10.1109%2FTNN.1997.641482) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=The%20nature%20of%20statistical%20learning%20theory&journal=IEEE%20Trans.%20Neural%20Netw.&doi=10.1109%2FTNN.1997.641482&volume=8&issue=6&publication_year=1997&author=Cherkassky%2CV)\n\n04. Fayyad, U.M., Piatetsky-Shapiro, G., Uthurusamy, R.: Summary from the KDD-03 panel: data mining: the next 10 years. SIGKDD Explor. **5**(2), 191\u2013196 (2003). doi: [10.1145/980972.981004](https://doi.org/10.1145/980972.981004)\n\n    [Article](https://doi.org/10.1145%2F980972.981004) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Summary%20from%20the%20KDD-03%20panel%3A%20data%20mining%3A%20the%20next%2010%20years&journal=SIGKDD%20Explor.&doi=10.1145%2F980972.981004&volume=5&issue=2&pages=191-196&publication_year=2003&author=Fayyad%2CUM&author=Piatetsky-Shapiro%2CG&author=Uthurusamy%2CR)\n\n05. John, G.H., Langley, P.: Estimating continuous distributions in bayesian classifiers. In: UAI 1995: Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence, Montreal, Quebec, Canada, 18\u201320 August 1995, pp. 338\u2013345 (1995). [https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article\\_id=450&proceeding\\_id=11](https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=450&proceeding_id=11)\n\n06. Lin, C., He, Y.: Joint sentiment/topic model for sentiment analysis. In: Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, 2\u20136 November 2009, pp. 375\u2013384 (2009). [http://doi.acm.org/10.1145/1645953.1646003](http://doi.acm.org/10.1145/1645953.1646003)\n\n07. Miller, G.A.: WordNet: a lexical database for english. Commun. ACM **38**(11), 39\u201341 (1995). doi: [10.1145/219717.219748](https://doi.org/10.1145/219717.219748)\n\n    [Article](https://doi.org/10.1145%2F219717.219748) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=WordNet%3A%20a%20lexical%20database%20for%20english&journal=Commun.%20ACM&doi=10.1145%2F219717.219748&volume=38&issue=11&pages=39-41&publication_year=1995&author=Miller%2CGA)\n\n08. Mohammad, S., Kiritchenko, S., Zhu, X.: NRC-Canada: building the state-of-the-art in sentiment analysis of tweets. In: Proceedings of the 7th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2013, Atlanta, Georgia, USA, 14\u201315 June 2013, pp. 321\u2013327 (2013). [http://aclweb.org/anthology/S/S13/S13-2053.pdf](http://aclweb.org/anthology/S/S13/S13-2053.pdf)\n\n09. Mohammad, S.M., Zhu, X., Kiritchenko, S., Martin, J.D.: Sentiment, emotion, purpose, and style in electoral tweets. Inf. Process. Manage. **51**(4), 480\u2013499 (2015). doi: [10.1016/j.ipm.2014.09.003](https://doi.org/10.1016/j.ipm.2014.09.003)\n\n    [Article](https://doi.org/10.1016%2Fj.ipm.2014.09.003) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Sentiment%2C%20emotion%2C%20purpose%2C%20and%20style%20in%20electoral%20tweets&journal=Inf.%20Process.%20Manage.&doi=10.1016%2Fj.ipm.2014.09.003&volume=51&issue=4&pages=480-499&publication_year=2015&author=Mohammad%2CSM&author=Zhu%2CX&author=Kiritchenko%2CS&author=Martin%2CJD)\n\n10. Mullen, T., Malouf, R.: A preliminary investigation into sentiment analysis of informal political discourse. In: Computational Approaches to Analyzing Weblogs, Papers from the 2006 AAAI Spring Symposium, Technical Report SS-06-03, Stanford, California, USA, 27\u201329 March 2006, pp. 159\u2013162 (2006). [http://www.aaai.org/Library/Symposia/Spring/2006/ss06-03-031.php](http://www.aaai.org/Library/Symposia/Spring/2006/ss06-03-031.php)\n\n11. Na, J.C., Sui, H., Khoo, C., Chan, S., Zhou, Y.: Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews. In: Conference of the International Society for Knowledge Organization (ISKO), pp. 49\u201354 (2004)\n\n    [Google Scholar](https://scholar.google.com/scholar?&q=Na%2C%20J.C.%2C%20Sui%2C%20H.%2C%20Khoo%2C%20C.%2C%20Chan%2C%20S.%2C%20Zhou%2C%20Y.%3A%20Effectiveness%20of%20simple%20linguistic%20processing%20in%20automatic...",
      "url": "https://link.springer.com/chapter/10.1007/978-3-319-67008-9_31?error=cookies_not_supported&code=642d80e0-126f-4e4f-b93f-8409e86be34c"
    },
    {
      "title": "",
      "text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5799\u20135810\nJuly 5 - 10, 2020. \rc 2020 Association for Computational Linguistics\n5799\nA Comprehensive Analysis of Preprocessing for Word Representation\nLearning in Affective Tasks\nNastaran Babanejad, Ameeta Agrawal, Aijun An, Manos Papagelis\nDepartment of Electrical Engineering and Computer Science,\nYork University, Toronto, Canada\n{nasba, ameeta, aan, papaggel}@eecs.yorku.ca\nAbstract\nAffective tasks such as sentiment analysis,\nemotion classification and sarcasm detection\nhave been popular in recent years due to abun\u0002dance of user-generated data, accurate com\u0002putational linguistic models, and broad range\nof relevant applications in various domains.\nAt the same time, many studies have high\u0002lighted the importance of text preprocessing,\nas an integral step to any natural language\nprocessing prediction model and downstream\ntask. While preprocessing in affective systems\nis well-studied, preprocessing in word vector\nbased models applied to affective systems, is\nnot. To address this limitation, we conduct a\ncomprehensive analysis of the role of prepro\u0002cessing techniques in affective analysis based\non word vector models. Our analysis is the\nfirst of its kind and provides useful insights\nof the importance of each preprocessing tech\u0002nique when applied at the training phase, com\u0002monly ignored in pretrained word vector mod\u0002els, and/or at the downstream task phase.\n1 Introduction\nAffective tasks such as sentiment analysis, emo\u0002tion classification and sarcasm detection have en\u0002joyed great popularity in recent years. This success\ncan be largely attributed to the fundamental and\nstraightforward nature of the methods employed,\nthe availability of vast amounts of user-generated\nnatural language data, and the wide range of useful\napplications, spanning from hate speech detection\nto monitoring the sentiment of financial markets\nand news recommendation (Djuric et al., 2015; Ba\u0002banejad et al., 2019). Most early models of affect\nanalysis employed pretrained word embeddings\nthat have been obtained under the assumption of\nthe distributional hypothesis (Mikolov et al., 2013;\nDevlin et al., 2018). The distributional hypothesis\nsuggests that two words occurring frequently in\nsimilar linguistic contexts tend to be more semanti\u0002cally similar, and therefore should be represented\ncloser to one another in the embedding space. How\u0002ever, while such embeddings are useful for several\nnatural language processing (NLP) downstream\ntasks, they are known to be less suitable for affec\u0002tive tasks in particular (Tang et al., 2014; Agrawal\net al., 2018). Although some authors claim that\nthere is a need for post-processing word embed\u0002dings for affective tasks, others find that off-the\u0002shelf vectors are very powerful for affective lexicon\nlearning (Lison and Kutuzov, 2017). For example,\nword2vec (Mikolov et al., 2013) estimates the\npair of words \u2018happy\u2019 and \u2018sad\u2019 to be more similar\nthan the pair of words \u2018happy\u2019 and \u2018joy\u2019, which\nis counterintuitive, and might affect the accuracy\nperformance of the models that depend on it.\nTo address the limitations of traditional word em\u0002beddings, several techniques have been proposed,\nincluding task-specific fine-tuning (Devlin et al.,\n2018), retrofitting (Faruqui et al., 2014), represent\u0002ing emotion with vectors using a multi-task training\nframework (Xu et al., 2018) and generating affec\u0002tive word embeddings (Felbo et al., 2017), to name\na few. Other attempts to overcome the limitation\nof word vectors include optimization of hyperpa\u0002rameters (Levy et al., 2015), as well as fine-tuned\npreprocessing strategies tailored to different NLP\ntasks. While these strategies have demonstrated\nevidence of improving the accuracy performance\nin tasks such as word similarity, word analogy, and\nothers (Lison and Kutuzov, 2017), their effect in af\u0002fective tasks has not received considerable attention\nand remains less explored. Our work is motivated\nby the observation that preprocessing factors such\nas stemming, stopwords removal and many others\nmake up an integral part of nearly every improved\ntext classification model, and affective systems in\nparticular (Danisman and Alpkocak, 2008; Patil\nand Patil, 2013). However, little work has been\n5800\nFigure 1: Framework of applying preprocessing in different stages in affective systems; (a) Pre, (b) Post.\ndone towards understanding the role of preprocess\u0002ing techniques applied to word embeddings in dif\u0002ferent stages of affective systems. To address this\nlimitation, the overarching goal of this research, is\nto perform an extensive and systematic assessment\nof the effect of a range of linguistic preprocess\u0002ing factors pertaining to three affective tasks, in\u0002cluding sentiment analysis, emotion classification\nand sarcasm detection. Towards that end, we sys\u0002tematically analyze the effectiveness of applying\npreprocessing to large training corpora before learn\u0002ing word embeddings, an approach that has largely\nbeen overlooked by the community. We investigate\nthe following research questions: (i) what is the ef\u0002fect of integrating preprocessing techniques earlier\ninto word embedding models, instead of later on\nin a downstream classification models? (ii) which\npreprocessing techniques yield the most benefit in\naffective tasks? (iii) does preprocessing of word\nembeddings provide any improvement over state\u0002of-the-art pretrained word embeddings? and if yes,\nhow much?\nFigure 1 illustrates the difference between a) pre\u0002processing word embeddings pipeline (Pre) vs. b)\npreprocessing classification dataset pipeline (Post),\nwhere preprocessing techniques in (a) are applied\nto the training corpus of the model and in (b) only\nto the classification dataset. In brief, the main con\u0002tributions of our work are as follows:\n\u2022 We conduct a comprehensive analysis of the\nrole of preprocessing techniques in affective\ntasks (including sentiment analysis, emotion\nclassification and sarcasm detection), employ\u0002ing different models, over nine datasets;\n\u2022 We perform a comparative analysis of the ac\u0002curacy performance of word vector models\nwhen preprocessing is applied at the training\nphase (training data) and/or at the downstream\ntask phase (classification dataset). Interest\u0002ingly, we obtain best results when preprocess\u0002ing is applied only to the training corpus or\nwhen it is applied to both the training corpus\nand the classification dataset of interest.\n\u2022 We evaluate the performance of our best pre\u0002processed word vector model against state-of\u0002the-art pretrained word embedding models;\n\u2022 We make source code and data publicly avail\u0002able to encourage reproducibility of results1\n.\nThe rest of the paper is organized as follows:\nSection 2 presents an overview of the related work.\nSection 3 elaborates on the preprocessing tech\u0002niques employed in the evaluation of models. Sec\u0002tion 4 describes the experimental evaluation frame\u0002work. In Section 5 a comprehensive analysis of the\nresults is provided. Section 6 concludes the paper\nwith key insights of the research.\n2 Related Work\nIn this section, we present an overview of related\nwork on preprocessing classification datasets and\npreprocessing word embeddings, and how our work\naims to bridge the gap between those efforts.\n2.1 Preprocessing Classification Datasets\nPreprocessing is a vital step in text mining and\ntherefore, evaluation of preprocessing techniques\nhas long been a part of many affective systems.\nSaif et al. (2014) indicated that, despite its popular\nuse in Twitter sentiment analysis, the use of pre\u0002compiled stoplist has a negative impact on the clas\u0002sification performance. Angiani et al. (2016) ana\u0002lyzed various preprocessing methods such as stop\u0002words removal, stemming, negation, emoticons,\nand so on, and found stemming to be most effec\u0002tive for the task of sentiment analysis. Similarly,\nSymeonidis et al. (2018) found that lemmatization\nincreases accuracy. Jianqiang and Xiaolin (2017)\nobserved that removing stopwords, numbers, and\n...",
      "url": "https://aclanthology.org/2020.acl-main.514.pdf"
    }
  ]
}