{
  "query": "How do Kaggle winners handle severe class imbalance in binary classification competitions with 25% positive class?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kaggle winners typically combine several proven tactics to tame a 75\u202f%/25\u202f% class\u2011imbalance in binary contests\u202f\u2014\u202fthey focus on the right metric, balance the data, weight the loss, engineer robust features, and blend strong models\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/473546);\u202f[Medium guide](https://medium.com/data-science/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a);\u202f[Neptune blog](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).\n\n1. **Pick an imbalance\u2011aware evaluation metric** \u2013 use AUC\u2011ROC, PR\u2011AUC or log\u2011loss (which penalises mis\u2011classifying the minority) rather than plain accuracy\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/473546)).  \n\n2. **Resample the training set** \u2013  \n   * **Undersample** the majority class to reduce its dominance,  \n   * **Oversample** the minority class or generate synthetic examples with **SMOTE**/ADASYN\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/473546);\u202f[Medium guide](https://medium.com/data-science/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a)).  \n\n3. **Apply class\u2011weighting or scale\u2011pos\u2011weight** in the loss function (e.g., `class_weight` in scikit\u2011learn, `scale_pos_weight` in LightGBM/XGBoost) so the model treats the 25\u202f% positives as equally important\u202f([TensorFlow tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)).  \n\n4. **Engineer features that boost signal for the minority** \u2013 use target encoding, polynomial/interaction features, and careful scaling/transformations; these tricks are repeatedly highlighted in Kaggle\u2011winning notebooks\u202f([Neptune blog](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).  \n\n5. **Use robust, high\u2011capacity models** \u2013 gradient\u2011boosted trees (LightGBM, XGBoost) and neural nets that handle weighted loss well; tune depth, learning rate, and regularisation to avoid over\u2011fitting the minority\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/473546)).  \n\n6. **Stratified cross\u2011validation** \u2013 split data preserving the 25\u202f%/75\u202f% ratio in each fold to get reliable estimates and avoid leakage\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/473546)).  \n\n7. **Ensemble & blend** \u2013 stack or blend several differently\u2011resampled models (e.g., one trained on undersampled data, another on SMOTE\u2011augmented data) to capture complementary patterns and push the leaderboard score\u202f([Kaggle\u202fQ&A](https://www.kaggle.com/questions-and-answers/473546)).  \n\nFollowing these steps mirrors the workflow of past Kaggle champions when confronting a severely imbalanced binary target.",
      "url": ""
    },
    {
      "title": "Dealing with extremely imbalanced dataset - binary classification",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/questions-and-answers/473546"
    },
    {
      "title": "Class Imbalance Strategies \u2014 A Visual Guide with Code - Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8bc8fae71e1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-8bc8fae71e1a---------------------------------------)\n\n\u00b7\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\n# Class Imbalance Strategies \u2014 A Visual Guide with Code\n\n## Understand Random Undersampling, Oversampling, SMOTE, ADASYN, and Tomek Links\n\n[Travis Tang](https://medium.com/@travis-tang?source=post_page---byline--8bc8fae71e1a---------------------------------------)\n\n13 min read\n\n\u00b7\n\nApr 24, 2023\n\n--\n\n1\n\nListen\n\nShare\n\nClass imbalance occurs when one class in a classification problem significantly outweighs the other class. It\u2019s common in many machine learning problems. Examples include fraud detection, anomaly detection, and medical diagnosis.\n\n## The Curse of Class Imbalance\n\nA model trained on an imbalanced dataset perform poorly on the minority class. At best, this can cause loss to the business in the case of a churn analysis. At worst, it can pervade systemic bias of a face recognition system.\n\nPress enter or click to view image in full size\n\nA balanced dataset might just be the missing ingredient (Source: Elena Mozhvilo on [Unsplash](https://unsplash.com/photos/j06gLuKK0GM))\n\nThe common approach to class imbalance is resampling. These can entail oversampling the majority class, undersampling the minority class, or a combination of both.\n\nIn this post, I use vivid visuals and code to illustrate these strategies for class imbalance:\n\n1. Random oversampling\n2. Random undersampling\n3. Oversampling with SMOTE\n4. Oversampling with ADASYN\n5. Undersampling with Tomek Link\n6. Oversampling with SMOTE, then undersample with TOMEK Link (SMOTE-Tomek)\n\nI will also be using these strategies on a real-world dataset, and evaluate their impact on a machine learning model. Let\u2019s go.\n\n> All source code is here.\n\n## Using Imbalance-learn\n\nWe will use the `imbalanced-learn` package in python to solve our imbalanced class problem. It is an open-sourced library relying on scikit-learn and provides tools when dealing with classification with imbalanced classes.\n\nTo install it, use the command.\n\n```\npip install -U imbalanced-learn\n```\n\n## Dataset\n\nThe dataset that we are using is the [**Communities and Crime Data Set by UCI (CC BY 4.0)**.](https://archive.ics.uci.edu/ml/datasets/communities+and+crime) It contains 100 attributes of 1994 U.S. communities. We can use this to predict if the **crime rate is high** (defined as having **per capita violent crime** above 0.65). The data source is available in the UCI Machine Learning Repository and is created by Michael Redmond from La Salle University (Published in 2009).\n\n> The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units.\n\nThis dataset is imbalanced. It has 12 communities with low crime rates for every 1 community of high crime rate. This is perfect to illustrate our use case.\n\n```\n>>> from imblearn.datasets import fetch_datasets>>> # Fetch dataset from imbalanced-learn library >>> # as a dictionary of numpy array>>> us_crime = fetch_datasets()['us_crime']>>> us_crime{'data': array([[0.19, 0.33, 0.02, ..., 0.26, 0.2 , 0.32],        [0.  , 0.16, 0.12, ..., 0.12, 0.45, 0.  ],        [0.  , 0.42, 0.49, ..., 0.21, 0.02, 0.  ],        ...,        [0.16, 0.37, 0.25, ..., 0.32, 0.18, 0.91],        [0.08, 0.51, 0.06, ..., 0.38, 0.33, 0.22],        [0.2 , 0.78, 0.14, ..., 0.3 , 0.05, 1.  ]]), 'target': array([-1,  1, -1, ..., -1, -1, -1]), 'DESCR': 'us_crime'}\n```\n\nWe will convert this dictionary to a Pandas dataframe, then split it into train-test splits.\n\n```\n# Convert the dictionary to a pandas dataframecrime_df = pd.concat([pd.DataFrame(us_crime['data'], columns = [f'data_{i}' for i in range(us_crime.data.shape[1])]),           pd.DataFrame(us_crime['target'], columns = ['target'])], axis = 1)# Split data into train test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(crime_df.drop('target', axis = 1),                                                     crime_df['target'],                                                     test_size = 0.4,                                                     random_state = 42)\n```\n\nNote that we will only perform under- and over-sampling only on the train dataset. We will _not_ change the test sets with under- and over-sampling.\n\n### Preprocessing the dataset\n\nOur goal is to have a visualize an imbalanced dataset. In order to visualize the 128-dimensional dataset in a 2D graph, we do the following on the train set.\n\n- scale the dataset,\n- perform Principle Component Analysis (PCA) on the features to convert the 100 features to 2 principle components,\n- visualize the data.\n\nHere\u2019s the data, visualized in 2D.\n\nImage by author\n\nCode for the above graph:\n\n```\nfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt# Scale the dataset on both train and test sets.# Note that we fit MinMaxScaler on X_train only, not on the entire dataset.# This prevents data leakage from test set to train set.scaler = MinMaxScaler()scaler.fit(X_train)X_train = scaler.transform(X_train)X_test = scaler.transform(X_test)# Perform PCA Decomposition on both train and test sets# Note that we fit PCA on X_train only, not on the entire dataset.# This prevents data leakage from test set to train set.pca = PCA(n_components=2)pca.fit(X_train)X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)# Function for plotting dataset def plot_data(X,y,ax,title):    ax.scatter(X[:, 0], X[:, 1], c=y, alpha=0.5, s = 30, edgecolor=(0,0,0,0.5))    ax.set_ylabel('Principle Component 1')    ax.set_xlabel('Principle Component 2')    if title is not None:        ax.set_title(title)# Plot datasetfig,ax = plt.subplots(figsize=(5, 5))plot_data(X_train_pca, y_train, ax, title='Original Dataset')\n```\n\nWith the preprocessing done, we are ready to resample our dataset.\n\n## Strategy 1. Random Oversampling\n\nRandom oversampling duplicates existing examples from the minority class with replacement. Each data point in the minority class has an equal probability of being duplicated.\n\nPress enter or click to view image in full size\n\nImage by author\n\nHere\u2019s how to we can perform oversampling on our dataset.\n\n```\nfrom imblearn.over_sampling import RandomOverSampler# Perform random oversamplingros = RandomOverSampler(random_state=0)X_train_ros, ...",
      "url": "https://medium.com/data-science/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle ...",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Classification on imbalanced data \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Classification on imbalanced data | TensorFlow Core[Skip to main content](#main-content)\n[![TensorFlow](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/tensorflow/images/lockup.svg)](https://www.tensorflow.org/)\n* /\n* English\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4[GitHub](https://github.com/tensorflow)Sign in\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n* [TensorFlow](https://www.tensorflow.org/)\n* [Learn](https://www.tensorflow.org/learn)\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n# Classification on imbalanced dataStay organized with collectionsSave and categorize content based on your preferences.\n[![](https://www.tensorflow.org/images/tf_logo_32px.png)View on TensorFlow.org](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)|[![](https://www.tensorflow.org/images/colab_logo_32px.png)Run in Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/GitHub-Mark-32px.png)View source on GitHub](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/download_logo_32px.png)Download notebook](https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/imbalanced_data.ipynb)|\nThis tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use[Keras](https://www.tensorflow.org/guide/keras/overview)to define the model and[class weights](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model)to help the model learn from the imbalanced data. .\nThis tutorial contains complete code to:\n* Load a CSV file using Pandas.\n* Create train, validation, and test sets.\n* Define and train a model using Keras (including setting class weights).\n* Evaluate the model using various metrics (including precision and recall).\n* Select a threshold for a probabilistic classifier to get a deterministic classifier.\n* Try and compare with class weighted modelling and oversampling.## Setup\n```\n`importtensorflowastffromtensorflowimportkerasimportosimporttempfileimportmatplotlibasmplimportmatplotlib.pyplotaspltimportnumpyasnpimportpandasaspdimportseabornassnsimportsklearnfromsklearn.metricsimportconfusion\\_matrixfromsklearn.model\\_selectionimporttrain\\_test\\_splitfromsklearn.preprocessingimportStandardScaler`\n```\n```\n2024-08-20 01:23:52.305388: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 01:23:52.326935: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 01:23:52.333533: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n```\n`mpl.rcParams['figure.figsize']=(12,10)colors=plt.rcParams['axes.prop\\_cycle'].by\\_key()['color']`\n```\n## Data processing and exploration\n### Download the Kaggle Credit Card Fraud data set\nPandas is a Python library with many helpful utilities for loading and working with structured data. It can be used to download CSVs into a Pandas[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n**Note:**This dataset has been collected and analysed during a research collaboration of Worldline and the[Machine Learning Group](http://mlg.ulb.ac.be)of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available[here](https://www.researchgate.net/project/Fraud-detection-5)and the page of the[DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/)project\n```\n`file=tf.keras.utilsraw\\_df=pd.read\\_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')raw\\_df.head()`\n```\n```\n`raw\\_df[['Time','V1','V2','V3','V4','V5','V26','V27','V28','Amount','Class']].describe()`\n```\n### Examine the class label imbalance\nLet's look at the dataset imbalance:\n```\n`neg,pos=np.bincount(raw\\_df['Class'])total=neg+posprint('Examples:\\\\nTotal:{}\\\\nPositive:{}({:.2f}% of total)\\\\n'.format(total,pos,100\\*pos/total))`\n```\n```\nExamples:\nTotal: 284807\nPositive: 492 (0.17% of total)\n```\nThis shows the small fraction of positive samples.\n### Clean, split and normalize the data\nThe raw data has a few issues. First the`Time`and`Amount`columns are too variable to use directly. Drop the`Time`column (since it's not clear what it means) and take the log of the`Amount`column to reduce its range.\n```\n`cleaned\\_df=raw\\_df.copy()# You don't want the `Time` column.cleaned\\_df.pop('Time')# The `Amount` column covers a huge range. Convert to log-space.eps=0.001# 0 =&gt; 0.1\u00a2cleaned\\_df['Log Amount']=np.log(cleaned\\_df.pop('Amount')+eps)`\n```\nSplit the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where[overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)is a significant concern from the lack of training data.\n```\n`# Use a utility from sklearn to split and shuffle your dataset.train\\_df,test\\_df=train\\_test\\_split(cleaned\\_df,test\\_size=0.2)train\\_df,val\\_df=train\\_test\\_split(train\\_df,test\\_size=0.2)# Form np arrays of labels and features.train\\_labels=np.array(train\\_df.pop('Class')).reshape(-1,1)bool\\_train\\_labels=train\\_labels[:,0]!=0val\\_labels=np.array(val\\_df.pop('Class')).reshape(-1,1)test\\_labels=np.array(test\\_df.pop('Class')).reshape(-1,1)train\\_features=np.array(train\\_df)val\\_features=np.array(val\\_df)test\\_features=np.array(test\\_df)`\n```\nWe check whether the distribution of the classes in the three sets is about the same or not.\n```\n`print(f'Average class probability in training set:{train\\_labels.mean():.4f}')print(f'Average class probability in validation set:{val\\_labels.mean():.4f}')print(f'Average class probability in test set:{test\\_labels.mean():.4f}')`\n```\n```\nAverage class probability in training set: 0.0017\nAverage class probability in validation set: 0.0018\nAverage class probability in test set: 0.0018\n```\nGiven the small number of positive labels, this seems about right.\nNormalize the input features using the sklearn StandardScaler.\nThis will set the mean to 0 and standard deviation to 1.\n**Note:**The`StandardScaler`is only fit using the`train\\_features`to be sure the model is not peeking at the validation or test sets.\n```\n`scaler=StandardScaler()train\\_features=scaler.fit\\_transform(train\\_features)val\\_features=scaler.transform(val\\_features)test\\_features=scaler.transform(test\\_features)train\\_features=np.clip(train\\_features,-5,5)val\\_features=np.clip(val\\_features,-5,5)test\\_features=np.clip(test\\_features,-5,5)print('Training labels shape:',train\\_labels.shape)print('Validation labels shape:',...",
      "url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
    },
    {
      "title": "Understanding Class Imbalance in 1 Minute! - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/getting-started/467908"
    },
    {
      "title": "Is it OK to train a binary classifier using all the extremely imbalanced ...",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Is it OK to train a binary classifier using all the extremely imbalanced data if the majority class is negative?](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked5 years ago\n\nModified [5 years ago](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if?lastactivity)\n\nViewed\n1k times\n\n1\n\n$\\\\begingroup$\n\nI'm training a neural network as a binary classifier for text classification. The data is very imbalanced, where the ratio of TRUE:FALSE is approximately 100:10000\n\nIntuitively, it feels like using all the negative samples would prevent the classifier from learning invalid patterns (that might otherwise be learned using undersampling, for example).\n\nAm I underestimating the effect of the imbalance on classifier performance?\n\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [text](https://datascience.stackexchange.com/questions/tagged/text)\n- [binary](https://datascience.stackexchange.com/questions/tagged/binary)\n\n[Share](https://datascience.stackexchange.com/q/52978)\n\n[Improve this question](https://datascience.stackexchange.com/posts/52978/edit)\n\nFollow\n\nasked May 31, 2019 at 13:50\n\n[![Eric McLachlan's user avatar](https://lh4.googleusercontent.com/-MdNVPmjvtbs/AAAAAAAAAAI/AAAAAAAAN6A/Iy8t3D3jbWk/photo.jpg?sz=64)](https://datascience.stackexchange.com/users/68975/eric-mclachlan)\n\n[Eric McLachlan](https://datascience.stackexchange.com/users/68975/eric-mclachlan) Eric McLachlan\n\n19322 silver badges88 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Your data seems to be highly imbalance which will affect the model's performance significantly. The number of cases, where in the sample is positive but the model predicts it as negative, will increase.$\\\\endgroup$\n\n\u2013\u00a0[Shubham Panchal](https://datascience.stackexchange.com/users/68023/shubham-panchal)\n\nCommentedMay 31, 2019 at 15:17\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if)\u00a0\\|\n\n## 4 Answers 4\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n2\n\n$\\\\begingroup$\n\nAn imbalance of this magnitude is definitely a problem. Because machine learning algorithms penalize model complexity and try to maximize fit, a model that always predicts \"False\" will often outperform any other model because it predicts the right outcome for 99% of the data and is super simple!\n\nWhat one should do in this case is downsample ( [http://www.simafore.com/blog/handling-unbalanced-data-machine-learning-models](http://www.simafore.com/blog/handling-unbalanced-data-machine-learning-models)), train the model, and then scale the resulting predictions back up to reflect the imbalance in the original data.\n\n[Share](https://datascience.stackexchange.com/a/52998)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/52998/edit)\n\nFollow\n\nanswered May 31, 2019 at 17:46\n\n[![RandomCat's user avatar](https://i.sstatic.net/rqr9V.png?s=64)](https://datascience.stackexchange.com/users/41087/randomcat)\n\n[RandomCat](https://datascience.stackexchange.com/users/41087/randomcat) RandomCat\n\n2322 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if)\u00a0\\|\n\n1\n\n$\\\\begingroup$\n\nFirstly any imbalance class text classification will have biased towards majority population and will result in overfit/underfit. You can use Smote, imbalancelearn library and imbalance from Scikit learn to optimize the low population and arrive at fair results, even if not good accuracy.\n\nI hope this helps\n\n[Share](https://datascience.stackexchange.com/a/53018)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/53018/edit)\n\nFollow\n\nanswered Jun 1, 2019 at 0:05\n\n[![Syenix's user avatar](https://www.gravatar.com/avatar/46d7fde031741eb9a5c4462f645a93c7?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/74118/syenix)\n\n[Syenix](https://datascience.stackexchange.com/users/74118/syenix) Syenix\n\n35911 silver badge66 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if)\u00a0\\|\n\n1\n\n$\\\\begingroup$\n\nI am also currently working on a binary classification problem with an imbalanced data set! Here's what I've found useful:\n\n- Use class weights. If you're using Keras you can pass this as an argument to model.fit(). [Here](https://colab.research.google.com/drive/1xL2jSdY-MGlN60gGuSH_L30P7kxxwUfM) is a notebook from Francois Chollet, creator of Keras, using this on an imbalanced data set for a binary classification problem.\n- Use a large batch size during training so that each batch will have at least a few True data points.\n- Use an appropriate metric. For example, accuracy is generally not a good one to use with imbalanced data - for example if you were trying to optimize accuracy, the model would ultimately learn to just classify a sample as False every time as it would end up with an accuracy of 99% which seems great but completely defeats the purpose of what you're trying to do. Area under the precision-recall curve is a good choice for imbalanced datasets, but I encourage you to do some reading and find a metric that works best for your problem.\n\nHope this helps and let me know if you have any more questions!\n\n[Share](https://datascience.stackexchange.com/a/53051)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/53051/edit)\n\nFollow\n\nanswered Jun 1, 2019 at 15:08\n\n[![ggaugler's user avatar](https://www.gravatar.com/avatar/27a27c406fcf989af93998b30e6bf3d0?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/75218/ggaugler)\n\n[ggaugler](https://datascience.stackexchange.com/users/75218/ggaugler) ggaugler\n\n5111 silver badge44 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if)\u00a0\\|\n\n1\n\n$\\\\begingroup$\n\nI was working on a similar problem and had found these two articles which cover everything that you need to know about handling unbalanced data.\n1) [https://towardsdatascience.com/practical-tips-for-class-imbalance-in-binary-classification-6ee29bcdb8a7](https://towardsdatascience.com/practical-tips-for-class-imbalance-in-binary-classification-6ee29bcdb8a7)\n2) [https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n\nHope this helps!\n\n[Share](https://datascience.stackexchange.com/a/53055)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/53055/edit)\n\nFollow\n\nanswered Jun 1, 2019 at 16:41\n\n[![neha tamore's user avatar](https://lh5.googleusercontent.com/-489jXdzZJHc/AAAAAAAAAAI/AAAAAAAAAEo/CRlHfXQRSN0/photo.jpg?sz=64)](https://datascience.stackexchange.com/users/75270/neha-tamore)\n\n[neha tamore](https://datascience.stackexchange.com/users/75270/neha-tamore) neha tamore\n\n11111 bronze badge\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if)\u00a0\\|\n\n## Your Answe...",
      "url": "https://datascience.stackexchange.com/questions/52978/is-it-ok-to-train-a-binary-classifier-using-all-the-extremely-imbalanced-data-if"
    },
    {
      "title": "Machine Learning with Imbalanced data - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=f2bc5978f08b75213e7a:1:11021)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/liamarguedas/machine-learning-with-imbalanced-data"
    },
    {
      "title": "GitHub - siamak1399/Kaggle-Imbalanced-Binary-Classification: This is a binary classification for severely Imbalanced Kaggle dataset to predict Insurance renewal based on 11 features with missing values. The repository including two notebooks composed of EDA, tricks in data preprocessing, modeling with classic methods and presenting an innovative ML algorithm.",
      "text": "<div><div><article><p></p><h2>Kaggle-Imbalanced-Binary-Classification</h2><a href=\"#kaggle-imbalanced-binary-classification\"></a><p></p>\n<p>The main goal of this work is not stick to that special dataset but to review Important concepts such as generalizability of models, data preprocessing, feature extraction, modeling and evaluation. ML models such as logistic regression, random forest, support vector machine with different kernels, Gaussian Naive Bayes, AdaBoost classifier and Neural networks will be applied to predict class label of Kaggle dataset. This project inspired me to develop a new ML algorithm that is very powerful for cases that meet the required conditions.</p>\n<p>In Part I, the importance of choosing special evaluating metrics for imbalanced datasets and some innovative tricks on scaling, transforming and target encoding will be explained. Also, new features will be extracted via PCA, ICA, LDA and polynomial feature extraction methods. Feature selection section will be omitted since, a great innovative feature selection method presented in my previous notebook on car price prediction. Exploratory data analysis and plots are available at appendix in chapter 15.</p>\n<p>In part II, the theory behind innovative approach that called PMG, Parallel Modeling of Groups, for data are derived from multiple sources or from the one source with different modes is presented and two shallow version, PMG-clustered and PMG-boosted are applied to traindata_sia_train.csv and traindata_sia_test.csv datasets that are extracted in Part I. Finally, shallow and deep ML versions of the powerful innovative hybrid of PMG with bagging and boosting are described.</p>\n</article></div></div>",
      "url": "https://github.com/siamak1399/kaggle-imbalanced-binary-classification"
    }
  ]
}