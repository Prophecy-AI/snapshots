{
  "query": "Why do simple regex-based linguistic features underperform TF-IDF in text classification tasks, and what are better approaches for capturing nuanced language patterns?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Simple regex\u2011based linguistic features\u2014such as patterns that flag dates, URLs, or chemical formulas\u2014are usually turned into binary tokens that indicate only the *presence* of a pattern.  Because they are highly sparse and ignore how often a token occurs, they provide little discriminative signal compared with term\u2011frequency statistics.  Moreover, regexes capture only surface forms and cannot represent lexical variation, synonymy, or the surrounding context that often determines a document\u2019s class.  In contrast, TF\u2011IDF weights each term by its frequency in a document and its inverse frequency in the collection, giving a richer representation of word importance and allowing the classifier to exploit subtle differences in term distribution ([Stanford IR book \u2013 Features for text](https://nlp.stanford.edu/IR-book/html/htmledition/features-for-text-1.html)).  Empirical work shows that even a plain TF\u2011IDF vector fed to a linear model frequently outperforms handcrafted regex features on standard benchmarks ([Gonz\u00e1lez\u2011Carvajal et\u202fal., 2021](https://export.arxiv.org/pdf/2005.13012v2.pdf)).\n\nMore expressive approaches address these shortcomings by modelling word meaning and context.  N\u2011gram extensions (e.g., bigrams or trigrams) already boost performance over unigram TF\u2011IDF, especially for sentiment or short\u2011text tasks ([Wang & Manning, 2012](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)).  Encoder\u2011only pre\u2011trained transformers such as BERT provide contextual embeddings that capture nuanced semantics and have become the de\u2011facto state\u2011of\u2011the\u2011art for both single\u2011label and multi\u2011label classification ([Galke et\u202fal., 2025](https://arxiv.org/abs/2204.03954)).  Recent work also demonstrates that hybrid models that fuse statistical cues (TF\u2011IDF) with semantic representations can further improve accuracy; the Attention\u2011Guided Feature Fusion (AGFF) model dynamically weights statistical and semantic features and consistently outperforms either alone on news classification benchmarks ([Zare, 2025](https://arxiv.org/pdf/2511.17184)).  Finally, incorporating TF\u2011IDF as an auxiliary signal into transformer\u2011based pipelines has been shown to recover granular information that pure contextual embeddings miss, yielding up to 36\u202f% relative gains on fine\u2011grained similarity tasks ([Joshi et\u202fal., 2020](https://aclanthology.org/2020.coling-main.326.pdf)).  Together, these approaches provide richer, context\u2011aware representations that capture the nuanced language patterns that simple regex features cannot.",
      "url": ""
    },
    {
      "title": "Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification",
      "text": "Attention-Guided Feature Fusion (AGFF) Model for\nIntegrating Statistical and Semantic Features in News\nText Classification\nMohammad Zare\nAI lab at AriooBarzan Engineering Team, Shiraz, Iran\nmd.zare@sutech.ac.ir\nAbstract\nNews text classification is a crucial task in natural language processing, essen\u0002tial for organizing and filtering the massive volume of digital content. Traditional\nmethods typically rely on statistical features like term frequencies or TF-IDF val\u0002ues, which are effective at capturing word-level importance but often fail to reflect\ncontextual meaning. In contrast, modern deep learning approaches utilize semantic\nfeatures to understand word usage within context, yet they may overlook simple,\nhigh-impact statistical indicators. This paper introduces an Attention-Guided Fea\u0002ture Fusion (AGFF) model that combines statistical and semantic features in a uni\u0002fied framework. The model applies an attention-based mechanism to dynamically\ndetermine the relative importance of each feature type, enabling more informed clas\u0002sification decisions. Through evaluation on benchmark news datasets, the AGFF\nmodel demonstrates superior performance compared to both traditional statisti\u0002cal models and purely semantic deep learning models. The results confirm that\nstrategic integration of diverse feature types can significantly enhance classification\naccuracy. Additionally, ablation studies validate the contribution of each compo\u0002nent in the fusion process. The findings highlight the model\u2019s ability to balance\nand exploit the complementary strengths of statistical and semantic representa\u0002tions, making it a practical and effective solution for real-world news classification\ntasks.\nKeywords: attention mechanism; feature fusion; text classification; news classification;\nsemantic features; statistical features.\n1 Introduction\nWith the exponential growth of online news content, automatic news text classification\nhas become an essential technology for information organization, retrieval, and recom\u0002mendation (Minaee et al., 2021). Accurately categorizing news articles into topics or\nsections (e.g., politics, sports, finance) enables better content curation and user expe\u0002rience. Early text classification approaches largely relied on statistical features derived\nfrom the text, such as word occurrence frequencies, bag-of-words representations, and TF\u0002IDF term weights. Such features feed into machine learning classifiers like Na\u00a8\u0131ve Bayes\n1\narXiv:2511.17184v1 [cs.CL] 21 Nov 2025\nor Support Vector Machines, which have proven effective in many contexts (Sebastiani,\n2002). However, these models often struggle to capture contextual meaning; they treat\nwords as independent and ignore word order or semantics.\nIn recent years, advances in deep learning have led to semantic feature-based methods\nthat learn distributed representations of text. Techniques such as convolutional neural\nnetworks (CNN) and recurrent neural networks (RNN) (e.g., LSTM, GRU) can auto\u0002matically extract abstract features from word sequences, considering word context and\norder. Attention mechanisms further improved performance by allowing models to focus\non the most relevant words in a document (Bahdanau, Cho, & Bengio, 2015). Notably,\nthe Transformer architecture (Vaswani et al., 2017) and pre-trained language models like\nBERT (Devlin, Chang, Lee, & Toutanova, 2019) have set state-of-the-art results in NLP\ntasks by capturing rich semantic information. Despite this success, purely semantic mod\u0002els may sometimes overlook simple but useful signals such as the presence of particular\nkeywords highly indicative of a news category.\nThere is growing evidence that combining multiple types of features can enhance clas\u0002sification performance (Li, Li, Xie, & Li, 2021; Luo, Yu, Zhao, Zhao, & Wang, 2022). For\nexample, classical feature-based methods like TF-IDF remain strong predictors in many\ncases, and augmenting deep learning models with such features has shown improvements\nin some studies (Luo et al., 2022; Li et al., 2021). A challenge, however, lies in how\nto effectively integrate these heterogeneous features. A straightforward concatenation\nof feature vectors might not fully exploit their complementary strengths and could even\nintroduce noise if one feature type is less reliable for certain instances (Li et al., 2021).\nTo address this challenge, we propose the Attention-Guided Feature Fusion\n(AGFF) model, which explicitly integrates statistical and semantic features for news\ntext classification using an attention-based gating mechanism. The key idea is to let the\nmodel learn how much to rely on each type of feature for a given input, rather than fixing\na priori combination rules. Our contributions are summarized as follows:\n\u2022 We introduce a novel model (AGFF) that fuses TF-IDF-based statistical features\nwith deep semantic features from a BiLSTM encoder using an attention-guided\ngating mechanism.\n\u2022 We demonstrate through experiments on benchmark news datasets (20 Newsgroups\nand AG News) that AGFF outperforms baseline models, including those using either\nfeature type alone and a simple concatenation fusion, achieving higher classification\naccuracy.\n\u2022 We provide an analysis of the attention weights to interpret how the model balances\nstatistical vs. semantic information, and we perform ablation studies to quantify\nthe impact of the fusion module.\nThe remainder of this paper is organized as follows: Section 2 reviews related work on text\nclassification and feature fusion approaches. Section 3 details the proposed AGFF model\narchitecture. Section 4 describes the experimental setup, including datasets, baselines,\nand implementation details. Section 5 presents the results and analysis. We discuss the\nfindings and implications in Section 6, outline limitations in Section 7, propose future\nwork in Section 8, and conclude the paper in Section 9.\n2\n2 Related Work\n2.1 Text Classification Methods\nAutomatic text classification has been studied for decades, yielding a broad spectrum\nof approaches. Early methods used manual feature engineering and classical machine\nlearning. For instance, Support Vector Machines and logistic regression with TF-IDF\nfeatures were common benchmarks and often achieved strong performance on news data\n(Sebastiani, 2002; Kowsari et al., 2019). These statistical approaches treat the text as\na bag-of-words, capturing term importance but losing syntax and context. Comprehen\u0002sive surveys like (Sebastiani, 2002) and (Kowsari et al., 2019) provide overviews of such\ntraditional techniques and their evolution.\nThe rise of deep learning introduced models that learn semantic representations from\nraw text. (Kim, 2014) showed that a simple CNN on top of pre-trained word embeddings\ncan outperform earlier baselines by extracting local patterns (e.g., key phrases). RNN\u0002based models, especially LSTMs and BiLSTMs, became popular for text sequences due\nto their ability to capture long-term dependencies. Building on these, hierarchical models\nlike the Hierarchical Attention Network by (Yang et al., 2016) can handle long documents\n(such as news articles) by aggregating information from word- to sentence-level with at\u0002tentional weights. The attention mechanism, originally developed for machine translation\n(Bahdanau et al., 2015), has been widely applied to text classification to identify crucial\nwords or sentences for the task at hand. Transformers (Vaswani et al., 2017) dispense\nwith recurrence entirely, using self-attention to capture global context; when trained on\nlarge corpora and fine-tuned (e.g., BERT by (Devlin et al., 2019)), they achieve state-of\u0002the-art results on many classification benchmarks. These deep models learn rich semantic\nfeatures that encode contextual meaning and nuances of language.\n2.2 Feature Fusion and Hybrid Models\nWhile purely neural approaches dominate recent leaderboards, combining them with ex\u0002ternal or statistical features can sometimes further boost performance, particularly when\ndata...",
      "url": "https://arxiv.org/pdf/2511.17184"
    },
    {
      "title": "Are We Really Making Much Progress in Text Classification? A Comparative Review",
      "text": "# Computer Science > Computation and Language\n\n**arXiv:2204.03954** (cs)\n\n\\[Submitted on 8 Apr 2022 ( [v1](https://arxiv.org/abs/2204.03954v1)), last revised 19 Jan 2025 (this version, v6)\\]\n\n# Title:Are We Really Making Much Progress in Text Classification? A Comparative Review\n\nAuthors: [Lukas Galke](https://arxiv.org/search/cs?searchtype=author&query=Galke,+L), [Ansgar Scherp](https://arxiv.org/search/cs?searchtype=author&query=Scherp,+A), [Andor Diera](https://arxiv.org/search/cs?searchtype=author&query=Diera,+A), [Fabian Karl](https://arxiv.org/search/cs?searchtype=author&query=Karl,+F), [Bao Xin Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+B+X), [Bhakti Khera](https://arxiv.org/search/cs?searchtype=author&query=Khera,+B), [Tim Meuser](https://arxiv.org/search/cs?searchtype=author&query=Meuser,+T), [Tushar Singhal](https://arxiv.org/search/cs?searchtype=author&query=Singhal,+T)\n\nView a PDF of the paper titled Are We Really Making Much Progress in Text Classification? A Comparative Review, by Lukas Galke and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2204.03954) [HTML (experimental)](https://arxiv.org/html/2204.03954v6)\n\n> Abstract:We analyze various methods for single-label and multi-label text classification across well-known datasets, categorizing them into bag-of-words, sequence-based, graph-based, and hierarchical approaches. Despite the surge in methods like graph-based models, encoder-only pre-trained language models, notably BERT, remain state-of-the-art. However, recent findings suggest simpler models like logistic regression and trigram-based SVMs outperform newer techniques. While decoder-only generative language models show promise in learning with limited data, they lag behind encoder-only models in performance. We emphasize the superiority of discriminative language models like BERT over generative models for supervised tasks. Additionally, we highlight the literature's lack of robustness in method comparisons, particularly concerning basic hyperparameter optimizations like learning rate in fine-tuning encoder-only language models.\n>\n> Data availability: The source code is available at [this https URL](https://github.com/drndr/multilabel-text-clf)\n>\n> All datasets used for our experiments are publicly available except the NYT dataset.\n\n|     |     |\n| --- | --- |\n| Comments: | Update: covering single-label, multi-label, and hierarchical classification, small language models, and large language models. Extension of \"Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. ACL (1) 2022: 4038-4051\", URL: [this https URL](https://aclanthology.org/2022.acl-long.279/) |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2204.03954](https://arxiv.org/abs/2204.03954) \\[cs.CL\\] |\n|  | (or [arXiv:2204.03954v6](https://arxiv.org/abs/2204.03954v6) \\[cs.CL\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2204.03954](https://doi.org/10.48550/arXiv.2204.03954)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ansgar Scherp \\[ [view email](https://arxiv.org/show-email/24c860b5/2204.03954)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2204.03954v1)**\nFri, 8 Apr 2022 09:28:20 UTC (674 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2204.03954v2)**\nFri, 3 Mar 2023 14:36:11 UTC (694 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2204.03954v3)**\nFri, 5 May 2023 08:48:17 UTC (751 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/2204.03954v4)**\nFri, 26 May 2023 17:59:37 UTC (150 KB)\n\n**[\\[v5\\]](https://arxiv.org/abs/2204.03954v5)**\nSun, 4 Jun 2023 17:40:32 UTC (737 KB)\n\n**\\[v6\\]**\nSun, 19 Jan 2025 20:37:45 UTC (194 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Are We Really Making Much Progress in Text Classification? A Comparative Review, by Lukas Galke and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2204.03954)\n- [HTML (experimental)](https://arxiv.org/html/2204.03954v6)\n- [TeX Source](https://arxiv.org/src/2204.03954)\n- [Other Formats](https://arxiv.org/format/2204.03954)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2204.03954&function=prev&context=cs.CL)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2204.03954&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2022-04](https://arxiv.org/list/cs.CL/2022-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2204.03954?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2204.03954)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2204.03954)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2204.03954)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2204.03954&description=Are We Really Making Much Progress in Text Classification? A Comparative Review) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2204.03954&title=Are We Really Making Much Progress in Text Classification? A Comparative Review)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2204.03954) \\|\n[Disable MathJax](javascrip...",
      "url": "https://arxiv.org/abs/2204.03954"
    },
    {
      "title": "",
      "text": "Santiago Gonz\u00e1lez-Carvajal santiago.gonzalez-carvajal@alumnos.upm.eseduardo.garrido@uam.es\nEduardo C Garrido-Merch\u00e1n\nUniversidad Polit\u00e9cnica de Madrid\nMadridSpain\nUniversidad Aut\u00f3noma de Madrid\nMadridSpain\nComparing BERT against traditional machine learning text classification\n12 Jan 2021\nThe BERT model has arisen as a popular state-of-the-art model in recent years. It is able to cope with NLP tasks such as supervised text classification without human supervision. Its flexibility to cope with any corpus delivering great results has make this approach very popular in academia and industry. Although, other approaches have been used before successfully. We first present BERT and a review on classical NLP approaches. Then, we empirically test with a suite of different scenarios the behaviour of BERT against traditional TF-IDF vocabulary fed to machine learning algorithms. The purpose of this work is adding empirical evidence to support the use of BERT as a default on NLP tasks. Experiments show the superiority of BERT and its independence of features of the NLP problem such as the language of the text adding empirical evidence to use BERT as a default technique in NLP problems.\nIntroduction\nNatural Language Processing (NLP) methodologies have flourished and lots of papers solving different tasks of the field, such as text classification [1], named entity recognition [17] or summarization [19], have been published. We can differentiate, mainly, between two types of approaches to NLP problems: Firstly, linguistic approaches [6] that generally use different features of the text that the experts on the domain consider that are relevant have been extensively used. Those features could be combinations of words, or n-grams [22], grammatical categories, unambiguous meanings of words, words appearing in a particular position, categories of words and much more. These features could be built manually for an specific problem or can be retrieved by using different linguistic resources [3] such as ontologies [5].\nOn the other hand, Machine Learning (ML) [15] and deep learning based approaches [18] that classically have analyzed annotated corpora of texts inferring which features of the text, typically in a bag of words fashion [30] or by n-grams, are relevant for the classification automatically. Both approaches have their pros and cons, concretely, linguistic approaches have great precision but their recall is low as the context where the features are useful is not as big as the one processed by machine learning algorithms. Although, the precision of classical NLP systems was, until recently, generally better as the one delivered by machine learning [9]. Nevertheless, recently, thanks to the rise of computation, machine learning text classification dominates in scenarios where huge sizes of texts are processed.\nGenerally, linguistic approaches consist in applying a series of rules, which are designed by linguistic experts [14]. An example of linguistic approach can be found at [12]. The advantage of these type of approaches over ML based approaches is that they do not need large amounts of data. Regarding ML based approaches, they usually have a statistical base [14]. We can find many examples of these type of approaches: BERT [7], Transformers [27], etc.\nAnother issue with traditional NLP approaches is multilingualism [4]. We can design rules for a given language, but sentence structure, and even the alphabet, may change from one language to another, resulting in the need to design new rules. Some approaches such as the Universal Networking Language (UNL) standard [25] try to circumvent this issue, but the multilingual resource is hard to build and requires experts on the platform. Another problem with UNL approaches and related ones, would be that, given a specific language, the different forms of expression, i.e. the way we write in, for example, Twitter, is very different from the way we write a more formal document, such as a research paper [8].\nBidirectional Encoder Representations from Transformers (BERT) is a NLP model that was designed to pretrain deep bidirectional representations from unlabeled text and, after that, be fine-tuned using labeled text for different NLP tasks [7]. That way, with BERT model, we can create state-of-the-art models for many different NLP tasks [7]. We can see the results obtained by BERT in different NLP tasks at [7].\nIn this work we compare BERT model [7] with a traditional machine learning NLP approach that trains machine learning algorithms in features retrieved by the Term Frequency",
      "url": "https://export.arxiv.org/pdf/2005.13012v2.pdf"
    },
    {
      "title": "Features for text",
      "text": "**Next:** [Document zones in text](https://nlp.stanford.edu/document-zones-in-text-classification-1.html) **Up:** [Improving classifier performance](https://nlp.stanford.edu/improving-classifier-performance-1.html) **Previous:** [Large and difficult category](https://nlp.stanford.edu/large-and-difficult-category-taxonomies-1.html) **[Contents](https://nlp.stanford.edu/contents-1.html)** **[Index](https://nlp.stanford.edu/index-1.html)**\n\n### Features for text\n\nThe default in both ad hoc retrieval and text classification is to use\nterms as features. However, for text classification, a great deal of\nmileage can be achieved by designing additional features which are\nsuited to a specific problem. Unlike the case of IR query languages,\nsince these features are internal to the\nclassifier, there is no problem of communicating these features to an\nend user. This process is generally referred to as _feature_\n_engineering_ . At present, feature engineering remains a human craft,\nrather than something done by machine learning. Good feature\nengineering can often markedly improve the performance of a text\nclassifier. It is especially beneficial in some of the most important\napplications of text classification, like\n_spam_\nand _porn_ filtering.\n\nClassification problems will often contain large numbers of terms\nwhich can be conveniently grouped, and which have a similar vote in\ntext classification problems. Typical examples might be year mentions\nor strings of exclamation marks. Or they may be more specialized\ntokens like ISBNs or chemical formulas.\nOften, using them directly in a classifier would greatly increase\nthe vocabulary without providing classificatory power beyond\nknowing that, say, a chemical formula is present.\nIn such cases,\nthe number of features and feature sparseness can be reduced by\nmatching such items with regular expressions and converting them into\ndistinguished tokens. Consequently, effectiveness and classifier\nspeed are normally enhanced.\nSometimes all numbers are converted into a single feature,\nbut often some value can be had by distinguishing\ndifferent kinds of numbers, such as four digit numbers (which are\nusually years) versus other cardinal numbers versus real numbers with\na decimal point. Similar techniques can be applied to dates, ISBN\nnumbers, sports game scores, and so on.\n\nGoing in the other direction, it is often useful to\nincrease the number of features by matching parts of words, and by\nmatching selected multiword patterns that are particularly\ndiscriminative. Parts of words are often matched by character\n-gram features. Such features can be particularly good at providing\nclassification clues for otherwise unknown words when the classifier\nis deployed. For instance, an unknown word ending in _-rase_ is\nlikely to be an enzyme, even if it wasn't seen in the training data.\nGood multiword patterns are often found by looking for distinctively\ncommon word pairs (perhaps using a mutual information criterion\nbetween words, in a similar way to its use in Section\u00a0[13.5.1](https://nlp.stanford.edu/mutual-information-1.html#sec:mutualinfo) (page\u00a0)\nfor feature selection) and then using feature selection methods evaluated\nagainst classes. They are useful when the components of a compound\nwould themselves be misleading as classification cues. For instance,\nthis would be the case if the keyword _ethnic_ was most\nindicative of the categories _food_ and _arts_, the\nkeyword _cleansing_ was most indicative of the category\n_home_, but the collocation _ethnic cleansing_ instead\nindicates the category _world news_. Some text classifiers also\nmake use of features from named entity recognizers (cf. page [10](https://nlp.stanford.edu/xml-retrieval-1.html#p:ner-ref) ).\n\nDo techniques like stemming and lowercasing (vocabulary) help\nfor text classification? As always, the ultimate test is empirical\nevaluations conducted on an appropriate test collection. But it is\nnevertheless useful to note that such techniques have a more\nrestricted chance of being useful for classification. For IR, you\noften need to collapse forms of a word like _oxygenate_ and\n_oxygenation_, because the appearance of either in a document is\na good clue that the document will be relevant to a query about\noxygenation. Given copious training\ndata, stemming necessarily delivers no value for text classification.\nIf several forms that stem together have a similar\nsignal, the parameters estimated for all of them will have similar\nweights. Techniques like stemming help only in compensating for data\nsparseness. This can be a useful role (as noted at the start of this\nsection), but often different forms of a word can convey significantly\ndifferent cues about the correct document classification. Overly\naggressive stemming can easily degrade classification performance.\n\n **Next:** [Document zones in text](https://nlp.stanford.edu/document-zones-in-text-classification-1.html) **Up:** [Improving classifier performance](https://nlp.stanford.edu/improving-classifier-performance-1.html) **Previous:** [Large and difficult category](https://nlp.stanford.edu/large-and-difficult-category-taxonomies-1.html) **[Contents](https://nlp.stanford.edu/contents-1.html)** **[Index](https://nlp.stanford.edu/index-1.html)** \u00a9 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the [PDF edition](http://informationretrieval.org) of the book.\n2009-04-07",
      "url": "https://nlp.stanford.edu/IR-book/html/htmledition/features-for-text-1.html"
    },
    {
      "title": "",
      "text": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification\nSida Wang and Christopher D. Manning\nDepartment of Computer Science\nStanford University\nStanford, CA 94305\n{sidaw,manning}@stanford.edu\nAbstract\nVariants of Naive Bayes (NB) and Support\nVector Machines (SVM) are often used as\nbaseline methods for text classification, but\ntheir performance varies greatly depending on\nthe model variant, features used and task/\ndataset. We show that: (i) the inclusion of\nword bigram features gives consistent gains on\nsentiment analysis tasks; (ii) for short snippet\nsentiment tasks, NB actually does better than\nSVMs (while for longer documents the oppo\u0002site result holds); (iii) a simple but novel SVM\nvariant using NB log-count ratios as feature\nvalues consistently performs well across tasks\nand datasets. Based on these observations, we\nidentify simple NB and SVM variants which\noutperform most published results on senti\u0002ment analysis datasets, sometimes providing\na new state-of-the-art performance level.\n1 Introduction\nNaive Bayes (NB) and Support Vector Machine\n(SVM) models are often used as baselines for other\nmethods in text categorization and sentiment analy\u0002sis research. However, their performance varies sig\u0002nificantly depending on which variant, features and\ndatasets are used. We show that researchers have\nnot paid sufficient attention to these model selec\u0002tion issues. Indeed, we show that the better variants\noften outperform recently published state-of-the-art\nmethods on many datasets. We attempt to catego\u0002rize which method, which variants and which fea\u0002tures perform better under which circumstances.\nFirst, we make an important distinction between\nsentiment classification and topical text classifica\u0002tion. We show that the usefulness of bigram features\nin bag of features sentiment classification has been\nunderappreciated, perhaps because their usefulness\nis more of a mixed bag for topical text classifica\u0002tion tasks. We then distinguish between short snip\u0002pet sentiment tasks and longer reviews, showing that\nfor the former, NB outperforms SVMs. Contrary to\nclaims in the literature, we show that bag of features\nmodels are still strong performers on snippet senti\u0002ment classification tasks, with NB models generally\noutperforming the sophisticated, structure-sensitive\nmodels explored in recent work. Furthermore, by\ncombining generative and discriminative classifiers,\nwe present a simple model variant where an SVM is\nbuilt over NB log-count ratios as feature values, and\nshow that it is a strong and robust performer over all\nthe presented tasks. Finally, we confirm the well\u0002known result that MNB is normally better and more\nstable than multivariate Bernoulli NB, and the in\u0002creasingly known result that binarized MNB is bet\u0002ter than standard MNB. The code and datasets to\nreproduce the results in this paper are publicly avail\u0002able. 1\n2 The Methods\nWe formulate our main model variants as linear clas\u0002sifiers, where the prediction for test case k is\ny\n(k) = sign(wT x(k) + b) (1)\nDetails of the equivalent probabilistic formulations\nare presented in (McCallum and Nigam, 1998).\nLet f\n(i) \u2208 R|V | be the feature count vector for\ntraining case i with label y\n(i) \u2208 {\u22121, 1}. V is the\n1\nhttp://www.stanford.edu/\u223csidaw\nset of features, and f\n(i)\nj\nrepresents the number of oc\u0002currences of feature Vj in training case i. Define\nthe count vectors as p = \u03b1 +\nP\ni:y\n(i)=1 f\n(i)\nand\nq = \u03b1 +\nP\ni:y\n(i)=\u22121\nf\n(i)\nfor smoothing parameter\n\u03b1. The log-count ratio is:\nr = log\u0012\np/||p||1\nq/||q||1\n\u0013\n(2)\n2.1 Multinomial Naive Bayes (MNB)\nIn MNB, x\n(k) = f(k)\n, w = r and b = log(N+/N\u2212).\nN+, N\u2212 are the number of positive and negative\ntraining cases. However, as in (Metsis et al., 2006),\nwe find that binarizing f\n(k)\nis better. We take x\n(k) =\n\u02c6f\n(k) = 1{f(k) > 0}, where 1 is the indicator func\u0002tion. \u02c6p, \u02c6q,\u02c6r are calculated using \u02c6f\n(i)\ninstead of f\n(i)\nin (2).\n2.2 Support Vector Machine (SVM)\nFor the SVM, x\n(k) = \u02c6f(k)\n, and w, b are obtained by\nminimizing\nwT w + C\nX\ni\nmax(0, 1 \u2212 y\n(i)\n(wT\u02c6f\n(i) + b))2\n(3)\nWe find this L2-regularized L2-loss SVM to work\nthe best and L1-loss SVM to be less stable. The LI\u0002BLINEAR library (Fan et al., 2008) is used here.\n2.3 SVM with NB features (NBSVM)\nOtherwise identical to the SVM, except we use\nx\n(k) = \u02dcf(k)\n, where \u02dcf\n(k) = \u02c6r \u25e6 \u02c6f(k)\nis the elemen\u0002twise product. While this does very well for long\ndocuments, we find that an interpolation between\nMNB and SVM performs excellently for all docu\u0002ments and we report results using this model:\nw0 = (1 \u2212 \u03b2) \u00afw + \u03b2w (4)\nwhere w\u00af = ||w||1/|V | is the mean magnitude of w,\nand \u03b2 \u2208 [0, 1] is the interpolation parameter. This\ninterpolation can be seen as a form of regularization:\ntrust NB unless the SVM is very confident.\n3 Datasets and Task\nWe compare with published results on the following\ndatasets. Detailed statistics are shown in table 1.\nRT-s: Short movie reviews dataset containing one\nsentence per review (Pang and Lee, 2005).\nDataset (N+, N\u2212) l CV |V | \u2206\nRT-s (5331,5331) 21 10 21K 0.8\nCR (2406,1366) 20 10 5713 1.3\nMPQA (3316,7308) 3 10 6299 0.8\nSubj. (5000,5000) 24 10 24K 0.8\nRT-2k (1000,1000) 787 10 51K 1.5\nIMDB (25k,25k) 231 N 392K 0.4\nAthR (799,628) 345 N 22K 2.9\nXGraph (980,973) 261 N 32K 1.8\nBbCrypt (992,995) 269 N 25K 0.5\nTable 1: Dataset statistics. (N+, N\u2212): number of\npositive and negative examples. l: average num\u0002ber of words per example. CV: number of cross\u0002validation splits, or N for train/test split. |V |: the\nvocabulary size. \u2206: upper-bounds of the differences\nrequired to be statistically significant at the p < 0.05\nlevel.\nCR: Customer review dataset (Hu and Liu, 2004)\nprocessed like in (Nakagawa et al., 2010).2\nMPQA: Opinion polarity subtask of the MPQA\ndataset (Wiebe et al., 2005).3\nSubj: The subjectivity dataset with subjective re\u0002views and objective plot summaries (Pang and\nLee, 2004).\nRT-2k: The standard 2000 full-length movie re\u0002view dataset (Pang and Lee, 2004).\nIMDB: A large movie review dataset with 50k full\u0002length reviews (Maas et al., 2011).4\nAthR, XGraph, BbCrypt: Classify pairs of\nnewsgroups in the 20-newsgroups dataset with\nall headers stripped off (the third (18828) ver\u0002sion5\n), namely: alt.atheism vs. religion.misc,\ncomp.windows.x vs. comp.graphics, and\nrec.sport.baseball vs. sci.crypt, respectively.\n4 Experiments and Results\n4.1 Experimental setup\nWe use the provided tokenizations when they exist.\nIf not, we split at spaces for unigrams, and we filter\nout anything that is not [A-Za-z] for bigrams. We do\n2\nhttp://www.cs.uic.edu/\u223cliub/FBS/sentiment-analysis.html\n3\nhttp://www.cs.pitt.edu/mpqa/\n4\nhttp://ai.stanford.edu/\u223camaas/data/sentiment\n5\nhttp://people.csail.mit.edu/jrennie/20Newsgroups\nnot use stopwords, lexicons or other resources. All\nresults reported use \u03b1 = 1, C = 1, \u03b2 = 0.25 for\nNBSVM, and C = 0.1 for SVM.\nFor comparison with other published results, we\nuse either 10-fold cross-validation or train/test split\ndepending on what is standard for the dataset. The\nCV column of table 1 specifies what is used. The\nstandard splits are used when they are available.\nThe approximate upper-bounds on the difference re\u0002quired to be statistically significant at the p < 0.05\nlevel are listed in table 1, column \u2206.\n4.2 MNB is better at snippets\n(Moilanen and Pulman, 2007) suggests that while\n\u201cstatistical methods\u201d work well for datasets with\nhundreds of words in each example, they cannot\nhandle snippets datasets and some rule-based sys\u0002tem is necessary. Supporting this claim are examples\nsuch as not an inhumane monster6, or killing cancer\nthat express an overall positive sentiment with nega\u0002tive words.\nSome previous work on classifying snippets in\u0002clude using pre-defined polarity reversing rules\n(Moilanen and Pulman, 2007), and learning com\u0002plex models on parse trees such as in (Nakagawa et\nal., 2010) and (Socher et al., 2011). These works\nseem promising as they perform better than many\nsophisticated, rule-based methods used as baselines\nin (Nakagawa et al., 2010). However, we find that\nseveral NB/S...",
      "url": "https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf"
    },
    {
      "title": "",
      "text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3652\u20133659\nBarcelona, Spain (Online), December 8-13, 2020\n3652\nThe Devil is in the Details: Evaluating Limitations of Transformer-based\nMethods for Granular Tasks\nBrihi Joshi\u2020 Neil Shah\u2663 Francesco Barbieri\u2663 Leonardo Neves\u2663\n\u2020\nIndraprastha Institute of Information Technology Delhi, New Delhi, India\n\u2663Snap Inc., Santa Monica, CA 90405, USA\n\u2020brihi16142@iiitd.ac.in,\n\u2663{nshah,fbarbieri,lneves}@snap.com,\nAbstract\nContextual embeddings derived from transformer-based neural language models have shown\nstate-of-the-art performance for various tasks such as question answering, sentiment analysis,\nand textual similarity in recent years. Extensive work shows how accurately such models can\nrepresent abstract, semantic information present in text. In this expository work, we explore a\ntangent direction and analyze such models\u2019 performance on tasks that require a more granular\nlevel of representation. We focus on the problem of textual similarity from two perspectives:\nmatching documents on a granular level (requiring embeddings to capture fine-grained attributes\nin the text), and an abstract level (requiring embeddings to capture overall textual semantics).\nWe empirically demonstrate, across two datasets from different domains, that despite high per\u0002formance in abstract document matching as expected, contextual embeddings are consistently\n(and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We\nthen propose a simple but effective method to incorporate TF-IDF into models that use contextual\nembeddings, achieving relative improvements of up to 36% on granular tasks.\n1 Introduction\nIn recent years, contextual embeddings (Peters et al., 2018; Devlin et al., 2018) have made immense\nprogress in semantic understanding-based tasks. After being trained using large amounts of data, for\nexample via a self-supervised task like masked language-modeling, such models learn crucial elements\nof language, such as syntax and semantics (Jawahar et al., 2019; Goldberg, 2019; Wiedemann et al.,\n2019) from just raw text. The best performing contextual embeddings are trained with Transformer\u0002based methods (TBM) (Vaswani et al., 2017; Devlin et al., 2018). These embeddings have been shown\nto frequently achieve state-of-the-art results in downstream tasks like question answering and sentiment\nanalysis (van Aken et al., 2019; Sun et al., 2019). Contextual embeddings are also often used to capture\nthe similarity between pairs of documents; for example, on the Semantic Textual Similarity (STS) task\n(Cer et al., 2017) included in the GLUE benchmark (Wang et al., 2018), TBMs have shown competitive\nperformance, substantially outperforming embedding baselines like Word2Vec (Mikolov et al., 2013)\nand GloVE (Pennington et al., 2014). However, their performance on similarity tasks beyond abstract,\nsemantic ones (Mickus et al., 2019) \u2013 for example, on granular news article matching \u2013 is less understood.\nIn this work, we study the performance of TBMs in textual similarity tasks with the following research\nquestion: Are transformer-based methods as performant for granular tasks as they are for abstract ones?\nHere, granular and abstract reflect varying amounts of coarseness in the concept of similarity. For\nexample, consider the news domain: A granular notion of similarity might be whether a pair of articles\nboth report the exact same news event. Conversely, an abstract notion might be when the articles share\nthe same topical category, like sports or finance. Figure 1 illustrates this with an example for clarity.\nFirstly, we define separate tasks to explore these two notions of similarity on two datasets from dif\u0002ferent domains \u2013 News Articles, and Bug Reports. Our analysis on both datasets reveals that contextual\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n3653\nFigure 1: An example pair of articles from the News Dedup dataset: Both report the same news event,\nand are thus similar on a granular level; the colored text indicates fine-grained details associated with\nthis determination. Both articles are also of the \u201csports\u201d topic, and are thus similar on an abstract level.\nembeddings do not perform well on granular tasks, and are outperformed by simple baselines like TF\u0002IDF. Secondly, we demonstrate that TBM contextual embeddings do in fact contain important semantic\ninformation, and a simple interpolation strategy between the two methods can help boost the relative\nindividual performance of TBMs (TF-IDF) by up to 36% (6%) on the granular task.\n2 Related Work\nWe discuss related work in two areas: textual similarity, and TBMs.\nTextual Similarity has been studied from various perspectives \u2013 comparing documents of different\nlengths in order to capture varying levels of detail (Gong et al., 2018), evaluating semantic similarity\nbetween reference and generated corpus (Clark et al., 2019), and semantic similarity for long documents\nin a hierarchical fashion (Jiang et al., 2019). It is also shown that sentence meta-embeddings (obtained\nfrom combining ensembles of sentence embeddings) perform better (Poerner et al., 2019) for semantic\nsimilarity tasks compared to the individual baselines. For duplicate detection, which is a more granular\ntask compared to semantic similarity, Rodier and Carter (2020) show that detection of near-duplicates in\nnews articles can be identified by evaluating n-gram level overlap in documents. In the news domain,\nLiu et al. (2018) shows that article similarity can be improved by extracting common \u2018concepts\u2019 from the\ntwo articles using graph-based approaches.\nTBMs (Liu et al., 2019; Devlin et al., 2018) have been shown to consistently perform better in the\nsemantic similarity tasks. Peinelt et al. (2020) also shows that BERT-based architectures appended with\ntopic-related details from topic models lead to an increase in semantic similarity performance. However,\nfew works have highlighted TBMs\u2019 ability to capture granular information. (Khattab and Zaharia, 2020)\nshows that BERT can be used for document retrieval by matching embeddings of each word in the query\nand document, capturing granular similarity.\nUnlike previous approaches that focus on either a granular or abstract similarity task, we compare\nthe performance of TBMs with other baseline methods across the two tasks, and in addition, provide a\nsimple method to improve the performance of TBMs on granular similarity tasks.\n3 Method\nIn this section, we describe the methodology used to compare two documents from a granular and abstract\nperspective. Further, we also define the granular and abstract text similarity tasks in detail.\n3.1 Problem Definition\nWe consider both granular and abstract tasks to be similarity classification tasks operating on a pair of\ndocuments. The task-specific labels are binary, indicating whether the pair is judged to be similar or not\n(one label for abstract, one for granular). From a corpus C, we consider a pair of two documents d1, d2\nand their task-specific similarity judgment y (without loss of generality). We define ek = f(dk), where\n3654\nek is dk\u2019s embedding, produced by f(\u00b7). In practice, f could be a vector space method like TF-IDF,\nor the final layer from a TBM. Upon obtaining e1, e2, we generate a (symmetric) pair similarity score\ng(e1, e2) corresponding to the given task, and use it to arrive at a binary prediction y\u02c6. Performance is\nmeasured using standard metrics quantifying agreement between y\u02c6 and y across pairs.\nFigure 2: Our experimentation setups take n pairs consisting of two documents (d1, d2) and their simi\u0002larity label (y) and yields their similarity score (y\u02c6)\n3.2 Experimental Setup\nWe consider two experimental settings: indirect and direct. These are also illustrated in Figure 21. In\nthe indirect setting, we indirectly learn to predict y via a fixed g. Specifically...",
      "url": "https://aclanthology.org/2020.coling-main.326.pdf"
    },
    {
      "title": "Review Article A comprehensive survey of text classification techniques and their research applications: Observational and experimental insights",
      "text": "A comprehensive survey of text classification techniques and their research applications: Observational and experimental insights - ScienceDirect\n[Skip to main content](#screen-reader-main-content)[Skip to article](#screen-reader-main-title)\n[![Elsevier logo](https://www.sciencedirect.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg)ScienceDirect](https://www.sciencedirect.com/)\n[My account](https://www.sciencedirect.com/user/login?targetURL=/science/article/pii/S1574013724000480&amp;from=globalheader)\n[Sign in](https://www.sciencedirect.com/user/institution/login?targetURL=/science/article/pii/S1574013724000480)\n* [View**PDF**](https://www.sciencedirect.com/science/article/pii/S1574013724000480/pdfft?md5=d276b7a8b6796ecd0218ecd68c33f3ff&amp;pid=1-s2.0-S1574013724000480-main.pdf)\n* Download full issue\nSearch ScienceDirect\n## Outline\n1. [Abstract](#abs0001)\n2. [Keywords](#keys0001)\n3. [1. Introduction](#sec0001)\n4. [2. Data science research field-based category](#sec0004)\n5. [3. Artificial intelligence (AI) research field-based category](#sec0028)\n6. [4. Experimental evaluation](#sec0047)\n7. [5. Challenges and potential future perspectives for text classification](#sec0055)\n8. [6. Conclusion](#sec0056)\n9. [Declaration of competing interest](#coi0001)\n10. [Data availability](#refdata001)\n11. [References](#cebibl1)Show full outline\n## [Cited by (33)](#section-cited-by)\n## Figures (10)\n1. [![Fig. 1. Our research field-based taxonomy for text classification employs a detailed,\u2026](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724000480-gr1.sml)](#fig0001)\n2. [![Fig. 2. The procedure of Decision Tree is illustrated in the figure](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724000480-gr2.sml)](#fig0002)\n3. [![Fig. 3. The figure illustrates an example of the probabilistic graphical model process](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724000480-gr3.sml)](#fig0003)\n4. [![Fig. 4. The figure depicts the process of the K-Nearest Neighbors-based classifier](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724000480-gr4.sml)](#fig0004)\n5. [![Fig. 5. The figure demonstrates the process of deep learning](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724000480-gr5.sml)](#fig0005)\n6. [![Fig. 6. Depicting the results of comparing Recurrent Neural Network-Based Data Science\u2026](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724000480-gr6.sml)](#fig0006)Show 4 more figures\n## Tables (21)\n1. [Table 1](#tbl0001)\n2. [Table 2](#tbl0002)\n3. [Table 3](#tbl0003)\n4. [Table 4](#tbl0004)\n5. [Table 5](#tbl0005)\n6. [Table 6](#tbl0006)Show all tables\n[![Elsevier](https://www.sciencedirect.com/us-east-1/prod/bd96d51d266808527bf1018bd38b59c0b4bc6286/image/elsevier-non-solus.svg)](https://www.sciencedirect.com/journal/computer-science-review)\n## [Computer Science Review](https://www.sciencedirect.com/journal/computer-science-review)\n[Volume 54](https://www.sciencedirect.com/journal/computer-science-review/vol/54/suppl/C),November 2024, 100664\n[![Computer Science Review](https://ars.els-cdn.com/content/image/1-s2.0-S1574013724X00040-cov150h.gif)](https://www.sciencedirect.com/journal/computer-science-review/vol/54/suppl/C)\n# Review Article\nA comprehensive survey of text classification techniques and their research applications: Observational and experimental insights\nAuthor links open overlay panel[KamalTahaa](https://www.sciencedirect.com/author/23398734900/kamal-hassan-taha),[Paul D.Yoob](https://www.sciencedirect.com/author/23979079900/paul-d-yoo),[ChanYeunc](https://www.sciencedirect.com/author/6508380997/chan-yeob-yeun),[DirarHomouzd](https://www.sciencedirect.com/author/15520719300/dirar-mohammad-al-homouz),AyaTahae\nShow more\nOutline\nAdd to Mendeley\nShare\nCite\n[https://doi.org/10.1016/j.cosrev.2024.100664](https://doi.org/10.1016/j.cosrev.2024.100664)[Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S1574013724000480&amp;orderBeanReset=true)\nUnder a Creative Commons[license](http://creativecommons.org/licenses/by-nc/4.0/)\nOpen access\n## Abstract\nThe[exponential growth](https://www.sciencedirect.com/topics/computer-science/exponential-growth)of textual data presents substantial challenges in management and analysis, notably due to high storage and processing costs.[Text classification](https://www.sciencedirect.com/topics/computer-science/text-classification), a vital aspect of text mining, provides[robust solutions](https://www.sciencedirect.com/topics/computer-science/robust-solution)by enabling efficient categorization and organization of text data. These techniques allow individuals, researchers, and businesses to derive meaningful patterns and insights from large volumes of text. This survey paper introduces a comprehensive taxonomy specifically designed for text classification based on research fields. The taxonomy is structured into hierarchical levels: research field-based category, research field-based sub-category, methodology-based technique, methodology sub-technique, and research field applications. We employ a dual evaluation approach: empirical and experimental. Empirically, we assess text[classification techniques](https://www.sciencedirect.com/topics/computer-science/classification-technique)across four critical criteria. Experimentally, we compare and rank the methodology sub-techniques within the same methodology technique and within the same overall research field sub-category. This structured taxonomy, coupled with thorough evaluations, provides a detailed and nuanced understanding of text[classification algorithms](https://www.sciencedirect.com/topics/computer-science/classification-algorithm)and their applications, empowering researchers to make informed decisions based on precise, field-specific insights.\n* [Previousarticlein issue](https://www.sciencedirect.com/science/article/pii/S1574013724000492)\n* [Nextarticlein issue](https://www.sciencedirect.com/science/article/pii/S1574013724000510)\n## Keywords\nText classification\nText data mining\nData science\nArtificial intelligence\nDeep learning\n## 1. Introduction\nThe advent of automated data acquisition tools and[high throughput](https://www.sciencedirect.com/topics/computer-science/high-throughput)technologies has significantly contributed to the generation of vast volumes of text data, with sources like Wikipedia and Twitter leading the charge in this data explosion [[1](#bib0001),[2](#bib0002)]. This proliferation is further augmented by technologies such as cloud storage, sensor networks, and social networks, collectively adding to the vast data landscape [[3](#bib0003)]. However, the sheer volume and complexity of managing and processing such data can be daunting due to the extensive storage requirements and the high costs and time involved in processing.[Text classification](https://www.sciencedirect.com/topics/computer-science/text-classification), as a critical component of text mining, has emerged as a pivotal technique in addressing these challenges [[4](#bib0004)].\nText classification specifically focuses on categorizing and organizing text data to facilitate easier management and analysis. These techniques leverage[natural language processing](https://www.sciencedirect.com/topics/computer-science/natural-language-processing),[machine learning](https://www.sciencedirect.com/topics/computer-science/machine-learning), and data mining to extract meaningful patterns and categorize text data, thereby transforming unstructured text into structured, actionable insights [[5](#bib0005)]. Applications of these methods are crucial in areas such as[document classification](https://www.sciencedirect.com/topics/computer-science/document-classification), where texts are categorized based on their content, and information retrieval, which involves searching through large datasets to find relevant documents [[2](#bib0002)]. Additionally, text classification is pivotal in[sentiment analysis](https://www.sciencedir...",
      "url": "https://www.sciencedirect.com/science/article/pii/S1574013724000480"
    },
    {
      "title": "Semantic text classification: A survey of past and recent advances",
      "text": "Semantic text classification: A survey of past and recent advances - ScienceDirect\n[Skip to main content](#screen-reader-main-content)[Skip to article](#screen-reader-main-title)\n[![Elsevier logo](https://www.sciencedirect.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg)ScienceDirect](https://www.sciencedirect.com/)\n[My account](https://www.sciencedirect.com/user/login?targetURL=/science/article/pii/S0306457317305757&amp;from=globalheader)\n[Sign in](https://www.sciencedirect.com/user/institution/login?targetURL=/science/article/pii/S0306457317305757)\n* [Access through**your organization**](https://www.sciencedirect.com/user/institution/login?targetUrl=/science/article/pii/S0306457317305757)\n* [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0306457317305757/purchase)\nSearch ScienceDirect\n## Article preview\n* [Abstract](#preview-section-abstract)\n* [Introduction](#preview-section-introduction)\n* [Section snippets](#preview-section-snippets)\n* [References (136)](#preview-section-references)\n* [Cited by (174)](#preview-section-cited-by)\n[![Elsevier](https://www.sciencedirect.com/us-east-1/prod/2e1df968e387a91477d5f463ad9484efb3b257c9/image/elsevier-non-solus.svg)](https://www.sciencedirect.com/journal/information-processing-and-management)\n## [Information Processing &amp; Management](https://www.sciencedirect.com/journal/information-processing-and-management)\n[Volume 54, Issue 6](https://www.sciencedirect.com/journal/information-processing-and-management/vol/54/issue/6),November 2018, Pages 1129-1153\n[![Information Processing &amp; Management](https://ars.els-cdn.com/content/image/1-s2.0-S0306457318X00053-cov150h.gif)](https://www.sciencedirect.com/journal/information-processing-and-management/vol/54/issue/6)\n# Semantic text classification: A survey of past and recent advances\nAuthor links open overlay panelBernaAlt\u0131nel,Murat CanGaniz\nShow more\nAdd to Mendeley\nShare\nCite\n[https://doi.org/10.1016/j.ipm.2018.08.001](https://doi.org/10.1016/j.ipm.2018.08.001)[Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0306457317305757&amp;orderBeanReset=true)\n## Abstract\nAutomatic[text classification](https://www.sciencedirect.com/topics/computer-science/text-classification)is the task of organizing documents into pre-determined classes, generally using[machine learning algorithms](https://www.sciencedirect.com/topics/computer-science/machine-learning-algorithm). Generally speaking, it is one of the most important methods to organize and make use of the gigantic amounts of information that exist in unstructured textual format. Text classification is a widely studied research area of language processing and text mining. In traditional text classification, a document is represented as a bag of words where the words in other words terms are cut from their finer context i.e. their location in a sentence or in a document. Only the broader context of document is used with some type of term[frequency information](https://www.sciencedirect.com/topics/computer-science/frequency-information)in the vector space. Consequently, semantics of words that can be inferred from the finer context of its location in a sentence and its relations with neighboring words are usually ignored. However, meaning of words, semantic connections between words, documents and even classes are obviously important since methods that capture semantics generally reach better classification performances. Several surveys have been published to analyze diverse approaches for the traditional text classification methods. Most of these surveys cover application of different semantic term relatedness methods in text classification up to a certain degree. However, they do not specifically target semantic text[classification algorithms](https://www.sciencedirect.com/topics/computer-science/classification-algorithm)and their advantages over the traditional text classification. In order to fill this gap, we undertake a comprehensive discussion of semantic text classification vs. traditional text classification. This survey explores the past and recent advancements in semantic text classification and attempts to organize existing approaches under five fundamental categories; domain knowledge-based approaches, corpus-based approaches,[deep learning](https://www.sciencedirect.com/topics/computer-science/deep-learning)based approaches, word/character sequence enhanced approaches and linguistic enriched approaches. Furthermore, this survey highlights the advantages of semantic text classification algorithms over the traditional text classification algorithms.\n## Introduction\nText mining studies steadily gain importance in recent years due to the wide range of sources that produce enormous amounts of data, such as social networks, blogs/forums, web sites, e-mails, and online libraries publishing research papers. The growth of electronic textual data will no doubt continue to increase with new developments in technology such as speech to text engines and digital assistants or intelligent personal assistants. Automatically processing, organizing and handling this textual data is a fundamental problem. Text mining has several important applications like classification (i.e., supervised, unsupervised and semi-supervised classification), document filtering, summarization, and sentiment analysis/opinion classification. Natural Language Processing (NLP), Machine Learning (ML) and Data Mining (DM) methods work together to detect patterns from the different types of the documents and classify them in an automatic manner (Sebastiani,2005).\nA traditional method for representing documents is called Bag of Words (BOW). This representation technique only include information about the terms and their corresponding frequencies in a document independent of their locations in the sentence or document. It is also called the Vector Space Model (VSM) since each document is represented as a vector of term frequencies in the vocabulary. Each of these terms in the vocabulary denotes an independent (orthogonal) dimension in the vector space, which usually results in a very high dimensional document vectors with only a few of them taking a frequency value which in turn yields to high sparsity. Furthermore, this representation does not take into account semantic associations between words. For instance, two words written as a different sequence of characters constitute different orthogonal dimensions of this vector space although they may be synonymous. Additionally, order of these words in the sentences are completely lost in the BOW representation. This approach mainly emphasizes the existence of some form of frequency information of terms. The BOW methodology makes the representation of documents simpler by disregarding the following several different semantic and syntactic relations between words in natural language: Firstly, it disregards the multi-word expressions by separating them into independent terms. Secondly, it treats polysemous words (words with multiple meanings) as a single entity because the word is separated from its neighboring words that determine its sense. Thirdly, the BOW approach maps synonymous words into distinct terms (Salton &amp;&amp; Yang,1973).\nA text classifier is expected to label textual documents with pre-determined classes with an obvious assumption that each class consist of similar documents, usually talking about a particular topic that is different from the topics of other classes. However, vector space demonstration of texts usually results in high dimensionality and consequently high sparsity. This is a big difficulty especially when there are numerous class labels but inadequate training data for each of them. Obtaining labeled quality data for training is usually very expensive in real world applications. Accordingly, an accurate text classifier should have the capability of using this semantic information.\nIn semantic text c...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0306457317305757"
    }
  ]
}