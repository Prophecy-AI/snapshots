{
  "query": "How to handle high missing value rates in categorical features like 75% missing for Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "High missing\u2011value rates (e.g.,\u202f\u2248\u202f75\u202f%) in categorical columns are common in Kaggle datasets, and the most reliable way to keep useful signal while avoiding noisy bias is to treat the missingness explicitly before encoding.\u202fThe steps below follow the best\u2011practice methods described in the Analytics\u202fVidhya and Medium guides and the scikit\u2011learn recommendations\u202f([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables), [Medium \u2013 Lomash Bhuva](https://medium.com/@lomashbhuva/handling-missing-values-in-categorical-data-techniques-and-best-practices-0a3ddd523824), [Stack\u202fOverflow](https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o)).\n\n1. **Quantify missingness**  \n   * Compute the percentage of missing values for each categorical feature.  \n   * Flag any column with a missing rate\u202f\u2265\u202f70\u202f% (or a domain\u2011specific threshold) for special handling.\n\n2. **Create an explicit \u201cmissing\u201d category**  \n   * Replace NaNs with a placeholder string such as `\"Missing\"` or `\"Not_provided\"` using `SimpleImputer(strategy='constant', fill_value='Missing')`.  \n   * This preserves the row (no deletion) and lets the model learn whether the absence itself is predictive\u202f([Medium \u2013 Lomash Bhuva](https://medium.com/@lomashbhuva/handling-missing-values-in-categorical-data-techniques-and-best-practices-0a3ddd523824)).\n\n3. **Consider dropping the column if it carries little information**  \n   * If the column is >\u202f90\u202f% missing *and* the \u201cMissing\u201d category appears in >\u202f95\u202f% of its rows, the feature may add little variance. Deleting the column is acceptable when it does not degrade validation scores\u202f([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables)).\n\n4. **Impute with the most frequent value (optional)**  \n   * For columns with moderate missingness (e.g., 30\u201370\u202f%), you can also try `SimpleImputer(strategy='most_frequent')`.  \n   * Compare performance against the explicit \u201cMissing\u201d category to decide which works better\u202f([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables)).\n\n5. **Model\u2011based imputation (advanced)**  \n   * If you suspect systematic patterns, train a lightweight model (e.g., a decision tree) on rows where the column is present and predict the missing entries.  \n   * Use the predictions as imputed values only after validating that they improve cross\u2011validation metrics\u202f([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables)).\n\n6. **Encode the processed categorical features**  \n   * Feed the cleaned columns into a pipeline: `SimpleImputer` \u2192 `OneHotEncoder(handle_unknown='ignore')` (or CatBoost/LightGBM which handle \u201cMissing\u201d natively).  \n   * This ensures the encoder sees a consistent set of categories, including the explicit missing label\u202f([Stack\u202fOverflow](https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o)).\n\n7. **Validate and iterate**  \n   * Evaluate model performance on a hold\u2011out set or via cross\u2011validation for each strategy (drop, \u201cMissing\u201d category, most\u2011frequent, model\u2011imputed).  \n   * Choose the approach that yields the highest validation score while keeping the pipeline simple and reproducible.\n\n**Quick checklist for a 75\u202f% missing categorical feature in a Kaggle competition**\n\n- \u2705 Replace NaNs with a dedicated \u201cMissing\u201d label (SimpleImputer constant).  \n- \u2705 If the column becomes almost entirely \u201cMissing\u201d, drop it.  \n- \u2705 Optionally test most\u2011frequent imputation or a small predictive model.  \n- \u2705 Encode with OneHotEncoder (or a tree\u2011based model that natively handles missing categories).  \n- \u2705 Compare validation metrics; keep the version that improves score.\n\nFollowing these steps lets you retain potentially useful missing\u2011ness information without inflating noise, a practice that consistently ranks well in Kaggle competitions.",
      "url": ""
    },
    {
      "title": "Handling Missing Values of Categorical Variables - Analytics Vidhya",
      "text": "[Master Generative AI with 10+ Real-world Projects in 2025!\\\n\\\n- d:\\\n- h:\\\n- m:\\\n- s\\\n\\\nDownload Projects](https://www.analyticsvidhya.com/pinnacleplus/pinnacleplus-projects?utm_source=blog_india&utm_medium=desktop_flashstrip&utm_campaign=15-Feb-2025||&utm_content=projects)\n\n[Interview Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)\n\n[Career](https://www.analyticsvidhya.com/blog/category/career/?ref=category)\n\n[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=category)\n\n[Prompt Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)\n\n[ChatGPT](https://www.analyticsvidhya.com/blog/category/chatgpt/?ref=category)\n\n[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=category)\n\n[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)\n\n[RAG](https://www.analyticsvidhya.com/blog/category/rag/?ref=category)\n\n[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)\n\n[Machine Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)\n\n[Deep Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)\n\n[GenAI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)\n\n[LLMOps](https://www.analyticsvidhya.com/blog/category/llmops/?ref=category)\n\n[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)\n\n[NLP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)\n\n[SQL](https://www.analyticsvidhya.com/blog/category/sql/?ref=category)\n\n[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category)\n\n#### Reading list\n\n##### Basics of Machine Learning\n\n[Machine Learning Basics for a Newbie](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)\n\n##### Machine Learning Lifecycle\n\n[6 Steps of Machine learning Lifecycle](https://www.analyticsvidhya.com/blog/2020/09/10-things-know-before-first-data-science-project/) [Introduction to Predictive Modeling](https://www.analyticsvidhya.com/blog/2015/09/build-predictive-model-10-minutes-python/)\n\n##### Importance of Stats and EDA\n\n[Introduction to Exploratory Data Analysis & Data Insights](https://www.analyticsvidhya.com/blog/2021/02/introduction-to-exploratory-data-analysis-eda/) [Descriptive Statistics](https://www.analyticsvidhya.com/blog/2021/06/how-to-learn-mathematics-for-machine-learning-what-concepts-do-you-need-to-master-in-data-science/) [Inferential Statistics](https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/) [How to Understand Population Distributions?](https://www.analyticsvidhya.com/blog/2014/07/statistics/)\n\n##### Understanding Data\n\n[Reading Data Files into Python](https://www.analyticsvidhya.com/blog/2021/09/how-to-extract-tabular-data-from-doc-files-using-python/) [Different Variable Datatypes](https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-data-types-in-statistics-for-data-science/)\n\n##### Probability\n\n[Probability for Data Science](https://www.analyticsvidhya.com/blog/2021/03/statistics-for-data-science/) [Basic Concepts of Probability](https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/) [Axioms of Probability](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/) [Conditional Probability](https://www.analyticsvidhya.com/blog/2017/03/conditional-probability-bayes-theorem/)\n\n##### Exploring Continuous Variable\n\n[Central Tendencies for Continuous Variables](https://www.analyticsvidhya.com/blog/2021/07/the-measure-of-central-tendencies-in-statistics-a-beginners-guide/) [Spread of Data](https://www.analyticsvidhya.com/blog/2021/04/dispersion-of-data-range-iqr-variance-standard-deviation/) [KDE plots for Continuous Variable](https://www.analyticsvidhya.com/blog/2020/07/univariate-analysis-visualization-with-illustrations-in-python/) [Overview of Distribution for Continuous variables](https://www.analyticsvidhya.com/blog/2015/11/8-ways-deal-continuous-variables-predictive-modeling/) [Normal Distribution](https://www.analyticsvidhya.com/blog/2020/04/statistics-data-science-normal-distribution/) [Skewed Distribution](https://www.analyticsvidhya.com/blog/2021/05/how-to-transform-features-into-normal-gaussian-distribution/) [Skeweness and Kurtosis](https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/) [Distribution for Continuous Variable](https://www.analyticsvidhya.com/blog/2021/07/probability-types-of-probability-distribution-functions/)\n\n##### Exploring Categorical Variables\n\n[Central Tendencies for Categorical Variables](https://www.analyticsvidhya.com/blog/2021/04/3-central-tendency-measures-mean-mode-median/) [Understanding Discrete Distributions](https://www.analyticsvidhya.com/blog/2021/01/discrete-probability-distributions/) [Performing EDA on Categorical Variables](https://www.analyticsvidhya.com/blog/2020/08/exploratory-data-analysiseda-from-scratch-in-python/)\n\n##### Missing Values and Outliers\n\n[Dealing with Missing Values](https://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/) [Understanding Outliers](https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/) [Identifying Outliers in Data](https://www.analyticsvidhya.com/blog/2021/07/how-to-treat-outliers-in-a-data-set/) [Outlier Detection in Python](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/) [Outliers Detection Using IQR, Z-score, LOF and DBSCAN](https://www.analyticsvidhya.com/blog/2022/08/dealing-with-outliers-using-the-z-score-method/)\n\n##### Central Limit theorem\n\n[Sample and Population](https://www.analyticsvidhya.com/blog/2021/06/introductory-statistics-for-data-science/) [Central Limit Theorem](https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/) [Confidence Interval and Margin of Error](https://www.analyticsvidhya.com/blog/2021/08/intermediate-statistical-concepts-for-data-science/)\n\n##### Bivariate Analysis Introduction\n\n[Bivariate Analysis Introduction](https://www.analyticsvidhya.com/blog/2021/04/top-python-libraries-to-automate-exploratory-data-analysis-in-2021/)\n\n##### Continuous - Continuous Variables\n\n[Covariance](https://www.analyticsvidhya.com/blog/2021/09/different-type-of-correlation-metrics-used-by-data-scientist/) [Pearson Correlation](https://www.analyticsvidhya.com/blog/2021/01/beginners-guide-to-pearsons-correlation-coefficient/) [Spearman's Correlation & Kendall's Tau](https://www.analyticsvidhya.com/blog/2021/03/comparison-of-pearson-and-spearman-correlation-coefficients/) [Correlation versus Causation](https://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/) [Tabular and Graphical methods for Bivariate Analysis](https://www.analyticsvidhya.com/blog/2020/10/the-clever-ingredient-that-decide-the-rise-and-the-fall-of-your-machine-learning-model-exploratory-data-analysis/) [Performing Bivariate Analysis on Continuous-Continuous Variables](https://www.analyticsvidhya.com/blog/2022/03/exploratory-data-analysis-eda-credit-card-fraud-detection-case-study/)\n\n##### Continuous Categorical\n\n[Tabular and Graphical methods for Continuous-Categorical Variables](https://www.analyticsvidhya.com/blog/2015/05/data-visualization-resource/) [Introduction to Hypothesis Testing](https://www.analyticsvidhya.com/blog/2021/09/hypothesis-testing-in-machine-learning-everything-you-need-to-know/) [P-value](https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/) [Two sample Z-test](https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/) [T-test](https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/) [T-test vs Z-test](https://www.analyticsvidhya.com/blog/2021/06/featu...",
      "url": "https://www.analyticsvidhya.com/blog/2021/04/how-to-handle-missing-values-of-categorical-variables"
    },
    {
      "title": "Handling Missing Categorical Data: Techniques & Practices",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F0a3ddd523824&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40lomashbhuva%2Fhandling-missing-values-in-categorical-data-techniques-and-best-practices-0a3ddd523824&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40lomashbhuva%2Fhandling-missing-values-in-categorical-data-techniques-and-best-practices-0a3ddd523824&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# **Handling Missing Values in Categorical Data: Techniques and Best Practices** \ud83c\udf1f\ud83d\ude80\n\n[Lomash Bhuva](https://medium.com/@lomashbhuva?source=post_page---byline--0a3ddd523824---------------------------------------)\n\n4 min read\n\n\u00b7\n\nFeb 4, 2025\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nIn machine learning and data science, missing data is one of the most common challenges when preprocessing datasets. Missing values in categorical columns can distort the accuracy of predictive models and create biases in data analysis. Therefore, effective strategies for handling these missing values are essential.\n\nIn this blog, we\u2019ll explore two fundamental techniques for handling missing values in categorical data: **Most Frequent Imputation** and **Creating a New Category for Missing Values**. Additionally, we\u2019ll discuss their advantages, disadvantages, and scenarios where each technique is best applied.\n\n# Why Handling Missing Categorical Data is Important\n\nCategorical variables represent distinct categories or labels rather than numerical values. Examples include columns for gender, city, and product types. Missing values in these variables can disrupt machine learning models, causing poor predictions and inefficiencies. Therefore, handling missing categorical data is critical to building reliable machine learning models.\n\nMachine learning algorithms are not inherently designed to handle missing values, and most require complete datasets to function correctly. Effective imputation techniques can ensure that the absence of data doesn\u2019t compromise your model\u2019s accuracy.\n\n# Techniques for Handling Missing Values in Categorical Data\n\nLet\u2019s dive into two practical methods for handling missing values in categorical columns.\n\n# 1\\. Most Frequent Imputation\n\n## What is Most Frequent Imputation?\n\nIn this approach, missing values are replaced by the **most frequently occurring category (mode)** of the column. This technique is based on the assumption that the most frequent category is the most representative value for the missing entries.\n\n## When to Use This Technique\n\n- When missing data is **Missing Completely at Random (MCAR)** and doesn\u2019t follow a specific pattern.\n- When a particular category naturally dominates the distribution of the column.\n\n## Advantages\n\n- Simple and computationally efficient.\n- Helps maintain the consistency of categorical variables.\n- Works well for datasets where missing data is minimal (less than 5% of the column).\n\n## Disadvantages\n\n- Can distort the natural distribution of the data if the missing values are not random.\n- May introduce bias by overrepresenting the most frequent category.\n\n## Example\n\nImagine you have the following dataset with a \u201cCity\u201d column:\n\nCity Mumbai Delhi None Mumbai Kolkata\n\nThe most frequent city in the column is \u201cMumbai.\u201d Using most frequent imputation, all missing values will be replaced by \u201cMumbai.\u201d\n\n## Code Example for Most Frequent Imputation\n\n```\nfrom sklearn.impute import SimpleImputerimport pandas as pd\n```\n\n```\n# Sample datasetdata = {    'City': ['Mumbai', 'Delhi', 'Kolkata', None, 'Mumbai']}df = pd.DataFrame(data)# Imputation using most frequent strategyimputer_frequent = SimpleImputer(strategy='most_frequent')df['City'] = imputer_frequent.fit_transform(df[['City']])print(df)\n```\n\n# 2\\. Creating a New Category for Missing Values\n\n## What is This Technique?\n\nInstead of filling missing values with existing categories, this technique introduces a new category like **\u201cMissing\u201d** or **\u201cUnknown\u201d** to represent all missing values.\n\n## When to Use This Technique\n\n- When the missing data is **not random** and may provide meaningful insights.\n- When missing values account for a significant portion of the data (more than 5%).\n- When preserving information about the fact that data was missing is important for model training.\n\n## Advantages\n\n- Preserves the information that data was missing, which may be valuable for machine learning models.\n- Helps in identifying patterns related to missing data.\n- Works well for datasets with a high proportion of missing categorical values.\n\n## Disadvantages\n\n- Adds an additional category that may not always be interpretable.\n- May lead to overfitting in some models due to the introduction of an artificial category.\n\n## Example\n\nIf the \u201cFireplace Quality\u201d column in a real estate dataset has 60% missing values, instead of imputing with the most frequent value, you can replace missing values with a new category like \u201cMissing.\u201d\n\n## Code Example for Creating a New Category\n\n```\nimport pandas as pd\n```\n\n```\n# Sample datasetdata = {    'Garage_Quality': ['Good', 'Excellent', None, 'Poor', None, 'Good']}df = pd.DataFrame(data)# Replace missing values with a new category \"Missing\"df['Garage_Quality'] = df['Garage_Quality'].fillna('Missing')print(df)\n```\n\n# Comparison of the Two Techniques\n\nTechnique Best for Advantages Disadvantages Most Frequent Imputation MCAR with minimal missing data Easy to implement, computationally efficient Can distort data distribution New Category Imputation Non-random or significant missing data Preserves information, handles large missing data May lead to overfitting\n\n# Key Considerations When Handling Missing Categorical Data\n\n1. **Visualize Data Distribution:** Always analyze the distribution of categories before and after imputation to ensure that the selected technique hasn\u2019t introduced biases.\n2. **Evaluate Model Performance:** Post-imputation, monitor your machine learning model\u2019s accuracy and performance to validate the chosen technique.\n3. **Keep a Copy of the Original Data:** Never overwrite your original dataset when applying imputation techniques to maintain a reference for validation.\n4. **Incorporate Domain Knowledge:** Use domain knowledge to decide whether creating a new category or using the most frequent imputation is more appropriate.\n5. **Avoid Overfitting:** Introducing artificial categories can sometimes lead to overfitting in models. Validate your imputation method with cross-validation.\n\n# Conclusion\n\nHandling missing values in categorical data is crucial for building robust and reliable machine learning models. **Most Frequent Imputation** and **Creating a New Category for Missing Values** are two simple yet effective techniques to handle missing categorical data. Choosing the right approach depends on the nature of the data, the proportion of missing values, and the data distribution.\n\nBy carefully analyzing the data and applying the appropriate imputation technique, you can ensure that your machine learning models remain accurate and effective, even when faced with incomplete datasets.\n\n# Next Steps\n\n- Explore advanced techniques like KNN Imputer and Multivariate Imputation for more complex datasets.\n- Learn how to handle missing val...",
      "url": "https://medium.com/@lomashbhuva/handling-missing-values-in-categorical-data-techniques-and-best-practices-0a3ddd523824"
    },
    {
      "title": "How to handle missing values (NaN) in categorical data when using ...",
      "text": "Join Stack Overflow\u2019s first live community AMA on February 26th, at 3 PM ET.\n[Learn more](https://meta.stackexchange.com/questions/406399/join-us-for-our-first-community-wide-ama-ask-me-anything-with-stack-overflow-s?utm_medium=ppc&utm_source=stackoverflow-community&utm_campaign=community-ama&utm_content=announcement-banner1)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [How to handle missing values (NaN) in categorical data when using scikit-learn OneHotEncoder?](https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked4 years, 8 months ago\n\nModified [11 months ago](https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o?lastactivity)\n\nViewed\n33k times\n\n28\n\nI have recently started learning python to develop a predictive model for a research project using machine learning methods. I have a large dataset comprised of both numerical and categorical data. The dataset has lots of missing values. I am currently trying to encode the categorical features using OneHotEncoder. When I read about OneHotEncoder, my understanding was that for a missing value (NaN), OneHotEncoder would assign 0s to all the feature's categories, as such:\n\n```\n0     Male\n1     Female\n2     NaN\n\n```\n\nAfter applying OneHotEncoder:\n\n```\n0     10\n1     01\n2     00\n\n```\n\nHowever, when running the following code:\n\n```\n    # Encoding categorical data\n    from sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing import OneHotEncoder\n\n    ct = ColumnTransformer([('encoder', OneHotEncoder(handle_unknown='ignore'), [1])],\n                           remainder='passthrough')\n    obj_df = np.array(ct.fit_transform(obj_df))\n    print(obj_df)\n\n```\n\nI am getting the error **ValueError: Input contains NaN**\n\nSo I am guessing my previous understanding of how OneHotEncoder handles missing values is wrong.\nIs there a way for me to get the functionality described above? I know imputing the missing values before encoding will resolve this issue, but I am reluctant to do this as I am dealing with medical data and fear that imputation may decrease the predictive accuracy of my model.\n\nI found this [question](https://stackoverflow.com/questions/58222008/nan-giving-valueerror-in-onehotencoder-in-scikit-learn) that is similar but the answer doesn't offer a detailed enough solution on how to deal with the NaN values.\n\nLet me know what your thoughts are, thanks.\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [machine-learning](https://stackoverflow.com/questions/tagged/machine-learning)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n\n[Share](https://stackoverflow.com/q/62409303)\n\n[Improve this question](https://stackoverflow.com/posts/62409303/edit)\n\nFollow\n\n[edited Jun 16, 2020 at 13:23](https://stackoverflow.com/posts/62409303/revisions)\n\nsums22\n\nasked Jun 16, 2020 at 13:09\n\n[![sums22's user avatar](https://www.gravatar.com/avatar/6df86729130c9a20a2df0e2aeefd7443?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/7373787/sums22)\n\n[sums22](https://stackoverflow.com/users/7373787/sums22) sums22\n\n2,03333 gold badges1717 silver badges2727 bronze badges\n\n2\n\n- 1\n\n\n\n\n\nKeep an eye on [github.com/scikit-learn/scikit-learn/issues/11996](https://github.com/scikit-learn/scikit-learn/issues/11996).\n\n\u2013\u00a0[Ben Reiniger](https://stackoverflow.com/users/10495893/ben-reiniger)\n\nCommentedJun 16, 2020 at 16:12\n\n- Thank you @BenReiniger, good to know the developers agree OneHotEncoder should handle missing values! :)\n\n\u2013\u00a0[sums22](https://stackoverflow.com/users/7373787/sums22)\n\nCommentedJun 16, 2020 at 21:16\n\n\n[Add a comment](https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o)\u00a0\\|\n\n## 8 Answers 8\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n14\n\nYou will need to impute the missing values before. You can define a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) with an imputing step using [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) setting a `constant` strategy to input a new category for null fields, prior to the OneHot encoding:\n\n```\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, [0])\n    ])\n\n```\n\n* * *\n\n```\ndf = pd.DataFrame(['Male', 'Female', np.nan])\npreprocessor.fit_transform(df)\narray([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.]])\n\n```\n\n[Share](https://stackoverflow.com/a/62409663)\n\n[Improve this answer](https://stackoverflow.com/posts/62409663/edit)\n\nFollow\n\n[edited May 22, 2022 at 13:23](https://stackoverflow.com/posts/62409663/revisions)\n\n[![Fernando Wittmann's user avatar](https://i.sstatic.net/5XNbO.jpg?s=64)](https://stackoverflow.com/users/3423639/fernando-wittmann)\n\n[Fernando Wittmann](https://stackoverflow.com/users/3423639/fernando-wittmann)\n\n2,5472424 silver badges1818 bronze badges\n\nanswered Jun 16, 2020 at 13:27\n\n[![yatu's user avatar](https://i.sstatic.net/Hj2XY.jpg?s=64)](https://stackoverflow.com/users/9698684/yatu)\n\n[yatu](https://stackoverflow.com/users/9698684/yatu) yatu\n\n88.2k1212 gold badges9292 silver badges145145 bronze badges\n\n6\n\n- Thanks for the answer and the helpful code. Do you think imputing using the most frequent value in the column will not compromise the predictive accuracy of the model?\n\n\u2013\u00a0[sums22](https://stackoverflow.com/users/7373787/sums22)\n\nCommentedJun 16, 2020 at 13:32\n\n- 1\n\n\n\n\n\nThis is quite a common strategy for categorical data. Its not ideal but you do have missing data afterall. Though in the cases where there isn't too much missing data it is a reasonable approach @sums\n\n\u2013\u00a0[yatu](https://stackoverflow.com/users/9698684/yatu)\n\nCommentedJun 16, 2020 at 13:34\n\n- 3\n\n\n\n\n\nI would suggest using `strategy='constant'`, `fill_value='missing'` to preserve the categories' structure, rather than imputing with the most frequent.\n\n\u2013\u00a0[Ben Reiniger](https://stackoverflow.com/users/10495893/ben-reiniger)\n\nCommentedJun 16, 2020 at 16:10\n\n- 1\n\n\n\n\n\n@BenReiniger does this mean I will have an additional category after encoding that will be the 'missing' category? And if so, do you suggest I then delete the 'missing' category from all encoded features (similar to the answer posted by Om Rastogi) or leave it?\n\n\u2013\u00a0[sums22](https://stackoverflow.com/users/7373787/sums22)\n\nCommentedJun 16, 2020 at 20:34\n\n- 1\n\n\n\n\n\n@sums22, that depends on your subsequent modeling I think. Personally: if you need to avoid multicolinearity, then drop it; and otherwise, keep it.\n\n\u2013\u00a0[Ben Reiniger](https://stackoverflow.com/users/10495893/ben-reiniger)\n\nCommentedJun 16, 2020 at 21:18\n\n\n\\|\u00a0[Show **1** more comment](https://stackoverflow.com/questions/62409303/how-to-handle-mis...",
      "url": "https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o"
    },
    {
      "title": "A Guide to Handling High Cardinality in Categorical Variables",
      "text": "Zoom image will be displayed\n\nPhoto by [Riccardo Pelati](https://unsplash.com/@craig000?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n**High cardinality** refers to a situation in a dataset where a particular feature has a large number of distinct values. In other words, a feature with high cardinality has many unique categories or levels.\n\n> For example, consider a feature like \u201ccustomer ID\u201d or \u201cproduct ID.\u201d If you have a dataset with a large number of unique customer IDs or product IDs, it results in high cardinality for that particular feature.\n\n**Imbalanced targets** refer to a situation in a classification problem where the distribution of the target classes is not uniform, meaning that some classes have significantly fewer instances than others. In other words, there is an imbalance in the number of observations for each class in the target variable.\n\n> For example, consider a binary classification problem where the goal is to predict whether an email is spam (positive class) or not spam (negative class). If 95% of the emails are not spam, and only 5% are spam, this dataset has imbalanced targets.\n\nDealing with **high cardinality** features is important because they can **impact model performance**, **increase computation time, and introduce complexity** in data analysis. Simultaneously, **imbalanced datasets** can pose challenges for machine learning models because the **model may become biased toward the majority class**, leading to poor performance on the minority class.\n\n**A Practical Example on Dealing with High Cardinality problem:**\n\nStrategies for handling high cardinality data often involve techniques such as feature engineering, dimensionality reduction, or specific encoding methods to transform categorical variables into a more manageable format for machine learning algorithms.\n\nLet\u2019s examine the Titanic dataset as an illustrative example to grasp the concept of high cardinality in data analysis.\n\nZoom image will be displayed\n\nWithin the provided dataset, we can omit identification variables such as PassengerId, Name, and Ticket during the analysis. The target variable, \u201cSurvived,\u201d indicates whether a passenger survived (1) or not (0), where 0 signifies the individual did not survive, and 1 signifies survival.\n\nAs we commence exploring the dataset, we identify the presence of null values. Notably, the variable \u201c **Cabin\u201d** exhibits a substantial count of 687 missing values, suggesting a significant proportion. Consequently, it may be prudent to discard this variable. Conversely, variables such as **\u201cAge\u201d** and **\u201cEmbarked\u201d** exhibit a lower count of missing values, making it viable to retain them and subsequently apply imputation techniques to address the gaps.\n\nTo illustrate the effects of diminishing high cardinality, let\u2019s initially measure the accuracy of the Random Forest Classifier model in its current state. In this model, categorical variables like Sex and Embarked are solely mapped to corresponding numerical values as below.\n\nZoom image will be displayed\n\nUsing the current random forest classifier model, the captured accuracy stands at 81.56%. Lets discuss the Pros and cons of Mapping to numerical variables:\n\n**_Pros_**: Mapping categorical variables to numerical values reduces dimensionality, making the data more manageable. It is a straightforward conversion that allows the algorithm to interpret ordinal relationships between categories.\n\n**_Cons_** _:_ It may introduce an unintended ordinality, implying a meaningful order among categories where none exists. This can potentially lead to a misinterpretation by the model, especially if the categorical variable doesn\u2019t have a clear ordinal relationship.\n\nLet\u2019s now employ the **One-Hot Encoding method** to investigate the potential impact on the accuracy using the exact dataset.\n\nPrior to that, let\u2019s visualize the distribution of three categorical variables. The \u201cPClass\u201d variable consists of three categories, \u201cSex\u201d has two categories, and \u201cEmbarked\u201d contains 3 distinct values or categories.\n\nZoom image will be displayed\n\nBegin by initializing the OneHotEncoder object, followed by applying it to the designated categorical variables. This process transforms the categorical features into a one-hot encoded format, wherein each category is denoted by a binary column. Subsequently, a new DataFrame, referred to as \u201c **encoded\\_df**,\u201d is generated using these one-hot encoded features. The column names for this DataFrame are acquired through the \u201c **get\\_feature\\_names\\_out**\u201d method, providing names for the created binary columns based on the original categorical feature names.\n\nCombine the newly encoded data with the original dataset and remove the initial three variables. The resulting dataset will appear as follows:\n\nZoom image will be displayed\n\nExecuting the Random Forest Classifier model on the aforementioned dataset is expected to yield a marginal improvement in the model\u2019s accuracy, as indicated below:\n\nZoom image will be displayed\n\nIs it accurate to conclude that One-Hot Encoding is always the superior solution for addressing the cardinality issue of a categorical variable? The answer is No.\n\nTo understand better, let us discuss the pros and cons of One-Hot Encoding method:\n\n**_Pros_** _:_ One-Hot Encoding is effective in dealing with nominal categorical variables (categories without inherent order). It creates binary columns for each category, representing their presence or absence. This method prevents introducing artificial ordinality.\n\n**_Cons_** _:_ It increases **dimensionality significantly**, especially when dealing with categorical variables having a large number of unique categories (high cardinality). This can lead to the \u201c **curse of dimensionality**,\u201d making the dataset more sparse and potentially affecting the performance of some machine learning algorithms.\n\n> How to deal with the problem of \u201cCurse of dimensionality\u201d, I have explained in the below note:\n>\n> [https://medium.com/@niranjan.appaji/addressing-the-curse-of-dimensionality-unveiling-pcas-power-247956bc9a4d](https://medium.com/@niranjan.appaji/addressing-the-curse-of-dimensionality-unveiling-pcas-power-247956bc9a4d)\n\n**Choosing Between Them:**\n\n**_Mapping to Numerical Values:_** Suitable when there is an inherent ordinal relationship among categories, and preserving that order is important. It is a simpler approach and may work well when the cardinality is not excessively high.\n\n**_One-Hot Encoding:_** Preferred for nominal variables with no clear order, preventing the introduction of spurious relationships. However, its impact on dimensionality should be considered, especially in the context of the chosen machine learning algorithm.\n\nIn summary, the choice between mapping to numerical values and One-Hot Encoding depends on the nature of the categorical variable and its impact on the overall performance of the machine learning model. Experimentation and assessing the model\u2019s performance on validation data can help determine the most effective approach for a specific dataset.\n\n**My LinkedIn:** [**https://www.linkedin.com/in/niranjan-appaji-5b043990/**](https://www.linkedin.com/in/niranjan-appaji-5b043990/)",
      "url": "https://niranjanappaji.medium.com/a-guide-to-handling-high-cardinality-in-categorical-variables-7b4101d3af68"
    },
    {
      "title": "Handling Categorical Features - With Examples | kaggle_tutorials - Wandb",
      "text": "An application error occurred.\n\nClick to refresh the page.\n\n![Company Logo](https://cdn.cookielaw.org/logos/7ae3d8ca-f23a-4e50-a21c-edc54404815f/247bb628-f945-430d-a691-71470ed4595c/23d4561e-633b-46ba-844d-0eb603a26414/Weights_&_Biases_White_Text_(1).png)\n\n## Privacy Preference Center\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\nAllow All\n\n### Manage Consent Preferences\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nBack Button\n\n### Cookie List\n\nSearch Icon\n\nFilter Icon\n\nClear\n\ncheckbox labellabel\n\nApplyCancel\n\nConsentLeg.Interest\n\ncheckbox labellabel\n\ncheckbox labellabel\n\ncheckbox labellabel\n\nReject AllConfirm My Choices\n\n[![Powered by Onetrust](https://cdn.cookielaw.org/logos/static/powered_by_logo.svg)](https://www.onetrust.com/products/cookie-consent/)",
      "url": "https://wandb.ai/wandb_fc/kaggle_tutorials/reports/Handling-Categorical-Features-With-Examples--VmlldzoyMTY4NDgz"
    },
    {
      "title": "Handling categorical missing values ML",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Handling categorical missing values ML](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked4 years, 1 month ago\n\nModified [3 years ago](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml?lastactivity)\n\nViewed\n1k times\n\n2\n\n$\\\\begingroup$\n\nI have gone through [this](https://stackoverflow.com/questions/46120727/replace-missing-values-in-categorical-data) regarding handling missing values in categorical data.\n\nDataset has about `6 categorical columns` with `missing values`. This would be for a binary classification problem\n\nI see different approaches where one is to `just leave the missing values in category column as such`, other to impute using `from sklearn.preprocessing import Imputer`, but unsure which is better option.\n\nIn case if `imputing` is better option, which libraries could I use before applying the model like `LR,Decision Tree, RandomForest`.\n\nThanks!\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [python](https://datascience.stackexchange.com/questions/tagged/python)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [missing-data](https://datascience.stackexchange.com/questions/tagged/missing-data)\n- [data-imputation](https://datascience.stackexchange.com/questions/tagged/data-imputation)\n\n[Share](https://datascience.stackexchange.com/q/74391)\n\n[Improve this question](https://datascience.stackexchange.com/posts/74391/edit)\n\nFollow\n\n[edited Aug 8, 2020 at 12:51](https://datascience.stackexchange.com/posts/74391/revisions)\n\n[![Zephyr's user avatar](https://i.sstatic.net/8DjHv.jpg?s=64)](https://datascience.stackexchange.com/users/98307/zephyr)\n\n[Zephyr](https://datascience.stackexchange.com/users/98307/zephyr)\n\n99744 gold badges1111 silver badges2020 bronze badges\n\nasked May 18, 2020 at 11:23\n\n[![omdurg's user avatar](https://www.gravatar.com/avatar/9753f6586a0cb036109c9a87a52fa3de?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/91383/omdurg)\n\n[omdurg](https://datascience.stackexchange.com/users/91383/omdurg) omdurg\n\n15366 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml)\u00a0\\|\n\n## 4 Answers 4\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n2\n\n$\\\\begingroup$\n\nFirst of all I would look how many missing values there are in the column. If there are too much (~20%, generally its difficult to say how much is too much), I would drop the column because imputing 20 % of your data (without prior expert knowledge) or even more probably does not give you meaningful information anymore.\n\nSecondly I would look at correlations between missing values and other features. Maybe you are lucky and there is some correlation between missing values in column x and a categorical value in column y. Simply look at conditional distributions.\n\nIf you choose to impute, check the distribution of the categorical values for non-missing entries. If the distribution is heavily skewed, say there is 95% value 0 and only 5% value 1, you can use the median to impute. Again, the question is how informative this is in the end. Otherwise create an additional categorical value which simply represents a missing value.\n\n[Share](https://datascience.stackexchange.com/a/74403)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/74403/edit)\n\nFollow\n\nanswered May 18, 2020 at 14:49\n\n[![Tinu's user avatar](https://i.sstatic.net/6wFcg.jpg?s=64)](https://datascience.stackexchange.com/users/84576/tinu)\n\n[Tinu](https://datascience.stackexchange.com/users/84576/tinu) Tinu\n\n50811 gold badge33 silver badges88 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Don't you think 20% is way to less to drop the feature?? I have been imputing features with 50% nan values.$\\\\endgroup$\n\n\u2013\u00a0[spectre](https://datascience.stackexchange.com/users/119921/spectre)\n\nCommentedNov 10, 2021 at 4:27\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml)\u00a0\\|\n\n1\n\n$\\\\begingroup$\n\nThe first question we must ask is **\u201cwhy are these values missing?\u201d**\n\n1. **Skip** the feature if it's > 25%\n\n2. Try to **know the reason** from the data source/provider. They might give a clue and you may use that e.g. One city has Power failure during data collection.\n\n3. Simply **create a new category** for the missing and check the result. This will only work when there is an underlying reason for missing\n\n4. Try Calculating/Guessing on domain knowledge the **correlation with other Feature** and then fill with respective values. I am making this point to avoid mean/median on full columns e.g. In below data, Mean on full column ~750 but we should fill with ~100\n\n\n[![enter image description here](https://i.sstatic.net/kluzf.jpg)](https://i.sstatic.net/kluzf.jpg)\n\n1. **K Nearest Neighbour** \\- This can do both the steps of #3 in one go. Fortunately, SciKitLearn has an Imputer. _e.g. [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) (Keep one categorical at a time)_\n\n2. **Blind approach** \\- Simply replace with Mean/Median. For categorical - most\\_frequent (Mode)\n`SimpleImputer(strategy=\"most_frequent\")`\n\n3. **Try a few** and monitor the result to decide the best approach\n\n4. A thoughtful read -\n\n[Max Kuhn and Kjell Johnson](http://www.feat.engineering/imputation-methods.html)\n\n\n> _One point that I wanted to make is to look into the data as an event/cause-effect and try to figure out things before directly looking for hammer/guns esp. if its a real project. It's ok if it is learning stuff._\n\n\n[Share](https://datascience.stackexchange.com/a/74642)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/74642/edit)\n\nFollow\n\nanswered May 22, 2020 at 12:17\n\n[![10xAI's user avatar](https://i.sstatic.net/KVtTy.jpg?s=64)](https://datascience.stackexchange.com/users/58341/10xai)\n\n[10xAI](https://datascience.stackexchange.com/users/58341/10xai) 10xAI\n\n5,63422 gold badges88 silver badges2424 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$25% is way too less to drop the feature. I have imputed features with 50% nan values.$\\\\endgroup$\n\n\u2013\u00a0[spectre](https://datascience.stackexchange.com/users/119921/spectre)\n\nCommentedNov 10, 2021 at 4:30\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml)\u00a0\\|\n\n0\n\n$\\\\begingroup$\n\nHow about training a ML classification model where all the features are used as an input and label is your categorical values. In that way we can predict the missing value.\n\n[Share](https://datascience.stackexchange.com/a/74483)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/74483/edit)\n\nFollow\n\nanswered May 19, 2020 at 17:56\n\n[![vipin bansal's user avatar](https://www.gravatar.com/avatar/c3d2a4a692cc775028e4296fdaeafe3f?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/73441/vipin-bansal)\n\n[vipin bansal](https://datascience.stackexchange.com/users/73441/vipin-bansal) vipin bansal\n\n1,2721010 silver badges1818 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml)\u00a0\\|\n\n0\n\n$\\\\begingroup$\n\nI would add to the other answers that columns might be useful even if they contain more than half of missing values _if those missing values carry meaning_. For example, in credit risk modeling, there might be a column with ...",
      "url": "https://datascience.stackexchange.com/questions/74391/handling-categorical-missing-values-ml"
    },
    {
      "title": "Handle missing values Categorical Features | Analytics Vidhya",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb7c5b073dda2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fhandle-missing-values-in-categorical-features-b7c5b073dda2&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fhandle-missing-values-in-categorical-features-b7c5b073dda2&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Handle missing values in Categorical Features\n\n## An useful guide to a proper deal with missing categorical data, with use cases\n\n[![Daniele Salerno](https://miro.medium.com/v2/resize:fill:88:88/1*09UQsgvmrQTx3lrw0l8-Aw.jpeg)](https://medium.com/@daniele-salerno?source=post_page---byline--b7c5b073dda2---------------------------------------)\n\n[![Analytics Vidhya](https://miro.medium.com/v2/resize:fill:48:48/1*Qw8AOQSnnlz7SLiwAda2jw.png)](https://medium.com/analytics-vidhya?source=post_page---byline--b7c5b073dda2---------------------------------------)\n\n[Daniele Salerno](https://medium.com/@daniele-salerno?source=post_page---byline--b7c5b073dda2---------------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd7acfb6402af&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fhandle-missing-values-in-categorical-features-b7c5b073dda2&user=Daniele+Salerno&userId=d7acfb6402af&source=post_page-d7acfb6402af--byline--b7c5b073dda2---------------------post_header------------------)\n\nPublished in\n\n[Analytics Vidhya](https://medium.com/analytics-vidhya?source=post_page---byline--b7c5b073dda2---------------------------------------)\n\n\u00b7\n\n7 min read\n\n\u00b7\n\nNov 17, 2020\n\n--\n\nListen\n\nShare\n\nIn this post, it will be shown how to deal with categorical features with missing values with several examples compared to each other. It will be used the [Classified Ads for Cars](https://www.kaggle.com/mirosval/personal-cars-classifieds) dataset to predict the price of ADs through a simple model of Linear Regression.\n\nTo show the various strategies and relevant pros / cons, we will focus on a particular categorical feature of this dataset, the **maker**, the name of the brand of cars (Toyota, Kia, Ford, Bmw, \u2026).\n\n## Post Steps:\n\n- **Show Raw Data**: let\u2019s see how our dataset looks like.\n- **Deal with missing values in Categorical Features**: we will deal missing values by comparing different techniques.\n- 1 \u2014 **Delete** the entire column **maker**.\n- 2 \u2014 **Replace** missing values with the _most frequent values._\n- 3 \u2014 **Delete** rows with null values.\n- 4 \u2014 **Predict** values using a Classifier Algorithm (supervised or unsupervised).\n- **Conclusions!**\n\n## Show Raw Data\n\nLet\u2019s start importing some libraries\n\n```\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n```\n\nFirst of all let\u2019s see how our dataset looks like\n\n```\nfilename = \"cars.csv\"\n\ndtypes = {\n    \"maker\": str, # brand name\n    \"model\": str,\n    \"mileage\": float, # km\n    \"manufacture_year\": float,\n    \"engine_displacement\": float,\n    \"engine_power\": float,\n    \"body_type\": str, # almost never present\n    \"color_slug\": str, # also almost never present\n    \"stk_year\": str,\n    \"transmission\": str, # automatic or manual\n    \"door_count\": str,\n    \"seat_count\": str,\n    \"fuel_type\": str, # gasoline or diesel\n    \"date_created\": str, # when the ad was scraped\n    \"date_last_seen\": str, # when the ad was last seen\n    \"price_eur\": float} # list price converted to EUR\n\ndf_cleaned = pd.read_csv(filename, dtype=dtypes)\nprint(f\"Raw data has {df_cleaned.shape[0]} rows, and   {df_cleaned.shape[1]} columns\")\n```\n\n`Raw data has 3552912 rows, and 16 columns`\n\nAfter cleaning all columns from missing data and not useful features (the whole procedure is shown on my [github](https://github.com/daniele-salerno/Handle-missing-values-in-Categorical-Features)), with the exception of the **maker**,\n\nwe will find ourselves in this situation:\n\n```\n# Missing values\nprint(df_cleaned.isna().sum())maker                  212897\nmileage                     0\nmanufacture_year            0\nengine_displacement         0\nengine_power                0\nprice_eur                   0\nfuel_type_diesel            0\nfuel_type_gasoline          0\nad_duration                 0\nseat_str_large              0\nseat_str_medium             0\nseat_str_small              0\ntransmission_auto           0\ntransmission_man            0\ndtype: int64\n```\n\n## Correlation Matrix\n\n```\ncorr = df_cleaned.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, annot=True, )\n```\n\n## Deal with missing values in Categorical Features\n\nNow we just have to handle the **maker** feature, and we will do it in four different ways. Then we will create a simple model of Linear Regression for each ways to predict the price.\n\n- **1st Model**: Delete the entire column **maker**.\n- **2nd Model**: Replace missing values with the _most frequent values_.\n- **3rd Model**: Delete rows with null values.\n- **4th Model**: Predict the missing values with the RandomForestClassifier.\n\n```\nmse_list = []\nr2_score_list = []\n\ndef remove_outliers(dataframe):\n    '''\n    return a dataframe without rows that are outliers in any column\n    '''\n    return dataframe\\\n    .loc[:, lambda df: df.std() > 0.04]\\\n    .loc[lambda df: (np.abs(stats.zscore(df)) < 3).all(axis=1)]\n\ndef plot_regression(Y_test, Y_pred):\n    '''\n    method that plot a linear regression line on a scatter plot\n    '''\n    x = Y_test\n    y = Y_pred\n\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    plt.plot(x, y, 'o')\n\n    m, b = np.polyfit(x, y, 1)\n\n    plt.plot(x, m*x + b)\n\ndef train_and_score_regression(df):\n\n    df_new = remove_outliers(df)\n\n    # split the df\n    X = df_new.drop(\"price_eur\", axis=1).values\n    Y = np.log1p(df_new[\"price_eur\"].values)\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,\n    test_size=0.1, random_state=0)\n\n    # train and test of the model\n    ll = LinearRegression()\n    ll.fit(X_train, Y_train)\n    Y_pred = ll.predict(X_test)\n\n    mse_list.append(mean_squared_error(Y_test, Y_pred))\n    r2_score_list.append(r2_score(Y_test, Y_pred))\n\n    # print the metrics\n    print(\"MSE: \"+str(mean_squared_error(Y_test, Y_pred)))\n    print(\"R2 score: \"+str(r2_score(Y_test, Y_pred)))\n\n    # plot the regression\n    plot_regression(Y_test, Y_pred)\n```\n\n## Delete the entire column _Maker_\n\nOur first basic approach will be to create a model without the column **maker**. This \u201cno-deal\u201d practice is necessary when the large amount of missing data threatens to invalidate the entire feature.\n\nWith this scenario we will probably have the worst scores on metrics, but we can use them to compare the other methods\n\n```\ndf_no_maker = df_cleaned.copy()\n\n# delete the entire column maker\ndf_no_maker = df_no_maker.drop(\"maker\", axis=\"columns\")\n\ntrain_and_score_regression(df_no_maker)# MSE: 0.1384341569294906\n# R2 score: 0.8401186412746953\n```\n\nThis is our first attempt. Can we improve it?\n\n## Replace missing values with the most frequent data\n\nCertainly more effective method is to assign the missing values with the mo...",
      "url": "https://medium.com/analytics-vidhya/handle-missing-values-in-categorical-features-b7c5b073dda2"
    },
    {
      "title": "",
      "text": "<div><div><main><div><p>The missing values processing mode depends on the feature type and the selected package.</p>\n<h2>Numerical features</h2>\n<p>CatBoost interprets the value of a numerical feature as a missing value if it is equal to one of the following values, which are package-dependant:</p>\n<div>\n<div>\n<p>Python package</p>\n<p>R package</p>\n<p>Command-line version</p>\n</div>\n<div>\n<ul>\n<li>\n<p><code>None</code></p>\n</li>\n<li>\n<p><a href=\"https://en.wikipedia.org/wiki/NaN\">Floating point NaN value</a></p>\n</li>\n<li>\n<p>One of the following strings when loading the values from files or as Python strings:</p>\n<p>, <q>#N/A</q>, <q>#N/A N/A</q>, <q>#NA</q>, <q>-1.#IND</q>, <q>-1.#QNAN</q>, <q>-NaN</q>, <q>-nan</q>, <q>1.#IND</q>, <q>1.#QNAN</q>, <q>N/A</q>, <q>NA</q>, <q>NULL</q>, <q>NaN</q>, <q>n/a</q>, <q>nan</q>, <q>null</q>, <q>NAN</q>, <q>Na</q>, <q>na</q>, <q>Null</q>, <q>none</q>, <q>None</q>, <q>-</q></p>\n<p>This is an extended version of the default missing values list in\u00a0<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\">pandas</a>.</p>\n</li>\n</ul>\n</div>\n<div>\n<ul>\n<li>\n<p><a href=\"https://en.wikipedia.org/wiki/NaN\">Floating point NaN value</a></p>\n</li>\n<li>\n<p>One of the following strings when loading the values from files:</p>\n<p>, <q>#N/A</q>, <q>#N/A N/A</q>, <q>#NA</q>, <q>-1.#IND</q>, <q>-1.#QNAN</q>, <q>-NaN</q>, <q>-nan</q>, <q>1.#IND</q>, <q>1.#QNAN</q>, <q>N/A</q>, <q>NA</q>, <q>NULL</q>, <q>NaN</q>, <q>n/a</q>, <q>nan</q>, <q>null</q>, <q>NAN</q>, <q>Na</q>, <q>na</q>, <q>Null</q>, <q>none</q>, <q>None</q>, <q>-</q></p>\n<p>This is an extended version of the default missing values list in\u00a0<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\">pandas</a>.</p>\n</li>\n</ul>\n</div>\n<div>\n<p>One of the following strings when loading the values from files when reading from an input file:</p>\n<p>, <q>#N/A</q>, <q>#N/A N/A</q>, <q>#NA</q>, <q>-1.#IND</q>, <q>-1.#QNAN</q>, <q>-NaN</q>, <q>-nan</q>, <q>1.#IND</q>, <q>1.#QNAN</q>, <q>N/A</q>, <q>NA</q>, <q>NULL</q>, <q>NaN</q>, <q>n/a</q>, <q>nan</q>, <q>null</q>, <q>NAN</q>, <q>Na</q>, <q>na</q>, <q>Null</q>, <q>none</q>, <q>None</q>, <q>-</q></p>\n<p>This is an extended version of the default missing values list in\u00a0<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\">pandas</a>.</p>\n</div>\n</div>\n<p>The following modes for processing missing values are supported:</p>\n<ul>\n<li>\"Forbidden\" \u2014 Missing values are not supported, their presence is interpreted as an error.</li>\n<li>\"Min\" \u2014 Missing values are processed as the minimum value (less than all other values) for the feature. It is guaranteed that a split that separates missing values from all other values is considered when selecting trees.</li>\n<li>\"Max\" \u2014 Missing values are processed as the maximum value (greater than all other values) for the feature. It is guaranteed that a split that separates missing values from all other values is considered when selecting trees.</li>\n</ul>\n<p>The default processing mode is\u00a0Min. The methods for changing the default mode are package-dependant:</p>\n<div>\n<div>\n<p>Python package</p>\n<p>R package</p>\n<p>Command-line version</p>\n</div>\n<div>\n<ul>\n<li>Globally for all features in the <code>nan_mode</code> <a href=\"https://catboost.ai/docs/en/concepts/en/references/training-parameters/\">training parameter</a>.</li>\n<li>Individually for each feature in the\u00a0<a href=\"https://catboost.ai/docs/en/concepts/en/concepts/input-data_custom-borders\">Custom quantization borders and missing value modes</a> input file. Such values override the global default setting.</li>\n</ul>\n</div>\n<div>\n<ul>\n<li>Globally for all features in the <code>nan_mode</code> <a href=\"https://catboost.ai/docs/en/concepts/en/references/training-parameters/\">training parameter</a>.</li>\n<li>Individually for each feature in the\u00a0<a href=\"https://catboost.ai/docs/en/concepts/en/concepts/input-data_custom-borders\">Custom quantization borders and missing value modes</a> input file. Such values override the global default setting.</li>\n</ul>\n</div>\n<div>\n<ul>\n<li>Globally for all features in the <code>--nan-mode</code> <a href=\"https://catboost.ai/docs/en/concepts/en/references/training-parameters/\">training parameter</a>.</li>\n<li>Individually for each feature in the\u00a0<a href=\"https://catboost.ai/docs/en/concepts/en/concepts/input-data_custom-borders\">Custom quantization borders and missing value modes</a> input file. Such values override the global default setting.</li>\n</ul>\n</div>\n</div>\n<h2>Categorical features</h2>\n<p>CatBoost does not process categorical features in any specific way.</p>\n</div><div><h3>Was the article helpful?</h3></div></main></div></div>",
      "url": "https://catboost.ai/docs/en/concepts/algorithm-missing-values-processing"
    }
  ]
}