{
  "query": "What are winning approaches for text classification competitions on Kaggle that combine text features with tabular metadata?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle text\u2011classification solutions that also have tabular metadata treat the problem as a **multimodal learning task** \u2013 the raw text is first turned into dense embeddings (usually with a pretrained transformer such as BERT, RoBERTa or a domain\u2011specific LLM) and those vectors are concatenated with the engineered numeric and categorical features before being fed to a powerful tabular learner or an ensemble\u202f([AutoGluon documentation](https://auto.gluon.ai/scoredebugweight/tutorials/tabular_prediction/tabular-multimodal-text-others.html))\u200b.  AutoGluon\u2019s\u202f`TabularPredictor` automatically handles raw text columns, trains a suite of models (LightGBM, CatBoost, Random\u202fForest, and a multimodal neural net that ingests the transformer embeddings) and blends them, which has become a go\u2011to baseline for many Kaggle competitions involving mixed data\u200b([AutoGluon documentation](https://auto.gluon.ai/scoredebugweight/tutorials/tabular_prediction/tabular-multimodal-text-others.html))\u200b.\n\nA common winning pattern is to **extract high\u2011quality text features separately** (e.g., TF\u2011IDF, n\u2011grams, or sentence\u2011level embeddings) and then train a gradient\u2011boosting model such as XGBoost, LightGBM or CatBoost on the combined feature matrix.  XGBoost can ingest sparse TF\u2011IDF vectors together with one\u2011hot or target\u2011encoded categorical metadata, and its regularisation and tree\u2011based structure often out\u2011performs pure neural nets on tabular\u2011heavy problems\u200b([Medium \u2013 XGBoost for text classification](https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa))\u200b.  More recent work shows that **enriching the tabular rows with contextual LLM embeddings** before feeding them to a classical tabular model yields large gains, as demonstrated in an ablation study that stacked transformer\u2011derived vectors with LightGBM/CatBoost and achieved state\u2011of\u2011the\u2011art scores on public benchmarks\u200b([arXiv\u202f2024\u202f\u201cEnriching Tabular Data with Contextual LLM Embeddings\u201d](https://arxiv.org/abs/2411.01645))\u200b.\n\nFinally, Kaggle\u2011level best practices reinforce these modeling choices:\u202fuse stratified cross\u2011validation, blend multiple modalities (a transformer\u2011only model, a pure tabular booster, and a stacked ensemble), apply target\u2011encoding or frequency encoding for high\u2011cardinality metadata, and aggressively optimise memory (e.g., parquet/feather formats) to handle large corpora\u200b([Neptune\u202fblog \u2013 Text\u2011classification tips from 5 Kaggle competitions](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions))\u200b.  When the text is short (product reviews, comments) a simple \u201ctext\u202f+\u202fmetadata\u201d pipeline built with HuggingFace\u202fTransformers plus a tabular head can be implemented in a few lines and still rank in the top\u202f5\u202f% of the leaderboard\u200b([Medium \u2013 Incorporate tabular data with HuggingFace Transformers](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4))\u200b.\n",
      "url": ""
    },
    {
      "title": "XG Boost for text classification - Medium",
      "text": "XG Boost for text classification. XGBoost (eXtreme Gradient Boosting) is\u2026 | by Gauri Bhatnagar | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# XG Boost for text classification\n[\n![Gauri Bhatnagar](https://miro.medium.com/v2/resize:fill:64:64/1*3Toq7r_mcTrPicXaWPbNSg.jpeg)\n](https://medium.com/@gaurishah143?source=post_page---byline--9c8b1f8f24aa---------------------------------------)\n[Gauri Bhatnagar](https://medium.com/@gaurishah143?source=post_page---byline--9c8b1f8f24aa---------------------------------------)\n3 min read\n\u00b7Feb 13, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/9c8b1f8f24aa&amp;operation=register&amp;redirect=https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa&amp;user=Gauri+Bhatnagar&amp;userId=1402fbcb5501&amp;source=---header_actions--9c8b1f8f24aa---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/9c8b1f8f24aa&amp;operation=register&amp;redirect=https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa&amp;source=---header_actions--9c8b1f8f24aa---------------------bookmark_footer------------------)\nListen\nShare\nXGBoost (eXtreme Gradient Boosting) is a popular machine learning algorithm used for various tasks including text classification.**In text classification, XGBoost can be used to predict the class label of a given text document based on its contents.**\n*The basic idea behind XGBoost is to use decision trees as base models and then to build an ensemble of these trees to improve the accuracy of predictions. In the context of text classification, each tree takes a set of features extracted from the text document as input and outputs a predicted class label. XGBoost then trains multiple trees on the training data and aggregates their predictions to produce the final prediction for each text document.*\nOne of the advantages of XGBoost is that it can handle large amounts of data and can be run on parallel computing systems, making it a fast and efficient algorithm for text classification. Additionally, XGBoost also supports various regularization techniques to prevent overfitting, which is a common problem in text classification due to the large number of features often present in text data.\nTo implement XGBoost for text classification, the first step is to preprocess the text data and extract meaningful features from it, such as term frequency-inverse document frequency (TF-IDF) values. These features are then used as input to the XGBoost model, which is trained on the preprocessed data. Finally, the trained model can be used to predict the class label for new text documents.\nHere is a simple example of how to use XGBoost for text classification in Python:\n```\nimport xgboost as xgb\nfrom sklearn.feature\\_extraction.text import TfidfVectorizer\nfrom sklearn.model\\_selection import train\\_test\\_split\nfrom sklearn.metrics import accuracy\\_score\n# Text data and labels\ntexts = [...]\nlabels = [...]\n# Split the data into train and test sets\ntexts\\_train, texts\\_test, labels\\_train, labels\\_test = train\\_test\\_split(texts, labels, test\\_size=0.2, random\\_state=42)\n# Convert text data into numerical features using TF-IDF\nvectorizer = TfidfVectorizer()\nX\\_train = vectorizer.fit\\_transform(texts\\_train)\nX\\_test = vectorizer.transform(texts\\_test)\n# Train the XGBoost model\nmodel = xgb.XGBClassifier(n\\_jobs=-1)\nmodel.fit(X\\_train, labels\\_train)\n# Make predictions on the test set\npredictions = model.predict(X\\_test)\n# Evaluate the model performance\naccuracy = accuracy\\_score(labels\\_test, predictions)\nprint(&quot;&quot;Accuracy: {:.2f}%&quot;&quot;.format(accuracy \\* 100))\n```\nIn this example,`texts`is a list of text documents, and`labels`is a list of class labels. The data is split into training and testing sets using the`train\\_test\\_split`function from scikit-learn. The text data is then converted into numerical features using the`TfidfVectorizer`from scikit-learn. The XGBoost model is trained on the training data using the`fit`method, and predictions are made on the test set using the`predict`method. Finally, the accuracy of the model is evaluated using the`accuracy\\_score`function from scikit-learn.\nNote that this is just a simple example, and in practice, you might need to perform additional steps such as hyperparameter tuning or cross-validation to achieve optimal performance.\n**There are several reasons why XGBoost is a popular choice for text classification:**\n1. Speed and Scalability: XGBoost is designed to handle large datasets and can be run on parallel computing systems, making it a fast and efficient algorithm for text classification. This is important in text classification because text data can often be large and complex.\n2. Good Performance: XGBoost has a good track record of achieving high accuracy in text classification tasks, especially when compared to other traditional machine learning algorithms such as linear models or Naive Bayes.\n3. Feature Importance: XGBoost provides feature importances, which can give insights into which words or features are most important in determining the class label of a text document. This can be useful in understanding the underlying patterns in the data and can also be used for feature selection.\n4. Regularization: XGBoost supports various regularization techniques, such as L1 and L2 regularization, which can prevent overfitting. Overfitting is a common problem in text classification due to the large number of features often present in text data.\n5. Model Interpretability: XGBoost uses decision trees as base models, which are easy to interpret and understand. This makes it easier to understand the relationship between the features and the predicted class labels.\nOverall, XGBoost is a powerful and flexible algorithm that can handle the complex and large-scale nature of text data and has a good track record of achieving high accuracy in text classification tasks.\n[\nText Classification\n](https://medium.com/tag/text-classification?source=post_page-----9c8b1f8f24aa---------------------------------------)\n[\nMachine Learning\n](https://medium.com/tag/machine-learning?source=post_page-----9c8b1f8f24aa---------------------------------------)\n[\nNaturallanguageprocessing\n](https://medium.com/tag/naturallanguageprocessing?source=post_page-----9c8b1f8f24aa---------------------------------------)\n[\n![Gauri Bhatnagar](https://miro.medium.com/v2/resize:fill:96:96/1*3Toq7r_mcTrPicXaWPbNSg.jpeg)\n](https://medium.com/@gaurishah143?source=post_page---post_author_info--9c8b1f8f24aa---------------------------------------)\n[\n![Gauri Bhatnagar](https://miro.medium.com/v2/resize:fill:128:128/1*3Toq7r_mcTrPicXaWPbNSg.jpeg)\n](https://medium.com/@gaurishah143?source=post_page---post...",
      "url": "https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa"
    },
    {
      "title": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2411.01645"
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.1.1 documentation",
      "text": "# Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models)\n\nHere we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, **raw text data** is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c [What\u2019s happening inside?](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-architecture)\u201d of\n[Text Prediction - Multimodal Table with Text](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-multimodal) (used by AutoGluon\u2019s\n`TextPredictor`).\n\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nfrom autogluon.tabular import TabularPredictor\nimport mxnet as mx\n\nnp.random.seed(123)\nrandom.seed(123)\nmx.random.seed(123)\n```\n\n## Product Sentiment Analysis Dataset [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#product-sentiment-analysis-dataset)\n\nWe consider the product sentiment analysis dataset from a [MachineHack\\\nhackathon](https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard).\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).\n\n```\n!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n\n```\n\n```\n--2021-03-10 04:16:17--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 673.33K  2.05MB/s    in 0.3s\n\n2021-03-10 04:16:18 (2.05 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n\n--2021-03-10 04:16:18--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\n\nproduct_sentiment_m 100%[===================>]  73.75K   490KB/s    in 0.2s\n\n2021-03-10 04:16:19 (490 KB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n\n--2021-03-10 04:16:19--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 304.88K   943KB/s    in 0.3s\n\n2021-03-10 04:16:20 (943 KB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n\n```\n\n```\nsubsample_size = 2000  # for quick demo, try setting to larger values\nfeature_columns = ['Product_Description', 'Product_Type']\nlabel = 'Sentiment'\n\ntrain_df = pd.read_csv('product_sentiment_machine_hack/train.csv', index_col=0).sample(2000, random_state=123)\ndev_df = pd.read_csv('product_sentiment_machine_hack/dev.csv', index_col=0)\ntest_df = pd.read_csv('product_sentiment_machine_hack/test.csv', index_col=0)\n\ntrain_df = train_df[feature_columns + [label]]\ndev_df = dev_df[feature_columns + [label]]\ntest_df = test_df[feature_columns]\nprint('Number of training samples:', len(train_df))\nprint('Number of dev samples:', len(dev_df))\nprint('Number of test samples:', len(test_df))\n```\n\n```\nNumber of training samples: 2000\nNumber of dev samples: 637\nNumber of test samples: 2728\n```\n\nThere are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.\n\n```\ntrain_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 4532 | they took away the lego pit but replaced it wi... | 0 | 1 |\n| 1831 | #Apple to Open Pop-Up Shop at #SXSW \\[REPORT\\]: ... | 9 | 2 |\n| 3536 | RT @mention False Alarm: Google Circles Not Co... | 5 | 1 |\n| 5157 | Will Google reveal a new social network called... | 9 | 2 |\n| 4643 | Niceness RT @mention Less than 2 hours until w... | 6 | 3 |\n\n```\ndev_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 3170 | Do it. RT @mention Come party w/ Google tonigh... | 3 | 3 |\n| 6301 | Line for iPads at #SXSW. Doesn't look too bad!... | 6 | 3 |\n| 5643 | First up: iPad Design Headaches (2 Tablets, Ca... | 6 | 2 |\n| 1953 | #SXSW: Mint Talks Mobile App Development Chall... | 9 | 2 |\n| 2658 | \u0089\u00db\u00cf@mention Apple store downtown Austin open t... | 9 | 2 |\n\n```\ntest_df.head()\n```\n\n| Product\\_Description | Product\\_Type |\n| --- | --- |\n| Text\\_ID |\n| --- |\n| 5786 | RT @mention Going to #SXSW? The new iPhone gui... | 7 |\n| 5363 | RT @mention 95% of iPhone and Droid apps have ... | 9 |\n| 6716 | RT @mention Thank you to @mention for letting ... | 9 |\n| 4339 | #Thanks @mention we're lovin' the @mention app... | 7 |\n| 66 | At #sxsw? @mention / @mention wanna buy you a ... | 9 |\n\n## AutoGluon Tabular with Multimodal Support [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#autogluon-tabular-with-multimodal-support)\n\nTo utilize the `TextPredictor` model inside of `TabularPredictor`,\nwe must specify the `hyperparameters = 'multimodal'` in AutoGluon\nTabular. Internally, this will train multiple tabular models as well as\nthe TextPredictor model, and then combine them via either a weighted\nensemble or stack ensemble, as explained in [AutoGluon Tabular\\\nPaper](https://arxiv.org/pdf/2003.06505.pdf). If you do not specify\n`hyperparameters = 'multimodal'`, then AutoGluon Tabular will simply\nfeaturize text fields using N-grams and train only tabular models (which\nmay work better if your text is mostly uncommon strings/vocabulary).\n\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\npredictor.fit(train_df, hyperparameters='multimodal')\n```\n\n```\nBeginning AutoGluon training ...\nAutoGluon will save models to \"ag_tabular_product_sentiment_multimodal/\"\nAutoGluon Version:  0.1.1b20210310\nTrain Data Rows:    2000\nTrain Data Columns: 2\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n    4 unique label values:  [1, 2, 3, 0]\n    If 'multiclass' is not the correct ...",
      "url": "https://auto.gluon.ai/scoredebugweight/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "How to Incorporate Tabular Data with HuggingFace Transformers",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb70ac45fcfb4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fgeorgian-impact-blog%2Fhow-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fgeorgian-impact-blog%2Fhow-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# How to Incorporate Tabular Data with HuggingFace Transformers\n\n[![Georgian](https://miro.medium.com/v2/resize:fill:88:88/1*FJwbo1EyZGjafbGs212LKw.jpeg)](https://medium.com/@georgian-io?source=post_page-----b70ac45fcfb4--------------------------------)[![Georgian Impact Blog](https://miro.medium.com/v2/resize:fill:48:48/1*mpni9rTVETi-KKf8OOXrTQ.jpeg)](https://medium.com/georgian-impact-blog?source=post_page-----b70ac45fcfb4--------------------------------)\n\n[Georgian](https://medium.com/@georgian-io?source=post_page-----b70ac45fcfb4--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd3d81cdc0d9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeorgian-impact-blog%2Fhow-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4&user=Georgian&userId=d3d81cdc0d9&source=post_page-d3d81cdc0d9----b70ac45fcfb4---------------------post_header-----------)\n\nPublished in\n\n[Georgian Impact Blog](https://medium.com/georgian-impact-blog?source=post_page-----b70ac45fcfb4--------------------------------)\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nOct 23, 2020\n\n--\n\n2\n\nListen\n\nShare\n\n[\\[Colab\\]](https://colab.research.google.com/github/georgianpartners/Multimodal-Toolkit/blob/master/notebooks/text_w_tabular_classification.ipynb) [\\[Github\\]](https://github.com/georgian-io/Multimodal-Toolkit)\n\nBy [Ken Gu](https://www.linkedin.com/in/ken-gu/)\n\nTransformer-based models are a game-changer when it comes to using unstructured text data. As of September 2020, the top-performing models in the General Language Understanding Evaluation (GLUE) benchmark are all BERT transformer-based models. At [Georgian](http://georgian.io), we find ourselves working with supporting tabular feature information as well as unstructured text data. We found that by using the tabular data in [our models](http://georgian.io/platform/research-at-georgian/), we could further improve performance, so we set out to build a toolkit that makes it easier for others to do the same.\n\n**_The 9 tasks that are part of the GLUE benchmark_**\n\n# Building on Top of Transformers\n\nThe main benefits of using transformers are that they can learn long-range dependencies between text and can be trained in parallel (as opposed to sequence to sequence models), meaning they can be pretrained on large amounts of data.\n\nGiven these advantages, BERT is now a staple model in many real-world applications. Likewise, with libraries such as [HuggingFace Transformers](https://huggingface.co/transformers/), it\u2019s easy to build high-performance transformer models on common NLP problems.\n\nTransformer models using unstructured text data are well understood. However, in the real-world, text data is often supported by rich structured data or other unstructured data like audio or visual information. Each one of these might provide signals that one alone would not. We call these different ways of experiencing data \u2014 audio, visual or text \u2014 modalities.\n\nThink about E-commerce reviews as an example. In addition to the review text itself, we also have information about the seller, buyer and product available as numerical and categorical features.\n\nWe set out to explore how we could use text and tabular data together to provide stronger signals in our projects. We started by exploring the field known as multimodal learning, which focuses on how to process different modalities in machine learning.\n\n# Multimodal Literature Review\n\nThe current models for multimodal learning mainly focus on learning from the sensory modalities such as audio, visual, and text.\n\nWithin multimodal learning, there are several branches of research. The MultiComp Lab at Carnegie Mellon University provides an excellent [taxonomy](http://multicomp.cs.cmu.edu/research/taxonomy/). Our problem falls under what is known as **Multimodal Fusion \u2014** joining information from two or more modalities to make a prediction.\n\nAs text data is our primary modality, our review focued on the literature that treats text as the main modality and introduces models that leverage the transformer architecture.\n\n**Trivial Solution to Structured Data**\n\nBefore we dive into the literature, it\u2019s worth mentioning that there is a simple solution that can be used where the structured data is treated as regular text and is appended to the standard text inputs. Taking the E-commerce reviews example, the input can be structured as follows: Review. Buyer Info. Seller Info. Numbers/Labels. Etc. One caveat with this approach, however, is that it is limited by the maximum token length that a transformer can handle.\n\n# Transformer on Images and Text\n\nIn the last couple of years, transformer extensions for image and text have really advanced. [Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf) by Kiela et al. (2019) uses pretrained ResNet and pretrained BERT features on unimodal images and text respectively and feeds this into a Bidirectional transformer. The key innovation is adapting the image features as additional tokens to the transformer model.\n\nAn illustration of the multimodal transformer. This model takes the output of ResNet on subregions of the image as input image tokens.\n\nAdditionally, there are models \u2014 [ViLBERT](https://arxiv.org/abs/1908.02265) (Lu et al. 2019) and [VLBert](https://arxiv.org/pdf/1908.08530.pdf) (Su et al. 2020) \u2014 which define pretraining tasks for images and text. Both models pretrain on the [Conceptual Captions dataset](https://ai.google.com/research/ConceptualCaptions), which contains roughly 3.3 million image-caption pairs (web images with captions from alt text). In both cases, for any given image, a pretrained object detection model like Faster R-CNN obtains vector representations for regions of the image, which count as input token embeddings to the transformer model.\n\nThe VLBert model diagram. It takes image regions outputted by Faster R-CNN as input image tokens.\n\nAs an example, ViLBert pretrains on the following training objectives:\n\n1. **Masked multimodal modeling:** Mask input image and word tokens. For the image, the model tries to predict a vector capturing image features for the corresponding image region, while for text, it predicts the masked text based on the textual and visual clues.\n2. **Multimodal alignment:** Whether the image and text pair are actually from the same image and caption pair.\n\nThe two pre training tasks for ViLBert\n\nAn example of masked multimodal learning. Given the image and text, if we mask out _dog_, the model should be able to use the unmasked visual information to correctly predict the masked word to be _dog_.\n\nAll these models use the bidirectional transformer model that is the backbone of BERT. The differences are the pretraining tasks the models are trained on and slight additions to the transformer. In the case of ViLBERT, the authors also introduce a co-attenti...",
      "url": "https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4"
    },
    {
      "title": "Text Classification Techniques - Explained - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/eraikako/text-classification-techniques-explained"
    },
    {
      "title": "Advanced Techniques of Text Classification - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/meharshanali/advanced-techniques-of-text-classification"
    },
    {
      "title": "Research on Text Classification Method Based on LSTM Neural ...",
      "text": "Research on Text Classification Method Based on LSTM Neural Network Model | IEEE Conference Publication | IEEE Xplore\n[**]()\n[](https://ieeexplore.ieee.org/rest/api/hpdata)\n### IEEE Account\n* [Change Username/Password](<https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n* [Update Address](<https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n### Purchase Details\n* [Payment Options](<https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n* [Order History](<https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n* [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n### Profile Information\n* [Communications Preferences](<https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n* [Profession and Education](<https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n* [Technical Interests](<https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore>)\n### Need Help?\n* **US &amp; Canada:**+1 800 678 4333\n* **Worldwide:**+1 732 981 0060\n* [Contact &amp; Support](https://ieeexplore.ieee.org/xpl/contact)\n* [About IEEE*Xplore*](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n* [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n* [Help](https://ieeexplore.ieee.org/Xplorehelp)\n* [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n* [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n* [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n* [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n* [Privacy &amp; Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n&copy; Copyright 2026 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n**",
      "url": "https://ieeexplore.ieee.org/document/9421225"
    }
  ]
}