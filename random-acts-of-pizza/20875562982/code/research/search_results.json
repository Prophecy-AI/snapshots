{
  "query": "How do Kaggle winners implement proper cross-validation with text feature engineering to avoid data leakage when using TF-IDF and dimensionality reduction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Kaggle winners avoid TF\u2011IDF leakage by fitting every text\u2011processing step inside the cross\u2011validation loop, typically using a scikit\u2011learn\u202fPipeline that includes TF\u2011IDF and any dimensionality\u2011reduction (e.g.,\u202fTruncatedSVD).**\u202f([Understanding Data Leakage in Machine Learning: A Focus on TF\u2011IDF](https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak), [Avoiding Data Leakage in Cross\u2011Validation](https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0), [Do I use the same Tfidf vocabulary in k\u2011fold cross_validation](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation), [scikit\u2011learn Common Pitfalls](https://scikit-learn.org/stable/common_pitfalls.html), [Sample pipeline for text feature extraction and evaluation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_text_feature_extraction.html), [Pipelines for text classification in scikit\u2011learn](https://buhrmann.github.io/sklearn-pipelines.html))\n\n1. **Create a single Pipeline that chains the text steps.**  \n   ```python\n   from sklearn.pipeline import Pipeline\n   from sklearn.feature_extraction.text import TfidfVectorizer\n   from sklearn.decomposition import TruncatedSVD\n   from sklearn.linear_model import LogisticRegression\n\n   pipe = Pipeline([\n       ('tfidf', TfidfVectorizer(...)),          # raw text \u2192 sparse TF\u2011IDF\n       ('svd',   TruncatedSVD(n_components=...)),# dimensionality reduction\n       ('clf',   LogisticRegression(...))       # model\n   ])\n   ```  \n   The pipeline guarantees that each fold fits TF\u2011IDF and SVD **only on the training split** and then transforms the validation split ([scikit\u2011learn recommended practice](https://scikit-learn.org/stable/common_pitfalls.html)).\n\n2. **Choose a CV splitter that respects the problem structure.**  \n   Use `StratifiedKFold` for class\u2011imbalanced data or `GroupKFold` when documents share a group (e.g., same author). Pass the splitter to `cross_val_score` or `GridSearchCV`. This prevents information from leaking across folds ([Avoiding Data Leakage in Cross\u2011Validation](https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0)).\n\n3. **Fit the pipeline inside the CV loop, not beforehand.**  \n   ```python\n   from sklearn.model_selection import cross_val_score\n   scores = cross_val_score(pipe, X_text, y, cv=skf)\n   ```  \n   Because the pipeline is instantiated once and *fit* is called by the CV routine, a fresh TF\u2011IDF vocabulary and IDF weights are computed for each training fold. This matches the StackOverflow consensus that the TF\u2011IDF model must be rebuilt per fold to avoid leakage ([Do I use the same Tfidf vocabulary in k\u2011fold cross_validation](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation)).\n\n4. **If you need hyper\u2011parameter tuning, wrap the pipeline in `GridSearchCV` or `RandomizedSearchCV`.**  \n   The search will internally perform nested CV, again fitting TF\u2011IDF and SVD only on inner\u2011training folds. The scikit\u2011learn example demonstrates this exact pattern for text data ([Sample pipeline for text feature extraction and evaluation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_text_feature_extraction.html)).\n\n5. **Validate the final model on a hold\u2011out test set that was never seen by any pipeline step.**  \n   After selecting the best hyper\u2011parameters, fit the pipeline on the full training data **once**, then call `predict` on the untouched test set. This mirrors the Kaggle\u2011winner workflow where the public leaderboard split is treated as a true hold\u2011out ([Pipelines for text classification in scikit\u2011learn](https://buhrmann.github.io/sklearn-pipelines.html)).\n\n**Summary:** Build a scikit\u2011learn Pipeline that includes `TfidfVectorizer` \u2192 dimensionality reduction \u2192 estimator, and feed that pipeline to a proper CV splitter (Stratified/Group K\u2011Fold). Let the CV routine fit the pipeline each fold so the TF\u2011IDF vocabulary and IDF values are computed **only on training data**, thereby eliminating leakage and yielding reliable validation scores.",
      "url": ""
    },
    {
      "title": "Understanding Data Leakage in Machine Learning: A Focus on TF-IDF",
      "text": "Understanding Data Leakage in Machine Learning: A Focus on TF-IDF | UCSC OSPO\n# Search\n[**](#)\n# Understanding Data Leakage in Machine Learning: A Focus on TF-IDF\n[Kyrillos Ishak](https://ucsc-ospo.github.io/author/kyrillos-ishak/)\nSep 5, 2024**[SummerofReproducibility24](https://ucsc-ospo.github.io/category/summerofreproducibility24/)\nHello again!\nThis is my final blog post, and I will be discussing the second material I created for the 2024 Summer of Reproducibility Fellowship. As you may recall from my first post, I am working on the**Exploring Data Leakage in Applied ML: Reproducing Examples of Irreproducibility**project with[Fraida Fund](https://ucsc-ospo.github.io/author/fraida-fund/)and[Mohamed Saeed](https://ucsc-ospo.github.io/author/mohamed-saeed/)as my mentors.\nThis blog post will explore how data leakage can occur during feature extraction, particularly with the commonly used**TF-IDF**vectorizer, and its impact on model generalization.\n# Introduction\nIn machine learning, data leakage is a critical issue that can severely impact model performance. It occurs when information from outside the training dataset is improperly used to create the model, leading to overly optimistic performance during evaluation. One common source of leakage comes from how features, such as those extracted using**TF-IDF**(Term Frequency-Inverse Document Frequency), are handled. In this post, we&rsquo;ll explore how data leakage can happen during feature extraction with TF-IDF and how it affects model accuracy.\n# What is TF-IDF?\nTF-IDF is a method used to evaluate how important a word is in a document relative to a collection of documents. It consists of two components:\n1. **Term Frequency (TF)**: Measures how frequently a term appears in a document.\n2. **Inverse Document Frequency (IDF)**: Reduces the importance of terms that appear frequently across many documents.\nTogether, they provide a weighted value for each word, reflecting its importance relative to the dataset.\n# How Data Leakage Occurs with TF-IDF\nData leakage with TF-IDF happens when the inverse document frequency (IDF) is calculated using the entire dataset (including the test set) before splitting it into training and test sets. This means the model has access to information from the test set during training, leading to artificially inflated results. This is a subtle form of data leakage, as it often goes unnoticed.\nFor example, when calculating the TF-IDF score, if the word &ldquo;banana&rdquo; appears more frequently in the test set but is considered during training, the model downplays its significance. As a result, the model may fail to predict correctly when &ldquo;banana&rdquo; is important in the test data.\n# Why Does This Matter?\nIf the test data is included when calculating the IDF, the model gains unintended insight into the test set&rsquo;s word distribution. In real-world scenarios, the test data is supposed to be unseen during training. By allowing the model to see this information, you&rsquo;re essentially reducing the uncertainty that the model should have about future data.\n# Impact of Data Leakage on Model Performance\nLet&rsquo;s consider two cases to understand the impact of data leakage in detail:\n1. **When a word is rare in the training set but common in the test set**: The model will underestimate the importance of this word during training, leading to poor performance when the word is critical in test documents.\n2. **When a word is common in the training set but rare in the test set**: The model will overemphasize the word during training, leading to poor predictions when the word doesn\u2019t appear as often in unseen data.### Case Study: Data Leakage in TF-IDF\nTo see this effect in action, consider a small toy dataset where the presence of the word &ldquo;banana&rdquo; determines the label. If the word &ldquo;banana&rdquo; appears in a sentence, the label is 1; otherwise, the label is 0. Using**TF-IDF**to vectorize the text, we train a machine learning model to predict this label.\nIn the**first scenario**, we calculate the**TF-IDF**using the entire dataset before splitting it into training and testing sets. This causes data leakage since the model now knows the distribution of words across both sets. For instance, if &ldquo;banana&rdquo; is more common in the test set than the training set, the**IDF**score for &ldquo;banana&rdquo; will be lower across the entire dataset, leading the model to downplay its importance.\nIn the**second scenario**, we calculate**TF-IDF**only on the training set, ensuring that the test set remains unseen. This preserves the integrity of the test set, giving us a more realistic evaluation of the model&rsquo;s performance.\nIn both scenarios, the model&rsquo;s accuracy is drastically different. When leakage is present, performance is artificially high during training but poor when tested on unseen data. Without leakage, the model generalizes better, as it is evaluated on truly unseen data.\n# Avoiding Data Leakage\nAvoiding data leakage is essential for building reliable machine learning models that generalize well to new data. Here are a few guidelines to help prevent leakage:\n1. **Split the dataset before feature extraction**: Always divide your data into training and test sets before applying any feature engineering techniques.\n2. **Ensure proper cross-validation**: When using cross-validation, ensure that the training and test splits do not overlap in any way that can leak information between them.\n3. **Be cautious with time-series data**: In time-series models, avoid using future data to predict past events, as this can lead to leakage.# Conclusion\nAvoiding data leakage is crucial for building robust machine learning models. In the case of TF-IDF, ensuring that feature extraction is done**only on the training set**and not on the entire dataset is key to preventing leakage. Properly addressing this issue leads to better generalization and more reliable models in real-world applications.\nThis blog post provided a case study on how TF-IDF can introduce data leakage and why it&rsquo;s important to carefully handle your dataset before feature extraction. By splitting your data properly and ensuring that no test data &ldquo;leaks&rdquo; into the training process, you can build models that truly reflect real-world performance.\nThanks for reading!\n[osre24](https://ucsc-ospo.github.io/tag/osre24/)[reproducibility](https://ucsc-ospo.github.io/tag/reproducibility/)\n* [**](https://twitter.com/intent/tweet?url=https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak/&amp;text=Understanding+Data+Leakage+in+Machine+Learning:+A+Focus+on+TF-IDF)\n* [**](https://www.facebook.com/sharer.php?u=https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak/&amp;t=Understanding+Data+Leakage+in+Machine+Learning:+A+Focus+on+TF-IDF)\n* [**](<mailto:?subject=Understanding Data Leakage in Machine Learning: A Focus on TF-IDF&amp;body=https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak/>)\n* [**](https://www.linkedin.com/shareArticle?url=https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak/&amp;title=Understanding+Data+Leakage+in+Machine+Learning:+A+Focus+on+TF-IDF)\n* [**](<whatsapp://send?text=Understanding+Data+Leakage+in+Machine+Learning:+A+Focus+on+TF-IDF https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak/>)\n* [**](https://service.weibo.com/share/share.php?url=https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak/&amp;title=Understanding+Data+Leakage+in+Machine+Learning:+A+Focus+on+TF-IDF)\n[![Kyrillos Ishak](https://ucsc-ospo.github.io/author/kyrillos-ishak/avatar_hu8f4700b1a96a053cd93e514df7e18caa_103379_270x270_fill_q75_lanczos_center.jpg)](https://ucsc-ospo.github.io/author/kyrillos-ishak/)\n##### [Kyrillos Ishak](https://ucsc-ospo.github.io/author/kyrillos-ishak/)\n###### Computer Engineering student\nComputer Engineering...",
      "url": "https://ucsc-ospo.github.io/report/osre24/nyu/data-leakage/20240905-kyrillosishak"
    },
    {
      "title": "Avoiding Data Leakage in Cross-Validation",
      "text": "Avoiding Data Leakage in Cross-Validation | by Silva.f.francis | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Avoiding Data Leakage in Cross-Validation\n[\n![Silva.f.francis](https://miro.medium.com/v2/resize:fill:64:64/1*25duiuqZOXK0QMQYmnH-kQ.jpeg)\n](https://medium.com/@silva.f.francis?source=post_page---byline--ba344d4d55c0---------------------------------------)\n[Silva.f.francis](https://medium.com/@silva.f.francis?source=post_page---byline--ba344d4d55c0---------------------------------------)\n8 min read\n\u00b7Jan 14, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/ba344d4d55c0&amp;operation=register&amp;redirect=https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0&amp;user=Silva.f.francis&amp;userId=1f0f08b96a75&amp;source=---header_actions--ba344d4d55c0---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/ba344d4d55c0&amp;operation=register&amp;redirect=https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0&amp;source=---header_actions--ba344d4d55c0---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nData leakage is one of the most common pitfalls in machine learning model evaluation. It occurs when information from the validation set unintentionally influences the training process, leading to overoptimistic performance estimates. This can significantly impact the reliability of the model and skew results. One of the major causes of data leakage is performing preprocessing steps like scaling or feature selection before splitting the data into training and validation sets.\n## The Problem with Data Leakage\nIn traditional machine learning workflows, it\u2019s essential to separate the training and validation sets to get unbiased performance estimates. However, when preprocessing steps are applied to the entire dataset before splitting, the information from the validation set may leak into the training process.\nHere\u2019s an example of a common mistake:\n```\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model\\_selection import KFold\nimport numpy as np\n```\n```\n# Creating a random dataset\nX = np.random.randn(1000, 20)\ny = np.random.randint(0, 2, 1000)\n# Wrong: scaling before splitting\nscaler = StandardScaler()\nX\\_scaled = scaler.fit\\_transform(X)\nkf = KFold(n\\_splits=5)\nfor train\\_idx, val\\_idx in kf.split(X\\_scaled):\nX\\_train, X\\_val = X\\_scaled[train\\_idx], X\\_scaled[val\\_idx]\n# This leads to data leakage as validation data influenced scaling\n```\nIn the above example, the scaling is done before the data is split into training and validation sets. This means the model is trained on data that has already been influenced by the validation data, thus leading to data leakage.\n## A Proper Cross-Validation Pipeline\nTo prevent data leakage, preprocessing should happen inside each fold during cross-validation. This ensures that the training data never gets influenced by the validation data.\nA correct approach looks like this:\n```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear\\_model import LogisticRegression\nfrom sklearn.model\\_selection import cross\\_val\\_score\n```\n```\n# Correct approach: scaling inside each fold\npipeline = Pipeline([\n('scaler', StandardScaler()),\n('classifier', LogisticRegression())\n])\n# Preprocessing happens inside each fold\nscores = cross\\_val\\_score(pipeline, X, y, cv=5)\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Mean CV score: {scores.mean():.3f} (+/- {scores.std() \\* 2:.3f})\")\n```\nBy using a pipeline, preprocessing steps like scaling occur within each fold, ensuring that no information leaks from the validation set into the training process.\n## Dealing with Imbalanced Datasets\nImbalanced datasets can lead to misleading performance metrics because regular k-fold cross-validation may create training and validation sets with imbalanced class distributions. This can result in biased model performance, especially when the minority class is underrepresented in the validation set.\nTo address this, we use**Stratified K-Fold**, which ensures that each fold maintains the same class distribution as the original dataset.\n```\nfrom sklearn.model\\_selection import StratifiedKFold\nimport numpy as np\n```\n```\n# Create an imbalanced dataset\nX = np.random.randn(1000, 10)\ny = np.concatenate([np.ones(50), np.zeros(950)]) # 5% positive class\n# Stratified K-Fold maintains class proportions\nskf = StratifiedKFold(n\\_splits=5, shuffle=True, random\\_state=42)\nf1\\_scores = []\nfor train\\_idx, val\\_idx in skf.split(X, y):\nX\\_train, X\\_val = X[train\\_idx], X[val\\_idx]\ny\\_train, y\\_val = y[train\\_idx], y[val\\_idx]\n# Check class distribution in each fold\nprint(f\"Training set class distribution: {np.bincount(y\\_train.astype(int))}\")\nprint(f\"Validation set class distribution: {np.bincount(y\\_val.astype(int))}\")\n```\nThis ensures that each fold has an equal representation of the classes, which provides more reliable performance metrics.\n## Time Series Cross-Validation\nFor time series data, traditional cross-validation is not applicable, as it assumes that data points are independent of each other. Using future data to predict past events violates the temporal dependencies that exist in time series datasets. In these cases,**Time Series Split**should be used.\n```\nfrom sklearn.model\\_selection import TimeSeriesSplit\nimport pandas as pd\nimport numpy as np\n```\n```\n# Generate time series data\ndates = pd.date\\_range(start='2023-01-01', periods=1000, freq='D')\nX = np.random.randn(1000, 5)\ny = np.random.randn(1000)\ntscv = TimeSeriesSplit(n\\_splits=5)\nfor train\\_idx, val\\_idx in tscv.split(X):\nprint(f\"Training set: {dates[train\\_idx[0]]} to {dates[train\\_idx[-1]]}\")\nprint(f\"Validation set: {dates[val\\_idx[0]]} to {dates[val\\_idx[-1]]}\\\\n\")\n```\nThis method ensures that the training set only includes data from prior periods, preserving the temporal order and preventing future data from influencing the model\u2019s training.\n## Nested Cross-Validation for Hyperparameter Tuning\nWhen tuning hyperparameters, nested cross-validation ensures unbiased performance estimates by separating the model selection and evaluation steps. This is particularly important when you want to assess the model\u2019s generalization ability without the risk of overfitting due to hyperparameter optimization.\n```\nfrom sklearn.model\\_selection import GridSearchCV, cross\\_val\\_score\nfrom sklearn.svm import SVC\n```\n```\n# Outer cross-validation loop\nouter\\_cv = KFold(n\\_splits=5, shuffle=True, random\\_state=42)\ninner\\_cv = KFold(n\\_splits=3, shuffle=True, random\\_state=42)\n# Parameter grid for tuning\npara...",
      "url": "https://medium.com/@silva.f.francis/avoiding-data-leakage-in-cross-validation-ba344d4d55c0"
    },
    {
      "title": "Do I use the same Tfidf vocabulary in k-fold cross_validation",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [Do I use the same Tfidf vocabulary in k-fold cross\\_validation](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked6 years, 9 months ago\n\nModified [4 years, 11 months ago](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation?lastactivity)\n\nViewed\n5k times\n\nPart of [NLP](https://stackoverflow.com/collectives/nlp) Collective\n\n13\n\nI am doing text classification based on `TF-IDF` Vector Space Model.I have only no more than 3000 samples.For the fair evaluation, I'm evaluating the classifier using 5-fold cross-validation.But what confuses me is that whether it is necessary to rebuild the `TF-IDF` Vector Space Model in each fold cross-validation. Namely, would I need to rebuild the vocabulary and recalculate the `IDF` value in vocabulary in each fold cross-validation?\n\nCurrently I'm doing TF-IDF tranforming based on scikit-learn toolkit, and training my classifier using SVM. My method is as follows: firstly,I'm dividing the sample in hand by the ratio of 3:1, 75 percent of them are applied to fit the parameter of the TF-IDF Vector Space Model.Herein, the parameter is the size of vocabulary and the terms that contained in it, also the `IDF` value of each term in vocabulary.Then I'm transforming the remainder in this `TF-IDF` `SVM` and using these vectors to make 5-fold cross-validation (Notably, I don't use the previous 75 percent samples for transforming).\n\nMy code is as follows:\n\n```\n# train, test split, the train data is just for TfidfVectorizer() fit\nx_train, x_test, y_train, y_test = train_test_split(data_x, data_y, train_size=0.75, random_state=0)\ntfidf = TfidfVectorizer()\ntfidf.fit(x_train)\n\n# vectorizer test data for 5-fold cross-validation\nx_test = tfidf.transform(x_test)\n\n scoring = ['accuracy']\n clf = SVC(kernel='linear')\n scores = cross_validate(clf, x_test, y_test, scoring=scoring, cv=5, return_train_score=False)\n print(scores)\n\n```\n\nMy confusion is that whether my method doing `TF-IDF` transforming and making 5-fold cross-validation is correct, or whether it's necessary to rebuild the `TF-IDF` Vector Model Space using train data and then transform into `TF-IDF` vectors with both train and test data? Just as follows:\n\n```\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor train_index, test_index in skf.split(data_x, data_y):\n    x_train, x_test = data_x[train_index], data_x[test_index]\n    y_train, y_test = data_y[train_index], data_y[test_index]\n\n    tfidf = TfidfVectorizer()\n    x_train = tfidf.fit_transform(x_train)\n    x_test = tfidf.transform(x_test)\n\n    clf = SVC(kernel='linear')\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    score = accuracy_score(y_test, y_pred)\n    print(score)\n\n```\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation)\n- [tf-idf](https://stackoverflow.com/questions/tagged/tf-idf)\n\n[Share](https://stackoverflow.com/q/46010617)\n\n[Improve this question](https://stackoverflow.com/posts/46010617/edit)\n\nFollow\n\n[edited Jul 5, 2019 at 11:26](https://stackoverflow.com/posts/46010617/revisions)\n\n[![Venkatachalam's user avatar](https://i.sstatic.net/JUZVe.jpg?s=64)](https://stackoverflow.com/users/6347629/venkatachalam)\n\n[Venkatachalam](https://stackoverflow.com/users/6347629/venkatachalam)\n\n16.7k1010 gold badges5151 silver badges7777 bronze badges\n\nasked Sep 2, 2017 at 4:57\n\n[![lx.F's user avatar](https://www.gravatar.com/avatar/89b468dfe5becae5ad62f690db1a3104?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/8543167/lx-f)\n\n[lx.F](https://stackoverflow.com/users/8543167/lx-f) lx.F\n\n13111 silver badge33 bronze badges\n\n1\n\n- Yes. Yes it is.\n\n\u2013\u00a0[juanpa.arrivillaga](https://stackoverflow.com/users/5014455/juanpa-arrivillaga)\n\nCommentedSep 2, 2017 at 5:06\n\n\n[Add a comment](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n5\n\nThe `StratifiedKFold` approach, which you had adopted to build the `TfidfVectorizer()` is the right way, by doing so you are making sure that features are generated only based out of the training dataset.\n\nIf you think about building the `TfidfVectorizer()` on the whole dataset, then its situation of leaking the test dataset to the model even though we are not explicitly feeding the test dataset. The parameters such as size of vocabulary, IDF value of each term in vocabulary would greatly differ when test documents are included.\n\nThe simpler way could be using pipeline and cross\\_validate.\n\nUse this!\n\n```\nfrom sklearn.pipeline import make_pipeline\nclf = make_pipeline(TfidfVectorizer(), svm.SVC(kernel='linear'))\n\nscores = cross_validate(clf, data_x, data_y, scoring=['accuracy'], cv=5, return_train_score=False)\nprint(scores)\n\n```\n\nNote: It is not useful to do `cross_validate` on the test data alone. we have to do on the `[train + validation]` dataset.\n\n[Share](https://stackoverflow.com/a/54277672)\n\n[Improve this answer](https://stackoverflow.com/posts/54277672/edit)\n\nFollow\n\n[edited Jan 20, 2019 at 15:13](https://stackoverflow.com/posts/54277672/revisions)\n\nanswered Jan 20, 2019 at 14:54\n\n[![Venkatachalam's user avatar](https://i.sstatic.net/JUZVe.jpg?s=64)](https://stackoverflow.com/users/6347629/venkatachalam)\n\n[Venkatachalam](https://stackoverflow.com/users/6347629/venkatachalam) Venkatachalam\n\n16.7k1010 gold badges5151 silver badges7777 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation)\u00a0\\|\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack Overflow. [Learn more](https://stackoverflow.com/help/gen-ai-policy)\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f46010617%2fdo-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation%23new-answer)\n\nSign up using Google\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [python](https://stackoverflow.com/questions/tagged/python) - [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn) - [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation) - [tf-idf](https://stackoverflow.com/questions/tagged/tf-idf)   or [ask your own qu...",
      "url": "https://stackoverflow.com/questions/46010617/do-i-use-the-same-tfidf-vocabulary-in-k-fold-cross-validation"
    },
    {
      "title": "11.  Common pitfalls and recommended practices #",
      "text": "11. Common pitfalls and recommended practices &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](_static/scikit-learn-logo-without-subtitle.svg)](index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 11.Common pitfalls and recommended practices[#](#common-pitfalls-and-recommended-practices)\nThe purpose of this chapter is to illustrate some common pitfalls and\nanti-patterns that occur when using scikit-learn. It provides\nexamples of what**not**to do, along with a corresponding correct\nexample.\n## 11.1.Inconsistent preprocessing[#](#inconsistent-preprocessing)\nscikit-learn provides a library of[Dataset transformations](data_transforms.html#data-transforms), which\nmay clean (see[Preprocessing data](modules/preprocessing.html#preprocessing)), reduce\n(see[Unsupervised dimensionality reduction](modules/unsupervised_reduction.html#data-reduction)), expand (see[Kernel Approximation](modules/kernel_approximation.html#kernel-approximation))\nor generate (see[Feature extraction](modules/feature_extraction.html#feature-extraction)) feature representations.\nIf these data transforms are used when training a model, they also\nmust be used on subsequent datasets, whether it\u2019s test data or\ndata in a production system. Otherwise, the feature space will change,\nand the model will not be able to perform effectively.\nFor the following example, let\u2019s create a synthetic dataset with a\nsingle feature:\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_regression&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;random\\_state=42&gt;&gt;&gt;X,y=make\\_regression(random\\_state=random\\_state,n\\_features=1,noise=1)&gt;&gt;&gt;X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(...X,y,test\\_size=0.4,random\\_state=random\\_state)\n```\n**Wrong**\nThe train dataset is scaled, but not the test dataset, so model\nperformance on the test dataset is worse than expected:\n```\n&gt;&gt;&gt;fromsklearn.metricsimportmean\\_squared\\_error&gt;&gt;&gt;fromsklearn.linear\\_modelimportLinearRegression&gt;&gt;&gt;fromsklearn.preprocessingimportStandardScaler&gt;&gt;&gt;scaler=StandardScaler()&gt;&gt;&gt;X\\_train\\_transformed=scaler.fit\\_transform(X\\_train)&gt;&gt;&gt;model=LinearRegression().fit(X\\_train\\_transformed,y\\_train)&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test))62.80...\n```\n**Right**\nInstead of passing the non-transformed`X\\_test`to`predict`, we should\ntransform the test data, the same way we transformed the training data:\n```\n&gt;&gt;&gt;X\\_test\\_transformed=scaler.transform(X\\_test)&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test\\_transformed))0.90...\n```\nAlternatively, we recommend using a[`Pipeline`](modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), which makes it easier to chain transformations\nwith estimators, and reduces the possibility of forgetting a transformation:\n```\n&gt;&gt;&gt;fromsklearn.pipelineimportmake\\_pipeline&gt;&gt;&gt;model=make\\_pipeline(StandardScaler(),LinearRegression())&gt;&gt;&gt;model.fit(X\\_train,y\\_train)Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),(&#39;linearregression&#39;, LinearRegression())])&gt;&gt;&gt;mean\\_squared\\_error(y\\_test,model.predict(X\\_test))0.90...\n```\nPipelines also help avoiding another common pitfall: leaking the test data\ninto the training data.\n## 11.2.Data leakage[#](#data-leakage)\nData leakage occurs when information that would not be available at prediction\ntime is used when building the model. This results in overly optimistic\nperformance estimates, for example from[cross-validation](modules/cross_validation.html#cross-validation), and thus poorer performance when the model is used\non actually novel data, for example during production.\nA common cause is not keeping the test and train data subsets separate.\nTest data should never be used to make choices about the model.**The general rule is to never call**`fit`**on the test data**. While this\nmay sound obvious, this is easy to miss in some cases, for example when\napplying certain pre-processing steps.\nAlthough both train and test data subsets should receive the same\npreprocessing transformation (as described in the previous section), it is\nimportant that these transformations are only learnt from the training data.\nFor example, if you have a\nnormalization step where you divide by the average value, the average should\nbe the average of the train subset,**not**the average of all the data. If the\ntest subset is included in the average calculation, information from the test\nsubset is influencing the model.\n### 11.2.1.How to avoid data leakage[#](#how-to-avoid-data-leakage)\nBelow are some tips on avoiding data leakage:\n* Always split the data into train and test subsets first, particularly\nbefore any preprocessing steps.\n* Never include test data when using the`fit`and`fit\\_transform`methods. Using all the data, e.g.,`fit(X)`, can result in overly optimistic\nscores.\nConversely, the`transform`method should be used on both train and test\nsubsets as the same preprocessing should be applied to all the data.\nThis can be achieved by using`fit\\_transform`on the train subset and`transform`on the test subset.\n* The scikit-learn[pipeline](modules/compose.html#pipeline)is a great way to prevent data\nleakage as it ensures that the appropriate method is performed on the\ncorrect data subset. The pipeline is ideal for use in cross-validation\nand hyper-parameter tuning functions.\nAn example of data leakage during preprocessing is detailed below.\n### 11.2.2.Data leakage during pre-processing[#](#data-leakage-during-pre-processing)\nNote\nWe here choose to illustrate data leakage with a feature selection step.\nThis risk of leakage is however relevant with almost all transformations\nin scikit-learn, including (but not limited to)[`StandardScaler`](modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler),[`SimpleImputer`](modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer), and[`PCA`](modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA).\nA number of[Feature selection](modules/feature_selection.html#feature-selection)functions are available in scikit-learn.\nThey can help remove irrelevant, redundant and noisy features as well as\nimprove your model build time and performance. As with any other type of\npreprocessing, feature selection should**only**use the training data.\nIncluding the test data in feature selection will optimistically bias your\nmodel.\nTo demonstrate we will create this binary classification problem with\n10,000 randomly generated features:\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;n\\_samples,n\\_features,n\\_classes=200,10000,2&gt;&gt;&gt;rng=np.random.RandomState(42)&gt;&gt;&gt;X=rng.standard\\_normal((n\\_samples,n\\_features))&gt;&gt;&gt;y=rng.choice(n\\_classes,n\\_samples)\n```\n**Wrong**\nUsing all the data to perform feature selection results in an accuracy score\nmuch higher than chance, even though our targets are completely random.\nThis randomness means that our`X`and`y`are independent and we thus expect\nthe accuracy to be around 0.5. However, since the feature selection step\n\u2018sees\u2019 the test data, the model has an unfair advantage. In the incorrect\nexample below we first use all the data for feature selection and then split\nthe data into training and test subsets for model fitting. The result is a\nmuch higher than expected accuracy score:\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;fromsklearn.feature\\_selectionimportSelectKBest&gt;&gt;&gt;fromsklearn.ensembleimportHistGradientBoostingClassifier&gt;&gt;&gt;fromsklearn.metricsimportaccuracy\\_score&gt;&gt;&gt;# Incorrect preprocessing: the entire data is tr...",
      "url": "https://scikit-learn.org/stable/common_pitfalls.html"
    },
    {
      "title": "Sample pipeline for text feature extraction and evaluation #",
      "text": "Sample pipeline for text feature extraction and evaluation &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\nNote\n[Go to the end](#sphx-glr-download-auto-examples-model-selection-plot-grid-search-text-feature-extraction-py)to download the full example code or to run this example in your browser via JupyterLite or Binder.\n# Sample pipeline for text feature extraction and evaluation[#](#sample-pipeline-for-text-feature-extraction-and-evaluation)\nThe dataset used in this example is[The 20 newsgroups text dataset](../../datasets/real_world.html#newsgroups-dataset)which will be\nautomatically downloaded, cached and reused for the document classification\nexample.\nIn this example, we tune the hyperparameters of a particular classifier using a[`RandomizedSearchCV`](../../modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV). For a demo on the\nperformance of some other classifiers, see the[Classification of text documents using sparse features](../text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py)notebook.\n```\n# Authors: The scikit-learn developers# SPDX-License-Identifier: BSD-3-Clause\n```\n## Data loading[#](#data-loading)\nWe load two categories from the training set. You can adjust the number of\ncategories by adding their names to the list or setting`categories=None`when\ncalling the dataset loader[`fetch\\_20newsgroups`](../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)to get\nthe 20 of them.\n```\nfromsklearn.datasetsimport[fetch\\_20newsgroups](../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)categories=[&quot;alt.atheism&quot;,&quot;talk.religion.misc&quot;,]data\\_train=[fetch\\_20newsgroups](../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)(subset=&quot;train&quot;,categories=categories,shuffle=True,random\\_state=42,remove=(&quot;headers&quot;,&quot;footers&quot;,&quot;quotes&quot;),)data\\_test=[fetch\\_20newsgroups](../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)(subset=&quot;test&quot;,categories=categories,shuffle=True,random\\_state=42,remove=(&quot;headers&quot;,&quot;footers&quot;,&quot;quotes&quot;),)print(f&quot;Loading 20 newsgroups dataset for{len(data\\_train.target\\_names)}categories:&quot;)print(data\\_train.target\\_names)print(f&quot;{len(data\\_train.data)}documents&quot;)\n```\n```\nLoading 20 newsgroups dataset for 2 categories:\n[&#39;alt.atheism&#39;, &#39;talk.religion.misc&#39;]\n857 documents\n```\n## Pipeline with hyperparameter tuning[#](#pipeline-with-hyperparameter-tuning)\nWe define a pipeline combining a text feature vectorizer with a simple\nclassifier yet effective for text classification.\n```\nfromsklearn.feature\\_extraction.textimport[TfidfVectorizer](../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)fromsklearn.naive\\_bayesimport[ComplementNB](../../modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB)fromsklearn.pipelineimport[Pipeline](../../modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)pipeline=[Pipeline](../../modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)([(&quot;vect&quot;,[TfidfVectorizer](../../modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)()),(&quot;clf&quot;,[ComplementNB](../../modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB)()),])pipeline\n```\n```\nPipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, ComplementNB())])\n```\n**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\nOn GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**\nPipeline\n[?Documentation for Pipeline](https://scikit-learn.org/1.8/modules/generated/sklearn.pipeline.Pipeline.html)iNot fitted\nParameters**|[stepssteps: list of tuples\nList of (name of step, estimator) tuples that are to be chained in\nsequential order. To be compatible with the scikit-learn API, all steps\nmust define `fit`. All non-last steps must also define `transform`. See\n:ref:`Combining Estimators` for more details.](<https://scikit-learn.org/1.8/modules/generated/sklearn.pipeline.Pipeline.html#:~:text=steps,-list of tuples>)|[(&#x27;vect&#x27;, ...), (&#x27;clf&#x27;, ...)]|\n**|[transform\\_inputtransform\\_input: list of str, default=None\nThe names of the :term:`metadata` parameters that should be transformed by the\npipeline before passing it to the step consuming it.\nThis enables transforming some input arguments to ``fit`` (other than ``X``)\nto be transformed by the steps of the pipeline up to the step which requires\nthem. Requirement is defined via :ref:`metadata routing`.\nFor instance, this can be used to pass a validation set through the pipeline.\nYou can only set this if metadata routing is enabled, which you\ncan enable using ``sklearn.set\\_config(enable\\_metadata\\_routing=True)``.\n.. versionadded:: 1.6](<https://scikit-learn.org/1.8/modules/generated/sklearn.pipeline.Pipeline.html#:~:text=transform_input,-list of str, default=None>)|None|\n**|[memorymemory: str or object with the joblib.Memory interface, default=None\nUsed to cache the fitted transformers of the pipeline. The last step\nwill never be cached, even if it is a transformer. By default, no\ncaching is performed. If a string is given, it is the path to the\ncaching directory. Enabling caching triggers a clone of the transformers\nbefore fitting. Therefore, the transformer instance given to the\npipeline cannot be inspected directly. Use the attribute ``named\\_steps``\nor ``steps`` to inspect estimators within the pipeline. Caching the\ntransformers is advantageous when fitting is time consuming. See\n:ref:`sphx\\_glr\\_auto\\_examples\\_neighbors\\_plot\\_caching\\_nearest\\_neighbors.py`\nfor an example on how to enable caching.](<https://scikit-learn.org/1.8/modules/generated/sklearn.pipeline.Pipeline.html#:~:text=memory,-str or object with the joblib.Memory interface, default=None>)|None|\n**|[verboseverbose: bool, default=False\nIf True, the time elapsed while fitting each step will be printed as it\nis completed.](<https://scikit-learn.org/1.8/modules/generated/sklearn.pipeline.Pipeline.html#:~:text=verbose,-bool, default=False>)|False|\nTfidfVectorizer\n[?Documentation for TfidfVectorizer](https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\nParameters**|[inputinput: {'filename', 'file', 'content'}, default='content'\n- If `'filename'`, the sequence passed as an argument to fit is\nexpected to be a list of filenames that need reading to fetch\nthe raw content to analyze.\n- If `'file'`, the sequence items must have a 'read' method (file-like\nobject) that is called to fetch the bytes in memory.\n- If `'content'`, the input is expected to be a sequence of items that\ncan be of type string or byte.](<https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=input,-{'filename', 'file', 'content'}, default='content'>)|&#x27;content&#x27;|\n**|[encodingencoding: str, default='utf-8'\nIf bytes or files are given to analyze, this encoding is used to\ndecode.](<https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVector...",
      "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_text_feature_extraction.html"
    },
    {
      "title": "Pipelines for text classification in\u00a0scikit-learn",
      "text": "[Scikit-learn\u2019s](http://scikit-learn.org) [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) provide a useful layer of abstraction for building complex estimators or classification models. Its purpose is to aggregate a number of data transformation steps, and a model operating on the result of these transformations, into a single object that can then be used in place of a simple estimator. This allows for the one-off definition of complex pipelines that can be re-used, for example, in cross-validation functions, grid-searches, learning curves and so on. I will illustrate their use, and some pitfalls, in the context of a kaggle text-classification\u00a0challenge.\n\n### The\u00a0challenge\n\nThe goal in the [StumbleUpon Evergreen](https://www.kaggle.com/c/stumbleupon) classification challenge is the prediction of whether a given web page is relevant for a short period of time only (ephemeral) or can be recommended still a long time after initial discovery\u00a0(evergreen).\n\nEach webpage in the provided dataset is represented by its html content as well as additional meta-data, the latter of which I will ignore here for simplicity. Instead I will focus on the use of pipelines to 1) transform text data into a numerical form appropriate for machine learning purposes, and 2) for creating ensembles of different classifiers to (hopefully) improve prediction accuracy (or at least its\u00a0variance).\n\n### Text\u00a0transformation\n\nA useful tool for the representation of text in a machine learning context is the so-called [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) transformation, short for \u201cterm frequency\u2013inverse document frequency\u201d. The idea is simple. Each word in a document is represented by a number that is proportional to its frequency in the document, and inversely proportional to the number of documents in which it occurs. Very common words, such as \u201ca\u201d or \u201cthe\u201d, thereby receive heavily discounted tf-df scores, in contrast to words that are very specific to the document in question. Scikit-learn provides a TfidfVectorizer class, which implements this transformation, along with a few other text-processing options, such as removing the most common words in the given language (stop words). The result is a matrix with one row per document and as many columns as there are different words in the dataset\u00a0(corpus).\n\n### Pipelines\n\nIn few cases, however, is the vectorization of text into numerical values as simple as applying tf-idf to the raw data. Often, the relevant text to be converted needs to be extracted first. Also, the tf-idf transformation will usually result in matrices too large to be used with certain machine learning algorithms. Hence dimensionality reduction techniques are often applied too. Manually implementing these steps everytime text needs to be transformed quickly becomes repetitive and tedious. It needs to be done for the training as well as test set. Ideally, when using cross-validation to assess one\u2019s model, the transformation needs to be applied separately in each fold, particularly when feature selection (dimensionality reduction) is involved. If care is not taken, information about the whole dataset otherwise leaks into supposedly independent evaluations of individual\u00a0folds.\n\nPipelines help reduce this repetition. What follows is an example of a typical vectorization\u00a0pipeline:\n\n```\ndef get_vec_pipe(num_comp=0, reducer='svd'):\n''' Create text vectorization pipeline with optional dimensionality reduction. '''\n\n    tfv = TfidfVectorizer(\n        min_df=6, max_features=None, strip_accents='unicode',\n        analyzer=\"word\", token_pattern=r'\\w{1,}', ngram_range=(1, 2),\n        use_idf=1, smooth_idf=1, sublinear_tf=1)\n\n    # Vectorizer\n    vec_pipe = [\n        ('col_extr', JsonFields(0, ['title', 'body', 'url'])),\n        ('squash', Squash()),\n        ('vec', tfv)\n    ]\n\n    # Reduce dimensions of tfidf\n    if num_comp > 0:\n        if reducer == 'svd':\n            vec_pipe.append(('dim_red', TruncatedSVD(num_comp)))\n        elif reducer == 'kbest':\n            vec_pipe.append(('dim_red', SelectKBest(chi2, k=num_comp)))\n        elif reducer == 'percentile':\n            vec_pipe.append(('dim_red', SelectPercentile(f_classif, percentile=num_comp)))\n\n        vec_pipe.append(('norm', Normalizer()))\n\n    return Pipeline(vec_pipe)\n```\n\nHere, we first create an instance of the tf-idf vectorizer (for its parameters see [documentation)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). We then create a list of tuples, each of which represents a data transformation step and its name (the latter of which is required, e.g., for identifying individual transformer parameters in a grid search). The first two are custom transformers and the last one our vectorizer. The first transformer (\u201cJsonFields\u201d), for example, extracts a particular column from the dataset, in this case the first (0-indexed), interprets its content as json-encoded text, and extracts the json fields with the keys \u2018title\u2019, \u2018body\u2019 and \u2018url\u2019. The corresponding values are concatenated into a single string per row in the dataset. The result is a new transformed dataset with a single column containing the extracted text, which can then be processed by the vectorizer. After the vectorization step, an optional dimensionality reduction is added to the list of transformations before the final pipeline is constructed and\u00a0returned.\n\n#### Transformers\n\nCustom transformers such as those above are easily created by subclassing from scikit\u2019s [TransformerMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html). This base class exposes a single fit\\_transform() function, which in turn calls (to be implemented) fit() and transform() functions. For transformers that do not require fitting (no internal parameters to be selected based on the dataset), we can create a simpler base class that only needs the transform function to be\u00a0implemented:\n\n```\nclass Transformer(TransformerMixin):\n''' Base class for pure transformers that don't need a fit method '''\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        return X\n\n    def get_params(self, deep=True):\n        return dict()\n```\n\nWith this in place, the JsonFields transformer looks like\u00a0this:\n\n```\nclass JsonFields(Transformer):\n    ''' Extract json encoded fields from a numpy array. Returns (iterable) numpy array so it can be used as input to e.g. Tdidf '''\n\n    def __init__(self, column, fields=[], join=True):\n        self.column = column\n        self.fields = fields\n        self.join = join\n\n    def get_params(self, deep=True):\n        return dict(column=self.column, fields=self.fields, join=self.join)\n\n    def transform(self, X, **transform_params):\n        col = Select(self.column, to_np=True).transform(X)\n        res = np.vectorize(extract_json, excluded=['fields'])(col, fields=self.fields)\n        return res\n```\n\nJsonFields itself encapsulates another custom transformer (Select), used here to keep the specification of pipelines concise. It could also have been used as a prior step in the definition of the pipeline. The Select transformer does nothing other than extracting a number of specified columns from a\u00a0dataset:\n\n```\nclass Select(Transformer):\n    '''  Extract specified columns from a pandas df or numpy array '''\n\n    def __init__(self, columns=0, to_np=True):\n        self.columns = columns\n        self.to_np = to_np\n\n    def get_params(self, deep=True):\n        return dict(columns=self.columns, to_np=self.to_np)\n\n    def transform(self, X, **transform_params):\n        if isinstance(X, pd.DataFrame):\n            allint = isinstance(self.columns, int) or\n                (isinstance(self.columns, list) and\n                 all([isinstance(x, int) for x in self.columns]))\n            if allint:\n                res = X.ix[:, self.columns]\n            elif all([isinstance(x, str) for x in self.columns]):\n                res = X[se...",
      "url": "https://buhrmann.github.io/sklearn-pipelines.html"
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    },
    {
      "title": "Smart Splitting Prevents Cross-Validation Disasters",
      "text": "Prevent Data Leakage in Cross-Validation | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://rsci.app.link/?$canonical_url=https://medium.com/p/894863a93553&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderUser&amp;~stage=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Smart Splitting Prevents Cross-Validation Disasters\n[\n![Paco Sun](https://miro.medium.com/v2/resize:fill:64:64/1*ZKhzskdSkU9jVOKVWtuTxg.jpeg)\n](https://medium.com/@pacosun?source=post_page---byline--894863a93553---------------------------------------)\n[Paco Sun](https://medium.com/@pacosun?source=post_page---byline--894863a93553---------------------------------------)\n5 min read\n\u00b7May 3, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/894863a93553&amp;operation=register&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;user=Paco+Sun&amp;userId=a49c41746153&amp;source=---header_actions--894863a93553---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/894863a93553&amp;operation=register&amp;redirect=https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553&amp;source=---header_actions--894863a93553---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nYou\u2019ve just trained a model that boasts 95% accuracy, your metrics are stellar, and you\u2019re ready to deploy. Then reality hits like a cold shower: in production, your high-performing model barely beats random guessing. What went wrong?\nThe culprit is often**data leakage**, when information from your test set sneaks into your training process, creating an illusion of competence. This deception usually pairs with**class imbalance**, where one category dominates your dataset.\nEven with cross-validation, your model can still mislead you if you\u2019re not careful. The truth is,**preprocessing decisions and skewed distributions can turn your CV pipeline into an exercise in self-deception.**In this article, we\u2019ll explore how to build robust validation strategies that won\u2019t crumble when they meet real-world data.\n## Data Leakage Explained\nData leakage is the ML equivalent of a student peeking at the answer key before the test. It occurs when information from your test set contaminates your training process, giving your model an unfair advantage that vanishes in production.\n### **Common Types of Leakage:**\n* **Preprocessing leakage:**The sneaky one. When you scale your entire dataset before splitting, you\u2019re telling your model about the test set\u2019s distribution\n```\n# WRONG: This causes leakage!\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX\\_scaled = scaler.fit\\_transform(X) # Scaling BEFORE splitting\nX\\_train, X\\_test = train\\_test\\_split(X\\_scaled, y)\n```\n* **Feature engineering leakage:**Creating features that implicitly contain information about the target such as using**average payment amount**in fraud detection when fraudulent transactions are typically higher\n* **Target leakage:**The most dangerous form, where features directly derived from the target variable sneak into your model\nHere\u2019s a real-world scenario: A cancer detection model achieved 98% accuracy by including`patient\\_id`as a feature. Why? The hospital\u2019s ID system assigned sequential numbers to patients, and later IDs correlated with advanced diagnostic equipment availability.**The model wasn\u2019t detecting cancer, but when patients were diagnosed instead.**\nThese leaks create models that only**look**brilliant.\n## Your First Line of Defence\nPipelines ensure transformations happen at the right time, in the right order, preventing information from crossing the train-test boundary.\nScikit-learn\u2019s`Pipeline`and`ColumnTransformer`are your weapons of choice. They bundle preprocessing steps with your model, ensuring**transformations are learned from training data only**and applied consistently to test data.\nWhy? Because fitting a scaler on your entire dataset is like telling your model information it shouldn\u2019t have during training.\n```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n# CORRECT: Transformations inside the pipeline\nnumeric\\_pipeline = Pipeline([\n(&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;median&#x27;)),\n(&#x27;scaler&#x27;, StandardScaler())\n])\npreprocessor = ColumnTransformer([\n(&#x27;&#x27;num&#x27;&#x27;, numeric\\_pipeline, some\\_numeric\\_features),\n(&#x27;&#x27;cat&#x27;&#x27;, OneHotEncoder(), some\\_categorical\\_features)\n])\n# Combine preprocessing with model\nfull\\_pipeline = Pipeline([\n(&#x27;preprocessor&#x27;, preprocessor),\n(&#x27;classifier&#x27;, RandomForestClassifier())\n])\n# Now CV won&#x27;t leak\nscores = cross\\_val\\_score(full\\_pipeline, X, y, cv=5)\n```\nFor each fold, the pipeline:\n* Fits transformers on training data only\n* Transforms both training and validation data\n* Trains the model\n* Evaluates on truly unseen, properly transformed validation data\nThis simple choice can be the difference between a model that**generalizes**and one that**memorizes**.\n## The Problem of Imbalanced Data\nImagine training a fraud detector where 99% of transactions are legitimate. Regular K-Fold might create folds with 100% legitimate transactions, which means your model learns to predict**\u201cnot fraud\u201d**for everything and still achieves 99% accuracy!\nClass imbalance turns standard CV into a game of chance. Some folds might completely miss the minority class, leading to wildly inconsistent performance estimates.\n**Stratified K-Fold**to the rescue! It preserves class proportions in each fold, ensuring every split represents your data\u2019s true distribution.\n> For more details, feel free to check out my stratified k-fold article [**> here\n**](https://medium.com/@pacosun/stratified-k-fold-cross-validation-when-balance-matters-c28b9a7cb9bc)> .\n## Pitfalls with Resampling Techniques (SMOTE, Undersampling)\nSMOTE before splitting is like photocopying your test answers, you\u2019re creating synthetic samples using information from data you\u2019re supposed to be testing on.\nWhen you apply SMOTE to your entire dataset before cross-validation, the synthetic samples in your training set contain information about the test set\u2019s minority class distribution. This creates optimistic performance estimates that crumble in production.\n```\n# WRONG: SMOTE before splitting causes leakage\nfrom imblearn.over\\_sampling import SMOTE\n# This leaks test information into training!\nX\\_resampled, y\\_resampled = SMOTE().fit\\_resample(X, y)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X\\_resampled, y\\_resampled)\n```\nSolution?**Keep it inside your pipeline.**\n```\n# CORRECT: SMOTE within the pipeline\nfrom i...",
      "url": "https://medium.com/@pacosun/split-smart-avoiding-data-leakage-in-cross-validation-894863a93553"
    }
  ]
}