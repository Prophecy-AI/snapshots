{
  "query": "What are the specific Stanford study hand-crafted textual cues that predict successful pizza requests, and how can they be implemented as features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The Stanford \u201cRandom Acts of\u202fPizza\u201d study identified a handful of hand\u2011crafted textual cues that are strong predictors of whether a pizza request will be fulfilled, and each cue can be turned into a simple engineered feature for a classifier.  \n\n* **Gratitude** \u2013 Requests that contain explicit thank\u2011you expressions (e.g., \u201cthank you\u201d, \u201cthanks so much\u201d) are significantly more likely to succeed\u202f([PDF](https://nlp.stanford.edu/pubs/icwsm2014_pizza.pdf)).\u202fImplementation: a binary flag\u202f`has_gratitude`\u202for a count\u202f`gratitude_word_count`\u202fbased on a predefined gratitude lexicon.  \n* **Evidentiality (Evidence)** \u2013 Providing concrete proof (photos, links, or specific details) boosts success\u202f([PDF](https://nlp.stanford.edu/pubs/icwsm2014_pizza.pdf); HuffPost summary).\u202fImplementation: a binary feature\u202f`has_evidence`\u202fthat checks for URLs, image links, or keywords such as \u201cphoto\u201d, \u201cpicture\u201d, \u201cproof\u201d, plus an optional count of evidence tokens.  \n* **Generalized reciprocity** \u2013 Phrases indicating the asker will \u201cpay it forward\u201d or \u201creturn the favor\u201d raise the likelihood of a donation\u202f([PDF](https://nlp.stanford.edu/pubs/icwsm2014_pizza.pdf)).\u202fImplementation: a binary flag\u202f`offers_reciprocity`\u202for a count of reciprocity cues (e.g., \u201cI\u2019ll repay\u201d, \u201cI\u2019ll give back\u201d).  \n* **Narrative length** \u2013 Longer, more detailed stories receive more pizza\u202f(Thrillist recap).\u202fImplementation: numeric features such as\u202f`word_count`,\u202f`char_count`, or\u202f`sentence_count`.  \n* **Requester status** \u2013 High\u2011status askers (e.g., high Reddit karma or long account age) have higher success rates\u202f([PDF](https://nlp.stanford.edu/pubs/icwsm2014_pizza.pdf)).\u202fImplementation: include the asker\u2019s\u202f`karma_score`\u202for\u202f`account_age_days`\u202fas numeric predictors.  \n\nBy extracting these cues from each request (e.g., using regex or a small lexicon) and feeding the resulting binary or numeric columns into a logistic regression or tree\u2011based model, you can replicate the hand\u2011crafted feature set that the Stanford authors showed to be predictive of pizza\u2011request success.",
      "url": ""
    },
    {
      "title": "",
      "text": "https://nlp.stanford.edu/pubs/icwsm2014_pizza.pdf\nNone\nNone\n2014-04-10\nHow to Ask for a Favor: A Case Study on the Success of Altruistic Requests\nTim Althoff\u2217, Cristian Danescu-Niculescu-Mizil\u2020, Dan Jurafsky\u2217\n\u2217Stanford University, \u2020Max Planck Institute SWS\nalthoff|jurafsky@stanford.edu, cristian@mpi-sws.org\nAbstract\nRequests are at the core of many social media systems such\nas question & answer sites and online philanthropy commu\u0002nities. While the success of such requests is critical to the\nsuccess of the community, the factors that lead community\nmembers to satisfy a request are largely unknown. Success\nof a request depends on factors like who is asking, how they\nare asking, when are they asking, and most critically what\nis being requested, ranging from small favors to substantial\nmonetary donations. We present a case study of altruistic re\u0002quests in an online community where all requests ask for the\nvery same contribution and do not offer anything tangible in\nreturn, allowing us to disentangle what is requested from tex\u0002tual and social factors. Drawing from social psychology liter\u0002ature, we extract high-level social features from text that op\u0002erationalize social relations between recipient and donor and\ndemonstrate that these extracted relations are predictive of\nsuccess. More specifically, we find that clearly communicat\u0002ing need through the narrative is essential and that linguistic\nindications of gratitude, evidentiality, and generalized reci\u0002procity, as well as high status of the asker further increase the\nlikelihood of success. Building on this understanding, we de\u0002velop a model that can predict the success of unseen requests,\nsignificantly improving over several baselines. We link these\nfindings to research in psychology on helping behavior, pro\u0002viding a basis for further analysis of success in social media\nsystems.\n1 Introduction\nWe live in a time where people increasingly turn to the\nweb for help. Our needs, however, often go far beyond\nmere information from existing webpages and we need\nhelp from real people. For example, we ask for answers to\nspecific questions on StackOverflow.com, for donations on\nDonorsChoose.org, or for help on online social communi\u0002ties such as Reddit.com. In each of these cases a user per\u0002forms a request, which we define as an act of asking for\u0002mally for something. All these communities rely heavily on\ntheir members to help satisfy the request. Yet, the factors\nthat lead community members to satisfy a request remain\nlargely unknown. Understanding the dynamics and factors\nof successful requests has the potential to substantially im\u0002prove such communities by educating users about better for\u0002Copyright \u20ddc 2014, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nmulating requests and promoting likely-to-succeed requests\n(Greenberg et al. 2013; Mitra and Gilbert 2014). In addi\u0002tion to these practical benefits, understanding the factors that\nmake a request successful has implications for questions in\nsocial psychology and linguistic pragmatics.\nStudies on the popular crowdfunding platform Kickstarter\nhave shown that the success of a request depends most\ncrucially on what is being requested, that is, whether it\nis a small favor like an answer to a simple question or a\nlarge financial contribution (Mitra and Gilbert 2014; Mol\u0002lick 2014). Many other factors need to be controlled as well;\nwhat the giver receives in return, when they are asking, and\neven group dynamics, since people are more likely to give\nto projects that others are already giving to (Etter, Gross\u0002glauser, and Thiran 2013; Ceyhan, Shi, and Leskovec 2011;\nMitra and Gilbert 2014). Satisfying a request on peer-to-peer\nlending or crowd-funding platforms can also bring a reward,\nand this also can drive the selection process. It is extremely\ndifficult to disentangle the effects of all these factors in deter\u0002mining what makes people satisfy requests, and what makes\nthem select some requests over others.\nIn this paper, we develop a framework for controlling for\neach of these potential confounds while studying the role\nof two aspects that characterize compelling requests: social\nfactors (who is asking and how the recipient is related to\nthe donor and community) and linguistic factors (how they\nare asking and what linguistic devices accompany successful\nrequests). With the notable exception of Mitra and Gilbert\n(2014), the effect of language on the success of requests has\nlargely been ignored thus far.1\nOur goal is to understand what motivates people to give\nwhen they do not receive anything tangible in return. That is,\nwe focus on the important special case of altruistic requests\nin which the giver receives no rewards. This controls for the\nincentive to obtain attractive rewards commonly offered on\ncrowdfunding sites such as Kickstarter; the absence of exter\u0002nal factors such as tangible rewards also makes the language\nitself all the more important in persuading others to help. In\nthis domain we also do not need to consider crowdfunding\u0002related marketing strategies such as emphasizing limited\n1Linguistic factors have also been considered to influence the\nresponse quantity, quality, and speed to questions in online com\u0002munities and social networks (Teevan, Morris, and Panovich 2011;\nBurke et al. 2007, inter alia).\ntime offers (scarcity) or showing that other people made the\nsame decision already (social proof) (Cialdini 2001), which\nare known to manifest themselves in language (Mitra and\nGilbert 2014). Second, we focus on requests that a single\nuser can fulfill, thereby additionally eliminating group be\u0002havior effects such as herding (Ceyhan, Shi, and Leskovec\n2011) or completing donation biases (Wash 2013). Finally,\nwe focus on one community in which what is being asked\nfor is held constant. This allows us to explore a large num\u0002ber of different requests of different individual users, at dif\u0002ferent times, that all have the same goal. Controlling for the\nrequest goal therefore allows us to study how to optimize a\nparticular request solely by optimizing its presentation, and\nhelps provide a direct practical benefit to the requester (by\ncontrast, advising a requester who needs something to in\u0002stead ask for something different may be advice of limited\npractical use).\nWe therefore chose to study donations in \u201cRandom Acts\nof Pizza\u201d, an online community devoted to giving away free\npizza to strangers that ask for one. Random Acts of Pizza2\n(RAOP) is a community within the social news and enter\u0002tainment website Reddit.com. Users can submit requests for\nfree pizza and if their story is compelling enough a fellow\nuser might decide to send them one, \u201cbecause... who doesn\u2019t\nlike helping out a stranger? The purpose is to have fun, eat\npizza and help each other out. Together, we aim to restore\nfaith in humanity, one slice at a time.3\u201d A typical post might\nsound something like this: \u201cIt\u2019s been a long time since my\nmother and I have had proper food. I\u2019ve been struggling to\nfind any kind of work so I can supplement my mom\u2019s so\u0002cial security... A real pizza would certainly lift our spirits\n(Berman 2011).\u201d\nThis platform addresses many of the potential confounds\nthat complicate other platforms or studies: all requests ask\nfor the same thing, a pizza, there are no additional incen\u0002tives or rewards, each request is satisfied by a single user,\nusers and requests are embedded in a social network within\nReddit, and requests are largely textual. This dataset thus\nprovides us with an unusually clear picture of the effect of\nlanguage and social factors on success.\nThe remainder of this paper is organized as follows: in\u0002spired by studies in crowdfunding, user-to-user evaluations\nin social networks, and helping behavior in social psychol\u0002ogy, we introduce a variety of textual and social factors that\nare potentially associated with successful requests. We use\ntopic modeling and automatic detection to extract a particu\u0002larly complex factor, the narrative s...",
      "url": "https://nlp.stanford.edu/pubs/icwsm2014_pizza.pdf"
    },
    {
      "title": "",
      "text": "https://nlp.stanford.edu/pubs/yang2019persuasion.pdf\nNone\nNone\n2019-04-09\nLet\u2019s Make Your Request More Persuasive: Modeling Persuasive\nStrategies via Semi-Supervised Neural Nets on Crowdfunding Platforms\nDiyi Yang\u2217, Jiaao Chen\u2217, Zichao Yang, Dan Jurafsky, Eduard Hovy\nGeorgia Institute of Technology, Carnegie Mellon University, Stanford University\ndiyi.yang@cc.gatech.edu\n{jiaaoc, zichaoy, hovy}@andrew.cmu.edu\njurafsky@stanford.edu\nAbstract\nModeling what makes a request persuasive\u2014\neliciting the desired response from a reader\u2014\nis critical to the study of propaganda, behav\u0002ioral economics, and advertising. Yet current\nmodels can\u2019t quantify the persuasiveness of re\u0002quests or extract successful persuasive strate\u0002gies. Building on theories of persuasion, we\npropose a neural network to quantify persua\u0002siveness and identify the persuasive strategies\nin advocacy requests. Our semi-supervised hi\u0002erarchical neural network model is supervised\nby the number of people persuaded to take ac\u0002tions and partially supervised at the sentence\nlevel with human-labeled rhetorical strategies.\nOur method outperforms several baselines,\nuncovers persuasive strategies\u2014offering in\u0002creased interpretability of persuasive speech\u2014\nand has applications for other situations with\ndocument-level supervision but only partial\nsentence supervision.\n1 Introduction\nCrowdfunding platforms are a popular way to\nraise funds for projects. For example, Kiva, a peer\u0002to-peer lending platform, has crowd-funded more\nthan a million loans, totaling over $1 billion since\n2005. Kickstarter, another online crowdfunding\nplatform, successfully funded 110,270 projects\nwith a total of over 2 billion dollars. Yet most\nprojects still suffer from low success rates. How\ncan we help requesters craft persuasive and suc\u0002cessful pitches to convince others to take actions?\nPersuasive communication has the potential to\nshape and change people\u2019s attitudes and behaviors\n(Hovland et al., 1953), and has been widely re\u0002searched in various fields such as social psychol\u0002ogy, marketing, behavioral economics, and politi\u0002cal campaigning (Shrum et al., 2012). One of the\n\u2217 Equal contribution. This work was done when the first\ntwo authors were students at CMU.\nmost influential theories in the advertising liter\u0002ature is Chaiken\u2019s systematic-heuristic dual pro\u0002cessing theory, which suggests that people pro\u0002cess persuasive communication by evaluating the\nquality of arguments or by relying on inferential\nrules. Some such heuristic rules are commonly\nused in consumer behaviors; commercial websites\nmay highlight the limited availability of their items\n\u201cIn high demand - only 2 left on our site!\u201d or em\u0002phasize the person in authority \u201cSpeak to our head\nof sales\u2014he has over 15 years\u2019 experience sell\u0002ing properties\u201d to attract potential consumers. Al\u0002though numerous studies on persuasion have been\nconducted (Chaiken, 1980), we still know little\nabout the way how persuasion functions in the\nwild and how it can be modeled computationally.\nIn this work, we utilize neural-network based\nmethods to computationally model persuasion in\nrequests from crowdfunding websites. We build\non theoretical models of persuasion to operational\u0002ize persuasive strategies and ensure generalizabil\u0002ity. We propose to identify the persuasive strat\u0002egy employed in each sentence in each request.\nHowever, constructing a large dataset with persua\u0002sion strategies labeled at the sentence level is time\u0002consuming and expensive. Instead, we propose to\nuse a small amount of hand-labeled sentences to\u0002gether with a large number of requests automati\u0002cally labeled at the document level by the number\nof persuaded support actions. Our model is a semi\u0002supervised hierarchical neural network that iden\u0002tifies the persuasive strategies employed in each\nsentence, where the supervision comes from the\noverall persuasiveness of the request. We propose\nthat the success of requests could have substan\u0002tive explanatory power to uncover their persuasive\nstrategies. We also introduce an annotated cor\u0002pus with sentence-level persuasion strategy labels\nand document-level persuasiveness labels, to fa\u0002cilitate future work on persuasion. Experiments\nshow that our semi-supervised model outperforms\nseveral baselines. We then apply this automated\nmodel to unseen requests from different domains\nand obtain nuanced findings of the importance of\ndifferent strategies on persuasion success. Our\nmodel can be useful in any situation in which we\nhave exogenous document-level supervision, but\nonly small amounts of expensive human-annotated\nsentence labels.\n2 Related Work\nComputational argumentation has received much\nrecent attention (Ghosh et al., 2016; Stab and\nGurevych, 2017; Peldszus and Stede, 2013; Stab\net al., 2018; Ghosh et al., 2014). Most work has ei\u0002ther identified the arguments in news articles (Sar\u0002dianos et al., 2015) or user-generated web con\u0002tent (Habernal and Gurevych, 2017; Musi et al.,\n2018), or classified argument components (Zhang\nand Litman, 2015) into claims and premises, sup\u0002porting and opposing claims, or backings, re\u0002buttals and refutations . For example, Stab\nand Gurevych (2014) proposed structural, lex\u0002ical, syntactic and contextual features to iden\u0002tify convincing components of Web arguments in\u0002cluding claim, major claim, and premise. Sim\u0002ilarly, Zhang and Litman (2015) studied stu\u0002dent essay revisions and classified a set of ar\u0002gumentative actions associated with successful\nwriting such as warrant/reasoning/backing, rebut\u0002tal/reservation, and claims/ideas. Habernal and\nGurevych (2016) investigated the persuasiveness\nof arguments in any given argument pair using\nbidirectional LSTM. Hidey et al., (2017) utilized\nthe persuasive modes\u2014ethos, logos, pathos\u2014to\nmodel premises and the semantic types of argu\u0002ment components in an online persuasive forum.\nWhile most computational argumentation fo\u0002cuses on the relational support structures and fac\u0002tual evidence to make claims, persuasion focuses\nmore on language cues aimed at shaping, rein\u0002forcing and changing people\u2019s opinions and be\u0002liefs. How language changes people\u2019s attitudes\nand behaviors have received less attention from\nthe computational community than argumentation,\nalthough there have been important preliminary\nwork (Persing and Ng, 2017; Carlile et al., 2018).\nFarra et al., (2015) built regression models to pre\u0002dict essay scores based on features extracted from\nopinion expressions and topical elements. Chat\u0002terjee et al., (2014) used verbal descriptors and\npara-verbal markers of hesitation to predict speak\u0002ers\u2019 persuasiveness on website housing videos of\nproduct reviews. When looking at persuasion in\nthe context of online forum discussions (Wei et al.,\n2016), Tan et al., (2016) found that on the Change\nMy View subreddit, interaction dynamics such as\nthe language interplay between opinion holders\nand other participants provides highly predictive\ncues for persuasiveness. Using the same dataset,\nWel et al., (2016) extracted a set of textual infor\u0002mation and social interaction features to identify\npersuasive posts.\nRecently, Pryzant et al., (2017) introduced a\nneural network with an adversarial objective to\nselect text features that are predictive of some\noutcomes but decorrelated with others and fur\u0002ther analyzed the narratives highlighted by such\ntext features. Further work extended the model\nto induce narrative persuasion lexicons predictive\nof enrollment from course descriptions and sales\nfrom product descriptions (Pryzant et al., 2018a),\nand the efficacy of search advertisements (Pryzant\net al., 2018b). Similar to their settings, we use\nthe outcomes of a persuasive description to su\u0002pervise the learning of persuasion tactics, and our\nmodel can similarly induce lexicons associated\nwith successful narrative persuasion by examining\nhighly attentional words associated with persua\u0002sion outcomes. Our work differs both in our semi\u0002supervised method and also because we explicitly\ndraw on the theoretical literature to model the per\u0002suasion strat...",
      "url": "https://nlp.stanford.edu/pubs/yang2019persuasion.pdf"
    },
    {
      "title": "Let's Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms",
      "text": "Proceedings of NAACL-HLT 2019, pages 3620\u20133630\nMinneapolis, Minnesota, June 2 - June 7, 2019. \rc 2019 Association for Computational Linguistics\n3620\nLet\u2019s Make Your Request More Persuasive: Modeling Persuasive\nStrategies via Semi-Supervised Neural Nets on Crowdfunding Platforms\nDiyi Yang\u2217, Jiaao Chen\u2217, Zichao Yang, Dan Jurafsky, Eduard Hovy\nGeorgia Institute of Technology, Carnegie Mellon University, Stanford University\ndiyi.yang@cc.gatech.edu\n{jiaaoc, zichaoy, hovy}@andrew.cmu.edu\njurafsky@stanford.edu\nAbstract\nModeling what makes a request persuasive\u2014\neliciting the desired response from a reader\u2014\nis critical to the study of propaganda, behav\u0002ioral economics, and advertising. Yet current\nmodels can\u2019t quantify the persuasiveness of re\u0002quests or extract successful persuasive strate\u0002gies. Building on theories of persuasion, we\npropose a neural network to quantify persua\u0002siveness and identify the persuasive strategies\nin advocacy requests. Our semi-supervised hi\u0002erarchical neural network model is supervised\nby the number of people persuaded to take ac\u0002tions and partially supervised at the sentence\nlevel with human-labeled rhetorical strategies.\nOur method outperforms several baselines,\nuncovers persuasive strategies\u2014offering in\u0002creased interpretability of persuasive speech\u2014\nand has applications for other situations with\ndocument-level supervision but only partial\nsentence supervision.\n1 Introduction\nCrowdfunding platforms are a popular way to\nraise funds for projects. For example, Kiva, a peer\u0002to-peer lending platform, has crowd-funded more\nthan a million loans, totaling over $1 billion since\n2005. Kickstarter, another online crowdfunding\nplatform, successfully funded 110,270 projects\nwith a total of over 2 billion dollars. Yet most\nprojects still suffer from low success rates. How\ncan we help requesters craft persuasive and suc\u0002cessful pitches to convince others to take actions?\nPersuasive communication has the potential to\nshape and change people\u2019s attitudes and behaviors\n(Hovland et al., 1953), and has been widely re\u0002searched in various fields such as social psychol\u0002ogy, marketing, behavioral economics, and politi\u0002cal campaigning (Shrum et al., 2012). One of the\n\u2217 Equal contribution. This work was done when the first\ntwo authors were students at CMU.\nmost influential theories in the advertising liter\u0002ature is Chaiken\u2019s systematic-heuristic dual pro\u0002cessing theory, which suggests that people pro\u0002cess persuasive communication by evaluating the\nquality of arguments or by relying on inferential\nrules. Some such heuristic rules are commonly\nused in consumer behaviors; commercial websites\nmay highlight the limited availability of their items\n\u201cIn high demand - only 2 left on our site!\u201d or em\u0002phasize the person in authority \u201cSpeak to our head\nof sales\u2014he has over 15 years\u2019 experience sell\u0002ing properties\u201d to attract potential consumers. Al\u0002though numerous studies on persuasion have been\nconducted (Chaiken, 1980), we still know little\nabout the way how persuasion functions in the\nwild and how it can be modeled computationally.\nIn this work, we utilize neural-network based\nmethods to computationally model persuasion in\nrequests from crowdfunding websites. We build\non theoretical models of persuasion to operational\u0002ize persuasive strategies and ensure generalizabil\u0002ity. We propose to identify the persuasive strat\u0002egy employed in each sentence in each request.\nHowever, constructing a large dataset with persua\u0002sion strategies labeled at the sentence level is time\u0002consuming and expensive. Instead, we propose to\nuse a small amount of hand-labeled sentences to\u0002gether with a large number of requests automati\u0002cally labeled at the document level by the number\nof persuaded support actions. Our model is a semi\u0002supervised hierarchical neural network that iden\u0002tifies the persuasive strategies employed in each\nsentence, where the supervision comes from the\noverall persuasiveness of the request. We propose\nthat the success of requests could have substan\u0002tive explanatory power to uncover their persuasive\nstrategies. We also introduce an annotated cor\u0002pus with sentence-level persuasion strategy labels\nand document-level persuasiveness labels, to fa\u0002cilitate future work on persuasion. Experiments\n3621\nshow that our semi-supervised model outperforms\nseveral baselines. We then apply this automated\nmodel to unseen requests from different domains\nand obtain nuanced findings of the importance of\ndifferent strategies on persuasion success. Our\nmodel can be useful in any situation in which we\nhave exogenous document-level supervision, but\nonly small amounts of expensive human-annotated\nsentence labels.\n2 Related Work\nComputational argumentation has received much\nrecent attention (Ghosh et al., 2016; Stab and\nGurevych, 2017; Peldszus and Stede, 2013; Stab\net al., 2018; Ghosh et al., 2014). Most work has ei\u0002ther identified the arguments in news articles (Sar\u0002dianos et al., 2015) or user-generated web con\u0002tent (Habernal and Gurevych, 2017; Musi et al.,\n2018), or classified argument components (Zhang\nand Litman, 2015) into claims and premises, sup\u0002porting and opposing claims, or backings, re\u0002buttals and refutations . For example, Stab\nand Gurevych (2014) proposed structural, lex\u0002ical, syntactic and contextual features to iden\u0002tify convincing components of Web arguments in\u0002cluding claim, major claim, and premise. Sim\u0002ilarly, Zhang and Litman (2015) studied stu\u0002dent essay revisions and classified a set of ar\u0002gumentative actions associated with successful\nwriting such as warrant/reasoning/backing, rebut\u0002tal/reservation, and claims/ideas. Habernal and\nGurevych (2016) investigated the persuasiveness\nof arguments in any given argument pair using\nbidirectional LSTM. Hidey et al., (2017) utilized\nthe persuasive modes\u2014ethos, logos, pathos\u2014to\nmodel premises and the semantic types of argu\u0002ment components in an online persuasive forum.\nWhile most computational argumentation fo\u0002cuses on the relational support structures and fac\u0002tual evidence to make claims, persuasion focuses\nmore on language cues aimed at shaping, rein\u0002forcing and changing people\u2019s opinions and be\u0002liefs. How language changes people\u2019s attitudes\nand behaviors have received less attention from\nthe computational community than argumentation,\nalthough there have been important preliminary\nwork (Persing and Ng, 2017; Carlile et al., 2018).\nFarra et al., (2015) built regression models to pre\u0002dict essay scores based on features extracted from\nopinion expressions and topical elements. Chat\u0002terjee et al., (2014) used verbal descriptors and\npara-verbal markers of hesitation to predict speak\u0002ers\u2019 persuasiveness on website housing videos of\nproduct reviews. When looking at persuasion in\nthe context of online forum discussions (Wei et al.,\n2016), Tan et al., (2016) found that on the Change\nMy View subreddit, interaction dynamics such as\nthe language interplay between opinion holders\nand other participants provides highly predictive\ncues for persuasiveness. Using the same dataset,\nWel et al., (2016) extracted a set of textual infor\u0002mation and social interaction features to identify\npersuasive posts.\nRecently, Pryzant et al., (2017) introduced a\nneural network with an adversarial objective to\nselect text features that are predictive of some\noutcomes but decorrelated with others and fur\u0002ther analyzed the narratives highlighted by such\ntext features. Further work extended the model\nto induce narrative persuasion lexicons predictive\nof enrollment from course descriptions and sales\nfrom product descriptions (Pryzant et al., 2018a),\nand the efficacy of search advertisements (Pryzant\net al., 2018b). Similar to their settings, we use\nthe outcomes of a persuasive description to su\u0002pervise the learning of persuasion tactics, and our\nmodel can similarly induce lexicons associated\nwith successful narrative persuasion by examining\nhighly attentional words associated with persua\u0002sion outcomes. Our work differs both in our semi\u0002supervised method and also because...",
      "url": "https://aclanthology.org/N19-1364.pdf"
    },
    {
      "title": "",
      "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 151\u2013156\nBrussels, Belgium, October 31\u2013November 4, 2018. \rc 2018 Association for Computational Linguistics\n151\nPizzaPal: Conversational Pizza Ordering using a High-Density\nConversational AI Platform\nAntoine Raux, Yi Ma, Paul Yang, and Felicia Wong\nb4.ai / botbotbotbot, Inc.\n3225 Ash Street\nPalo Alto, CA, USA\n{antoine,yi,paul,felicia}@b4.ai\nAbstract\nThis paper describes PizzaPal, a voice-only\nagent for ordering pizza, as well as the Conver\u0002sational AI architecture built at b4.ai. Based\non the principles of high-density conversa\u0002tional AI, it supports natural and flexible in\u0002teractions through neural conversational lan\u0002guage understanding, robust dialog state track\u0002ing, and hierarchical task decomposition.\n1 High-Density Conversational AI\nFollowing the recent rise to prominence of smart\nspeakers, as well as the continuous improvement\nof core technologies for speech recognition and\nnatural language understanding, voice-only inter\u0002active applications, whether in the home or in car,\nhave attracted increasing attention and investment\nfrom the industry. Such applications generally fall\nunder two broad categories: assistants and bots.\nVoice assistants, pioneered by Siri in 2010, aim at\nproviding a broad range of information and ser\u0002vices across many domains, primarily by lever\u0002aging natural language\u2019s evocative power, i.e. its\nability to summon any intent, concept or entity at\nany point in a conversation. On the other hand,\nbots (also known as skills on Alexa and action on\nGoogle Assistant) are much narrower in scope, of\u0002ten providing a voice interface to a single brand,\nservice, or API.\nWhile technological progresses are undeniable,\nthese applications have only met limited suc\u0002cess1\n, and largely fail to sustain even simple task\u0002oriented conversations with humans. We believe\nthis relatively poor user experience stems from\nthe fact that neither assistants nor bots are able\nto cover the space of possible (or even reason\u0002able) conversations with enough density. In other\n1According to Smith (2017), the retention rate after two\nweeks for Alexa skills was only 6% in September 2017.\nwords, while a given set of user intents are rec\u0002ognized and supported, even small variations over\nthose are not properly handled. There are several\nroot causes to these limitations. While platforms\nsuch as Google\u2019s DialogFlow or Facebook\u2019s wit.ai\nprovide a simple way of building a relatively small\nset of distinct intents to a large developer commu\u0002nity, these alone cannot support truly natural, sus\u0002tained, interaction. Therefore, companies (typi\u0002cally startups) that develop bots might not have the\nnecessary resources or knowledge to build truly\ncompelling conversational experiences. On the\nother hand, some of the largest tech companies\nare behind most voice assistants (Apple, Google,\nAmazon) have plenty of resources, financial, hu\u0002man, and intellectual, but they are typically fo\u0002cused on expanding the breadth of their applica\u0002tions, to the expense of its density. Figure 1 gives\nan example of the contrast between breadth and\ndensity.\nAt b4.ai, we believe that only high-density con\u0002versational AI can deliver the seamless, natural ex\u0002perience that matches users\u2019 expectations of Ar\u0002tificial Intelligence and leads to truly successful\nconversational consumer products. The following\nsections describe how we are tackling this chal\u0002lenge by first focusing on domain-specific appli\u0002cations in the form of Alexa skills and Google Ac\u0002tions.\n2 The PizzaPal Conversational Ordering\nSystem\nThe PizzaPal system is a voice-only conversa\u0002tional agent that supports ordering pizza, drinks\nand side dishes for delivery or pickup. It runs ei\u0002ther as an Alexa skill or a Google Assistant ac\u0002tion and is connected to the Domino\u2019s Pizza API.\nWhile there exist other conversational agents (as\u0002sistants, skills or actions) that support pizza or-\n152\nFigure 1: Leveraging Natural Conversation for Breadth vs Density\ndering or similar e-commerce tasks, PizzaPal is\nunique in the naturalness and flexibility of its in\u0002teractions, as shown in the example of Figure 2.\nP: (...) What will you be having?\nU: Can I get a large spinach pizza?\nP: One large spinach pizza. Can I get you\nanything else?\nU: I\u2019ll also take a pepperoni pizza.\nP: One pepperoni pizza. Do you want\nthat in large as well?*\nU: How much was the spinach pizza?\nP: A large spinach pizza is $18.49. Let\u2019s\ncontinue with the pepperoni pizza.**\nShould I make that a large as well?\nU: Do you have anything smaller?***\nP: Medium is smaller. It is 12 inches.\nShall I order it for you?\nU: Sure.\nP: Sure. Can I get you anything else?\nFigure 2: Example dialogue with PizzaPal.2\nThe italicized sentences show instances of natu\u00022The video of this dialog is available at https://\nyoutu.be/BQrzgJk4-yI\nral conversational behavior exhibited by PizzaPal:\n\u2217 PizzaPal proactively suggests sizes and items\nbased on the dialog history when possible.\n\u2217\u2217 While the system can drive the conversation\nto completion by asking the user direct ques\u0002tions, it also allows the user to take the initia\u0002tive at any time.\n\u2217 \u2217 \u2217 The user can express constraints and in\u0002tents freely, including using contextual ex\u0002pressions.\nThese are a few examples of features character\u0002istic of high-density AI, which are implemented\nin PizzaPal. In the remainder of the paper, we\nwill describe the b4.ai framework architecture and\ncomponents that support the PizzaPal system.\n3 Architecture\n3.1 Overview\nAt a high level, the b4.ai framework is composed\nof three main services that run on top of a front\nend platform such as Amazon Alexa or Google\nAssistant. As illustrated in Figure 3, the front\nend controls application launch, voice recognition\n153\nFigure 3: Overview of the b4.ai dialog framework.\nand voice synthesis. The front end sends the tran\u0002scription of each user input to the core dialog ser\u0002vice, which first calls the NLU service to extract\nnon-contextual semantic information, and inter\u0002acts with the knowledge backend, before return\u0002ing the bot response to the front end as a string to\nbe spoken back to the user. All three services are\nimplemented as REST APIs hosted on AWS EC2\ninstances.\n3.2 Knowledge Service\nThe role of the Knowledge Service is to retrieve\ndomain knowledge (e.g. menus, restaurant ad\u0002dresses, as well as user prefererences), help re\u0002solve references (e.g. when the user says \u201dadd\nchicken to my pizza\u201d, \u201dchicken\u201d might match to\ndifferent possible toppings like \u201dBBQ Chicken\u201d,\n\u201dGarlic Chicken\u201d or \u201dTeriyaki Chicken\u201d), and ex\u0002ecute transactions with 3rd party services (e.g.\nplacing the order for delivery). Even for a sin\u0002gle domain application such as pizza ordering, this\nrequires access to a variety of databases, APIs\nand services. The Knowledge Service abstracts\naway these different source schemas and allows\nunified access from the dialog service. In gen\u0002eral, the Knowledge Service supports a variety of\nqueries, both structured (e.g. getting the menu of\na given store given its store ID) and unstructured\n(e.g. finding menu items matching certain key\u0002words and key phrases), which are performed via\nan API defined in terms of questions about enti\u0002ties and properties (e.g. \u201dget the available toppings\nfor menu item X\u201d, \u201dresolve ingredient named N\nfor dish type T\u201d). The initial implementation of\nthe Knowledge Service relies on 3rd party REST\nAPIs, and our own PostGres and ElasticSearch\ndatabases to access restaurant and menu informa\u0002tion.\n3.3 Natural Language Generation\nThe NLG service takes the semantic output gener\u0002ated by the Dialog service and converts it to natu\u0002ral language. We have implemented a simple, scal\u0002able template based approach to NLG, that allows\nto control the language used by the system with\nsome amount of variation. The templates incor\u0002porate some conditionals so that entities such as\nmenu items or ingredients can be rendered differ\u0002ently based on entity properties and context.\n4 Natural Lang...",
      "url": "https://aclanthology.org/D18-2026.pdf"
    },
    {
      "title": "Here's The Secret Formula For Getting Free Pizza, According To Science",
      "text": "[Skip to Main Content](https://www.huffpost.com/entry/free-pizza-study-stanford-study_n_5441068#main)\n\n## Main Menu\n\nU.S. Edition\n\n## Follow Us\n\n[Terms](https://www.huffpost.com/static/user-agreement)\u00a0\\|\u00a0[Privacy Policy](https://www.huffpost.com/static/privacy-policy)\n\nPart of HuffPost Science. \u00a92024 BuzzFeed, Inc. All rights reserved.\n\n\u00d7\n\n## What's Hot\n\n![](https://img.huffingtonpost.com/asset/5baebfe61f0000250123072e.jpeg?ops=scalefit_720_noupscale)\n\nEvery day, people with problems big and small seek help from social media sites. Some people seek [organ donors](http://www.nydailynews.com/life-style/health/facebook-great-find-kidney-organ-donations-matched-social-media-rise-article-1.999851). Others ask for [baby name submissions](http://theweek.com/article/index/234249/yahoos-ceonbspcrowdsources-for-a-baby-name-a-hot-new-parenting-trend). And some people are just [looking for free pizza.](http://www.reddit.com/r/Random_Acts_Of_Pizza/)\n\nThough not all these requests are indulged, often enough someone in the online community does satisfy a stranger\u2019s request -- usually without expecting anything in return.\n\nAdvertisement\n\nWhat is it that would motivate someone to, say, give free pizza to a perfect stranger? That\u2019s the fascinating question [a trio of researchers at Stanford University](http://arxiv.org/pdf/1405.3282v1.pdf) set out to answer.\n\nFor their case study, the researchers scoured requests on [Reddit\u2019s Random Acts of Pizza forum](http://www.reddit.com/r/Random_Acts_Of_Pizza/), where everyone ranging from impoverished college students to the recently dumped can plead their case in an effort to convince anonymous users that they deserve a free pizza.\n\nAnalyzing the forum's 5,738 fulfilled pizza requests, the researchers identified [three specific factors that seem to boost the chance that a request will be fulfilled:](http://www.technologyreview.com/view/527496/data-mining-reddit-posts-reveals-how-to-ask-for-a-favor-and-get-it/)\n\n**Narrative:** Requests that mentioned money problems, job struggles, and special occasions were most successful. Requests that expressed simply a craving for pizza were less successful.\n\nAdvertisement\n\n**Length:** Long narratives were more likely to be successful--presumably, according to the scientists, because extra length allowed more room to explain difficult situations.\n\n**Reciprocity:** Requests that expressed gratitude and a \"willingness to give back to the community\" were significantly more likely to be successful.\n\nHere's an example of [a successful request on Reddit](http://www.reddit.com/r/Random_Acts_Of_Pizza/comments/26z51d/request_so_guess_who_just_ended_their/). Note that it meets the criteria:\n\n> Feeling really really good right now. I've been at probably the lowest point of my life and have managed to crawl my way back into a really nice room in a shared house.\n>\n> I'm suffering pretty badly from PTSD which kinda managed to wreck my life when it caused a really bad bout of depression that sucked all life out of me and basically accelerated my own life falling apart.\n>\n> Things were looking pretty bleak, until a stray cat adopted me. Ended up falling in love with her. She's a huge sweetie.\n>\n> With the sudden mouth to feed, I managed to get a hold of my doctor and have worked out a way into recovery. I'm now on short term disability, which has just now come in. I kinda thank this cat for miraculously showing up to even help me get the willpower together to get this far.\n>\n> SADLY. All my current on hand cash went to paying my first month rent and deposit. I would really appreciate some pizza for the weekend.\n>\n> I promise to pay this forward in any way I can, once next cheque comes in.\n>\n> Edit: I forgot to say that I'm in Victoria, BC. Dominos has some -very- cheap online coupons \\[all sic\\].\n\nOn the flip side, here's a [request that failed.](http://www.reddit.com/r/Random_Acts_Of_Pizza/comments/2735c3/request_i_have_lost_faith_in_humankind_do_nice/):\n\n> I have lost faith in humankind. Do nice people still exist?\n\nIn their study, the researchers also identified another factor that seems to affect the success or failure of any particular request: Requests from Reddit users who have high seniority in the online community and who contribute to the community (such as with previous posts) tend to be more successful.\n\nThough the study covers only online generosity, it may hold lessons about altruism in the real world and perhaps for those involved in philanthropy.\n\n\u201cWe hope that practical implications are that if you understand what factors are important, maybe you set up a community in a way in which [people help each other out](http://www.fastcoexist.com/3031252/stanford-scientists-are-studying-altruism-in-the-free-pizza-subreddit) effectively,\" study co-author Tim Althoff, told Fast Company. \"What are the factors that are important that you should incorporate into your writing to be successful? You can also do that with automatic techniques, automatic tools that could that help you check your writing.\u201d\n\n## Related\n\n[Reddit](https://www.huffpost.com/impact/topic/reddit) [altruism](https://www.huffpost.com/topic/altruism) [science](https://www.huffpost.com/topic/science) [Pay It Forward](https://www.huffpost.com/topic/pay-it-forward) [random acts of kindness](https://www.huffpost.com/impact/topic/good-news)\n\n## Before You Go\n\n![Caffeine For Imprisoned Twins](https://img.huffingtonpost.com/asset/5baeab1d200000e500ff5a9f.jpeg?ops=scalefit_960_noupscale)\n\n## The Most Bizarre Scientific Experiments\n\nSee Gallery\n\n[Go to Homepage](https://www.huffpost.com)\n\nLEAVE A COMMENT\n\nSuggest a correction\n\n\\|\n\n[Submit a tip](https://img.huffingtonpost.com/securedrop)\n\n## Popular in the Community\n\n## From Our Partner\n\nClose\n\n## What's Hot\n\n## More In Science",
      "url": "https://www.huffpost.com/entry/free-pizza-study-stanford-study_n_5441068"
    },
    {
      "title": "Computer Science > Computation and Language",
      "text": "[1405.3282] How to Ask for a Favor: A Case Study on the Success of Altruistic Requests\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1405.3282\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1405.3282**(cs)\n[Submitted on 13 May 2014]\n# Title:How to Ask for a Favor: A Case Study on the Success of Altruistic Requests\nAuthors:[Tim Althoff](https://arxiv.org/search/cs?searchtype=author&amp;query=Althoff,+T),[Cristian Danescu-Niculescu-Mizil](https://arxiv.org/search/cs?searchtype=author&amp;query=Danescu-Niculescu-Mizil,+C),[Dan Jurafsky](https://arxiv.org/search/cs?searchtype=author&amp;query=Jurafsky,+D)\nView a PDF of the paper titled How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, by Tim Althoff and 2 other authors\n[View PDF](https://arxiv.org/pdf/1405.3282)> > Abstract:\n> Requests are at the core of many social media systems such as question &amp; answer sites and online philanthropy communities. While the success of such requests is critical to the success of the community, the factors that lead community members to satisfy a request are largely unknown. Success of a request depends on factors like who is asking, how they are asking, when are they asking, and most critically what is being requested, ranging from small favors to substantial monetary donations. We present a case study of altruistic requests in an online community where all requests ask for the very same contribution and do not offer anything tangible in return, allowing us to disentangle what is requested from textual and social factors. Drawing from social psychology literature, we extract high-level social features from text that operationalize social relations between recipient and donor and demonstrate that these extracted relations are predictive of success. More specifically, we find that clearly communicating need through the narrative is essential and that that linguistic indications of gratitude, evidentiality, and generalized reciprocity, as well as high status of the asker further increase the likelihood of success. Building on this understanding, we develop a model that can predict the success of unseen requests, significantly improving over several baselines. We link these findings to research in psychology on helping behavior, providing a basis for further analysis of success in social media systems. Comments:|To appear at ICWSM 2014. 10pp, 3 fig. Data and other info available at[this http URL](http://www.mpi-sws.org/~cristian/How_to_Ask_for_a_Favor.html)|\nSubjects:|Computation and Language (cs.CL); Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)|\nACMclasses:|I.2.7; J.4|\nCite as:|[arXiv:1405.3282](https://arxiv.org/abs/1405.3282)[cs.CL]|\n|(or[arXiv:1405.3282v1](https://arxiv.org/abs/1405.3282v1)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1405.3282](https://doi.org/10.48550/arXiv.1405.3282)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Cristian Danescu-Niculescu-Mizil [[view email](https://arxiv.org/show-email/a9d82295/1405.3282)]\n**[v1]**Tue, 13 May 2014 20:00:07 UTC (115 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, by Tim Althoff and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1405.3282)\n* [TeX Source](https://arxiv.org/src/1405.3282)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1405.3282&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1405.3282&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2014-05](https://arxiv.org/list/cs.CL/2014-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/1405.3282?context=cs)\n[cs.SI](https://arxiv.org/abs/1405.3282?context=cs.SI)\n[physics](https://arxiv.org/abs/1405.3282?context=physics)\n[physics.soc-ph](https://arxiv.org/abs/1405.3282?context=physics.soc-ph)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1405.3282)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1405.3282)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1405.3282)\n### [2 blog links](https://arxiv.org/tb/1405.3282)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1405.html#AlthoffDJ14)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/AlthoffDJ14)\n[Tim Althoff](<https://dblp.uni-trier.de/search/author?author=Tim Althoff>)\n[Cristian Danescu-Niculescu-Mizil](<https://dblp.uni-trier.de/search/author?author=Cristian Danescu-Niculescu-Mizil>)\n[Dan Jurafsky](<https://dblp.uni-trier.de/search/author?author=Dan Jurafsky>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/1405.3282&amp;description=How to Ask for a Favor: A Case Study on the Success of Altruistic Requests>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/1405.3282&amp;title=How to Ask for a Favor: A Case Study on the Success of Altruistic Requests>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https...",
      "url": "https://arxiv.org/abs/1405.3282?_gclid=5b80922acd6570.83189470-5b80922acd6623.75529174&_utm_source=xakep&_utm_campaign=mention55263&_utm_medium=inline&_utm_content=lnk139087023660"
    },
    {
      "title": "Researchers reveal how to score free pizza",
      "text": "[Skip to main content](https://www.thrillist.com/eat/nation/stanford-study-analyzes-random-acts-of-pizza#main-content)\n\n- [Eat](https://www.thrillist.com/eat)\n- [Food And Drink](https://www.thrillist.com/topics/food-and-drink)\n\n# Researchers reveal how to score free pizza\n\nBy [Kristin Hunt](https://www.thrillist.com/authors/kristin-hunt)\n\nPublished on June 5, 2014 at 11:00 AM\n\n![Pepperoni pizza](https://assets3.thrillist.com/v1/image/1250187/414x310/crop;webp=auto;jpeg_quality=60;progressive.jpg)[Andrew Zimmer](https://www.thrillist.com/authors/andrew-zimmer)[Andrew Zimmer](https://www.thrillist.com/authors/andrew-zimmer)\n\nIf you missed your shot to score [free Dominos on Josh Beckett](https://www.thrillist.com/eat/nation/dominono-free-domino-s-pizza-for-mlb-no-hitters), fear not. The lab rats at Stanford have put together a real, published study on what gets people free pizza. Now, keep in mind this is **a hyper-specific [case study](http://www.stanford.edu/~jurafsky/pubs/icwsm2014_pizza.pdf)** on the Reddit mainstay [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/), where people submit pleas for free pizza to the Reddit community... and sometimes score. But it tells us about how to successfully grovel for pies in general, so listen up:\n\n- **Be grateful**. The team looked at 19 different \"politeness features\" and found that only gratitude -- not \"pleeeease Mom\" -- had a big impact.\n- **Back up your story**. People were more likely to donate pizza if there was evidence backing up the person's tale. So if you're telling people you need free pizza because your car is busted and you need to pay for repairs, show them a photo of your bashed-up Fiero.\n- **Offer to return the favor**. Statistically speaking, Redditors tend to give to peeps who sound like they're gonna pay back the pizza karma.\n- **Ramble**. This is one case where long-winded stories are a good idea. Longer posts meant more pizza, so go ahead and ignore your word count.\n- **Don't just say you're hungry**. Make a real case for yourself: the study found that people who hinged their pleas on a pizza-craving got shut down.\n\nNow that you're wise to how the system works, go forth and offer your pitch to the board.\n\n_**[Kristin Hunt](https://www.thrillist.com/authors/kristin-hunt)** is a food/drink staff writer for Thrillist and is always on the hunt for free pizza. Follow her at [@kristin\\_hunt](https://twitter.com/kristin_hunt)._",
      "url": "https://www.thrillist.com/eat/nation/stanford-study-analyzes-random-acts-of-pizza"
    },
    {
      "title": "CS224U: Natural Language Understanding",
      "text": "CS224U: Natural Language Understanding - Spring 2023\n# [CS224U: Natural Language Understanding](index.html)\n### [Course info](#info)\n**CS224u can be taken entirely online and asynchronously. Our class meetings will be recorded, and the core content will also be delivered via slides, videos, and Python notebooks.**\nMW 3:00&ndash;4:20 pm,[Gates B1](http://campus-map.stanford.edu/?srch=Gates+B1)\n[Discussion](https://edstem.org/us/courses/38101/discussion/)\n[Canvas](https://canvas.stanford.edu/courses/170806)\n[Gradescope](https://www.gradescope.com/courses/524125)\n[**Github](https://github.com/cgpotts/cs224u/)\nStaff email:\nCloud credits and computing support for student projects generously provided by[AWS Educate](https://aws.amazon.com/education/awseducate/)\n### [Teaching team](#)\n[![Chris](images/ChrisPotts.jpg)](http://web.stanford.edu/~cgpotts/)\n#### [Christopher Potts](http://web.stanford.edu/~cgpotts/)\n[![Kawin](https://kawine.github.io/assets/new_profile.jpeg)](https://kawine.github.io/)\n#### [Kawin Ethayarajh](https://kawine.github.io/)\n[![Sidd](https://www.siddkaramcheti.com/assets/img/sidd.jpg)](https://www.siddkaramcheti.com/)\n#### [Sidd Karamcheti](https://www.siddkaramcheti.com/)\n[![Mina](https://brushbrushbrushyourteeth.files.wordpress.com/2022/07/md_25-2.jpg)](https://brushbrushbrushyourteeth.files.wordpress.com/)\n#### [Mina Lee](https://brushbrushbrushyourteeth.files.wordpress.com/)\n[![Siyan](https://siyan-sylvia-li.com/img/avatar.jpg)](https://siyan-sylvia-li.com/)\n#### [Siyan Li](https://siyan-sylvia-li.com/)\n[![Lisa](https://xiangli1999.github.io/img/lisa.jpg)](https://xiangli1999.github.io/)\n#### [Xiang (Lisa) Li](https://xiangli1999.github.io/)\n[![Tol\u00falope](https://tolulope.carrd.co/assets/images/image01.jpg)](https://tolulope.carrd.co/)\n#### [Tol\u00falope \u00d2g\u00fanrem\u00ed](https://tolulope.carrd.co/)\n[![Tianyi](https://tiiiger.github.io/images/headshot_new.jpeg)](https://tiiiger.github.io/)\n#### [Tianyi Zhang](https://tiiiger.github.io/)\n### [Previous years](#previous)\n* [2022](2022/)\n* [2021](2021/)\n* [2020](2020/)\n* [2019](2019/)\n* [2018](2018/)\n* [2016](2016/)\n* [2015](2015/)\n* [2014](2014/)\n* [2013](2013/)\n* [2012](2012/)\n## [Schedule](#)\n||Materials|Resources|Assignments|\nApr3|\n1. [Course introduction and overview](slides/cs224u-intro-2023-handout.pdf)\n2. Notebook:[Course set-up](https://github.com/cgpotts/cs224u/blob/main/setup.ipynb)\n3. [Optional background materials](background.html)|\n1. [Levesque 2013](http://www.cs.toronto.edu/~hector/Papers/ijcai-13-paper.pdf)\n2. [Manning 2015](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239)\n3. [Philosophy of understanding](https://crfm.stanford.edu/assets/report.pdf#philosophy)(&sect;2.6 of the &quot;Foundation Models&quot; report)\n4. [Artificial Intelligence: Last Week Tonight with John Oliver](https://youtu.be/Sqa8Zo2XWc4)||\n|Domain adaptation for supervised sentiment|\nApr5|\n1. Overview of Assign/bakeoff1\n2. [Contextual word representations](slides/cs224u-contextualreps-2023-handout.pdf)\n3. [Diffusion objectives for text](slides/lisa-224u-diffusion.pdf)[Lisa]\n4. [Fantastic language models and how to build them](slides/sidd-fantastic-lms-cs224u.pdf)[Sidd]|\n1. SST:[Socher et al. 2013](https://aclanthology.org/D13-1170/)\n2. DynaSent:[Potts, Wu et al. 2020](https://aclanthology.org/2021.acl-long.186/)\n3. [Smith 2020](https://arxiv.org/abs/1902.06006)\n4. The Pile:[Gao et al. 2020](https://arxiv.org/abs/2101.00027)\n5. Transformer:[Vaswani et al. 2017](https://papers.nips.cc/paper/7181-attention-is-all-you-need)\n6. [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n7. Relative positional encoding:[Shaw et al. 2018](https://arxiv.org/abs/1803.02155)\n8. GPT:[Radford et al. 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n9. BERT:[Devlin et al. 2018](https://www.aclweb.org/anthology/N19-1423/)\n10. RoBERTa:[Liu et al. 2019](https://arxiv.org/abs/1907.11692)\n11. ELECTRA:[Clark et al. 2019](https://openreview.net/pdf?id=r1xMH1BtvB)\n12. T5:[Raffel et al. 2019](https://arxiv.org/abs/1910.10683)\n13. [BART: Lewis et al. 2020](https://aclanthology.org/2020.acl-main.703/)\n14. DistilBERT:[Sanh et al. 2019](https://arxiv.org/abs/1910.01108)\n15. Diffusion-LM:[Li et al. 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/1be5bc25d50895ee656b8c2d9eb89d6a-Abstract-Conference.html)|\n1. [Assign/bakeoff1](https://github.com/cgpotts/cs224u/blob/main/hw_sentiment.ipynb): due Apr17, 3:00pm Pacific\n2. [Quiz 0](https://canvas.stanford.edu/courses/170806/quizzes/133191): due Apr17, 3:00pm Pacific\n3. [Quiz 1](https://canvas.stanford.edu/courses/170806/quizzes/133506): due Apr17, 3:00pm Pacific|\nApr10|\nApr12|\n|Retrieval augmented in-context learning|\nApr17|\n1. [Overview of Assign/bakeoff2](slides/cs224u-hw2-overview-2023.pdf)\n2. [Information retrieval](slides/cs224u-neuralir-2023-handout.pdf)\n3. [In-context learning](slides/cs224u-incontextlearning-2023-handout.pdf)\n4. [Prompters before prompts and promptees](https://drive.google.com/file/d/1RIOAOTOOPyVLezFiIfGnYJSE8ofKuR4L/view)[Mina]|\n1. [Khattab et al. 2021](https://ai.stanford.edu/blog/retrieval-based-NLP/)\n2. ColBERT:[Khattab and Zaharia 2020](https://arxiv.org/abs/2004.12832)\n3. DPR:[Karpukhin et al. 2020](https://aclanthology.org/2020.emnlp-main.550/)\n4. SPLADE:[Formal et al. 2021](https://dl.acm.org/doi/10.1145/3404835.3463098)\n5. RAG:[Lewis et al. 2020](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)\n6. GPT-3:[Brown et al. 2020](https://arxiv.org/abs/2005.14165)\n7. RLHF for LLMs:[Ouyang et al. 2022](https://arxiv.org/abs/2203.02155)\n8. CoT:[Wei et al. 2022](https://arxiv.org/abs/2201.11903)\n9. Retrive-then-read:[Lazaridou et al. 2022](https://arxiv.org/abs/2203.05115)\n10. DSP:[Khattab et al. 2022](https://arxiv.org/abs/2212.14024)|\n1. [Assign/bakeoff2](https://github.com/cgpotts/cs224u/blob/main/hw_openqa.ipynb): due Apr26, 3:00pm Pacific\n2. [Quiz 2](https://canvas.stanford.edu/courses/170806/quizzes/134114): due Apr26, 3:00pm Pacific|\nApr19|\nApr24|\n|Advanced behavioral evaluation|\nApr26|\n1. [Overview of Assign/bakeoff3](slides/cs224u-hw3-overview-2023.pdf)\n2. [Advanced behavioral evaluation of NLU models](slides/cs224u-behavioraleval-2023-handout.pdf)|\n1. [Jia and Liang 2017](http://aclweb.org/anthology/D17-1215)\n2. [Glockner et al. 2018](https://www.aclweb.org/anthology/P18-2103)\n3. [Liu et al. 2019](https://www.aclweb.org/anthology/N19-1225)\n4. [Naik et al. 2019](https://www.aclweb.org/anthology/C18-1198)\n5. ANLI:[Nie et al. 2020](https://aclanthology.org/2020.acl-main.441/)\n6. Dynabench:[Kiela et al. 2021](https://aclanthology.org/2021.naacl-main.324/)\n7. COGS:[Kim and Linzen 2020](https://aclanthology.org/2020.emnlp-main.731/)\n8. ReCOGS:[Wu et al. 2023](https://arxiv.org/abs/2303.13716)|\n1. [Assign/bakeoff 3](https://github.com/cgpotts/cs224u/blob/main/hw_recogs.ipynb)due May8, 3:00pm Pacific\n2. [Quiz 3](https://canvas.stanford.edu/courses/170806/quizzes/134298): due May8, 3:00pm Pacific|\nMay1|\nMay3|\n|Analysis methods|\nMay8|\n1. [Lit review overview](slides/cs224u-litreview-overview-2023.pdf)\n2. [Siyan: What I did for 224u final project](slides/siyan-projects-cs224u.pdf)\n3. [Course review in the form of a Jeopardy! game](https://jeopardylabs.com/play/nlu-jeopardy-4)\n4. [Analysis methods for NLU](slides/cs224u-analysis-2023-handout.pdf)|\n1. LIME:[Ribeiro et al. 2016](https://dl.acm.org/doi/10.1145/2939672.2939778)\n2. Probing:[Tenney et al. 2018](https://aclanthology.org/P19-1452/)\n3. IG:[Sundararajan et al. 2017](http://proceedings.mlr.press/v70/sundararajan17a.html)\n4. Causal abstraction:[Geiger et al. 2022](https://ai.stanford.edu/blog/causal-abstraction/)\n5. IIT:[Geiger, Wu, et al. 2022](https://proceedings.mlr.press/v162/geiger22a.html)\n6. DAS:[Geiger, Wu, et al. 2023](https://arxiv.org/abs/2303.02536)\n7. Circuits:[Cammarata et al. 2020](https://distill.pub/2020/circuits/)|\n1. [Lit review](projects.html#litrevie...",
      "url": "https://web.stanford.edu/class/cs224u"
    }
  ]
}