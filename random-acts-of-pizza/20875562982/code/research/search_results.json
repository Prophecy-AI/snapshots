{
  "query": "How do Kaggle winners ensemble models for text classification competitions? What stacking strategies work best for combining text features with tabular metadata?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the LSHTC4 winning solution (which used a Feature\u2011Weighted Linear Stacking ensemble for large\u2011scale text classification) and recent Kaggle\u2011level tutorials on stacking, here\u2019s a concise workflow that top competitors use to blend pure\u2011text models with tabular metadata\u202f([arXiv\u202f1405.0546](https://arxiv.org/abs/1405.0546);\u202f[Kaggler\u202fTV\u202fBlog\u202f2021](https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html)):\n\n1. **Prepare parallel feature streams**  \n   * **Text** \u2013 compute sparse TF\u2011IDF/BM25 vectors *or* dense embeddings (e.g., Sentence\u2011BERT, fastText).  \n   * **Metadata** \u2013 keep the original numeric/categorical columns and optionally engineer interaction features.  \n\n2. **Train diverse Level\u20111 base models on each stream**  \n   * Text\u2011only: Multinomial Na\u00efve Bayes, LightGBM on TF\u2011IDF, a fine\u2011tuned transformer.  \n   * Tabular\u2011only: CatBoost, LightGBM, Random Forest.  \n   * Optionally include hybrid models that ingest concatenated text embeddings\u202f+\u202fmetadata.  \n   Use **out\u2011of\u2011fold (OOF) predictions** for every model so the meta\u2011learner sees unbiased estimates\u202f([Kaggler\u202f2021](https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html)).\n\n3. **Create the stacking matrix**  \n   Stack all OOF prediction columns (probability vectors for each class) side\u2011by\u2011side; this becomes the new feature set for the meta\u2011model.  \n\n4. **Choose a meta\u2011learner that respects the classification objective**  \n   * **Linear meta\u2011models** \u2013 Logistic Regression or Ridge\u2011regularized linear regression work well and are fast.  \n   * **Feature\u2011Weighted Linear Stacking (FWLS)** \u2013 weights each base\u2011model\u2019s contribution per class, as used in the LSHTC4 solution, and directly optimizes macro\u2011F\u2011score\u202f([arXiv\u202f1405.0546](https://arxiv.org/abs/1405.0546)).  \n   * **Non\u2011linear meta\u2011models** \u2013 Gradient\u2011boosted trees or shallow neural nets can capture interactions; KDnuggets notes their effectiveness for stacking\u202f([KDnuggets\u202f2015\u2011p3](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html)).  \n\n5. **Train the meta\u2011model on the full OOF matrix**  \n   Fit the chosen meta\u2011learner using the same cross\u2011validation splits that generated the OOF predictions, then predict on the test set by feeding the **test\u2011time base\u2011model predictions** (generated without leakage).  \n\n6. **(Optional) Multi\u2011level stacking for GPU\u2011accelerated pipelines**  \n   If you have many base models, wrap them in a three\u2011level stack (base \u2192 first\u2011level meta \u2192 second\u2011level meta) using cuML\u2019s GPU\u2011powered stacking to reduce training time while preserving accuracy\u202f([NVIDIA\u202f2025](https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml)).  \n\n7. **Post\u2011process and blend**  \n   Apply a small weight\u2011averaging or rank\u2011based blend between the final stacked predictions and any high\u2011performing single model (e.g., a large transformer) to squeeze out the last few leaderboard points.  \n\n**Key take\u2011aways:**  \n* Use **OOF predictions** to avoid leakage.  \n* **Feature\u2011Weighted Linear Stacking** is especially effective for text\u2011heavy tasks because it can assign different importance to each base model per label.  \n* Combine **linear meta\u2011learners** (fast, robust) with **non\u2011linear alternatives** when you suspect complex interactions between text and metadata.  \n\nFollowing these steps mirrors the strategies that Kaggle winners have documented for text classification contests and provides a solid, reproducible stacking pipeline for mixing text and tabular features.",
      "url": ""
    },
    {
      "title": "Stacking Ensemble",
      "text": "Stacking Ensemble | Kaggler TV Blog\n[Kaggler TV Blog](https://kaggler.com/)\n# Stacking Ensemble\nA tutorial of stacking ensemble (a.k.a. stacked generalization)\nApr 26, 2021\u20225 min read\n**</i>[notebook](https://kaggler.com/categories/#notebook)[kaggle](https://kaggler.com/categories/#kaggle)\n[![View On GitHub](https://kaggler.com/assets/badges/github.svg)](https://github.com/kaggler-tv/blog/tree/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Binder](https://kaggler.com/assets/badges/binder.svg)](https://mybinder.org/v2/gh/kaggler-tv/blog/master?filepath=_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Colab](https://kaggler.com/assets/badges/colab.svg)](https://colab.research.google.com/github/kaggler-tv/blog/blob/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n* [Part 1: Data Loading &amp; Feature Engineering](#Part-1:-Data-Loading-&-Feature-Engineering)\n* [Part 2: Level-1 Base Model Training](#Part-2:-Level-1-Base-Model-Training)\n* [Part 3: Level-2 Stacking](#Part-3:-Level-2-Stacking)\nThis notebook was originally published[here](https://www.kaggle.com/jeongyoonlee/stacking-ensemble)at Kaggle.\nThis notebook shows how to perform stacking ensemble (a.k.a. stacked generalization).\nIn[Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking), @remekkinas shares how to do stacking ensemble using`MLExtend'`s`StackingCVClassifier`.\nTo demonstrate how stacking works, this notebook shows how to prepare the baseline model predictions using cross-validation (CV), then use them for level-2 stacking. It trains four classifiers, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost as level-1 base models. It also uses CV predictions of two models, LightGBM with DAE features and supervised DAE trained from my previous notebook,[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)to show why keeping CV predictions for**every**model is important. :)\nThe contents of this notebook are as follows:\n1. **Feature Engineering**: Same as in the[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)and[AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb).\n2. **Level-1 Base Model Training**: Training four base models, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost using the same 5-fold CV.\n3. **Level-2 Stacking**: Training the LightGBM model with CV predictions of base models, original features, and DAE features. Performing feature selection and hyperparameter optimization using`Kaggler`'s`AutoLGB`.\nThis notebook is inspired and/or based on other Kagglers' notebooks as follows:\n* [TPS-APR21-EDA+MODEL](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model)by @udbhavpangotra\n* [Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking)by @remekkinas\n* [TPS Apr 2021 pseudo labeling/voting ensemble](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble?scriptVersionId=60616606)by @hiro5299834\nThanks!\n# [](#Part-1:-Data-Loading-&amp;-Feature-Engineering)Part 1: Data Loading &amp; Feature Engineering[](#Part-1:-Data-Loading-&amp;-Feature-Engineering)\n```\nfromcatboostimportCatBoostClassifierfromjoblibimportdumpimportlightgbmaslgbfromlightgbmimportLGBMClassifierfrommatplotlibimportpyplotaspltimportnumpyasnpimportpandasaspdfrompathlibimportPathfromsklearn.ensembleimportRandomForestClassifierfromsklearn.ensembleimportExtraTreesClassifierfromsklearn.metricsimportroc\\_auc\\_score,confusion\\_matrixfromsklearn.model\\_selectionimportStratifiedKFoldfromsklearn.preprocessingimportStandardScalerimportwarnings\n```\n```\n!pip install kaggler\n```\n```\nimportkagglerfromkaggler.modelimportAutoLGBfromkaggler.preprocessingimportLabelEncoderprint(f'Kaggler:{kaggler.\\_\\_version\\_\\_}')\n```\n```\nwarnings.simplefilter('ignore')pd.set\\_option('max\\_columns',100)\n```\n```\ndata\\_dir=Path('/kaggle/input/tabular-playground-series-apr-2021/')trn\\_file=data\\_dir/'train.csv'tst\\_file=data\\_dir/'test.csv'sample\\_file=data\\_dir/'sample\\_submission.csv'pseudo\\_label\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/tps04-sub-006.csv'dae\\_feature\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/dae.csv'lgb\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.val.txt'lgb\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.tst.txt'sdae\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.val.txt'sdae\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.tst.txt'target\\_col='Survived'id\\_col='PassengerId'feature\\_name='dae'algo\\_name='esb'model\\_name=f'{algo\\_name}\\_{feature\\_name}'feature\\_file=f'{feature\\_name}.csv'predict\\_val\\_file=f'{model\\_name}.val.txt'predict\\_tst\\_file=f'{model\\_name}.tst.txt'submission\\_file=f'{model\\_name}.sub.csv'\n```\n```\nn\\_fold=5seed=42n\\_est=1000encoding\\_dim=128\n```\n```\ntrn=pd.read\\_csv(trn\\_file,index\\_col=id\\_col)tst=pd.read\\_csv(tst\\_file,index\\_col=id\\_col)sub=pd.read\\_csv(sample\\_file,index\\_col=id\\_col)pseudo\\_label=pd.read\\_csv(pseudo\\_label\\_file,index\\_col=id\\_col)dae\\_features=np.loadtxt(dae\\_feature\\_file,delimiter=',')lgb\\_dae\\_predict\\_val=np.loadtxt(lgb\\_dae\\_predict\\_val\\_file)lgb\\_dae\\_predict\\_tst=np.loadtxt(lgb\\_dae\\_predict\\_tst\\_file)sdae\\_dae\\_predict\\_val=np.loadtxt(sdae\\_dae\\_predict\\_val\\_file)sdae\\_dae\\_predict\\_tst=np.loadtxt(sdae\\_dae\\_predict\\_tst\\_file)print(trn.shape,tst.shape,sub.shape,pseudo\\_label.shape,dae\\_features.shape)print(lgb\\_dae\\_predict\\_val.shape,lgb\\_dae\\_predict\\_tst.shape)print(sdae\\_dae\\_predict\\_val.shape,sdae\\_dae\\_predict\\_tst.shape)\n```\n```\ntst[target\\_col]=pseudo\\_label[target\\_col]n\\_trn=trn.shape[0]df=pd.concat([trn,tst],axis=0)df.head()\n```\nLoading 128 DAE features generated from[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder/).\n```\ndf\\_dae=pd.DataFrame(dae\\_features,columns=[f'enc\\_{x}'forxinrange(encoding\\_dim)])print(df\\_dae.shape)df\\_dae.head()\n```\nFeature engineering using @udbhavpangotra's[code](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model).\n```\ndf['Embarked']=df['Embarked'].fillna('No')df['Cabin']=df['Cabin'].fillna('\\_')df['CabinType']=df['Cabin'].apply(lambdax:x[0])df.Ticket=df.Ticket.map(lambdax:str(x).split()[0]iflen(str(x).split())&gt;1else'X')df['Age'].fillna(round(df['Age'].median()),inplace=True,)df['Age']=df['Age'].apply(round).astype(int)# Fare, fillna with mean valuefare\\_map=df[['Fare','Pclass']].dropna().groupby('Pclass').median().to\\_dict()df['Fare']=df['Fare'].fillna(df['Pclass'].map(fare\\_map['Fare']))df['FirstName']=df['Name'].str.split(', ').str[0]df['SecondName']=df['Name'].str.split(', ').str[1]df['n']=1gb=df.groupby('FirstName')df\\_names=gb['n'].sum()df['SameFirstName']=df['FirstName'].apply(lambdax:df\\_names[x]).fillna(1)gb=df.groupby('SecondName')df\\_names=gb['n'].sum()df['SameSecondName']=df['SecondName'].apply(lambdax:df\\_names[x]).fillna(1)df['Sex']=(df['Sex']=='male').astype(int)df['FamilySize']=df.SibSp+df.Parch+1feature\\_cols=['Pclass','Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName','SameSecondName','Sex','FamilySize','FirstName','SecondName']cat\\_cols=['Pclass','Embarked','CabinType','Ticket','FirstName','SecondName']num\\_cols=[xforxinfeature\\_colsifxnotincat\\_cols]print(len(feature\\_cols),len(cat\\_cols),len(num\\_cols))\n```\nApplying`log2(1 + x)`for numerical features and label-encoding categorical features using`kaggler.preprocessing.LabelEncoder`, which handles`NaN`s and groups rare categories together.\n```\nforcolin['SameFirstName','SameSecondName','Fare','FamilySize','Parch','SibSp']:df[col]=np.log2(1+df[col])scaler=StandardScaler()df[num\\_cols]=...",
      "url": "https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html"
    },
    {
      "title": "Kaggle LSHTC4 Winning Solution",
      "text": "# Computer Science > Artificial Intelligence\n\n**arXiv:1405.0546** (cs)\n\n\\[Submitted on 3 May 2014 ( [v1](https://arxiv.org/abs/1405.0546v1)), last revised 9 May 2014 (this version, v2)\\]\n\n# Title:Kaggle LSHTC4 Winning Solution\n\nAuthors: [Antti Puurula](https://arxiv.org/search/cs?searchtype=author&query=Puurula,+A), [Jesse Read](https://arxiv.org/search/cs?searchtype=author&query=Read,+J), [Albert Bifet](https://arxiv.org/search/cs?searchtype=author&query=Bifet,+A)\n\nView a PDF of the paper titled Kaggle LSHTC4 Winning Solution, by Antti Puurula and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1405.0546)\n\n> Abstract:Our winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores. This document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package. A package omitting precomputed result files is also distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.\n\n|     |     |\n| --- | --- |\n| Comments: | Kaggle LSHTC winning solution description |\n| Subjects: | Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR) |\n| Cite as: | [arXiv:1405.0546](https://arxiv.org/abs/1405.0546) \\[cs.AI\\] |\n|  | (or [arXiv:1405.0546v2](https://arxiv.org/abs/1405.0546v2) \\[cs.AI\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.1405.0546](https://doi.org/10.48550/arXiv.1405.0546)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Antti Puurula \\[ [view email](https://arxiv.org/show-email/ce50eba9/1405.0546)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1405.0546v1)**\nSat, 3 May 2014 01:41:27 UTC (243 KB)\n\n**\\[v2\\]**\nFri, 9 May 2014 04:57:19 UTC (243 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Kaggle LSHTC4 Winning Solution, by Antti Puurula and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1405.0546)\n- [TeX Source](https://arxiv.org/src/1405.0546)\n- [Other Formats](https://arxiv.org/format/1405.0546)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.AI\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1405.0546&function=prev&context=cs.AI)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1405.0546&function=next&context=cs.AI)\n\n[new](https://arxiv.org/list/cs.AI/new) \\| [recent](https://arxiv.org/list/cs.AI/recent) \\| [2014-05](https://arxiv.org/list/cs.AI/2014-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1405.0546?context=cs)\n\n[cs.CL](https://arxiv.org/abs/1405.0546?context=cs.CL)\n\n[cs.IR](https://arxiv.org/abs/1405.0546?context=cs.IR)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1405.0546)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1405.0546)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1405.0546)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1405.html#PuurulaRB14) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/PuurulaRB14)\n\n[Antti Puurula](https://dblp.uni-trier.de/search/author?author=Antti%20Puurula)\n\n[Jesse Read](https://dblp.uni-trier.de/search/author?author=Jesse%20Read)\n\n[Albert Bifet](https://dblp.uni-trier.de/search/author?author=Albert%20Bifet)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1405.0546&description=Kaggle LSHTC4 Winning Solution) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1405.0546&title=Kaggle LSHTC4 Winning Solution)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1405.0546) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/1405.0546"
    },
    {
      "title": "Grandmaster Pro Tip: Winning First Place in a Kaggle Competition with Stacking Using cuML",
      "text": "Grandmaster Pro Tip: Winning First Place in a Kaggle Competition with Stacking Using cuML | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# Grandmaster Pro Tip: Winning First Place in a Kaggle Competition with Stacking Using cuML\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/05/stacking-three-levels-models-1-1024x576.png)\nMay 22, 2025\nBy[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (0)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml/)\n* [T](https://twitter.com/intent/tweet?text=Grandmaster+Pro+Tip:+Winning+First+Place+in+a+Kaggle+Competition+with+Stacking+Using+cuML+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml/&amp;title=Grandmaster+Pro+Tip:+Winning+First+Place+in+a+Kaggle+Competition+with+Stacking+Using+cuML+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml/>)\nAI-Generated Summary\nLike\nDislike\n* To win a Kaggle competition in 2025, Chris Deotte used a stacking strategy with GPU-accelerated modeling using cuML, which allowed for fast exploration of hundreds of diverse models.\n* The stacking technique involved training multiple Level 1 models, including gradient boosted decision trees and deep learning neural networks, and then using their output as input for Level 2 models, which learned to combine the predictions in different ways.\n* By leveraging cuML&#039;s GPU acceleration, Deotte was able to build a three-level stack that achieved a private leaderboard RMSE of 11.44, winning first place in the April 2025 Kaggle Playground competition predicting podcast listening times.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nWhat does it take to win a Kaggle competition in 2025? In the April Playground challenge, the goal was to predict how long users would listen to a podcast\u2014and the top solution wasn\u2019t just accurate, it was fast. In this post, Kaggle Grandmaster Chris Deotte will break down the exact stacking strategy that powered his first-place finish using GPU-accelerated modeling with[cuML](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/rapidsai/resources/rapids_cuml). You\u2019ll learn a powerful pro tip: how to explore hundreds of diverse models quickly and combine them into a layered ensemble that outperforms the rest.\n## Overview of stacking[**](#overview_of_stacking)\nStacking is an advanced tabular data modeling technique that achieves high performance by combining the predictions of multiple diverse models. Leveraging the computational speed of GPUs, a large number of models can be efficiently trained. These include gradient boosted decision trees (GBDT), deep learning neural networks (NN), and other machine learning (ML) models such as support vector regression (SVR) and k-nearest neighbors (KNN). These individual models are referred to as Level 1 models.\nLevel 2 models are then trained, which use the output of Level 1 models as their input. The Level 2 models learn to use different combinations of Level 1 models to predict the target in different scenarios. Finally, simple Level 3 models are trained, which average the Level 2 models\u2019 outputs. The result of this process is a three-level stack.\nFigure 1 depicts 12 Level 1 models. The final solution uses 75 Level 1 models that were chosen from 500 experimental models. The secret to building a strong stack is to explore many Level 1 models. It is therefore crucial to train and evaluate models as fast as possible using GBDT with GPU, NN with GPU, and ML with GPU. This is done using[cuML](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/rapidsai/resources/rapids_cuml), which accelerates ML models on GPU.\n**Figure 1. The winning entry in the Kaggle April 2025 Playground competition used stacking with three levels of models, with the results of each level used in subsequent levels**## Diverse approaches to predict target[**](#diverse_approaches_to_predict_target&nbsp;)\nDiverse models are built by solving a problem in different ways. Different types of models can be used as well as different architectures and hyperparameters. Different preprocessing methods and feature engineering can also be used.\nIn the[April 2025 Kaggle Playground competition](https://www.kaggle.com/competitions/playground-series-s5e4)there were at least four different ways to predict the target of Podcast Listening Time. The target`Listening\\_Time\\_minutes`is approximately equal to the linear relationship`0.72 x Episode\\_Length\\_minutes`. For more details, see the discussion posts,[Strong Feature Interaction Exists](https://www.kaggle.com/competitions/playground-series-s5e4/discussion/573002),[Direct versus Indirect &#8211; Relationship with Target](https://www.kaggle.com/competitions/playground-series-s5e4/discussion/574249), and[Strong Correlation Between Features and Target](https://www.kaggle.com/competitions/playground-series-s5e4/discussion/571549). The other nine features modulate this linear relationship.\n**Figure 2. Scatter plot showing strong linear relationship between feature`Episode\\_Length\\_minutes`and target`Listening\\_Time\\_minutes`**\nBased on the insight depicted in Figure 2, you can predict the target in at least the following four ways:\n* Predict TARGET as is\n* Predict RATIO of target divided by episode length minutes\n* Predict RESIDUAL from linear relationship\n* Predict MISSING`episode\\_length\\_minutes`\nEach of these four approaches has two cases:\n* 88% of rows have episode length minutes\n* 12% of rows are missing episode length minutes### Predict target[**](#predict_target)\nThe common way to build a model is to use the given target. From the 10 given feature columns, you can engineer additional feature columns, then train your model using all these features to predict the given target:\n```\nmodel = Regressor().fit(train&#x5B;&#x5B;FEATURES],train&#x5B;&#x5B;&#039;&#039;Listening\\_Time\\_minutes&#039;&#039;])\nPREDICTION = model.predict(train&#x5B;&#x5B;FEATURES])\n```\nUsing this approach alone won first place in the February 2025 Kaggle Playground competition. For more details, see[Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas](https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/).\n### Predict ratio[**](#predict_ratio)\nAn alternative to predicting the given target is to predict the ratio between target and feature`Episode\\_Length\\_minutes`:\n```\ntrain&#x5B;&#x5B;&#039;&#039;new\\_target&#039;&#039;] = train.Listening\\_Time\\_minutes / train.Episode\\_Length\\_minutes\n```\nYou can train models to predict this new target. Then multiply this prediction by`Episode\\_Length\\_minutes`or an imputed value of`Episode\\_Length\\_minutes`.\n### Predict residual[**](#predict_residual)\nAnother alternative to predicting the given target is to predict the residual between the target and a li...",
      "url": "https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml"
    },
    {
      "title": "Using Ensembles in Kaggle Data Science Competitions- Part 3 - KDnuggets",
      "text": "# Using Ensembles in Kaggle Data Science Competitions- Part 3\n\nEarlier, we showed how to create stacked ensembles with stacked generalization and out-of-fold predictions. Now we'll learn how to implement various stacking techniques.\n\n* * *\n\n![c](https://www.kdnuggets.com/images/comment.gif)[comments](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html#comments)\n\n**By Henk van Veen**\n\n_The natural world is complex, so it figures that ensembling different models can capture more of this complexity- Ben Hamner_\n\n[![This image shows Models visualized as a network can be trained used back-propagation](https://www.kdnuggets.com/wp-content/uploads/backward-propagation-stacker-models-300x197.jpg)](https://www.kdnuggets.com/wp-content/uploads/backward-propagation-stacker-models.jpg)\n\nBackward propagation of networked models\n\n**Stacking with logistic regression:**\n\nStacking with logistic regression is one of the more basic and traditional ways of stacking. You can create predictions for the test set in one go, or take an average of the out-of-fold predictors. Either works well.\n\nThough taking the average is a clean and accurate way to do this, you might want to consider one go as that slightly lowers both model and coding complexity.\n\n**Kaggle use: \u201cPapirusy z Edhellond\u201d:**\n\nThe author uses [blend.py](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py) to compete in this classification competition.\nBy stacking 8 base models (diverse ET\u2019s, RF\u2019s and GBM\u2019s) with Logistic Regression he is able to score 0.99409 accuracy, good for first place.\n\n**Kaggle use: KDD-cup 2014:**\n\nHere the author again used [blend.py](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py) to improve a model. The model before stacking scored ~0.605 AUC, and with stacking this improved to ~0.625.\n\n**Stacking with non-linear algorithms:**\n\nPopular non-linear algorithms for stacking are GBM, KNN, NN, RF and ET. You can note a couple of interesting points here:\n\n- Non-linear stacking with the original features on multiclass problems gives surprising gains.\n- Non-linear algorithms find useful interactions between the original features and the meta-model features.\n\n**Kaggle use: TUT Headpose Estimation Challenge:**\n\nThe [TUT Headpose Estimation challenge](https://inclass.kaggle.com/c/tut-head-pose-estimation-challenge%20) can be treated as a multi-class multi-label classification challenge. For every label a separate ensemble model was trained. The key point is that stacking the predicted class probabilities with an extremely randomized trees model improved the scores. The author stacked generalization with standard models and was able to reduce the error by around 30%.\n\n**Feature weighted linear stacking:**\n\nFeature-weighted linear stacking stacks engineered meta-features together with model predictions. Linear algorithms are used to keep the resulting model fast and simple to inspect.\n\n**Quadratic linear stacking of models:**\n\nThe author framed the name \u2013 Quadratic linear stacking of models. It works similar to feature-weighted linear stacking, but creates combinations of model predictions. This technique improved the author's score in many competitions, most noticeably on the [Modeling Women\u2019s Healthcare Decision competition](http://www.drivendata.org/competitions/6/) on DrivenData.\n\n**Stacking classifiers with regressors and vice versa:**\n\nBy stacking you can use classifiers for regression problems and vice versa. Even though regression is usually not the best classifier. But it is a bit tricky.\n\n- You use binning first and turn a regression problem into a multiclass classification problem.\n- The predicted probabilities for these classes can help a stacking regressor make better predictions.\n\nA good stacker must be able to take information from the predictions.\n\n**Stacking unsupervised learned features:**\n\nYou can also stack with unsupervised learning techniques as well. A sensible popular technique is the K-Means Clustering. An interested recent addition is to use\n[t-SNE](http://lvdmaaten.github.io/tsne/):\n\n- Reduce the dataset to 2 or 3 dimensions.\n- stack this with a non-linear stacker.\n- Use a holdout set for stacking/blending (safe choice).\n\n**Online Stacking:**\n\nA good example of online (or semi-) stacking is with ad click prediction. Models trained on recent data perform better here.\n\n- So when a dataset has a temporal effect, you could use Vowpal Wabbit to train on the entire dataset.\n- Combine it with a more complex and powerful tool like XGBoost to train on the last day of data.\n- Finally stack the XGBoost predictions together with the samples and let Vowpal Wabbit do what it does best - optimizing loss functions.\n\n**Everything is a hyper-parameter:**\n\nWhen doing stacking/blending/meta-modeling, think of every action as a hyper-parameter for the stacker model.\n\nSo this makes the below simply extra parameters to be tuned to improve the ensemble performance.\n\n- Not scaling the data\n- Standard-Scaling the data\n- Minmax scaling the data\n\n**Model Selection:**\n\nYou can further optimize scores by combining multiple ensembled models.\n\n- Use averaging, voting or rank averaging on manually-selected well-performing ensembles.\n- Start with a base ensemble of 3 or so good models. Add a model when it increases the train set score the most. By allowing put-back of models, a single model may be picked multiple times (weighing).\n- Use of genetic algorithms (from Genetic Model Selection) and CV-scores as the fitness function.\n- The author uses a fully random method: Create a 100 or so ensembles from randomly selected ensembles (without placeback). Then pick the highest scoring model.\n\n**Automation:**\n\nAdding many base models along with multiple stacked ensembles can only get you so far in a competition.\nFor the rest, you might consider the below for automating:\n\n- Models visualized as a network can be trained used back-propagation\n- Consider CV-scores and their standard deviation (smaller the better).\n- There is scope to optimizing complexity/memory usage and running times.\n- Also look at making the script prefer uncorrelated model predictions when creating ensembles.\n- Consider parallelizing and distributing your automation to improve speed.\n\n**Kaggle use: [Otto product classification:](https://www.kaggle.com/c/otto-group-product-classification-challenge)**\n\nUsing the automated stacker in this competition, the author got to top 10% score without any tuning or manual model selection. Here's his approach:\n\n- For **base models** is to generate random algorithms with pure random parameters and train.\n- Wrappers can be written to make classifiers like VW, Sofia-ML, RGF, MLP and XGBoost play nicely with the Scikit-learn API.\n- For **stackers** let the script use SVM, random forests, extremely randomized trees, GBM and XGBoost with random parameters and a random subset of base models.\n- Finally average the created stackers when their fold-predictions on the train set produces a lower loss.\n\n**Why create these Frankenstein ensembles?**\n\nYou may wonder why this exercise in futility: stacking and combining 1000s of models and computational hours is insanity right? Well\u2026 yes. But these monster ensembles still have their uses:\n\n- You can win Kaggle competitions.\n- You can beat most state-of-the-art academic benchmarks with a single approach.\n- It is possible to transfer knowledge from the ensemble back to a simpler shallow model (Hinton\u2019s [Dark Knowledge](http://www.ttic.edu/dl/dark14.pdf), Caruana\u2019s [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf))\n- A good thing about ensembling is that loss of one model is not fatal for creating good predictions.\n- Automated large ensembles don't require much tuning or selection.\n- A 1% increase in accuracy may push an investment fund from making a loss, into making a little less loss. More seriously: Improving healthcare screening methods helps save lives.\n\nSee also\n[**Using Ensembles in Kaggle ...",
      "url": "https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html"
    },
    {
      "title": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2411.01645"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "",
      "text": "# Using Ensembles in Kaggle Data Science Competitions \u2013 Part 2\n\nAspiring to be a Top Kaggler? Learn more methods like Stacking & Blending. In the previous post we discussed about ensembling models by ways of weighing, averaging and ranks. There is much more to explore in Part-2!\n\n* * *\n\n![c](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2012%2012'%3E%3C/svg%3E)[comments](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html#comments)\n\n**By Henk van Veen**\n\n**Stacked Generalization & Blending**\n\nAveraging prediction files is nice and easy, but it\u2019s not the only method that the top Kagglers [Repetition code](https://www.kaggle.com/users) are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.\n\n**Netflix**\n\n[![This image shows netflix leaderboard results with blending hundreds of predictive models](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20213'%3E%3C/svg%3E)](https://www.kdnuggets.com/wp-content/uploads/compiling-blending-predictive-models-by-netflix-engineers.jpg)\n\nBlending hundreds of predictive models to finally cross the finish line.\n\nNetflix organized and popularized the first data science competitions. Competitors in the movie recommendation challenge really pushed the state of the art on ensemble creation, perhaps so much so that Netflix decided not to implement the winning solution in production. That one was simply too complex.\n\nNevertheless, a number of papers and novel methods resulted from this challenge:\n\n- [Feature-Weighted Linear Stacking](http://arxiv.org/pdf/0911.0460.pdf)\n- [Combining Predictions for Accurate Recommender Systems](http://elf-project.sourceforge.net/CombiningPredictionsForAccurateRecommenderSystems.pdf%20)\n- [The BigChaos Solution to the Netflix Prize](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf%20)\n\nAll are interesting, accessible and relevant reads when you want to improve your Kaggle game.\n\n**Stacked generalization**\n\nStacked generalization was introduced by Wolpert in a 1992 paper, 2 years before the seminal Breiman paper \u201cBagging Predictors\u201c. Wolpert is famous for another very popular machine learning theorem:\n\n_\u201cThere is no free lunch in search and optimization\u201c._\n\nThe basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.\n\nLet\u2019s say you want to do 2-fold stacking:\n\n- Split the train set in 2 parts: train\\_a and train\\_b\n- Fit a first-stage model on train\\_a and create predictions for train\\_b\n- Fit the same model on train\\_b and create predictions for train\\_a\n- Finally fit the model on the entire train set and create predictions for the test set.\n- Now train a second-stage stacker model on the probabilities from the first-stage model(s).\n\nA stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.\n\n**Blending**\n\nBlending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use \u201cstacked ensembling\u201d and \u201cblending\u201d interchangeably.\n\nWith blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\n\nBlending has a few benefits:\n\n- It is simpler than stacking.\n- It wards against an information leak: The generalizers and stackers use different data.\n- You do not need to share a seed for stratified folds with your teammates. Anyone can throw models in the \u2018blender\u2019 and the blender decides if it wants to keep that model or not.\n\nHowever, The cons are:\n\n- You use less data overall\n- The final model may overfit to the holdout set.\n- Your CV is more solid with stacking (calculated over more folds) than using a single small holdout set.\n\nAs for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. The author prefers stacking.\n\nIf you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage which we will explore next.\n\n[**Using Ensembles in Kaggle Data Science Competitions \u2013 Part 1**](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html)\n\n[**Using Ensembles in Kaggle Data Science Competitions \u2013 Part 3**](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html)\n\nHow are you planning to implement what you learned? Share your thoughts!\n\nOriginal: [**Kaggle Ensembling Guide**](http://mlwave.com/kaggle-ensembling-guide/) by Henk van Veen.\n\n**Related:**\n\n- [How to Lead a Data Science Contest without Reading the Data](https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html)\n- [Top 20 R Machine Learning and Data Science packages](https://www.kdnuggets.com/2015/06/top-20-r-machine-learning-packages.html)\n- [Netflix: Director \u2013 Product Analytics, Data Science and Engineering](https://www.kdnuggets.com/jobs/14/08-11-netflix-director-product-analytics-data-science-engineering.html)\n\n### More On This Topic\n\n- [Are Kaggle Competitions Useful for Real World Problems?](https://www.kdnuggets.com/are-kaggle-competitions-useful-for-real-world-problems)\n- [Learn Machine Learning 4X Faster by Participating in Competitions](https://www.kdnuggets.com/2022/01/learn-machine-learning-4x-faster-participating-competitions.html)\n- [7 Free Kaggle Micro-Courses for Data Science Beginners](https://www.kdnuggets.com/7-free-kaggle-micro-courses-for-data-science-beginners)\n- [Top 10 Kaggle Machine Learning Projects to Become Data Scientist in 2024](https://www.kdnuggets.com/top-10-kaggle-machine-learning-projects-to-become-data-scientist-in-2024)\n- [Top 4 tricks for competing on Kaggle and why you should start](https://www.kdnuggets.com/2022/05/packt-top-4-tricks-competing-kaggle-start.html)\n- [The Most Comprehensive List of Kaggle Solutions and Ideas](https://www.kdnuggets.com/2022/11/comprehensive-list-kaggle-solutions-ideas.html)\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%2056'%3E%3C/svg%3E)\n\n[Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox.](https://www.kdnuggets.com/news/subscribe.html)\n\nBy subscribing you accept KDnuggets [Privacy Policy](https://www.kdnuggets.com/news/privacy-policy.html)\n\nLeave this field empty if you're human:\n\n* * *\n\n[<= Previous post](https://www.kdnuggets.com/2015/06/top-20-r-packages.html)\n\n[Next post =>](https://www.kdnuggets.com/2015/06/open-source-interactive-analytics-overview.html)\n\n![Search](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2074%2074'%3E%3C/svg%3E)\n\n### [Latest Posts](https://www.kdnuggets.com/news/index.html)\n\n- [Breaking into Data Science: Essential Skills and How to Learn Them](https://www.kdnuggets.com/breaking-into-data-science-essential-skills-and-how-to-learn-them)\n- [Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)\n- [6 Startups Redefining 3D Workflows with OpenUSD and Generative AI](https://www.kdnuggets.com/6-startups-redefining-3d-workflows-with-openusd-and-generative-ai)\n- [What Data Scientists Should Know About OpenUSD](https://www.kdnuggets.com/what-data-scientists-should-know-about-openusd)\n- [I Took the Google Data Analytics Certification W...",
      "url": "https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p2.html"
    },
    {
      "title": "Combine predictors using stacking #",
      "text": "Combine predictors using stacking &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\nNote\n[Go to the end](#sphx-glr-download-auto-examples-ensemble-plot-stack-predictors-py)to download the full example code or to run this example in your browser via JupyterLite or Binder.\n# Combine predictors using stacking[#](#combine-predictors-using-stacking)\nStacking refers to a method to blend estimators. In this strategy, some\nestimators are individually fitted on some training data while a final\nestimator is trained using the stacked predictions of these base estimators.\nIn this example, we illustrate the use case in which different regressors are\nstacked together and a final linear penalized regressor is used to output the\nprediction. We compare the performance of each individual regressor with the\nstacking strategy. Stacking slightly improves the overall performance.\n```\n# Authors: The scikit-learn developers# SPDX-License-Identifier: BSD-3-Clause\n```\n## Download the dataset[#](#download-the-dataset)\nWe will use the[Ames Housing](http://jse.amstat.org/v19n3/decock.pdf)dataset which was first compiled by Dean De Cock\nand became better known after it was used in Kaggle challenge. It is a set\nof 1460 residential homes in Ames, Iowa, each described by 80 features. We\nwill use it to predict the final logarithmic price of the houses. In this\nexample we will use only 20 most interesting features chosen using\nGradientBoostingRegressor() and limit number of entries (here we won\u2019t go\ninto the details on how to select the most interesting features).\nThe Ames housing dataset is not shipped with scikit-learn and therefore we\nwill fetch it from[OpenML](https://www.openml.org/d/42165).\n```\nimportnumpyasnpfromsklearn.datasetsimport[fetch\\_openml](../../modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml)fromsklearn.utilsimport[shuffle](../../modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle)defload\\_ames\\_housing():df=[fetch\\_openml](../../modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml)(name=&quot;&quot;house\\_prices&quot;&quot;,as\\_frame=True)X=df.datay=df.targetfeatures=[&quot;YrSold&quot;,&quot;HeatingQC&quot;,&quot;Street&quot;,&quot;YearRemodAdd&quot;,&quot;Heating&quot;,&quot;MasVnrType&quot;,&quot;BsmtUnfSF&quot;,&quot;Foundation&quot;,&quot;MasVnrArea&quot;,&quot;MSSubClass&quot;,&quot;ExterQual&quot;,&quot;Condition2&quot;,&quot;GarageCars&quot;,&quot;GarageType&quot;,&quot;OverallQual&quot;,&quot;TotalBsmtSF&quot;,&quot;BsmtFinSF1&quot;,&quot;HouseStyle&quot;,&quot;MiscFeature&quot;,&quot;MoSold&quot;,]X=X.loc[:,features]X,y=[shuffle](../../modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle)(X,y,random\\_state=0)X=X.iloc[:600]y=y.iloc[:600]returnX,[np.log](https://numpy.org/doc/stable/reference/generated/numpy.log.html#numpy.log)(y)X,y=load\\_ames\\_housing()\n```\n## Make pipeline to preprocess the data[#](#make-pipeline-to-preprocess-the-data)\nBefore we can use Ames dataset we still need to do some preprocessing.\nFirst, we will select the categorical and numerical columns of the dataset to\nconstruct the first step of the pipeline.\n```\nfromsklearn.composeimport[make\\_column\\_selector](../../modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector)cat\\_selector=[make\\_column\\_selector](../../modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector)(dtype\\_include=[object,&quot;string&quot;])num\\_selector=[make\\_column\\_selector](../../modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector)(dtype\\_include=[np.number](https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.number))cat\\_selector(X)\n```\n```\n[&#39;HeatingQC&#39;, &#39;Street&#39;, &#39;Heating&#39;, &#39;MasVnrType&#39;, &#39;Foundation&#39;, &#39;ExterQual&#39;, &#39;Condition2&#39;, &#39;GarageType&#39;, &#39;HouseStyle&#39;, &#39;MiscFeature&#39;]\n```\n```\nnum\\_selector(X)\n```\n```\n[&#39;YrSold&#39;, &#39;YearRemodAdd&#39;, &#39;BsmtUnfSF&#39;, &#39;MasVnrArea&#39;, &#39;MSSubClass&#39;, &#39;GarageCars&#39;, &#39;OverallQual&#39;, &#39;TotalBsmtSF&#39;, &#39;BsmtFinSF1&#39;, &#39;MoSold&#39;]\n```\nThen, we will need to design preprocessing pipelines which depends on the\nending regressor. If the ending regressor is a linear model, one needs to\none-hot encode the categories. If the ending regressor is a tree-based model\nan ordinal encoder will be sufficient. Besides, numerical values need to be\nstandardized for a linear model while the raw numerical data can be treated\nas is by a tree-based model. However, both models need an imputer to\nhandle missing values.\nWe will first design the pipeline required for the tree-based models.\n```\nfromsklearn.composeimport[make\\_column\\_transformer](../../modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer)fromsklearn.imputeimport[SimpleImputer](../../modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)fromsklearn.pipelineimport[make\\_pipeline](../../modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)fromsklearn.preprocessingimport[OrdinalEncoder](../../modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)cat\\_tree\\_processor=[OrdinalEncoder](../../modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)(handle\\_unknown=&quot;&quot;use\\_encoded\\_value&quot;&quot;,unknown\\_value=-1,encoded\\_missing\\_value=-2,)num\\_tree\\_processor=[SimpleImputer](../../modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)(strategy=&quot;mean&quot;,add\\_indicator=True)tree\\_preprocessor=[make\\_column\\_transformer](../../modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer)((num\\_tree\\_processor,num\\_selector),(cat\\_tree\\_processor,cat\\_selector))tree\\_preprocessor\n```\n```\nColumnTransformer(transformers=[(&#x27;&#x27;simpleimputer&#x27;&#x27;,\nSimpleImputer(add\\_indicator=True),\n&lt;&lt;sklearn.compose.\\_column\\_transformer.make\\_column\\_selector object at 0x7fe89c46c710&gt;&gt;),\n(&#x27;&#x27;ordinalencoder&#x27;&#x27;,\nOrdinalEncoder(encoded\\_missing\\_value=-2,\nhandle\\_unknown=&#x27;&#x27;use\\_encoded\\_value&#x27;&#x27;,\nunknown\\_value=-1),\n&lt;&lt;sklearn.compose.\\_column\\_transformer.make\\_column\\_selector object at 0x7fe89c46de10&gt;&gt;)])\n```\n**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\nOn GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**\nColumnTransformer\n[?Documentation for ColumnTransformer](https://scikit-learn.org/1.8/modules/generated/sklearn.compose.ColumnTransformer.html)iNot fitted\nParameters**|[transformerstransformers: list of tuples\nList of (name, transformer, columns) tuples specifying the\ntransformer objects to be applied to subsets of the data.\nname : str\nLike in Pipeline and FeatureUnion, this allows the transformer and\nits parameters to be set using ``set\\_params`` and searched in grid\nsearch.\ntransformer : {'drop', 'passthrough'} or estimator\nEstimator must support :term:`fit` and :term:`transform`.\nSpecial-cased strings 'drop' and 'passthrough' are accepted as\nwell, to indicate to drop the columns or to pass them through\nuntransformed, respectively.\ncolumns : str, array-like of str, int, array-like of int, array-like of bool, slice or callable\nIndexes the data on its se...",
      "url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html"
    }
  ]
}