{
  "query": "What are the specific implementation details and best practices for using LightGBM with TF-IDF and SVD features for text classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Based on a recent Medium overview of LightGBM, a Stack\u202fOverflow implementation, and the official LightGBM documentation, here\u2019s a concise recipe for using LightGBM with TF\u2011IDF (and optional SVD) features in a text\u2011classification competition** ([Medium](https://medium.com/@jeremychrist23/understanding-lightgbm-a-performance-boosting-algorithm-for-machine-learning-756a1edb38c4), [Stack\u202fOverflow](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf), [LightGBM docs](https://lightgbm.readthedocs.io/en/stable/Features.html)).\n\n---\n\n1. **Prepare the raw text**  \n   ```python\n   import pandas as pd\n   from nltk.corpus import stopwords\n   stop_words = set(stopwords.words('english'))\n   # optional: lower\u2011case, remove punctuation, strip stop words\n   df['clean'] = df['contents'].str.lower().apply(\n       lambda x: ' '.join([w for w in x.split() if w not in stop_words]))\n   ```  \n   *Removing English stop words is shown in the Stack\u202fOverflow example* ([Stack\u202fOverflow](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf)).\n\n2. **Convert text to a TF\u2011IDF matrix**  \n   ```python\n   from sklearn.feature_extraction.text import TfidfVectorizer\n   tfidf = TfidfVectorizer(\n       ngram_range=(1, 2),          # unigrams + bigrams\n       max_features=60000,          # limit dimensionality for speed\n       sublinear_tf=True)           # log\u2011scale term frequencies\n   X = tfidf.fit_transform(df['clean'])\n   y = df['category'].astype('category').cat.codes\n   ```  \n   *Using n\u2011grams and a max\u2011feature cap is the approach recommended for Kaggle\u2011style pipelines* ([Pipeline article](https://buhrmann.github.io/sklearn-pipelines.html); Stack\u202fOverflow code also sets `ngram_range` and `max_features`) ([Stack\u202fOverflow](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf)).\n\n3. **(Optional) Reduce dimensionality with SVD**  \n   ```python\n   from sklearn.decomposition import TruncatedSVD\n   svd = TruncatedSVD(n_components=200, random_state=42)\n   X_reduced = svd.fit_transform(X)          # dense 200\u2011dim matrix\n   ```  \n   *While the provided sources focus on TF\u2011IDF, applying `TruncatedSVD` after TF\u2011IDF is a common way to obtain compact dense features for LightGBM.*\n\n4. **Create a LightGBM dataset that preserves sparsity**  \n   ```python\n   import lightgbm as lgb\n   # LightGBM natively handles scipy sparse matrices (see \u201cSparse Optimization\u201d)\n   train_data = lgb.Dataset(X, label=y, categorical_feature=None)\n   ```  \n   *The LightGBM docs note native support for sparse inputs, which avoids converting the TF\u2011IDF matrix to dense form* ([LightGBM docs](https://lightgbm.readthedocs.io/en/stable/Features.html)).\n\n5. **Set multiclass\u2011compatible parameters and train with early stopping**  \n   ```python\n   params = {\n       'objective': 'multiclass',\n       'num_class': y.nunique(),\n       'metric': 'multi_logloss',\n       'learning_rate': 0.05,\n       'num_leaves': 31,\n       'max_depth': -1,\n       'feature_fraction': 0.8,\n       'bagging_fraction': 0.8,\n       'bagging_freq': 5,\n       'verbosity': -1\n   }\n\n   X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2,\n                                               stratify=y, random_state=42)\n\n   train_set = lgb.Dataset(X_tr, label=y_tr)\n   valid_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n\n   model = lgb.train(params,\n                     train_set,\n                     num_boost_round=5000,\n                     valid_sets=[valid_set],\n                     early_stopping_rounds=100,\n                     verbose_eval=100)\n   ```  \n   *The Stack\u202fOverflow snippet uses `train_test_split`, `cross_val_score`, and `GridSearchCV` for tuning, illustrating the same workflow* ([Stack\u202fOverflow](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf)).\n\n6. **Evaluate and extract feature importance**  \n   ```python\n   from sklearn.metrics import accuracy_score\n   preds = model.predict(X_val, num_iteration=model.best_iteration)\n   pred_labels = preds.argmax(axis=1)\n   print('Accuracy:', accuracy_score(y_val, pred_labels))\n\n   # Feature importance (use the original TF\u2011IDF vocab)\n   import numpy as np\n   importance = model.feature_importance(importance_type='gain')\n   top_idx = np.argsort(importance)[-20:][::-1]\n   top_words = [tfidf.get_feature_names_out()[i] for i in top_idx]\n   print('Top tokens:', top_words)\n   ```  \n   *Getting token importance from LightGBM\u2019s `feature_importance` method is demonstrated in another Stack\u202fOverflow answer* ([Feature\u2011importance SO](https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer)).\n\n7. **Best\u2011practice checklist for competitions**  \n   - **Keep the TF\u2011IDF matrix sparse** (LightGBM handles it efficiently).  \n   - **Limit TF\u2011IDF vocabulary** (`max_features` 30\u201160\u202fk) to control memory.  \n   - **Use n\u2011grams (1\u20112)** to capture short phrases.  \n   - **Apply SVD only if memory or over\u2011fitting is a concern**; retain ~100\u2011300 components.  \n   - **Employ early stopping** on a hold\u2011out validation set.  \n   - **Run cross\u2011validation or a grid search** on `learning_rate`, `num_leaves`, `feature_fraction`, and `bagging_fraction`.  \n   - **Inspect feature importance** to verify that meaningful tokens are being used and to potentially prune noisy features.\n\nFollowing these steps lets you turn raw text into TF\u2011IDF (and optionally SVD) features, feed them efficiently into LightGBM, and apply competition\u2011grade tuning and validation.",
      "url": ""
    },
    {
      "title": "Understanding LightGBM: A Performance-Boosting Algorithm for ...",
      "text": "Understanding LightGBM: A Performance-Boosting Algorithm for Machine Learning | by Jeremy Christian Budiawan | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@jeremychrist23/understanding-lightgbm-a-performance-boosting-algorithm-for-machine-learning-756a1edb38c4&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@jeremychrist23/understanding-lightgbm-a-performance-boosting-algorithm-for-machine-learning-756a1edb38c4&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Understanding LightGBM: A Performance-Boosting Algorithm for Machine Learning\n[\n![Jeremy Christian Budiawan](https://miro.medium.com/v2/resize:fill:64:64/1*qd_vALjmyD-C2DGE9aMW6Q.jpeg)\n](https://medium.com/@jeremychrist23?source=post_page---byline--756a1edb38c4---------------------------------------)\n[Jeremy Christian Budiawan](https://medium.com/@jeremychrist23?source=post_page---byline--756a1edb38c4---------------------------------------)\n7 min read\n\u00b7Jan 18, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/756a1edb38c4&amp;operation=register&amp;redirect=https://medium.com/@jeremychrist23/understanding-lightgbm-a-performance-boosting-algorithm-for-machine-learning-756a1edb38c4&amp;user=Jeremy+Christian+Budiawan&amp;userId=72b6203637d9&amp;source=---header_actions--756a1edb38c4---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/756a1edb38c4&amp;operation=register&amp;redirect=https://medium.com/@jeremychrist23/understanding-lightgbm-a-performance-boosting-algorithm-for-machine-learning-756a1edb38c4&amp;source=---header_actions--756a1edb38c4---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nThe image was generated by ChatGPT, using the DALL-E tool\n**LightGBM (Light Gradient Boosting Machine)**is a machine learning algorithm used for tasks such as classification, regression, and ranking. LightGBM is an implementation of**Gradient Boosting**that is highly optimized for efficiency and performance, especially when dealing with large datasets. It is known for its fast training speed, low memory usage, and scalability.\n## 1. What is Gradient Boosting?\nBefore diving into LightGBM, it\u2019s important to understand the concept of gradient boosting.**Gradient boosting**is an ensemble learning technique that combines multiple models (typically decision trees) to form a stronger model. This process works by building models sequentially, each new model attempting to correct the errors (residuals) made by the previous one. In brief:\n* The first model is trained on the data and makes predictions.\n* The next model is trained to correct the mistakes made by the previous model.\n* This process is repeated for several iterations, producing a series of decision trees that correct each other\u2019s errors.## 2. Key Features of LightGBM\nLightGBM implements gradient boosting with several optimizations that make it faster and more efficient, especially when working with large datasets. Some of its key features include:\n* **Histogram-based learning:**LightGBM uses a histogram-based technique for splitting data, which reduces the number of comparisons needed to build decision trees, thus saving time and memory.\n* **Leaf-wise tree growth:**Unlike other gradient boosting implementations, LightGBM uses a leaf-wise (rather than level-wise) approach to grow trees. This approach allows LightGBM to build deeper trees, which can capture more complex patterns and lead to better accuracy, though it may increase the risk of overfitting on small datasets.\n* **Parallel and GPU learning:**LightGBM can run in parallel and also utilize GPU processing for faster model training. This is particularly beneficial for large datasets.\n* **Handling large datasets:**LightGBM is optimized to handle large datasets with lower memory consumption and faster training times compared to other gradient boosting algorithms like XGBoost.\n* **Categorical feature support:**LightGBM has native support for categorical features, meaning it can handle categorical data without the need for explicit encoding (like one-hot encoding).\n* **Regularization:**LightGBM supports L1 and L2 regularization to help prevent overfitting and improve model generalization.## 3. How LightGBM Works\nThe process of training a LightGBM model typically involves the following steps:\n1. **Histogram construction:**The input data is split into a set of bins (intervals), reducing the complexity of finding splits in decision trees.\n2. **Leaf-wise tree construction:**LightGBM starts by building decision trees from root to leaf, growing deeper trees with each iteration by choosing the best splits based on histogram data.\n3. **Boosting iterations:**After the first tree is built, new trees are added to correct the residual errors of the previous trees. This process is repeated for a set number of iterations until the model converges.\n4. **Final prediction:**The final prediction is made by combining the outputs of all the trees in the ensemble model.## 4. Advantages of LightGBM\n* **Speed:**By using histogram-based learning and leaf-wise tree construction, LightGBM is significantly faster compared to other boosting algorithms, especially on large datasets.\n* **Memory efficiency:**LightGBM uses less memory due to its histogram-based technique and efficient handling of categorical data.\n* **Scalability:**LightGBM is well-suited for large datasets and can run on systems with varying resources, including GPU support for faster training.\n* **Overfitting prevention:**While using deeper trees, LightGBM has built-in regularization mechanisms to mitigate the risk of overfitting.## 5. Disadvantages of LightGBM\n* **Prone to overfitting on small datasets:**The leaf-wise growth strategy, which builds deeper trees, can lead to overfitting on small datasets if not carefully tuned.\n* **Model interpretability:**LightGBM models are more complex and harder to interpret compared to simpler models, such as linear regression.## 6. Applications of LightGBM\nLightGBM is widely used for various machine learning tasks, particularly in data science competitions like those on Kaggle. Some common applications include:\n* **Classification:**For tasks such as classifying images, text, or tabular data.\n* **Regression:**Predicting numerical values such as house prices or sales volumes.\n* **Ranking:**In information retrieval, LightGBM is used to rank search results based on relevance.## 7. Important Hyperparameters in LightGBM\nThere are several important hyperparameters to consider when tuning a LightGBM model. The*num\\_leaves*parameter controls the number of leaves in each decision tree. A higher value generally increases the model\u2019s power, but it can also lead to overfitting. The*learning\\_rate*controls the step size during training, with lower values typically improving accuracy but requiring more boosting iterations. The*n\\_estimators*parameter determines the number of trees to build in the mod...",
      "url": "https://medium.com/@jeremychrist23/understanding-lightgbm-a-performance-boosting-algorithm-for-machine-learning-756a1edb38c4"
    },
    {
      "title": "python LightGBM text classicication with Tfidf",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [python LightGBM text classicication with Tfidf](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked6 years, 1 month ago\n\nModified [4 years, 9 months ago](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf?lastactivity)\n\nViewed\n7k times\n\nPart of [NLP](https://stackoverflow.com/collectives/nlp) Collective\n\n4\n\nI'm trying to introduce LightGBM for text multiclassification.\n2 columns in pandas dataframe, where 'category' and 'contents' are set as follows.\n\n**Dataframe:**\n\n```\n    contents               category\n1   this is example1...    A\n2   this is example2...    B\n3   this is example3...    C\n\n*Actual data frame consists of approx 600 rows and 2 columns.\n\n```\n\nHereby I'm trying to classify text into 3 categories as follows.\n\n**Codes:**\n\n```\nimport pandas as pd\nimport numpy as np\n\nfrom nltk.corpus import stopwords\nstopwords1 = set(stopwords.words('english'))\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier, LGBMRegressor\n\n#--main code--#\nX_train, X_test, Y_train, Y_test = train_test_split(df['contents'], df['category'], random_state = 0, test_size=0.3, shuffle=True)\n\ncount_vect = CountVectorizer(ngram_range=(1,2), stop_words=stopwords1)\nX_train_counts = count_vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=True, norm='l2', sublinear_tf=True)\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nlgbm_train = lgbm.Dataset(X_train_tfidf, Y_train)\nlgbm_eval = lgbm.Dataset(count_vect.transform(X_test), Y_test, reference=lgbm_train)\n\nparams = {\n    'boosting_type':'gbdt',\n    'objective':'multiclass',\n    'learning_rate': 0.02,\n    'num_class': 3,\n    'early_stopping': 100,\n    'num_iteration': 2000,\n    'num_leaves': 31,\n    'is_enable_sparse': 'true',\n    'tree_learner': 'data',\n    'max_depth': 4,\n    'n_estimators': 50\n    }\n\nclf_gbm = lgbm.train(params, valid_sets=lgbm_eval)\npredicted_LGBM = clf_gbm.predict(count_vect.transform(X_test))\n\nprint(accuracy_score(Y_test, predicted_LGBM))\n\n```\n\nThen I got an error as:\n\n```\nValueError: could not convert string to float: 'b'\n\n```\n\nI also convert 'category' column \\['a', 'b', 'c'\\] to int as \\[0, 1, 2\\] but got an error as\n\n```\nTypeError: Expected np.float32 or np.float64, met type(int64).\n\n```\n\nWhat's wrong with my code?\n\nAny advice / suggestions will be greatly appreciated.\n\nThanks in advance.\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [tf-idf](https://stackoverflow.com/questions/tagged/tf-idf)\n- [text-classification](https://stackoverflow.com/questions/tagged/text-classification)\n- [lightgbm](https://stackoverflow.com/questions/tagged/lightgbm)\n\n[Share](https://stackoverflow.com/q/50250432)\n\n[Improve this question](https://stackoverflow.com/posts/50250432/edit)\n\nFollow\n\nasked May 9, 2018 at 9:53\n\n[![SY9's user avatar](https://lh6.googleusercontent.com/-5HAEklILSxw/AAAAAAAAAAI/AAAAAAAEnYs/Wueg0YweCHk/photo.jpg?sz=64)](https://stackoverflow.com/users/9478615/sy9)\n\n[SY9](https://stackoverflow.com/users/9478615/sy9) SY9\n\n16522 silver badges1111 bronze badges\n\n2\n\n- Curios. Why use a classifier built for categorical data when the features are sparse and non categorical??\n\n\u2013\u00a0[Isbister](https://stackoverflow.com/users/1818667/isbister)\n\nCommentedNov 27, 2018 at 23:07\n\n- @Isbister This code is for classification with the one-hot vector of extracted thousands of sentences so the data is sparse. In one-hot vector made by Scikit-learn CountVect is numerical since CV counts words in the sentence and put them to the vector. I think this is a bit classical but typical way for the text classification.\n\n\u2013\u00a0[SY9](https://stackoverflow.com/users/9478615/sy9)\n\nCommentedDec 3, 2018 at 1:52\n\n\n[Add a comment](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n5\n\nI managed to deal with this issue. Very simple but noted here for reference.\n\nSince LightGBM expects float32/64 for input, so 'categories' should be number, rather than str.\nAnd input data should be converted to float32/64 using .astype().\n\n**Changes1:**\n\n**added following 4 lines after** _X\\_train\\_tfidf = tfidf\\_transformer.fit\\_transform(X\\_train\\_counts)_\n\n```\n X_train_tfidf = X_train_tfidf.astype('float32')\n X_test_counts = X_test_counts.astype('float32')\n Y_train = Y_train.astype('float32')\n Y_test = Y_test.astype('float32')\n\n```\n\n**changes2:**\n\n**just convert 'category' column** from \\[A, B, C, ...\\] to \\[0.0, 1.0, 2.0, ...\\]\n\nMaybe just assigning attirbute as TfidfVecotrizer(dtype=np.float32) works in this case.\n\nAnd putting vectorized data to LGBMClassifier will be much simpler.\n\n**Update**\n\nUsing TfidfVectorizer is much simpler:\n\n```\ntfidf_vec = TfidfVectorizer(dtype=np.float32, sublinear_tf=True, use_idf=True, smooth_idf=True)\nX_data_tfidf = tfidf_vec.fit_transform(df['contents'])\nX_train_tfidf = tfidf_vec.transform(X_train)\nX_test_tfidf = tfidf_vec.transform(X_test)\n\nclf_LGBM = lgbm.LGBMClassifier(objective='multiclass', verbose=-1, learning_rate=0.5, max_depth=20, num_leaves=50, n_estimators=120, max_bin=2000,)\nclf_LGBM.fit(X_train_tfidf, Y_train, verbose=-1)\npredicted_LGBM = clf_LGBM.predict(X_test_tfidf)\n\n```\n\n[Share](https://stackoverflow.com/a/50406572)\n\n[Improve this answer](https://stackoverflow.com/posts/50406572/edit)\n\nFollow\n\n[edited Jan 26, 2019 at 1:17](https://stackoverflow.com/posts/50406572/revisions)\n\nanswered May 18, 2018 at 8:02\n\n[![SY9's user avatar](https://lh6.googleusercontent.com/-5HAEklILSxw/AAAAAAAAAAI/AAAAAAAEnYs/Wueg0YweCHk/photo.jpg?sz=64)](https://stackoverflow.com/users/9478615/sy9)\n\n[SY9](https://stackoverflow.com/users/9478615/sy9) SY9\n\n16522 silver badges1111 bronze badges\n\n3\n\n- I am still getting this error even after adding those 4 lines you suggested.\n\n\u2013\u00a0[phaigeim](https://stackoverflow.com/users/3879625/phaigeim)\n\nCommentedJan 18, 2019 at 19:18\n\n- TypeError: Expected np.float32 or np.float64, met type(int64) This is happening during \"train\" method\n\n\u2013\u00a0[phaigeim](https://stackoverflow.com/users/3879625/phaigeim)\n\nCommentedJan 22, 2019 at 8:15\n\n- 1\n\n\n\n\n\n@phaigeim Have you tried using TfidfVectorizer instead CountVectorizer + Tfidftransformer? TfidfVect support dtype option so you can convert data type. See Update in my answer above. If it doesn't resolve your issue, directly convert your dataset to float before voctorize data.\n\n\u2013\u00a0[SY9](https://stackoverflow.com/users/9478615/sy9)\n\nCommentedJan 26, 2019 at 1:21\n\n\n[Add a comment](https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf)\u00a0\\|\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack...",
      "url": "https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf"
    },
    {
      "title": "How to get important words using LGBM feature importance and Tfidf vectorizer?",
      "text": "##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [How to get important words using LGBM feature importance and Tfidf vectorizer?](https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked4 years, 8 months ago\n\nModified [4 years, 8 months ago](https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer?lastactivity)\n\nViewed\n725 times\n\n1\n\nI am working a Kaggle dataset that predicts a price of an item using its description and other attributes. [Here](https://www.kaggle.com/c/mercari-price-suggestion-challenge/overview) is the link to the competition. As part of an experiment, I am currently, only using an item's description to predict its price. The description is free text and I use sklearn's Tfidf's vectorizer with a bi-gram and max features set to 60000 as input to a lightGBM model.\n\nAfter training, I would like to know the most influential tokens for predicting the price. I assumed lightGBM's `feature_importance` method will be able to give me this. This will return a 60000 dim numpy array, whose index I can use to retrieve the token from the Tfidf's vectorizer's vocab dictionary.\n\nHere is the code:\n\n```\nvectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=60000)\nx_train = vectorizer.fit_transform(train_df['text'].values.astype('U'))\nx_valid = vectorizer.transform(valid_df['text'].values.astype('U'))\n\nidx2tok = {v: k for k, v in vectorizer.vocabulary_.items()}\nfeatures = [f'token_{i}' for i in range(len(vectorizer.vocabulary_))]\nget_tok = lambda x, idxmap: idxmap[int(x[6:])]\n\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_valid = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n\ngbm = lgb.train(lgb_params, lgb_train, num_boost_round=10, valid_sets=[lgb_train, lgb_valid], early_stopping_rounds=10, verbose_eval=True)\n\n```\n\nThe model trains, however, after training when I call `gbm.feature_importance()`, I get a sparse array of integers, that really doesn't make sense to me:\n\n```\nfi = gbm.feature_importance()\nfi[:100]\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype=int32)\n\nnp.unique(fi)\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 33, 34, 38, 45],\n      dtype=int32)\n\n```\n\nI'm not sure how to interpret this. I thought that earlier indices of the feature importance array will have higher value and thus tokens corresponding to that index in the vectorizer's vocab will be more important/influential than other tokens. Is this assumption wrong? How do I get the most influential/important terms that determines the model outcome? Any help is appreciated.\n\nThanks.\n\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [lightgbm](https://stackoverflow.com/questions/tagged/lightgbm)\n- [tfidfvectorizer](https://stackoverflow.com/questions/tagged/tfidfvectorizer)\n\n[Share](https://stackoverflow.com/q/58550351)\n\n[Improve this question](https://stackoverflow.com/posts/58550351/edit)\n\nFollow\n\n[edited Jun 20, 2020 at 9:12](https://stackoverflow.com/posts/58550351/revisions)\n\n[![Community's user avatar](https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/-1/community)\n\n[Community](https://stackoverflow.com/users/-1/community) Bot\n\n111 silver badge\n\nasked Oct 24, 2019 at 23:22\n\n[![shaun's user avatar](https://www.gravatar.com/avatar/58264bf7ce5ad8f1768197c573d1f43e?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/1338584/shaun)\n\n[shaun](https://stackoverflow.com/users/1338584/shaun) shaun\n\n56011 gold badge1212 silver badges2929 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer)\u00a0\\|\n\nRelated questions\n\n[53\\\n\\\nScikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score](https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score)\n\n[20\\\n\\\nGet selected feature names TFIDF Vectorizer](https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer)\n\n[3\\\n\\\nHighlighting important words in a sentence using Deep Learning](https://stackoverflow.com/questions/51477977/highlighting-important-words-in-a-sentence-using-deep-learning)\n\nRelated questions\n\n[53\\\n\\\nScikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score](https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score)\n\n[20\\\n\\\nGet selected feature names TFIDF Vectorizer](https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer)\n\n[3\\\n\\\nHighlighting important words in a sentence using Deep Learning](https://stackoverflow.com/questions/51477977/highlighting-important-words-in-a-sentence-using-deep-learning)\n\n[27\\\n\\\nFeature importance using lightgbm](https://stackoverflow.com/questions/53413701/feature-importance-using-lightgbm)\n\n[3\\\n\\\nsklearn TfidfVectorizer : How to make few words to only be part of bi gram in the features](https://stackoverflow.com/questions/55162090/sklearn-tfidfvectorizer-how-to-make-few-words-to-only-be-part-of-bi-gram-in-th)\n\n[1\\\n\\\nhow to view tf-idf score against each word](https://stackoverflow.com/questions/56914017/how-to-view-tf-idf-score-against-each-word)\n\n[3\\\n\\\nHow to get \"Word\" Importance in NLP (TFIDF + Logistic Regression)](https://stackoverflow.com/questions/58165337/how-to-get-word-importance-in-nlp-tfidf-logistic-regression)\n\n[4\\\n\\\nFeature importance with LightGBM](https://stackoverflow.com/questions/64455949/feature-importance-with-lightgbm)\n\n[2\\\n\\\nHow to get the TF-IDF scores as well for the most important words?](https://stackoverflow.com/questions/64807140/how-to-get-the-tf-idf-scores-as-well-for-the-most-important-words)\n\n[2\\\n\\\nHow to find important words using TfIdfVectorizer?](https://stackoverflow.com/questions/66091139/how-to-find-important-words-using-tfidfvectorizer)\n\nLoad 7 more related questions\nShow fewer related questions\n\n## 0\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n## Know someone who can answer? Share a link to this [question](https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer) via [email](https://stackoverflow.com/cdn-cgi/l/email-protection\\#fbc4888e99919e988fc6a88f9a9890dec9cbb48d9e899d97948cdec9cbaa8e9e888f929495dd9a968bc099949f82c6b3948cdec9cb8f94dec9cb9c9e8fdec9cb92968b94898f9a958fdec9cb8c94899f88dec9cb8e8892959cdec9cbb7bcb9b6dec9cb9d9e9a8f8e899edec9cb92968b94898f9a95989edec9cb9a959fdec9cbaf9d929f9ddec9cb8d9e988f948992819e89dec89ddecbba938f8f8b88dec89adec99ddec99d888f9a9890948d9e899d97948cd5989496dec99d8adec99dcec...",
      "url": "https://stackoverflow.com/questions/58550351/how-to-get-important-words-using-lgbm-feature-importance-and-tfidf-vectorizer"
    },
    {
      "title": "Features \uf0c1",
      "text": "Features &mdash; LightGBM 4.6.0 documentation\n* [](index.html)\n* Features\n* [View page source](_sources/Features.rst.txt)\n# Features[\uf0c1](#features)\nThis is a conceptual overview of how LightGBM works[[1]](#references). We assume familiarity with decision tree boosting algorithms to focus instead on aspects of LightGBM that may differ from other boosting packages. For detailed algorithms, please refer to the citations or source code.\n## Optimization in Speed and Memory Usage[\uf0c1](#optimization-in-speed-and-memory-usage)\nMany boosting tools use pre-sort-based algorithms[[2, 3]](#references)(e.g. default algorithm in xgboost) for decision tree learning. It is a simple solution, but not easy to optimize.\nLightGBM uses histogram-based algorithms[[4, 5, 6]](#references), which bucket continuous feature (attribute) values into discrete bins. This speeds up training and reduces memory usage. Advantages of histogram-based algorithms include the following:\n* **Reduced cost of calculating the gain for each split**\n* Pre-sort-based algorithms have time complexity`O(#data)`\n* Computing the histogram has time complexity`O(#data)`, but this involves only a fast sum-up operation. Once the histogram is constructed, a histogram-based algorithm has time complexity`O(#bins)`, and`#bins`is far smaller than`#data`.\n* **Use histogram subtraction for further speedup**\n* To get one leaf\u2019s histograms in a binary tree, use the histogram subtraction of its parent and its neighbor\n* So it needs to construct histograms for only one leaf (with smaller`#data`than its neighbor). It then can get histograms of its neighbor by histogram subtraction with small cost (`O(#bins)`)\n* **Reduce memory usage**\n* Replaces continuous values with discrete bins. If`#bins`is small, can use small data type, e.g. uint8\\_t, to store training data\n* No need to store additional information for pre-sorting feature values\n* **Reduce communication cost for distributed learning**\n## Sparse Optimization[\uf0c1](#sparse-optimization)\n* Need only`O(2\\*#non\\_zero\\_data)`to construct histogram for sparse features\n## Optimization in Accuracy[\uf0c1](#optimization-in-accuracy)\n### Leaf-wise (Best-first) Tree Growth[\uf0c1](#leaf-wise-best-first-tree-growth)\nMost decision tree learning algorithms grow trees by level (depth)-wise, like the following image:\n![A diagram depicting level wise tree growth in which the best possible node is split one level down. The strategy results in a symmetric tree, where every node in a level has child nodes resulting in an additional layer of depth.](_images/level-wise.png)\nLightGBM grows trees leaf-wise (best-first)[[7]](#references). It will choose the leaf with max delta loss to grow.\nHolding`#leaf`fixed, leaf-wise algorithms tend to achieve lower loss than level-wise algorithms.\nLeaf-wise may cause over-fitting when`#data`is small, so LightGBM includes the`max\\_depth`parameter to limit tree depth. However, trees still grow leaf-wise even when`max\\_depth`is specified.\n![A diagram depicting leaf wise tree growth in which only the node with the highest loss change is split and not bother with the rest of the nodes in the same level. This results in an asymmetrical tree where subsequent splitting is happening only on one side of the tree.](_images/leaf-wise.png)\n### Optimal Split for Categorical Features[\uf0c1](#optimal-split-for-categorical-features)\nIt is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy.\nInstead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets. If the feature has`k`categories, there are`2^(k-1)-1`possible partitions.\nBut there is an efficient solution for regression trees[[8]](#references). It needs about`O(k\\*log(k))`to find the optimal partition.\nThe basic idea is to sort the categories according to the training objective at each split.\nMore specifically, LightGBM sorts the histogram (for a categorical feature) according to its accumulated values (`sum\\_gradient/sum\\_hessian`) and then finds the best split on the sorted histogram.\n## Optimization in Network Communication[\uf0c1](#optimization-in-network-communication)\nIt only needs to use some collective communication algorithms, like \u201cAll reduce\u201d, \u201cAll gather\u201d and \u201cReduce scatter\u201d, in distributed learning of LightGBM.\nLightGBM implements state-of-the-art algorithms[[9]](#references).\nThese collective communication algorithms can provide much better performance than point-to-point communication.\n## Optimization in Distributed Learning[\uf0c1](#optimization-in-distributed-learning)\nLightGBM provides the following distributed learning algorithms.\n### Feature Parallel[\uf0c1](#feature-parallel)\n#### Traditional Algorithm[\uf0c1](#traditional-algorithm)\nFeature parallel aims to parallelize the \u201cFind Best Split\u201d in the decision tree. The procedure of traditional feature parallel is:\n1. Partition data vertically (different machines have different feature set).\n2. Workers find local best split point {feature, threshold} on local feature set.\n3. Communicate local best splits with each other and get the best one.\n4. Worker with best split to perform split, then send the split result of data to other workers.\n5. Other workers split data according to received data.\nThe shortcomings of traditional feature parallel:\n* Has computation overhead, since it cannot speed up \u201csplit\u201d, whose time complexity is`O(#data)`.\nThus, feature parallel cannot speed up well when`#data`is large.\n* Need communication of split result, which costs about`O(#data/8)`(one bit for one data).\n#### Feature Parallel in LightGBM[\uf0c1](#feature-parallel-in-lightgbm)\nSince feature parallel cannot speed up well when`#data`is large, we make a little change: instead of partitioning data vertically, every worker holds the full data.\nThus, LightGBM doesn\u2019t need to communicate for split result of data since every worker knows how to split data.\nAnd`#data`won\u2019t be larger, so it is reasonable to hold the full data in every machine.\nThe procedure of feature parallel in LightGBM:\n1. Workers find local best split point {feature, threshold} on local feature set.\n2. Communicate local best splits with each other and get the best one.\n3. Perform best split.\nHowever, this feature parallel algorithm still suffers from computation overhead for \u201csplit\u201d when`#data`is large.\nSo it will be better to use data parallel when`#data`is large.\n### Data Parallel[\uf0c1](#data-parallel)\n#### Traditional Algorithm[\uf0c1](#id1)\nData parallel aims to parallelize the whole decision learning. The procedure of data parallel is:\n1. Partition data horizontally.\n2. Workers use local data to construct local histograms.\n3. Merge global histograms from all local histograms.\n4. Find best split from merged global histograms, then perform splits.\nThe shortcomings of traditional data parallel:\n* High communication cost.\nIf using point-to-point communication algorithm, communication cost for one machine is about`O(#machine\\*#feature\\*#bin)`.\nIf using collective communication algorithm (e.g. \u201cAll Reduce\u201d), communication cost is about`O(2\\*#feature\\*#bin)`(check cost of \u201cAll Reduce\u201d in chapter 4.5 at[[9]](#references)).\n#### Data Parallel in LightGBM[\uf0c1](#data-parallel-in-lightgbm)\nWe reduce communication cost of data parallel in LightGBM:\n1. Instead of \u201cMerge global histograms from all local histograms\u201d, LightGBM uses \u201cReduce Scatter\u201d to merge histograms of different (non-overlapping) features for different workers.\nThen workers find the local best split on local merged histograms and sync up the global best split.\n2. As aforementioned, LightGBM uses histogram subtraction to speed up training.\nBased on this, we can communicate histograms only for one leaf, and get its neighbor\u2019s histograms by subtraction as well.\nAll things considered, data parallel in Li...",
      "url": "https://lightgbm.readthedocs.io/en/stable/Features.html"
    },
    {
      "title": "Pipelines for text classification in\u00a0scikit-learn",
      "text": "[Scikit-learn\u2019s](http://scikit-learn.org) [pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) provide a useful layer of abstraction for building complex estimators or classification models. Its purpose is to aggregate a number of data transformation steps, and a model operating on the result of these transformations, into a single object that can then be used in place of a simple estimator. This allows for the one-off definition of complex pipelines that can be re-used, for example, in cross-validation functions, grid-searches, learning curves and so on. I will illustrate their use, and some pitfalls, in the context of a kaggle text-classification\u00a0challenge.\n\n### The\u00a0challenge\n\nThe goal in the [StumbleUpon Evergreen](https://www.kaggle.com/c/stumbleupon) classification challenge is the prediction of whether a given web page is relevant for a short period of time only (ephemeral) or can be recommended still a long time after initial discovery\u00a0(evergreen).\n\nEach webpage in the provided dataset is represented by its html content as well as additional meta-data, the latter of which I will ignore here for simplicity. Instead I will focus on the use of pipelines to 1) transform text data into a numerical form appropriate for machine learning purposes, and 2) for creating ensembles of different classifiers to (hopefully) improve prediction accuracy (or at least its\u00a0variance).\n\n### Text\u00a0transformation\n\nA useful tool for the representation of text in a machine learning context is the so-called [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) transformation, short for \u201cterm frequency\u2013inverse document frequency\u201d. The idea is simple. Each word in a document is represented by a number that is proportional to its frequency in the document, and inversely proportional to the number of documents in which it occurs. Very common words, such as \u201ca\u201d or \u201cthe\u201d, thereby receive heavily discounted tf-df scores, in contrast to words that are very specific to the document in question. Scikit-learn provides a TfidfVectorizer class, which implements this transformation, along with a few other text-processing options, such as removing the most common words in the given language (stop words). The result is a matrix with one row per document and as many columns as there are different words in the dataset\u00a0(corpus).\n\n### Pipelines\n\nIn few cases, however, is the vectorization of text into numerical values as simple as applying tf-idf to the raw data. Often, the relevant text to be converted needs to be extracted first. Also, the tf-idf transformation will usually result in matrices too large to be used with certain machine learning algorithms. Hence dimensionality reduction techniques are often applied too. Manually implementing these steps everytime text needs to be transformed quickly becomes repetitive and tedious. It needs to be done for the training as well as test set. Ideally, when using cross-validation to assess one\u2019s model, the transformation needs to be applied separately in each fold, particularly when feature selection (dimensionality reduction) is involved. If care is not taken, information about the whole dataset otherwise leaks into supposedly independent evaluations of individual\u00a0folds.\n\nPipelines help reduce this repetition. What follows is an example of a typical vectorization\u00a0pipeline:\n\n```\ndef get_vec_pipe(num_comp=0, reducer='svd'):\n''' Create text vectorization pipeline with optional dimensionality reduction. '''\n\n    tfv = TfidfVectorizer(\n        min_df=6, max_features=None, strip_accents='unicode',\n        analyzer=\"word\", token_pattern=r'\\w{1,}', ngram_range=(1, 2),\n        use_idf=1, smooth_idf=1, sublinear_tf=1)\n\n    # Vectorizer\n    vec_pipe = [\n        ('col_extr', JsonFields(0, ['title', 'body', 'url'])),\n        ('squash', Squash()),\n        ('vec', tfv)\n    ]\n\n    # Reduce dimensions of tfidf\n    if num_comp > 0:\n        if reducer == 'svd':\n            vec_pipe.append(('dim_red', TruncatedSVD(num_comp)))\n        elif reducer == 'kbest':\n            vec_pipe.append(('dim_red', SelectKBest(chi2, k=num_comp)))\n        elif reducer == 'percentile':\n            vec_pipe.append(('dim_red', SelectPercentile(f_classif, percentile=num_comp)))\n\n        vec_pipe.append(('norm', Normalizer()))\n\n    return Pipeline(vec_pipe)\n```\n\nHere, we first create an instance of the tf-idf vectorizer (for its parameters see [documentation)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). We then create a list of tuples, each of which represents a data transformation step and its name (the latter of which is required, e.g., for identifying individual transformer parameters in a grid search). The first two are custom transformers and the last one our vectorizer. The first transformer (\u201cJsonFields\u201d), for example, extracts a particular column from the dataset, in this case the first (0-indexed), interprets its content as json-encoded text, and extracts the json fields with the keys \u2018title\u2019, \u2018body\u2019 and \u2018url\u2019. The corresponding values are concatenated into a single string per row in the dataset. The result is a new transformed dataset with a single column containing the extracted text, which can then be processed by the vectorizer. After the vectorization step, an optional dimensionality reduction is added to the list of transformations before the final pipeline is constructed and\u00a0returned.\n\n#### Transformers\n\nCustom transformers such as those above are easily created by subclassing from scikit\u2019s [TransformerMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html). This base class exposes a single fit\\_transform() function, which in turn calls (to be implemented) fit() and transform() functions. For transformers that do not require fitting (no internal parameters to be selected based on the dataset), we can create a simpler base class that only needs the transform function to be\u00a0implemented:\n\n```\nclass Transformer(TransformerMixin):\n''' Base class for pure transformers that don't need a fit method '''\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, **transform_params):\n        return X\n\n    def get_params(self, deep=True):\n        return dict()\n```\n\nWith this in place, the JsonFields transformer looks like\u00a0this:\n\n```\nclass JsonFields(Transformer):\n    ''' Extract json encoded fields from a numpy array. Returns (iterable) numpy array so it can be used as input to e.g. Tdidf '''\n\n    def __init__(self, column, fields=[], join=True):\n        self.column = column\n        self.fields = fields\n        self.join = join\n\n    def get_params(self, deep=True):\n        return dict(column=self.column, fields=self.fields, join=self.join)\n\n    def transform(self, X, **transform_params):\n        col = Select(self.column, to_np=True).transform(X)\n        res = np.vectorize(extract_json, excluded=['fields'])(col, fields=self.fields)\n        return res\n```\n\nJsonFields itself encapsulates another custom transformer (Select), used here to keep the specification of pipelines concise. It could also have been used as a prior step in the definition of the pipeline. The Select transformer does nothing other than extracting a number of specified columns from a\u00a0dataset:\n\n```\nclass Select(Transformer):\n    '''  Extract specified columns from a pandas df or numpy array '''\n\n    def __init__(self, columns=0, to_np=True):\n        self.columns = columns\n        self.to_np = to_np\n\n    def get_params(self, deep=True):\n        return dict(columns=self.columns, to_np=self.to_np)\n\n    def transform(self, X, **transform_params):\n        if isinstance(X, pd.DataFrame):\n            allint = isinstance(self.columns, int) or\n                (isinstance(self.columns, list) and\n                 all([isinstance(x, int) for x in self.columns]))\n            if allint:\n                res = X.ix[:, self.columns]\n            elif all([isinstance(x, str) for x in self.columns]):\n                res = X[se...",
      "url": "https://buhrmann.github.io/sklearn-pipelines.html"
    },
    {
      "title": "If You're Not Using TF-IDF In Data Analysis, You're Missing Out | Sigma",
      "text": "If You\u2019re Not Using TF-IDF In Data Analysis, You\u2019re Missing Out | Sigma\n[\n00\nDAYS\n00\nHRS\n00\nMIN\n00\nSEC\nWORKFLOW \u00b7SIGMA&#x27;S\u00a0FIRST\u00a0USER\u00a0CONFERENCE \u00b7March 5\nGET\u00a0EARLY\u00a0BIRD\u00a0PRICING\n![arrow right](https://cdn.prod.website-files.com/666bbba4ff7240a20f4ccccf/66e87ea3554705da8e694dd0_Arrow-light.svg)\n](https://www.sigmacomputing.com/workflow-conference)\n![search icon](https://cdn.prod.website-files.com/666bbba4ff7240a20f4ccccf/666bbba4ff7240a20f4ccd9c_search.svg)![search icon](https://cdn.prod.website-files.com/666bbba4ff7240a20f4ccccf/68151aa973df4a94b622fcc6_search-white.svg)\n[Login](https://app.sigmacomputing.com/login)[Try Sigma free](https://www.sigmacomputing.com/go/free-trial)\nMarch 11, 2025\n# If You\u2019re Not Using TF-IDF In Data Analysis, You\u2019re Missing Half the Story\nMarch 11, 2025\n![]()\n![Team Sigma](https://cdn.prod.website-files.com/666bbba4ff7240a20f4cccf8/66917426656e3ebe45e75bd6_I43qeVBP_400x400.avif)\nTeam Sigma\n[![Facebook](https://cdn.prod.website-files.com/666bbba4ff7240a20f4ccccf/666bbba4ff7240a20f4ccd37_logo-facebook.svg)](#)[![Twitter](https://cdn.prod.website-files.com/666bbba4ff7240a20f4ccccf/666bbba4ff7240a20f4ccd95_logo-twitter.svg)](#)[![LinkedIn](https://cdn.prod.website-files.com/666bbba4ff7240a20f4ccccf/666bbba4ff7240a20f4cccfa_logo-linkedin.svg)](#)\n##### Table of Contents\n[](https://www.sigmacomputing.com/)\n[](https://www.sigmacomputing.com/)\n![If You\u2019re Not Using TF-IDF In Data Analysis, You\u2019re Missing Half the Story](https://cdn.prod.website-files.com/666bbba4ff7240a20f4cccf8/67d0941887369a6cc83293b1_Image_2x-3.webp)\nThink about the last time you searched for something online. You know the answers are buried somewhere in there, but where? Whether you were looking for a product review, a research paper, or the best taco spot in town, your results weren\u2019t random. Search engines don\u2019t just scan for words; they weigh them.\nTF-IDF, short for Term Frequency-Inverse Document Frequency, might sound technical, but its concept is straightforward. It\u2019s the math behind why some pages rank higher than others and why some text is more relevant than the rest. But it isn\u2019t just for search engines. It plays a role in business analytics, market research, and fraud detection. Identifying important words in large datasets helps teams make sharper decisions and find patterns they might have missed. Think of it as a spotlight highlighting the keywords defining your data.\nSo why does this matter for you? If your work involves analyzing customer feedback, sorting through unstructured data, or improving search functionality, understanding TF-IDF gives you an edge. It helps filter out noise, highlight trends, and reveal what truly matters in your data. It\u2019s not just a tool for data scientists, it&#x27;s for anyone who works with text data and wants to make smarter, faster decisions.\nHere, we\u2019ll explain how TF-IDF works, its role in business intelligence, and how it compares to other text analysis methods. By the end, you\u2019ll see why ignoring TF-IDF means missing a big part of the story hidden in your data.\n## What is TF-IDF, and why does it matter?\nThe sheer volume of text data businesses handle is overwhelming. Customer reviews, support tickets, research reports, and internal documentation all contain valuable insights, but important details can be overlooked without the right tools.\nTF-IDF is a statistical method that measures the importance of a word within a document compared to a larger collection of documents. Simple word counts can be misleading, but TF-IDF assigns greater importance to words that appear frequently in a document while remaining rare across a broader dataset. This makes it useful for identifying the most meaningful terms in a text.\n### A brief history\nTF-IDF, which has been around since the 1970s, was developed to improve how search engines and information retrieval systems rank documents. Before modern machine learning techniques, it was one of the most effective ways to determine relevance. While newer models supplement it, TF-IDF is still widely used in business intelligence, natural language processing (NLP), and cybersecurity.\n### Why businesses use TF-IDF\nText is often the most underutilized resource. Why? Because it\u2019s messy, unstructured, and hard to quantify. TF-IDF solves this problem by giving you a way to measure the importance of words in a document relative to a larger dataset.\nFor example, you\u2019re analyzing customer reviews for a new product. Words like \u201cgreat\u201d or \u201cbuy\u201d might appear frequently, but they don\u2019t tell you much on their own. TF-IDF helps you identify meaningful words like \u201cbattery life\u201d or \u201ceasy setup\u201d so you can focus on what matters. Companies rely on TF-IDF to analyze text beyond simple keyword matching. It helps with:\n* **Search optimization:**Improves how internal search tools and recommendation engines surface relevant content.\n* **Market research:**Identifies trending topics, competitive insights, and customer concerns from large datasets.\n* **Customer sentiment analysis:**Detects which words carry the most weight in reviews and social media conversations.\n* **Fraud detection:**Flags unusual language patterns in financial transactions and compliance reports.\nTF-IDF remains a practical and accessible tool that bridges basic keyword analysis with more[advanced NLP](https://www.sigmacomputing.com/blog/natural-language-processing-nlp)techniques. Understanding how it works can help businesses better use unstructured text data.\n## How TF-IDF works: Breaking down the mechanics\nAt its core, TF-IDF is made up of two components: Term Frequency (TF) and Inverse Document Frequency (IDF).\n### Term Frequency (TF): How often does a word appear?\nThe first part of TF-IDF is Term Frequency (TF). This measures how often a word appears in a document. Words that appear more frequently get higher scores. For example, if the word \u201cdata\u201d appears 10 times in a 100-word document, its term frequency is 10/100, or 0.1.\nBut here\u2019s the catch: just because a word appears frequently doesn\u2019t mean it\u2019s important. Common words like \u201cthe\u201d or \u201cand\u201d might have high term frequencies, but they don\u2019t say much about the document\u2019s content. That\u2019s where the second part comes in.\n### Inverse Document Frequency (IDF): How unique is the word?\nInverse Document Frequency (IDF) measures how rare or unique a word is across a collection of documents. It reduces the importance of common words by giving higher scores to words that appear in fewer documents. This helps filter out generic terms that don\u2019t add much meaning. For example, if the word \u201cblockchain\u201d appears in only 5 out of 1,000 documents, its IDF score will be high, indicating it\u2019s a significant term.\n### Combining TF and IDF\nTF-IDF combines these two measures to give you a score that reflects how often a word appears in a document and how unique it is across the entire dataset. The higher the TF-IDF score, the more important the word is to that document.\nThe formula for TF-IDF combines these two calculations:*TF \u2212IDF = TF \u00d7IDF*\nImagine a business analyzing customer reviews. A word like &quot;great&quot; might appear often, but since it is used across thousands of reviews, it doesn\u2019t reveal much about what customers value. In contrast, a word like &quot;refund&quot; might appear less frequently, but if it shows up in negative reviews more often than in positive ones, its TF-IDF score would be higher, signaling that it carries important meaning.\n### Challenges and limitations\nWhile TF-IDF is useful, it has its limitations:\n* **Does not account for word relationships:**It treats words individually and doesn\u2019t recognize context.\n* **Sensitive to document size:**Longer documents tend to have higher term frequencies, potentially skewing results if not properly normalized.\n* **Less effective with synonyms:**It doesn\u2019t recognize that &quot;cheap&quot; and &quot;affordable&quot; might mean the same thing.\nDespite these limitations, TF-IDF remains a valuable tool in text analysis, especially whe...",
      "url": "https://www.sigmacomputing.com/blog/tf-idf-definition"
    },
    {
      "title": "Evolution of NLP \u2014 Part 1 \u2014 Bag of Words, TF-IDF | by Kanishk Jain",
      "text": "<div><div><figure></figure><div><h2>Getting started with NLP Basics for Sentiment Classification</h2><div><div><a href=\"https://medium.com/@jainkanishk001?source=post_page---byline--9518cb59d2d1---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/analytics-vidhya?source=post_page---byline--9518cb59d2d1---------------------------------------\"><div><p></p></div></a></div></div></div><p>This is the first blog in a series of posts where I try to talk about the changes in modeling techniques for <strong><em>Natural Language Processing</em></strong> tasks over the past few years. Right from the basics, Bag of Words, we reach to the current State of The Art (SOTA) \u2014 Transformers! I\u2019ll try to give a brief about the algorithm itself, and then we drive straight into coding! We\u2019ll see over these series of posts, how much of a change NLP has seen over the years, and how you can get started or catch-up to the latest quickly. I hope you enjoy this journey ;)</p><p>For this post, we\u2019ll focus on using simple Bag-of-Words and TF-IDF based models, coupled with Ensemble Decision Trees to get the highest accuracy score!</p><p>You can also find this tutorial on the Kaggle Kernel \u2014 <a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Evolution of NLP \u2014 Part 1 \u2014 Bag of Words, TF-IDF</a> with complete code!</p><h2>Understanding the Data</h2><p>I\u2019ve used the <a href=\"https://datahack.analyticsvidhya.com/contest/janatahack-nlp-hackathon/True/#ProblemStatement\">JantaHack NLP Hackathon</a> dataset here. This dataset essentially consists of <strong>Steam User Reviews</strong> for different kinds of games, collected during 2015\u20132019. The goal here is to predict whether based on the user review, the user recommends or doesn\u2019t recommend the game. So, our goal is essentially <strong>Sentiment Classification. </strong>This will be our task for this whole series!</p><p>Combined Training and Test Data had 25000+ reviews. We\u2019ll only focus on using <em>user_review </em>for our predictions.</p><figure><figcaption><a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Complete Dataset for our analysis \u2014 Image from Author</a></figcaption></figure><p>Let\u2019s dive right into it! For this and the series of tutorials, I\u2019m only going to use the reviews, and no other columns. In practice, it\u2019s good to perform EDA to get a better sense of your data.</p><h2>Using Bag of Words, N-Grams, TF-IDF</h2><p>The approach below essentially covers some of the very first tools that anyone trying to experiment with NLP starts with. And, over time, lots and lots of libraries, like SpaCy and NLTK have popped up, which have simplified using this approach tremendously. There are also libraries like Textblob, which stand on the shoulders of mighty NLTK and provide a better and faster interface to perform a lot of NLTK operations and then some more.</p><p>I\u2019ll try and give a quick overview of the methods and libraries, however, I would recommend going to the websites of each of the libraries (attached below) to understand their complete set of capabilities.</p><h2>Step 1. Pre-Processing</h2><p>Cleaning up the user reviews!</p><ol><li><strong>decontracted</strong> function would convert short forms of general phrases into their longer versions</li><li><strong>lemmatize_with_postag</strong> function reduces the words to their base form. I\u2019ve used TextBlob library\u2019s implementation here, which is build on top of NLTK. Feel free to try out other functions. The main idea is to reduce additional vocabulary \u2014 which is helpful both computationally, and also helps in reduce over-fitting to certain key-words. There are other methods like Stemming, but it is generally inferior to Lemmatization.</li><li>Further cleaning to remove links, punctuation, etc.</li></ol><figure><figcaption><a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Before and After Pre-Processing \u2014 Image from Author</a></figcaption></figure><h2>Step 2. Structuring the Data for ML</h2><h2>Using Count Vectorizer (Bag of Words)</h2><p>Bag of words, put simply, indicates the count of appearance of a certain word in a review, irrespective of it\u2019s order. To do this, firstly we create a dictionary (or vocabulary) of all the words (or tokens) present in the reviews. Each token from the vocabulary is then converted to a column with it\u2019s<em> </em><strong><em>row[j]</em></strong> indicating \u2014 <em>\u201cHow many times was the token \u2014 \u201cthe\u201d present in </em><strong><em>review[j]</em></strong><em>?\u201d</em></p><p>I\u2019ve used a scikit-learn implementation below, but there are other libraries which can handle this well as well.</p><figure><figcaption><a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Final Dataset after using Bag-of-Words \u2014 Image from Author</a></figcaption></figure><p>The column names represent the token and rows represent individual sentences. If that token is present in the sentence, the respective column will have a value 1, otherwise 0.</p><h2>Using N-grams</h2><p>However, sometimes it\u2019s the combination of words that is important, and not just the words themselves. Example \u2014 \u201cnot good\u201d and \u201cgood\u201d would have same flag for \u201cgood\u201d token. Hence it becomes important to find these phrases that occur in our corpus which might affect overall meaning of review. This is what we call N-grams<br/>However, the cost to find these grows polynomially as Vocab Size (V) increases, as in essence we are looking at potentially <strong>\ud835\udc42(\ud835\udc49*\ud835\udc49)</strong> combinations of phrases at worse (where V is the size of vocabulary).</p><p>In our implementation, we limit to 2 and 3 grams. We further the select a total of top 3000 features, sorted based on their appearance in data.</p><figure><figcaption><a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Final Dataset after using N-Grams Bag-of-Words \u2014 Image from Author</a></figcaption></figure><p>Same idea as earlier, however, this time we look for specific n-gram sequences in the sentence.</p><h2>Using TF-IDF (Term Frequency \u2014 Inverse Document Frequency)</h2><p>Now if you are wondering what is term frequency , it is the relative frequency of a word in a document and is given as (term instances/total instances). Inverse Document Frequency is the relative count of documents containing the term is given as log(number of documents/documents with term) The overall importance of each word to the documents in which they appear is equal to <strong>TF * IDF</strong></p><p>This will give you a matrix where each column represents a word in the vocabulary (all the words that appear in at least one document) and each row represents a review, as before. This is done to reduce the importance of words that occur frequently in review and therefore, their significance in overall rating of the review.</p><p>Fortunately, scikit-learn gives you a built-in <strong>TfIdfVectorizer</strong> class that produces the TF-IDF matrix in a couple of lines.</p><figure><figcaption><a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Final Dataset after using TF-IDF \u2014 Image from Author</a></figcaption></figure><p>Once again, rows constitute sentences and individual tokens represent the columns. If the token is present within the sentence, based on the method discussed above a TFIDF score is calculated and filled in the respective column, otherwise the value is 0.</p><p>Similar to <strong>CountVectorizer</strong>, we use n-grams here as well.</p><figure><figcaption><a href=\"https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf\">Final Dataset after using N-grams TF-IDF \u2014 Image from Author</a></figcaption></figure><h2>Step 3. Modeling</h2><p>Let\u2019s first try out dataset with only TF-IDFs.</p><h2>Training using LGBM</h2><p>I\u2019ve used Light GBM because of it\u2019s high accuracy and speed, amon...",
      "url": "https://medium.com/analytics-vidhya/evolution-of-nlp-part-1-bag-of-words-tf-idf-9518cb59d2d1"
    },
    {
      "title": "What are the most effective text classification algorithms for NLP?",
      "text": "How to Choose the Best Text Classification Algorithm for NLP\nAgree & Join LinkedIn\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n``````````````\n![]()## Sign in to view more content\nCreate your free account or sign in to continue your search\nSign in\n## Welcome back\n````````````````````\nEmail or phone\nPassword\nShow\n[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_forgot_password)Sign in\nor\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy).\nNew to LinkedIn?[Join now](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&trk=pulse-article_contextual-sign-in-modal_sign-in-modal_join-link)\nor\nNew to LinkedIn?[Join now](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&trk=pulse-article_contextual-sign-in-modal_join-link)\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\nLinkedIn\nLinkedIn is better on the app\nDon\u2019t have the app? Get it in the Microsoft Store.\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&amp;mode=mini&amp;cid=guest_desktop_upsell)\n``\n````````````\n[Skip to main content](#base-template__workspace)\n````````\n1. [All](https://www.linkedin.com/pulse/topics/home/)\n2. [Engineering](https://www.linkedin.com/pulse/topics/engineering-s166/)\n3. [Machine Learning](https://www.linkedin.com/pulse/topics/engineering-s166/machine-learning-s3289/)\n# What are the most effective text classification algorithms for NLP?\nPowered by AI and the LinkedIn community\n### 1\n[Naive Bayes](#naive-bayes)\n### 2\n[Logistic Regression](#logistic-regression)\n### 3\n[Support Vector Machines](#support-vector-machines)\n### 4\n[Neural Networks](#neural-networks)\n### 5\n[Decision Trees and Random Forests](#decision-trees-and-random-forests)\n### 6\n[Here\u2019s what else to consider](#here\u2019s-what-else-to-consider)\nText classification is a common task in natural language processing (NLP), where you want to assign a label or category to a piece of text based on its content and context. For example, you might want to classify an email as spam or not, a product review as positive or negative, or a news article as political or sports. But how do you choose the best algorithm for your text classification problem? In this article, you will learn about some of the most effective text classification algorithms for NLP, and how to apply them to your data.\n``\nTop experts in this article\nSelected by the community from 47 contributions.[Learn more](https://www.linkedin.com/help/linkedin/answer/a1652832)\n* [![Member profile image]()\nTavishi Jaglan\nData Science Manager @Publicis Sapient | 4xGoogle Cloud Certified | Gen AI | LLM | RAG | Graph RAG | Mlops |DL | NLP |\u2026\n](https://in.linkedin.com/in/tavishi1402?trk=article-ssr-frontend-x-article)\nView contribution````\n![]()![]()18\n``````````````\n* [![Member profile image]()\nJyothsna Devi Tiruveedhula\nEngineer-1 @Verizon | DE Intern\u201925 @Verizon | DS Intern\u201924 @PayPal | CSE-AI ML Grad (VIT-AP Rank 4) | Data Science |\u2026\n](https://in.linkedin.com/in/jyothsna-devi-tiruveedhula-b70a33222?trk=article-ssr-frontend-x-article)\nView contribution````\n![]()![]()15\n``````````````\n* [![Member profile image]()\nRishabh Mishra\nData Analyst @ Highspring (On-site Google) | Ex. AI Engineer Intern @ THEFINANSOL | Aspiring Data Scientist |\u2026\n](https://in.linkedin.com/in/rishabhh-mishra?trk=article-ssr-frontend-x-article)\nView contribution````\n![]()![]()8\n``````````````\n![]()![]()![]()\nSee what others are saying\n``\n## [](#naive-bayes)1Naive Bayes\nNaive Bayes is a simple and fast algorithm that works well for many text classification problems. It is based on the assumption that the words in a text are independent of each other, and that the probability of a text belonging to a class is proportional to the product of the probabilities of each word in that class. Naive Bayes can handle large and sparse data sets, and can deal with multiple classes. However, it may not perform well when the words are not independent, or when there are strong correlations between features and classes. To use Naive Bayes for text classification, you need to first convert your text into a vector of word counts or frequencies, and then apply the Bayes theorem to calculate the class probabilities.\n``````\n[\nAdd your perspective\n](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&amp;trk=article-ssr-frontend-x-article)\n````````````````````````\nHelp others by sharing more (125 characters min.)Cancel\nAddSave\n* [![Contributor profile photo]()![Contributor profile photo]()\nJyothsna Devi Tiruveedhula\nEngineer-1 @Verizon | DE Intern\u201925 @Verizon | DS Intern\u201924 @PayPal | CSE-AI ML Grad (VIT-AP Rank 4) | Data Science | Machine Learning | Deep Learning | NLP\n](https://in.linkedin.com/in/jyothsna-devi-tiruveedhula-b70a33222?trk=article-ssr-frontend-x-article)\n* Copy link to contribution\n* [Report contribution](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/advice/1/what-most-effective-text-classification-algorithms&amp;trk=comment-semaphore-sign-in-redirect&amp;guestReportContentType=AUTO_GENERATED_SKILL_ARTICLE_CONTRIBUTION&amp;_f=guest-reporting)\nThanks for letting us know&#33; You&#39;ll no longer see this contribution\n``````\nNaive Bayes is indeed a solid choice for text classification due to its simplicity and efficiency, especially when dealing with large datasets. However, its assumption of word independence can be limiting in capturing more complex relationships within text data. In scenarios where semantic meaning and context play a crucial role, more advanced algorithms such as recurrent neural networks (RNNs) or transformer-based models like BERT and GPT-3 might outperform Naive Bayes. These deep learning models excel in capturing intricate patterns and contextual nuances in language, making them particularly effective for sophisticated NLP tasks.\n\u2026see more\n``\n[\nLike\n](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&amp;trk=article-ssr-frontend-x-article_contribution-social-activity_like-cta)\n![Like]()Like\n![Celebrate]()Celebrate\n![Support]()Support\n![Love]()Love\n![Insightful]()Insightful\n![Funny]()Funny\n````\n![]()![]()14\n``````````````\n* [![Contributor profile photo]()![Contributor profile photo]()\nShravan Kumar K.\nAI Leader | AI Speaker | IIT Madras - IIM Bangalore Alumnus | Associate Director at Novartis| Gen AI | 40 under 40 DS, AIM-25\n](https://in.linkedin.com/in/shravankoninti?trk=article-ssr-frontend-x-article)\n* Copy link to contribution\n* [Report contribution](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/advice/1/what-most-effective-text-classification-algorithms&...",
      "url": "https://www.linkedin.com/advice/1/what-most-effective-text-classification-algorithms"
    }
  ]
}