{
  "query": "What are the most effective ensemble strategies for text classification competitions that combine TF-IDF features with tabular metadata?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensembling TF\u2011IDF text vectors together with tabular metadata works best when the two feature types are first merged into a single training matrix (e.g., by horizontally stacking the sparse TF\u2011IDF matrix with the dense metadata columns using a\u202f`ColumnTransformer` or `hstack`).\u202fOnce the combined representation is built, the most reliable competition\u2011level strategies are\u202f**heterogeneous voting ensembles** and **stacked generalization**.\n\nA common recipe is to train a diverse set of base classifiers on the merged features \u2013 for example a linear\u202fSVM or Logistic Regression that excels on high\u2011dimensional sparse text, together with tree\u2011based learners (Random\u202fForest, Gradient\u202fBoosting, AdaBoost) that can exploit the numeric metadata.\u202fThe predictions of these models are then fused with a hard or soft voting classifier, which has been shown to raise accuracy to the high\u201190\u202f% range on hate\u2011speech and other Kaggle tasks\u202f([ResearchGate](https://www.researchgate.net/publication/377742208_Ensemble_Text_Classification_with_TF-I...)).\u202fA Kaggle notebook demonstrates the same pattern, using a hard\u2011voting classifier and a stacking model that treats an\u202fXGBClassifier as the meta\u2011learner to combine the base outputs\u202f([Kaggle](https://www.kaggle.com/code/dipankarsrirag/ensemble-methods-text-classification-0-74-acc)).\n\nStacking can be taken a step further by feeding the cross\u2011validated predictions of the base models into a second\u2011level learner (e.g., LightGBM, CatBoost, or XGBoost).\u202fThe \u201cstacking\u2011CV\u201d workflow described in a Kaggle tutorial shows how to generate level\u20111 predictions, then train a meta\u2011model that learns to weight each base classifier optimally\u202f([Kaggler](https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html)).\u202fIn practice, the strongest pipelines combine: (1) TF\u2011IDF\u202f+\u202fmetadata concatenation, (2) a mix of linear and tree\u2011based base models, (3) a voting or stacking layer (soft voting or XGB meta\u2011learner).\u202fThese approaches consistently outperform single\u2011model baselines in recent text\u2011classification competitions.",
      "url": ""
    },
    {
      "title": "Ensemble Methods|Text Classification|0.74 Acc. - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/dipankarsrirag/ensemble-methods-text-classification-0-74-acc"
    },
    {
      "title": "Stacking Ensemble",
      "text": "Stacking Ensemble | Kaggler TV Blog\n[Kaggler TV Blog](https://kaggler.com/)\n# Stacking Ensemble\nA tutorial of stacking ensemble (a.k.a. stacked generalization)\nApr 26, 2021\u20225 min read\n**</i>[notebook](https://kaggler.com/categories/#notebook)[kaggle](https://kaggler.com/categories/#kaggle)\n[![View On GitHub](https://kaggler.com/assets/badges/github.svg)](https://github.com/kaggler-tv/blog/tree/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Binder](https://kaggler.com/assets/badges/binder.svg)](https://mybinder.org/v2/gh/kaggler-tv/blog/master?filepath=_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Colab](https://kaggler.com/assets/badges/colab.svg)](https://colab.research.google.com/github/kaggler-tv/blog/blob/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n* [Part 1: Data Loading &amp; Feature Engineering](#Part-1:-Data-Loading-&-Feature-Engineering)\n* [Part 2: Level-1 Base Model Training](#Part-2:-Level-1-Base-Model-Training)\n* [Part 3: Level-2 Stacking](#Part-3:-Level-2-Stacking)\nThis notebook was originally published[here](https://www.kaggle.com/jeongyoonlee/stacking-ensemble)at Kaggle.\nThis notebook shows how to perform stacking ensemble (a.k.a. stacked generalization).\nIn[Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking), @remekkinas shares how to do stacking ensemble using`MLExtend'`s`StackingCVClassifier`.\nTo demonstrate how stacking works, this notebook shows how to prepare the baseline model predictions using cross-validation (CV), then use them for level-2 stacking. It trains four classifiers, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost as level-1 base models. It also uses CV predictions of two models, LightGBM with DAE features and supervised DAE trained from my previous notebook,[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)to show why keeping CV predictions for**every**model is important. :)\nThe contents of this notebook are as follows:\n1. **Feature Engineering**: Same as in the[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)and[AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb).\n2. **Level-1 Base Model Training**: Training four base models, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost using the same 5-fold CV.\n3. **Level-2 Stacking**: Training the LightGBM model with CV predictions of base models, original features, and DAE features. Performing feature selection and hyperparameter optimization using`Kaggler`'s`AutoLGB`.\nThis notebook is inspired and/or based on other Kagglers' notebooks as follows:\n* [TPS-APR21-EDA+MODEL](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model)by @udbhavpangotra\n* [Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking)by @remekkinas\n* [TPS Apr 2021 pseudo labeling/voting ensemble](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble?scriptVersionId=60616606)by @hiro5299834\nThanks!\n# [](#Part-1:-Data-Loading-&amp;-Feature-Engineering)Part 1: Data Loading &amp; Feature Engineering[](#Part-1:-Data-Loading-&amp;-Feature-Engineering)\n```\nfromcatboostimportCatBoostClassifierfromjoblibimportdumpimportlightgbmaslgbfromlightgbmimportLGBMClassifierfrommatplotlibimportpyplotaspltimportnumpyasnpimportpandasaspdfrompathlibimportPathfromsklearn.ensembleimportRandomForestClassifierfromsklearn.ensembleimportExtraTreesClassifierfromsklearn.metricsimportroc\\_auc\\_score,confusion\\_matrixfromsklearn.model\\_selectionimportStratifiedKFoldfromsklearn.preprocessingimportStandardScalerimportwarnings\n```\n```\n!pip install kaggler\n```\n```\nimportkagglerfromkaggler.modelimportAutoLGBfromkaggler.preprocessingimportLabelEncoderprint(f'Kaggler:{kaggler.\\_\\_version\\_\\_}')\n```\n```\nwarnings.simplefilter('ignore')pd.set\\_option('max\\_columns',100)\n```\n```\ndata\\_dir=Path('/kaggle/input/tabular-playground-series-apr-2021/')trn\\_file=data\\_dir/'train.csv'tst\\_file=data\\_dir/'test.csv'sample\\_file=data\\_dir/'sample\\_submission.csv'pseudo\\_label\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/tps04-sub-006.csv'dae\\_feature\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/dae.csv'lgb\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.val.txt'lgb\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.tst.txt'sdae\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.val.txt'sdae\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.tst.txt'target\\_col='Survived'id\\_col='PassengerId'feature\\_name='dae'algo\\_name='esb'model\\_name=f'{algo\\_name}\\_{feature\\_name}'feature\\_file=f'{feature\\_name}.csv'predict\\_val\\_file=f'{model\\_name}.val.txt'predict\\_tst\\_file=f'{model\\_name}.tst.txt'submission\\_file=f'{model\\_name}.sub.csv'\n```\n```\nn\\_fold=5seed=42n\\_est=1000encoding\\_dim=128\n```\n```\ntrn=pd.read\\_csv(trn\\_file,index\\_col=id\\_col)tst=pd.read\\_csv(tst\\_file,index\\_col=id\\_col)sub=pd.read\\_csv(sample\\_file,index\\_col=id\\_col)pseudo\\_label=pd.read\\_csv(pseudo\\_label\\_file,index\\_col=id\\_col)dae\\_features=np.loadtxt(dae\\_feature\\_file,delimiter=',')lgb\\_dae\\_predict\\_val=np.loadtxt(lgb\\_dae\\_predict\\_val\\_file)lgb\\_dae\\_predict\\_tst=np.loadtxt(lgb\\_dae\\_predict\\_tst\\_file)sdae\\_dae\\_predict\\_val=np.loadtxt(sdae\\_dae\\_predict\\_val\\_file)sdae\\_dae\\_predict\\_tst=np.loadtxt(sdae\\_dae\\_predict\\_tst\\_file)print(trn.shape,tst.shape,sub.shape,pseudo\\_label.shape,dae\\_features.shape)print(lgb\\_dae\\_predict\\_val.shape,lgb\\_dae\\_predict\\_tst.shape)print(sdae\\_dae\\_predict\\_val.shape,sdae\\_dae\\_predict\\_tst.shape)\n```\n```\ntst[target\\_col]=pseudo\\_label[target\\_col]n\\_trn=trn.shape[0]df=pd.concat([trn,tst],axis=0)df.head()\n```\nLoading 128 DAE features generated from[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder/).\n```\ndf\\_dae=pd.DataFrame(dae\\_features,columns=[f'enc\\_{x}'forxinrange(encoding\\_dim)])print(df\\_dae.shape)df\\_dae.head()\n```\nFeature engineering using @udbhavpangotra's[code](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model).\n```\ndf['Embarked']=df['Embarked'].fillna('No')df['Cabin']=df['Cabin'].fillna('\\_')df['CabinType']=df['Cabin'].apply(lambdax:x[0])df.Ticket=df.Ticket.map(lambdax:str(x).split()[0]iflen(str(x).split())&gt;1else'X')df['Age'].fillna(round(df['Age'].median()),inplace=True,)df['Age']=df['Age'].apply(round).astype(int)# Fare, fillna with mean valuefare\\_map=df[['Fare','Pclass']].dropna().groupby('Pclass').median().to\\_dict()df['Fare']=df['Fare'].fillna(df['Pclass'].map(fare\\_map['Fare']))df['FirstName']=df['Name'].str.split(', ').str[0]df['SecondName']=df['Name'].str.split(', ').str[1]df['n']=1gb=df.groupby('FirstName')df\\_names=gb['n'].sum()df['SameFirstName']=df['FirstName'].apply(lambdax:df\\_names[x]).fillna(1)gb=df.groupby('SecondName')df\\_names=gb['n'].sum()df['SameSecondName']=df['SecondName'].apply(lambdax:df\\_names[x]).fillna(1)df['Sex']=(df['Sex']=='male').astype(int)df['FamilySize']=df.SibSp+df.Parch+1feature\\_cols=['Pclass','Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName','SameSecondName','Sex','FamilySize','FirstName','SecondName']cat\\_cols=['Pclass','Embarked','CabinType','Ticket','FirstName','SecondName']num\\_cols=[xforxinfeature\\_colsifxnotincat\\_cols]print(len(feature\\_cols),len(cat\\_cols),len(num\\_cols))\n```\nApplying`log2(1 + x)`for numerical features and label-encoding categorical features using`kaggler.preprocessing.LabelEncoder`, which handles`NaN`s and groups rare categories together.\n```\nforcolin['SameFirstName','SameSecondName','Fare','FamilySize','Parch','SibSp']:df[col]=np.log2(1+df[col])scaler=StandardScaler()df[num\\_cols]=...",
      "url": "https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html"
    },
    {
      "title": "Ensemble Text Classification with TF-IDF Vectorization for Hate ...",
      "text": "<div><section><div><div><p>The development of artificial intelligence (AI) has changed how hate speech is detected. In hate speech identification using machine learning, a number of methods are used to automatically find text that uses vocabulary that is considered to be derogatory, discriminatory, or motivated by hatred. Supervised learning techniques like neural networks, decision trees, and SVMs need a labelled dataset comprising samples of hate speech and non-hate speech. This project investigates the use of AI and machine learning techniques to automatically detect material that uses offensive, intolerant, or hostile words. A voting classifier and TF-IDF representations are combined to improve classification accuracy. The ensemble of classifiers, powered by AI approaches, shows impressive accuracy in identifying hate speech by training five different classifiers (Random Forest, Bagging, Support Vector Machine, AdaBoost, and Gradient Boosting) on a labelled dataset of tweets. The TF-IDF representation prioritises textual terms, whereas the ensemble method uses classifier diversity to capture distinctive patterns. Results from experiments show the strategy's effectiveness, with precision 0.95, recall 0.96, f1-score 0.95 and accuracy 0.97 for detecting hate speech. By successfully utilising AI's capacity to fight hate speech, this research helps the development of a diverse and secure online environment. The suggested approach works well for automatically identifying hate speech, making the internet a safer and more welcoming place for all users.</p></div><div><p><strong>Discover the world's research</strong></p><ul><li>25+ million members</li><li>160+ million publication pages</li><li>2.3+ billion citations</li></ul><p><a href=\"https://www.researchgate.net/publication/signup.SignUp.html\"><span>Join for free</span></a></p></div></div><section><span></span><a href=\"https://www.researchgate.net/publication/publication/377742208_Ensemble_Text_Classification_with_TF-IDF_Vectorization_for_Hate_Speech_Detection_in_Social_Media#read-preview\"></a><div>\n \n \n \n \n \n \n <div><div><p>Ensemble Text Classification with TF-IDF </p><p>Vectorization for Hate Speech Detection in Social </p><p>Media </p><p>Sathishkumar R<span>1</span>, Karthikeyan T<span>2</span>, Praveen kumar P<span>3</span>, Shamsundar S M<span>4</span> <span> </span></p><p><span> </span> <span> </span> <span> </span> <span> </span><span>Department of Co<span></span>mputer Science and Engi<span></span>neering<span>1,4</span> </span></p><p>Department of Co<span></span>mputer Science and Business Sy<span></span>stems<span>2 </span></p><p>Department of Info<span></span>rmation Technology<span>3 </span></p><p>Manakula Vinay<span></span>agar Institute<span></span> of Technolo<span></span>gy<span>1,4</span>, Puducherry, </p><p>Sri Manakula Vin<span></span>ayag<span></span>ar Engineering College<span>2,3</span>, Puduc<span></span>herry. </p><p>sathishma<span></span>il26@g<span></span>mail.com<span>1</span><span>, <span></span><span>karthi4c<span></span>se@gmail.c<span></span>om<span>2</span><span>, </span>prhsel@g<span></span>mail.com<span>3</span><span>, <span></span><span>shamsundar@g<span></span>mail.com<span>4</span><span> </span></span></span></span></span></p><p>Abstract<span>\u2014</span><span>The<span></span> <span> </span>developm<span></span>ent <span> </span>of <span> </span>artificial <span> </span>intelligence <span> </span>(AI) </span></p><p>has <span> </span>changed <span> </span>how <span> </span>hate <span> </span>speech <span> </span>is <span> </span>detected. <span> </span>In <span> </span>hate <span> </span>speech </p><p>identification u<span></span>sing machine lea<span></span>rning, a numbe<span></span>r of methods are<span></span> </p><p>used <span> </span>to <span> </span>autom<span></span>atically <span> </span>find <span> </span>te<span></span>xt <span> </span>that <span> </span>uses <span> </span>v<span></span>ocabulary <span> </span>th<span></span>at <span> </span>is </p><p>considered <span></span>to <span></span>be <span></span>derogatory, <span></span>discriminatory<span></span>, <span> </span>o<span></span>r <span></span>motivated <span></span>by </p><p>hatred. <span></span>Supervised <span></span>learning <span> </span>techn<span></span>iques <span></span>like <span> </span>neural <span></span>networks, </p><p>decision <span></span>trees, <span> </span>and<span></span> <span> </span>SV<span></span>Ms <span></span>need <span> </span>a<span></span> <span> </span>labe<span></span>lled <span></span>dataset <span> </span>com<span></span>prising </p><p>samples <span> </span>of <span> </span>hate <span> </span>speech <span> </span>and <span> </span>non-hate <span> </span>speech. <span> </span>This <span> </span>project </p><p>investigates the <span></span>use <span></span>of <span></span>AI <span></span>and <span></span>machine <span></span>learning<span></span> <span></span>techniques to </p><p>automatically detect material that uses offensive, intolerant, or </p><p>hostile <span></span>words. <span></span>A <span></span>voting <span></span>classifier <span></span>and <span></span>TF-IDF <span></span>representatio<span></span>ns </p><p>are combined to improve classifica<span></span>tion accuracy. The ensemble </p><p>of <span> </span>classifiers, <span> </span>powered <span> </span>by <span> </span>AI <span> </span>approaches, <span> </span>shows <span> </span>impress<span></span>ive </p><p>accuracy <span></span>in <span> </span>ide<span></span>ntifying <span></span>hate <span></span>speech <span> </span>by<span></span> <span></span>training <span></span>five <span> </span>different<span></span> </p><p>classifiers (Random Forest, Bagging, Support Vector Machine, </p><p>AdaBoost, <span> </span>and <span> </span>Gradient <span> </span>Boosting) <span> </span>on <span> </span>a <span> </span>labelled <span> </span>dataset <span> </span>of </p><p>tweets. <span> </span>The <span> </span>T<span></span>F-IDF <span> </span>representa<span></span>tion <span> </span>prioriti<span></span>ses <span> </span>textual <span> </span>te<span></span>rms, </p><p>whereas <span> </span>the <span> </span>ensemble <span> </span>method <span> </span>uses <span> </span>classifier <span> </span>divers<span></span>ity <span> </span>to </p><p>capture <span> </span>distinct<span></span>ive <span> </span>patterns.<span></span> <span> </span>Results <span> </span>from <span> </span>exper<span></span>iments <span> </span>show </p><p>the st<span></span>rategy's<span></span> <span></span>effectiveness<span></span>, <span></span>with precision <span></span>0.95, recall <span></span>0.96, f1-</p><p>score <span> </span>0.95 <span> </span>and <span> </span>accuracy <span> </span>0.97 <span> </span>for <span> </span>dete<span></span>cting <span> </span>hate <span> </span>speech. <span> </span>By </p><p>successfully<span></span> <span> </span>utilising <span> </span>AI's <span> </span>capac<span></span>ity <span> </span>to <span> </span>fight<span></span> <span> </span>hate <span> </span>speech,<span></span> <span> </span>this </p><p>research helps <span></span>the <span></span>developm<span></span>ent <span></span>of a <span></span>diverse <span></span>and secure <span></span>online </p><p>environment. <span> </span>The <span> </span>suggested <span> </span>approach <span> </span>works <span> </span>well <span> </span>for </p><p>automatically <span></span>identifying <span> </span>hate<span></span> <span> </span>speech, <span></span>making <span> </span>the <span></span>internet <span> </span>a </p><p>safer and mo<span></span>re welcoming p<span></span>lace for all user<span></span>s. </p><p>Keywords<span>\u2014</span><span> <span> </span>Hate <span> </span>Speech, <span> </span>Machin<span></span>e <span> </span>learning, <span> </span>SVM, <span> </span>Naive </span></p><p>Bayes, <span></span>Random <span> </span>For<span></span>est <span> </span>C<span></span>lassifier, <span></span>Stochastic <span> </span>G<span></span>radi...",
      "url": "https://www.researchgate.net/publication/377742208_Ensemble_Text_Classification_with_TF-IDF_Vectorization_for_Hate_Speech_Detection_in_Social_Media"
    },
    {
      "title": "Comparative Study of Ensemble Learning Techniques for Text ...",
      "text": "<div><div>\n<div>\n</div>\n<div>\n<div>\n<div>\n<h3>IEEE Account</h3>\n<ul>\n<li><a href=\"https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Change Username/Password</a></li>\n<li><a href=\"https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Update Address</a></li>\n</ul>\n</div>\n<div>\n<h3>Purchase Details</h3>\n<ul>\n<li><a href=\"https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Payment Options</a></li>\n<li><a href=\"https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Order History</a></li>\n<li><a href=\"https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp\">View Purchased Documents</a></li>\n</ul>\n</div>\n<div>\n<h3>Profile Information</h3>\n<ul>\n<li><a href=\"https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Communications Preferences</a></li>\n<li><a href=\"https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Profession and Education</a></li>\n<li><a href=\"https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE Xplore\">Technical Interests</a></li>\n</ul>\n</div>\n<div>\n<h3>Need Help?</h3>\n<ul>\n<li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>\n<li><strong>Worldwide: </strong> +1 732 981 0060<br/>\n</li>\n<li><a href=\"https://ieeexplore.ieee.org/xpl/contact\">Contact &amp; Support</a></li>\n</ul>\n</div>\n</div>\n<div>\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore\">About IEEE <em>Xplore</em></a></li>\n<li><a href=\"https://ieeexplore.ieee.org/xpl/contact\">Contact Us</a></li>\n<li><a href=\"https://ieeexplore.ieee.org/Xplorehelp\">Help</a></li>\n<li><a href=\"https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement\">Accessibility</a></li>\n<li><a href=\"https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use\">Terms of Use</a></li>\n<li><a href=\"http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html\">Nondiscrimination Policy</a></li>\n<li><a href=\"https://ieeexplore.ieee.org/xpl/sitemap.jsp\">Sitemap</a></li>\n<li><a href=\"http://www.ieee.org/about/help/security_privacy.html\">Privacy &amp; Opting Out of Cookies</a></li>\n</ul>\n<p>\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br/>\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n</p>\n</div>\n</div>\n</div></div>",
      "url": "https://ieeexplore.ieee.org/document/9692306"
    },
    {
      "title": "An Ensemble Framework for Text Classification - MDPI",
      "text": "An Ensemble Framework for Text Classification\n**\nNext Article in Journal\n[Evaluation of Academic Stress Employing Network and Time Series Analysis on EEG Data](https://www.mdpi.com/2078-2489/16/2/86)\nNext Article in Special Issue\n[CNN-Based Optimization for Fish Species Classification: Tackling Environmental Variability, Class Imbalance, and Real-Time Constraints](https://www.mdpi.com/2078-2489/16/2/154)\n**\nPrevious Article in Journal\n[Fine-Tuning QurSim on Monolingual and Multilingual Models for Semantic Search](https://www.mdpi.com/2078-2489/16/2/84)\nPrevious Article in Special Issue\n[Fitness Approximation Through Machine Learning with Dynamic Adaptation to the Evolutionary State](https://www.mdpi.com/2078-2489/15/12/744)\n## Journals\n[Active Journals](https://www.mdpi.com/about/journals)[Find a Journal](https://www.mdpi.com/about/journalfinder)[Journal Proposal](https://www.mdpi.com/about/journals/proposal)[Proceedings Series](https://www.mdpi.com/about/proceedings)\n[## Topics\n](https://www.mdpi.com/topics)\n## Information\n[For Authors](https://www.mdpi.com/authors)[For Reviewers](https://www.mdpi.com/reviewers)[For Editors](https://www.mdpi.com/editors)[For Librarians](https://www.mdpi.com/librarians)[For Publishers](https://www.mdpi.com/publishing_services)[For Societies](https://www.mdpi.com/societies)[For Conference Organizers](https://www.mdpi.com/conference_organizers)\n[Open Access Policy](https://www.mdpi.com/openaccess)[Institutional Open Access Program](https://www.mdpi.com/ioap)[Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines)[Editorial Process](https://www.mdpi.com/editorial_process)[Research and Publication Ethics](https://www.mdpi.com/ethics)[Article Processing Charges](https://www.mdpi.com/apc)[Awards](https://www.mdpi.com/awards)[Testimonials](https://www.mdpi.com/testimonials)\n[## Author Services\n](https://www.mdpi.com/authors/english)\n## Initiatives\n[Sciforum](https://sciforum.net)[MDPI Books](https://www.mdpi.com/books)[Preprints.org](https://www.preprints.org)[Scilit](https://www.scilit.com)[SciProfiles](https://sciprofiles.com)[Encyclopedia](https://encyclopedia.pub)[JAMS](https://jams.pub)[Proceedings Series](https://www.mdpi.com/about/proceedings)\n## About\n[Overview](https://www.mdpi.com/about)[Contact](https://www.mdpi.com/about/contact)[Careers](https://careers.mdpi.com)[News](https://www.mdpi.com/about/announcements)[Press](https://www.mdpi.com/about/press)[Blog](http://blog.mdpi.com/)\n[Sign In / Sign Up](https://www.mdpi.com/user/login)\n## Notice\nYou can make submissions to other journals[here](https://susy.mdpi.com/user/manuscripts/upload).\n*clear*\n## Notice\nYou are accessing a machine-readable page. In order to be human-readable, please install an RSS reader.\nContinueCancel\n*clear*\nAll articles published by MDPI are made immediately available worldwide under an open access license. No special permission is required to reuse all or part of the article published by MDPI, including figures and tables. For articles published under an open access Creative Common CC BY license, any part of the article may be reused without permission provided that the original article is clearly cited. For more information, please refer to[https://www.mdpi.com/openaccess](https://www.mdpi.com/openaccess).\nFeature papers represent the most advanced research with significant potential for high impact in the field. A Feature Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for future research directions and describes possible research applications.\nFeature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive positive feedback from the reviewers.\nEditor\u2019s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. Editors select a small number of articles recently published in the journal that they believe will be particularly interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the most exciting work published in the various research areas of the journal.\nOriginal Submission Date Received:.\n[![](https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1765786175 \"MDPI Open Access Journals\")](https://www.mdpi.com/)\n* [Journals](https://www.mdpi.com/about/journals)\n* * [Active Journals](https://www.mdpi.com/about/journals)\n* [Find a Journal](https://www.mdpi.com/about/journalfinder)\n* [Journal Proposal](https://www.mdpi.com/about/journals/proposal)\n* [Proceedings Series](https://www.mdpi.com/about/proceedings)\n* [Topics](https://www.mdpi.com/topics)\n* [Information](https://www.mdpi.com/authors)\n* * [For Authors](https://www.mdpi.com/authors)\n* [For Reviewers](https://www.mdpi.com/reviewers)\n* [For Editors](https://www.mdpi.com/editors)\n* [For Librarians](https://www.mdpi.com/librarians)\n* [For Publishers](https://www.mdpi.com/publishing_services)\n* [For Societies](https://www.mdpi.com/societies)\n* [For Conference Organizers](https://www.mdpi.com/conference_organizers)\n* [Open Access Policy](https://www.mdpi.com/openaccess)\n* [Institutional Open Access Program](https://www.mdpi.com/ioap)\n* [Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines)\n* [Editorial Process](https://www.mdpi.com/editorial_process)\n* [Research and Publication Ethics](https://www.mdpi.com/ethics)\n* [Article Processing Charges](https://www.mdpi.com/apc)\n* [Awards](https://www.mdpi.com/awards)\n* [Testimonials](https://www.mdpi.com/testimonials)\n* [Author Services](https://www.mdpi.com/authors/english)\n* [Initiatives](https://www.mdpi.com/about/initiatives)\n* * [Sciforum](https://sciforum.net)\n* [MDPI Books](https://www.mdpi.com/books)\n* [Preprints.org](https://www.preprints.org)\n* [Scilit](https://www.scilit.com)\n* [SciProfiles](https://sciprofiles.com)\n* [Encyclopedia](https://encyclopedia.pub)\n* [JAMS](https://jams.pub)\n* [Proceedings Series](https://www.mdpi.com/about/proceedings)\n* [About](https://www.mdpi.com/about)\n* * [Overview](https://www.mdpi.com/about)\n* [Contact](https://www.mdpi.com/about/contact)\n* [Careers](https://careers.mdpi.com)\n* [News](https://www.mdpi.com/about/announcements)\n* [Press](https://www.mdpi.com/about/press)\n* [Blog](http://blog.mdpi.com/)\n[Sign In / Sign Up](https://www.mdpi.com/user/login)[Submit](< https://susy.mdpi.com/user/manuscripts/upload?journal=information\n>)\nSearchfor Articles:\nTitle / Keyword\nAuthor / Affiliation / Email\nJournal\nAll JournalsAccounting and AuditingAcousticsActa Microbiologica Hellenica (AMH)ActuatorsAdhesivesAdministrative SciencesAdolescentsAdvances in Respiratory Medicine (ARM)AerobiologyAerospaceAgricultureAgriEngineeringAgrochemicalsAgronomyAIAI ChemistryAI for EngineeringAI in EducationAI in MedicineAI MaterialsAI SensorsAirAlgorithmsAllergiesAlloysAnalogAnalyticaAnalyticsAnatomiaAnesthesia ResearchAnimalsAntibioticsAntibodiesAntioxidantsApplied BiosciencesApplied MechanicsApplied MicrobiologyApplied NanoApplied SciencesApplied System Innovation (ASI)AppliedChemAppliedMathAppliedPhysAquaculture JournalArchitectureArthropodaArtsAstronauticsAstronomyAtmosphereAtomsAudiology ResearchAutomationAxiomsBacteriaBatteriesBehavioral SciencesBeveragesBig Data and Cognitive Computing (BDCC)BioChemBioengineeringBiologicsBiologyBiology and Life Sciences ForumBiomassBiomechanicsBioMedBiomedicinesBioMedInformaticsBiomimeticsBiomoleculesBiophysicaBioresources and BioproductsBiosensorsBiosphereBioTechBirdsBlockchainsBrain SciencesBuildingsBusinessesC (Journal of Carbon Research)CancersCardiogeneticsCardiovascular MedicineCatalystsCellsCeramicsChallengesChemEngineeringChemistryChemistry ProceedingsChemosensorsChildrenChipsCivilEngClean Technologies (Clean Technol.)ClimateClinical and Translational Neuroscience (CTN)Clinical BioenergeticsClinics and PracticeClocks &amp; SleepCoastsCoatingsColloids and InterfacesColorantsCommodities...",
      "url": "https://www.mdpi.com/2078-2489/16/2/85"
    },
    {
      "title": "How do I do classification using TfidfVectorizer plus metadata in ...",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [How do I do classification using TfidfVectorizer plus metadata in practice?](https://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked10 years, 8 months ago\n\nModified [8 years, 1 month ago](https://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice?lastactivity)\n\nViewed\n3k times\n\nPart of [NLP](https://stackoverflow.com/collectives/nlp) Collective\n\n7\n\nI am using trying to classify some documents into two classes, in which I use TfidfVectorizer as an feature extraction technique.\n\nInput data consists of rows of data containing about a dozen fields of float data, label and the text blob of the body of the document. In order of use the body, I applied the TfidfVectorizer and got a sparse matrix (which I can examine by converting to array via toarray() ). This matrix is usually very large, thousands by thousands dimensions - let's call this F which has size 1000 x 15000.\n\nTo use a classifier in Scikit, I give it an input matrix X which is (number of rows \\* number of features). If I do not use the body, I have maybe an X of size 1000 x 15.\n\nHere is the problem, suppose I append horizontally stack this F to X, so X will become 1000 x 15015, which introduces a few problems:\n1) The first 15 features will be playing a very little role now;\n2) Out-of-memory;\n\nScikit has provided an example where using solely the TfidfVectorizer input, but shed no light on how to use it _along side_ the metadata.\n\nMy question is: **How do you use the TfidfVectorizer output along with the metadata to fit into a classifier for training?**\n\nThank you.\n\n- [machine-learning](https://stackoverflow.com/questions/tagged/machine-learning)\n- [classification](https://stackoverflow.com/questions/tagged/classification)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [tf-idf](https://stackoverflow.com/questions/tagged/tf-idf)\n\n[Share](https://stackoverflow.com/q/19466868)\n\n[Improve this question](https://stackoverflow.com/posts/19466868/edit)\n\nFollow\n\nasked Oct 19, 2013 at 14:01\n\n[![log0's user avatar](https://www.gravatar.com/avatar/68d36a84c5e63ac2fda6926c38e2e8cb?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/797444/log0)\n\n[log0](https://stackoverflow.com/users/797444/log0) log0\n\n55122 gold badges77 silver badges1111 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice)\u00a0\\|\n\n## 3 Answers 3\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n8\n\n1. Extract bag of words (tf-idf) features, call these `X_tfidf`.\n\n2. Extract metadata features, call these `X_metadata`.\n\n3. Stack them together:\n\n\n```\nX = scipy.sparse.hstack([X_tfidf, X_metadata])\n\n```\n\n4. If it doesn't work as expected, try re-normalizing:\n\n\n```\nfrom sklearn.preprocessing import normalize\nX = normalize(X, copy=False)\n\n```\n\n\nIf you use a linear estimator such as `LinearSVC`, `LogisticRegression` or `SGDClassifier`, you shouldn't worry about the role that features play in the classification; this is the estimator's work. Linear estimators assign a weight to each individual feature that tells how informative the feature is, i.e. they figure this out for you.\n\n(Non-parametric, distance/similarity-based models such as kernel SVMs or k-NN may have a harder time on such datasets.)\n\n[Share](https://stackoverflow.com/a/19468894)\n\n[Improve this answer](https://stackoverflow.com/posts/19468894/edit)\n\nFollow\n\n[edited Oct 21, 2013 at 12:33](https://stackoverflow.com/posts/19468894/revisions)\n\nanswered Oct 19, 2013 at 17:26\n\n[![Fred Foo's user avatar](https://www.gravatar.com/avatar/f5a1ea24626d10a87d73d47998dc8816?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/166749/fred-foo)\n\n[Fred Foo](https://stackoverflow.com/users/166749/fred-foo) Fred Foo\n\n360k7878 gold badges750750 silver badges841841 bronze badges\n\n4\n\n- Thanks for reply. You're right that I should leave it to the estimator's to figure out. I will try it out, though (probably specific to implementation), the ensembles (RandomForest, GDT) etc doesn't seem to work on sparse matrix with error message : \"A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\", which is a dead-end, Ogriesel already confirmed \\[link\\] [stackoverflow.com/questions/17184079/\u2026](http://stackoverflow.com/questions/17184079/sklearn-cannot-use-encoded-data-in-random-forest-classifier[/link]) .\n\n\u2013\u00a0[log0](https://stackoverflow.com/users/797444/log0)\n\nCommentedOct 20, 2013 at 1:07\n\n- @log0: that's a known defect. Try a linear model, or do some dimensionality reduction.\n\n\u2013\u00a0[Fred Foo](https://stackoverflow.com/users/166749/fred-foo)\n\nCommentedOct 20, 2013 at 8:20\n\n- So it's a bug in Scikit Learn to be fixed? I have always thought about trying to pick up something to contribute maybe, albeit small ... =\\]\n\n\u2013\u00a0[log0](https://stackoverflow.com/users/797444/log0)\n\nCommentedOct 20, 2013 at 11:37\n\n- @log0: we call it a missing feature, but it's definitely something worth implementing and any assistance is welcomed.\n\n\u2013\u00a0[Fred Foo](https://stackoverflow.com/users/166749/fred-foo)\n\nCommentedOct 20, 2013 at 15:47\n\n\n[Add a comment](https://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice)\u00a0\\|\n\n3\n\nThere is no generic way of merging tf-idf descriptors with other type of data, everything depends on your particular model and data:\n\n- Some models are designed to deal with data that can be of arbitrary scales, and so - they use the strongest predictors, no matter if they are just 1% of the whole feature vector. Some decision trees information criterions can be a good example of such methods\n- Some models let you directly \"weight\" the features to make them more important then others, so you can include some expert knowledge in order to weight the meta data with the large non-meta part, by for example N\\_not\\_meta/N\\_meta scale, where N\\_x is the number of x-type features dimensions. SVMs let you do such thing, as they are scale dependant linear models, so simple feature rescaling can have such effect. Also in probabilistic models like Naive Bayes you can force some predictors to be \"strong\" by multiplying their respective \"probability estimates\" by some predefined factor.\n- The more advanced approach would be to create an ensamble of classifiers - one for meta data, and one for the tfidf and some meta-classifier (as the voting scheme for 2 models is rather useless) trained on their outputs\n- You can also simply reduce dimensionality of the second part by performing some dimensionality reduction method (eg. PCA)\n\nThe choice of particular method is strongly problem specific, but as you can see - there are many possibilities and no possibility of selecting simply \"the best one\".\n\nFor out-of-memory problems you shoul...",
      "url": "https://stackoverflow.com/questions/19466868/how-do-i-do-classification-using-tfidfvectorizer-plus-metadata-in-practice"
    },
    {
      "title": "A Simple Ensemble Strategy for LLM Inference - arXiv",
      "text": "<div>\n<div>\n<p><a href=\"https://www.cornell.edu/\"></a>\n</p>\n<div>\n<p><a href=\"https://confluence.cornell.edu/x/ALlRF\">We gratefully acknowledge support from<br/>\nthe Simons Foundation and member institutions.</a>\n</p></div>\n</div>\n<div>\n<h2><a href=\"https://arxiv.org/\">\n</a></h2>\n</div>\n</div>",
      "url": "https://arxiv.org/html/2504.18884v2"
    },
    {
      "title": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2411.01645"
    }
  ]
}