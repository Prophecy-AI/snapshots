{
  "query": "What are the most effective feature engineering techniques for the Random Acts of Pizza Kaggle competition based on winning solutions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The top\u2011scoring submissions for the **Random Acts of Pizza** competition relied heavily on turning the Reddit request data into a rich set of **user\u2011metadata** and **text\u2011derived** variables before feeding them to linear or tree\u2011based classifiers.  Participants extracted information about each requester \u2013 such as account age, total karma, number of previous pizza requests, and other Reddit activity metrics \u2013 and treated these as numeric features that capture a user\u2019s reputation and experience on the platform\u202f([caroarriaga.com](https://www.caroarriaga.com/projects/raop)).  In parallel, the free\u2011form request text was vectorised with standard NLP tools (bag\u2011of\u2011words/TF\u2011IDF, character\u2011 and word\u2011level n\u2011grams) to capture length, word choice, sentiment cues, and narrative structure; the notebook from a 2017 Kaggle project explicitly imports the Scikit\u2011learn text\u2011extraction utilities for this purpose\u202f([notebook.community](https://notebook.community/omaraltaher/kaggle-pizza-project/Random_Acts_of_Pizza_Kaggle_Competition_Project)).  \n\nBecause the raw TF\u2011IDF matrix is high\u2011dimensional, the most successful teams applied **dimensionality reduction** (typically truncated Singular Value Decomposition, i.e., LSA) to obtain a compact set of latent semantic features that still preserve the predictive signal\u202f([caroarriaga.com](https://www.caroarriaga.com/projects/raop)).  These engineered features were then combined in pipelines with models such as Logistic Regression, Random Forests, and XGBoost, which consistently outperformed baseline approaches.  In summary, the winning feature\u2011engineering recipe consists of:  \n\n1. **User metadata** (account age, karma, prior pizza\u2011request count, etc.).  \n2. **Textual features** \u2013 word/character n\u2011grams, TF\u2011IDF vectors, length and readability metrics.  \n3. **Latent semantic reduction** via SVD/LSA to create a lower\u2011dimensional representation.  \n\nUsing this blend of structured and unstructured features proved most effective for achieving top scores on the competition leaderboard.",
      "url": ""
    },
    {
      "title": "Random Acts of Pizza | Caro Arriaga",
      "text": "* * *\n**Random Acts of Pizza** is a project in which we trained multiple classification models to predict if a user gets a free pizza from the Reddit random acts of pizza community. This work was the final project of the course Applied Machine Learning. _Authors: Carolina Arriaga, Kanika Mahajan._\n**Overview**\n> We used data from the Kaggle competition Random Acts of Pizza to train multiple classifiers to predict a binary class. We used feature engineering by collecting user metadata and adding text-based features. We also applied dimensionality reduction using SVD. We explored four models: Logistic regression, Random Forests (Ada and XGB), Multilayer Neural Network, and Dense Neural Network. Finally, we proceeded with the hyperparameter tuning. We were able to predict whether a user would get a pizza or not.\n* * *\n* * *\n## Templates (for web app):\nLoading\u2026\n# Error\nSorry, an error occurred while loading .\nForwardBack[Permalink](https://www.caroarriaga.com/projects/raop/)Dark Mode",
      "url": "https://www.caroarriaga.com/projects/raop"
    },
    {
      "title": "| notebook.community",
      "text": "| notebook.community https://notebook.community/omaraltaher/kaggle-pizza-project/Random_Acts_of_Pizza_Kaggle_Competition_Project\n| notebook.community\nNone\n2017-01-01T00:00:00-20:17\n# W207 Summer 2017 Final Project iPython notebook\n\n### Omar Al Taher, Ted Pham, Chris SanChez\n\n\nIn\u00a0[1]:\n\n# This tells matplotlib not to try opening a new window for each plot.\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# General libraries.\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\n\n# SK-learn libraries for learning.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV #update module model_selection\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# SK-learn libraries for evaluation.\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\n\nfrom sklearn import preprocessing\nfrom sklearn.mixture import GMM\n\n# SK-learn libraries for feature extraction from text.\nfrom sklearn.feature_extraction.text import *\n\n\n# I. Background:\n\n#### This project aims to predict whether a reddit post asking for pizzas would get funded. Since it's a binary classification problem, we will explore several algorithms with a focus on logistic regression. In particular, we will look into details how to extract features from text.\n\n# II. Data Pre-Processing:\n\n#### The data in its raw form consists of 4040 observations of 31 features. The original columns consist of 19 integer values, 4 floats, and 8 objects (there is one boolean column which is the outcome variable). In order to extract predictive value from the dataset a good deal of pre-processing and feature engineering was required. A walkthrough of the various steps taken follows below in a narrative format:\n\n\nIn\u00a0[2]:\n\n#load json training data into pandas dataframe\ndf = pd.read_json('train.json')\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4040 entries, 0 to 4039\nData columns (total 32 columns):\ngiver_username_if_known 4040 non-null object\nnumber_of_downvotes_of_request_at_retrieval 4040 non-null int64\nnumber_of_upvotes_of_request_at_retrieval 4040 non-null int64\npost_was_edited 4040 non-null int64\nrequest_id 4040 non-null object\nrequest_number_of_comments_at_retrieval 4040 non-null int64\nrequest_text 4040 non-null object\nrequest_text_edit_aware 4040 non-null object\nrequest_title 4040 non-null object\nrequester_account_age_in_days_at_request 4040 non-null float64\nrequester_account_age_in_days_at_retrieval 4040 non-null float64\nrequester_days_since_first_post_on_raop_at_request 4040 non-null float64\nrequester_days_since_first_post_on_raop_at_retrieval 4040 non-null float64\nrequester_number_of_comments_at_request 4040 non-null int64\nrequester_number_of_comments_at_retrieval 4040 non-null int64\nrequester_number_of_comments_in_raop_at_request 4040 non-null int64\nrequester_number_of_comments_in_raop_at_retrieval 4040 non-null int64\nrequester_number_of_posts_at_request 4040 non-null int64\nrequester_number_of_posts_at_retrieval 4040 non-null int64\nrequester_number_of_posts_on_raop_at_request 4040 non-null int64\nrequester_number_of_posts_on_raop_at_retrieval 4040 non-null int64\nrequester_number_of_subreddits_at_request 4040 non-null int64\nrequester_received_pizza 4040 non-null bool\nrequester_subreddits_at_request 4040 non-null object\nrequester_upvotes_minus_downvotes_at_request 4040 non-null int64\nrequester_upvotes_minus_downvotes_at_retrieval 4040 non-null int64\nrequester_upvotes_plus_downvotes_at_request 4040 non-null int64\nrequester_upvotes_plus_downvotes_at_retrieval 4040 non-null int64\nrequester_user_flair 994 non-null object\nrequester_username 4040 non-null object\nunix_timestamp_of_request 4040 non-null int64\nunix_timestamp_of_request_utc 4040 non-null int64\ndtypes: bool(1), float64(4), int64(19), object(8)\nmemory usage: 1013.9+ KB\n\n\n#### We'll start by transforming the outcome variable into a binary variable.\n\n\nIn\u00a0[3]:\n\ndf['requester_received_pizza'] = np.where(df['requester_received_pizza'] == True, 1, 0)\ndf['requester_received_pizza'].value_counts()\n\n\nOut[3]:\n\n0 3046\n1 994\nName: requester_received_pizza, dtype: int64\n\n\n#### Next, we'll remove all the \"_at_retrieval\" columns from the dataset as they are not found in the test data set and therefore represent data that is not avaialable at the time of the request for pizza.\n\n\nIn\u00a0[4]:\n\ngood_indexes = []\nfor i, name in enumerate(df.columns):\n if re.findall('retrieval', name):\n pass\n else:\n good_indexes.append(i)\n\n# Remove at_retrieval fields from dataframce df\ncolumns = df.columns[good_indexes]\ndf = df.loc[:,columns]\n\n\n#### We also found that there were several other columns that were not needed for predictive power, and we therefore removed them as well.\n\n\nIn\u00a0[5]:\n\n#Drop six more columns from dataset\ndf.drop(['giver_username_if_known', 'post_was_edited', 'request_id',\n 'requester_user_flair', 'requester_username'], axis=1, inplace=True)\ndf.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4040 entries, 0 to 4039\nData columns (total 16 columns):\nrequest_text 4040 non-null object\nrequest_text_edit_aware 4040 non-null object\nrequest_title 4040 non-null object\nrequester_account_age_in_days_at_request 4040 non-null float64\nrequester_days_since_first_post_on_raop_at_request 4040 non-null float64\nrequester_number_of_comments_at_request 4040 non-null int64\nrequester_number_of_comments_in_raop_at_request 4040 non-null int64\nrequester_number_of_posts_at_request 4040 non-null int64\nrequester_number_of_posts_on_raop_at_request 4040 non-null int64\nrequester_number_of_subreddits_at_request 4040 non-null int64\nrequester_received_pizza 4040 non-null int64\nrequester_subreddits_at_request 4040 non-null object\nrequester_upvotes_minus_downvotes_at_request 4040 non-null int64\nrequester_upvotes_plus_downvotes_at_request 4040 non-null int64\nunix_timestamp_of_request 4040 non-null int64\nunix_timestamp_of_request_utc 4040 non-null int64\ndtypes: float64(2), int64(10), object(4)\nmemory usage: 536.6+ KB\n\n\n#### Removing the \"at_retrieval\" columns and the other six columns, reduces our dataset by 17 total features (leaving us with 14 features, not including the outcome variable ).\n\n#### During the EDA phase of this project we found that several (104 to be exact), observations had a \"request_text\" length of zero, some of these observations actually ended up being given a pizza. After looking through the data, we discovered that some people had left their request in the \"request_title\" field of the RAOP Reddit page and had left their request_text field blank. In order to clean this discrepancy up, we decided to combine these two fields together, as it is unclear if the benefactors (those who ended up giving pizzas away), were responding to the '\"request_title\" field, the \"request_text\" field, or both when they made their altruistic decision.\n\n\nIn\u00a0[6]:\n\n#Show that 104 observations have a blank \"request_text\" field\nlen(df[df['request_text'].str.len() == 0])\n\n\nOut[6]:\n\n104\n\n\nIn\u00a0[7]:\n\n#1. Combine request_text and request_title fields\n#2. Lowercase all words\n\ndf['request_text_n_title'] = (df['request_title'] + ' ' + df['request_text_edit_aware'])\ndf['request_text_n_title'] = [ text.split(\" \",1)[1].lower() for text in df['request_text_n_title']]\nprint df['request_text_n_title'].head()\n\n#3. Add a total length feature to the dataset\ndf['total_length'] =df['request_text_n_title'].apply(lambda x: len(x.split(' ')))\n\n#4. Ensure there are no zero length requests in the new feature/column\nprint 'nAfter combining request_title ...",
      "url": "https://notebook.community/omaraltaher/kaggle-pizza-project/Random_Acts_of_Pizza_Kaggle_Competition_Project"
    },
    {
      "title": "Random Acts of Pizza - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/c/random-acts-of-pizza"
    },
    {
      "title": "How to Win Kaggle Competitions",
      "text": "How to Win Kaggle Competitions | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n[\n](https://www.kaggle.com/zusmani)\n[Zeeshan-ul-hassan Usmani](https://www.kaggle.com/zusmani)\u00b7Posted8 years agoin[Getting Started](https://www.kaggle.com/discussions/getting-started)\narrow\\_drop\\_up534\nmore\\_vert\n# How to Win Kaggle Competitions\nKaggle is the perfect platform for a data scientist to hone their skills, build a great reputation and potentially get some quick cash. However, succeeding on Kaggle is no small task; it takes patience, hard work, and consistent practice. Keep in mind that this platform is home to some of the most brilliant minds in data sciences, so the competition is tough. To become a grandmaster, you need a high level of commitment and industry insights. This chapter will give you a brief guideline on how to succeed on Kaggle.\n**Step one**is to start by reading the competition guidelines thoroughly. Many Kagglers who are struggling to succeed on this platform do not have a thorough understanding of the competition, that is the overview, description, timeline, evaluation and eligibility criteria and the prize. Ignoring these little details will cost you big time in the long run. You need to know the deadline for your last submission. Small details such as the timeline of a particular competition are deal breakers. By studying the guidelines clearly, you will also uncover other commonly missed details such as the appropriate submission format and a guide on reproducing benchmarks. Do not start working on a Kaggle competition before you are clear about all the instructions. Take your time before jumping in.\n**The second and very crucial step**is to understand the performance measures. How the performance measure works is the yardstick your submission will be measured against, and you need to know it inside out. According to most experienced Kagglers, an optimised approach that is suitable to a particular measure makes it substantially easy to boost your score. For instance, Mean Square Error (MSE) and Mean Absolute Error (MAE) are closely related, not knowing the difference will penalize your end score.\n**Step three**is to understand the data in detail. You start with exploratory data analysis to find missing and null values and hidden patterns in the dataset. The more you know about the data, the better models you can build on top of it to improve your performance. Over-specialisation works in your favor as far as you do not over-fit. See what data weaknesses you can exploit for your own advantage, can you extract second fields from the given primary values, or can you typecast the given values to any other format to make it more machine learning friendly.\n**Step four**is to know what you want (objective) before worrying about how. Most novoices on Kaggle tend to worry excessively about which language to use (R or Python). It is wise, to begin with learning the data and ascertaining the patterns you intend to model. Knowing the domain and understanding data goes a long way when it comes to winning the competition.\n**Step five**and the often neglected step is to setup your own local validation environment. By doing that, you will be able to move at a faster pace. This will enable you to produce dependable results instead of solely relying on leader-board scores. You can skip this step if you are out of time or the dataset is too small and can easily be managed and executed on Kaggle dockers. By setting up your own environment, you can run the submission as many times as you like and you are not bound with five submissions a day restriction on Kaggle competitions. Once you feel confident enough about the results, you can submit it to live competition. It gives you an immense edge over your peers who do not have their local environments setup. By reducing the number of submissions you make, you are also substantially reducing the probability of over-fitting the leader-board, and it will save you for poor results at the evaluation stage.\n**Step six**is to read the forums. Forums and discussions are your friend. Take your time to consistently monitor the forum as you work on the competition, there is no way around it. Please subscribe to the forum and receive notifications related to the competition you are participating in. The forum will help you keep abreast with what the competition is up to. This has been made possible by the recent Kaggle trend of sharing code as the competition is going on. The host also shares their insights and directions about the competition on the forum more often. Even if you do not win, you can keep trying and learn from the post-competition summaries available at the forum to see where you went wrong or what your peers did to supersede your brilliance. This is a great way to learn from the best and improve consistently.\n**Step seven**is to research exhaustively. There is a good possibility that the competition you are participating is by people who have dedicated their lives to finding a viable solution. The people who host such competitions often have codes, benchmarks, official company blogs and extensive published papers or patents that come in handy. Even if you do not win in your first several attempts, you will learn, hone your skills and become a better data scientist.\n**Step eight**to stay with basics and apply it rigorously. While playing around with obscure methods is fun for data scientists, it is the basics that will get you far in a competition. The common algorithms you may ignore have great implementations. It is wise to do manual tuning or main parameters when experimenting with methods. Experienced Kagglers admit that one of the winning habits is to do the manual tuning.\n**Step nine**is the mother of all steps. It\u2019s time to ensemble models. It simply means combining all the models that you have developed independently. In most high profile competitions, different teams usually come together to combine their models to boost their scores. Since no competition on Kaggle has ever been won through a single model, it is wise to merge different independent models even when you are doing the solo ride.\n**Step ten**is the commitment to work on a single or selected few projects. If you commit and try to compete in every single competition, you will lose focus. It is better to focus on one or two and prove your mettle. The rank progression all the way to grand master will come naturally doing that. Remember the time and patience are two prime factors along with your data science expertise to move forward.\n**Step eleven**is the final step to pick the right approach. In the history of Kaggle, there are only two winning approaches that keep emerging from all the competitions. Feature engineering and Neural/Deep Learning Networks.\nFeature engineering is the best approach if you understand the data. The first step is taking the provided data and using it to accurately plot histograms to help you explore more. You will then typically spend a large amount of time generating features and then testing which ones correlate with the given target variables. For example, in a recent Kaggle competition titled Don\u2019t Get Kicked hosted by a chain of dealers known as Carvana. The participants were required to predict the cars that would go up for sale in a second hand (pre-owned) auction and the ones that will not be sold. Many participants put forward their algorithms and models. Ultimately, it turns out that the most feasible predictive feature was color. The participants grouped the cars into two categories: standard colors and unusual colors. It turns out that unusually colored car is more likely to be sold at a second-hand auction. Before Kaggle was able to arrive at this conclusion, there were numerous hypotheses, models, and kernel that did not perform the way expected.\nThe most popular winning al...",
      "url": "https://www.kaggle.com/discussions/getting-started/44997"
    },
    {
      "title": "Leaderboard - Random Acts of Pizza - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/c/random-acts-of-pizza/leaderboard"
    },
    {
      "title": "[PDF] Random Acts of Pizza: Success Factors of Online Requests",
      "text": "Random Acts of Pizza: Success Factors of Online Requests\nTim Althoff and Niloufar Salehi and Tuan Nguyen\n{althoff, niloufar, tylernht}@stanford.edu\nProject Group Number: 25\nAbstract\nMany online communities such as Q&A sites, crowd-funding\nplatforms, and online charities are based on users request\u0002ing from one another and receiving responses from within\nthe community; the success of these requests is critical to the\nsuccess of the community. In this paper, we explore the vari\u0002ous factors that influence success. One of the most interesting\naspects in this regard is how the formulation of the requests\naffects it\u2019s chances of succeeding. We argue that previous at\u0002tempts at unraveling factors of success were complicated by\ntheir diverse nature. We introduce a dataset of several thou\u0002sand of requests over the course of more than two years where\nevery requests asks for the very same, a pizza. This allows us\nto analyze the question of how successful request were made,\nin a way that has not been approached before. Our findings\ninclude that putting in some effort, creating a sense of trust,\nand the constructing the right kind of narrative are signifi\u0002cantly correlated with success.\nIntroduction\nWe live in a time where we increasingly turn to the web for\nhelp. However, our needs often go far beyond what a public\ndomain can offer, we need help from real people. A large\nnumber of platforms allow users to post requests through\nwhich they can ask others for help. Understanding the dy\u0002namics of successful requests is critical in many domains\nsuch as crowdfunding projects for startups on crowdfunding\nplatforms like Kickstarter [4], asking for answers to specific\nquestions on Quora [7] or StackOverflow [9], disadvantaged\npeople asking for loans on social peer to peer lending sites\nsuch as Kiva [5] or donations to non-profits and charities\nsuch as GlobalGiving [3] or Donors Choose [2], and people\nfacing hardship and asking for help on online communities\nsuch as Reddit [8]. Not all of these requests are success\u0002ful however, which raises the question of what differentiates\nthe fortunate. Prior research suggests aspects such as what\nis asked for, how it is asked, and who is asking whom as\nlargely influential in the process.\nWe explore these factors by simplifying some of the com\u0002plexities faced in previous work. In this paper we present\na novel case study of factors of online requests that natu\u0002rally controls for the subject of the request (try asking the\nCopyright \rc 2013, Stanford Social Network Analysis course\nproject (CS224W). All rights reserved.\ninternet for a Ferrari!). Further, we eliminate group dynam\u0002ics by looking at requests that a single person could satisfy.\nMoreover, knowledge gathering platforms discourage dupli\u0002cate requests (questions) and thereby introducing complex\nbiases (particularly, when one is trying to control for con\u0002tent to try to understand how people are asking for it). In\nthis project, we study \u201cRandom Acts of Pizza\u201d, an online\ncommunity devoted to giving away free pizzas to strangers\nthat ask for one. We argue that this is a unique platform for\nour study since all requests ask for the same thing, a pizza.\nCompared to many previously studied settings, the structure\nof textual requests in this particular community is relatively\nsimple. For instance, prior interaction between the requester\nand the giver before the request or during the decision mak\u0002ing process is rare and there are no video messages or com\u0002munication (as on Kickstarter) that have been shown to have\na strong influence on success [28]. Furthermore, it is not dis\u0002couraged to post a request with similar content to an existing\nrequest. This is not the case in the study of online popularity,\nQ&A sites or Kickstarter-like crowdfunding platforms.\nOur contribution in this paper is three-fold: (1) We present\na new dataset of online requests that all ask for the same\nthing and that has, to the best of our knowledge, not been\nsubject to scientific analysis before. (2) We present an anal\u0002ysis of various factors of success for online requests through\nmatching and statistical hypothesis testing. (3) We trans\u0002late our findings into concrete guidelines for the requester\nto maximize her chances of success.\nWe find that several factors are significantly correlated\nwith success of online requests: It helps to put in effort to\nwrite a longer narrative, to create a sense of trust by telling a\npersonal story and including pictures, to signal to give back\nto the community in the future, and to be an active and well\u0002regarded member of the community (see Results section for\nmore details and a discussion)\nThe remainder of this paper is structured as follows. We\nfirst present related work on success of online requests. Then\nthe problem statement and the corresponding dataset is in\u0002troduced. The next section elaborates on several factors that\ncould have an impact on success. We explain our methodol\u0002ogy in Methods section before we summarize our findings\nin the Results section. Lastly, we conclude this paper with\na short discussion and summary and describe avenues for\nfuture work.\nRelated Work\nIn the following, we review related work in the domain of\nonline requests and different factors of success such as con\u0002tent, temporal dynamics, the narrative, user similarity and\nstatus.\nThe Cost Spectrum of Online Interaction\nOnline communities and online social networks allow users\nto interact with each other as well as each other\u2019s content.\nMost will allow you to like or up-vote posts of other people,\nto (re)share them, or to comment on them. These modes of\ninteractions have been studied extensively in the context of\nonline popularity [23, 35]. Often the goal of studying online\npopularity is to understand what drivers user consumption,\nwhat content to display on websites, or how to design \u201cviral\u201d\nmarketing campaigns.\nOne dimension that has (to the best of our knowledge) not\nbeen studied explicitly yet is the cost of these interactions.\nVotes, re-shares and comments are usually free of charge\nand also cost very little time. Thus, the threshold to interact\nin these ways is fairly low and they happen relatively often.\nHowever, other modes of interaction have higher costs. For\ninstance, answering somebody else\u2019s question might take\nconsiderable effort and time, and help funding projects on\ncrowdsourcing platforms or donating to certain non-profits\neven comes at a financial cost. In contrast to the mentioned\nlow cost interactions people specifically ask for a nontrivial\namount of help. The act of asking for something is com\u0002monly called a request (see our problem definition below)\nand have adopted this terminology in this paper.\nHowever, determining the factors that influence popu\u0002larity poses similar challenges in that the content either\nneeds to be modeled explicitely [35] or controlled for [23].\nLakkaraju et al. [23] use resubmissions of the same content\n(in this case a picture) to control for content and study how\nthe title, the community, and the time matter for online pop\u0002ularity. When analyzing factors of success of online requests\nthis \u201ccontent\u201d is very similar to \u201cwhat is being asked for\u201d. In\nthis paper, we attempt to understand how one should ask for\nit by controlling what is being asked for.\nTemporal Dynamics of Successful Requests\nPrevious research has largely focused on crowdfunding plat\u0002forms such as Kickstarter [4] or peer to peer lending sites\nsuch as Prosper [6] and more often studied their temporal\ndynamics rather than the content of the requests. For in\u0002stance, Ceyhan et al. [16] find that loans with some early\nfunding are more likely to get fully funded (coined herd\u0002ing effect). Similar effects were found on Kickstarter when\nmodeling funding success as a time series prediction task\nthat included previous donations as well as social features\nsuch as the number of tweets about the project [18]. Partic\u0002ularly, the last donation to fully fund a project was found\nto exhibit special characteristics such...",
      "url": "http://snap.stanford.edu/class/cs224w-2013/projects2013/cs224w-025-final.pdf"
    },
    {
      "title": "Xavier Conort on maximising your machine learning success with ...",
      "text": "By Aidan Sussman Xavier Conort on maximising your machine learning success with innovative feature engineering - Actuaries Digital https://actuaries.asn.au/research-analysis/xavier-conort-on-maximising-your-machine-learning-success-with-innovative-feature-engineering\nXavier Conort on maximising your machine learning success with innovative feature engineering - Actuaries Digital\nBy Aidan Sussman\n2025-04-07T22:17:09Z\nMenu\n\nBecome an actuary\n\n[What is an actuary?](https://actuaries.asn.au/careers/what-is-an-actuary) [Learn more](https://actuaries.asn.au/careers/what-is-an-actuary)\n\n- [Why become an actuary](https://actuaries.asn.au/careers/why-become-an-actuary)\n- [Career paths for actuaries](https://actuaries.asn.au/careers/career-paths-in-actuarial-science)\n\n[How to become an actuary](https://actuaries.asn.au/careers/how-to-become-an-actuary) [Learn more](https://actuaries.asn.au/careers/how-to-become-an-actuary)\n\n- [Qualification pathway](https://actuaries.asn.au/careers/qualification-pathway)\n- [Accredited universities](https://actuaries.asn.au/careers/accredited-universities)\n- [Exemptions](https://actuaries.asn.au/careers/exemptions)\n- [Alternative qualification pathways](https://actuaries.asn.au/careers/alternative-qualification-pathways)\n- [Become a University Subscriber](https://actuaries.asn.au/careers/how-to-become-an-actuary#become-university-subscriber)\n\nActuarial careers\n\n- [Jobs board](https://actuaries.asn.au/careers/jobs-board)\n- [Practice areas](https://actuaries.asn.au/practice-areas)\n- [Actuarial competencies](https://actuaries.asn.au/careers/actuarial-competencies)\n\nPractice hub\n\n[Practice areas](https://actuaries.asn.au/practice-areas) [Learn more](https://actuaries.asn.au/practice-areas)\n\n- [Data science and AI](https://actuaries.asn.au/practice-areas/data-science-ai)\n- [Climate and sustainability](https://actuaries.asn.au/practice-areas/climate-and-sustainability)\n- [General insurance](https://actuaries.asn.au/practice-areas/general-insurance)\n- [Health](https://actuaries.asn.au/practice-areas/health)\n- [Life insurance](https://actuaries.asn.au/practice-areas/life-insurance)\n- [Risk management](https://actuaries.asn.au/practice-areas/risk-management)\n- [Superannuation and investments](https://actuaries.asn.au/practice-areas/superannuation-investments)\n- [Professionalism and ethics](https://actuaries.asn.au/practice-areas/professionalism-ethics)\n\nIndustry topics\n\n- [APRA](https://actuaries.asn.au/search?tags=APRA)\n- [Asia](https://actuaries.asn.au/search?tags=Asia)\n- [Banking](https://actuaries.asn.au/search?practiceAreas=Banking)\n- [Career development](https://actuaries.asn.au/search?tags=Career%20development)\n- [Diversity and inclusion](https://actuaries.asn.au/search?tags=Diversity%20and%20Inclusion)\n- [Mortality](https://actuaries.asn.au/search?tags=Mortality)\n- [Professionalism](https://actuaries.asn.au/search?practiceAreas=Professionalism%20Training)\n\n[Professional Standards and regulation](https://actuaries.asn.au/professional-standards-and-regulation) [Learn more](https://actuaries.asn.au/professional-standards-and-regulation)\n\n- [Code of Conduct](https://actuaries.asn.au/professional-standards-and-regulation/code-of-conduct)\n- [Professional Standards and Guidance](https://actuaries.asn.au/professional-standards-and-regulation/standards-guidance)\n- [CPD compliance](https://actuaries.asn.au/professional-standards-and-regulation/cpd-compliance)\n- [Disciplinary Scheme](https://actuaries.asn.au/professional-standards-and-regulation/disciplinary-scheme)\n- [Members' Sounding Board](https://actuaries.asn.au/professional-standards-and-regulation/members-sounding-board)\n- [Actuarial Capabilities Framework](https://actuaries.asn.au/professional-standards-and-regulation/actuarial-capabilities-framework)\n\nResearch & analysis\n\n[Research and analysis](https://actuaries.asn.au/research-analysis) [Learn more](https://actuaries.asn.au/research-analysis)\n\n- [Actuaries Digital](https://actuaries.asn.au/research-analysis/actuaries-digital)\n- [All articles](https://actuaries.asn.au/search?format=Article)\n- [Presentations](https://actuaries.asn.au/search?contentTypes=Presentation%20slides)\n- [Interviews](https://actuaries.asn.au/search?contentTypes=Article%20>%20Interview)\n- [Podcasts and audio](https://actuaries.asn.au/search?contentTypes=Podcast/Vodcast/Audio)\n\n[Thought leadership](https://actuaries.asn.au/research-analysis/thought-leadership) [Learn more](https://actuaries.asn.au/research-analysis/thought-leadership)\n\n- [Reports](https://actuaries.asn.au/search?contentTypes=Publication%20>%20Report)\n- [All publications](https://actuaries.asn.au/search?contentTypes=Publication)\n- [Submissions](https://actuaries.asn.au/search?contentTypes=Publication%20>%20Submission)\n- [Australian Actuaries Climate Index](https://actuaries.asn.au/climate-index)\n- [Public Policy approach](https://actuaries.asn.au/research-analysis/public-policy-approach)\n\n[News](https://actuaries.asn.au/research-analysis/news) [Catch up](https://actuaries.asn.au/research-analysis/news)\n\n- [Media releases](https://actuaries.asn.au/search?contentTypes=Media%20Release)\n- [Awards](https://actuaries.asn.au/awards)\n\nWhat's on\n\nUpcoming events\n\n- [See what\u2019s on](https://actuaries.asn.au/whats-on)\n- [Injury and Disability Schemes Seminar 2025](https://actuaries.asn.au/injury-and-disability-schemes-seminar-2025)\n- [All Actuaries Summit 2026](https://actuaries.asn.au/all-actuaries-summit-2026)\n- [Event partnerships](https://actuaries.asn.au/whats-on/event-partnerships)\n\nEvent types\n\n- [Major events](https://actuaries.asn.au/search?format=Upcoming%20event&eventTypes=Major%20Events)\n- [Insight sessions](https://actuaries.asn.au/search?format=Upcoming%20event&eventTypes=Insights)\n- [Young Actuaries Program events](https://actuaries.asn.au/search?format=Upcoming%20event&eventTypes=Young%20Actuaries%20Program)\n\n[Past events](https://actuaries.asn.au/whats-on/past-events) [Learn more](https://actuaries.asn.au/whats-on/past-events)\n\n- [All past event content](https://actuaries.asn.au/whats-on/past-events)\n- [All Actuaries Summit 2024 content](https://actuaries.asn.au/search?tags=All%20Actuaries%20Summit)\n- [Injury and Disability Schemes Seminar 2023 content](https://actuaries.asn.au/search?tags=Injury%20and%20Disability%20Schemes%20Seminar&range[year]=2023:)\n\nQualification & learning\n\n[Qualification programs](https://actuaries.asn.au/learning/qualification-programs) [View all programs](https://actuaries.asn.au/learning/qualification-programs)\n\n- [Qualification pathway](https://actuaries.asn.au/careers/qualification-pathway)\n- [Foundation Program](https://actuaries.asn.au/learning/foundation-program)\n- [Actuary Program](https://actuaries.asn.au/learning/actuary-program)\n- [Fellowship Program](https://actuaries.asn.au/learning/fellowship-program)\n- [Practical experience requirement](https://actuaries.asn.au/learning/practical-experience-requirement)\n- [Key dates](https://actuaries.asn.au/learning/key-dates)\n- [Graduation ceremonies](https://actuaries.asn.au/learning/graduation-ceremonies)\n- [Results](https://actuaries.asn.au/learning/results)\n- [Global CERA](https://actuaries.asn.au/learning/global-cera)\n\nLifelong learning\n\n- [Microcredentials](https://actuaries.asn.au/learning/microcredentials)\n- [CPD eLearning courses](https://actuaries.asn.au/learning/cpd-elearning-courses)\n- [Learning resources](https://actuaries.asn.au/learning/learning-resources)\n\nKey links\n\n- [Canvas LMS log in](https://your.actuaries.asn.au/account/?Issuer=Actuaries&RelayState=https://actuaries.instructure.com/)\n- [Education forms & governance](https://actuaries.asn.au/learning/forms-governance)\n\nThe Institute\n\nThe Institute\n\n- [About us](https://actuaries.asn.au/about)\n- [Council and governance](https://actuaries.asn.au/about/council-and-governance)\n- [Our team](https://actuaries.asn.au/about/our-team)\n- [Year in Review and financials](https://actuaries.asn.au/about/year-in-review-and-financials)\n- [Constitution](htt...",
      "url": "https://www.actuaries.asn.au/research-analysis/xavier-conort-on-maximising-your-machine-learning-success-with-innovative-feature-engineering"
    },
    {
      "title": "Beat the Benchmark: Random Acts of Pizza",
      "text": "[Skip to content](https://gist.github.com/roycoding/1e24bce664cce16503b8#start-of-content)\n\nSearch Gists\n\nSearch Gists\n\n[Sign\u00a0in](https://gist.github.com/auth/github?return_to=https%3A%2F%2Fgist.github.com%2Froycoding%2F1e24bce664cce16503b8) [Sign\u00a0up](https://gist.github.com/join?return_to=https%3A%2F%2Fgist.github.com%2Froycoding%2F1e24bce664cce16503b8&source=header-gist)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n{{ message }}\n\nInstantly share code, notes, and snippets.\n\n[![@roycoding](https://avatars.githubusercontent.com/u/747219?s=64&v=4)](https://gist.github.com/roycoding)\n\n# [roycoding](https://gist.github.com/roycoding)/ **[pizza-rf.md](https://gist.github.com/roycoding/1e24bce664cce16503b8)**\n\nLast active\nJune 27, 2017 18:36\n\nShow Gist options\n\n- [Download ZIP](https://gist.github.com/roycoding/1e24bce664cce16503b8/archive/75d8c43ee5b36303fffecf969d30d9328bd01c89.zip)\n\n- [Star1(1)](https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Froycoding%2F1e24bce664cce16503b8) You must be signed in to star a gist\n- [Fork0(0)](https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Froycoding%2F1e24bce664cce16503b8) You must be signed in to fork a gist\n\n- Embed\n\n\n\n\n\n\n\n- Embed\nEmbed this gist in your website.\n- Share\nCopy sharable link for this gist.\n- Clone via HTTPS\nClone using the web URL.\n- [Learn more about clone URLs](https://docs.github.com/articles/which-remote-url-should-i-use)\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/roycoding/1e24bce664cce16503b8.js&quot;&gt;&lt;/script&gt;\n\n- Save roycoding/1e24bce664cce16503b8 to your computer and use it in GitHub Desktop.\n\nEmbed\n\n- Embed\nEmbed this gist in your website.\n- Share\nCopy sharable link for this gist.\n- Clone via HTTPS\nClone using the web URL.\n- [Learn more about clone URLs](https://docs.github.com/articles/which-remote-url-should-i-use)\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/roycoding/1e24bce664cce16503b8.js&quot;&gt;&lt;/script&gt;\n\nSave roycoding/1e24bce664cce16503b8 to your computer and use it in GitHub Desktop.\n\n[Download ZIP](https://gist.github.com/roycoding/1e24bce664cce16503b8/archive/75d8c43ee5b36303fffecf969d30d9328bd01c89.zip)\n\nBeat the Benchmark: Random Acts of Pizza\n\n[Raw](https://gist.github.com/roycoding/1e24bce664cce16503b8/raw/75d8c43ee5b36303fffecf969d30d9328bd01c89/pizza-rf.md)\n\n[**pizza-rf.md**](https://gist.github.com/roycoding/1e24bce664cce16503b8#file-pizza-rf-md)\n\n# Beating the Random Acts of Pizza Benchmark\n\n## Day 2 of the [Beat 5 Kaggle Becnhmarks in 5 Days Challenge](https://www.kaggle.com/forums/t/10062/beat-5-kaggle-benchmarks-in-5-days-challenge).\n\nThe [Random Acts of Pizza](https://www.kaggle.com/c/random-acts-of-pizza) competition is about predicting when a request for a free pizza on the Random Acts of Pizza sub-reddit is granted. The benchmark is simply guessing that no pizzas are given (or all). This results in an AUC score of 50.\n\nTo beat the AUC = 50 benchmark with a simple model, I first looked at the training and test data to find simple features. I decided to use the word counts of the request title and comment text, as longer comments might be skipped by readers.\n\nTo build the model I first extracted only the desired fields from the original JSON files with jq and used json2csv to write out CSV.\n\n```\ncat train.json|jq -c '.[]' | json2csv -p -k=request_id,requester_received_pizza,request_title,request_text_edit_aware -d=\"|\" > train_1.csv\n\ncat test.json|jq -c '.[]' | json2csv -p -k=request_id,request_title,request_text_edit_aware -d=\"|\" > test_1.csv\n```\n\nI then built a very basic random forest model using the default settings in scikit-learn. With a 80/20 training/test split I achieved a local AUC of about 0.52 (single validation). Using the entire training set to build a random forest, I was able to score an AUC of 0.51274 on the competition leaderboard.\n\nNot great, but not bad for a very simple model.\n\n```\nimport pandas as pd\nfrom sklearn import cross_validation\nfrom sklearn import ensemble\nfrom sklearn import metrics\n\ntrain = pd.read_csv('train_1.csv',delimiter='|')\ntest = pd.read_csv('test_1.csv',delimiter='|')\n\n# Create text word count and title word count fields and binarize pizza received\ntrain.requester_received_pizza = train.requester_received_pizza.apply(lambda x: 1 if x else 0)\ntrain['title_count'] = train.request_title.apply(lambda x: len(x.split()))\ntrain['text_count'] = train.request_text_edit_aware.apply(lambda x: len(str(x).split()))\n\ntest['title_count'] = test.request_title.apply(lambda x: len(x.split()))\ntest['text_count'] = test.request_text_edit_aware.apply(lambda x: len(str(x).split()))\n\n# Create training and testing arrays as well as validation splits\ntrain_X = train.drop(['request_id', u'requester_received_pizza', u'request_title', u'request_text_edit_aware'],axis=1).values\ntrain_y = train.requester_received_pizza.values\nX,X_,y,y_ = cross_validation.train_test_split(train_X,train_y,test_size=0.2)\n\ntest_X = test.drop(['request_id','request_title','request_text_edit_aware'],axis=1).values\n\n# Train and test random forest model\nrf = ensemble.RandomForestClassifier() # Default values\nrf.fit(X,y)\ny_rf = rf.predict(X_)\nprint metrics.roc_auc_score(y_,y_rf)\n\n# Train model with full training data and predict test y's\nrf.fit(train_X,train_y)\ny_test_rf = rf.predict(test_X)\n\n# Write submission file\ntest_out = pd.DataFrame({'request_id':test.request_id.values,'requester_received_pizza':y_test_rf.astype('int')})\ntest_out.to_csv('rf1.csv',index=False)\n```\n\n[Sign up for free](https://gist.github.com/join?source=comment-gist) **to join this conversation on GitHub**.\nAlready have an account?\n[Sign in to comment](https://gist.github.com/login?return_to=https%3A%2F%2Fgist.github.com%2Froycoding%2F1e24bce664cce16503b8)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://gist.github.com/roycoding/1e24bce664cce16503b8"
    }
  ]
}