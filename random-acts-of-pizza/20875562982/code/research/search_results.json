{
  "query": "How do Kaggle winners implement Feature-Weighted Linear Stacking for ensemble models in classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the original description of Feature\u2011Weighted Linear Stacking (FWLS) by Lin\u202fet\u202fal., Kaggle winners who use this technique follow a small, well\u2011defined pipeline that treats the stacking weights themselves as linear functions of meta\u2011features\u202f([ui.adsabs.harvard.edu](https://ui.adsabs.harvard.edu/abs/2009arXiv0911.0460S)):\n\n1. **Generate base\u2011model predictions** \u2013 Train each first\u2011level model on the full training set and produce out\u2011of\u2011fold (or hold\u2011out) predictions for every training example. Save these predictions as separate columns.  \n\n2. **Create meta\u2011features** \u2013 For each training example compute additional descriptive variables (e.g., row\u2011level statistics, target\u2011encoding of high\u2011cardinality categories, time\u2011based features, etc.). These meta\u2011features capture information that may explain why a particular model performs better on certain rows.  \n\n3. **Form interaction features** \u2013 Multiply each base\u2011model prediction by each meta\u2011feature (prediction\u202f\u00d7\u202fmeta\u2011feature). This yields a set of \u201cweighted\u2011prediction\u201d columns whose linear coefficients will become the feature\u2011dependent stacking weights.  \n\n4. **Fit a linear meta\u2011learner** \u2013 Using the interaction features as inputs and the true target as the response, fit a regularised linear regression (e.g., ridge or Lasso). Because the coefficients are linear in the meta\u2011features, the model learns how to adjust each base\u2011model\u2019s contribution for different regions of the data.  \n\n5. **Produce test\u2011set weights and final predictions** \u2013 For the test data, compute the same meta\u2011features, generate the interaction columns (base\u2011model prediction\u202f\u00d7\u202fmeta\u2011feature), and apply the trained linear model to obtain the final stacked probabilities or class scores.  \n\n6. **Optional post\u2011processing** \u2013 Clip or calibrate the stacked outputs, then submit the predictions.  \n\nBy letting the stacking coefficients vary with meta\u2011features, FWLS retains the speed and interpretability of linear regression while gaining the accuracy boost of a non\u2011linear ensemble\u2014a strategy that helped the second\u2011place Netflix\u2011Prize team and is routinely adopted by top Kaggle competitors.",
      "url": ""
    },
    {
      "title": "Feature-Weighted Linear Stacking",
      "text": "Now on home page\n\n## NASA/ADS\n\n## Feature-Weighted Linear Stacking\n\n- [Sill, Joseph](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Sill%2C+Joseph%22);\n- [Takacs, Gabor](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Takacs%2C+Gabor%22);\n- [Mackey, Lester](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Mackey%2C+Lester%22);\n- [Lin, David](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Lin%2C+David%22)\n\n#### Abstract\n\nEnsemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.\n\nPublication:\n\narXiv e-prints\n\nPub Date:November 2009DOI:[10.48550/arXiv.0911.0460](https://ui.adsabs.harvard.edu/link_gateway/2009arXiv0911.0460S/doi:10.48550/arXiv.0911.0460)arXiv:[arXiv:0911.0460](https://ui.adsabs.harvard.edu/link_gateway/2009arXiv0911.0460S/arXiv:0911.0460)Bibcode:[2009arXiv0911.0460S](https://ui.adsabs.harvard.edu/abs/2009arXiv0911.0460S/abstract)Keywords:\n\n- Computer Science - Machine Learning;\n- Computer Science - Artificial Intelligence\n\nE-Print:17 pages, 1 figure, 2 tables\n\nfull text sources\n\narXiv\n\n\\|\n\n\ud83c\udf13",
      "url": "https://ui.adsabs.harvard.edu/abs/2009arXiv0911.0460S"
    },
    {
      "title": "Using Ensembles in Kaggle Data Science Competitions- Part 3 - KDnuggets",
      "text": "# Using Ensembles in Kaggle Data Science Competitions- Part 3\n\nEarlier, we showed how to create stacked ensembles with stacked generalization and out-of-fold predictions. Now we'll learn how to implement various stacking techniques.\n\n* * *\n\n![c](https://www.kdnuggets.com/images/comment.gif)[comments](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html#comments)\n\n**By Henk van Veen**\n\n_The natural world is complex, so it figures that ensembling different models can capture more of this complexity- Ben Hamner_\n\n[![This image shows Models visualized as a network can be trained used back-propagation](https://www.kdnuggets.com/wp-content/uploads/backward-propagation-stacker-models-300x197.jpg)](https://www.kdnuggets.com/wp-content/uploads/backward-propagation-stacker-models.jpg)\n\nBackward propagation of networked models\n\n**Stacking with logistic regression:**\n\nStacking with logistic regression is one of the more basic and traditional ways of stacking. You can create predictions for the test set in one go, or take an average of the out-of-fold predictors. Either works well.\n\nThough taking the average is a clean and accurate way to do this, you might want to consider one go as that slightly lowers both model and coding complexity.\n\n**Kaggle use: \u201cPapirusy z Edhellond\u201d:**\n\nThe author uses [blend.py](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py) to compete in this classification competition.\nBy stacking 8 base models (diverse ET\u2019s, RF\u2019s and GBM\u2019s) with Logistic Regression he is able to score 0.99409 accuracy, good for first place.\n\n**Kaggle use: KDD-cup 2014:**\n\nHere the author again used [blend.py](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py) to improve a model. The model before stacking scored ~0.605 AUC, and with stacking this improved to ~0.625.\n\n**Stacking with non-linear algorithms:**\n\nPopular non-linear algorithms for stacking are GBM, KNN, NN, RF and ET. You can note a couple of interesting points here:\n\n- Non-linear stacking with the original features on multiclass problems gives surprising gains.\n- Non-linear algorithms find useful interactions between the original features and the meta-model features.\n\n**Kaggle use: TUT Headpose Estimation Challenge:**\n\nThe [TUT Headpose Estimation challenge](https://inclass.kaggle.com/c/tut-head-pose-estimation-challenge%20) can be treated as a multi-class multi-label classification challenge. For every label a separate ensemble model was trained. The key point is that stacking the predicted class probabilities with an extremely randomized trees model improved the scores. The author stacked generalization with standard models and was able to reduce the error by around 30%.\n\n**Feature weighted linear stacking:**\n\nFeature-weighted linear stacking stacks engineered meta-features together with model predictions. Linear algorithms are used to keep the resulting model fast and simple to inspect.\n\n**Quadratic linear stacking of models:**\n\nThe author framed the name \u2013 Quadratic linear stacking of models. It works similar to feature-weighted linear stacking, but creates combinations of model predictions. This technique improved the author's score in many competitions, most noticeably on the [Modeling Women\u2019s Healthcare Decision competition](http://www.drivendata.org/competitions/6/) on DrivenData.\n\n**Stacking classifiers with regressors and vice versa:**\n\nBy stacking you can use classifiers for regression problems and vice versa. Even though regression is usually not the best classifier. But it is a bit tricky.\n\n- You use binning first and turn a regression problem into a multiclass classification problem.\n- The predicted probabilities for these classes can help a stacking regressor make better predictions.\n\nA good stacker must be able to take information from the predictions.\n\n**Stacking unsupervised learned features:**\n\nYou can also stack with unsupervised learning techniques as well. A sensible popular technique is the K-Means Clustering. An interested recent addition is to use\n[t-SNE](http://lvdmaaten.github.io/tsne/):\n\n- Reduce the dataset to 2 or 3 dimensions.\n- stack this with a non-linear stacker.\n- Use a holdout set for stacking/blending (safe choice).\n\n**Online Stacking:**\n\nA good example of online (or semi-) stacking is with ad click prediction. Models trained on recent data perform better here.\n\n- So when a dataset has a temporal effect, you could use Vowpal Wabbit to train on the entire dataset.\n- Combine it with a more complex and powerful tool like XGBoost to train on the last day of data.\n- Finally stack the XGBoost predictions together with the samples and let Vowpal Wabbit do what it does best - optimizing loss functions.\n\n**Everything is a hyper-parameter:**\n\nWhen doing stacking/blending/meta-modeling, think of every action as a hyper-parameter for the stacker model.\n\nSo this makes the below simply extra parameters to be tuned to improve the ensemble performance.\n\n- Not scaling the data\n- Standard-Scaling the data\n- Minmax scaling the data\n\n**Model Selection:**\n\nYou can further optimize scores by combining multiple ensembled models.\n\n- Use averaging, voting or rank averaging on manually-selected well-performing ensembles.\n- Start with a base ensemble of 3 or so good models. Add a model when it increases the train set score the most. By allowing put-back of models, a single model may be picked multiple times (weighing).\n- Use of genetic algorithms (from Genetic Model Selection) and CV-scores as the fitness function.\n- The author uses a fully random method: Create a 100 or so ensembles from randomly selected ensembles (without placeback). Then pick the highest scoring model.\n\n**Automation:**\n\nAdding many base models along with multiple stacked ensembles can only get you so far in a competition.\nFor the rest, you might consider the below for automating:\n\n- Models visualized as a network can be trained used back-propagation\n- Consider CV-scores and their standard deviation (smaller the better).\n- There is scope to optimizing complexity/memory usage and running times.\n- Also look at making the script prefer uncorrelated model predictions when creating ensembles.\n- Consider parallelizing and distributing your automation to improve speed.\n\n**Kaggle use: [Otto product classification:](https://www.kaggle.com/c/otto-group-product-classification-challenge)**\n\nUsing the automated stacker in this competition, the author got to top 10% score without any tuning or manual model selection. Here's his approach:\n\n- For **base models** is to generate random algorithms with pure random parameters and train.\n- Wrappers can be written to make classifiers like VW, Sofia-ML, RGF, MLP and XGBoost play nicely with the Scikit-learn API.\n- For **stackers** let the script use SVM, random forests, extremely randomized trees, GBM and XGBoost with random parameters and a random subset of base models.\n- Finally average the created stackers when their fold-predictions on the train set produces a lower loss.\n\n**Why create these Frankenstein ensembles?**\n\nYou may wonder why this exercise in futility: stacking and combining 1000s of models and computational hours is insanity right? Well\u2026 yes. But these monster ensembles still have their uses:\n\n- You can win Kaggle competitions.\n- You can beat most state-of-the-art academic benchmarks with a single approach.\n- It is possible to transfer knowledge from the ensemble back to a simpler shallow model (Hinton\u2019s [Dark Knowledge](http://www.ttic.edu/dl/dark14.pdf), Caruana\u2019s [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf))\n- A good thing about ensembling is that loss of one model is not fatal for creating good predictions.\n- Automated large ensembles don't require much tuning or selection.\n- A 1% increase in accuracy may push an investment fund from making a loss, into making a little less loss. More seriously: Improving healthcare screening methods helps save lives.\n\nSee also\n[**Using Ensembles in Kaggle ...",
      "url": "https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html"
    },
    {
      "title": "Ensemble Learning to Improve Machine Learning Results - KDNuggets",
      "text": "### **Stacking**\n\nStacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n\nThe base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous. The algorithm below summarizes stacking.\n\nThe following accuracy is visualized in the top right plot of the figure above:\n\nAccuracy: 0.91 (+/- 0.01) \\[KNN\\]\n\nAccuracy: 0.91 (+/- 0.06) \\[Random Forest\\]\n\nAccuracy: 0.92 (+/- 0.03) \\[Naive Bayes\\]\n\nAccuracy: 0.95 (+/- 0.03) \\[Stacking Classifier\\]\n\nThe stacking ensemble is illustrated in the figure above. It consists of k-NN, Random Forest, and Naive Bayes base classifiers whose predictions are combined by Logistic Regression as a meta-classifier. We can see the blending of decision boundaries achieved by the stacking classifier. The figure also shows that stacking achieves higher accuracy than individual classifiers and based on learning curves, it shows no signs of overfitting.\n\nStacking is a commonly used technique for winning the Kaggle data science competition. For example, the first place for the Otto Group Product Classification challenge was won by a stacking ensemble of over 30 models whose output was used as features for three meta-classifiers: XGBoost, Neural Network, and Adaboost. See the following\u00a0[link](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)\u00a0for details.\n\n### **Code**\n\nIn order to view the code used to generate all figures, have a look at the following\u00a0[ipython notebook](https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb).\n\n### **Conclusion**\n\nIn addition to the methods studied in this article, it is common to use ensembles in deep learning by training diverse and accurate classifiers. Diversity can be achieved by varying architectures, hyper-parameter settings, and training techniques.\n\nEnsemble methods have been very successful in setting record performance on challenging datasets and are among the top winners of Kaggle data science competitions.\n\n**Recommended reading**\n\n- Zhi-Hua Zhou, \u201cEnsemble Methods: Foundations and Algorithms\u201d, CRC Press, 2012\n- L. Kuncheva, \u201cCombining Pattern Classifiers: Methods and Algorithms\u201d, Wiley, 2004\n- [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/)\n- [Scikit Learn Ensemble Guide](http://scikit-learn.org/stable/modules/ensemble.html)\n- [S. Rachka, MLxtend library](http://rasbt.github.io/mlxtend/)\n- [Kaggle Winning Ensemble](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)\n\n**Bio: [Vadim Smolyakov](https://blog.statsbot.co/@vsmolyakov)** is passionate about data science and machine learning. Check out his [**Github**](https://github.com/vsmolyakov).\n\n[Original](https://blog.statsbot.co/ensemble-learning-d1dcd548e936). Reposted with permission.\n\n**Related:**\n\n- [Data Science Basics: An Introduction to Ensemble Learners](https://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html)\n- [Stacking Models for Improved Predictions](https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html)\n- [Must-Know: What is the idea behind ensemble learning?](https://www.kdnuggets.com/2017/05/must-know-ensemble-learning.html)",
      "url": "https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html/2"
    },
    {
      "title": "GitHub - MLWave/Kaggle-Ensemble-Guide: Code for the Kaggle Ensembling Guide Article on MLWave",
      "text": "A combination of Model Ensembling methods that is extremely useful for increasing accuracy of Kaggle's submission.\nFor more information: [http://mlwave.com/kaggle-ensembling-guide/](http://mlwave.com/kaggle-ensembling-guide/)\n\n```\n$ pip install -r requirements.txt\n\n```\n\n```\n$ python ./src/correlations.py ./samples/method1.csv ./samples/method2.csv\nFinding correlation between: ./samples/method1.csv and ./samples/method2.csv\nColumn to be measured: Label\nPearson's correlation score: 0.67898\nKendall's correlation score: 0.66667\nSpearman's correlation score: 0.71053\n$ python ./src/kaggle_vote.py \"./samples/method*.csv\" \"./samples/kaggle_vote.csv\"\nparsing: ./samples/method1.csv\nparsing: ./samples/method2.csv\nparsing: ./samples/method3.csv\nwrote to ./samples/kaggle_vote.csv\n$ python ./src/kaggle_vote.py \"./samples/_*.csv\" \"./samples/kaggle_vote_weighted.csv\" \"weighted\"\nparsing: ./samples/_w3_method1.csv\nUsing weight: 3\nparsing: ./samples/_w2_method2.csv\nUsing weight: 2\nparsing: ./samples/_w2_method3.csv\nUsing weight: 2\nwrote to ./samples/kaggle_vote_weighted.csv\n$ python ./src/kaggle_rankavg.py \"./samples/method*.csv\" \"./samples/kaggle_rankavg.csv\"\nparsing: ./samples/method1.csv\nparsing: ./samples/method2.csv\nparsing: ./samples/method3.csv\nwrote to ./samples/kaggle_rankavg.csv\n$ python ./src/kaggle_avg.py \"./samples/method*.csv\" \"./samples/kaggle_avg.csv\"\nparsing: ./samples/method1.csv\nparsing: ./samples/method2.csv\nparsing: ./samples/method3.csv\nwrote to ./samples/kaggle_avg.csv\n$ python ./src/kaggle_geomean.py \"./samples/method*.csv\" \"./samples/kaggle_geomean.csv\"\nparsing: ./samples/method1.csv\nparsing: ./samples/method2.csv\nparsing: ./samples/method3.csv\nwrote to ./samples/kaggle_geomean.csv\n\n```\n\n```\n==> ./samples/method1.csv <==\nImageId,Label\n1,1\n2,0\n3,9\n4,9\n5,3\n==> ./samples/method2.csv <==\nImageId,Label\n1,2\n2,0\n3,6\n4,2\n5,3\n==> ./samples/method3.csv <==\nImageId,Label\n1,2\n2,0\n3,9\n4,2\n5,3\n==> ./samples/kaggle_avg.csv <==\nImageId,Label\n1,1.666667\n2,0.000000\n3,8.000000\n4,4.333333\n5,3.000000\n==> ./samples/kaggle_rankavg.csv <==\nImageId,Label\n1,0.25\n2,0.0\n3,1.0\n4,0.5\n5,0.75\n==> ./samples/kaggle_vote.csv <==\nImageId,Label\n1,2\n2,0\n3,9\n4,2\n5,3\n==> ./samples/kaggle_geomean.csv <==\nImageId,Label\n1,1.587401\n2,0.000000\n3,7.862224\n4,3.301927\n5,3.000000\n\n```",
      "url": "https://github.com/MLWave/Kaggle-Ensemble-Guide"
    },
    {
      "title": "Stacking Ensemble",
      "text": "Stacking Ensemble | Kaggler TV Blog\n[Kaggler TV Blog](https://kaggler.com/)\n# Stacking Ensemble\nA tutorial of stacking ensemble (a.k.a. stacked generalization)\nApr 26, 2021\u20225 min read\n**</i>[notebook](https://kaggler.com/categories/#notebook)[kaggle](https://kaggler.com/categories/#kaggle)\n[![View On GitHub](https://kaggler.com/assets/badges/github.svg)](https://github.com/kaggler-tv/blog/tree/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Binder](https://kaggler.com/assets/badges/binder.svg)](https://mybinder.org/v2/gh/kaggler-tv/blog/master?filepath=_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Colab](https://kaggler.com/assets/badges/colab.svg)](https://colab.research.google.com/github/kaggler-tv/blog/blob/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n* [Part 1: Data Loading &amp; Feature Engineering](#Part-1:-Data-Loading-&-Feature-Engineering)\n* [Part 2: Level-1 Base Model Training](#Part-2:-Level-1-Base-Model-Training)\n* [Part 3: Level-2 Stacking](#Part-3:-Level-2-Stacking)\nThis notebook was originally published[here](https://www.kaggle.com/jeongyoonlee/stacking-ensemble)at Kaggle.\nThis notebook shows how to perform stacking ensemble (a.k.a. stacked generalization).\nIn[Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking), @remekkinas shares how to do stacking ensemble using`MLExtend'`s`StackingCVClassifier`.\nTo demonstrate how stacking works, this notebook shows how to prepare the baseline model predictions using cross-validation (CV), then use them for level-2 stacking. It trains four classifiers, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost as level-1 base models. It also uses CV predictions of two models, LightGBM with DAE features and supervised DAE trained from my previous notebook,[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)to show why keeping CV predictions for**every**model is important. :)\nThe contents of this notebook are as follows:\n1. **Feature Engineering**: Same as in the[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)and[AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb).\n2. **Level-1 Base Model Training**: Training four base models, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost using the same 5-fold CV.\n3. **Level-2 Stacking**: Training the LightGBM model with CV predictions of base models, original features, and DAE features. Performing feature selection and hyperparameter optimization using`Kaggler`'s`AutoLGB`.\nThis notebook is inspired and/or based on other Kagglers' notebooks as follows:\n* [TPS-APR21-EDA+MODEL](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model)by @udbhavpangotra\n* [Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking)by @remekkinas\n* [TPS Apr 2021 pseudo labeling/voting ensemble](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble?scriptVersionId=60616606)by @hiro5299834\nThanks!\n# [](#Part-1:-Data-Loading-&amp;-Feature-Engineering)Part 1: Data Loading &amp; Feature Engineering[](#Part-1:-Data-Loading-&amp;-Feature-Engineering)\n```\nfromcatboostimportCatBoostClassifierfromjoblibimportdumpimportlightgbmaslgbfromlightgbmimportLGBMClassifierfrommatplotlibimportpyplotaspltimportnumpyasnpimportpandasaspdfrompathlibimportPathfromsklearn.ensembleimportRandomForestClassifierfromsklearn.ensembleimportExtraTreesClassifierfromsklearn.metricsimportroc\\_auc\\_score,confusion\\_matrixfromsklearn.model\\_selectionimportStratifiedKFoldfromsklearn.preprocessingimportStandardScalerimportwarnings\n```\n```\n!pip install kaggler\n```\n```\nimportkagglerfromkaggler.modelimportAutoLGBfromkaggler.preprocessingimportLabelEncoderprint(f'Kaggler:{kaggler.\\_\\_version\\_\\_}')\n```\n```\nwarnings.simplefilter('ignore')pd.set\\_option('max\\_columns',100)\n```\n```\ndata\\_dir=Path('/kaggle/input/tabular-playground-series-apr-2021/')trn\\_file=data\\_dir/'train.csv'tst\\_file=data\\_dir/'test.csv'sample\\_file=data\\_dir/'sample\\_submission.csv'pseudo\\_label\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/tps04-sub-006.csv'dae\\_feature\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/dae.csv'lgb\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.val.txt'lgb\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.tst.txt'sdae\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.val.txt'sdae\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.tst.txt'target\\_col='Survived'id\\_col='PassengerId'feature\\_name='dae'algo\\_name='esb'model\\_name=f'{algo\\_name}\\_{feature\\_name}'feature\\_file=f'{feature\\_name}.csv'predict\\_val\\_file=f'{model\\_name}.val.txt'predict\\_tst\\_file=f'{model\\_name}.tst.txt'submission\\_file=f'{model\\_name}.sub.csv'\n```\n```\nn\\_fold=5seed=42n\\_est=1000encoding\\_dim=128\n```\n```\ntrn=pd.read\\_csv(trn\\_file,index\\_col=id\\_col)tst=pd.read\\_csv(tst\\_file,index\\_col=id\\_col)sub=pd.read\\_csv(sample\\_file,index\\_col=id\\_col)pseudo\\_label=pd.read\\_csv(pseudo\\_label\\_file,index\\_col=id\\_col)dae\\_features=np.loadtxt(dae\\_feature\\_file,delimiter=',')lgb\\_dae\\_predict\\_val=np.loadtxt(lgb\\_dae\\_predict\\_val\\_file)lgb\\_dae\\_predict\\_tst=np.loadtxt(lgb\\_dae\\_predict\\_tst\\_file)sdae\\_dae\\_predict\\_val=np.loadtxt(sdae\\_dae\\_predict\\_val\\_file)sdae\\_dae\\_predict\\_tst=np.loadtxt(sdae\\_dae\\_predict\\_tst\\_file)print(trn.shape,tst.shape,sub.shape,pseudo\\_label.shape,dae\\_features.shape)print(lgb\\_dae\\_predict\\_val.shape,lgb\\_dae\\_predict\\_tst.shape)print(sdae\\_dae\\_predict\\_val.shape,sdae\\_dae\\_predict\\_tst.shape)\n```\n```\ntst[target\\_col]=pseudo\\_label[target\\_col]n\\_trn=trn.shape[0]df=pd.concat([trn,tst],axis=0)df.head()\n```\nLoading 128 DAE features generated from[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder/).\n```\ndf\\_dae=pd.DataFrame(dae\\_features,columns=[f'enc\\_{x}'forxinrange(encoding\\_dim)])print(df\\_dae.shape)df\\_dae.head()\n```\nFeature engineering using @udbhavpangotra's[code](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model).\n```\ndf['Embarked']=df['Embarked'].fillna('No')df['Cabin']=df['Cabin'].fillna('\\_')df['CabinType']=df['Cabin'].apply(lambdax:x[0])df.Ticket=df.Ticket.map(lambdax:str(x).split()[0]iflen(str(x).split())&gt;1else'X')df['Age'].fillna(round(df['Age'].median()),inplace=True,)df['Age']=df['Age'].apply(round).astype(int)# Fare, fillna with mean valuefare\\_map=df[['Fare','Pclass']].dropna().groupby('Pclass').median().to\\_dict()df['Fare']=df['Fare'].fillna(df['Pclass'].map(fare\\_map['Fare']))df['FirstName']=df['Name'].str.split(', ').str[0]df['SecondName']=df['Name'].str.split(', ').str[1]df['n']=1gb=df.groupby('FirstName')df\\_names=gb['n'].sum()df['SameFirstName']=df['FirstName'].apply(lambdax:df\\_names[x]).fillna(1)gb=df.groupby('SecondName')df\\_names=gb['n'].sum()df['SameSecondName']=df['SecondName'].apply(lambdax:df\\_names[x]).fillna(1)df['Sex']=(df['Sex']=='male').astype(int)df['FamilySize']=df.SibSp+df.Parch+1feature\\_cols=['Pclass','Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName','SameSecondName','Sex','FamilySize','FirstName','SecondName']cat\\_cols=['Pclass','Embarked','CabinType','Ticket','FirstName','SecondName']num\\_cols=[xforxinfeature\\_colsifxnotincat\\_cols]print(len(feature\\_cols),len(cat\\_cols),len(num\\_cols))\n```\nApplying`log2(1 + x)`for numerical features and label-encoding categorical features using`kaggler.preprocessing.LabelEncoder`, which handles`NaN`s and groups rare categories together.\n```\nforcolin['SameFirstName','SameSecondName','Fare','FamilySize','Parch','SibSp']:df[col]=np.log2(1+df[col])scaler=StandardScaler()df[num\\_cols]=...",
      "url": "https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html"
    },
    {
      "title": "",
      "text": "Stacking Ensemble Learning : Combining XGBoost,\nLightGBM, CatBoost, and AdaBoost with Random\nForest Meta Model\nSindhu\u00a0\nNugroho https://orcid.org/0009-0002-1558-4574\nResearch Article\nKeywords: Articial Intelligence, Deep Learning, Machine Learning, Ensemble Learning\nPosted Date: October 30th, 2025\nDOI: https://doi.org/10.21203/rs.3.rs-7944070/v1\nLicense: \uf25e \uf4e7 This work is licensed under a Creative Commons Attribution 4.0 International License. \u00a0\nRead Full License\nAdditional Declarations: The authors declare no competing interests.\nSTACKING ENSEMBLE LEARNING : COMBINING XGBOOST,\nLIGHTGBM, CATBOOST, AND ADABOOST WITH RANDOM\nFOREST META MODEL\nA PREPRINT\nSindhu Wijaya Mulyo Nugroho, S.Kom*\nProduct Development, Educourse.id\nSemarang, Indonesia\nsindhu.nugroho99@gmail.com\nORCID: 0009-0002-1558-4574\nOctober 27, 2025\nABSTRACT\nEnsemble learning has become a powerful approach in machine learning, particularly for improving\nprediction accuracy and generalization. This work proposes a stacking ensemble framework that\nintegrates four popular boosting algorithms, XGBoost, LightGBM, CatBoost, and AdaBoost, as base\nlearners, with Random Forest employed as the meta-lover. The design leverages the complementary\nstrengths of boosting algorithms in handling tabular data, categorical variables, and imbalanced\ndatasets, while Random Forest ensures robust decision-making at the meta-level. The results show\nthat the proposed stacking ensemble consistently outperforms individual models, achieving better\nstability and reducing both variance and bias. This highlights the effectiveness of combining multiple\nboosting algorithms with a Random Forest metamodel to build a hybrid system that is accurate,\ngeneralizable, and applicable across different machine learning domains.\nKeywords Artificial Intelligence \u00b7 Deep Learning \u00b7 Machine Learning \u00b7 Ensemble Learning\n1 Introduction\nEnsemble learning has become a core paradigm in machine learning due to its ability to improve prediction accuracy\nand generalization by combining multiple models. Unlike single learners, ensembles exploit model diversity to capture\ndifferent aspects of data distributions, thereby reducing both bias and variance. This makes them highly effective across\ndomains where robustness and reliability are critical.\nSeveral ensemble strategies exist, including bagging, boosting, and stacking. Bagging reduces variance by training\nmultiple models on bootstrapped subsets of data and aggregating their outputs, with Random Forest being the most\nwidely used example. Boosting reduces bias by sequentially training learners, where each subsequent model focuses on\ncorrecting the errors of its predecessors. Stacking differs by training multiple base models in parallel and using a meta\nlearner to integrate their predictions. Stacking is particularly powerful because it can capture complementary strengths\nof heterogeneous learners.\nAmong ensemble methods, boosting algorithms such as XGBoost, LightGBM, CatBoost, and AdaBoost have consis\u0002tently shown strong performance. Each algorithm has unique strengths. XGBoost employs regularization and efficient\ntree pruning, making it scalable and less prone to overfitting. LightGBM introduces histogram-based feature binning\nand leaf-wise growth, enabling faster training and reduced memory usage, especially on large datasets. CatBoost\nnatively handles categorical variables using ordered boosting and statistical encodings, which minimizes target leakage\nand improves generalization. AdaBoost, though simpler, provides robustness by iteratively reweighting misclassified\nsamples, ensuring that weak learners are adaptively strengthened.\n\u2217\nSTACKING ENSEMBLE LEARNING A PREPRINT\nWhile each boosting method is powerful, none consistently dominates across all tasks. XGBoost may demand extensive\ntuning, LightGBM risks overfitting with deep trees, CatBoost can be computationally heavier, and AdaBoost may\nunderperform on highly complex feature interactions. These trade-offs make boosting algorithms ideal candidates for\nstacking, where their individual advantages can be combined to achieve superior performance.\nIn this work, we propose a stacking ensemble framework that employs XGBoost, LightGBM, CatBoost, and AdaBoost\nas base learners. Their predictions are combined by a meta learner, for which we choose Random Forest. The choice of\nRandom Forest is motivated by its robustness and interpretability. As a bagging-based method, Random Forest builds\nmultiple decision trees using bootstrapped samples and random feature subsets, which reduces variance and prevents\noverfitting. As a meta learner, Random Forest can effectively capture non-linear dependencies between base predictions,\noffering a richer integration mechanism than simple averaging or linear models.\n2 Boosting and Bagging in a Nutshell\nBoosting, introduced by Freund and Schapire (1997), combines many weak learners to create a highly accurate model.\nFriedman (2001, 2002) and Natekin & Knoll (2013) refined this through Gradient Boosting Machines (GBM), which\niteratively improve predictions by minimizing a loss function. Instead of training one model, GBM starts with a\nsimple base model and repeatedly adds new models to correct previous errors. This paper focuses on the mathematical\nfoundation of gradient boosting\u2014covering optimization methods, loss functions, various boosting algorithms, and their\napplication to ranking real-world data (1).\nDecision Contribution algorithm designed to make Gradient Boosted Decision Trees (GBDT) locally explainable by\ndecomposing each prediction into the sum of node-level contributions along the decision path.\nMathematically, each model iteration is defined as:\nFt(x) = Ft\u22121(x) + \u03b1ht(x), (1)\nwhere \u03b1 is the learning rate and ht(x) is the residual tree. Each tree prediction can be rewritten as the sum of\ncontributions from every node decision:\nhl(x) = X\ni(l)\nj=0\ngl(sj ), (2)\nleading to the overall GBDT formulation:\nFt(x) = Xt\nl=0\nX\ni(l)\nj=0\n\u03b1gl(sj ), (3)\nwhere gl(sj ) quantifies the influence of each decision node sj on the final prediction.\nThe authors demonstrate that this approach provides intrinsic explainability unlike post-hoc XAI models such as LIME\nand SHAP, which approximate explanations externally. Through experiments on Diabetes and Concrete datasets, the\nmethod proved consistent under correlation and noise tests, and outperformed SHAP in handling outliers (2).\n2.1 Decision Tree\na decision tree is described as a binary tree structure used for classification or regression tasks. Each internal node\nrepresents a condition on an attribute (for example, Ai > vj ), and each leaf node provides a numerical value or class\nlabel. To classify an input instance x = (v1, v2, . . . , vn), the model follows a path from the root to a leaf based on\nwhether the conditions at each node are satisfied. The output of the tree, denoted as w(T, x), corresponds to the value at\nthe leaf reached by x.\nA forest is defined as a collection of several decision trees F\nj = {T\nj\n1\n, Tj\n2\n, . . . , Tj\npj\n}, where the total output for an\ninstance x is obtained by summing the outputs of the individual trees:\nw(F\nj\n, x) = Xpj\nk=1\nw(T\nj\nk\n, x)\nA boosted tree model (BT) consists of multiple forests, each corresponding to a class. For binary classification, the final\nprediction is defined as:\nBT(x) = \u001a\n1, if w(F, x) > 0\n0, otherwise\n2\nSTACKING ENSEMBLE LEARNING A PREPRINT\nand for multiclass classification, the class with the highest total weight is selected:\nBT(x) = j if w(F\nj\n, x) > w(F\ni\n, x) \u2200i \u0338= j\nThe paper also introduces the concept of abductive explanations, which are minimal subsets of features that are sufficient\nto produce the same classification result as the full input. Formally, a subset t of the feature assignments tx is an\nabductive explanation if, for any instance x\n\u2032\nconsistent with t, the model gives the same output as for x:\n\u2200x\n\u2032\nsuch that t \u2286 tx\u2032 =\u21d2 f(x\n\u2032\n) = f(x)\nThe minimal version of such a subset, containing no unnecessary features, ...",
      "url": "https://www.researchsquare.com/article/rs-7944070/v1.pdf?c=1761822346000"
    },
    {
      "title": "Blending Ensemble Machine Learning With Python",
      "text": "Blending Ensemble Machine Learning With Python - MachineLearningMastery.comBlending Ensemble Machine Learning With Python - MachineLearningMastery.com\n### [Navigation](#navigation)\n[![MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2024/10/mlm-logo.svg)](https://machinelearningmastery.com/)\nMaking developers awesome at machine learning\nMaking developers awesome at machine learning\n[Click to Take the FREE Ensemble Learning Crash-Course]()\n# Blending Ensemble Machine Learning With Python\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onApril 27, 2021in[Ensemble Learning](https://machinelearningmastery.com/category/ensemble-learning/)[**30](https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python/#comments)\nShare*Post*Share\n**Blending**is an ensemble machine learning algorithm.\nIt is a colloquial name for**stacked generalization**or stacking ensemble where instead of fitting the meta-model on out-of-fold predictions made by the base model, it is fit on predictions made on a holdout dataset.\nBlending was used to describe stacking models that combined many hundreds of predictive models by competitors in the $1M Netflix machine learning competition, and as such, remains a popular technique and name for stacking in competitive machine learning circles, such as the Kaggle community.\nIn this tutorial, you will discover how to develop and evaluate a blending ensemble in python.\nAfter completing this tutorial, you will know:\n* Blending ensembles are a type of stacking where the meta-model is fit using predictions on a holdout validation dataset instead of out-of-fold predictions.\n* How to develop a blending ensemble, including functions for training the model and making predictions on new data.\n* How to evaluate blending ensembles for classification and regression predictive modeling problems.\n**Kick-start your project**with my new book[Ensemble Learning Algorithms With Python](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/), including*step-by-step tutorials*and the*Python source code*files for all examples.\nLet\u2019s get started.\n![Blending Ensemble Machine Learning With Python](https://machinelearningmastery.com/wp-content/uploads/2020/11/Blending-Ensemble-Machine-Learning-With-Python.jpg)\nBlending Ensemble Machine Learning With Python\nPhoto by[Nathalie](https://www.flickr.com/photos/nathalie-photos/37421605474/), some rights reserved.\n## Tutorial Overview\nThis tutorial is divided into four parts; they are:\n1. Blending Ensemble\n2. Develop a Blending Ensemble\n3. Blending Ensemble for Classification\n4. Blending Ensemble for Regression## Blending Ensemble\nBlending is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models.\nAs such, blending is the same as[stacked generalization](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/), known as stacking, broadly conceived. Often, blending and stacking are used interchangeably in the same paper or model description.\n> Many machine learning practitioners have had success using stacking and related techniques to boost prediction accuracy beyond the level obtained by any of the individual models. In some contexts, stacking is also referred to as blending, and we will use the terms interchangeably here.\n&#8212;[Feature-Weighted Linear Stacking](https://arxiv.org/abs/0911.0460), 2009.\nThe architecture of a stacking model involves two or more base models, often referred to as level-0 models, and a meta-model that combines the predictions of the base models, referred to as a level-1 model. The meta-model is trained on the predictions made by base models on out-of-sample data.\n* **Level-0 Models**(*Base-Models*): Models fit on the training data and whose predictions are compiled.\n* **Level-1 Model**(*Meta-Model*): Model that learns how to best combine the predictions of the base models.\nNevertheless, blending has specific connotations for how to construct a stacking ensemble model.\nBlending may suggest developing a stacking ensemble where the base-models are machine learning models of any type, and the meta-model is a linear model that &#8220;*blends*&#8221; the predictions of the base-models.\nFor example, a linear regression model when predicting a numerical value or a logistic regression model when predicting a class label would calculate a weighted sum of the predictions made by base models and would be considered a blending of predictions.\n* **Blending Ensemble**: Use of a linear model, such as linear regression or logistic regression, as the meta-model in a stacking ensemble.\nBlending was the term commonly used for stacking ensembles during the Netflix prize in 2009. The prize involved teams seeking movie recommendation predictions that performed better than the native Netflix algorithm and a[US$1M prize](https://en.wikipedia.org/wiki/Netflix_Prize)was awarded to the team that achieved a 10 percent performance improvement.\n> Our RMSE=0.8643^2 solution is a linear blend of over 100 results. [&#8230;] Throughout the description of the methods, we highlight the specific predictors that participated in the final blended solution.\n&#8212;[The BellKor 2008 Solution to the Netflix Prize](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf), 2008.\nAs such, blending is a colloquial term for ensemble learning with a stacking-type architecture model. It is rarely, if ever, used in textbooks or academic papers, other than those related to competitive machine learning.\nMost commonly, blending is used to describe the specific application of stacking where the meta-model is trained on the predictions made by base-models on a hold-out validation dataset. In this context, stacking is reserved for a meta-model that is trained on out-of fold predictions during a cross-validation procedure.\n* **Blending**: Stacking-type ensemble where the meta-model is trained on predictions made on a holdout dataset.\n* **Stacking**: Stacking-type ensemble where the meta-model is trained on out-of-fold predictions made during k-fold cross-validation.\nThis distinction is common among the Kaggle competitive machine learning community.\n> Blending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. [&#8230;] With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\n&#8212;[Kaggle Ensemble Guide](https://mlwave.com/kaggle-ensembling-guide/), MLWave, 2015.\nWe will use this latter definition of blending.\nNext, let&#8217;s look at how we can implement blending.\n### Want to Get Started With Ensemble Learning?\nTake my free 7-day email crash course now (with sample code).\nClick to sign-up and also get a free PDF Ebook version of the course.\nDownload Your FREE Mini-Course\n## Develop a Blending Ensemble\nThe scikit-learn library does not natively support blending at the time of writing.\nInstead, we can implement it ourselves using scikit-learn models.\nFirst, we need to create a number of base models. These can be any models we like for a regression or classification problem. We can define a function*get\\_models()*that returns a list of models where each model is defined as a tuple with a name and the configured classifier or regression object.\nFor example, for a classification problem, we might use a logistic regression, kNN, decision tree, SVM, and Naive Bayes model.\n# get a list of base models\rdef get\\_models():\rmodels = list()\rmodels.append(('lr', LogisticRegression()))\rmodels.append(('knn', KNeighborsClassifier()))\rmodels.append(('cart', DecisionTreeClassifier()))\rmodels.append(('svm', SVC(probability=True)))\rmodels.append(('bayes', GaussianNB()))\rreturn models\n1\n2\n3\n4\n5\n6\n7\n8\n9\n|\n# ...",
      "url": "https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python"
    },
    {
      "title": "Otto Product Classification Winner\u2019s Interview: 2nd place, Alexander Guschin \u00af\\_(\u30c4)_/\u00af",
      "text": "<div><div><div><div><a href=\"https://medium.com/@kaggleteam?source=post_page---byline--e9248c318f30---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/kaggle-blog?source=post_page---byline--e9248c318f30---------------------------------------\"><div><p></p></div></a></div></div><figure></figure><p>The <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/otto-group-product-classification-challenge\">Otto Group Product Classification Challenge</a> made Kaggle history as our most popular competition ever. <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/aguschin\">Alexander Guschin</a> finished <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/otto-group-product-classification-challenge/leaderboard/private\">in 2nd place</a> ahead of 3,845 other data scientists. In this blog, Alexander shares his stacking centered approach and explains why you should never underestimate the nearest neighbours algorithm.</p><h2>The Basics</h2><h2>What was your background prior to entering this challenge?</h2><p>I have some theoretical understanding of machine learning thanks to my base institute ( <a href=\"https://web.archive.org/web/20180114032307/http://mipt.ru/en/\">Moscow Institute of Physics and Technology</a>) and our professor <a href=\"https://web.archive.org/web/20180114032307/https://scholar.google.com/citations?user=KIW4fnsAAAAJ\">Konstantin Vorontsov</a>, one of the top Russian machine learning specialists. As for my acquaintance with practical problems, another great Russian data scientist who once was Top-1 on Kaggle, <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/dyakonov\">Alexander D\u2019yakonov</a>, used to teach a course on practical machine learning every autumn which gave me very good basis. Kagglers may know this course as <a href=\"https://web.archive.org/web/20180114032307/http://www.machinelearning.ru/wiki/index.php?title=%D0%A1%D0%BF%D0%B5%D1%86%D0%BA%D1%83%D1%80%D1%81_%C2%AB%D0%9F%D1%80%D0%B8%D0%BA%D0%BB%D0%B0%D0%B4%D0%BD%D1%8B%D0%B5_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%C2%BB\">PZAD</a>.</p><h2>How did you get started competing on Kaggle?</h2><p>I got started in 2014\u2019s autumn in \u201c <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/forest-cover-type-prediction\">Forest Cover Type Prediction</a>\u201d. At that time I had no experience in solving machine learning problems. I found excellent benchmarks in \u201c <a href=\"https://web.archive.org/web/20180114032307/https://www.kaggle.com/c/titanic\">Titanic: Machine Learning from Disaster</a> \u201c which helped me a lot. After that I understand that machine learning is extremely interesting for me and just tried to participate in every competition I could.</p><h2>What made you decide to enter this competition?</h2><p>I wanted to check some ideas for my bachelor work. I liked that Otto competition has quite reliable dataset. You can check everything on cross-validation and changes on CV were close enough to leaderboard. Also, the spirit of competition is quite appropriate for checking ideas.</p><h2>Let\u2019s Get Technical</h2><h2>What preprocessing and supervised learning methods did you use?</h2><p><strong>The main idea of my solution is stacking.</strong> Stacking helps you to combine different methods\u2019 predictions of Y (or labels when it comes to multiclass problems) as \u201cmetafeatures\u201d. Basically, to obtain metafeature for train, you split your data into K folds, training K models on K-1 parts while making prediction for 1 part that was left aside for each K-1 group. To obtain metafeature for test, you can average predictions from these K models or make single prediction based on all train data. After that you train metaclassifier on features &amp; metafeatures and average predictions if you have several metaclassifiers.</p><p>In the beginning of working on the competition I found useful to split data in two groups : (1) train &amp; test, (2) TF-IDF(train) &amp; TF-IDF(test). Many parts of my solution use these two groups in parallel.</p><p><strong>Talking about supervised methods, I\u2019ve found that Xgboost and neural networks both give good results on data.</strong> Thus I decided to use them as metaclassifiers in my ensemble.</p><p>Nevertheless, KNN usually gives predictions that are very different from decision trees or neural networks, so I include them on the first level of ensemble as metafeatures. Random forest and xgboost also happened to be useful as metafeatures.</p><h2>What was your most important insight into the data?</h2><p>Probably the main insight was that <strong>KNN is capable of making very good metafeatures</strong>. Never underestimate nearest neighbours algorithm.</p><p>Very important were to combine NN and XGB predictions on the second level. While my final second- level NN and XGB separately scored around .391 on private LB, the combination of them achieved .386, which is very significant improvement. Bagging on the second level helped a lot too.</p><p>Beside this, TSNE in 2 dimensions looks very interesting. We can see on the plot that we have some examples which most likely will be misclassified by our algorithm. It does mean that it won\u2019t be easy to find a way to post-process our predictions to improve logloss.</p><p>Also, it seemed interesting that some classes were related closer than others, for example class 1 and class 2. It\u2019s worth trying to distinguish these classes specially.</p><h2>Were you surprised by any of your findings?</h2><p>Unfortunately, it appears that you won\u2019t necessarily improve your model if you will make your metafeatures better. And when it comes to ensembling, all that you can count on is your understanding of algorithms<strong> (basically, the more diverse metafeatures you have, the better)</strong> and effort to try as many metafeatures as possible.</p><h2>Which tools did you use?</h2><p>I only used sklearn, xgboost, lasagne. These are perfect machine learning libraries and I would recommend them to anyone who is starting to compete on Kaggle. Relying on my past experience they are sufficient to try different methods and achieve great results in most Kaggle competitions.</p><h2>Words of Wisdom</h2><h2>Do you have any advice for those just getting started in data science?</h2><p>I think that the most useful advice here is try not to stuck trying to fine-tune parameters or stuck using the same approaches every competition. Read through forums, understand winning solutions of past competitions and all of this will give you significant boost whatever your level is. <strong>In another words, my point is that reading past solutions is as important as solving competitions.</strong></p><p>Also, when you first starting to work on machine learning problems you could make some nasty mistakes which will cost you a lot of time and efforts. Thus it is great if you can work in a team with someone and ask him to check you code or try the same methods on his own. Besides always compare your performance with people on forums. <strong>When you see that you algorithm performs much worse than people report on forum, go and check benchmarks for this and other recent competitions and try to figure out the mistake.</strong></p><h2>Bio</h2><p><strong>Alexander Guschin</strong></p><figure></figure><p>is 4th year student in <a href=\"https://web.archive.org/web/20180114032307/http://mipt.ru/en/\">Moscow Institute of Physics and Technology</a>. Currently, Alexander is finishing his bachelor diploma work about ensembling methods.</p></div></div>",
      "url": "https://medium.com/kaggle-blog/otto-product-classification-winners-interview-2nd-place-alexander-guschin-%E3%83%84-e9248c318f30"
    }
  ]
}