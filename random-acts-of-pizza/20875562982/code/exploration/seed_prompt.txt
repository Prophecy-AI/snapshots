## Problem Type
Binary classification combining text features (request title, text) with tabular metadata (Reddit activity metrics, user flair, timestamps).

## Reference Notebooks for Data Characteristics
- `exploration/eda.ipynb` - Contains full EDA: 2,878 training samples, 24.8% positive class (class imbalance), text length distributions, feature types (16 int, 9 object, 6 float, 1 bool), missing values in requester_user_flair (75% missing)

## Data Understanding
**Class Imbalance**: ~25% positive class requires special handling. See eda.ipynb for exact distribution.
**Text Features**: request_text (avg 402 chars), request_title (avg 72 chars) - both need preprocessing
**Categorical Features**: requester_user_flair with high cardinality (3 categories but 75% missing)
**Temporal Features**: Unix timestamps available for feature engineering

## Models
For text + tabular classification problems, winning Kaggle solutions typically use:
- **Gradient Boosting (Primary)**: XGBoost, LightGBM, or CatBoost trained on combined text embeddings + tabular features
- **Neural Networks (Secondary)**: BERT/RoBERTa for text encoding combined with tabular features in downstream classifier
- **Ensemble Size**: 3-5 diverse models (mix of tree-based and neural approaches)

## Text Feature Engineering
**Preprocessing** (Critical for Reddit/social media language):
- Preserve informal cues: DON'T remove all punctuation (can indicate sentiment/sarcasm)
- Normalize elongated words: "soooo" → "so"
- Handle Reddit-specific artifacts: strip/normalize URLs, user mentions (/u/username), subreddit tags (/r/subreddit)
- Clean markdown formatting while preserving emoji for sentiment
- Apply lemmatization (preferred over stemming for social media)
- Create custom stopword list including Reddit-specific terms
- Combine request_title and request_text into single document

**Feature Extraction**:
- TF-IDF vectors (unigrams + bigrams) for gradient boosting models
- Sentence embeddings (BERT, RoBERTa) for neural approaches
- Text length features: char count, word count, avg word length
- Sentiment analysis scores
- Named entity recognition features
- Punctuation density and patterns
- Capitalization patterns (ALL CAPS words count)

## Tabular Feature Engineering
**Metadata Features**:
- Log transforms for count features (upvotes, comments, posts) to reduce skewness
- Ratios: upvotes/comments, comments/posts, karma metrics
- Differences between request time and retrieval time metrics
- User activity rates: comments per day, posts per day
- Subreddit diversity metrics from requester_subreddits_at_request
- Account age normalized by activity (comments per day of account age)

**Categorical Encoding**:
- **requester_user_flair**: Create explicit "Missing" category for 75% missing values, then apply target encoding
- One-hot encoding for low-cardinality categorical features
- Frequency encoding for high-cardinality features
- Target encoding with careful cross-validation to avoid leakage

**Temporal Features**:
- Extract hour of day, day of week from timestamps
- Cyclical encoding for time features (sin/cos transforms)
- Time since account creation normalized by request time
- Posting time relative to peak Reddit hours

## Handling Class Imbalance
**Critical for this dataset (24.8% positive class)**:
- Use AUC-ROC as evaluation metric (provided in competition)
- Apply scale_pos_weight in XGBoost/LightGBM (calculate as negative/positive ratio ≈ 3.0)
- Consider class_weight='balanced' in scikit-learn models
- Optional: Try SMOTE oversampling on minority class
- Focus on PR-AUC during validation for imbalanced metrics
- Use stratified sampling throughout to preserve class distribution

## Validation Strategy
- Stratified K-Fold (k=5) to preserve class distribution
- Time-based splits if temporal leakage is a concern
- Use early stopping on validation AUC-ROC
- Monitor both AUC-ROC and PR-AUC for imbalanced performance
- Create separate validation sets for text-based and tabular-based models

## Ensembling
**Stacking Approach**:
- Level 1: Diverse models (XGBoost on TF-IDF, LightGBM on embeddings, CatBoost on combined)
- Level 2: Logistic regression or simple averaging
- Use out-of-fold predictions for meta-features
- Include both text-heavy and metadata-heavy models for diversity

**Blending**:
- Weighted average based on validation performance
- Rank averaging for robustness
- Geometric mean for probability calibration

## Optimization
**Hyperparameter Tuning**:
- Bayesian optimization (Optuna) for efficient search
- Focus on: learning_rate, max_depth, min_child_samples, subsample
- Use early stopping to prevent overfitting
- Tune scale_pos_weight carefully for class imbalance

**Feature Selection**:
- SHAP values for feature importance interpretation
- Recursive feature elimination based on validation score
- Correlation analysis to remove redundant features
- Focus on features that work well across multiple model types