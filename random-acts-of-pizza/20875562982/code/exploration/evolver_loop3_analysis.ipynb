{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e136b4b",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis\n",
    "\n",
    "## Objective\n",
    "Analyze the current experimental results and identify critical issues based on evaluator feedback.\n",
    "\n",
    "Focus areas:\n",
    "1. Model mismatch - why logistic regression is suboptimal\n",
    "2. Data leakage risk in current feature engineering\n",
    "3. Dimensionality vs sample size analysis\n",
    "4. Feature scaling issues\n",
    "5. Path forward to gold threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439bbdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:40:54.423563Z",
     "iopub.status.busy": "2026-01-10T11:40:54.422791Z",
     "iopub.status.idle": "2026-01-10T11:40:56.148445Z",
     "shell.execute_reply": "2026-01-10T11:40:56.147793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training samples: 2878\n",
      "Features per sample: ~11,158 (from exp_002)\n",
      "Dimensionality ratio: 3.9:1 (MASSIVE OVERFITTING RISK)\n",
      "Positive class rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Features per sample: ~11,158 (from exp_002)\")\n",
    "print(f\"Dimensionality ratio: {11158/len(df_train):.1f}:1 (MASSIVE OVERFITTING RISK)\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbd8422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:40:56.151108Z",
     "iopub.status.busy": "2026-01-10T11:40:56.150327Z",
     "iopub.status.idle": "2026-01-10T11:40:56.191081Z",
     "shell.execute_reply": "2026-01-10T11:40:56.190469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALE ANALYSIS ===\n",
      "\n",
      "Current exp_002 features:\n",
      "- TF-IDF word features: 0-1 range (sparse)\n",
      "- TF-IDF char features: 0-1 range (sparse)\n",
      "- SVD components: centered, but varying scales\n",
      "- Numeric features: varying scales\n",
      "\n",
      "Numeric feature ranges:\n",
      "requester_number_of_comments_at_request_log: 0.00 to 6.89\n",
      "requester_number_of_posts_at_request_log: 0.00 to 6.77\n",
      "requester_upvotes_plus_downvotes_at_request_log: 0.00 to 14.07\n",
      "upvotes_per_comment: 0.00 to 98921.00\n",
      "comments_per_post: 0.00 to 384.00\n",
      "account_age_years: 0.00 to 7.69\n",
      "text_length: 0.00 to 4460.00\n",
      "word_count: 0.00 to 854.00\n",
      "\n",
      "⚠️ PROBLEM: Features have wildly different scales!\n",
      "   - account_age_years: ~0-10\n",
      "   - text_length: ~0-5000\n",
      "   - upvotes_per_comment: ~0-100\n",
      "   - TF-IDF: 0-1\n",
      "\n",
      " Logistic regression will be dominated by large-scale features!\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature scale mismatches\n",
    "print(\"=== FEATURE SCALE ANALYSIS ===\")\n",
    "print(\"\\nCurrent exp_002 features:\")\n",
    "print(\"- TF-IDF word features: 0-1 range (sparse)\")\n",
    "print(\"- TF-IDF char features: 0-1 range (sparse)\")\n",
    "print(\"- SVD components: centered, but varying scales\")\n",
    "print(\"- Numeric features: varying scales\")\n",
    "\n",
    "# Create sample numeric features like in exp_002\n",
    "count_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feat in count_features:\n",
    "    df_train[f'{feat}_log'] = np.log1p(df_train[feat])\n",
    "\n",
    "df_train['upvotes_per_comment'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_comments_at_request'] + 1)\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days_at_request'] / 365.25\n",
    "df_train['text_length'] = df_train['request_text_edit_aware'].fillna('').str.len()\n",
    "df_train['word_count'] = df_train['request_text_edit_aware'].fillna('').str.split().str.len()\n",
    "\n",
    "numeric_features = [\n",
    "    'requester_number_of_comments_at_request_log',\n",
    "    'requester_number_of_posts_at_request_log', \n",
    "    'requester_upvotes_plus_downvotes_at_request_log',\n",
    "    'upvotes_per_comment',\n",
    "    'comments_per_post',\n",
    "    'account_age_years',\n",
    "    'text_length',\n",
    "    'word_count'\n",
    "]\n",
    "\n",
    "print(\"\\nNumeric feature ranges:\")\n",
    "for feat in numeric_features:\n",
    "    print(f\"{feat}: {df_train[feat].min():.2f} to {df_train[feat].max():.2f}\")\n",
    "\n",
    "print(\"\\n⚠️ PROBLEM: Features have wildly different scales!\")\n",
    "print(\"   - account_age_years: ~0-10\")\n",
    "print(\"   - text_length: ~0-5000\")\n",
    "print(\"   - upvotes_per_comment: ~0-100\")\n",
    "print(\"   - TF-IDF: 0-1\")\n",
    "print(\"\\n Logistic regression will be dominated by large-scale features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9839e4bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:40:56.193000Z",
     "iopub.status.busy": "2026-01-10T11:40:56.192771Z",
     "iopub.status.idle": "2026-01-10T11:40:56.198469Z",
     "shell.execute_reply": "2026-01-10T11:40:56.197877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LEAKAGE DEMONSTRATION ===\n",
      "\n",
      "Current approach (WRONG):\n",
      "1. Fit TF-IDF on ALL training data\n",
      "2. Fit SVD on ALL training data\n",
      "3. Split into CV folds\n",
      "4. Train model on train_idx, evaluate on val_idx\n",
      "\n",
      "❌ PROBLEM: IDF statistics and SVD components contain information from validation set!\n",
      "\n",
      "Correct approach:\n",
      "1. Split into CV folds first\n",
      "2. For each fold:\n",
      "   - Fit TF-IDF ONLY on train_idx\n",
      "   - Fit SVD ONLY on train_idx\n",
      "   - Transform both train_idx and val_idx\n",
      "   - Train model, evaluate\n",
      "\n",
      "✓ This prevents leakage of distributional information\n",
      "\n",
      "=== IMPACT OF LEAKAGE ===\n",
      "With 2,878 samples and 5-fold CV:\n",
      "- Each validation fold: ~576 samples\n",
      "- Each training fold: ~2,302 samples\n",
      "- Validation set is 20% of total data\n",
      "\n",
      "Fitting transformers on full data means:\n",
      "- IDF scores incorporate validation set word frequencies\n",
      "- SVD components are optimized for validation set patterns\n",
      "- CV scores may be OVEROPTIMISTIC\n",
      "\n",
      "This is especially problematic with small datasets!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the leakage issue\n",
    "print(\"=== DATA LEAKAGE DEMONSTRATION ===\")\n",
    "print(\"\\nCurrent approach (WRONG):\")\n",
    "print(\"1. Fit TF-IDF on ALL training data\")\n",
    "print(\"2. Fit SVD on ALL training data\")\n",
    "print(\"3. Split into CV folds\")\n",
    "print(\"4. Train model on train_idx, evaluate on val_idx\")\n",
    "print(\"\\n❌ PROBLEM: IDF statistics and SVD components contain information from validation set!\")\n",
    "\n",
    "print(\"\\nCorrect approach:\")\n",
    "print(\"1. Split into CV folds first\")\n",
    "print(\"2. For each fold:\")\n",
    "print(\"   - Fit TF-IDF ONLY on train_idx\")\n",
    "print(\"   - Fit SVD ONLY on train_idx\")\n",
    "print(\"   - Transform both train_idx and val_idx\")\n",
    "print(\"   - Train model, evaluate\")\n",
    "print(\"\\n✓ This prevents leakage of distributional information\")\n",
    "\n",
    "# Show how much this matters with a small example\n",
    "print(\"\\n=== IMPACT OF LEAKAGE ===\")\n",
    "print(\"With 2,878 samples and 5-fold CV:\")\n",
    "print(\"- Each validation fold: ~576 samples\")\n",
    "print(\"- Each training fold: ~2,302 samples\")\n",
    "print(\"- Validation set is 20% of total data\")\n",
    "print(\"\\nFitting transformers on full data means:\")\n",
    "print(\"- IDF scores incorporate validation set word frequencies\")\n",
    "print(\"- SVD components are optimized for validation set patterns\")\n",
    "print(\"- CV scores may be OVEROPTIMISTIC\")\n",
    "print(\"\\nThis is especially problematic with small datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ecf72b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:40:56.200905Z",
     "iopub.status.busy": "2026-01-10T11:40:56.200284Z",
     "iopub.status.idle": "2026-01-10T11:40:56.208475Z",
     "shell.execute_reply": "2026-01-10T11:40:56.207929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL CAPACITY ANALYSIS ===\n",
      "\n",
      "Logistic Regression limitations:\n",
      "1. Linear decision boundaries only\n",
      "2. Assumes feature independence (no interactions)\n",
      "3. Sensitive to feature scaling\n",
      "4. Struggles with high dimensionality\n",
      "5. No built-in feature selection\n",
      "\n",
      "Current situation:\n",
      "- Samples: 2,878\n",
      "- Features: 11,158\n",
      "- Ratio: 3.9 features per sample\n",
      "- Feature types: Mixed (sparse TF-IDF + dense numeric)\n",
      "- Scales: Inconsistent (0-1 vs 0-5000)\n",
      "\n",
      "Why LightGBM/XGBoost would be better:\n",
      "1. Handle mixed data types natively\n",
      "2. Capture non-linear interactions automatically\n",
      "3. Robust to unscaled features\n",
      "4. Built-in feature importance/selection\n",
      "5. Better regularization for high-dimensional data\n",
      "6. Tree-based splitting handles sparse features well\n",
      "\n",
      "Expected improvement from model upgrade:\n",
      "- Competition post-mortems: +0.03 to +0.08 AUC\n",
      "- Our gap to gold: 0.3346 points\n",
      "- Model upgrade alone won't get us to gold, but it's necessary\n",
      "- Combined with proper feature engineering: +0.10 to +0.15 potential\n"
     ]
    }
   ],
   "source": [
    "# Analyze model capacity issues\n",
    "print(\"=== MODEL CAPACITY ANALYSIS ===\")\n",
    "\n",
    "print(\"\\nLogistic Regression limitations:\")\n",
    "print(\"1. Linear decision boundaries only\")\n",
    "print(\"2. Assumes feature independence (no interactions)\")\n",
    "print(\"3. Sensitive to feature scaling\")\n",
    "print(\"4. Struggles with high dimensionality\")\n",
    "print(\"5. No built-in feature selection\")\n",
    "\n",
    "print(\"\\nCurrent situation:\")\n",
    "print(f\"- Samples: 2,878\")\n",
    "print(f\"- Features: 11,158\")\n",
    "print(f\"- Ratio: 3.9 features per sample\")\n",
    "print(f\"- Feature types: Mixed (sparse TF-IDF + dense numeric)\")\n",
    "print(f\"- Scales: Inconsistent (0-1 vs 0-5000)\")\n",
    "\n",
    "print(\"\\nWhy LightGBM/XGBoost would be better:\")\n",
    "print(\"1. Handle mixed data types natively\")\n",
    "print(\"2. Capture non-linear interactions automatically\")\n",
    "print(\"3. Robust to unscaled features\")\n",
    "print(\"4. Built-in feature importance/selection\")\n",
    "print(\"5. Better regularization for high-dimensional data\")\n",
    "print(\"6. Tree-based splitting handles sparse features well\")\n",
    "\n",
    "print(\"\\nExpected improvement from model upgrade:\")\n",
    "print(\"- Competition post-mortems: +0.03 to +0.08 AUC\")\n",
    "print(\"- Our gap to gold: 0.3346 points\")\n",
    "print(\"- Model upgrade alone won't get us to gold, but it's necessary\")\n",
    "print(\"- Combined with proper feature engineering: +0.10 to +0.15 potential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6a3888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:40:56.210545Z",
     "iopub.status.busy": "2026-01-10T11:40:56.210334Z",
     "iopub.status.idle": "2026-01-10T11:40:59.835262Z",
     "shell.execute_reply": "2026-01-10T11:40:59.833796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIMENSIONALITY REDUCTION ANALYSIS ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TF-IDF dimensions: (2878, 5000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD(50): 0.134 variance explained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD(75): 0.178 variance explained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD(100): 0.216 variance explained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD(150): 0.281 variance explained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD(200): 0.336 variance explained\n",
      "\n",
      "Recommendation:\n",
      "- Current: 100 word + 50 char = 150 components\n",
      "- Total features: 150 + 8 numeric = 158\n",
      "- Ratio: 158/2878 = 5.5% (much better than 387%)\n",
      "\n",
      "But we can be more aggressive:\n",
      "- Try 50 word + 25 char = 75 components\n",
      "- Total features: 75 + 8 numeric = 83\n",
      "- Ratio: 83/2878 = 2.9% (even better)\n",
      "- Competition winners often use 50-100 components total\n"
     ]
    }
   ],
   "source": [
    "# Analyze dimensionality reduction options\n",
    "print(\"=== DIMENSIONALITY REDUCTION ANALYSIS ===\")\n",
    "\n",
    "# Simulate different SVD component counts\n",
    "sample_text = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Fit TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
    "tfidf_matrix = vectorizer.fit_transform(sample_text)\n",
    "\n",
    "print(f\"Original TF-IDF dimensions: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Test different SVD component counts\n",
    "component_counts = [50, 75, 100, 150, 200]\n",
    "explained_variances = []\n",
    "\n",
    "for n_components in component_counts:\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    svd.fit(tfidf_matrix)\n",
    "    explained_variances.append(svd.explained_variance_ratio_.sum())\n",
    "    print(f\"SVD({n_components}): {svd.explained_variance_ratio_.sum():.3f} variance explained\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"- Current: 100 word + 50 char = 150 components\")\n",
    "print(\"- Total features: 150 + 8 numeric = 158\")\n",
    "print(\"- Ratio: 158/2878 = 5.5% (much better than 387%)\")\n",
    "print(\"\\nBut we can be more aggressive:\")\n",
    "print(\"- Try 50 word + 25 char = 75 components\")\n",
    "print(\"- Total features: 75 + 8 numeric = 83\")\n",
    "print(\"- Ratio: 83/2878 = 2.9% (even better)\")\n",
    "print(\"- Competition winners often use 50-100 components total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d1d39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:40:59.840025Z",
     "iopub.status.busy": "2026-01-10T11:40:59.839382Z",
     "iopub.status.idle": "2026-01-10T11:40:59.848816Z",
     "shell.execute_reply": "2026-01-10T11:40:59.848067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEY FINDINGS ===\n",
      "\n",
      "1. MODEL MISMATCH (CRITICAL)\n",
      "   - Logistic regression is fundamentally wrong for this problem\n",
      "   - Need tree-based model (LightGBM/XGBoost) for non-linear patterns\n",
      "   - Expected gain: +0.03 to +0.08 AUC\n",
      "\n",
      "2. DATA LEAKAGE (HIGH PRIORITY)\n",
      "   - TF-IDF and SVD fitted on full data before CV splits\n",
      "   - Validation set information leaks into transformers\n",
      "   - CV scores may be optimistic\n",
      "   - Fix: Move ALL fitting inside CV loops\n",
      "\n",
      "3. DIMENSIONALITY CRISIS (HIGH PRIORITY)\n",
      "   - 11,158 features vs 2,878 samples = 3.9:1 ratio\n",
      "   - Extreme overfitting risk\n",
      "   - Solution: Aggressive SVD reduction (50-75 components total)\n",
      "\n",
      "4. FEATURE SCALING (MEDIUM PRIORITY)\n",
      "   - TF-IDF (0-1) mixed with numeric features (0-5000)\n",
      "   - Logistic regression dominated by large-scale features\n",
      "   - LightGBM/XGBoost don't need scaling, but proper preprocessing helps\n",
      "\n",
      "5. CONVERGENCE ISSUES (MEDIUM PRIORITY)\n",
      "   - Warnings despite max_iter=1000\n",
      "   - Indicates poor conditioning\n",
      "   - Will resolve with better model and dimensionality reduction\n",
      "\n",
      "=== PATH TO GOLD ===\n",
      "Current: 0.6445\n",
      "Target: 0.9791\n",
      "Gap: 0.3346\n",
      "\n",
      "Next experiment priorities:\n",
      "1. Switch to LightGBM (highest impact)\n",
      "2. Fix data leakage (proper CV)\n",
      "3. Reduce dimensionality (50-75 SVD components)\n",
      "4. Optimize feature combinations\n",
      "5. Ensemble diverse models\n",
      "\n",
      "Expected trajectory:\n",
      "- Experiment 004 (LightGBM + fix leakage): 0.68-0.72\n",
      "- Experiment 005 (optimized features): 0.75-0.80\n",
      "- Experiment 006 (ensembling): 0.82-0.85\n",
      "- Further iterations needed to reach 0.9791\n"
     ]
    }
   ],
   "source": [
    "# Summary of findings\n",
    "print(\"=== KEY FINDINGS ===\")\n",
    "print(\"\\n1. MODEL MISMATCH (CRITICAL)\")\n",
    "print(\"   - Logistic regression is fundamentally wrong for this problem\")\n",
    "print(\"   - Need tree-based model (LightGBM/XGBoost) for non-linear patterns\")\n",
    "print(\"   - Expected gain: +0.03 to +0.08 AUC\")\n",
    "\n",
    "print(\"\\n2. DATA LEAKAGE (HIGH PRIORITY)\")\n",
    "print(\"   - TF-IDF and SVD fitted on full data before CV splits\")\n",
    "print(\"   - Validation set information leaks into transformers\")\n",
    "print(\"   - CV scores may be optimistic\")\n",
    "print(\"   - Fix: Move ALL fitting inside CV loops\")\n",
    "\n",
    "print(\"\\n3. DIMENSIONALITY CRISIS (HIGH PRIORITY)\")\n",
    "print(\"   - 11,158 features vs 2,878 samples = 3.9:1 ratio\")\n",
    "print(\"   - Extreme overfitting risk\")\n",
    "print(\"   - Solution: Aggressive SVD reduction (50-75 components total)\")\n",
    "\n",
    "print(\"\\n4. FEATURE SCALING (MEDIUM PRIORITY)\")\n",
    "print(\"   - TF-IDF (0-1) mixed with numeric features (0-5000)\")\n",
    "print(\"   - Logistic regression dominated by large-scale features\")\n",
    "print(\"   - LightGBM/XGBoost don't need scaling, but proper preprocessing helps\")\n",
    "\n",
    "print(\"\\n5. CONVERGENCE ISSUES (MEDIUM PRIORITY)\")\n",
    "print(\"   - Warnings despite max_iter=1000\")\n",
    "print(\"   - Indicates poor conditioning\")\n",
    "print(\"   - Will resolve with better model and dimensionality reduction\")\n",
    "\n",
    "print(\"\\n=== PATH TO GOLD ===\")\n",
    "print(\"Current: 0.6445\")\n",
    "print(\"Target: 0.9791\")\n",
    "print(\"Gap: 0.3346\")\n",
    "print(\"\\nNext experiment priorities:\")\n",
    "print(\"1. Switch to LightGBM (highest impact)\")\n",
    "print(\"2. Fix data leakage (proper CV)\")\n",
    "print(\"3. Reduce dimensionality (50-75 SVD components)\")\n",
    "print(\"4. Optimize feature combinations\")\n",
    "print(\"5. Ensemble diverse models\")\n",
    "print(\"\\nExpected trajectory:\")\n",
    "print(\"- Experiment 004 (LightGBM + fix leakage): 0.68-0.72\")\n",
    "print(\"- Experiment 005 (optimized features): 0.75-0.80\")\n",
    "print(\"- Experiment 006 (ensembling): 0.82-0.85\")\n",
    "print(\"- Further iterations needed to reach 0.9791\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
