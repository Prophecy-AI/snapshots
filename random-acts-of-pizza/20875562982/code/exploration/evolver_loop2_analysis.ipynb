{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69111927",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Why Linguistic Features Failed & Path Forward\n",
    "\n",
    "**Goal:** Analyze why the linguistic features experiment underperformed and identify promising directions based on what's working.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Why did linguistic features (0.6118) perform worse than baseline TF-IDF (0.6386)?\n",
    "2. What patterns in the data can we exploit better?\n",
    "3. What should be our next priority?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2429623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")\n",
    "\n",
    "# Combine text for analysis\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "print(f\"Average text length: {df_train['combined_text'].str.len().mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze why linguistic features might have failed\n",
    "# Let's examine the distribution of linguistic patterns\n",
    "\n",
    "# Define the patterns from the linguistic features experiment\n",
    "def count_gratitude(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    gratitude_words = ['thank', 'thanks', 'appreciate', 'grateful', 'bless', 'blessing']\n",
    "    return sum(1 for word in gratitude_words if word in str(text).lower())\n",
    "\n",
    "def count_need_words(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    need_words = ['need', 'desperate', 'urgent', 'emergency', 'starving', 'hungry', 'broke', 'bills', 'rent']\n",
    "    return sum(1 for word in need_words if word in str(text).lower())\n",
    "\n",
    "def count_reciprocity(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    reciprocity_words = ['pay it forward', 'help others', 'contribute', 'give back', 'return favor', 'when i can']\n",
    "    return sum(1 for phrase in reciprocity_words if phrase in str(text).lower())\n",
    "\n",
    "# Apply pattern detection\n",
    "df_train['gratitude_count'] = df_train['combined_text'].apply(count_gratitude)\n",
    "df_train['need_count'] = df_train['combined_text'].apply(count_need_words)\n",
    "df_train['reciprocity_count'] = df_train['combined_text'].apply(count_reciprocity)\n",
    "\n",
    "print(\"Pattern frequency analysis:\")\n",
    "print(f\"Gratitude mentions - Mean: {df_train['gratitude_count'].mean():.2f}, Std: {df_train['gratitude_count'].std():.2f}\")\n",
    "print(f\"Need words - Mean: {df_train['need_count'].mean():.2f}, Std: {df_train['need_count'].std():.2f}\")\n",
    "print(f\"Reciprocity mentions - Mean: {df_train['reciprocity_count'].mean():.2f}, Std: {df_train['reciprocity_count'].std():.2f}\")\n",
    "\n",
    "# Check how often these patterns appear\n",
    "print(f\"\\nPosts with gratitude: {(df_train['gratitude_count'] > 0).mean():.1%}\")\n",
    "print(f\"Posts with need words: {(df_train['need_count'] > 0).mean():.1%}\")\n",
    "print(f\"Posts with reciprocity: {(df_train['reciprocity_count'] > 0).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze success rates by pattern presence\n",
    "\n",
    "gratitude_success = df_train[df_train['gratitude_count'] > 0]['requester_received_pizza'].mean()\n",
    "no_gratitude_success = df_train[df_train['gratitude_count'] == 0]['requester_received_pizza'].mean()\n",
    "\n",
    "need_success = df_train[df_train['need_count'] > 0]['requester_received_pizza'].mean()\n",
    "no_need_success = df_train[df_train['need_count'] == 0]['requester_received_pizza'].mean()\n",
    "\n",
    "reciprocity_success = df_train[df_train['reciprocity_count'] > 0]['requester_received_pizza'].mean()\n",
    "no_reciprocity_success = df_train[df_train['reciprocity_count'] == 0]['requester_received_pizza'].mean()\n",
    "\n",
    "print(\"Success rates by pattern presence:\")\n",
    "print(f\"With gratitude: {gratitude_success:.1%} vs Without: {no_gratitude_success:.1%} (Diff: {gratitude_success - no_gratitude_success:+.1%})\")\n",
    "print(f\"With need words: {need_success:.1%} vs Without: {no_need_success:.1%} (Diff: {need_success - no_need_success:+.1%})\")\n",
    "print(f\"With reciprocity: {reciprocity_success:.1%} vs Without: {no_reciprocity_success:.1%} (Diff: {reciprocity_success - no_reciprocity_success:+.1%})\")\n",
    "\n",
    "# Check if patterns are rare\n",
    "print(f\"\\nPattern prevalence in successful requests:\")\n",
    "successful = df_train[df_train['requester_received_pizza'] == 1]\n",
    "print(f\"Gratitude in successes: {(successful['gratitude_count'] > 0).mean():.1%}\")\n",
    "print(f\"Need words in successes: {(successful['need_count'] > 0).mean():.1%}\")\n",
    "print(f\"Reciprocity in successes: {(successful['reciprocity_count'] > 0).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dab9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the actual text of successful vs unsuccessful requests\n",
    "# to understand what patterns we're missing\n",
    "\n",
    "successful_text = df_train[df_train['requester_received_pizza'] == 1]['combined_text'].sample(3, random_state=42).tolist()\n",
    "unsuccessful_text = df_train[df_train['requester_received_pizza'] == 0]['combined_text'].sample(3, random_state=42).tolist()\n",
    "\n",
    "print(\"=== SAMPLE SUCCESSFUL REQUESTS ===\")\n",
    "for i, text in enumerate(successful_text, 1):\n",
    "    print(f\"\\n{i}. {text[:300]}...\")\n",
    "    print(f\"   Length: {len(text)} chars, Gratitude: {count_gratitude(text)}, Need: {count_need_words(text)}, Reciprocity: {count_reciprocity(text)}\")\n",
    "\n",
    "print(\"\\n\\n=== SAMPLE UNSUCCESSFUL REQUESTS ===\")\n",
    "for i, text in enumerate(unsuccessful_text, 1):\n",
    "    print(f\"\\n{i}. {text[:300]}...\")\n",
    "    print(f\"   Length: {len(text)} chars, Gratitude: {count_gratitude(text)}, Need: {count_need_words(text)}, Reciprocity: {count_reciprocity(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TF-IDF performance vs pattern-based features\n",
    "# Let's see what words are most predictive in TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create TF-IDF on a sample to see top features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))\n",
    "X_tfidf = vectorizer.fit_transform(df_train['combined_text'])\n",
    "y = df_train['requester_received_pizza']\n",
    "\n",
    "# Train a simple model to get feature importances\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "model.fit(X_tfidf, y)\n",
    "\n",
    "# Get top positive and negative features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Top features for positive class (pizza received)\n",
    "top_positive_idx = np.argsort(coefficients)[-20:]\n",
    "top_negative_idx = np.argsort(coefficients)[:20]\n",
    "\n",
    "print(\"Top 10 TF-IDF features predicting SUCCESS (pizza received):\")\n",
    "for idx in reversed(top_positive_idx[-10:]):\n",
    "    print(f\"  {feature_names[idx]:<20} : {coefficients[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF features predicting FAILURE (no pizza):\")\n",
    "for idx in top_negative_idx[:10]:\n",
    "    print(f\"  {feature_names[idx]:<20} : {coefficients[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d72df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insights from the analysis\n",
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS FROM EVOLVER LOOP 2 ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. WHY LINGUISTIC FEATURES FAILED:\")\n",
    "print(\"   - Simple regex patterns are too crude\")\n",
    "print(\"   - Patterns are rare (gratitude in only 25% of posts)\")\n",
    "print(\"   - Context and nuance matter (e.g., 'thanks' vs genuine gratitude)\")\n",
    "print(\"   - TF-IDF captures subtle word patterns better than hand-crafted rules\")\n",
    "\n",
    "print(\"\\n2. WHAT TF-IDF IS CAPTURING:\")\n",
    "print(\"   - Specific words like 'request', 'tonight', 'help', 'appreciate' predict success\")\n",
    "print(\"   - Negative indicators: 'account', 'karma', 'post', 'please' (overly generic?)\")\n",
    "print(\"   - Context matters more than simple word presence\")\n",
    "\n",
    "print(\"\\n3. PATH FORWARD:\")\n",
    "print(\"   - Enhanced TF-IDF (character n-grams, better preprocessing)\")\n",
    "print(\"   - Better text representation (SVD, embeddings)\")\n",
    "print(\"   - More sophisticated models (XGBoost, ensembles)\")\n",
    "print(\"   - User metadata features (account age, karma, history)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
