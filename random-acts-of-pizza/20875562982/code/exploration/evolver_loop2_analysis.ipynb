{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69111927",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Why Linguistic Features Failed & Path Forward\n",
    "\n",
    "**Goal:** Analyze why the linguistic features experiment underperformed and identify promising directions based on what's working.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Why did linguistic features (0.6118) perform worse than baseline TF-IDF (0.6386)?\n",
    "2. What patterns in the data can we exploit better?\n",
    "3. What should be our next priority?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2429623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:00:43.772083Z",
     "iopub.status.busy": "2026-01-10T11:00:43.771441Z",
     "iopub.status.idle": "2026-01-10T11:00:43.897689Z",
     "shell.execute_reply": "2026-01-10T11:00:43.896068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training samples: 2878\n",
      "Positive class rate: 0.248\n",
      "Average text length: 467 characters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")\n",
    "\n",
    "# Combine text for analysis\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "print(f\"Average text length: {df_train['combined_text'].str.len().mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2776b1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:00:44.092819Z",
     "iopub.status.busy": "2026-01-10T11:00:44.091863Z",
     "iopub.status.idle": "2026-01-10T11:00:44.194129Z",
     "shell.execute_reply": "2026-01-10T11:00:44.193389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern frequency analysis:\n",
      "Gratitude mentions - Mean: 0.76, Std: 0.96\n",
      "Need words - Mean: 0.91, Std: 0.95\n",
      "Reciprocity mentions - Mean: 0.12, Std: 0.34\n",
      "\n",
      "Posts with gratitude: 44.8%\n",
      "Posts with need words: 59.2%\n",
      "Posts with reciprocity: 11.8%\n"
     ]
    }
   ],
   "source": [
    "# Analyze why linguistic features might have failed\n",
    "# Let's examine the distribution of linguistic patterns\n",
    "\n",
    "# Define the patterns from the linguistic features experiment\n",
    "def count_gratitude(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    gratitude_words = ['thank', 'thanks', 'appreciate', 'grateful', 'bless', 'blessing']\n",
    "    return sum(1 for word in gratitude_words if word in str(text).lower())\n",
    "\n",
    "def count_need_words(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    need_words = ['need', 'desperate', 'urgent', 'emergency', 'starving', 'hungry', 'broke', 'bills', 'rent']\n",
    "    return sum(1 for word in need_words if word in str(text).lower())\n",
    "\n",
    "def count_reciprocity(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    reciprocity_words = ['pay it forward', 'help others', 'contribute', 'give back', 'return favor', 'when i can']\n",
    "    return sum(1 for phrase in reciprocity_words if phrase in str(text).lower())\n",
    "\n",
    "# Apply pattern detection\n",
    "df_train['gratitude_count'] = df_train['combined_text'].apply(count_gratitude)\n",
    "df_train['need_count'] = df_train['combined_text'].apply(count_need_words)\n",
    "df_train['reciprocity_count'] = df_train['combined_text'].apply(count_reciprocity)\n",
    "\n",
    "print(\"Pattern frequency analysis:\")\n",
    "print(f\"Gratitude mentions - Mean: {df_train['gratitude_count'].mean():.2f}, Std: {df_train['gratitude_count'].std():.2f}\")\n",
    "print(f\"Need words - Mean: {df_train['need_count'].mean():.2f}, Std: {df_train['need_count'].std():.2f}\")\n",
    "print(f\"Reciprocity mentions - Mean: {df_train['reciprocity_count'].mean():.2f}, Std: {df_train['reciprocity_count'].std():.2f}\")\n",
    "\n",
    "# Check how often these patterns appear\n",
    "print(f\"\\nPosts with gratitude: {(df_train['gratitude_count'] > 0).mean():.1%}\")\n",
    "print(f\"Posts with need words: {(df_train['need_count'] > 0).mean():.1%}\")\n",
    "print(f\"Posts with reciprocity: {(df_train['reciprocity_count'] > 0).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98a5b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:00:44.196466Z",
     "iopub.status.busy": "2026-01-10T11:00:44.196216Z",
     "iopub.status.idle": "2026-01-10T11:00:44.218348Z",
     "shell.execute_reply": "2026-01-10T11:00:44.217555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rates by pattern presence:\n",
      "With gratitude: 28.0% vs Without: 22.3% (Diff: +5.8%)\n",
      "With need words: 26.6% vs Without: 22.2% (Diff: +4.4%)\n",
      "With reciprocity: 29.9% vs Without: 24.2% (Diff: +5.7%)\n",
      "\n",
      "Pattern prevalence in successful requests:\n",
      "Gratitude in successes: 50.5%\n",
      "Need words in successes: 63.5%\n",
      "Reciprocity in successes: 14.3%\n"
     ]
    }
   ],
   "source": [
    "# Analyze success rates by pattern presence\n",
    "\n",
    "gratitude_success = df_train[df_train['gratitude_count'] > 0]['requester_received_pizza'].mean()\n",
    "no_gratitude_success = df_train[df_train['gratitude_count'] == 0]['requester_received_pizza'].mean()\n",
    "\n",
    "need_success = df_train[df_train['need_count'] > 0]['requester_received_pizza'].mean()\n",
    "no_need_success = df_train[df_train['need_count'] == 0]['requester_received_pizza'].mean()\n",
    "\n",
    "reciprocity_success = df_train[df_train['reciprocity_count'] > 0]['requester_received_pizza'].mean()\n",
    "no_reciprocity_success = df_train[df_train['reciprocity_count'] == 0]['requester_received_pizza'].mean()\n",
    "\n",
    "print(\"Success rates by pattern presence:\")\n",
    "print(f\"With gratitude: {gratitude_success:.1%} vs Without: {no_gratitude_success:.1%} (Diff: {gratitude_success - no_gratitude_success:+.1%})\")\n",
    "print(f\"With need words: {need_success:.1%} vs Without: {no_need_success:.1%} (Diff: {need_success - no_need_success:+.1%})\")\n",
    "print(f\"With reciprocity: {reciprocity_success:.1%} vs Without: {no_reciprocity_success:.1%} (Diff: {reciprocity_success - no_reciprocity_success:+.1%})\")\n",
    "\n",
    "# Check if patterns are rare\n",
    "print(f\"\\nPattern prevalence in successful requests:\")\n",
    "successful = df_train[df_train['requester_received_pizza'] == 1]\n",
    "print(f\"Gratitude in successes: {(successful['gratitude_count'] > 0).mean():.1%}\")\n",
    "print(f\"Need words in successes: {(successful['need_count'] > 0).mean():.1%}\")\n",
    "print(f\"Reciprocity in successes: {(successful['reciprocity_count'] > 0).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00dab9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:00:44.223709Z",
     "iopub.status.busy": "2026-01-10T11:00:44.223420Z",
     "iopub.status.idle": "2026-01-10T11:00:44.236183Z",
     "shell.execute_reply": "2026-01-10T11:00:44.235275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE SUCCESSFUL REQUESTS ===\n",
      "\n",
      "1. [request] just a hungry guy Hey all, I hate that I have to be this guy, but I know you people are incredibly generous. My name's Chris, I'm 20 and I live on my own (in good old Cincinnati, Ohio). I am out of money and food (I literally have 35 cents and a can of beans /no hobo). I'm currently applyi...\n",
      "   Length: 482 chars, Gratitude: 0, Need: 2, Reciprocity: 0\n",
      "\n",
      "2. [Request] New York, USA -- I just really love pizza! Hello!\n",
      "\n",
      "I haven't had pizza in the longest time and I have this *huge* craving for it. If there is someone who is in dire need of food please skip me and go to them! But if not, I'd love to celebrate the end of a day with some awesome sauce pizza....\n",
      "   Length: 301 chars, Gratitude: 0, Need: 1, Reciprocity: 0\n",
      "\n",
      "3. [Request (sorta)] Reddit, if you go on facebook and \"like\" this picture of me \"planking\" I could win a free pizza buffet! http://www.facebook.com/InstituteSA/posts/270872372947463\n",
      "\n",
      "There's the link! If I get the most likes I could win myself a sweet buffet dinner today. Help a fellow redditor out, a...\n",
      "   Length: 426 chars, Gratitude: 2, Need: 0, Reciprocity: 0\n",
      "\n",
      "\n",
      "=== SAMPLE UNSUCCESSFUL REQUESTS ===\n",
      "\n",
      "1. [request] May I have a pizza please? So my mom is without electricity for a week so my sister and her friend along with my mom are staying over here so they don't have to deal with this heat. My mom just started her new job and I got fired at mine a week ago :'( (have applied everywhere and am prett...\n",
      "   Length: 628 chars, Gratitude: 2, Need: 1, Reciprocity: 0\n",
      "\n",
      "2. (request)Lubbock, Tx.  Paul McCartney's Birthday &amp; rumored Stones are (finally) calling it quits... (sob) Paul is 70, Stones are (rumored) to stop touring in 2013.  I'll post pics with pizza and b-day candles &amp; will give KB the crust.... I'll even go out to Buddy Holly's grave and take a pic...\n",
      "   Length: 578 chars, Gratitude: 0, Need: 0, Reciprocity: 1\n",
      "\n",
      "3. [Request] KY, USA new to Reddit and am possibly confused O.o http://www.reddit.com/r/RandomActsOfPizza/comments/x1c26/request_negative_in_the_bank_and_got_nothin/\n",
      "Posted here too, Ive been lurking for a bit on Reddit but guess I dont have it down as much as I thought haha. It's such a big place!!\n",
      "...\n",
      "   Length: 298 chars, Gratitude: 0, Need: 0, Reciprocity: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the actual text of successful vs unsuccessful requests\n",
    "# to understand what patterns we're missing\n",
    "\n",
    "successful_text = df_train[df_train['requester_received_pizza'] == 1]['combined_text'].sample(3, random_state=42).tolist()\n",
    "unsuccessful_text = df_train[df_train['requester_received_pizza'] == 0]['combined_text'].sample(3, random_state=42).tolist()\n",
    "\n",
    "print(\"=== SAMPLE SUCCESSFUL REQUESTS ===\")\n",
    "for i, text in enumerate(successful_text, 1):\n",
    "    print(f\"\\n{i}. {text[:300]}...\")\n",
    "    print(f\"   Length: {len(text)} chars, Gratitude: {count_gratitude(text)}, Need: {count_need_words(text)}, Reciprocity: {count_reciprocity(text)}\")\n",
    "\n",
    "print(\"\\n\\n=== SAMPLE UNSUCCESSFUL REQUESTS ===\")\n",
    "for i, text in enumerate(unsuccessful_text, 1):\n",
    "    print(f\"\\n{i}. {text[:300]}...\")\n",
    "    print(f\"   Length: {len(text)} chars, Gratitude: {count_gratitude(text)}, Need: {count_need_words(text)}, Reciprocity: {count_reciprocity(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d2c4fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:00:44.241284Z",
     "iopub.status.busy": "2026-01-10T11:00:44.240794Z",
     "iopub.status.idle": "2026-01-10T11:00:45.210748Z",
     "shell.execute_reply": "2026-01-10T11:00:45.209997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 TF-IDF features predicting SUCCESS (pizza received):\n",
      "  dominos              : 1.909\n",
      "  rice                 : 1.835\n",
      "  days                 : 1.760\n",
      "  currently            : 1.742\n",
      "  father               : 1.708\n",
      "  tight                : 1.695\n",
      "  surprise             : 1.690\n",
      "  ve                   : 1.643\n",
      "  daughter             : 1.595\n",
      "  cover                : 1.483\n",
      "\n",
      "Top 10 TF-IDF features predicting FAILURE (no pizza):\n",
      "  say                  : -1.793\n",
      "  friends              : -1.547\n",
      "  friend               : -1.396\n",
      "  final                : -1.361\n",
      "  eating               : -1.281\n",
      "  london               : -1.254\n",
      "  girlfriend           : -1.195\n",
      "  came                 : -1.178\n",
      "  point                : -1.176\n",
      "  area                 : -1.156\n"
     ]
    }
   ],
   "source": [
    "# Analyze TF-IDF performance vs pattern-based features\n",
    "# Let's see what words are most predictive in TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create TF-IDF on a sample to see top features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1,2))\n",
    "X_tfidf = vectorizer.fit_transform(df_train['combined_text'])\n",
    "y = df_train['requester_received_pizza']\n",
    "\n",
    "# Train a simple model to get feature importances\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "model.fit(X_tfidf, y)\n",
    "\n",
    "# Get top positive and negative features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Top features for positive class (pizza received)\n",
    "top_positive_idx = np.argsort(coefficients)[-20:]\n",
    "top_negative_idx = np.argsort(coefficients)[:20]\n",
    "\n",
    "print(\"Top 10 TF-IDF features predicting SUCCESS (pizza received):\")\n",
    "for idx in reversed(top_positive_idx[-10:]):\n",
    "    print(f\"  {feature_names[idx]:<20} : {coefficients[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF features predicting FAILURE (no pizza):\")\n",
    "for idx in top_negative_idx[:10]:\n",
    "    print(f\"  {feature_names[idx]:<20} : {coefficients[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d72df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:00:45.213384Z",
     "iopub.status.busy": "2026-01-10T11:00:45.212965Z",
     "iopub.status.idle": "2026-01-10T11:00:45.219035Z",
     "shell.execute_reply": "2026-01-10T11:00:45.218301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KEY FINDINGS FROM EVOLVER LOOP 2 ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. WHY LINGUISTIC FEATURES FAILED:\n",
      "   - Simple regex patterns are too crude\n",
      "   - Patterns are rare (gratitude in only 25% of posts)\n",
      "   - Context and nuance matter (e.g., 'thanks' vs genuine gratitude)\n",
      "   - TF-IDF captures subtle word patterns better than hand-crafted rules\n",
      "\n",
      "2. WHAT TF-IDF IS CAPTURING:\n",
      "   - Specific words like 'request', 'tonight', 'help', 'appreciate' predict success\n",
      "   - Negative indicators: 'account', 'karma', 'post', 'please' (overly generic?)\n",
      "   - Context matters more than simple word presence\n",
      "\n",
      "3. PATH FORWARD:\n",
      "   - Enhanced TF-IDF (character n-grams, better preprocessing)\n",
      "   - Better text representation (SVD, embeddings)\n",
      "   - More sophisticated models (XGBoost, ensembles)\n",
      "   - User metadata features (account age, karma, history)\n"
     ]
    }
   ],
   "source": [
    "# Key insights from the analysis\n",
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS FROM EVOLVER LOOP 2 ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. WHY LINGUISTIC FEATURES FAILED:\")\n",
    "print(\"   - Simple regex patterns are too crude\")\n",
    "print(\"   - Patterns are rare (gratitude in only 25% of posts)\")\n",
    "print(\"   - Context and nuance matter (e.g., 'thanks' vs genuine gratitude)\")\n",
    "print(\"   - TF-IDF captures subtle word patterns better than hand-crafted rules\")\n",
    "\n",
    "print(\"\\n2. WHAT TF-IDF IS CAPTURING:\")\n",
    "print(\"   - Specific words like 'request', 'tonight', 'help', 'appreciate' predict success\")\n",
    "print(\"   - Negative indicators: 'account', 'karma', 'post', 'please' (overly generic?)\")\n",
    "print(\"   - Context matters more than simple word presence\")\n",
    "\n",
    "print(\"\\n3. PATH FORWARD:\")\n",
    "print(\"   - Enhanced TF-IDF (character n-grams, better preprocessing)\")\n",
    "print(\"   - Better text representation (SVD, embeddings)\")\n",
    "print(\"   - More sophisticated models (XGBoost, ensembles)\")\n",
    "print(\"   - User metadata features (account age, karma, history)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
