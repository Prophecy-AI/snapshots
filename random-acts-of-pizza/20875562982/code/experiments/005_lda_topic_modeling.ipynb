{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be622139",
   "metadata": {},
   "source": [
    "# Experiment 005: LDA Topic Modeling\n",
    "\n",
    "This experiment adds Latent Dirichlet Allocation (LDA) topic modeling to capture latent semantic themes in pizza requests.\n",
    "\n",
    "**Key improvements:**\n",
    "1. Add 15 LDA topics as features (competition winner technique)\n",
    "2. Fit LDA inside CV loop to prevent leakage\n",
    "3. Combine with existing TF-IDF + SVD + numeric features\n",
    "4. Use LightGBM for robust handling of mixed features\n",
    "\n",
    "**Expected impact**: +0.05 to +0.08 AUC (target: 0.71-0.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abf24f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T16:22:11.771847Z",
     "iopub.status.busy": "2026-01-10T16:22:11.770933Z",
     "iopub.status.idle": "2026-01-10T16:22:13.180559Z",
     "shell.execute_reply": "2026-01-10T16:22:13.179430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "Positive class rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text features\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "df_test['combined_text'] = df_test['request_title'].fillna('') + ' ' + df_test['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text while preserving important patterns\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'EDIT:\\s*', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['combined_text_clean'] = df_train['combined_text'].apply(preprocess_text)\n",
    "df_test['combined_text_clean'] = df_test['combined_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e72ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced numeric features\n",
    "y = df_train['requester_received_pizza'].values\n",
    "\n",
    "# Log transforms for count features\n",
    "count_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feat in count_features:\n",
    "    df_train[f'{feat}_log'] = np.log1p(df_train[feat])\n",
    "    df_test[f'{feat}_log'] = np.log1p(df_test[feat])\n",
    "\n",
    "# Ratios\n",
    "df_train['upvotes_per_comment'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_comments_at_request'] + 1)\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_test['upvotes_per_comment'] = df_test['requester_upvotes_plus_downvotes_at_request'] / (df_test['requester_number_of_comments_at_request'] + 1)\n",
    "df_test['comments_per_post'] = df_test['requester_number_of_comments_at_request'] / (df_test['requester_number_of_posts_at_request'] + 1)\n",
    "\n",
    "# Account age in years\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days_at_request'] / 365.25\n",
    "df_test['account_age_years'] = df_test['requester_account_age_in_days_at_request'] / 365.25\n",
    "\n",
    "# Text statistics\n",
    "df_train['text_length'] = df_train['combined_text_clean'].str.len()\n",
    "df_test['text_length'] = df_test['combined_text_clean'].str.len()\n",
    "df_train['word_count'] = df_train['combined_text_clean'].str.split().str.len()\n",
    "df_test['word_count'] = df_test['combined_text_clean'].str.split().str.len()\n",
    "\n",
    "# Select numeric features\n",
    "numeric_features = [\n",
    "    'requester_number_of_comments_at_request_log',\n",
    "    'requester_number_of_posts_at_request_log',\n",
    "    'requester_upvotes_plus_downvotes_at_request_log',\n",
    "    'upvotes_per_comment',\n",
    "    'comments_per_post',\n",
    "    'account_age_years',\n",
    "    'text_length',\n",
    "    'word_count'\n",
    "]\n",
    "\n",
    "train_numeric = df_train[numeric_features].values\n",
    "test_numeric = df_test[numeric_features].values\n",
    "\n",
    "print(f\"Created {len(numeric_features)} numeric features\")\n",
    "print(f\"Train numeric shape: {train_numeric.shape}\")\n",
    "print(f\"Test numeric shape: {test_numeric.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b25464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline functions for text features with SVD and LDA\n",
    "def create_text_pipeline(n_word_components=50, n_char_components=25, n_lda_topics=15):\n",
    "    \"\"\"Create pipelines that transform text to SVD components and LDA topics\"\"\"\n",
    "    \n",
    "    # Word-level TF-IDF + SVD\n",
    "    word_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('svd', TruncatedSVD(n_components=n_word_components, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Character-level TF-IDF + SVD\n",
    "    char_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=2000,\n",
    "            lowercase=False,\n",
    "            min_df=5,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('svd', TruncatedSVD(n_components=n_char_components, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # LDA topic modeling (using CountVectorizer for LDA)\n",
    "    lda_pipeline = Pipeline([\n",
    "        ('count', CountVectorizer(\n",
    "            max_features=3000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            min_df=5,\n",
    "            max_df=0.95\n",
    "        )),\n",
    "        ('lda', LatentDirichletAllocation(\n",
    "            n_components=n_lda_topics,\n",
    "            random_state=42,\n",
    "            max_iter=10,\n",
    "            learning_method='batch',\n",
    "            evaluate_every=5\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return word_pipeline, char_pipeline, lda_pipeline\n",
    "\n",
    "print(\"Text pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified CV setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(df_train))\n",
    "test_predictions = np.zeros(len(df_test))\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Starting 5-fold CV with LightGBM + LDA topics...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(df_train, y):\n",
    "    fold += 1\n",
    "    print(f\"\\nFold {fold}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_text = df_train['combined_text_clean'].iloc[train_idx]\n",
    "    X_val_text = df_train['combined_text_clean'].iloc[val_idx]\n",
    "    X_train_num = train_numeric[train_idx]\n",
    "    X_val_num = train_numeric[val_idx]\n",
    "    \n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create and fit text pipelines (INSIDE CV LOOP - NO LEAKAGE)\n",
    "    word_pipe, char_pipe, lda_pipe = create_text_pipeline(n_word_components=50, n_char_components=25, n_lda_topics=15)\n",
    "    \n",
    "    # Fit on training data only\n",
    "    X_train_word_svd = word_pipe.fit_transform(X_train_text)\n",
    "    X_train_char_svd = char_pipe.fit_transform(X_train_text)\n",
    "    X_train_lda = lda_pipe.fit_transform(X_train_text)\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val_word_svd = word_pipe.transform(X_val_text)\n",
    "    X_val_char_svd = char_pipe.transform(X_val_text)\n",
    "    X_val_lda = lda_pipe.transform(X_val_text)\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_combined = np.hstack([X_train_word_svd, X_train_char_svd, X_train_lda, X_train_num])\n",
    "    X_val_combined = np.hstack([X_val_word_svd, X_val_char_svd, X_val_lda, X_val_num])\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train_combined.shape}\")\n",
    "    print(f\"  Validation features shape: {X_val_combined.shape}\")\n",
    "    \n",
    "    # Train LightGBM\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        max_depth=7,\n",
    "        min_child_samples=50,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_combined, y_train,\n",
    "        eval_set=[(X_val_combined, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    val_pred = model.predict_proba(X_val_combined)[:, 1]\n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    \n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    print(f\"  Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    print(f\"  Best iteration: {model.best_iteration_}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall CV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Individual folds: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7af124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions using full training data\n",
    "print(\"Training final model on full data...\")\n",
    "\n",
    "# Create and fit text pipelines on full training data\n",
    "word_pipe_full, char_pipe_full, lda_pipe_full = create_text_pipeline(n_word_components=50, n_char_components=25, n_lda_topics=15)\n",
    "\n",
    "train_word_svd_full = word_pipe_full.fit_transform(df_train['combined_text_clean'])\n",
    "train_char_svd_full = char_pipe_full.fit_transform(df_train['combined_text_clean'])\n",
    "train_lda_full = lda_pipe_full.fit_transform(df_train['combined_text_clean'])\n",
    "\n",
    "test_word_svd_full = word_pipe_full.transform(df_test['combined_text_clean'])\n",
    "test_char_svd_full = char_pipe_full.transform(df_test['combined_text_clean'])\n",
    "test_lda_full = lda_pipe_full.transform(df_test['combined_text_clean'])\n",
    "\n",
    "# Combine all features\n",
    "train_combined_full = np.hstack([train_word_svd_full, train_char_svd_full, train_lda_full, train_numeric])\n",
    "test_combined_full = np.hstack([test_word_svd_full, test_char_svd_full, test_lda_full, test_numeric])\n",
    "\n",
    "print(f\"Final training features shape: {train_combined_full.shape}\")\n",
    "print(f\"Final test features shape: {test_combined_full.shape}\")\n",
    "\n",
    "# Train final model\n",
    "final_model = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    max_depth=7,\n",
    "    min_child_samples=50,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    train_combined_full, y,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "final_predictions = final_model.predict_proba(test_combined_full)[:, 1]\n",
    "\n",
    "print(f\"Final model training completed\")\n",
    "print(f\"Test predictions shape: {final_predictions.shape}\")\n",
    "print(f\"Prediction range: {final_predictions.min():.4f} to {final_predictions.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4563eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': df_test['request_id'],\n",
    "    'requester_received_pizza': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure proper format\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].astype(float)\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_lda_topic_modeling.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: {submission['requester_received_pizza'].min():.4f} to {submission['requester_received_pizza'].max():.4f}\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5dfbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze LDA topics to understand what they capture\n",
    "print(\"Analyzing LDA topics...\")\n",
    "\n",
    "# Get feature names from the count vectorizer used in LDA\n",
    "vectorizer = lda_pipe_full.named_steps['count']\n",
    "lda_model = lda_pipe_full.named_steps['lda']\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print top words for each topic\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx}:\")\n",
    "    print(f\"  Top words: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Try to interpret the topic\n",
    "    if any(word in ['family', 'kids', 'children', 'son', 'daughter', 'mother', 'father'] for word in top_words):\n",
    "        print(f\"  Interpretation: Family hardship/children\")\n",
    "    elif any(word in ['student', 'college', 'school', 'tuition', 'loan', 'debt'] for word in top_words):\n",
    "        print(f\"  Interpretation: Student struggles/financial hardship\")\n",
    "    elif any(word in ['job', 'work', 'unemployed', 'laid', 'fired', 'bills'] for word in top_words):\n",
    "        print(f\"  Interpretation: Job loss/unemployment\")\n",
    "    elif any(word in ['hospital', 'sick', 'medical', 'doctor', 'health', 'surgery'] for word in top_words):\n",
    "        print(f\"  Interpretation: Medical emergency/health issues\")\n",
    "    elif any(word in ['thank', 'thanks', 'appreciate', 'grateful', 'bless'] for word in top_words):\n",
    "        print(f\"  Interpretation: Gratitude expressions\")\n",
    "    elif any(word in ['pay', 'forward', 'help', 'others', 'contribute', 'return'] for word in top_words):\n",
    "        print(f\"  Interpretation: Reciprocity promises\")\n",
    "    elif any(word in ['pizza', 'hungry', 'food', 'eat', 'dinner', 'lunch'] for word in top_words):\n",
    "        print(f\"  Interpretation: Direct food requests\")\n",
    "    elif any(word in ['dominos', 'papa', 'johns', 'hut', 'delivery', 'order'] for word in top_words):\n",
    "        print(f\"  Interpretation: Specific pizza chains\")\n",
    "    else:\n",
    "        print(f\"  Interpretation: General discussion\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Experiment 005 completed!\")\n",
    "print(f\"CV AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Improvement over exp_004: {cv_mean - 0.6660:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
