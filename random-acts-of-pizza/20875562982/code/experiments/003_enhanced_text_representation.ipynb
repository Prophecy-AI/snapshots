{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9bb45e8",
   "metadata": {},
   "source": [
    "# Enhanced Text Representation Experiment\n",
    "\n",
    "This experiment focuses on improving text representation using:\n",
    "1. **TruncatedSVD (LSA)**: Dimensionality reduction on TF-IDF matrix to capture latent semantics (100 components)\n",
    "2. **Character n-grams**: Add 2-4 character n-grams to capture stylistic patterns\n",
    "3. **Enhanced TF-IDF**: Better parameters and preprocessing\n",
    "\n",
    "Based on the strategy, these techniques should capture more signal than basic TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca9cf54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:14:00.748185Z",
     "iopub.status.busy": "2026-01-10T11:14:00.747293Z",
     "iopub.status.idle": "2026-01-10T11:14:02.828569Z",
     "shell.execute_reply": "2026-01-10T11:14:02.827764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "Positive class rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8a6f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:14:02.831430Z",
     "iopub.status.busy": "2026-01-10T11:14:02.830747Z",
     "iopub.status.idle": "2026-01-10T11:14:02.979881Z",
     "shell.execute_reply": "2026-01-10T11:14:02.978906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "# Combine text features\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "df_test['combined_text'] = df_test['request_title'].fillna('') + ' ' + df_test['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text while preserving important patterns\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove Reddit usernames (e.g., /u/username)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    \n",
    "    # Remove edit markers\n",
    "    text = re.sub(r'EDIT:\\s*', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['combined_text_clean'] = df_train['combined_text'].apply(preprocess_text)\n",
    "df_test['combined_text_clean'] = df_test['combined_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623b558e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:14:02.982874Z",
     "iopub.status.busy": "2026-01-10T11:14:02.982408Z",
     "iopub.status.idle": "2026-01-10T11:14:03.054481Z",
     "shell.execute_reply": "2026-01-10T11:14:03.053584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 numeric features: ['requester_number_of_comments_at_request_log', 'requester_number_of_posts_at_request_log', 'requester_upvotes_plus_downvotes_at_request_log', 'upvotes_per_comment', 'comments_per_post', 'account_age_years', 'text_length', 'word_count']\n",
      "Train numeric shape: (2878, 8)\n",
      "Test numeric shape: (1162, 8)\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced numeric features\n",
    "y = df_train['requester_received_pizza'].values\n",
    "\n",
    "# Log transforms for count features\n",
    "count_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feat in count_features:\n",
    "    df_train[f'{feat}_log'] = np.log1p(df_train[feat])\n",
    "    df_test[f'{feat}_log'] = np.log1p(df_test[feat])\n",
    "\n",
    "# Ratios\n",
    "df_train['upvotes_per_comment'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_comments_at_request'] + 1)\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_test['upvotes_per_comment'] = df_test['requester_upvotes_plus_downvotes_at_request'] / (df_test['requester_number_of_comments_at_request'] + 1)\n",
    "df_test['comments_per_post'] = df_test['requester_number_of_comments_at_request'] / (df_test['requester_number_of_posts_at_request'] + 1)\n",
    "\n",
    "# Account age in years\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days_at_request'] / 365.25\n",
    "df_test['account_age_years'] = df_test['requester_account_age_in_days_at_request'] / 365.25\n",
    "\n",
    "# Text length features\n",
    "df_train['text_length'] = df_train['combined_text_clean'].str.len()\n",
    "df_train['word_count'] = df_train['combined_text_clean'].str.split().str.len()\n",
    "df_test['text_length'] = df_test['combined_text_clean'].str.len()\n",
    "df_test['word_count'] = df_test['combined_text_clean'].str.split().str.len()\n",
    "\n",
    "# Post edited flag (handle missing in test)\n",
    "if 'post_was_edited' in df_test.columns:\n",
    "    df_test['post_was_edited'] = df_test['post_was_edited'].astype(int)\n",
    "    post_was_edited_flag = True\n",
    "else:\n",
    "    df_test['post_was_edited'] = 0\n",
    "    post_was_edited_flag = False\n",
    "\n",
    "numeric_features = [f'{feat}_log' for feat in count_features] + [\n",
    "    'upvotes_per_comment', 'comments_per_post', 'account_age_years',\n",
    "    'text_length', 'word_count'\n",
    "]\n",
    "\n",
    "if post_was_edited_flag:\n",
    "    df_train['post_was_edited'] = df_train['post_was_edited'].astype(int)\n",
    "    numeric_features.append('post_was_edited')\n",
    "\n",
    "# Prepare numeric matrices\n",
    "train_numeric = df_train[numeric_features].fillna(0).values\n",
    "test_numeric = df_test[numeric_features].fillna(0).values\n",
    "\n",
    "print(f\"Created {len(numeric_features)} numeric features: {numeric_features}\")\n",
    "print(f\"Train numeric shape: {train_numeric.shape}\")\n",
    "print(f\"Test numeric shape: {test_numeric.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5ace7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:14:03.056756Z",
     "iopub.status.busy": "2026-01-10T11:14:03.056482Z",
     "iopub.status.idle": "2026-01-10T11:14:07.492218Z",
     "shell.execute_reply": "2026-01-10T11:14:07.491183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced TF-IDF features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF shape: (2878, 8000)\n",
      "Char TF-IDF shape: (2878, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced TF-IDF with character n-grams\n",
    "print(\"Creating enhanced TF-IDF features...\")\n",
    "\n",
    "# Word-level TF-IDF\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Character-level TF-IDF for stylistic patterns\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 4),\n",
    "    max_features=3000,\n",
    "    lowercase=False,\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "train_text_word = tfidf_word.fit_transform(df_train['combined_text_clean'])\n",
    "test_text_word = tfidf_word.transform(df_test['combined_text_clean'])\n",
    "\n",
    "train_text_char = tfidf_char.fit_transform(df_train['combined_text_clean'])\n",
    "test_text_char = tfidf_char.transform(df_test['combined_text_clean'])\n",
    "\n",
    "print(f\"Word TF-IDF shape: {train_text_word.shape}\")\n",
    "print(f\"Char TF-IDF shape: {train_text_char.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f706d75b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:14:07.495853Z",
     "iopub.status.busy": "2026-01-10T11:14:07.494936Z",
     "iopub.status.idle": "2026-01-10T11:14:10.266846Z",
     "shell.execute_reply": "2026-01-10T11:14:10.266136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying TruncatedSVD for latent semantic analysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD components shape: (2878, 100)\n",
      "Explained variance ratio: 0.1785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char SVD components shape: (2878, 50)\n",
      "Char explained variance ratio: 0.2724\n"
     ]
    }
   ],
   "source": [
    "# Apply TruncatedSVD for dimensionality reduction\n",
    "print(\"Applying TruncatedSVD for latent semantic analysis...\")\n",
    "\n",
    "# Reduce word TF-IDF to 100 components\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "train_svd = svd.fit_transform(train_text_word)\n",
    "test_svd = svd.transform(test_text_word)\n",
    "\n",
    "print(f\"SVD components shape: {train_svd.shape}\")\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Also reduce char TF-IDF to 50 components\n",
    "svd_char = TruncatedSVD(n_components=50, random_state=42)\n",
    "train_svd_char = svd_char.fit_transform(train_text_char)\n",
    "test_svd_char = svd_char.transform(test_text_char)\n",
    "\n",
    "print(f\"Char SVD components shape: {train_svd_char.shape}\")\n",
    "print(f\"Char explained variance ratio: {svd_char.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042e2e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:14:10.269639Z",
     "iopub.status.busy": "2026-01-10T11:14:10.269275Z",
     "iopub.status.idle": "2026-01-10T11:15:15.131044Z",
     "shell.execute_reply": "2026-01-10T11:15:15.130367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold CV with enhanced text representation...\n",
      "\n",
      "Fold 1/5\n",
      "  Training features shape: (2302, 11158)\n",
      "  Validation features shape: (576, 11158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 AUC: 0.6559\n",
      "\n",
      "Fold 2/5\n",
      "  Training features shape: (2302, 11158)\n",
      "  Validation features shape: (576, 11158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2 AUC: 0.6321\n",
      "\n",
      "Fold 3/5\n",
      "  Training features shape: (2302, 11158)\n",
      "  Validation features shape: (576, 11158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3 AUC: 0.6623\n",
      "\n",
      "Fold 4/5\n",
      "  Training features shape: (2303, 11158)\n",
      "  Validation features shape: (575, 11158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4 AUC: 0.6166\n",
      "\n",
      "Fold 5/5\n",
      "  Training features shape: (2303, 11158)\n",
      "  Validation features shape: (575, 11158)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5 AUC: 0.6557\n",
      "\n",
      "Overall CV AUC: 0.6450\n",
      "CV scores: [0.6558729953649122, 0.632148452009884, 0.6623491981459649, 0.6166472416472417, 0.6557239057239057]\n",
      "Mean ± Std: 0.6445 ± 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Stratified CV setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(df_train))\n",
    "test_predictions = np.zeros(len(df_test))\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Starting 5-fold CV with enhanced text representation...\")\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(df_train, y):\n",
    "    fold += 1\n",
    "    print(f\"\\nFold {fold}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_word, X_val_word = train_text_word[train_idx], train_text_word[val_idx]\n",
    "    X_train_char, X_val_char = train_text_char[train_idx], train_text_char[val_idx]\n",
    "    X_train_svd, X_val_svd = train_svd[train_idx], train_svd[val_idx]\n",
    "    X_train_svd_char, X_val_svd_char = train_svd_char[train_idx], train_svd_char[val_idx]\n",
    "    X_train_num, X_val_num = train_numeric[train_idx], train_numeric[val_idx]\n",
    "    \n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_combined = hstack([\n",
    "        X_train_word, X_train_char, X_train_svd, X_train_svd_char, X_train_num\n",
    "    ])\n",
    "    X_val_combined = hstack([\n",
    "        X_val_word, X_val_char, X_val_svd, X_val_svd_char, X_val_num\n",
    "    ])\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train_combined.shape}\")\n",
    "    print(f\"  Validation features shape: {X_val_combined.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        C=0.5  # Slightly stronger regularization for high-dimensional data\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_combined, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict_proba(X_val_combined)[:, 1]\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate score\n",
    "    score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(score)\n",
    "    print(f\"  Fold {fold} AUC: {score:.4f}\")\n",
    "    \n",
    "    # Predict on test for this fold\n",
    "    test_combined = hstack([\n",
    "        test_text_word, test_text_char, test_svd, test_svd_char, test_numeric\n",
    "    ])\n",
    "    fold_test_pred = model.predict_proba(test_combined)[:, 1]\n",
    "    test_predictions += fold_test_pred\n",
    "\n",
    "# Average test predictions across folds\n",
    "test_predictions /= 5\n",
    "\n",
    "# Overall CV score\n",
    "overall_score = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_score:.4f}\")\n",
    "print(f\"CV scores: {cv_scores}\")\n",
    "print(f\"Mean ± Std: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d1b65a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:15:15.133221Z",
     "iopub.status.busy": "2026-01-10T11:15:15.132976Z",
     "iopub.status.idle": "2026-01-10T11:15:30.576598Z",
     "shell.execute_reply": "2026-01-10T11:15:30.575758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on full data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained on 11158 features\n",
      "Final predictions shape: (1162,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Generate final predictions using full training data\n",
    "print(\"Training final model on full data...\")\n",
    "\n",
    "# Combine all features for final model\n",
    "train_combined = hstack([\n",
    "    train_text_word, train_text_char, train_svd, train_svd_char, train_numeric\n",
    "])\n",
    "test_combined = hstack([\n",
    "    test_text_word, test_text_char, test_svd, test_svd_char, test_numeric\n",
    "])\n",
    "\n",
    "# Train final model\n",
    "final_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    C=0.5\n",
    ")\n",
    "\n",
    "final_model.fit(train_combined, y)\n",
    "\n",
    "# Generate final predictions\n",
    "final_predictions = final_model.predict_proba(test_combined)[:, 1]\n",
    "\n",
    "print(f\"Final model trained on {train_combined.shape[1]} features\")\n",
    "print(f\"Final predictions shape: {final_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f4131b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T11:15:30.580039Z",
     "iopub.status.busy": "2026-01-10T11:15:30.579147Z",
     "iopub.status.idle": "2026-01-10T11:15:30.597285Z",
     "shell.execute_reply": "2026-01-10T11:15:30.596139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /home/submission/submission_enhanced_text.csv\n",
      "Submission shape: (1162, 2)\n",
      "Prediction range: 0.1069 to 0.8835\n",
      "\n",
      "Submission preview:\n",
      "  request_id  requester_received_pizza\n",
      "0  t3_1aw5zf                  0.601710\n",
      "1   t3_roiuw                  0.467747\n",
      "2   t3_mjnbq                  0.338807\n",
      "3   t3_t8wd1                  0.518842\n",
      "4  t3_1m4zxu                  0.565939\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': df_test['request_id'],\n",
    "    'requester_received_pizza': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure proper format\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].astype(float)\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_enhanced_text.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: {submission['requester_received_pizza'].min():.4f} to {submission['requester_received_pizza'].max():.4f}\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
