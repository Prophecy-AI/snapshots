{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9bb45e8",
   "metadata": {},
   "source": [
    "# Enhanced Text Representation Experiment\n",
    "\n",
    "This experiment focuses on improving text representation using:\n",
    "1. **TruncatedSVD (LSA)**: Dimensionality reduction on TF-IDF matrix to capture latent semantics (100 components)\n",
    "2. **Character n-grams**: Add 2-4 character n-grams to capture stylistic patterns\n",
    "3. **Enhanced TF-IDF**: Better parameters and preprocessing\n",
    "\n",
    "Based on the strategy, these techniques should capture more signal than basic TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text features\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "df_test['combined_text'] = df_test['request_title'].fillna('') + ' ' + df_test['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text while preserving important patterns\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove Reddit usernames (e.g., /u/username)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    \n",
    "    # Remove edit markers\n",
    "    text = re.sub(r'EDIT:\\s*', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['combined_text_clean'] = df_train['combined_text'].apply(preprocess_text)\n",
    "df_test['combined_text_clean'] = df_test['combined_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced numeric features\n",
    "y = df_train['requester_received_pizza'].values\n",
    "\n",
    "# Log transforms for count features\n",
    "count_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feat in count_features:\n",
    "    df_train[f'{feat}_log'] = np.log1p(df_train[feat])\n",
    "    df_test[f'{feat}_log'] = np.log1p(df_test[feat])\n",
    "\n",
    "# Ratios\n",
    "df_train['upvotes_per_comment'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_comments_at_request'] + 1)\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_test['upvotes_per_comment'] = df_test['requester_upvotes_plus_downvotes_at_request'] / (df_test['requester_number_of_comments_at_request'] + 1)\n",
    "df_test['comments_per_post'] = df_test['requester_number_of_comments_at_request'] / (df_test['requester_number_of_posts_at_request'] + 1)\n",
    "\n",
    "# Account age in years\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days_at_request'] / 365.25\n",
    "df_test['account_age_years'] = df_test['requester_account_age_in_days_at_request'] / 365.25\n",
    "\n",
    "# Text length features\n",
    "df_train['text_length'] = df_train['combined_text_clean'].str.len()\n",
    "df_train['word_count'] = df_train['combined_text_clean'].str.split().str.len()\n",
    "df_test['text_length'] = df_test['combined_text_clean'].str.len()\n",
    "df_test['word_count'] = df_test['combined_text_clean'].str.split().str.len()\n",
    "\n",
    "# Post edited flag (handle missing in test)\n",
    "if 'post_was_edited' in df_test.columns:\n",
    "    df_test['post_was_edited'] = df_test['post_was_edited'].astype(int)\n",
    "    post_was_edited_flag = True\n",
    "else:\n",
    "    df_test['post_was_edited'] = 0\n",
    "    post_was_edited_flag = False\n",
    "\n",
    "numeric_features = [f'{feat}_log' for feat in count_features] + [\n",
    "    'upvotes_per_comment', 'comments_per_post', 'account_age_years',\n",
    "    'text_length', 'word_count'\n",
    "]\n",
    "\n",
    "if post_was_edited_flag:\n",
    "    df_train['post_was_edited'] = df_train['post_was_edited'].astype(int)\n",
    "    numeric_features.append('post_was_edited')\n",
    "\n",
    "# Prepare numeric matrices\n",
    "train_numeric = df_train[numeric_features].fillna(0).values\n",
    "test_numeric = df_test[numeric_features].fillna(0).values\n",
    "\n",
    "print(f\"Created {len(numeric_features)} numeric features: {numeric_features}\")\n",
    "print(f\"Train numeric shape: {train_numeric.shape}\")\n",
    "print(f\"Test numeric shape: {test_numeric.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ace7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced TF-IDF with character n-grams\n",
    "print(\"Creating enhanced TF-IDF features...\")\n",
    "\n",
    "# Word-level TF-IDF\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Character-level TF-IDF for stylistic patterns\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 4),\n",
    "    max_features=3000,\n",
    "    lowercase=False,\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "train_text_word = tfidf_word.fit_transform(df_train['combined_text_clean'])\n",
    "test_text_word = tfidf_word.transform(df_test['combined_text_clean'])\n",
    "\n",
    "train_text_char = tfidf_char.fit_transform(df_train['combined_text_clean'])\n",
    "test_text_char = tfidf_char.transform(df_test['combined_text_clean'])\n",
    "\n",
    "print(f\"Word TF-IDF shape: {train_text_word.shape}\")\n",
    "print(f\"Char TF-IDF shape: {train_text_char.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TruncatedSVD for dimensionality reduction\n",
    "print(\"Applying TruncatedSVD for latent semantic analysis...\")\n",
    "\n",
    "# Reduce word TF-IDF to 100 components\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "train_svd = svd.fit_transform(train_text_word)\n",
    "test_svd = svd.transform(test_text_word)\n",
    "\n",
    "print(f\"SVD components shape: {train_svd.shape}\")\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Also reduce char TF-IDF to 50 components\n",
    "svd_char = TruncatedSVD(n_components=50, random_state=42)\n",
    "train_svd_char = svd_char.fit_transform(train_text_char)\n",
    "test_svd_char = svd_char.transform(test_text_char)\n",
    "\n",
    "print(f\"Char SVD components shape: {train_svd_char.shape}\")\n",
    "print(f\"Char explained variance ratio: {svd_char.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified CV setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(df_train))\n",
    "test_predictions = np.zeros(len(df_test))\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Starting 5-fold CV with enhanced text representation...\")\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(df_train, y):\n",
    "    fold += 1\n",
    "    print(f\"\\nFold {fold}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_word, X_val_word = train_text_word[train_idx], train_text_word[val_idx]\n",
    "    X_train_char, X_val_char = train_text_char[train_idx], train_text_char[val_idx]\n",
    "    X_train_svd, X_val_svd = train_svd[train_idx], train_svd[val_idx]\n",
    "    X_train_svd_char, X_val_svd_char = train_svd_char[train_idx], train_svd_char[val_idx]\n",
    "    X_train_num, X_val_num = train_numeric[train_idx], train_numeric[val_idx]\n",
    "    \n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_combined = hstack([\n",
    "        X_train_word, X_train_char, X_train_svd, X_train_svd_char, X_train_num\n",
    "    ])\n",
    "    X_val_combined = hstack([\n",
    "        X_val_word, X_val_char, X_val_svd, X_val_svd_char, X_val_num\n",
    "    ])\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train_combined.shape}\")\n",
    "    print(f\"  Validation features shape: {X_val_combined.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        C=0.5  # Slightly stronger regularization for high-dimensional data\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_combined, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict_proba(X_val_combined)[:, 1]\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate score\n",
    "    score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(score)\n",
    "    print(f\"  Fold {fold} AUC: {score:.4f}\")\n",
    "    \n",
    "    # Predict on test for this fold\n",
    "    test_combined = hstack([\n",
    "        test_text_word, test_text_char, test_svd, test_svd_char, test_numeric\n",
    "    ])\n",
    "    fold_test_pred = model.predict_proba(test_combined)[:, 1]\n",
    "    test_predictions += fold_test_pred\n",
    "\n",
    "# Average test predictions across folds\n",
    "test_predictions /= 5\n",
    "\n",
    "# Overall CV score\n",
    "overall_score = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_score:.4f}\")\n",
    "print(f\"CV scores: {cv_scores}\")\n",
    "print(f\"Mean ± Std: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions using full training data\n",
    "print(\"Training final model on full data...\")\n",
    "\n",
    "# Combine all features for final model\n",
    "train_combined = hstack([\n",
    "    train_text_word, train_text_char, train_svd, train_svd_char, train_numeric\n",
    "])\n",
    "test_combined = hstack([\n",
    "    test_text_word, test_text_char, test_svd, test_svd_char, test_numeric\n",
    "])\n",
    "\n",
    "# Train final model\n",
    "final_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    C=0.5\n",
    ")\n",
    "\n",
    "final_model.fit(train_combined, y)\n",
    "\n",
    "# Generate final predictions\n",
    "final_predictions = final_model.predict_proba(test_combined)[:, 1]\n",
    "\n",
    "print(f\"Final model trained on {train_combined.shape[1]} features\")\n",
    "print(f\"Final predictions shape: {final_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': df_test['request_id'],\n",
    "    'requester_received_pizza': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure proper format\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].astype(float)\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_enhanced_text.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: {submission['requester_received_pizza'].min():.4f} to {submission['requester_received_pizza'].max():.4f}\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
