{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ad4cfe",
   "metadata": {},
   "source": [
    "# Experiment 004: LightGBM with Fixed Leakage\n",
    "\n",
    "This experiment addresses the critical issues identified in exp_003:\n",
    "1. **Model upgrade**: Switch from LogisticRegression to LightGBM\n",
    "2. **Fix data leakage**: Move ALL feature fitting inside CV loop\n",
    "3. **Optimize dimensionality**: Reduce to 75 total SVD components (50 word + 25 char)\n",
    "4. **Better regularization**: LightGBM handles high dimensionality better than logistic regression\n",
    "\n",
    "Expected improvement: +0.03 to +0.08 AUC (target: 0.68-0.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text features\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "df_test['combined_text'] = df_test['request_title'].fillna('') + ' ' + df_test['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text while preserving important patterns\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'EDIT:\\s*', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['combined_text_clean'] = df_train['combined_text'].apply(preprocess_text)\n",
    "df_test['combined_text_clean'] = df_test['combined_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced numeric features\n",
    "y = df_train['requester_received_pizza'].values\n",
    "\n",
    "# Log transforms for count features\n",
    "count_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feat in count_features:\n",
    "    df_train[f'{feat}_log'] = np.log1p(df_train[feat])\n",
    "    df_test[f'{feat}_log'] = np.log1p(df_test[feat])\n",
    "\n",
    "# Ratios\n",
    "df_train['upvotes_per_comment'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_comments_at_request'] + 1)\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_test['upvotes_per_comment'] = df_test['requester_upvotes_plus_downvotes_at_request'] / (df_test['requester_number_of_comments_at_request'] + 1)\n",
    "df_test['comments_per_post'] = df_test['requester_number_of_comments_at_request'] / (df_test['requester_number_of_posts_at_request'] + 1)\n",
    "\n",
    "# Account age in years\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days_at_request'] / 365.25\n",
    "df_test['account_age_years'] = df_test['requester_account_age_in_days_at_request'] / 365.25\n",
    "\n",
    "# Text statistics\n",
    "df_train['text_length'] = df_train['combined_text_clean'].str.len()\n",
    "df_test['text_length'] = df_test['combined_text_clean'].str.len()\n",
    "df_train['word_count'] = df_train['combined_text_clean'].str.split().str.len()\n",
    "df_test['word_count'] = df_test['combined_text_clean'].str.split().str.len()\n",
    "\n",
    "# Select numeric features\n",
    "numeric_features = [\n",
    "    'requester_number_of_comments_at_request_log',\n",
    "    'requester_number_of_posts_at_request_log', \n",
    "    'requester_upvotes_plus_downvotes_at_request_log',\n",
    "    'upvotes_per_comment',\n",
    "    'comments_per_post',\n",
    "    'account_age_years',\n",
    "    'text_length',\n",
    "    'word_count'\n",
    "]\n",
    "\n",
    "train_numeric = df_train[numeric_features].values\n",
    "test_numeric = df_test[numeric_features].values\n",
    "\n",
    "print(f\"Created {len(numeric_features)} numeric features\")\n",
    "print(f\"Train numeric shape: {train_numeric.shape}\")\n",
    "print(f\"Test numeric shape: {test_numeric.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d49b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transformer for text features with SVD\n",
    "def create_text_pipeline(n_word_components=50, n_char_components=25):\n",
    "    \"\"\"Create a pipeline that transforms text to SVD components\"\"\"\n",
    "    \n",
    "    # Word-level TF-IDF + SVD\n",
    "    word_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('svd', TruncatedSVD(n_components=n_word_components, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Character-level TF-IDF + SVD\n",
    "    char_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=2000,\n",
    "            lowercase=False,\n",
    "            min_df=5,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        ('svd', TruncatedSVD(n_components=n_char_components, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return word_pipeline, char_pipeline\n",
    "\n",
    "print(\"Text pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified CV setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(df_train))\n",
    "test_predictions = np.zeros(len(df_test))\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Starting 5-fold CV with LightGBM and fixed leakage...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(df_train, y):\n",
    "    fold += 1\n",
    "    print(f\"\\nFold {fold}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_text = df_train['combined_text_clean'].iloc[train_idx]\n",
    "    X_val_text = df_train['combined_text_clean'].iloc[val_idx]\n",
    "    X_train_num = train_numeric[train_idx]\n",
    "    X_val_num = train_numeric[val_idx]\n",
    "    \n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create and fit text pipelines (INSIDE CV LOOP - NO LEAKAGE)\n",
    "    word_pipe, char_pipe = create_text_pipeline(n_word_components=50, n_char_components=25)\n",
    "    \n",
    "    # Fit on training data only\n",
    "    X_train_word_svd = word_pipe.fit_transform(X_train_text)\n",
    "    X_train_char_svd = char_pipe.fit_transform(X_train_text)\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val_word_svd = word_pipe.transform(X_val_text)\n",
    "    X_val_char_svd = char_pipe.transform(X_val_text)\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_combined = np.hstack([X_train_word_svd, X_train_char_svd, X_train_num])\n",
    "    X_val_combined = np.hstack([X_val_word_svd, X_val_char_svd, X_val_num])\n",
    "    \n",
    "    print(f\"  Training features shape: {X_train_combined.shape}\")\n",
    "    print(f\"  Validation features shape: {X_val_combined.shape}\")\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        max_depth=7,\n",
    "        min_child_samples=50,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_combined, y_train,\n",
    "        eval_set=[(X_val_combined, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    val_pred = model.predict_proba(X_val_combined)[:, 1]\n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    print(f\"  Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    print(f\"  Best iteration: {model.best_iteration_}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Overall CV AUC: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"CV scores: {cv_scores}\")\n",
    "print(f\"Mean ± Std: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions using full training data\n",
    "print(\"Training final model on full data...\")\n",
    "\n",
    "# Create and fit text pipelines on full training data\n",
    "word_pipe_full, char_pipe_full = create_text_pipeline(n_word_components=50, n_char_components=25)\n",
    "\n",
    "train_word_svd_full = word_pipe_full.fit_transform(df_train['combined_text_clean'])\n",
    "train_char_svd_full = char_pipe_full.fit_transform(df_train['combined_text_clean'])\n",
    "\n",
    "test_word_svd_full = word_pipe_full.transform(df_test['combined_text_clean'])\n",
    "test_char_svd_full = char_pipe_full.transform(df_test['combined_text_clean'])\n",
    "\n",
    "# Combine all features\n",
    "train_combined_full = np.hstack([train_word_svd_full, train_char_svd_full, train_numeric])\n",
    "test_combined_full = np.hstack([test_word_svd_full, test_char_svd_full, test_numeric])\n",
    "\n",
    "print(f\"Final training features shape: {train_combined_full.shape}\")\n",
    "print(f\"Final test features shape: {test_combined_full.shape}\")\n",
    "\n",
    "# Train final model\n",
    "final_model = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    max_depth=7,\n",
    "    min_child_samples=50,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    train_combined_full, y,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Generate final predictions\n",
    "final_predictions = final_model.predict_proba(test_combined_full)[:, 1]\n",
    "\n",
    "print(f\"Final model trained on {train_combined_full.shape[1]} features\")\n",
    "print(f\"Final predictions shape: {final_predictions.shape}\")\n",
    "print(f\"Feature importance - top 10:\")\n",
    "for i, (idx, imp) in enumerate(sorted(enumerate(final_model.feature_importances_), key=lambda x: x[1], reverse=True)[:10]):\n",
    "    print(f\"  {i+1}. Feature {idx}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': df_test['request_id'],\n",
    "    'requester_received_pizza': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure proper format\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].astype(float)\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_lightgbm_fixed_leakage.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: {submission['requester_received_pizza'].min():.4f} to {submission['requester_received_pizza'].max():.4f}\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
