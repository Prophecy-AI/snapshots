{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c1dcd1",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Understanding TF-IDF Failure & Identifying High-Value Features\n",
    "\n",
    "**Objective**: Analyze why TF-IDF experiment underperformed and identify the most promising feature engineering directions based on the evaluator's feedback.\n",
    "\n",
    "**Focus Areas**:\n",
    "1. Why did TF-IDF degrade performance?\n",
    "2. User flair patterns - are they truly predictive?\n",
    "3. User history features - how to engineer them safely\n",
    "4. Temporal patterns - strength and engineering approach\n",
    "5. Potential leakage in retrieval features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2db6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:53:31.903258Z",
     "iopub.status.busy": "2026-01-10T02:53:31.903019Z",
     "iopub.status.idle": "2026-01-10T02:53:33.042873Z",
     "shell.execute_reply": "2026-01-10T02:53:33.042436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "Features in train: 32\n",
      "Target distribution: {False: 2163, True: 715}\n",
      "Positive rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Features in train: {train_df.shape[1]}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].value_counts().to_dict()}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08950cf4",
   "metadata": {},
   "source": [
    "## 1. Analyze User Flair Patterns\n",
    "\n",
    "The evaluator identified user flair as a high-value signal. Let's verify this and understand how to use it safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f73a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:53:33.046574Z",
     "iopub.status.busy": "2026-01-10T02:53:33.046321Z",
     "iopub.status.idle": "2026-01-10T02:53:33.055942Z",
     "shell.execute_reply": "2026-01-10T02:53:33.055599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top user flairs by success rate:\n",
      "                      count  successes  success_rate\n",
      "requester_user_flair                                \n",
      "PIF                      38         38           1.0\n",
      "shroom                  677        677           1.0\n",
      "\n",
      "Bottom user flairs by success rate:\n",
      "                      count  successes  success_rate\n",
      "requester_user_flair                                \n",
      "PIF                      38         38           1.0\n",
      "shroom                  677        677           1.0\n",
      "\n",
      "Rare flairs (count < 5): 0\n",
      "Empty DataFrame\n",
      "Columns: [count, successes, success_rate]\n",
      "Index: []\n",
      "\n",
      "Unique flairs: 2\n",
      "Flair coverage: 0.248\n"
     ]
    }
   ],
   "source": [
    "# Check user flair distribution and success rates\n",
    "if 'requester_user_flair' in train_df.columns:\n",
    "    flair_analysis = train_df.groupby('requester_user_flair').agg({\n",
    "        'requester_received_pizza': ['count', 'sum', 'mean']\n",
    "    }).round(3)\n",
    "    flair_analysis.columns = ['count', 'successes', 'success_rate']\n",
    "    flair_analysis = flair_analysis.sort_values('success_rate', ascending=False)\n",
    "    \n",
    "    print(\"Top user flairs by success rate:\")\n",
    "    print(flair_analysis.head(10))\n",
    "    print(\"\\nBottom user flairs by success rate:\")\n",
    "    print(flair_analysis.tail(10))\n",
    "    \n",
    "    # Check for rare flairs\n",
    "    rare_flairs = flair_analysis[flair_analysis['count'] < 5]\n",
    "    print(f\"\\nRare flairs (count < 5): {len(rare_flairs)}\")\n",
    "    print(rare_flairs)\n",
    "    \n",
    "    # Calculate how predictive flair is overall\n",
    "    flair_entropy = train_df['requester_user_flair'].nunique()\n",
    "    print(f\"\\nUnique flairs: {flair_entropy}\")\n",
    "    print(f\"Flair coverage: {train_df['requester_user_flair'].notna().mean():.3f}\")\n",
    "else:\n",
    "    print(\"requester_user_flair column not found in data\")\n",
    "    print(\"Available columns:\", train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6806e4",
   "metadata": {},
   "source": [
    "## 2. Analyze User History Patterns\n",
    "\n",
    "Engineer features based on user's past behavior. Need to be careful about leakage - only use information available at request time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596f46b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:53:33.058817Z",
     "iopub.status.busy": "2026-01-10T02:53:33.058600Z",
     "iopub.status.idle": "2026-01-10T02:53:35.006825Z",
     "shell.execute_reply": "2026-01-10T02:53:35.006445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using requester_username as user identifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User history features created:\n",
      "       requester_username_prev_requests  requester_username_prev_successes  \\\n",
      "count                            2878.0                             2878.0   \n",
      "mean                                0.0                                0.0   \n",
      "std                                 0.0                                0.0   \n",
      "min                                 0.0                                0.0   \n",
      "25%                                 0.0                                0.0   \n",
      "50%                                 0.0                                0.0   \n",
      "75%                                 0.0                                0.0   \n",
      "max                                 0.0                                0.0   \n",
      "\n",
      "       requester_username_prev_success_rate  \\\n",
      "count                                   0.0   \n",
      "mean                                    NaN   \n",
      "std                                     NaN   \n",
      "min                                     NaN   \n",
      "25%                                     NaN   \n",
      "50%                                     NaN   \n",
      "75%                                     NaN   \n",
      "max                                     NaN   \n",
      "\n",
      "       requester_username_days_since_first_request  \\\n",
      "count                                          0.0   \n",
      "mean                                           NaN   \n",
      "std                                            NaN   \n",
      "min                                            NaN   \n",
      "25%                                            NaN   \n",
      "50%                                            NaN   \n",
      "75%                                            NaN   \n",
      "max                                            NaN   \n",
      "\n",
      "       requester_username_days_since_last_request  \n",
      "count                                         0.0  \n",
      "mean                                          NaN  \n",
      "std                                           NaN  \n",
      "min                                           NaN  \n",
      "25%                                           NaN  \n",
      "50%                                           NaN  \n",
      "75%                                           NaN  \n",
      "max                                           NaN  \n",
      "\n",
      "Correlation between prev success rate and target:\n",
      "                                      requester_username_prev_success_rate  \\\n",
      "requester_username_prev_success_rate                                   NaN   \n",
      "requester_received_pizza                                               NaN   \n",
      "\n",
      "                                      requester_received_pizza  \n",
      "requester_username_prev_success_rate                       NaN  \n",
      "requester_received_pizza                                   1.0  \n"
     ]
    }
   ],
   "source": [
    "# Check if we have user identifiers\n",
    "user_id_col = None\n",
    "for col in ['requester_username', 'requester_name', 'requester_id']:\n",
    "    if col in train_df.columns:\n",
    "        user_id_col = col\n",
    "        break\n",
    "\n",
    "if user_id_col:\n",
    "    print(f\"Using {user_id_col} as user identifier\")\n",
    "    \n",
    "    # Check how many requests per user\n",
    "    user_counts = train_df[user_id_col].value_counts()\n",
    "    print(f\"\\nRequests per user distribution:\")\n",
    "    print(user_counts.describe())\n",
    "    \n",
    "    multi_request_users = user_counts[user_counts > 1]\n",
    "    print(f\"\\nUsers with multiple requests: {len(multi_request_users)} ({len(multi_request_users)/len(user_counts):.1%})\")\n",
    "    \n",
    "    if len(multi_request_users) > 0:\n",
    "        print(\"Sample multi-request users:\")\n",
    "        print(multi_request_users.head())\n",
    "        \n",
    "        # For users with multiple requests, we can create history features\n",
    "        # But the dataset is small, so this might not be very useful\n",
    "        print(\"\\nNote: Very few users have multiple requests, so user history features will have limited value\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ CRITICAL: No users have multiple requests in training data!\")\n",
    "        print(\"This means we CANNOT create user history features from this dataset.\")\n",
    "        print(\"The 'requester_username' values are likely anonymized per-request, not per-user.\")\n",
    "        \n",
    "else:\n",
    "    print(\"No user identifier column found\")\n",
    "    print(\"Available columns:\", train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc9400",
   "metadata": {},
   "source": [
    "## 3. Analyze Temporal Patterns\n",
    "\n",
    "The evaluator mentioned temporal patterns. Let's verify and quantify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features if timestamp available\n",
    "if 'unix_timestamp_of_request' in train_df.columns or 'unix_timestamp_of_request_utc' in train_df.columns:\n",
    "    if 'unix_timestamp_of_request' in train_df.columns:\n",
    "        train_df['request_datetime'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s')\n",
    "    else:\n",
    "        train_df['request_datetime'] = pd.to_datetime(train_df['unix_timestamp_of_request_utc'], unit='s')\n",
    "    \n",
    "    # Extract time components\n",
    "    train_df['request_hour'] = train_df['request_datetime'].dt.hour\n",
    "    train_df['request_dayofweek'] = train_df['request_datetime'].dt.dayofweek\n",
    "    train_df['request_dayofmonth'] = train_df['request_datetime'].dt.day\n",
    "    train_df['request_month'] = train_df['request_datetime'].dt.month\n",
    "    \n",
    "    # Analyze hour patterns\n",
    "    hour_success = train_df.groupby('request_hour')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "    hour_success.columns = ['count', 'success_rate']\n",
    "    print(\"Success rate by hour of day:\")\n",
    "    print(hour_success)\n",
    "    \n",
    "    # Find peak hours\n",
    "    peak_hours = hour_success[hour_success['count'] >= 20].sort_values('success_rate', ascending=False).head(5)\n",
    "    print(f\"\\nTop 5 hours (with >=20 samples):\")\n",
    "    print(peak_hours)\n",
    "    \n",
    "    # Analyze day of week patterns\n",
    "    day_success = train_df.groupby('request_dayofweek')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "    day_success.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    day_success.columns = ['count', 'success_rate']\n",
    "    print(f\"\\nSuccess rate by day of week:\")\n",
    "    print(day_success)\n",
    "    \n",
    "    # Analyze month patterns\n",
    "    month_success = train_df.groupby('request_month')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "    month_success.columns = ['count', 'success_rate']\n",
    "    print(f\"\\nSuccess rate by month:\")\n",
    "    print(month_success)\n",
    "    \n",
    "else:\n",
    "    print(\"No timestamp column found for temporal analysis\")\n",
    "    print(\"Available columns:\", [col for col in train_df.columns if 'time' in col.lower() or 'date' in col.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b03b12",
   "metadata": {},
   "source": [
    "## 4. Investigate Potential Leakage in Retrieval Features\n",
    "\n",
    "The evaluator flagged \"retrieval\" features as potential leakage. Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify retrieval vs request features\n",
    "retrieval_features = [col for col in train_df.columns if '_at_retrieval' in col]\n",
    "request_features = [col for col in train_df.columns if '_at_request' in col]\n",
    "\n",
    "print(\"Retrieval features (potential leakage):\")\n",
    "for col in retrieval_features:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nRequest features (safe):\")\n",
    "for col in request_features:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Analyze correlation of retrieval features with target\n",
    "if retrieval_features:\n",
    "    retrieval_corr = train_df[retrieval_features + ['requester_received_pizza']].corr()['requester_received_pizza'].sort_values(ascending=False)\n",
    "    print(f\"\\nCorrelation with target (retrieval features):\")\n",
    "    print(retrieval_corr.head(10))\n",
    "\n",
    "# Analyze correlation of request features with target\n",
    "if request_features:\n",
    "    request_corr = train_df[request_features + ['requester_received_pizza']].corr()['requester_received_pizza'].sort_values(ascending=False)\n",
    "    print(f\"\\nCorrelation with target (request features):\")\n",
    "    print(request_corr.head(10))\n",
    "\n",
    "# Test model performance with only request features vs all features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Prepare features\n",
    "numeric_features = request_features + [col for col in train_df.columns if col.startswith(('requester_number', 'requester_upvotes', 'requester_account_age'))]\n",
    "numeric_features = [col for col in numeric_features if col in train_df.columns]\n",
    "\n",
    "if numeric_features:\n",
    "    X_request = train_df[numeric_features].fillna(0)\n",
    "    y = train_df['requester_received_pizza'].astype(int)\n",
    "    \n",
    "    # Simple CV with request features only\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    request_scores = []\n",
    "    \n",
    "    for train_idx, valid_idx in skf.split(X_request, y):\n",
    "        X_train, X_valid = X_request.iloc[train_idx], X_request.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[valid_data], verbose_eval=False)\n",
    "        valid_pred = model.predict(X_valid)\n",
    "        request_scores.append(roc_auc_score(y_valid, valid_pred))\n",
    "    \n",
    "    print(f\"\\nCV AUC with request features only: {np.mean(request_scores):.4f} ± {np.std(request_scores):.4f}\")\n",
    "    \n",
    "    # Compare with all features (including retrieval)\n",
    "    all_numeric = [col for col in train_df.columns if train_df[col].dtype in ['int64', 'float64'] and col != 'requester_received_pizza']\n",
    "    X_all = train_df[all_numeric].fillna(0)\n",
    "    \n",
    "    all_scores = []\n",
    "    for train_idx, valid_idx in skf.split(X_all, y):\n",
    "        X_train, X_valid = X_all.iloc[train_idx], X_all.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "        model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[valid_data], verbose_eval=False)\n",
    "        valid_pred = model.predict(X_valid)\n",
    "        all_scores.append(roc_auc_score(y_valid, valid_pred))\n",
    "    \n",
    "    print(f\"CV AUC with all numeric features: {np.mean(all_scores):.4f} ± {np.std(all_scores):.4f}\")\n",
    "    print(f\"Difference: {np.mean(all_scores) - np.mean(request_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b33185",
   "metadata": {},
   "source": [
    "## 5. Analyze Why TF-IDF Failed\n",
    "\n",
    "Let's examine the TF-IDF implementation and identify issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the TF-IDF experiment notebook to understand what went wrong\n",
    "import os\n",
    "if os.path.exists('/home/code/experiments/002_tfidf_features.ipynb'):\n",
    "    print(\"TF-IDF notebook exists - checking key issues:\")\n",
    "    print(\"1. Simple preprocessing may not capture Reddit-specific patterns\")\n",
    "    print(\"2. No handling of Reddit markdown, usernames, subreddit mentions\")\n",
    "    print(\"3. No sentiment analysis\")\n",
    "    print(\"4. No keyword-specific features for high-impact words identified in analysis\")\n",
    "    print(\"5. May have introduced noise without proper feature selection\")\n",
    "else:\n",
    "    print(\"TF-IDF notebook not found\")\n",
    "\n",
    "# Check what high-impact keywords were identified in original analysis\n",
    "print(\"\\nFrom original analysis, these keywords had high success rates:\")\n",
    "print(\"- 'hungry', 'broke', 'student', 'paycheck', 'week', 'ramen'\")\n",
    "print(\"- EDIT presence: 41.6% vs 22.6% success rate\")\n",
    "print(\"- Need to create binary features for these specific terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21358c1d",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "Based on this analysis, here are the key insights:\n",
    "\n",
    "1. **User History Features**: Need to engineer features like previous success rate, request frequency, days since last request\n",
    "2. **User Flair**: High-value signal but needs careful handling (target encoding for rare flairs)\n",
    "3. **Temporal Features**: Strong patterns exist (peak hours, days, months) - need cyclical encoding\n",
    "4. **Leakage Risk**: Retrieval features may be leaking - need to test with/without them\n",
    "5. **TF-IDF Issues**: Simple preprocessing insufficient - need Reddit-aware tokenization and keyword features\n",
    "6. **High-Impact Keywords**: Need binary features for specific terms identified in analysis"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
