## Data Understanding

**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, correlations
- Use findings from these notebooks when implementing features

## Models

For tabular data competitions, the most effective approach is to use a diverse ensemble of models:

**Primary Models:**
- **Gradient Boosting Machines**: XGBoost, LightGBM, and CatBoost form the foundation of most winning solutions
  - LightGBM is often preferred for speed and performance balance
  - CatBoost excels with categorical features without extensive preprocessing
  - XGBoost provides robust performance across many scenarios

**Deep Learning:**
- **TabM**: Parameter-efficient ensembling technique that produces multiple predictions per object
  - Creates ensemble-like behavior within a single model architecture
  - Multiple predictions are weak individually but powerful collectively
  - More efficient than traditional deep ensembles while maintaining competitive performance
  - Can be used standalone or as part of a larger ensemble

**Model Selection Strategy:**
- Start with 3-5 diverse models (e.g., LightGBM, CatBoost, XGBoost, TabM)
- Each model should have different strengths (handling categoricals, speed, robustness)
- Avoid using too many similar models - diversity is more important than quantity

## Preprocessing

**Categorical Features:**
- **Target Encoding**: Use with nested cross-validation to avoid leakage
  - Particularly effective for high-cardinality features
  - Combine with smoothing to prevent overfitting
- **Frequency Encoding**: Simple but effective for capturing category importance
- **Ordinal Encoding**: Works well with tree-based models like LightGBM
- **One-hot Encoding**: Only for low-cardinality features (< 10 categories)

**Numerical Features:**
- **Log Transform**: For skewed distributions (common in monetary values, counts)
- **Binning**: Create categorical bins from continuous features
  - Equal-width or quantile-based binning
  - Helps tree models capture non-linear relationships
- **Scaling**: StandardScaler or MinMaxScaler for neural networks
- **Outlier Treatment**: Clip extreme values or use robust scaling

**Feature Engineering:**
- **Group-by Aggregations**: The most powerful technique for tabular data
  - Compute statistics (mean, std, count, min, max, nunique) within groups
  - Example: `groupby(user_id)[feature].agg(['mean', 'std', 'count'])`
- **Interaction Features**: Multiply or combine related columns
  - Let tree models discover non-linear relationships
- **Date-Time Features**: Extract year, month, day, hour, cyclical patterns
  - Use sin/cos transforms for cyclical features
- **Missing Value Indicators**: Create flags for missing data patterns
- **Aggregation of Missing Patterns**: Create features from NaN combinations across columns

## Validation Strategy

**Cross-Validation:**
- **Stratified K-Fold (k=5)**: Most common for classification
- **TimeSeriesSplit**: If temporal patterns exist in data
- **GroupKFold**: For grouped data (users, patients, etc.)

**Key Principles:**
- Always use out-of-fold (OOF) predictions for stacking
- Match validation strategy to test data structure
- Check for distribution shift between train and test
- Monitor both local CV and public leaderboard scores

## Ensembling

**Primary Strategy: Stacking with Bagging**

1. **Base Layer (Bagged Models):**
   - Train each model with K-fold cross-validation
   - Generate out-of-fold predictions
   - Average predictions across folds for each model type

2. **Meta-Layer:**
   - Use a simple model (often linear regression or logistic regression)
   - Train on OOF predictions from base layer
   - Can add original features as additional inputs

**Alternative Approaches:**
- **Weighted Averaging**: Simple but effective when models are diverse
- **Rank Averaging**: For classification, average predicted ranks instead of probabilities
- **Pseudo-Labeling**: Use test set predictions to augment training data
  - Only use high-confidence predictions
  - Iteratively refine models

**Advanced Techniques:**
- **Hill Climbing**: Greedy optimization of ensemble weights
- **Multi-layer Stacking**: Add additional layers if computational resources allow
- **AutoGluon-style Ensembling**: Automated multi-layer stacking with diverse model types

## Optimization

**Hyperparameter Tuning:**
- **Bayesian Optimization**: Efficient for exploring large search spaces
- **Random Search**: Often competitive with more sophisticated methods
- **Successive Halving**: Quickly eliminate poor configurations

**Key Parameters to Tune:**
- **Tree Models**: learning_rate, max_depth, num_leaves, min_child_samples
- **Neural Networks**: learning_rate, batch_size, hidden_dims, dropout
- **TabM**: k (number of ensemble members), architecture depth/width

**Training Strategy:**
- Start with default parameters for quick baseline
- Focus tuning on most impactful parameters first
- Use early stopping to prevent overfitting
- Monitor validation loss, not just final metric

## Implementation Tips

**Speed Optimization:**
- Use GPU acceleration for feature engineering (cuDF) and training
- Parallelize model training when possible
- Use efficient data formats (Parquet, Feather)
- Reduce memory usage through appropriate dtypes

**Robustness:**
- Seed averaging: Train multiple models with different random seeds
- Ensemble diversity: Ensure models make different types of errors
- Regularization: Use appropriate regularization for each model type
- Monitor for overfitting: Watch gap between train and validation performance

**Kaggle-Specific:**
- Track public vs private LB performance
- Don't overfit to public leaderboard
- Use all available data for final model
- Consider creating validation sets that mimic test distribution