{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b640bef",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis: Retrieval Feature Leakage Investigation\n",
    "\n",
    "This notebook investigates whether retrieval features contain leakage and explains the flat score trajectory.\n",
    "\n",
    "**Hypothesis**: Features with `_at_retrieval` contain post-outcome information and are causing score degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4418d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load data\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nTraining columns: {train_df.shape[1]}\")\n",
    "print(f\"Test columns: {test_df.shape[1]}\")\n",
    "\n",
    "# Show column names to identify retrieval vs request features\n",
    "print(\"\\nAll training columns:\")\n",
    "for i, col in enumerate(train_df.columns):\n",
    "    print(f\"{i:2d}: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ef899",
   "metadata": {},
   "source": [
    "## Identify Retrieval vs Request Features\n",
    "\n",
    "Let's categorize features based on whether they contain `_at_request` or `_at_retrieval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features\n",
    "request_features = [col for col in train_df.columns if '_at_request' in col]\n",
    "retrieval_features = [col for col in train_df.columns if '_at_retrieval' in col]\n",
    "other_features = [col for col in train_df.columns if '_at_request' not in col and '_at_retrieval' not in col]\n",
    "\n",
    "print(\"=== REQUEST FEATURES (pre-outcome) ===\")\n",
    "for f in request_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\n=== RETRIEVAL FEATURES (potential leakage) ===\")\n",
    "for f in retrieval_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\n=== OTHER FEATURES ===\")\n",
    "for f in other_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nCounts:\")\n",
    "print(f\"  Request features: {len(request_features)}\")\n",
    "print(f\"  Retrieval features: {len(retrieval_features)}\")\n",
    "print(f\"  Other features: {len(other_features)}\")\n",
    "\n",
    "# Check which features exist in test data\n",
    "print(f\"\\n=== FEATURE AVAILABILITY IN TEST DATA ===\")\n",
    "all_features = request_features + retrieval_features + other_features\n",
    "test_columns = set(test_df.columns)\n",
    "\n",
    "available_in_test = []\n",
    "not_in_test = []\n",
    "\n",
    "for f in all_features:\n",
    "    if f in test_columns:\n",
    "        available_in_test.append(f)\n",
    "    else:\n",
    "        not_in_test.append(f)\n",
    "\n",
    "print(f\"Features available in test: {len(available_in_test)}\")\n",
    "print(f\"Features NOT in test: {len(not_in_test)}\")\n",
    "\n",
    "if not_in_test:\n",
    "    print(\"\\nMissing from test:\")\n",
    "    for f in not_in_test:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9ee47",
   "metadata": {},
   "source": [
    "## Analyze Feature Distributions\n",
    "\n",
    "Check if retrieval features have different distributions than request features, which could indicate they contain different information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distributions for numeric features\n",
    "def analyze_feature_distributions():\n",
    "    results = []\n",
    "    \n",
    "    # Get numeric features from both categories\n",
    "    numeric_request = []\n",
    "    numeric_retrieval = []\n",
    "    \n",
    "    for f in request_features:\n",
    "        if f in train_df.columns and pd.api.types.is_numeric_dtype(train_df[f]):\n",
    "            numeric_request.append(f)\n",
    "    \n",
    "    for f in retrieval_features:\n",
    "        if f in train_df.columns and pd.api.types.is_numeric_dtype(train_df[f]):\n",
    "            numeric_retrieval.append(f)\n",
    "    \n",
    "    print(\"=== COMPARING REQUEST vs RETRIEVAL FEATURES ===\\n\")\n",
    "    \n",
    "    # Compare features that have both _at_request and _at_retrieval versions\n",
    "    for req_feat in numeric_request:\n",
    "        # Find corresponding retrieval feature\n",
    "        base_name = req_feat.replace('_at_request', '')\n",
    "        ret_feat = base_name + '_at_retrieval'\n",
    "        \n",
    "        if ret_feat in numeric_retrieval:\n",
    "            # Calculate correlation with target for both\n",
    "            req_corr = train_df[req_feat].corr(train_df['requester_received_pizza'])\n",
    "            ret_corr = train_df[ret_feat].corr(train_df['requester_received_pizza'])\n",
    "            \n",
    "            # Calculate difference in values\n",
    "            value_diff = (train_df[ret_feat] - train_df[req_feat]).abs().mean()\n",
    "            \n",
    "            results.append({\n",
    "                'feature_base': base_name,\n",
    "                'request_corr': req_corr,\n",
    "                'retrieval_corr': ret_corr,\n",
    "                'value_diff_mean': value_diff,\n",
    "                'request_missing_pct': (train_df[req_feat].isna().sum() / len(train_df)) * 100,\n",
    "                'retrieval_missing_pct': (train_df[ret_feat].isna().sum() / len(train_df)) * 100\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('retrieval_corr', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Features with both request and retrieval versions:\")\n",
    "    print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "results_df = analyze_feature_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9853139",
   "metadata": {},
   "source": [
    "## Test the Leakage Hypothesis\n",
    "\n",
    "Run two experiments:\n",
    "1. Only request features (safe, pre-outcome)\n",
    "2. All features including retrieval\n",
    "\n",
    "If request-only performs better, retrieval features are leaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, feature_list):\n",
    "    \"\"\"Prepare feature matrix from list of features\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for f in feature_list:\n",
    "        if f in df.columns:\n",
    "            # Fill missing values\n",
    "            if pd.api.types.is_numeric_dtype(df[f]):\n",
    "                features[f] = df[f].fillna(0)\n",
    "            else:\n",
    "                # For categorical, use label encoding\n",
    "                features[f] = pd.Categorical(df[f]).codes\n",
    "                features[f] = features[f].fillna(-1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def run_cv_experiment(feature_set_name, feature_list):\n",
    "    \"\"\"Run 5-fold CV with given features\"\"\"\n",
    "    print(f\"\\n=== {feature_set_name} ===\")\n",
    "    print(f\"Features: {len(feature_list)}\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X = prepare_features(train_df, feature_list)\n",
    "    y = train_df['requester_received_pizza']\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Missing values: {X.isna().sum().sum()}\")\n",
    "    \n",
    "    # 5-fold stratified CV\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    scores = []\n",
    "    predictions = np.zeros(len(train_df))\n",
    "    \n",
    "    feature_importance_list = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train LightGBM\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=63,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        predictions[val_idx] = val_pred\n",
    "        \n",
    "        # Score\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_,\n",
    "            'fold': fold\n",
    "        })\n",
    "        feature_importance_list.append(importance)\n",
    "        \n",
    "        print(f\"  Fold {fold}: AUC = {score:.4f}\")\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = roc_auc_score(y, predictions)\n",
    "    print(f\"  Overall CV AUC: {overall_score:.4f} ± {np.std(scores):.4f}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = pd.concat(feature_importance_list)\n",
    "    feature_importance = feature_importance.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\n  Top 5 features:\")\n",
    "    for i, (feat, imp) in enumerate(feature_importance.head().items()):\n",
    "        print(f\"    {i+1}. {feat}: {imp:.1f}\")\n",
    "    \n",
    "    # Prediction distribution\n",
    "    pred_range = predictions.max() - predictions.min()\n",
    "    print(f\"\\n  Prediction range: {predictions.min():.3f} - {predictions.max():.3f} (range: {pred_range:.3f})\")\n",
    "    \n",
    "    return {\n",
    "        'score': overall_score,\n",
    "        'std': np.std(scores),\n",
    "        'predictions': predictions,\n",
    "        'feature_importance': feature_importance\n",
    "    }\n",
    "\n",
    "# Define feature sets\n",
    "all_safe_features = request_features + other_features\n",
    "all_features = request_features + retrieval_features + other_features\n",
    "\n",
    "print(\"Preparing to test leakage hypothesis...\")\n",
    "print(f\"Request features: {len(request_features)}\")\n",
    "print(f\"Retrieval features: {len(retrieval_features)}\")\n",
    "print(f\"Other features: {len(other_features)}\")\n",
    "\n",
    "# Run experiments\n",
    "request_only_results = run_cv_experiment(\"REQUEST-ONLY FEATURES (SAFE)\", all_safe_features)\n",
    "all_features_results = run_cv_experiment(\"ALL FEATURES (WITH RETRIEVAL)\", all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a582f",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Compare the two experiments to determine if retrieval features are leaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94462428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEAKAGE ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nRequest-only features: {request_only_results['score']:.4f} ± {request_only_results['std']:.4f}\")\n",
    "print(f\"All features (with retrieval): {all_features_results['score']:.4f} ± {all_features_results['std']:.4f}\")\n",
    "print(f\"Difference: {all_features_results['score'] - request_only_results['score']:.4f}\")\n",
    "\n",
    "if all_features_results['score'] > request_only_results['score']:\n",
    "    print(\"\\n✓ RETRIEVAL FEATURES ARE SAFE (improved performance)\")\n",
    "    print(\"  Keep all features including retrieval.\")\n",
    "elif all_features_results['score'] < request_only_results['score']:\n",
    "    print(\"\\n✗ RETRIEVAL FEATURES ARE LEAKING (hurt performance)\")\n",
    "    print(\"  Remove all _at_retrieval features immediately.\")\n",
    "else:\n",
    "    print(\"\\n? RETRIEVAL FEATURES ARE NEUTRAL (no impact)\")\n",
    "    print(\"  Can keep or remove - minimal impact.\")\n",
    "\n",
    "# Check prediction distributions\n",
    "print(f\"\\nRequest-only prediction range: {request_only_results['predictions'].min():.3f} - {request_only_results['predictions'].max():.3f}\")\n",
    "print(f\"All features prediction range: {all_features_results['predictions'].min():.3f} - {all_features_results['predictions'].max():.3f}\")\n",
    "\n",
    "# Save findings\n",
    "finding = f\"Retrieval feature leakage test: Request-only={request_only_results['score']:.4f}, All={all_features_results['score']:.4f}. \"\n",
    "if all_features_results['score'] < request_only_results['score']:\n",
    "    finding += \"RETRIEVAL FEATURES ARE LEAKING - REMOVE THEM.\"\n",
    "else:\n",
    "    finding += \"Retrieval features are safe to use.\"\n",
    "\n",
    "print(f\"\\n{finding}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
