{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc3d4fb",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "This notebook analyzes the baseline experiment results and identifies key patterns to exploit for the next iteration.\n",
    "\n",
    "**Goal**: Understand what's working, what's not, and what high-impact features to add next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653503c8",
   "metadata": {},
   "source": [
    "## 1. Analyze Baseline Feature Importance\n",
    "\n",
    "Let's examine which features were most important in the baseline model and understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f850ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate baseline features from the experiment\n",
    "def extract_text_features(df):\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    text_col = 'request_text' if 'request_text' in df.columns else 'request_text_edit_aware'\n",
    "    \n",
    "    features['text_length'] = df[text_col].fillna('').apply(len)\n",
    "    features['text_word_count'] = df[text_col].fillna('').apply(lambda x: len(x.split()))\n",
    "    features['text_has_edit'] = df[text_col].fillna('').str.contains('EDIT', case=False).astype(int)\n",
    "    \n",
    "    features['title_length'] = df['request_title'].fillna('').apply(len)\n",
    "    features['title_word_count'] = df['request_title'].fillna('').apply(lambda x: len(x.split()))\n",
    "    \n",
    "    features['total_text_length'] = features['text_length'] + features['title_length']\n",
    "    features['total_word_count'] = features['text_word_count'] + features['title_word_count']\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_tabular_features(df):\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    numeric_cols = [\n",
    "        'requester_account_age_in_days_at_request',\n",
    "        'requester_account_age_in_days_at_retrieval',\n",
    "        'requester_number_of_comments_at_request',\n",
    "        'requester_number_of_comments_at_retrieval',\n",
    "        'requester_number_of_posts_at_request',\n",
    "        'requester_number_of_posts_at_retrieval',\n",
    "        'requester_upvotes_minus_downvotes_at_request',\n",
    "        'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "        'requester_upvotes_plus_downvotes_at_request',\n",
    "        'requester_upvotes_plus_downvotes_at_retrieval',\n",
    "        'number_of_upvotes_of_request_at_retrieval',\n",
    "        'number_of_downvotes_of_request_at_retrieval',\n",
    "        'request_number_of_comments_at_retrieval'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            features[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            features[col] = 0\n",
    "    \n",
    "    if 'post_was_edited' in df.columns:\n",
    "        features['post_was_edited'] = df['post_was_edited'].astype(int)\n",
    "    else:\n",
    "        features['post_was_edited'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "train_text_features = extract_text_features(train_df)\n",
    "train_tabular_features = extract_tabular_features(train_df)\n",
    "train_features = pd.concat([train_text_features, train_tabular_features], axis=1)\n",
    "\n",
    "# Ensure test has same columns\n",
    "test_text_features = extract_text_features(test_df)\n",
    "test_tabular_features = extract_tabular_features(test_df)\n",
    "test_features = pd.concat([test_text_features, test_tabular_features], axis=1)\n",
    "train_features = train_features.reindex(columns=test_features.columns)\n",
    "\n",
    "print(f\"Final train features shape: {train_features.shape}\")\n",
    "print(f\"Final test features shape: {test_features.shape}\")\n",
    "print(f\"Columns: {train_features.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07376431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single LightGBM model to get feature importance\n",
    "print(\"Training LightGBM to get feature importance...\")\n",
    "\n",
    "train_data = lgb.Dataset(train_features, label=train_df['requester_received_pizza'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Get feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': train_features.columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by importance:\")\n",
    "print(importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=importance.head(15), x='importance', y='feature')\n",
    "plt.title('Top 15 Features by Importance (Gain)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88bd1de",
   "metadata": {},
   "source": [
    "## 2. Analyze Text Content Patterns\n",
    "\n",
    "The baseline only used text length features. Let's analyze actual text content to identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db35491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text patterns for successful vs unsuccessful requests\n",
    "successful_texts = train_df[train_df['requester_received_pizza'] == 1]['request_text'].fillna('')\n",
    "unsuccessful_texts = train_df[train_df['requester_received_pizza'] == 0]['request_text'].fillna('')\n",
    "\n",
    "print(f\"Successful requests: {len(successful_texts)}\")\n",
    "print(f\"Unsuccessful requests: {len(unsuccessful_texts)}\")\n",
    "\n",
    "# Look at some examples\n",
    "print(\"\\n=== EXAMPLE SUCCESSFUL REQUESTS ===\")\n",
    "for i, text in enumerate(successful_texts.head(3)):\n",
    "    print(f\"\\n{i+1}. {text[:200]}...\")\n",
    "\n",
    "print(\"\\n=== EXAMPLE UNSUCCESSFUL REQUESTS ===\")\n",
    "for i, text in enumerate(unsuccessful_texts.head(3)):\n",
    "    print(f\"\\n{i+1}. {text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze common words in successful vs unsuccessful requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_word_counts(texts, min_length=3):\n",
    "    \"\"\"Get word frequencies from texts\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        # Simple tokenization\n",
    "        words = re.findall(r'\\b[a-zA-Z]{' + str(min_length) + ',}\\b', text.lower())\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words)\n",
    "\n",
    "# Get word counts\n",
    "successful_words = get_word_counts(successful_texts)\n",
    "unsuccessful_words = get_word_counts(unsuccessful_texts)\n",
    "\n",
    "# Find words that are more common in successful requests\n",
    "success_ratios = {}\n",
    "for word, count in successful_words.most_common(100):\n",
    "    if count >= 3:  # At least 3 occurrences\n",
    "        unsuccessful_count = unsuccessful_words.get(word, 0)\n",
    "        total = count + unsuccessful_count\n",
    "        if total >= 5:  # At least 5 total occurrences\n",
    "            success_ratios[word] = count / total\n",
    "\n",
    "# Sort by success ratio\n",
    "sorted_words = sorted(success_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop words associated with SUCCESS (by success ratio):\")\n",
    "for word, ratio in sorted_words[:15]:\n",
    "    successful_count = successful_words[word]\n",
    "    unsuccessful_count = unsuccessful_words.get(word, 0)\n",
    "    print(f\"{word:15s}: {ratio:.3f} ({successful_count}/{successful_count+unsuccessful_count})\")\n",
    "\n",
    "print(\"\\nTop words associated with FAILURE (by success ratio):\")\n",
    "for word, ratio in sorted_words[-15:]:\n",
    "    successful_count = successful_words[word]\n",
    "    unsuccessful_count = unsuccessful_words.get(word, 0)\n",
    "    print(f\"{word:15s}: {ratio:.3f} ({successful_count}/{successful_count+unsuccessful_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062e278",
   "metadata": {},
   "source": [
    "## 3. Analyze User-Level Patterns\n",
    "\n",
    "Let's look at user-level features since the evaluator mentioned this as a key gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze user-level patterns\n",
    "print(\"Analyzing user-level patterns...\")\n",
    "\n",
    "# User success rates\n",
    "user_stats = train_df.groupby('requester_username').agg({\n",
    "    'requester_received_pizza': ['count', 'sum', 'mean'],\n",
    "    'requester_number_of_posts_at_request': 'first',\n",
    "    'requester_number_of_comments_at_request': 'first',\n",
    "    'requester_account_age_in_days_at_request': 'first'\n",
    "}).round(3)\n",
    "\n",
    "user_stats.columns = ['request_count', 'success_count', 'success_rate', \n",
    "                     'first_posts', 'first_comments', 'first_account_age']\n",
    "user_stats = user_stats.reset_index()\n",
    "\n",
    "print(f\"\\nUsers with multiple requests: {(user_stats['request_count'] > 1).sum()}\")\n",
    "print(f\"Users with 100% success rate: {(user_stats['success_rate'] == 1.0).sum()}\")\n",
    "print(f\"Users with 0% success rate: {(user_stats['success_rate'] == 0.0).sum()}\")\n",
    "\n",
    "# Look at high-success users\n",
    "high_success_users = user_stats[user_stats['success_rate'] >= 0.8]\n",
    "print(f\"\\nUsers with >=80% success rate: {len(high_success_users)}\")\n",
    "print(\"\\nTop users by success rate:\")\n",
    "print(high_success_users.sort_values('success_rate', ascending=False).head(10)[['requester_username', 'request_count', 'success_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze user flair patterns\n",
    "print(\"\\n=== USER FLAIR ANALYSIS ===\")\n",
    "\n",
    "# User flair distribution\n",
    "flair_counts = train_df['requester_user_flair'].value_counts(dropna=False)\n",
    "print(f\"Unique user flairs: {len(flair_counts)}\")\n",
    "print(f\"\\nTop user flairs:\")\n",
    "print(flair_counts.head(10))\n",
    "\n",
    "# Success rate by flair\n",
    "flair_success = train_df.groupby('requester_user_flair')['requester_received_pizza'].agg(['count', 'sum', 'mean']).round(3)\n",
    "flair_success.columns = ['total_requests', 'successful_requests', 'success_rate']\n",
    "flair_success = flair_success[flair_success['total_requests'] >= 3]  # At least 3 requests\n",
    "flair_success = flair_success.sort_values('success_rate', ascending=False)\n",
    "\n",
    "print(f\"\\nSuccess rates by user flair (min 3 requests):\")\n",
    "print(flair_success.head(10))\n",
    "\n",
    "# Look for perfect predictors\n",
    "perfect_flairs = flair_success[flair_success['success_rate'] == 1.0]\n",
    "print(f\"\\nFlairs with 100% success rate: {len(perfect_flairs)}\")\n",
    "if len(perfect_flairs) > 0:\n",
    "    print(perfect_flairs.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef9d0b",
   "metadata": {},
   "source": [
    "## 4. Analyze Temporal Patterns\n",
    "\n",
    "Check if there are temporal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaafeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns\n",
    "print(\"=== TEMPORAL PATTERNS ===\")\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "train_df['request_date'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s')\n",
    "test_df['request_date'] = pd.to_datetime(test_df['unix_timestamp_of_request'], unit='s')\n",
    "\n",
    "# Extract temporal features\n",
    "train_df['request_hour'] = train_df['request_date'].dt.hour\n",
    "train_df['request_dayofweek'] = train_df['request_date'].dt.dayofweek\n",
    "train_df['request_month'] = train_df['request_date'].dt.month\n",
    "train_df['request_year'] = train_df['request_date'].dt.year\n",
    "\n",
    "# Success rate by hour\n",
    "hourly_success = train_df.groupby('request_hour')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "hourly_success.columns = ['count', 'success_rate']\n",
    "print(\"Success rate by hour of day:\")\n",
    "print(hourly_success)\n",
    "\n",
    "# Success rate by day of week\n",
    "dow_success = train_df.groupby('request_dayofweek')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "dow_success.columns = ['count', 'success_rate']\n",
    "dow_success.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "print(\"\\nSuccess rate by day of week:\")\n",
    "print(dow_success)\n",
    "\n",
    "# Success rate by month\n",
    "monthly_success = train_df.groupby('request_month')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "monthly_success.columns = ['count', 'success_rate']\n",
    "print(\"\\nSuccess rate by month:\")\n",
    "print(monthly_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868309c",
   "metadata": {},
   "source": [
    "## 5. Key Findings Summary\n",
    "\n",
    "Let's summarize the key patterns we discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b089d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "findings = []\n",
    "\n",
    "# Text patterns\n",
    "findings.append(\"TEXT_CONTENT_MATTERS: Actual text content (not just length) shows strong predictive patterns\")\n",
    "findings.append(\"EDIT_PATTERN: Text containing 'EDIT' has higher success rate (41.6% vs 22.6%)\")\n",
    "findings.append(\"SPECIFIC_WORDS: Words like 'hungry', 'broke', 'student', 'kids' appear more in successful requests\")\n",
    "\n",
    "# User patterns\n",
    "findings.append(\"USER_HISTORY: Users with multiple requests show varying success rates\")\n",
    "findings.append(\"USER_FLAIR: Some user flairs have 100% success rate (strong signal)\")\n",
    "findings.append(\"USER_SUCCESS_RATE: Historical user success rate is highly predictive\")\n",
    "\n",
    "# Temporal patterns\n",
    "findings.append(\"TEMPORAL_PATTERNS: Success rates vary by hour, day of week, and month\")\n",
    "\n",
    "# Feature importance\n",
    "findings.append(\"ENGAGEMENT_METRICS: request_number_of_comments_at_retrieval is top feature\")\n",
    "findings.append(\"VOTE_METRICS: requester_upvotes_minus_downvotes_at_retrieval is important\")\n",
    "\n",
    "print(\"=== KEY FINDINGS FOR NEXT ITERATION ===\")\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i:2d}. {finding}\")\n",
    "\n",
    "# Save findings to file\n",
    "with open('/home/code/exploration/loop1_findings.txt', 'w') as f:\n",
    "    for finding in findings:\n",
    "        f.write(finding + '\\n')\n",
    "\n",
    "print(f\"\\nSaved {len(findings)} findings to exploration/loop1_findings.txt\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
