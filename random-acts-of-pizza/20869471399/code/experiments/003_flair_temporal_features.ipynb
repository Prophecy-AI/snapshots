{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74b6ddd",
   "metadata": {},
   "source": [
    "# Experiment 003: Safe Flair Handling + Temporal Features\n",
    "\n",
    "This experiment focuses on the highest priority from the strategy: SAFE FLAIR HANDLING.\n",
    "\n",
    "**Strategy:**\n",
    "- Target encode user flair with aggressive smoothing to prevent overfitting\n",
    "- Add temporal features (hour, day of week, month) with cyclical encoding\n",
    "- Use Reddit-aware text preprocessing for keyword features\n",
    "- Combine with existing tabular features\n",
    "- Use 5-fold stratified CV\n",
    "- LightGBM for training\n",
    "\n",
    "**Expected improvements:**\n",
    "- Capture the extremely predictive flair signal safely\n",
    "- Add temporal patterns that exist in data\n",
    "- Better prediction distribution and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62107a37",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Training columns: {train_df.shape[1]}\")\n",
    "print(f\"Test columns: {test_df.shape[1]}\")\n",
    "\n",
    "# Check flair distribution\n",
    "print(\"\\nFlair distribution in training data:\")\n",
    "print(train_df['requester_user_flair'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac47239",
   "metadata": {},
   "source": [
    "## Reddit-Aware Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22233fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_aware_preprocess(text):\n",
    "    \"\"\"Reddit-aware text preprocessing\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Handle Reddit-specific patterns\n",
    "    # User mentions: u/username -> USER_MENTION\n",
    "    text = re.sub(r'u/\\w+', 'USER_MENTION', text)\n",
    "    \n",
    "    # Subreddit mentions: r/subreddit -> SUBREDDIT_MENTION\n",
    "    text = re.sub(r'r/\\w+', 'SUBREDDIT_MENTION', text)\n",
    "    \n",
    "    # URLs -> URL\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'URL', text)\n",
    "    \n",
    "    # Markdown syntax\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', text)  # **bold**\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)      # *italic*\n",
    "    text = re.sub(r'~~(.+?)~~', r'\\1', text)       # ~~strikethrough~~\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine request text and title\n",
    "text_col = 'request_text' if 'request_text' in train_df.columns else 'request_text_edit_aware'\n",
    "\n",
    "train_df['combined_text'] = train_df[text_col].fillna('') + ' ' + train_df['request_title'].fillna('')\n",
    "test_df['combined_text'] = test_df['request_text_edit_aware'].fillna('') + ' ' + test_df['request_title'].fillna('')\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['combined_text_clean'] = train_df['combined_text'].apply(reddit_aware_preprocess)\n",
    "test_df['combined_text_clean'] = test_df['combined_text'].apply(reddit_aware_preprocess)\n",
    "\n",
    "print(\"Reddit-aware text preprocessing completed\")\n",
    "print(f\"Sample cleaned text: {train_df['combined_text_clean'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb875e9",
   "metadata": {},
   "source": [
    "## Extract High-Impact Keyword Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword_features(df, text_col):\n",
    "    \"\"\"Extract binary features for high-impact keywords\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Keywords from EDA that indicate need/urgency\n",
    "    need_keywords = ['hungry', 'broke', 'starving', 'desperate', 'struggling']\n",
    "    student_keywords = ['student', 'college', 'university', 'tuition', 'loan']\n",
    "    family_keywords = ['family', 'kids', 'children', 'baby', 'mother', 'father', 'parent']\n",
    "    job_keywords = ['job', 'work', 'unemployed', 'laid off', 'fired', 'paycheck']\n",
    "    gratitude_keywords = ['thank', 'appreciate', 'grateful', 'bless', 'kind']\n",
    "    edit_keywords = ['edit', 'update']\n",
    "    \n",
    "    # Count occurrences of each keyword group\n",
    "    for keyword_list, prefix in [\n",
    "        (need_keywords, 'need'),\n",
    "        (student_keywords, 'student'),\n",
    "        (family_keywords, 'family'),\n",
    "        (job_keywords, 'job'),\n",
    "        (gratitude_keywords, 'gratitude'),\n",
    "        (edit_keywords, 'edit')\n",
    "    ]:\n",
    "        features[f'keyword_{prefix}_count'] = df[text_col].apply(\n",
    "            lambda x: sum(1 for word in keyword_list if word in str(x).lower())\n",
    "        )\n",
    "    \n",
    "    # Binary features for presence of any keyword in each group\n",
    "    for keyword_list, prefix in [\n",
    "        (need_keywords, 'need'),\n",
    "        (student_keywords, 'student'),\n",
    "        (family_keywords, 'family'),\n",
    "        (job_keywords, 'job'),\n",
    "        (gratitude_keywords, 'gratitude'),\n",
    "        (edit_keywords, 'edit')\n",
    "    ]:\n",
    "        features[f'has_{prefix}_keyword'] = features[f'keyword_{prefix}_count'] > 0\n",
    "    \n",
    "    # Text length features\n",
    "    features['text_length'] = df[text_col].str.len()\n",
    "    features['word_count'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract keyword features\n",
    "train_keyword_features = extract_keyword_features(train_df, 'combined_text_clean')\n",
    "test_keyword_features = extract_keyword_features(test_df, 'combined_text_clean')\n",
    "\n",
    "print(f\"Keyword features shape: {train_keyword_features.shape}\")\n",
    "print(\"Sample keyword features:\")\n",
    "print(train_keyword_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a3d24",
   "metadata": {},
   "source": [
    "## Safe Flair Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_target_encode_flair(train_df, test_df, target_col, flair_col='requester_user_flair'):\n",
    "    \"\"\"Safely target encode user flair with aggressive smoothing\"\"\"\n",
    "    \n",
    "    # Check if flair column exists in train data\n",
    "    if flair_col not in train_df.columns:\n",
    "        print(f\"WARNING: {flair_col} not found in train data. Creating dummy features.\")\n",
    "        \n",
    "        # Create dummy features with neutral values\n",
    "        train_encoded = pd.DataFrame(index=train_df.index)\n",
    "        test_encoded = pd.DataFrame(index=test_df.index)\n",
    "        \n",
    "        # Use global mean as target encoding (neutral)\n",
    "        global_mean = train_df[target_col].mean()\n",
    "        train_encoded['flair_target_encoded'] = global_mean\n",
    "        test_encoded['flair_target_encoded'] = global_mean\n",
    "        \n",
    "        # No perfect flairs\n",
    "        train_encoded['has_perfect_flair'] = 0\n",
    "        test_encoded['has_perfect_flair'] = 0\n",
    "        \n",
    "        return train_encoded, test_encoded\n",
    "    \n",
    "    # Check if flair column exists in test data\n",
    "    if flair_col not in test_df.columns:\n",
    "        print(f\"WARNING: {flair_col} not found in test data. Creating dummy features.\")\n",
    "        \n",
    "        # Create dummy features with neutral values\n",
    "        train_encoded = pd.DataFrame(index=train_df.index)\n",
    "        test_encoded = pd.DataFrame(index=test_df.index)\n",
    "        \n",
    "        # Use global mean as target encoding (neutral)\n",
    "        global_mean = train_df[target_col].mean()\n",
    "        train_encoded['flair_target_encoded'] = global_mean\n",
    "        test_encoded['flair_target_encoded'] = global_mean\n",
    "        \n",
    "        # No perfect flairs in test data\n",
    "        train_encoded['has_perfect_flair'] = 0\n",
    "        test_encoded['has_perfect_flair'] = 0\n",
    "        \n",
    "        return train_encoded, test_encoded\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[[flair_col]].copy()\n",
    "    X_test = test_df[[flair_col]].copy()\n",
    "    y_train = train_df[target_col]\n",
    "    \n",
    "    # Use TargetEncoder with high smoothing to prevent overfitting\n",
    "    # smoothing=10 means we need 10 samples to trust the category mean\n",
    "    encoder = TargetEncoder(smoothing=10.0, min_samples_leaf=5)\n",
    "    \n",
    "    # Fit and transform\n",
    "    train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "    test_encoded = encoder.transform(X_test)\n",
    "    \n",
    "    # Rename columns\n",
    "    train_encoded.columns = ['flair_target_encoded']\n",
    "    test_encoded.columns = ['flair_target_encoded']\n",
    "    \n",
    "    # Also create a binary feature for perfect flairs (PIF and shroom)\n",
    "    perfect_flairs = ['shroom', 'PIF']\n",
    "    train_encoded['has_perfect_flair'] = train_df[flair_col].isin(perfect_flairs).astype(int)\n",
    "    test_encoded['has_perfect_flair'] = test_df[flair_col].isin(perfect_flairs).astype(int)\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Target encode flair\n",
    "train_flair_features, test_flair_features = safe_target_encode_flair(\n",
    "    train_df, test_df, 'requester_received_pizza'\n",
    ")\n",
    "\n",
    "print(\"Flair encoding completed\")\n",
    "print(\"Target encoded flair distribution:\")\n",
    "print(train_flair_features['flair_target_encoded'].describe())\n",
    "print(f\"\\nPerfect flair count in train: {train_flair_features['has_perfect_flair'].sum()}\")\n",
    "print(f\"Perfect flair count in test: {test_flair_features['has_perfect_flair'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7235a5b",
   "metadata": {},
   "source": [
    "## Temporal Features with Cyclical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb354f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df):\n",
    "    \"\"\"Extract cyclical temporal features from Unix timestamps\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Convert Unix timestamp to datetime\n",
    "    # Use request timestamp if available, otherwise use a retrieval timestamp\n",
    "    if 'unix_timestamp_of_request' in df.columns:\n",
    "        timestamp_col = 'unix_timestamp_of_request'\n",
    "    elif 'unix_timestamp_of_request_utc' in df.columns:\n",
    "        timestamp_col = 'unix_timestamp_of_request_utc'\n",
    "    else:\n",
    "        # If no request timestamp, skip temporal features\n",
    "        print(\"No request timestamp found, skipping temporal features\")\n",
    "        return features\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df[timestamp_col], unit='s')\n",
    "    \n",
    "    # Extract time components\n",
    "    features['hour'] = df['datetime'].dt.hour\n",
    "    features['day_of_week'] = df['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    features['day_of_month'] = df['datetime'].dt.day\n",
    "    features['month'] = df['datetime'].dt.month\n",
    "    \n",
    "    # Cyclical encoding for hour (24 hours)\n",
    "    features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)\n",
    "    features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)\n",
    "    \n",
    "    # Cyclical encoding for day of week (7 days)\n",
    "    features['day_of_week_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)\n",
    "    features['day_of_week_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)\n",
    "    \n",
    "    # Cyclical encoding for month (12 months)\n",
    "    features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)\n",
    "    features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)\n",
    "    \n",
    "    # Drop raw temporal features (keep only cyclical)\n",
    "    features = features.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract temporal features\n",
    "train_temporal_features = extract_temporal_features(train_df)\n",
    "test_temporal_features = extract_temporal_features(test_df)\n",
    "\n",
    "print(f\"Temporal features shape: {train_temporal_features.shape}\")\n",
    "if len(train_temporal_features.columns) > 0:\n",
    "    print(\"Sample temporal features:\")\n",
    "    print(train_temporal_features.head())\n",
    "else:\n",
    "    print(\"No temporal features extracted (no timestamp column found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0e1e3",
   "metadata": {},
   "source": [
    "## Extract Tabular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tabular_features(df):\n",
    "    \"\"\"Extract and preprocess tabular features\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Numeric features that might exist in the dataset\n",
    "    numeric_cols = [\n",
    "        'requester_account_age_in_days_at_request',\n",
    "        'requester_account_age_in_days_at_retrieval',\n",
    "        'requester_number_of_comments_at_request',\n",
    "        'requester_number_of_comments_at_retrieval',\n",
    "        'requester_number_of_posts_at_request',\n",
    "        'requester_number_of_posts_at_retrieval',\n",
    "        'requester_upvotes_minus_downvotes_at_request',\n",
    "        'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "        'requester_upvotes_plus_downvotes_at_request',\n",
    "        'requester_upvotes_plus_downvotes_at_retrieval',\n",
    "        'number_of_upvotes_of_request_at_retrieval',\n",
    "        'number_of_downvotes_of_request_at_retrieval',\n",
    "        'request_number_of_comments_at_retrieval'\n",
    "    ]\n",
    "    \n",
    "    # Add numeric features if they exist\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            features[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            features[col] = 0  # Add column with zeros if it doesn't exist\n",
    "    \n",
    "    # Binary features\n",
    "    if 'post_was_edited' in df.columns:\n",
    "        features['post_was_edited'] = df['post_was_edited'].astype(int)\n",
    "    else:\n",
    "        features['post_was_edited'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract tabular features\n",
    "train_tabular_features = extract_tabular_features(train_df)\n",
    "test_tabular_features = extract_tabular_features(test_df)\n",
    "\n",
    "print(f\"Tabular features shape: {train_tabular_features.shape}\")\n",
    "print(\"Sample tabular features:\")\n",
    "print(train_tabular_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d22eb",
   "metadata": {},
   "source": [
    "## Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfde495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all feature sets\n",
    "feature_sets = [\n",
    "    train_keyword_features, train_flair_features, train_temporal_features, train_tabular_features\n",
    "]\n",
    "test_sets = [\n",
    "    test_keyword_features, test_flair_features, test_temporal_features, test_tabular_features\n",
    "]\n",
    "\n",
    "train_features = pd.concat(feature_sets, axis=1)\n",
    "test_features = pd.concat(test_sets, axis=1)\n",
    "\n",
    "# Ensure both have same columns and order\n",
    "common_cols = [col for col in train_features.columns if col in test_features.columns]\n",
    "train_features = train_features[common_cols]\n",
    "test_features = test_features[common_cols]\n",
    "\n",
    "print(f\"Final train features shape: {train_features.shape}\")\n",
    "print(f\"Final test features shape: {test_features.shape}\")\n",
    "print(f\"Columns match: {list(train_features.columns) == list(test_features.columns)}\")\n",
    "\n",
    "# Prepare target\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "print(f\"\\nTarget distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921644d0",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f283e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any remaining NaN values\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = test_features.fillna(0)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(train_features))\n",
    "test_predictions = np.zeros(len(test_features))\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold stratified cross-validation...\")\n",
    "\n",
    "fold = 1\n",
    "for train_idx, valid_idx in skf.split(train_features, y):\n",
    "    print(f\"\\nFold {fold}/{n_folds}\")\n",
    "    \n",
    "    X_train, X_valid = train_features.iloc[train_idx], train_features.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "    \n",
    "    # Define parameters - slightly tuned for better confidence\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 63,  # Increased from 31 for more capacity\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'min_child_samples': 20  # Added to prevent overfitting\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1500,  # Increased for more training\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    oof_predictions[valid_idx] = valid_pred\n",
    "    \n",
    "    # Calculate AUC for this fold\n",
    "    fold_auc = roc_auc_score(y_valid, valid_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    print(f\"Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_auc:.4f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0acf7c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the last fold model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_features.columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Check flair feature importance\n",
    "flair_importance = feature_importance[feature_importance['feature'].str.contains('flair')]\n",
    "print(f\"\\nFlair feature importance:\")\n",
    "print(flair_importance)\n",
    "\n",
    "# Check keyword feature importance\n",
    "keyword_importance = feature_importance[feature_importance['feature'].str.contains('keyword|has_')]\n",
    "print(f\"\\nTop keyword features:\")\n",
    "print(keyword_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb5c04",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ea911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(\"Submission shape:\", submission.shape)\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_003_flair_temporal.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format\n",
    "sample_sub = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_sub.columns)}\")\n",
    "print(f\"Request IDs match: {set(submission['request_id']) == set(sample_sub['request_id'])}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
