{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74b6ddd",
   "metadata": {},
   "source": [
    "# Experiment 003: Safe Flair Handling + Temporal Features\n",
    "\n",
    "This experiment focuses on the highest priority from the strategy: SAFE FLAIR HANDLING.\n",
    "\n",
    "**Strategy:**\n",
    "- Target encode user flair with aggressive smoothing to prevent overfitting\n",
    "- Add temporal features (hour, day of week, month) with cyclical encoding\n",
    "- Use Reddit-aware text preprocessing for keyword features\n",
    "- Combine with existing tabular features\n",
    "- Use 5-fold stratified CV\n",
    "- LightGBM for training\n",
    "\n",
    "**Expected improvements:**\n",
    "- Capture the extremely predictive flair signal safely\n",
    "- Add temporal patterns that exist in data\n",
    "- Better prediction distribution and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424dd159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:20:58.230889Z",
     "iopub.status.busy": "2026-01-10T03:20:58.230661Z",
     "iopub.status.idle": "2026-01-10T03:20:59.163976Z",
     "shell.execute_reply": "2026-01-10T03:20:59.163381Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62107a37",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc809255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:20:59.165718Z",
     "iopub.status.busy": "2026-01-10T03:20:59.165268Z",
     "iopub.status.idle": "2026-01-10T03:20:59.229261Z",
     "shell.execute_reply": "2026-01-10T03:20:59.228836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "Training columns: 32\n",
      "Test columns: 17\n",
      "\n",
      "Flair distribution in training data:\n",
      "requester_user_flair\n",
      "shroom    677\n",
      "PIF        38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Training columns: {train_df.shape[1]}\")\n",
    "print(f\"Test columns: {test_df.shape[1]}\")\n",
    "\n",
    "# Check flair distribution\n",
    "print(\"\\nFlair distribution in training data:\")\n",
    "print(train_df['requester_user_flair'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac47239",
   "metadata": {},
   "source": [
    "## Reddit-Aware Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22233fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:20:59.230247Z",
     "iopub.status.busy": "2026-01-10T03:20:59.230138Z",
     "iopub.status.idle": "2026-01-10T03:20:59.372865Z",
     "shell.execute_reply": "2026-01-10T03:20:59.372508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit-aware text preprocessing completed\n",
      "Sample cleaned text: i will soon be going on a long deployment which i m not aloud to discuss but willing to give some in...\n"
     ]
    }
   ],
   "source": [
    "def reddit_aware_preprocess(text):\n",
    "    \"\"\"Reddit-aware text preprocessing\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Handle Reddit-specific patterns\n",
    "    # User mentions: u/username -> USER_MENTION\n",
    "    text = re.sub(r'u/\\w+', 'USER_MENTION', text)\n",
    "    \n",
    "    # Subreddit mentions: r/subreddit -> SUBREDDIT_MENTION\n",
    "    text = re.sub(r'r/\\w+', 'SUBREDDIT_MENTION', text)\n",
    "    \n",
    "    # URLs -> URL\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'URL', text)\n",
    "    \n",
    "    # Markdown syntax\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', text)  # **bold**\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)      # *italic*\n",
    "    text = re.sub(r'~~(.+?)~~', r'\\1', text)       # ~~strikethrough~~\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine request text and title\n",
    "text_col = 'request_text' if 'request_text' in train_df.columns else 'request_text_edit_aware'\n",
    "\n",
    "train_df['combined_text'] = train_df[text_col].fillna('') + ' ' + train_df['request_title'].fillna('')\n",
    "test_df['combined_text'] = test_df['request_text_edit_aware'].fillna('') + ' ' + test_df['request_title'].fillna('')\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['combined_text_clean'] = train_df['combined_text'].apply(reddit_aware_preprocess)\n",
    "test_df['combined_text_clean'] = test_df['combined_text'].apply(reddit_aware_preprocess)\n",
    "\n",
    "print(\"Reddit-aware text preprocessing completed\")\n",
    "print(f\"Sample cleaned text: {train_df['combined_text_clean'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb875e9",
   "metadata": {},
   "source": [
    "## Extract High-Impact Keyword Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fbad16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:20:59.373984Z",
     "iopub.status.busy": "2026-01-10T03:20:59.373713Z",
     "iopub.status.idle": "2026-01-10T03:20:59.499353Z",
     "shell.execute_reply": "2026-01-10T03:20:59.499004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword features shape: (2878, 14)\n",
      "Sample keyword features:\n",
      "   keyword_need_count  keyword_student_count  keyword_family_count  \\\n",
      "0                   0                      0                     0   \n",
      "1                   0                      2                     0   \n",
      "2                   0                      0                     2   \n",
      "3                   0                      0                     0   \n",
      "4                   0                      0                     0   \n",
      "\n",
      "   keyword_job_count  keyword_gratitude_count  keyword_edit_count  \\\n",
      "0                  0                        0                   0   \n",
      "1                  1                        1                   0   \n",
      "2                  1                        1                   1   \n",
      "3                  1                        0                   1   \n",
      "4                  0                        0                   0   \n",
      "\n",
      "   has_need_keyword  has_student_keyword  has_family_keyword  has_job_keyword  \\\n",
      "0             False                False               False            False   \n",
      "1             False                 True               False             True   \n",
      "2             False                False                True             True   \n",
      "3             False                False               False             True   \n",
      "4             False                False               False            False   \n",
      "\n",
      "   has_gratitude_keyword  has_edit_keyword  text_length  word_count  \n",
      "0                  False             False          271          55  \n",
      "1                   True             False          277          52  \n",
      "2                   True              True          752         162  \n",
      "3                  False              True         1041         213  \n",
      "4                  False             False          189          39  \n"
     ]
    }
   ],
   "source": [
    "def extract_keyword_features(df, text_col):\n",
    "    \"\"\"Extract binary features for high-impact keywords\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Keywords from EDA that indicate need/urgency\n",
    "    need_keywords = ['hungry', 'broke', 'starving', 'desperate', 'struggling']\n",
    "    student_keywords = ['student', 'college', 'university', 'tuition', 'loan']\n",
    "    family_keywords = ['family', 'kids', 'children', 'baby', 'mother', 'father', 'parent']\n",
    "    job_keywords = ['job', 'work', 'unemployed', 'laid off', 'fired', 'paycheck']\n",
    "    gratitude_keywords = ['thank', 'appreciate', 'grateful', 'bless', 'kind']\n",
    "    edit_keywords = ['edit', 'update']\n",
    "    \n",
    "    # Count occurrences of each keyword group\n",
    "    for keyword_list, prefix in [\n",
    "        (need_keywords, 'need'),\n",
    "        (student_keywords, 'student'),\n",
    "        (family_keywords, 'family'),\n",
    "        (job_keywords, 'job'),\n",
    "        (gratitude_keywords, 'gratitude'),\n",
    "        (edit_keywords, 'edit')\n",
    "    ]:\n",
    "        features[f'keyword_{prefix}_count'] = df[text_col].apply(\n",
    "            lambda x: sum(1 for word in keyword_list if word in str(x).lower())\n",
    "        )\n",
    "    \n",
    "    # Binary features for presence of any keyword in each group\n",
    "    for keyword_list, prefix in [\n",
    "        (need_keywords, 'need'),\n",
    "        (student_keywords, 'student'),\n",
    "        (family_keywords, 'family'),\n",
    "        (job_keywords, 'job'),\n",
    "        (gratitude_keywords, 'gratitude'),\n",
    "        (edit_keywords, 'edit')\n",
    "    ]:\n",
    "        features[f'has_{prefix}_keyword'] = features[f'keyword_{prefix}_count'] > 0\n",
    "    \n",
    "    # Text length features\n",
    "    features['text_length'] = df[text_col].str.len()\n",
    "    features['word_count'] = df[text_col].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract keyword features\n",
    "train_keyword_features = extract_keyword_features(train_df, 'combined_text_clean')\n",
    "test_keyword_features = extract_keyword_features(test_df, 'combined_text_clean')\n",
    "\n",
    "print(f\"Keyword features shape: {train_keyword_features.shape}\")\n",
    "print(\"Sample keyword features:\")\n",
    "print(train_keyword_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a3d24",
   "metadata": {},
   "source": [
    "## Safe Flair Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be21f848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:20:59.500428Z",
     "iopub.status.busy": "2026-01-10T03:20:59.500189Z",
     "iopub.status.idle": "2026-01-10T03:20:59.508938Z",
     "shell.execute_reply": "2026-01-10T03:20:59.508618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: requester_user_flair not found in test data. Creating dummy features.\n",
      "Flair encoding completed\n",
      "Target encoded flair distribution:\n",
      "count    2878.000000\n",
      "mean        0.248436\n",
      "std         0.000000\n",
      "min         0.248436\n",
      "25%         0.248436\n",
      "50%         0.248436\n",
      "75%         0.248436\n",
      "max         0.248436\n",
      "Name: flair_target_encoded, dtype: float64\n",
      "\n",
      "Perfect flair count in train: 0\n",
      "Perfect flair count in test: 0\n"
     ]
    }
   ],
   "source": [
    "def safe_target_encode_flair(train_df, test_df, target_col, flair_col='requester_user_flair'):\n",
    "    \"\"\"Safely target encode user flair with aggressive smoothing\"\"\"\n",
    "    \n",
    "    # Check if flair column exists in train data\n",
    "    if flair_col not in train_df.columns:\n",
    "        print(f\"WARNING: {flair_col} not found in train data. Creating dummy features.\")\n",
    "        \n",
    "        # Create dummy features with neutral values\n",
    "        train_encoded = pd.DataFrame(index=train_df.index)\n",
    "        test_encoded = pd.DataFrame(index=test_df.index)\n",
    "        \n",
    "        # Use global mean as target encoding (neutral)\n",
    "        global_mean = train_df[target_col].mean()\n",
    "        train_encoded['flair_target_encoded'] = global_mean\n",
    "        test_encoded['flair_target_encoded'] = global_mean\n",
    "        \n",
    "        # No perfect flairs\n",
    "        train_encoded['has_perfect_flair'] = 0\n",
    "        test_encoded['has_perfect_flair'] = 0\n",
    "        \n",
    "        return train_encoded, test_encoded\n",
    "    \n",
    "    # Check if flair column exists in test data\n",
    "    if flair_col not in test_df.columns:\n",
    "        print(f\"WARNING: {flair_col} not found in test data. Creating dummy features.\")\n",
    "        \n",
    "        # Create dummy features with neutral values\n",
    "        train_encoded = pd.DataFrame(index=train_df.index)\n",
    "        test_encoded = pd.DataFrame(index=test_df.index)\n",
    "        \n",
    "        # Use global mean as target encoding (neutral)\n",
    "        global_mean = train_df[target_col].mean()\n",
    "        train_encoded['flair_target_encoded'] = global_mean\n",
    "        test_encoded['flair_target_encoded'] = global_mean\n",
    "        \n",
    "        # No perfect flairs in test data\n",
    "        train_encoded['has_perfect_flair'] = 0\n",
    "        test_encoded['has_perfect_flair'] = 0\n",
    "        \n",
    "        return train_encoded, test_encoded\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[[flair_col]].copy()\n",
    "    X_test = test_df[[flair_col]].copy()\n",
    "    y_train = train_df[target_col]\n",
    "    \n",
    "    # Use TargetEncoder with high smoothing to prevent overfitting\n",
    "    # smoothing=10 means we need 10 samples to trust the category mean\n",
    "    encoder = TargetEncoder(smoothing=10.0, min_samples_leaf=5)\n",
    "    \n",
    "    # Fit and transform\n",
    "    train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "    test_encoded = encoder.transform(X_test)\n",
    "    \n",
    "    # Rename columns\n",
    "    train_encoded.columns = ['flair_target_encoded']\n",
    "    test_encoded.columns = ['flair_target_encoded']\n",
    "    \n",
    "    # Also create a binary feature for perfect flairs (PIF and shroom)\n",
    "    perfect_flairs = ['shroom', 'PIF']\n",
    "    train_encoded['has_perfect_flair'] = train_df[flair_col].isin(perfect_flairs).astype(int)\n",
    "    test_encoded['has_perfect_flair'] = test_df[flair_col].isin(perfect_flairs).astype(int)\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Target encode flair\n",
    "train_flair_features, test_flair_features = safe_target_encode_flair(\n",
    "    train_df, test_df, 'requester_received_pizza'\n",
    ")\n",
    "\n",
    "print(\"Flair encoding completed\")\n",
    "print(\"Target encoded flair distribution:\")\n",
    "print(train_flair_features['flair_target_encoded'].describe())\n",
    "print(f\"\\nPerfect flair count in train: {train_flair_features['has_perfect_flair'].sum()}\")\n",
    "print(f\"Perfect flair count in test: {test_flair_features['has_perfect_flair'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7235a5b",
   "metadata": {},
   "source": [
    "## Temporal Features with Cyclical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb354f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:20:59.510022Z",
     "iopub.status.busy": "2026-01-10T03:20:59.509785Z",
     "iopub.status.idle": "2026-01-10T03:20:59.526206Z",
     "shell.execute_reply": "2026-01-10T03:20:59.525875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal features shape: (2878, 7)\n",
      "Sample temporal features:\n",
      "   day_of_month  hour_sin  hour_cos  day_of_week_sin  day_of_week_cos  \\\n",
      "0            28  0.258819  0.965926         0.781831         0.623490   \n",
      "1            23 -0.707107 -0.707107        -0.974928        -0.222521   \n",
      "2            15 -0.866025 -0.500000         0.433884        -0.900969   \n",
      "3            14 -0.258819  0.965926         0.433884        -0.900969   \n",
      "4            27  0.258819  0.965926         0.781831         0.623490   \n",
      "\n",
      "      month_sin     month_cos  \n",
      "0  8.660254e-01  5.000000e-01  \n",
      "1 -5.000000e-01 -8.660254e-01  \n",
      "2 -2.449294e-16  1.000000e+00  \n",
      "3  1.000000e+00  6.123234e-17  \n",
      "4 -1.000000e+00 -1.836970e-16  \n"
     ]
    }
   ],
   "source": [
    "def extract_temporal_features(df):\n",
    "    \"\"\"Extract cyclical temporal features from Unix timestamps\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Convert Unix timestamp to datetime\n",
    "    # Use request timestamp if available, otherwise use a retrieval timestamp\n",
    "    if 'unix_timestamp_of_request' in df.columns:\n",
    "        timestamp_col = 'unix_timestamp_of_request'\n",
    "    elif 'unix_timestamp_of_request_utc' in df.columns:\n",
    "        timestamp_col = 'unix_timestamp_of_request_utc'\n",
    "    else:\n",
    "        # If no request timestamp, skip temporal features\n",
    "        print(\"No request timestamp found, skipping temporal features\")\n",
    "        return features\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df[timestamp_col], unit='s')\n",
    "    \n",
    "    # Extract time components\n",
    "    features['hour'] = df['datetime'].dt.hour\n",
    "    features['day_of_week'] = df['datetime'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    features['day_of_month'] = df['datetime'].dt.day\n",
    "    features['month'] = df['datetime'].dt.month\n",
    "    \n",
    "    # Cyclical encoding for hour (24 hours)\n",
    "    features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)\n",
    "    features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)\n",
    "    \n",
    "    # Cyclical encoding for day of week (7 days)\n",
    "    features['day_of_week_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)\n",
    "    features['day_of_week_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)\n",
    "    \n",
    "    # Cyclical encoding for month (12 months)\n",
    "    features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)\n",
    "    features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)\n",
    "    \n",
    "    # Drop raw temporal features (keep only cyclical)\n",
    "    features = features.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract temporal features\n",
    "train_temporal_features = extract_temporal_features(train_df)\n",
    "test_temporal_features = extract_temporal_features(test_df)\n",
    "\n",
    "print(f\"Temporal features shape: {train_temporal_features.shape}\")\n",
    "if len(train_temporal_features.columns) > 0:\n",
    "    print(\"Sample temporal features:\")\n",
    "    print(train_temporal_features.head())\n",
    "else:\n",
    "    print(\"No temporal features extracted (no timestamp column found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0e1e3",
   "metadata": {},
   "source": [
    "## Extract Tabular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c3a2e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:21:12.563127Z",
     "iopub.status.busy": "2026-01-10T03:21:12.562895Z",
     "iopub.status.idle": "2026-01-10T03:21:12.575220Z",
     "shell.execute_reply": "2026-01-10T03:21:12.574842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular features shape: (2878, 14)\n",
      "Sample tabular features:\n",
      "   requester_account_age_in_days_at_request  \\\n",
      "0                                  0.000000   \n",
      "1                                 99.526863   \n",
      "2                                  0.000000   \n",
      "3                                491.088264   \n",
      "4                                369.417558   \n",
      "\n",
      "   requester_account_age_in_days_at_retrieval  \\\n",
      "0                                  647.297060   \n",
      "1                                  966.278588   \n",
      "2                                  721.636238   \n",
      "3                                  757.480891   \n",
      "4                                 1170.732118   \n",
      "\n",
      "   requester_number_of_comments_at_request  \\\n",
      "0                                        0   \n",
      "1                                       40   \n",
      "2                                        0   \n",
      "3                                       46   \n",
      "4                                      195   \n",
      "\n",
      "   requester_number_of_comments_at_retrieval  \\\n",
      "0                                          0   \n",
      "1                                        147   \n",
      "2                                          0   \n",
      "3                                         54   \n",
      "4                                        309   \n",
      "\n",
      "   requester_number_of_posts_at_request  \\\n",
      "0                                     0   \n",
      "1                                    11   \n",
      "2                                     0   \n",
      "3                                     1   \n",
      "4                                    12   \n",
      "\n",
      "   requester_number_of_posts_at_retrieval  \\\n",
      "0                                       1   \n",
      "1                                      33   \n",
      "2                                       1   \n",
      "3                                       2   \n",
      "4                                      19   \n",
      "\n",
      "   requester_upvotes_minus_downvotes_at_request  \\\n",
      "0                                             3   \n",
      "1                                           491   \n",
      "2                                             1   \n",
      "3                                            25   \n",
      "4                                           942   \n",
      "\n",
      "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
      "0                                               3   \n",
      "1                                             883   \n",
      "2                                               1   \n",
      "3                                              21   \n",
      "4                                            2043   \n",
      "\n",
      "   requester_upvotes_plus_downvotes_at_request  \\\n",
      "0                                            7   \n",
      "1                                         1459   \n",
      "2                                            3   \n",
      "3                                          165   \n",
      "4                                         1906   \n",
      "\n",
      "   requester_upvotes_plus_downvotes_at_retrieval  \\\n",
      "0                                              7   \n",
      "1                                           2187   \n",
      "2                                              3   \n",
      "3                                            195   \n",
      "4                                           3483   \n",
      "\n",
      "   number_of_upvotes_of_request_at_retrieval  \\\n",
      "0                                          5   \n",
      "1                                          4   \n",
      "2                                          2   \n",
      "3                                          1   \n",
      "4                                         14   \n",
      "\n",
      "   number_of_downvotes_of_request_at_retrieval  \\\n",
      "0                                            2   \n",
      "1                                            2   \n",
      "2                                            1   \n",
      "3                                            1   \n",
      "4                                            3   \n",
      "\n",
      "   request_number_of_comments_at_retrieval  post_was_edited  \n",
      "0                                        0                0  \n",
      "1                                       20                0  \n",
      "2                                        0                1  \n",
      "3                                       32       1363315140  \n",
      "4                                        3                0  \n"
     ]
    }
   ],
   "source": [
    "def extract_tabular_features(df):\n",
    "    \"\"\"Extract and preprocess tabular features\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Numeric features that might exist in the dataset\n",
    "    numeric_cols = [\n",
    "        'requester_account_age_in_days_at_request',\n",
    "        'requester_account_age_in_days_at_retrieval',\n",
    "        'requester_number_of_comments_at_request',\n",
    "        'requester_number_of_comments_at_retrieval',\n",
    "        'requester_number_of_posts_at_request',\n",
    "        'requester_number_of_posts_at_retrieval',\n",
    "        'requester_upvotes_minus_downvotes_at_request',\n",
    "        'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "        'requester_upvotes_plus_downvotes_at_request',\n",
    "        'requester_upvotes_plus_downvotes_at_retrieval',\n",
    "        'number_of_upvotes_of_request_at_retrieval',\n",
    "        'number_of_downvotes_of_request_at_retrieval',\n",
    "        'request_number_of_comments_at_retrieval'\n",
    "    ]\n",
    "    \n",
    "    # Add numeric features if they exist\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            features[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            features[col] = 0  # Add column with zeros if it doesn't exist\n",
    "    \n",
    "    # Binary features\n",
    "    if 'post_was_edited' in df.columns:\n",
    "        features['post_was_edited'] = df['post_was_edited'].astype(int)\n",
    "    else:\n",
    "        features['post_was_edited'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract tabular features\n",
    "train_tabular_features = extract_tabular_features(train_df)\n",
    "test_tabular_features = extract_tabular_features(test_df)\n",
    "\n",
    "print(f\"Tabular features shape: {train_tabular_features.shape}\")\n",
    "print(\"Sample tabular features:\")\n",
    "print(train_tabular_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d22eb",
   "metadata": {},
   "source": [
    "## Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfde495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:21:12.576238Z",
     "iopub.status.busy": "2026-01-10T03:21:12.575984Z",
     "iopub.status.idle": "2026-01-10T03:21:12.583468Z",
     "shell.execute_reply": "2026-01-10T03:21:12.583156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train features shape: (2878, 37)\n",
      "Final test features shape: (1162, 37)\n",
      "Columns match: True\n",
      "\n",
      "Target distribution: {0: 2163, 1: 715}\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature sets\n",
    "feature_sets = [\n",
    "    train_keyword_features, train_flair_features, train_temporal_features, train_tabular_features\n",
    "]\n",
    "test_sets = [\n",
    "    test_keyword_features, test_flair_features, test_temporal_features, test_tabular_features\n",
    "]\n",
    "\n",
    "train_features = pd.concat(feature_sets, axis=1)\n",
    "test_features = pd.concat(test_sets, axis=1)\n",
    "\n",
    "# Ensure both have same columns and order\n",
    "common_cols = [col for col in train_features.columns if col in test_features.columns]\n",
    "train_features = train_features[common_cols]\n",
    "test_features = test_features[common_cols]\n",
    "\n",
    "print(f\"Final train features shape: {train_features.shape}\")\n",
    "print(f\"Final test features shape: {test_features.shape}\")\n",
    "print(f\"Columns match: {list(train_features.columns) == list(test_features.columns)}\")\n",
    "\n",
    "# Prepare target\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "print(f\"\\nTarget distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921644d0",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f283e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:21:12.584398Z",
     "iopub.status.busy": "2026-01-10T03:21:12.584164Z",
     "iopub.status.idle": "2026-01-10T03:31:10.683554Z",
     "shell.execute_reply": "2026-01-10T03:31:10.683057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold stratified cross-validation...\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.7933\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.7759\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.7988\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC: 0.7694\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 AUC: 0.7799\n",
      "\n",
      "Overall CV AUC: 0.7774\n",
      "Mean CV AUC: 0.7835 ± 0.0109\n"
     ]
    }
   ],
   "source": [
    "# Fill any remaining NaN values\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = test_features.fillna(0)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(train_features))\n",
    "test_predictions = np.zeros(len(test_features))\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold stratified cross-validation...\")\n",
    "\n",
    "fold = 1\n",
    "for train_idx, valid_idx in skf.split(train_features, y):\n",
    "    print(f\"\\nFold {fold}/{n_folds}\")\n",
    "    \n",
    "    X_train, X_valid = train_features.iloc[train_idx], train_features.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "    \n",
    "    # Define parameters - slightly tuned for better confidence\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 63,  # Increased from 31 for more capacity\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'min_child_samples': 20  # Added to prevent overfitting\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1500,  # Increased for more training\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    oof_predictions[valid_idx] = valid_pred\n",
    "    \n",
    "    # Calculate AUC for this fold\n",
    "    fold_auc = roc_auc_score(y_valid, valid_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    print(f\"Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_auc:.4f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0acf7c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d81a445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T03:31:10.684940Z",
     "iopub.status.busy": "2026-01-10T03:31:10.684759Z",
     "iopub.status.idle": "2026-01-10T03:31:10.692541Z",
     "shell.execute_reply": "2026-01-10T03:31:10.692182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most important features:\n",
      "                                           feature   importance\n",
      "35         request_number_of_comments_at_retrieval  2733.766954\n",
      "30  requester_upvotes_minus_downvotes_at_retrieval   762.414886\n",
      "24      requester_account_age_in_days_at_retrieval   635.930870\n",
      "13                                      word_count   489.561077\n",
      "26       requester_number_of_comments_at_retrieval   447.533247\n",
      "23        requester_account_age_in_days_at_request   389.167509\n",
      "32   requester_upvotes_plus_downvotes_at_retrieval   367.874944\n",
      "16                                    day_of_month   330.575661\n",
      "33       number_of_upvotes_of_request_at_retrieval   309.482448\n",
      "12                                     text_length   295.958139\n",
      "17                                        hour_sin   281.647960\n",
      "22                                       month_cos   260.356654\n",
      "28          requester_number_of_posts_at_retrieval   238.497646\n",
      "29    requester_upvotes_minus_downvotes_at_request   231.729634\n",
      "27            requester_number_of_posts_at_request   219.769208\n",
      "21                                       month_sin   202.501094\n",
      "25         requester_number_of_comments_at_request   194.123445\n",
      "31     requester_upvotes_plus_downvotes_at_request   168.732709\n",
      "18                                        hour_cos   168.273637\n",
      "34     number_of_downvotes_of_request_at_retrieval   166.174097\n",
      "\n",
      "Flair feature importance:\n",
      "                 feature  importance\n",
      "14  flair_target_encoded         0.0\n",
      "15     has_perfect_flair         0.0\n",
      "\n",
      "Top keyword features:\n",
      "                    feature  importance\n",
      "4   keyword_gratitude_count  131.629084\n",
      "10    has_gratitude_keyword   44.194011\n",
      "0        keyword_need_count   43.096750\n",
      "3         keyword_job_count   42.775544\n",
      "5        keyword_edit_count   32.812250\n",
      "1     keyword_student_count   29.766625\n",
      "2      keyword_family_count   24.235369\n",
      "11         has_edit_keyword   13.330350\n",
      "7       has_student_keyword   11.050380\n",
      "6          has_need_keyword    6.721770\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from the last fold model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_features.columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Check flair feature importance\n",
    "flair_importance = feature_importance[feature_importance['feature'].str.contains('flair')]\n",
    "print(f\"\\nFlair feature importance:\")\n",
    "print(flair_importance)\n",
    "\n",
    "# Check keyword feature importance\n",
    "keyword_importance = feature_importance[feature_importance['feature'].str.contains('keyword|has_')]\n",
    "print(f\"\\nTop keyword features:\")\n",
    "print(keyword_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb5c04",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ea911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(\"Submission shape:\", submission.shape)\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_003_flair_temporal.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format\n",
    "sample_sub = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_sub.columns)}\")\n",
    "print(f\"Request IDs match: {set(submission['request_id']) == set(sample_sub['request_id'])}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
