{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a127afc9",
   "metadata": {},
   "source": [
    "# Experiment 002: Adding TF-IDF Text Features\n",
    "\n",
    "This experiment adds TF-IDF features to capture actual text content from request_text and request_title.\n",
    "\n",
    "**Strategy:**\n",
    "- Extract TF-IDF features from text (unigrams and bigrams)\n",
    "- Limit to top 200 features to avoid sparsity\n",
    "- Combine with existing tabular features\n",
    "- Use 5-fold stratified CV\n",
    "- LightGBM for training\n",
    "\n",
    "**Expected improvements:**\n",
    "- Capture semantic content beyond just length\n",
    "- Identify key phrases that indicate need/urgency\n",
    "- Better prediction distribution (less underconfident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73a4984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:27:59.254282Z",
     "iopub.status.busy": "2026-01-10T02:27:59.254053Z",
     "iopub.status.idle": "2026-01-10T02:28:00.194887Z",
     "shell.execute_reply": "2026-01-10T02:28:00.194372Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f14886",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8831eff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:28:00.196268Z",
     "iopub.status.busy": "2026-01-10T02:28:00.196007Z",
     "iopub.status.idle": "2026-01-10T02:28:00.271012Z",
     "shell.execute_reply": "2026-01-10T02:28:00.270489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "Training columns: 32\n",
      "Test columns: 17\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_path = '/home/data/train.json'\n",
    "test_path = '/home/data/test.json'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Training columns: {train_df.shape[1]}\")\n",
    "print(f\"Test columns: {test_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823c26f",
   "metadata": {},
   "source": [
    "## Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672eefc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:28:00.272082Z",
     "iopub.status.busy": "2026-01-10T02:28:00.271965Z",
     "iopub.status.idle": "2026-01-10T02:28:00.389340Z",
     "shell.execute_reply": "2026-01-10T02:28:00.388981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed\n",
      "Sample cleaned text: i will soon be going on a long deployment which i m not aloud to discuss but willing to give some in...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Simple text preprocessing for TF-IDF\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine request text and title for richer features\n",
    "text_col = 'request_text' if 'request_text' in train_df.columns else 'request_text_edit_aware'\n",
    "\n",
    "train_df['combined_text'] = train_df[text_col].fillna('') + ' ' + train_df['request_title'].fillna('')\n",
    "test_df['combined_text'] = test_df['request_text_edit_aware'].fillna('') + ' ' + test_df['request_title'].fillna('')\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['combined_text_clean'] = train_df['combined_text'].apply(preprocess_text)\n",
    "test_df['combined_text_clean'] = test_df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed\")\n",
    "print(f\"Sample cleaned text: {train_df['combined_text_clean'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605a262",
   "metadata": {},
   "source": [
    "## Extract TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4741a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:28:00.390388Z",
     "iopub.status.busy": "2026-01-10T02:28:00.390182Z",
     "iopub.status.idle": "2026-01-10T02:28:00.768767Z",
     "shell.execute_reply": "2026-01-10T02:28:00.768355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: (2878, 200)\n",
      "TF-IDF test shape: (1162, 200)\n",
      "Top TF-IDF terms: ['able' 'account' 'advance' 'afford' 'ago' 'amp' 'apartment' 'appreciate'\n",
      " 'appreciated' 'area']\n"
     ]
    }
   ],
   "source": [
    "# Extract TF-IDF features\n",
    "# Limit to top features to avoid sparsity and overfitting\n",
    "max_features = 200\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    ngram_range=(1, 2),  # unigrams and bigrams\n",
    "    stop_words='english',\n",
    "    min_df=5,  # ignore very rare terms\n",
    "    max_df=0.8  # ignore very common terms\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "tfidf_train = vectorizer.fit_transform(train_df['combined_text_clean'])\n",
    "tfidf_test = vectorizer.transform(test_df['combined_text_clean'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_train_df = pd.DataFrame(\n",
    "    tfidf_train.toarray(),\n",
    "    columns=[f'tfidf_{i}' for i in range(max_features)],\n",
    "    index=train_df.index\n",
    ")\n",
    "\n",
    "tfidf_test_df = pd.DataFrame(\n",
    "    tfidf_test.toarray(),\n",
    "    columns=[f'tfidf_{i}' for i in range(max_features)],\n",
    "    index=test_df.index\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF train shape: {tfidf_train_df.shape}\")\n",
    "print(f\"TF-IDF test shape: {tfidf_test_df.shape}\")\n",
    "print(f\"Top TF-IDF terms: {vectorizer.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04f939",
   "metadata": {},
   "source": [
    "## Extract Tabular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052e8d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:28:00.769670Z",
     "iopub.status.busy": "2026-01-10T02:28:00.769566Z",
     "iopub.status.idle": "2026-01-10T02:28:00.779667Z",
     "shell.execute_reply": "2026-01-10T02:28:00.779328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular train shape: (2878, 14)\n",
      "Tabular test shape: (1162, 14)\n"
     ]
    }
   ],
   "source": [
    "def extract_tabular_features(df):\n",
    "    \"\"\"Extract and preprocess tabular features\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Numeric features that might exist in the dataset\n",
    "    numeric_cols = [\n",
    "        'requester_account_age_in_days_at_request',\n",
    "        'requester_account_age_in_days_at_retrieval',\n",
    "        'requester_number_of_comments_at_request',\n",
    "        'requester_number_of_comments_at_retrieval',\n",
    "        'requester_number_of_posts_at_request',\n",
    "        'requester_number_of_posts_at_retrieval',\n",
    "        'requester_upvotes_minus_downvotes_at_request',\n",
    "        'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "        'requester_upvotes_plus_downvotes_at_request',\n",
    "        'requester_upvotes_plus_downvotes_at_retrieval',\n",
    "        'number_of_upvotes_of_request_at_retrieval',\n",
    "        'number_of_downvotes_of_request_at_retrieval',\n",
    "        'request_number_of_comments_at_retrieval'\n",
    "    ]\n",
    "    \n",
    "    # Add numeric features if they exist\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            features[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            features[col] = 0  # Add column with zeros if it doesn't exist\n",
    "    \n",
    "    # Binary features\n",
    "    if 'post_was_edited' in df.columns:\n",
    "        features['post_was_edited'] = df['post_was_edited'].astype(int)\n",
    "    else:\n",
    "        features['post_was_edited'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract tabular features\n",
    "train_tabular_features = extract_tabular_features(train_df)\n",
    "test_tabular_features = extract_tabular_features(test_df)\n",
    "\n",
    "print(f\"Tabular train shape: {train_tabular_features.shape}\")\n",
    "print(f\"Tabular test shape: {test_tabular_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92df3b6",
   "metadata": {},
   "source": [
    "## Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65d7813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:28:21.095502Z",
     "iopub.status.busy": "2026-01-10T02:28:21.095282Z",
     "iopub.status.idle": "2026-01-10T02:28:21.124860Z",
     "shell.execute_reply": "2026-01-10T02:28:21.124468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train features shape: (2878, 214)\n",
      "Final test features shape: (1162, 214)\n",
      "Columns match: False\n",
      "Target distribution: {0: 2163, 1: 715}\n"
     ]
    }
   ],
   "source": [
    "# Combine TF-IDF and tabular features\n",
    "train_features = pd.concat([tfidf_train_df, train_tabular_features], axis=1)\n",
    "test_features = pd.concat([tfidf_test_df, test_tabular_features], axis=1)\n",
    "\n",
    "# Ensure both have same columns\n",
    "train_features = train_features.reindex(columns=test_features.columns)\n",
    "\n",
    "print(f\"Final train features shape: {train_features.shape}\")\n",
    "print(f\"Final test features shape: {test_features.shape}\")\n",
    "print(f\"Columns match: {list(train_features.columns) == list(test_features.shape)}\")\n",
    "\n",
    "# Prepare target\n",
    "y = train_df['requester_received_pizza'].astype(int)\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb79d00",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345263fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:28:21.125978Z",
     "iopub.status.busy": "2026-01-10T02:28:21.125874Z",
     "iopub.status.idle": "2026-01-10T02:29:52.081828Z",
     "shell.execute_reply": "2026-01-10T02:29:52.081296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold stratified cross-validation...\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.7935\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.7724\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.7851\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC: 0.7611\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 AUC: 0.7956\n",
      "\n",
      "Overall CV AUC: 0.7697\n",
      "Mean CV AUC: 0.7815 ± 0.0131\n"
     ]
    }
   ],
   "source": [
    "# Fill any remaining NaN values\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = test_features.fillna(0)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(train_features))\n",
    "test_predictions = np.zeros(len(test_features))\n",
    "cv_scores = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold stratified cross-validation...\")\n",
    "\n",
    "fold = 1\n",
    "for train_idx, valid_idx in skf.split(train_features, y):\n",
    "    print(f\"\\nFold {fold}/{n_folds}\")\n",
    "    \n",
    "    X_train, X_valid = train_features.iloc[train_idx], train_features.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': RANDOM_SEED\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    oof_predictions[valid_idx] = valid_pred\n",
    "    \n",
    "    # Calculate AUC for this fold\n",
    "    fold_auc = roc_auc_score(y_valid, valid_pred)\n",
    "    cv_scores.append(fold_auc)\n",
    "    print(f\"Fold {fold} AUC: {fold_auc:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict(test_features, num_iteration=model.best_iteration)\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_auc:.4f}\")\n",
    "print(f\"Mean CV AUC: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a164f64",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97fcf94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T02:29:52.083445Z",
     "iopub.status.busy": "2026-01-10T02:29:52.083323Z",
     "iopub.status.idle": "2026-01-10T02:29:52.094314Z",
     "shell.execute_reply": "2026-01-10T02:29:52.093937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most important features:\n",
      "                                            feature   importance\n",
      "212         request_number_of_comments_at_retrieval  2947.047934\n",
      "201      requester_account_age_in_days_at_retrieval   744.801151\n",
      "207  requester_upvotes_minus_downvotes_at_retrieval   731.755104\n",
      "203       requester_number_of_comments_at_retrieval   536.245789\n",
      "209   requester_upvotes_plus_downvotes_at_retrieval   502.315290\n",
      "137                                       tfidf_137   481.312739\n",
      "200        requester_account_age_in_days_at_request   466.218610\n",
      "204            requester_number_of_posts_at_request   386.892970\n",
      "210       number_of_upvotes_of_request_at_retrieval   375.567380\n",
      "206    requester_upvotes_minus_downvotes_at_request   343.229799\n",
      "205          requester_number_of_posts_at_retrieval   311.337381\n",
      "202         requester_number_of_comments_at_request   294.601562\n",
      "186                                       tfidf_186   271.277141\n",
      "211     number_of_downvotes_of_request_at_retrieval   228.676870\n",
      "36                                         tfidf_36   221.093349\n",
      "\n",
      "Top TF-IDF features:\n",
      "       feature  importance\n",
      "137  tfidf_137  481.312739\n",
      "186  tfidf_186  271.277141\n",
      "36    tfidf_36  221.093349\n",
      "169  tfidf_169  190.433229\n",
      "43    tfidf_43  184.248520\n",
      "93    tfidf_93  177.630780\n",
      "168  tfidf_168  175.708749\n",
      "35    tfidf_35  175.403940\n",
      "22    tfidf_22  171.406230\n",
      "0      tfidf_0  166.344740\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance from the last fold model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_features.columns,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Check how many TF-IDF features are in top features\n",
    "tfidf_top_features = feature_importance[feature_importance['feature'].str.startswith('tfidf_')].head(10)\n",
    "print(f\"\\nTop TF-IDF features:\")\n",
    "print(tfidf_top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c8ccf",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "print(\"Submission shape:\", submission.shape)\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_002_tfidf.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format\n",
    "sample_sub = pd.read_csv('/home/data/sampleSubmission.csv')\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_sub.columns)}\")\n",
    "print(f\"Request IDs match: {set(submission['request_id']) == set(sample_sub['request_id'])}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
