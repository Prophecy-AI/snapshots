## Current Status
- Best CV: 0.7851 from exp_000 (Baseline LightGBM)
- Gap to gold: ~0.194 (need 0.979080)
- Both experiments below gold, TF-IDF made it worse

## Response to Evaluator
- **Technical verdict was TRUSTWORTHY** - execution is sound but model is underconfident
- **Evaluator's top priority: Text content analysis** - I agree but my analysis reveals user flair is even MORE important
- **Key concerns raised**:
  - *No text content analysis*: Will fix with Reddit-aware preprocessing and keyword features
  - *No user-level aggregations*: **IMPOSSIBLE** - all usernames are unique (1 request per user), cannot create history features
  - *No temporal features*: Will implement cyclical encoding for hour/day patterns
  - *No handling of strong categorical signals*: This is CRITICAL - flair has 100% success rates

## Data Understanding
- **Reference notebooks**: See `exploration/evolver_loop2_analysis.ipynb` for latest findings
- **CRITICAL FINDINGS**:
  - User flair is EXTREMELY predictive: PIF (38/38, 100%) and shroom (677/677, 100%) flairs
  - These cover 24.8% of data but pose massive overfitting risk
  - No users have multiple requests - usernames are anonymized per-request
  - Temporal patterns exist (3pm peak, Thursday best) but are moderate
  - TF-IDF failed due to simple preprocessing without Reddit-specific handling

## Recommended Approaches (Priority Order)

### 1. SAFE FLAIR HANDLING (Highest Priority)
- **Target encode user flair with aggressive smoothing**
  - Use sklearn's TargetEncoder with internal cross-validation
  - Set high smoothing parameter to prevent overfitting on perfect predictors
  - Alternative: Create binary feature `has_perfect_flair` (1 if PIF or shroom, 0 otherwise)
  - **Why**: These flairs are 100% predictive but may not exist in test set or may be different
  - **Validation**: Must check if flair patterns exist in test set via adversarial validation

### 2. REDDIT-AWARE TEXT FEATURES
- **Implement Reddit-specific preprocessing**:
  - Use redditScore's CrazyTokenizer or custom regex to handle:
    - Markdown syntax (**bold**, *italic*, ~~strikethrough~~)
    - Subreddit mentions (r/pizza, r/RandomActsOfPizza)
    - User mentions (u/username)
    - URLs and emails
    - Emojis and emoticons
  - **Create binary features for high-impact keywords**:
    - From EDA: 'hungry', 'broke', 'student', 'paycheck', 'week', 'ramen'
    - Also: 'edit', 'update', 'thank', 'please', 'help', 'need', 'family', 'kids', 'job', 'lost'
    - 'EDIT' presence already shows 41.6% vs 22.6% success rate
  - **Sentiment analysis**: Use VADER or TextBlob for sentiment scores
  - **TF-IDF with proper preprocessing**: Try again with Reddit tokenization, limit to top 50-100 features
  - **Why**: Previous TF-IDF failed due to naive preprocessing, but text is the main content

### 3. TEMPORAL FEATURES
- **Extract cyclical time features**:
  - Hour of day: use sin/cos transformation to capture 3pm peak
  - Day of week: use sin/cos, Thursday shows 28.7% success rate
  - Day of month: raw value or sin/cos
  - Month: sin/cos, June shows 31.6% success rate
- **Why**: Clear patterns exist and are easy to engineer safely

### 4. LEAKAGE INVESTIGATION
- **Test retrieval vs request features**:
  - Create two versions: one with all features, one dropping `_at_retrieval` features
  - Compare CV scores - if retrieval features cause overfitting, they'll show high train score but low val score
  - **Why**: Features measured "at retrieval" may contain future information not available at request time

### 5. MODEL DIVERSITY
- **Try CatBoost**: Handles categorical features natively, may better capture flair signal
- **Try XGBoost**: Different algorithm, good for ensembling
- **Tune LightGBM**: Current model is underconfident (predictions 0.061-0.138), increase max_depth or reduce min_child_samples
- **Why**: Single model likely insufficient, need diversity for ensemble

### 6. FEATURE SELECTION
- **Remove low-importance features**: Use feature importance from baseline to drop noise
- **Create interaction features**: 
  - `flair_encoded * hour` (maybe certain flairs work better at certain times)
  - `text_length * number_of_comments`
- **Why**: Reduce dimensionality and capture non-linear relationships

## What NOT to Try
- **User history aggregations**: Impossible - all usernames are unique per request
- **Simple TF-IDF**: Already failed, need Reddit-aware preprocessing first
- **One-hot encoding for flair**: Will cause catastrophic overfitting on perfect predictors
- **Ignoring flair signal**: Too strong to ignore, but must handle safely

## Validation Notes
- **CV Scheme**: Continue with stratified 5-fold CV (current approach is sound)
- **Overfitting detection**: Monitor train/val gap - large gap indicates leakage (especially from flair)
- **Test set check**: Use adversarial validation to verify flair patterns exist in test set
- **Prediction calibration**: Ensure predictions span full [0,1] range, not clustered narrowly
- **Confidence**: Medium - flair handling is risky but necessary for big gains