{
  "query": "What are the most effective techniques for handling high-cardinality categorical features like usernames in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "High\u2011cardinality categorical columns\u2014such as user\u2011names, product IDs, or ad\u2011slot tags\u2014contain thousands or even millions of distinct values, so a na\u00efve one\u2011hot representation quickly explodes the feature space and leads to over\u2011fitting. Because most machine\u2011learning models expect numeric input, the goal is to compress the categorical information into a few informative numbers while preserving its predictive signal and handling unseen categories at inference time ([getgalaxy.io](https://www.getgalaxy.io/learn/glossary/encoding-categorical-variables-for-xgboost-ea9fe)).\n\nThe most effective encodings that Kaggle competitors use are:\n\n* **Target (mean) encoding** \u2013 replace each category with the average target value (or a smoothed version) computed on the training folds. Regularised variants that add noise or Bayesian smoothing dramatically reduce leakage and have been shown to outperform classic methods on high\u2011cardinality data ([springer.com](https://link.springer.com/article/10.1007/s00180-022-01207-6)).  \n* **Count / frequency encoding** \u2013 map a category to how often it appears (or its relative frequency). This simple signal often works well with tree\u2011based models and is cheap to compute.  \n* **Feature hashing (the \u201chash trick\u201d)** \u2013 project the original string into a fixed\u2011size integer space using a hash function, which keeps memory usage constant and gracefully handles new values at test time.  \n* **Learned embeddings** \u2013 train a shallow neural network (or use the embedding layer of a deep model) to produce dense vector representations of each category; embeddings capture complex interactions and are especially powerful when the categorical variable has semantic structure (e.g., usernames that share prefixes).  \n\nIn practice Kaggle participants combine these encodings with a few robustness tricks:\u202fbucket rare categories into an \u201cOther\u201d group, ensure that the cardinality distribution in the validation split mirrors the test set, and apply the encoding inside a cross\u2011validation pipeline to avoid target leakage ([kaggle.com](https://www.kaggle.com/discussions/general/16927)). Tree\u2011based libraries such as CatBoost or LightGBM also have built\u2011in handling for high\u2011cardinality features, but explicit target/frequency encodings or hashing often give finer control and better performance when paired with XGBoost or linear models ([getgalaxy.io](https://www.getgalaxy.io/learn/glossary/encoding-categorical-variables-for-xgboost-ea9fe)).",
      "url": ""
    },
    {
      "title": "How to deal with Features having high cardinality | Kaggle",
      "text": "<div><div><p>One thing I do is try to ensure that the cardinality of the categorical information in the training set resembles that in the test/validation sets. That is, if I have a feature with values {A,A,A,B,C,C,D} in train, but test only has {A,B,B}, then eliminating the C and D records, and undersampling the A or oversampling the B records may resist overfitting.</p>\n<p>Also, for individual featuers with low cardinality, it's often worth bucketing them. In the above example, you may end up replacement values for A and C, and then bucketing B and D into an \"Other\" category (similar to Triskelion's trick with COUNT replacement).</p>\n<p>But overfitting is also something you can solve for by careful training -- split your training set in multiple ways, relying on cross-validation to minimize and test for over fitting before going to the private test data sets.</p></div></div>",
      "url": "https://www.kaggle.com/discussions/general/16927"
    },
    {
      "title": "Effective Methods to Encode Categorical Variables for XGBoost | Galaxy",
      "text": "<div><p>Transforming non-numeric categorical features into numerical representations that preserve information and allow XGBoost to learn effectively.</p><div><p>Encoding Categorical Variables for XGBoost</p><p>XGBoost is one of the most popular gradient-boosting libraries, but it only understands numbers. Converting raw categorical columns\u2014product type, country, user segment\u2014into meaningful numerical signals is therefore a critical preprocessing step that directly impacts model accuracy, speed, and interpretability.</p><h2>Why Categorical Encoding Matters</h2><p>XGBoost, like most tree-based algorithms implemented for CPUs/GPUs, handles only floating-point inputs. N<a href=\"https://www.getgalaxy.io/features/ai\">ai</a>vely feeding string labels or arbitrary integers introduces spurious order, confuses the split criteria, and often degrades performance drastically. Proper encoding</p><ul><li>Preserves the inherent <strong>information content</strong> of categories.</li><li>Avoids the dreaded <strong>curse of dimensionality</strong> caused by exploding feature space.</li><li>Controls <strong>memory footprint</strong> and <strong>training speed</strong>\u2014critical on large data.</li><li>Supports <strong>generalization</strong> to unseen categories at inference time.</li></ul><h2>Common Encoding Strategies</h2><h3>1. One-Hot (Dummy) Encoding</h3><p>Creates a binary column for every category. Works well for low-cardinality (&lt; 25 distinct values) but scales poorly beyond that. XGBoost handles sparse matrices efficiently, yet thousands of one-hot columns still add CPU/GPU overhead and risk overfitting.</p><h3>2. Ordinal (Label) Encoding</h3><p>Maps each category to an integer 0\u2026n. Fast and memory-light, but imposes a <em>fake order</em>. Splits like \u201c&lt;= 4\u201d have no semantic meaning. Use only when the categories are <em>truly ordinal</em> (Small, Medium, Large) or when paired with <strong>frequency shuffling</strong> to mitigate ordering bias.</p><h3>3. Target (Mean) Encoding</h3><p>Replaces each category with the mean of the target variable (e.g., churn rate). Captures category \u2192 target relationship in a single numeric feature\u2014great for high cardinality (ZIP codes). Requires careful <strong>cross-validation</strong> or <strong>leave-one-out</strong> to avoid leakage.</p><h3>4. Frequency / Count Encoding</h3><p>Encodes a category via its occurrence count or probability. Simple, leakage-free, and often surprisingly strong. Allows the model to learn that rarely seen categories should be handled differently.</p><h3>5. Hashing</h3><p>Applies a hash function to project categories into a fixed number of buckets. No need for a dictionary and gracefully handles unseen categories. Downside: hash collisions merge unrelated categories, potentially injecting noise.</p><h3>6. CatBoost-like Ordered Target Encoding</h3><p>Simulates the CatBoost algorithm: for each row, the encoded value is computed from <em>previous</em> rows only, preserving training causality. Helpful when writing your own encoder or using <code>category_encoders.OrdinalEncoder</code> with <code>ordered=True</code>.</p><h3>7. Entity Embeddings</h3><p>Learn low-dimensional dense vectors for categories using neural networks, then feed these embeddings into XGBoost. Useful in deep tabular pipelines but adds complexity.</p><h2>Choosing the Right Encoder</h2><ul><li><strong>Low cardinality (\u2264 15)</strong>: One-hot usually wins.</li><li><strong>Medium cardinality (16-100)</strong>: Frequency or target encoding.</li><li><strong>High cardinality (&gt; 100)</strong>: Target, hash, or frequency encoding\u2014sometimes combined.</li><li><strong>Leakage-sensitive problems</strong> (e.g., time-series): Ordered target encoding or cross-fold target encoding is safest.</li></ul><h2>Step-by-Step Implementation in Python</h2><ol><li><strong>Inspect categories</strong>: count distinct values, missing value rate.</li><li><strong>Select encoders</strong> per column.</li><li><strong>Create pipelines</strong> with <code>sklearn.compose.ColumnTransformer</code> to keep train/test transformations identical.</li><li><strong>Fit encoders on training only</strong>; transform validation/test.</li><li><strong>Train XGBoost</strong> using the resulting numeric matrix.</li></ol><p><code>import pandas as pd<br/>from sklearn.model_selection import train_test_split, KFold<br/>from category_encoders import TargetEncoder, OneHotEncoder<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.pipeline import Pipeline<br/>from xgboost import XGBClassifier</code></p><p><code># Data<br/>df = pd.read_csv(\"customers.csv\")<br/>X = df.drop(\"churn\", axis=1)<br/>y = df[\"churn\"]</code></p><p><code>low_card = [\"gender\", \"plan\"]<br/>high_card = [\"zip\"]</code></p><p><code>preprocess = ColumnTransformer([<br/> (\"onehot\", OneHotEncoder(drop=\"first\"), low_card),<br/> (\"target\", TargetEncoder(smoothing=0.3), high_card)<br/>], remainder=\"passthrough\")</code></p><p><code>model = XGBClassifier(tree_method=\"hist\", max_depth=6, n_estimators=400, learning_rate=0.05)</code></p><p><code>clf = Pipeline([<br/> (\"prep\", preprocess),<br/> (\"xgb\", model)<br/>])</code></p><p><code>kf = KFold(n_splits=5, shuffle=True, random_state=42)<br/>for fold, (tr, val) in enumerate(kf.split(X)):<br/> clf.fit(X.iloc[tr], y.iloc[tr])<br/> print(f\"Fold {fold} AUC = {clf.score(X.iloc[val], y.iloc[val]):.4f}\")</code></p><p></p><h2>Best Practices</h2><ul><li>Always <strong>fit encoders on training data only</strong> to avoid target leakage.</li><li>Add <strong>regularization</strong> (smoothing, noise) to target encodings.</li><li>Store encoder objects with the model for repeatable inference.</li><li>Benchmark multiple schemes: one-hot vs. target encoding performance varies by dataset.</li><li>Watch memory: sparse one-hot + 10M rows may crash a laptop.</li></ul><h2>Common Pitfalls</h2><h3>Label Encoding Without Ordinal Meaning</h3><p>Creates artificial ordering. Fix by using one-hot or target encoding instead.</p><h3>One-Hot Explosion on High Cardinality</h3><p>Leads to millions of features and slow training. Combine rare categories, use hashing or target encoding.</p><h3>Training/Test Mismatch</h3><p>Encoding fitted on full data leaks future info; or categories unseen during training map to NA. Always fit on train and set <code>handle_unknown='ignore'</code> where possible.</p><h2>Real-World Case Study</h2><p>An e-commerce company predicted 30-day purchase propensity. Product catalog had 12k distinct <code>product_category</code> values. Switching from one-hot to target encoding cut feature dimensionality from 15k to 400, reduced training time from 90 min to 9 min, and improved AUC from 0.76 to 0.80 by mitigating sparsity.</p><h2>Where Galaxy Fits</h2><p>While encoding occurs in Python/R, many teams still <strong>prototype SQL transformations</strong> inside their data warehouse. Galaxy\u2019s modern <a href=\"https://www.getgalaxy.io/features/sql-editor\">SQL editor</a>\u2014complete with AI Copilot\u2014lets engineers <em>generate</em>, share, and <em>endorse</em> those preparatory SQL snippets (e.g., <code>SELECT country, COUNT(*) AS freq</code>) before exporting data to a notebook for advanced encodings. Rapid iteration in Galaxy shortens the feedback loop between data prep and model training.</p><h2>Conclusion</h2><p>Choosing the right categorical encoding for XGBoost is part art, part science. Understand your data, test multiple encoders, and protect against leakage. Done well, proper encoding unlocks the full predictive power of gradient boosting while keeping compute budgets in check.</p></div><div><p></p><h2> Why Encoding Categorical Variables for XGBoost is important </h2><p></p><p>Improper encoding can introduce target leakage, blow up feature space, or inject artificial order\u2014leading to poor accuracy, slow training, and unreliable models. Mastering encoding unlocks XGBoost\u2019s full power on real-world, mixed-type data.</p></div><div><h2>Frequently Asked Questions (FAQs)</h2><div><h3>Is...",
      "url": "https://www.getgalaxy.io/learn/glossary/encoding-categorical-variables-for-xgboost-ea9fe"
    },
    {
      "title": "Encoding High Cardinality Categorical Variables | Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1bc6d8fd7b13&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# 4 Ways to Encode Categorical Features with High Cardinality \u2014 with Python Implementation\n\n## Learn to apply target encoding, count encoding, feature hashing and Embedding using scikit-learn and TensorFlow\n\n[![Aicha Bokbot](https://miro.medium.com/v2/resize:fill:88:88/1*jJAsvCE_o2AHOXeWdjseKQ.jpeg)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Aicha Bokbot](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nJun 26, 2023\n\n--\n\nShare\n\n\u201cClick\u201d \u2014 Photo by [Cleo Vermij](https://unsplash.com/@cleovermij?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nIn this article, we will go through 4 popular methods to encode categorical variables with high cardinality: **(1) Target encoding, (2) Count encoding, (3) Feature hashing** and **(4) Embedding**.\n\nWe will explain how each method works, discuss its pros and cons and observe its impact on the performance of a classification task.\n\n## **Table of content**\n\n\u2014 [Introducing categorical features](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#8744)\n\n_(1)_ [_Why do we need to encode categorical features?_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#2429) _(2)_ [_Why one-hot encoding is not suited to high cardinality?_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#b13b)\n\n\u2014 [Application on an AdTech dataset](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#706a)\n\n\u2014 [Overview of each encoding method](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#a959)\n\n_(1)_ [_Target encoding_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#ffbc) _(2)_ [_Count encoding_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#fbd1) _(3)_ [_Feature hashing_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#e278) _(4)_ [_Embedding_](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#99d8)\n\n\u2014 [Benchmarking the performance to predict CTR](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#892c)\n\n\u2014 [Conclusion](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#033c)\n\n\u2014 [To go further](https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe#3bd1)\n\n# **Introducing**\u2026\n\n[![Aicha Bokbot](https://miro.medium.com/v2/resize:fill:144:144/1*jJAsvCE_o2AHOXeWdjseKQ.jpeg)](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------follow_profile-----------)\n\n[**Written by Aicha Bokbot**](https://medium.com/@aichabokbot?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[46 Followers](https://medium.com/@aichabokbot/followers?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n\u00b7Writer for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\nMachine learning engineering & Data science \\| [www.linkedin.com/in/aichabokbot/](http://www.linkedin.com/in/aichabokbot/)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F50566ce7e21&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13&user=Aicha+Bokbot&userId=50566ce7e21&source=post_page-50566ce7e21----1bc6d8fd7b13---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1bc6d8fd7b13--------------------------------)\n\n[Text to speech](https://speechify.com/mediu...",
      "url": "https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13?gi=0fd38505fffe"
    },
    {
      "title": "4 ways to encode categorical features with high cardinality",
      "text": "# 4 ways to encode categorical features with high cardinality\n\nWe explore 4 methods to encode categorical variables with high cardinality: target encoding, count encoding, feature hashing and embedding.\n\n[Aicha Bokbot](https://towardsdatascience.com/author/aichabokbot/)\n\nJun 26, 2023\n\n10 min read\n\nShare\n\n# 4 Ways to Encode Categorical Features with High Cardinality \u2013 with Python Implementation\n\n### Learn to apply target encoding, count encoding, feature hashing and Embedding using scikit-learn and TensorFlow\n\n\"Click\" \u2013 Photo by [Cleo Vermij](https://unsplash.com/@cleovermij?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)\n\nIn this article, we will go through 4 popular methods to encode categorical variables with high cardinality: **(1) Target encoding, (2) Count encoding, (3) Feature hashing** and **(4) Embedding**.\n\nWe will explain how each method works, discuss its pros and cons and observe its impact on the performance of a classification task.\n\n### **Table of content**\n\n\u2014 [Introducing categorical features](https://towardsdatascience.com/towardsdatascience.com#8744) _(1) [Why do we need to encode categorical features?](https://towardsdatascience.com/towardsdatascience.com#2429)_\n_(2) [Why one-hot encoding is not suited to high cardinality?](https://towardsdatascience.com/towardsdatascience.com#b13b)_\n\n- [Application on an AdTech dataset](https://towardsdatascience.com/towardsdatascience.com#706a)\n- [Overview of each encoding method](https://towardsdatascience.com/towardsdatascience.com#a959) _(1) [Target encoding](https://towardsdatascience.com/towardsdatascience.com#ffbc)_\n_(2) [Count encoding](https://towardsdatascience.com/towardsdatascience.com#fbd1)_\n_(3) [Feature hashing](https://towardsdatascience.com/towardsdatascience.com#e278)_\n_(4) [Embedding](https://towardsdatascience.com/towardsdatascience.com#99d8)_\n- [Benchmarking the performance to predict CTR](https://towardsdatascience.com/towardsdatascience.com#892c)\n- [Conclusion](https://towardsdatascience.com/towardsdatascience.com#033c)\n- [To go further](https://towardsdatascience.com/towardsdatascience.com#3bd1)\n\n## **Introducing categorical features**\n\n**Categorical features** are a type of variables that describe categories or groups (e.g. gender, color, country), as opposed to **numerical features** that measure a quantity (e.g. age, height, temperature).\n\nThere are two types of categorical data: **ordinal features** which categories can be ranked and sorted (e.g. sizes of T-shirt or restaurant ratings from 1 to 5 star) and **nominal features** which categories don\u2019t imply any meaningful order (e.g. name of a person, of a city).\n\n### Why do we need to encode categorical features?\n\nEncoding a categorical variable means finding a mapping that converts a category to a numerical value.\n\nWhile some algorithms can work with categorical data directly (like decision trees), **most machine learning models cannot handle categorical features** and were designed to operate with numerical data only. Encoding categorical variables is a necessary step.\n\nBesides, some machine learning libraries require all data to be numerical. This is the case of scikit-learn for example.\n\n### Why one-hot encoding is not suited to high cardinality?\n\nA common approach to encoding categorical features is to apply **one-hot encoding**. This method encodes categorical variables by adding one binary variable for each unique category.\n\nIf a feature describing colors has three categories \\[red, blue, green\\], a one-hot encoder would transform it into three binary variables, one for each category.\n\nIf a categorical feature has hundreds or thousands of categories, applying one-hot encoding would add hundreds or thousands of binary variables to the features vector. Models struggles with large sparse data as they face the the [curse of dimensionality](https://www.quora.com/What-is-the-curse-of-dimensionality): it is more difficult to search in a solution space with more dimensions, easier to overfit, computational time is increased, as well as space complexity.\n\nSo how to to encode highly cardinal categorical features without increasing the dimensionality of feature vectors?\n\n## Application on an AdTech dataset\n\nWe will answer that question by applying four encoding techniques on the dataset of **[Criteo\u2019s Display Advertising Challenge](https://www.kaggle.com/competitions/criteo-display-ad-challenge/data)** to predict click-through rates on display ads.\n\nIt is a famous Kaggle challenge launched in 2014 by **Criteo**, a French online advertising company specialized in programmatic advertising and real time bidding. **Click through rate (CTR)** of an ad is the number of times it was clicked divided by the number of times it was displayed on a page.\n\n**Datasets in AdTech usually contain ID variables with high cardinality**, such as \\_site _id_ (ID of the website on which an ad is displayed), \\_advertiser _id_ (ID of the brand behind the ad), \\_os _id_ (ID of operating system of the user for whom the ad is displayed).\n\nThe **Criteo dataset consists of 1 million rows, 39 anonymized columns**: 13 numerical variables and 26 categorical variables. Their cardinality is in the table below. We see that **many features have very high cardinalities (above 10k)**.\n\nCardinaliy of categorical features in the Criteo dataset\n\nThe dataset contains 241,338 categories overall. **Applying one hot encoding would mean transforming the feature space from 39 dimensions to 241,351 dimensions.** It is clear that applying computations to a sparse matrix of over 241k columns is very costly and inefficient.\n\nLet us split the dataset into training and testing set and explore the encoding methods.\n\n```\nfrom sklearn.model_selection import train_test_split\n\nfeatures = df.columns[1:]\ncategorical_features = [feature for feature in features if feature[0] == \"C\" ]\nx_train, x_test, y_train, y_test = train_test_split(df[features], df[\"label\"])\n```\n\n## Overview of each encoding method\n\n### (1) Target encoding\n\nWe use the target encoder of the library [category\\_encoder](https://contrib.scikit-learn.org/category_encoders/targetencoder.html), which is defined as follows:\n\n> Features are replaced with a blend of the expected value of the target given particular categorical value and the expected value of the target over all the training data.\n\n```\nfrom category_encoders.target_encoder import TargetEncoder\n\nenc = TargetEncoder(cols = categorical_features).fit(x_train, y_train)\nX_train_encoded = enc.transform(x_train)\nX_test_encoded = enc.transform(x_test)\n```\n\nNote that we only fit the encoder on the training dataset and then use the fitted encoder to transform both training and testing set. As we do not have access to y\\_test in real life, it would be cheating to use it to fit the encoder.\n\n- **DIMENSION OF ENCODED FEATURES SPACE:** 39 columns, X\\_train\\_encoded and X\\_test\\_encoded have the same shape as x\\_train and y\\_train.\n- **PROS:**\n**\u2013** parameter free\n\n- no increase in feature space\n- **CONS:**\n**\u2013** risk of _target leakage_ (target leakage means using some information from target to predict the target itself)\n\n- When categories have few samples, the target encoder would replace them by values very close to the target which makes the model prone to overfitting the training set\n- does not accept new values in testing set\n\n### (2) Count encoding\n\nWith count encoding, **also called Frequency encoding**, categories are replaced by their frequency in the dataset. If the ID _3f4ec687_ appears 957 times in the column C7, then we would replace the _3f4ec687_ by 957.\n\nIf two categories appear the same amount of times in the dataset, such a method encodes them with the same value although they do not hold the same information. This **creates what we call a collision**: two distinct categories are encoded with the same value.\n\n```\nfrom category_encoders.count import CountEncoder\n\nenc = CountEncoder(cols = cate...",
      "url": "https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13"
    },
    {
      "title": "Handling High Cardinality Categorical Features: From Basics to Super-Advanced",
      "text": "Handling High Cardinality Categorical Features: From Basics to Super-Advanced | by Adnan Mazraeh | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Handling High Cardinality Categorical Features: From Basics to Super-Advanced\n[\n![Adnan Mazraeh](https://miro.medium.com/v2/resize:fill:64:64/1*FDIg4KL7Wt6hpW-ID7zkow.jpeg)\n](https://medium.com/@adnan.mazraeh1993?source=post_page---byline--45dd9949b31b---------------------------------------)\n[Adnan Mazraeh](https://medium.com/@adnan.mazraeh1993?source=post_page---byline--45dd9949b31b---------------------------------------)\n3 min read\n\u00b7Mar 7, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/45dd9949b31b&amp;operation=register&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;user=Adnan+Mazraeh&amp;userId=4d6bb2142c5f&amp;source=---header_actions--45dd9949b31b---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/45dd9949b31b&amp;operation=register&amp;redirect=https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b&amp;source=---header_actions--45dd9949b31b---------------------bookmark_footer------------------)\nListen\nShare\n## 1. Introduction\nHigh-cardinality categorical features refer to categorical variables with a large number of unique values (e.g., user IDs, product codes, city names). Handling them efficiently is crucial for avoiding performance issues and improving model accuracy.\n## Examples of High-Cardinality Features\n* **Zip codes**in customer data.\n* **Product IDs**in an e-commerce dataset.\n* **Device types**in a web analytics dataset.\n* **IP addresses**in cybersecurity.\n## 2. Challenges of High-Cardinality Categorical Variables\nPress enter or click to view image in full size\n![]()\n## 3. Basic Techniques for Handling High Cardinality\n## 3.1 Dropping the Feature (if Irrelevant)\n* If a high-cardinality feature does not contribute to model performance, removing it can simplify the dataset.### Python Example\n```\ndf.drop(columns=[&#x27;&#x27;product\\_id&#x27;&#x27;], inplace=True)\n```\n### R Example\n```\ndf &lt;&lt;- df[, !names(df) %in% c(&quot;&quot;product\\_id&quot;&quot;)]\n```\n## 3.2 Frequency Encoding\n* Replaces categories with their frequency counts.\n* Helps capture important information without creating too many new columns.### Python Example\n```\ndf[&#x27;&#x27;category\\_freq&#x27;&#x27;] = df[&#x27;&#x27;category&#x27;&#x27;].map(df[&#x27;&#x27;category&#x27;&#x27;].value\\_counts())\n```\n### R Example\n```\ndf$category\\_freq &lt;&lt;- ave(df$category, df$category, FUN=length)\n```\n## 3.3 One-Hot Encoding (Not Recommended for High Cardinality)\n* Converts categorical variables into binary columns (dummy variables).\n* Can cause**memory explosion**with many unique values.### Python Example\n```\nimport pandas as pd\ndf = pd.get\\_dummies(df, columns=[&#x27;&#x27;category&#x27;&#x27;])\n```\n### R Example\n```\nlibrary(caret)\ndf &lt;&lt;- dummyVars(\\~category, data=df)\ndf\\_transformed &lt;&lt;- predict(df, df)\n```\n## 4. Intermediate Techniques\n## 4.1 Target Encoding (Mean Encoding)\n* Replaces categories with the**mean of the target variable**.\n* Works well for**classification and regression**but requires careful handling to avoid**data leakage**.### Python Example\n```\ndf[&#x27;&#x27;category\\_encoded&#x27;&#x27;] = df.groupby(&#x27;&#x27;category&#x27;&#x27;)[&#x27;&#x27;target&#x27;&#x27;].transform(&#x27;&#x27;mean&#x27;&#x27;)\n```\n### R Example\n```\ndf$category\\_encoded &lt;&lt;- ave(df$target, df$category, FUN=mean)\n```\n## 4.2 Weight of Evidence (WoE) Encoding\n* Used in**binary classification**.\n* Transforms categories based on their probability of belonging to each class.### Python Example\n```\nimport numpy as np\ndf[&#x27;woe&#x27;] = np.log(df.groupby(&#x27;category&#x27;)[&#x27;target&#x27;].mean() / (1 - df.groupby(&#x27;category&#x27;)[&#x27;target&#x27;].mean()))\n```\n### R Example\n```\nlibrary(smbinning)\ndf$woe &lt;- smbinning(df, &quot;category&quot;, &quot;target&quot;)$woe\n```\n## 4.3 Hash Encoding\n* Converts categories into**hashed numerical representations**.\n* Reduces dimensionality while preserving some uniqueness.### Python Example\n```\nfrom category\\_encoders import HashingEncoder\nencoder = HashingEncoder(n\\_components=8)\ndf\\_encoded = encoder.fit\\_transform(df[&#x27;&#x27;category&#x27;&#x27;])\n```\n### R Example\n```\nlibrary(FeatureHashing)\ndf\\_encoded &lt;&lt;- hashed.model.matrix(\\~category, data=df, hash.size=8)\n```\n## 5. Advanced Techniques\n## 5.1 Embedding Encoding (Using Neural Networks)\n* Converts categorical variables into**low-dimensional dense vectors**.\n* Requires**deep learning**frameworks like**TensorFlow**or**PyTorch**.### Python Example (TensorFlow)\n```\nimport tensorflow as tf\nimport numpy as np\n```\n```\nvocab\\_size = df[&#x27;&#x27;category&#x27;&#x27;].nunique()\nembedding\\_dim = 10 # Adjust based on dataset sizeembedding\\_layer = tf.keras.layers.Embedding(input\\_dim=vocab\\_size, output\\_dim=embedding\\_dim)\n```\n### R Example\n```\nlibrary(keras)\nembedding\\_layer &lt;&lt;- layer\\_embedding(input\\_dim = length(unique(df$category)), output\\_dim = 10)\n```\n## 5.2 Principal Component Analysis (PCA) on Encoded Data\n* Reduces the**dimensionality**of One-Hot or Hash Encoded data.\n* Useful when the number of categories is**too high**.### Python Example\n```\nfrom sklearn.decomposition import PCA\npca = PCA(n\\_components=10)\ndf\\_pca = pca.fit\\_transform(df\\_encoded)\n```\n### R Example\n```\ndf\\_pca &lt;&lt;- prcomp(df\\_encoded, center=TRUE, scale=TRUE)\n```\n## 5.3 Clustering-Based Encoding\n* Groups similar categories using**K-means clustering**.\n* Categories with similar effects are assigned the same cluster.### Python Example\n```\nfrom sklearn.cluster import KMeans\nn\\_clusters = 5\nkmeans = KMeans(n\\_clusters=n\\_clusters)\ndf[&#x27;&#x27;category\\_cluster&#x27;&#x27;] = kmeans.fit\\_predict(df[[&#x27;&#x27;category\\_encoded&#x27;&#x27;]])\n```\n### R Example\n```\nlibrary(cluster)\ndf$category\\_cluster &lt;&lt;- kmeans(df$category\\_encoded, centers=5)$cluster\n```\n## 6. Best Practices for Handling High-Cardinality Categorical Features\nPress enter or click to view image in full size\n![]()\n## 7. Libraries for Handling High-Cardinality Categorical Data\n## Python Libraries\nPress enter or click to view image in full size\n![]()\n## R Libraries\nPress enter or click to view image in full size\n![]()\n## 8. Summary\n1. **Start Simple**: If the categorical feature is unimportant, remove it.\n2. **Use Frequency Encoding**: If uniqueness does not add meaningful information.\n3. **Leverage...",
      "url": "https://medium.com/@adnan.mazraeh1993/handling-high-cardinality-categorical-features-from-basics-to-super-advanced-45dd9949b31b"
    },
    {
      "title": "Encoding Categorical Variables: A Deep Dive into Target Encoding",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2862217c2753&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fencoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fencoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-2862217c2753---------------------------------------)\n\n\u00b7\n\n[![TDS Archive](https://miro.medium.com/v2/resize:fill:76:76/1*JEuS4KBdakUcjg9sC7Wo4A.png)](https://medium.com/data-science?source=post_page---post_publication_sidebar-7f60cf5620c9-2862217c2753---------------------------------------)\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\n# Encoding Categorical Variables: A Deep Dive into Target Encoding\n\n## Data comes in different shapes and forms. One of those shapes and forms is known as categorical data.\n\n[![Juan Jose Munoz](https://miro.medium.com/v2/resize:fill:64:64/1*YeUOkLrhcC48xk632N_gjw.jpeg)](https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753---------------------------------------)\n\n[Juan Jose Munoz](https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753---------------------------------------)\n\nFollow\n\n10 min read\n\n\u00b7\n\nFeb 5, 2024\n\n--\n\n4\n\nListen\n\nShare\n\n**This poses a problem because most Machine Learning algorithms use only numerical data as input**. However, categorical data is usually not a challenge to deal with, thanks to simple, well-defined functions that transform them into numerical values. If you have taken any data science course, you will be familiar with the one hot encoding strategy for categorical features. This strategy is great when your features have limited categories. However, you will run into some issues when dealing with high cardinal features (features with many categories)\n\n**Here is how you can use target encoding to transform Categorical features into numerical values.**\n\nPhoto by [Sonika Agarwal](https://unsplash.com/@sonika_agarwal?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\n# The problem with One Hot encoding\n\n**Early in any data science course, you are introduced to one hot encoding as a key strategy to deal with categorical values**, and rightfully so, as this strategy works really well on low cardinal features (features with limited categories).\n\n**In a nutshell, One hot encoding transforms each category into a binary vector,** where the corresponding category is marked as \u2018True\u2019 or \u20181\u2019, and all other categories are marked with \u2018False\u2019 or \u20180\u2019.\n\n```\nimport pandas as pd\n\n# Sample categorical data\ndata = {'Category': ['Red', 'Green', 'Blue', 'Red', 'Green']}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Perform one-hot encoding\none_hot_encoded = pd.get_dummies(df['Category'])\n\n# Display the result\nprint(one_hot_encoded)\n```\n\nOne hot encoding output \u2014 we could improve this by dropping one column because if we know Blue and Green, we can figure the value of Red. Image by author\n\nWhile this works great for features with limited categories _(Less than 10\u201320 categories)_, as the number of categories increases, the one-hot encoded vectors become longer and sparser, potentially leading to increased memory usage and computational complexity, let\u2019s look at an example.\n\n_The below code uses Amazon Employee Access data, made publicity available in kaggle:_ [_https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge_](https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge)\n\nThe data contains eight categorical feature columns indicating characteristics of the required resource, role, and workgroup of the employee at Amazon.\n\n```\ndata.info()\n```\n\nColumn information. Image by author\n\n```\n# Display the number of unique values in each column\nunique_values_per_column = data.nunique()\n\nprint(\"Number of unique values in each column:\")\nprint(unique_values_per_column)\n```\n\nThe eight features have high cardinality. Image by author\n\n**Using one hot encoding could be challenging in a dataset like this due to the high number of distinct categories for each feature.**\n\n```\n#Initial data memory usage\nmemory_usage = data.memory_usage(deep=True)\ntotal_memory_usage = memory_usage.sum()\nprint(f\"\\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB\")\n```\n\nThe initial dataset is 11.24 MB. Image by author\n\n```\n#one-hot encoding categorical features\ndata_encoded = pd.get_dummies(data,\n                              columns=data.select_dtypes(include='object').columns,\n                              drop_first=True)\n\ndata_encoded.shape\n```\n\nAfter on-hot encoding, the dataset has 15 618 columns. Image by author\n\nThe resulting data set is highly sparse, meaning it contains a lot of 0s and 1. Image by author\n\n```\n# Memory usage for the one-hot encoded dataset\nmemory_usage = data_encoded.memory_usage(deep=True)\ntotal_memory_usage = memory_usage.sum()\nprint(f\"\\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB\")\n```\n\nDataset memory usage increased to 488.08 MB due to the increased number of columns. Image by author\n\nAs you can see, one-hot encoding is not a viable solution to deal with high cardinal categorical features, as it significantly increases the size of the dataset.\n\n**In cases with high cardinal features, target encoding is a better option.**\n\n# Target encoding \u2014 overview of basic principle\n\nTarget encoding transforms a categorical feature into a numeric feature without adding any extra columns, avoiding turning the dataset into a larger and sparser dataset.\n\n**Target encoding works by converting each category of a categorical feature into its corresponding expected value.** The approach to calculating the expected value will depend on the value you are trying to predict.\n\n> For Regression problems, the expected value is simply the average value for that category.\n>\n> For Classification problems, the expected value is the conditional probability given that category.\n\nIn both cases, we can get the results by simply using the \u2018group\\_by\u2019 function in pandas.\n\n```\n#Example of how to calculate the expected value for Target encoding of a Binary outcome\nexpected_values = data.groupby('ROLE_TITLE')['ACTION'].value_counts(normalize=True).unstack()\nexpected_values\n```\n\nThe resulting table indicates the probability of each \\`ACTION\\` outcome by unique \\`Role\\_title\\` ID. Image by author\n\nThe resulting table indicates the probability of each \u201c _ACTION\u201d_ outcome by unique \u201c _ROLE\\_TITLE_\u201d id. All that is left to do is replace the \u201c _ROLE\\_TITLE_\u201d id with the values from the probability of \u201cACTION\u201d being 1 in the original dataset. _(i.e instead of category 117879 the dataset will show 0.889331)_\n\n**While this can give us an intuition of how target encoding works, using this simple method runs the risk of overfitting**. Especially for rare categories, as in those cases, target encoding will essentially provide the target value to the model. Also, the above method can onl...",
      "url": "https://medium.com/data-science/encoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753"
    },
    {
      "title": "Categorical Encoding",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/pdf/2401.09682"
    },
    {
      "title": "Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features",
      "text": "Advertisement\n\n# Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features\n\n- Original paper\n- [Open access](https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 04 March 2022\n\n- Volume\u00a037,\u00a0pages 2671\u20132692, (2022)\n- [Cite this article](https://link.springer.com/article/10.1007/s00180-022-01207-6?error=cookies_not_supported&code=ed0a5798-ce1d-42ad-81d2-5649a3c568dd#citeas)\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s00180-022-01207-6.pdf)\n\nYou have full access to this [open access](https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research) article\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/journal/180?as=webp)Computational Statistics](https://link.springer.com/journal/180) [Aims and scope](https://link.springer.com/journal/180/aims-and-scope) [Submit manuscript](https://www.editorialmanager.com/cost/)\n\nRegularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s00180-022-01207-6.pdf)\n\n## Abstract\n\nSince most machine learning (ML) algorithms are designed for numerical inputs, efficiently encoding categorical variables is a crucial aspect in data analysis. A common problem are high cardinality features, i.e.\u00a0unordered categorical predictor variables with a high number of levels. We study techniques that yield numeric representations of categorical variables which can then be used in subsequent ML applications. We focus on the impact of these techniques on a subsequent algorithm\u2019s predictive performance, and\u2014if possible\u2014derive best practices on when to use which technique. We conducted a large-scale benchmark experiment, where we compared different encoding strategies together with five ML algorithms (lasso, random forest, gradient boosting, _k_-nearest neighbors, support vector machine) using datasets from regression, binary- and multiclass\u2013classification settings. In our study, regularized versions of target encoding (i.e.\u00a0using target predictions based on the feature levels in the training set as a new numerical feature) consistently provided the best results. Traditionally widely used encodings that make unreasonable assumptions to map levels to integers (e.g.\u00a0integer encoding) or to reduce the number of levels (possibly based on target information, e.g.\u00a0leaf encoding) before creating binary indicator variables (one-hot or dummy encoding) were not as effective in comparison.\n\n### Similar content being viewed by others\n\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-02056-8?as=webp)\n\n### [SLUG: Feature Selection Using Genetic Algorithms and\u00a0Genetic Programming](https://link.springer.com/10.1007/978-3-031-02056-8_5?fromPaywallRec=false)\n\nChapter\u00a9 2022\n\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-45093-9?as=webp)\n\n### [Data Aggregation for Reducing Training Data in Symbolic Regression](https://link.springer.com/10.1007/978-3-030-45093-9_46?fromPaywallRec=false)\n\nChapter\u00a9 2020\n\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-70411-6?as=webp)\n\n### [Smart Data Simplification: A Comprehensive Feature Selection Framework for High-Dimensional Datasets](https://link.springer.com/10.1007/978-3-031-70411-6_28?fromPaywallRec=false)\n\nChapter\u00a9 2024\n\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=180)\n\nAvoid common mistakes on your manuscript.\n\n## 1 Introduction\n\nWhile increasing sample size is usually considered the most important step to improve the predictive performance of a machine learning (ML) model, using effective feature engineering comes as a close second. One remaining challenge is how to handle high cardinality features\u2014categorical predictor variables with a high number of different levels but without any natural ordering. While categorical variables with only a small number of possible levels can often be efficiently dealt with using standard techniques such as one-hot encoding, this approach becomes inefficient as the number of levels increases. Despite this inefficiency, simpler strategies are often favored in practice because other methods are either not known, implementations are missing or because of a lack of trust due to missing validation studies. Although domain knowledge can sometimes be used to reduce the number of theoretically relevant levels, finding strategies that work well on a large variety of problems is highly important for many applications as well as in automated ML (Feurer et\u00a0al. [2015](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR18); Thomas et\u00a0al [2018](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR43); Thornton et\u00a0al. [2013](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR44)). Optimally, strategies should be model-agnostic because benchmarking encoding methods together with ML algorithms from different classes is often necessary for applications. While a variety of strategies exist, there are very few benchmarks that can be used to decide which technique is expected to yield good predictive performance. Furthermore, there has recently been increasing attention on scientific benchmark studies that compare different methods to provide a clearer picture in light of a large number of methods available to practitioners (Bommert et\u00a0al. [2020](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR5); Fern\u00e1ndez-Delgado et\u00a0al. [2014](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR16)), as they can provide at least partial answers to such questions. The goal of this study is to provide an overview of existing approaches for encoding categorical predictor variables and to study their effect on a model\u2019s predictive performance. Following calls in the computational statistics community for neutral benchmark studies (Boulesteix et\u00a0al. [2017](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR6)), which do not introduce a new method, thus reducing the risk of cherry picking methods (Dehghani et\u00a0al. [2021](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR15)) and reporting over-optimistic performance (Nie\u00dfl et\u00a0al. [2021](https://link.springer.com/article/10.1007/s00180-022-01207-6#ref-CR33)), we present a carefully designed experimental setting to discern the effect of encoding strategies and their interaction with different ML algorithms.\n\n### 1.1 Notation\n\nWe consider the classical setting of supervised learning from an \\\\(i.i.d.\\\\) tabular dataset \\\\(\\\\mathcal {D}\\\\) of size \\\\(N\\\\) sampled from a joint distribution \\\\(\\\\mathcal {P}(\\\\varvec{x}, y)\\\\) of a set of features \\\\(\\\\varvec{x}\\\\) and an associated target variable \\\\(y\\\\). Here, \\\\(\\\\varvec{x}\\\\) consists of a mix of numeric (real-valued or integer-valued) features and categorical features, the latter of which we seek to transform feature-wise to numeric features using a categorical encoding technique. Let \\\\(x\\\\) be a single unordered categorical feature from a feature space \\\\(\\\\mathcal {X}\\\\) with cardinality \\\\(card(\\\\mathcal {X}) \\\\le card(\\\\mathbb {N})\\\\). It holds either \\\\(y \\\\in \\\\mathbb {R}\\\\) (regression), \\\\(y \\\\in \\\\mathcal {C}\\\\) from a finite class space \\\\(\\\\mathcal {C} = \\\\{c\\_1,\\\\ldots , c\\_C\\\\}\\\\) with \\\\(C = 2\\\\) (binary classification) or \\\\(C > 2\\\\) (multiclass classification). We always assume to observe all \\\\(C\\\\) classes in our training sample, however we might only observe a subset \\\\(\\\\mathcal {L}^{train} \\\\subseteq \\\\mathcal {X}\\\\) of a feature\u2019s available \\\\(L\\\\) levels, \\\\(\\\\mathcal {L}^{train} = \\\\{l\\_1, \\\\ldots , l\\_L\\\\}\\\\) for categorical features. We denote the observed frequency of class \\\\(c\\\\) in the train...",
      "url": "https://link.springer.com/article/10.1007/s00180-022-01207-6?error=cookies_not_supported&code=ed0a5798-ce1d-42ad-81d2-5649a3c568dd"
    }
  ]
}