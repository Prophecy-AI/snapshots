{
  "query": "What are the winning ensembling strategies for binary classification competitions on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensembling is the cornerstone of most Kaggle\u2011winning solutions for binary classification.  The idea is to combine several diverse models so that their individual errors cancel out, yielding a stronger predictor than any single model could achieve\u202f\u2014\u202fespecially when the base learners are low\u2011correlated and have high variance (e.g., decision\u2011tree ensembles, gradient\u2011boosted trees, neural nets)\u202f([toptal](https://www.toptal.com/machine-learning/ensemble-methods-kaggle-machine-learn)).  \n\nThe most reliable tactics fall into three families. **Simple blending** averages the predicted probabilities from many models; an arithmetic mean works for both regression and classification, while a geometric mean often gives a slight edge for binary probabilities\u202f([datacamp](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8)).  Practitioners also weight the averages by validation scores or use rank\u2011based averaging to reduce the impact of outliers\u202f([kdnuggets](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p2.html)).  **Voting ensembles** take the majority class (or majority\u2011vote of probabilities) and are especially useful when some models produce crisp class labels\u202f([cnblogs](https://www.cnblogs.com/medsci/articles/9160663.html)).  \n\n**Stacking (or stacked generalization)** is the most powerful and widely used approach in top Kaggle binaries.  The workflow is: train several level\u20111 models (e.g., Random Forest, ExtraTrees, LightGBM, CatBoost) with K\u2011fold cross\u2011validation, collect their out\u2011of\u2011fold predictions, and feed those predictions as features to a level\u20112 meta\u2011learner (often another gradient\u2011boosted tree).  Multi\u2011level stacking and blending\u2014sometimes involving hundreds of base models\u2014were key to the winning Homesite Quote Conversion solution\u202f([datacamp](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8));\u202f([kdnuggets](https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html)).  Because the meta\u2011model is trained on predictions rather than raw data, it learns how to weight each base learner optimally, further reducing over\u2011fit.  \n\nFinally, many competitors create **submission\u2011file ensembles** without retraining: they simply combine the CSV predictions of different teammates or public kernels using the averaging or rank methods described above.  This \u201cquick\u2011blend\u201d step can add a few points to the leaderboard and is especially handy in the final stages of a competition\u202f([cnblogs](https://www.cnblogs.com/medsci/articles/9160663.html)).  In practice, a winning Kaggle binary\u2011classification pipeline typically mixes these techniques\u2014starting with a diverse set of high\u2011variance models, generating robust out\u2011of\u2011fold predictions, and then applying weighted averaging, geometric means, or multi\u2011layer stacking to produce the final submission.",
      "url": ""
    },
    {
      "title": "Model ensembling | Python",
      "text": "Model ensembling | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8\nModel ensembling | Python\nNone\n2022-06-13T00:00:00Z\n# Model ensembling\n####. Model ensembling\nSo far, we've been talking only about individual models. Now it's time to combine multiple models together.\n####. Model ensembling\nKaggle top solutions are usually not a single model, but a combination of a large number of various models. Different ways to combine models together is called model ensembling.\nFor example, here is an ensemble design for a winning solution in the Homesite Quote Conversion challenge. We can see hundreds of models with multi-level stacking and blending. Let's learn about these 'blending' and 'stacking' terms.\n####. Model blending\nThe idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models. The so-called blending approach is to just find an average of our multiple models predictions.\nSay we're solving a regression problem with a continuous target variable.\nAnd we have trained two models: A and B.\nSo, for each test observation, there are model A and model B predictions available.\n####. Model blending\nTo combine models together we can just find the predictions mean, taking the sum and divide it by two. As we see, it allows us to tweak predictions, and to take into account both model A and model B opinions.\nThat's it, such a simple ensembling method in the majority of cases will yield some improvement to our single models.\n####. Model blending\nArithmetic mean works for both regression and classification problems. However, for the classification, it's better to use a geometric mean of the class probabilities predicted.\n####. Model stacking\nThe more advanced ensembling approach is called model stacking. The idea is to train multiple single models, take their predictions and use these predictions as features in the 2nd level model.\nSo, we need to perform the following steps:\nSplit train data into two parts. Part 1 and Part 2.\nTrain multiple single models on the first part.\nMake predictions on the second part of the train data,\nand on the test data. Now, we have models predictions for both Part 2 of the train data and for the test data.\nIt means that we could create a new model using these predictions as features. This model is called the 2nd level model or meta-model.\nIts predictions on the test data give us the stacking output.\n####. Stacking example\nLet's consider all these steps on the example.\nSuppose we are given a binary classification problem with a bunch of numerical features: feature_1, feature_2 and so on to feature_N. For the train data, target variable is known.\nAnd we need to make predictions on the test data with the unknown target variable.\n####. Stacking example\nFirst of all, we split train data into two separate parts: Part 1 and Part 2.\n####. Stacking example\nThen we train multiple single models only on the first part of the train data. For example, we've trained three different models denoting them as A, B and C.\n####. Stacking example\nHaving these 3 models we make the predictions on part 2 of the train data. The columns with the predictions are denoted as A_pred, B_pred and C_pred.\nThen make the predictions on the test data as well.\n####. Stacking example\nSo, now we have models predictions for both Part 2 of the train data and for the test data.\n####. Stacking example\nIt's now possible to create a second level model using these predictions as features. It's trained on the Part 2 train data and is used to make predictions on the test data.\n####. Stacking example\nAs a result, we obtain stacking predictions for the test data. Thus, we combined individual model predictions into a single number using a 2nd level model.\n####. Let's practice!\nOK, having learned the theory, move on to build your own model ensembles!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8"
    },
    {
      "title": "Kaggle Champions: Ensemble Methods in Machine Learning | Toptal\u00ae",
      "text": "<div><div>\n<p>The proverb \u201cTwo heads are better than one\u201d takes on new meaning when it comes to <a href=\"https://www.toptal.com/machine-learning\">machine learning</a> ensembles. Ensemble methods are some of the most decorated ML families at Kaggle competitions, where they often win contests with their impressive results.</p>\n<p>But it was a century before Kaggle when statistician Sir Francis Galton noticed the potency of aggregated intelligence. He happened upon a competition at a livestock fair where participants had to guess the weight of an ox. Eight hundred people submitted guesses, but their skill levels varied: Farmers and butchers guessed alongside city dwellers who had never seen an ox up close, so Galton thought the average guess would be quite wrong.</p>\n<p>It <a href=\"https://projecteuclid.org/download/pdfview_1/euclid.ss/1411437521\">turned out</a> that the mean of the crowd\u2019s guesses was off by less than a pound (&lt; 0.1%). However, even the best individual predictions were well off the mark.</p>\n<p>How could that be? What made such an unexpected result possible?</p>\n<h2>What Makes Machine Ensembles so Effective</h2>\n<p>The event that forced Galton to question his beliefs also illustrates what makes ensembles so powerful: If you have <strong>different</strong> <strong>and independent models</strong>, trained using <strong>different parts of data</strong> for the same problem, they will work better together than individually. The reason? Each model will learn a different part of the concept. Therefore, each model will produce valid results and errors based on its \u201cknowledge.\u201d</p>\n<p>But the most interesting thing is that each true part will complement the others while the errors cancel out each other:</p>\n<div><figure><a href=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/750064/image-1631891397968-d95bfcfccb53db7971c25b2a53222b3f.png\"></a></figure></div>\n<p>You need to train models with high variance (like decision trees) over distinct subsets of data. This added variance means that each model overfits different data, but when combined, the variance disappears, as if by magic. This creates a new, more robust model.</p>\n<p>Just like in Galton\u2019s case, when all data from all sources is combined, the result is \u201csmarter\u201d than isolated data points.</p>\n<h3>A Closer Look at Ensemble Learning in Kaggle Competitions</h3>\n<p>At the <a href=\"https://www.kaggle.com/c/otto-group-product-classification-challenge/overview\">Otto Group Product Classification Challenge</a>, participants had to build a predictive model that was able to distinguish between main product categories.</p>\n<p>Here you can see how <a href=\"https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\">the winning model was built</a>. It was a stacking of three layers: The first had 33 models, the second added three more (XGBoost, a neural network, and AdaBoost), and the third was the weighted mean of the previous layer outputs. It was both a very complex model and an ensemble.</p>\n<p>Another Kaggle success is the model created by Chenglong Chen at the <a href=\"https://www.kaggle.com/c/crowdflower-search-relevance/overview\">Crowdflower Search Results Relevance competition</a>. The challenge was to create a predictor that could be used to measure the relevance of search results. You can read the complete <a href=\"https://github.com/ChenglongChen/kaggle-CrowdFlower\">explanation of his method</a>, but as our point of interest is ensembles, the critical part of the story is that the winning solution used an ensemble of 35 models, many of which were also ensembles\u2014a meta-ensemble, so to speak.</p>\n<h2>Ensemble Methods</h2>\n<p>There are many ways of implementing ensemble methods in machine learning. We\u2019ll explore some of the most popular methods:</p>\n<ul>\n <li>\n<strong>Bagging</strong>\n <ul>\n <li>Random Forest</li>\n </ul>\n </li>\n <li>\n<strong>Boosting</strong>\n <ul>\n <li>AdaBoost</li>\n <li>Gradient Boosting and XGBoost</li>\n </ul>\n </li>\n <li>\n<strong>Hybrid Ensemble Methods</strong>\n <ul>\n <li>Voting</li>\n <li>Stacking</li>\n <li>Cascading</li>\n </ul>\n </li>\n</ul>\n<h2>Bagging</h2>\n<p>As mentioned, you need to train many models over different subsets of data. In practice, this is not easy because you will need much more data for many models than you would for a single model, and sometimes it is not easy to obtain high-quality datasets. This is when bagging (bootstrap aggregating) comes in handy, as it splits the data through bootstrapping: a random sample with a replacement, resulting in different subsets that overlap.</p>\n<p>Once you have trained your ensemble models, you construct your final prediction by aggregating each model prediction through any metric you prefer: the mean, median, mode, and so on. You can also use model prediction probabilities to make a weighted metric:</p>\n<div><figure><a href=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/750065/image-1631891438024-8c044ee5d973b58299ba8a7a56d1484c.png\"></a></figure></div>\n<p>If we want to use <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">decision trees</a> as models but we have few strong predictive attributes in our data, all the trees will be similar. This is because the same attributes will tend to be in the root node, producing similar results in each branch of the tree.</p>\n<h3>Random Forest</h3>\n<p>One technique to address this problem is <strong>random forest</strong>. It makes a bagging ensemble using trees but each node constrains its possible attributes to a random subset. This forces the models to be different, resolving the previous problem. It also makes random forest a very good model for <a href=\"https://en.wikipedia.org/wiki/Feature_selection\">feature selection</a>.</p>\n<p>Random forest is one of the most popular ML models because it delivers good performance with low variance and training time.</p>\n<h2>Boosting</h2>\n<p>Boosting also uses bootstrapping to train the models, with the main difference being that it adds weights to each instance based on model prediction errors. While bagging is a parallel process, boosting is a sequential one, in which each model has more probabilities. This allows it access to some instances of previous model predictions.</p>\n<p>With this modification, boosting tries to increase the focus over misclassified instances to reach better global performance:</p>\n<div><figure><a href=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/750066/image-1631891461357-200322840b31404060220a892663aa15.png\"></a></figure></div>\n<p>It adds weights to models too. Predictors with better performance at training time will have a higher weight at the predicting stage.</p>\n<p>Let\u2019s take a closer look at some of the most popular boosting models:</p>\n<h3>AdaBoost</h3>\n<p>AdaBoost was one of the first implementations of boosting. It does almost exactly what we outlined about boosting in general and uses decision trees as models. Let\u2019s explain the training phase with some pseudo coding:</p>\n<pre><code>For each instance i\n Assign w[i] (weight, same for all)\nFor each iteration t\n Generate a subset s[t] by weighted boosting (using the w[i] weights)\n Train model m[t] using s[t]\n Store m[t]\n Calculate e[t] (error rate of m[t])\n Assign error rate e[t] to stored m[t] model\n If (e[t] &lt;= a_threshold)\n Exit for\n Update weights using m[t] errors\n</code></pre>\n<p>At prediction time, it weights each prediction based on the error rate <code>e[t]</code> calculated for each one. Results with a high error rate will have less weight than others with better accuracy.</p>\n<h3>Gradient Boosting and XGBoost</h3>\n<p>One of the major problems when training so many models and making them work together is finding the best hyperparameter configuration. It is difficult to find the best configuration for a single model; finding the best configuration for <em>n</em> models increases the complexity exponentially. The ideal configuration fo...",
      "url": "https://www.toptal.com/machine-learning/ensemble-methods-kaggle-machine-learn"
    },
    {
      "title": "KAGGLE ENSEMBLING GUIDE",
      "text": "[\u535a\u5ba2\u56ed](https://www.cnblogs.com/) [\u9996\u9875](https://www.cnblogs.com/medsci/) [\u65b0\u968f\u7b14](https://i.cnblogs.com/EditPosts.aspx?opt=1) [\u8054\u7cfb](https://msg.cnblogs.com/send/medsci) [\u8ba2\u9605](javascript:void(0)) [\u7ba1\u7406](https://i.cnblogs.com/)\n\n# [KAGGLE ENSEMBLING GUIDE](https://www.cnblogs.com/medsci/articles/9160663.html)\n\n\u8f6c\u8f7d\u81ea\u00a0 https://mlwave.com/kaggle-ensembling-guide/\n\ngithub:\u00a0 https://github.com/MLWave/Kaggle-Ensemble-Guide\n\nModel ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions.\n\nFor the first part we look at creating ensembles from submission files. The second part will look at creating ensembles through stacked generalization/blending.\n\nI answer why ensembling reduces the generalization error. Finally I show different methods of ensembling, together with their results and code to try it out for yourself.\n\n> This is how you win ML competitions: you take other peoples\u2019 work and ensemble them together.\u201d\u00a0[Vitaly Kuznetsov](http://cims.nyu.edu/~vitaly/)\u00a0NIPS2014\n\n## Creating ensembles from submission files\n\nThe most basic and convenient way to ensemble is to ensemble Kaggle submission CSV files. You only need the predictions on the test set for these methods \u2014 no need to retrain a model. This makes it a quick way to ensemble already existing model predictions, ideal when teaming up.\n\n### Voting ensembles.\n\nWe first take a look at a simple majority vote ensemble. Let\u2019s see why model ensembling reduces error rate and why it works better to ensemble low-correlated model predictions.\n\n#### Error correcting codes\n\nDuring space missions it is very important that all signals are correctly relayed.\n\nIf we have a signal in the form of a binary string like:\n\n```\n1110110011101111011111011011\n\n```\n\nand somehow this signal is corrupted (a bit is flipped) to:\n\n```\n1010110011101111011111011011\n\n```\n\nthen lives could be lost.\n\nA\u00a0[coding](http://en.wikipedia.org/wiki/Coding_theory)\u00a0solution was found in\u00a0[error correcting codes](http://en.wikipedia.org/wiki/Forward_error_correction). The simplest error correcting code is a\u00a0[repetition-code](http://en.wikipedia.org/wiki/Repetition_code): Relay the signal multiple times in equally sized chunks and have a majority vote.\n\n```\nOriginal signal:\n1110110011\n\nEncoded:\n10,3 101011001111101100111110110011\n\nDecoding:\n1010110011\n1110110011\n1110110011\n\nMajority vote:\n1110110011\n\n```\n\nSignal corruption is a very rare occurrence and often occur in small bursts. So then it figures that it is even rarer to have a corrupted majority vote.\n\nAs long as the corruption is not completely unpredictable (has a 50% chance of occurring) then signals can be repaired.\n\n#### A machine learning example\n\nSuppose we have a test set of 10 samples. The ground truth is all positive (\u201c1\u201d):\n\n```\n1111111111\n\n```\n\nWe furthermore have 3 binary classifiers (A,B,C) with a 70% accuracy. You can view these classifiers for now as pseudo-random number generators which output a \u201c1\u201d 70% of the time and a \u201c0\u201d 30% of the time.\n\nWe will now show how these pseudo-classifiers are able to obtain 78% accuracy through a voting ensemble.\n\n##### A pinch of maths\n\nFor a majority vote with 3 members we can expect 4 outcomes:\n\n```\nAll three are correct\n  0.7 * 0.7 * 0.7\n= 0.3429\n\nTwo are correct\n  0.7 * 0.7 * 0.3\n+ 0.7 * 0.3 * 0.7\n+ 0.3 * 0.7 * 0.7\n= 0.4409\n\nTwo are wrong\n  0.3 * 0.3 * 0.7\n+ 0.3 * 0.7 * 0.3\n+ 0.7 * 0.3 * 0.3\n= 0.189\n\nAll three are wrong\n  0.3 * 0.3 * 0.3\n= 0.027\n\n```\n\nWe see that most of the times (~44%) the majority vote corrects an error. This majority vote ensemble will be correct an average of ~78% (0.3429 + 0.4409 = 0.7838).\n\n#### Number of voters\n\nLike repetition codes increase in their error-correcting capability when more codes are repeated, so do ensembles usually improve when adding more ensemble members.\n\nUsing the same pinch of maths as above: a voting ensemble of 5 pseudo-random classifiers with 70% accuracy would be correct ~83% of the time. One or two errors are being corrected during ~66% of the majority votes. (0.36015 + 0.3087)\n\n#### Correlation\n\nWhen I first joined the team for KDD-cup 2014, Marios Michailidis ( [KazAnova](https://www.kaggle.com/kazanova)) proposed something peculiar. He calculated the\u00a0[Pearson correlation](http://onlinestatbook.com/2/describing_bivariate_data/pearson.html)\u00a0for all our submission files and gathered a few well-performing models which were less correlated.\n\nCreating an averaging ensemble from these diverse submissions gave us the\u00a0biggest 50-spot jump on the leaderboard. Uncorrelated submissions clearly do better when ensembled than correlated submissions. But why?\n\nTo see this, let us take 3 simple models again. The ground truth is still all 1\u2019s:\n\n```\n1111111100 = 80% accuracy\n1111111100 = 80% accuracy\n1011111100 = 70% accuracy.\n\n```\n\nThese models are highly correlated in their predictions. When we take a majority vote we see no improvement:\n\n```\n1111111100 = 80% accuracy\n\n```\n\nNow we compare to 3 less-performing, but highly uncorrelated models:\n\n```\n1111111100 = 80% accuracy\n0111011101 = 70% accuracy\n1000101111 = 60% accuracy\n\n```\n\nWhen we ensemble this with a majority vote we get:\n\n```\n1111111101 = 90% accuracy\n\n```\n\nWhich\u00a0_is_\u00a0an improvement: A lower correlation between ensemble model members seems to result in an increase in the error-correcting capability.\n\n#### Use for Kaggle: Forest Cover Type prediction\n\nMajority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.\n\nThe\u00a0[forest cover type prediction](https://www.kaggle.com/c/forest-cover-type-prediction)\u00a0challenge uses the\u00a0[UCI Forest CoverType dataset](https://archive.ics.uci.edu/ml/datasets/Covertype). The dataset has\u00a054 attributes and there are 6 classes.\n\nWe create a simple\u00a0[starter model](https://www.kaggle.com/triskelion/forest-cover-type-prediction/first-try-with-random-forests)\u00a0with a 500-tree Random Forest. We then create a few more models and pick the best performing one. For this task and our model selection an ExtraTreesClassifier works best.\n\n##### Weighing\n\nWe then use a weighted majority vote. Why weighing? Usually we want to give a better model more weight in a vote. So in our case we count the vote by the best model 3 times.\u00a0The other 4 models count for one vote each.\n\nThe reasoning is as follows: The only way for the inferior models to overrule the best model (expert) is for them to\u00a0collectively (and confidently) agree on an alternative.\n\nWe can expect this ensemble to repair a few erroneous choices by the best model, leading to a small improvement only.\u00a0That\u2019s our punishment for forgoing a democracy and creating a Plato\u2019s\u00a0_Republic_.\n\n> \u201cEvery city encompasses two cities that are at war with each other.\u201d\u00a0Plato in\u00a0The Republic\n\nTable 1. shows the result of training 5 models, and the resulting score when combining these with a weighted majority vote.\n\n| MODEL | PUBLIC ACCURACY SCORE |\n| --- | --- |\n| GradientBoostingMachine | 0.65057 |\n| RandomForest Gini | 0.75107 |\n| RandomForest Entropy | 0.75222 |\n| ExtraTrees Entropy | 0.75524 |\n| ExtraTrees Gini (Best) | 0.75571 |\n| Voting Ensemble (Democracy) | 0.75337 |\n| Voting Ensemble (3\\*Best vs. Rest) | 0.75667 |\n\n#### Use for Kaggle: CIFAR-10 Object detection in images\n\nCIFAR-10 is another multi-class classification challenge where accuracy matters.\n\nOur team leader for this challenge,\u00a0[Phil Culliton](https://www.kaggle.com/philculliton), first found the best setup to\u00a0[replicate a good model](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/)\u00a0from dr. Graham.\n\nThen he used a voting ensemble of around 30 convnets submissions (all scoring above 90% accuracy). The best\u00a0single model of the ensemble scored\u00a00.93170.\n\nA voting ensemble of 30 models scored\u00a00.94120. A ~0.01 reduction in error rate, pushing the resulting score beyond the\u00a0[estimated human clas...",
      "url": "https://www.cnblogs.com/medsci/articles/9160663.html"
    },
    {
      "title": "",
      "text": "# Using Ensembles in Kaggle Data Science Competitions \u2013 Part 2\n\nAspiring to be a Top Kaggler? Learn more methods like Stacking & Blending. In the previous post we discussed about ensembling models by ways of weighing, averaging and ranks. There is much more to explore in Part-2!\n\n* * *\n\n![c](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2012%2012'%3E%3C/svg%3E)[comments](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html#comments)\n\n**By Henk van Veen**\n\n**Stacked Generalization & Blending**\n\nAveraging prediction files is nice and easy, but it\u2019s not the only method that the top Kagglers [Repetition code](https://www.kaggle.com/users) are using. The serious gains start with stacking and blending. Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. Standing on top of 30 other dragons.\n\n**Netflix**\n\n[![This image shows netflix leaderboard results with blending hundreds of predictive models](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20213'%3E%3C/svg%3E)](https://www.kdnuggets.com/wp-content/uploads/compiling-blending-predictive-models-by-netflix-engineers.jpg)\n\nBlending hundreds of predictive models to finally cross the finish line.\n\nNetflix organized and popularized the first data science competitions. Competitors in the movie recommendation challenge really pushed the state of the art on ensemble creation, perhaps so much so that Netflix decided not to implement the winning solution in production. That one was simply too complex.\n\nNevertheless, a number of papers and novel methods resulted from this challenge:\n\n- [Feature-Weighted Linear Stacking](http://arxiv.org/pdf/0911.0460.pdf)\n- [Combining Predictions for Accurate Recommender Systems](http://elf-project.sourceforge.net/CombiningPredictionsForAccurateRecommenderSystems.pdf%20)\n- [The BigChaos Solution to the Netflix Prize](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf%20)\n\nAll are interesting, accessible and relevant reads when you want to improve your Kaggle game.\n\n**Stacked generalization**\n\nStacked generalization was introduced by Wolpert in a 1992 paper, 2 years before the seminal Breiman paper \u201cBagging Predictors\u201c. Wolpert is famous for another very popular machine learning theorem:\n\n_\u201cThere is no free lunch in search and optimization\u201c._\n\nThe basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.\n\nLet\u2019s say you want to do 2-fold stacking:\n\n- Split the train set in 2 parts: train\\_a and train\\_b\n- Fit a first-stage model on train\\_a and create predictions for train\\_b\n- Fit the same model on train\\_b and create predictions for train\\_a\n- Finally fit the model on the entire train set and create predictions for the test set.\n- Now train a second-stage stacker model on the probabilities from the first-stage model(s).\n\nA stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.\n\n**Blending**\n\nBlending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use \u201cstacked ensembling\u201d and \u201cblending\u201d interchangeably.\n\nWith blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\n\nBlending has a few benefits:\n\n- It is simpler than stacking.\n- It wards against an information leak: The generalizers and stackers use different data.\n- You do not need to share a seed for stratified folds with your teammates. Anyone can throw models in the \u2018blender\u2019 and the blender decides if it wants to keep that model or not.\n\nHowever, The cons are:\n\n- You use less data overall\n- The final model may overfit to the holdout set.\n- Your CV is more solid with stacking (calculated over more folds) than using a single small holdout set.\n\nAs for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. The author prefers stacking.\n\nIf you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage which we will explore next.\n\n[**Using Ensembles in Kaggle Data Science Competitions \u2013 Part 1**](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p1.html)\n\n[**Using Ensembles in Kaggle Data Science Competitions \u2013 Part 3**](https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p3.html)\n\nHow are you planning to implement what you learned? Share your thoughts!\n\nOriginal: [**Kaggle Ensembling Guide**](http://mlwave.com/kaggle-ensembling-guide/) by Henk van Veen.\n\n**Related:**\n\n- [How to Lead a Data Science Contest without Reading the Data](https://www.kdnuggets.com/2015/05/data-science-contest-leaderboard-without-reading-data.html)\n- [Top 20 R Machine Learning and Data Science packages](https://www.kdnuggets.com/2015/06/top-20-r-machine-learning-packages.html)\n- [Netflix: Director \u2013 Product Analytics, Data Science and Engineering](https://www.kdnuggets.com/jobs/14/08-11-netflix-director-product-analytics-data-science-engineering.html)\n\n### More On This Topic\n\n- [Are Kaggle Competitions Useful for Real World Problems?](https://www.kdnuggets.com/are-kaggle-competitions-useful-for-real-world-problems)\n- [Learn Machine Learning 4X Faster by Participating in Competitions](https://www.kdnuggets.com/2022/01/learn-machine-learning-4x-faster-participating-competitions.html)\n- [7 Free Kaggle Micro-Courses for Data Science Beginners](https://www.kdnuggets.com/7-free-kaggle-micro-courses-for-data-science-beginners)\n- [Top 10 Kaggle Machine Learning Projects to Become Data Scientist in 2024](https://www.kdnuggets.com/top-10-kaggle-machine-learning-projects-to-become-data-scientist-in-2024)\n- [Top 4 tricks for competing on Kaggle and why you should start](https://www.kdnuggets.com/2022/05/packt-top-4-tricks-competing-kaggle-start.html)\n- [The Most Comprehensive List of Kaggle Solutions and Ideas](https://www.kdnuggets.com/2022/11/comprehensive-list-kaggle-solutions-ideas.html)\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%2056'%3E%3C/svg%3E)\n\n[Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox.](https://www.kdnuggets.com/news/subscribe.html)\n\nBy subscribing you accept KDnuggets [Privacy Policy](https://www.kdnuggets.com/news/privacy-policy.html)\n\nLeave this field empty if you're human:\n\n* * *\n\n[<= Previous post](https://www.kdnuggets.com/2015/06/top-20-r-packages.html)\n\n[Next post =>](https://www.kdnuggets.com/2015/06/open-source-interactive-analytics-overview.html)\n\n![Search](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2074%2074'%3E%3C/svg%3E)\n\n### [Latest Posts](https://www.kdnuggets.com/news/index.html)\n\n- [Breaking into Data Science: Essential Skills and How to Learn Them](https://www.kdnuggets.com/breaking-into-data-science-essential-skills-and-how-to-learn-them)\n- [Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)\n- [6 Startups Redefining 3D Workflows with OpenUSD and Generative AI](https://www.kdnuggets.com/6-startups-redefining-3d-workflows-with-openusd-and-generative-ai)\n- [What Data Scientists Should Know About OpenUSD](https://www.kdnuggets.com/what-data-scientists-should-know-about-openusd)\n- [I Took the Google Data Analytics Certification W...",
      "url": "https://www.kdnuggets.com/2015/06/ensembles-kaggle-data-science-competition-p2.html"
    },
    {
      "title": "A Comprehensive Guide to Ensemble Learning \u2013 Exactly What You Need to Know - KDnuggets",
      "text": "# A Comprehensive Guide to Ensemble Learning \u2013 Exactly What You Need to Know\n\nThis article covers ensemble learning methods, and exactly what you need to know in order to understand and implement them.\n\nBy **[Derrick Mwiti](https://www.kdnuggets.com/author/derrick-mwiti)**, Data Scientist on May 6, 2021 in [CatBoost](https://www.kdnuggets.com/tag/catboost), [Ensemble Methods](https://www.kdnuggets.com/tag/ensemble-methods), [Machine Learning](https://www.kdnuggets.com/tag/machine-learning), [Python](https://www.kdnuggets.com/tag/python), [random forests algorithm](https://www.kdnuggets.com/tag/random-forests), [scikit-learn](https://www.kdnuggets.com/tag/scikit-learn), [XGBoost](https://www.kdnuggets.com/tag/xgboost)\n\n[comments](https://www.kdnuggets.com/www.kdnuggets.com#comments)\n\n[Ensemble learning techniques](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)\u00a0have been proven to yield better performance on machine learning problems. We can use these techniques for regression as well as classification problems.\n\nThe final prediction from these ensembling techniques is obtained by combining results from several base models. Averaging, voting and stacking are some of the ways the results are combined to obtain a final prediction.\n\nIn this article, we will explore how ensemble learning can be used to come up with optimal machine learning models.\n\n### What is ensemble learning?\n\nEnsemble learning is a combination of several machine learning models in one problem. These models are known as weak learners. The intuition is that when you combine several weak learners, they can become strong learners.\n\nEach weak learner is fitted on the training set and provides predictions obtained. The final prediction result is computed by combining the results from all the weak learners.\n\n### Basic ensemble learning techniques\n\nLet\u2019s take a moment and look at simple ensemble learning techniques.\n\n**Max voting**\n\nIn classification, the prediction from each model is a vote. In max voting, the final prediction comes from the prediction with the most votes.\n\nLet\u2019s take an example where you have three classifiers with the following predictions:\n\n- classifier 1 \u2013 class A\n- classifier 2 \u2013 class B\n- classifier 3 \u2013 class B\n\nThe final prediction here would be class B since it has the most votes.\n\n**Averaging**\n\nIn averaging, the final output is an average of all predictions. This goes for regression problems. For example, in\u00a0[random forest regression](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why), the final result is the average of the predictions from individual decision trees.\n\nLet\u2019s take an example of three regression models that predict the price of a commodity as follows:\n\n- regressor 1 \u2013 200\n- regressor 2 \u2013 300\n- regressor 3 \u2013 400\n\nThe final prediction would be the average of 200, 300, and 400.\n\n**Weighted average**\n\nIn weighted averaging, the base model with higher predictive power is more important. In the price prediction example, each of the regressors would be assigned a weight.\n\nThe sum of the weights would equal one. Let\u2019s say that the regressors are given weights of 0.35, 0.45, and 0.2 respectively. The final model prediction can be computed as follows:\n\n0.35 \\* 200 + 0.45\\*300 + 0.2\\*400 = 285\n\n### Advanced ensemble learning techniques\n\nAbove are simple techniques, now let\u2019s take a look at advanced techniques for ensemble learning.\n\n### Stacking\n\nStacking is the process of combining various estimators in order to reduce their biases. Predictions from each estimator are stacked together and used as input to a final estimator (usually called a\u00a0_meta-model_) that computes the final prediction. Training of the final estimator happens via cross-validation.\n\nStacking can be done for both regression and classification problems.\n\n[_Source_](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)\n\nStacking can be considered to happen in the following steps:\n\n1. Split the data into a training and validation set,\n2. Divide the training set into K folds, for example 10,\n3. Train a base model (say SVM) on 9 folds and make predictions on the 10th fold,\n4. Repeat until you have a prediction for each fold,\n5. Fit the base model on the whole training set,\n6. Use the model to make predictions on the test set,\n7. Repeat step 3 \u2013 6 for other base models (for example decision trees),\n8. Use predictions from the test set as features to a new model \u2013\u00a0_the meta-model,_\n9. Make final predictions on the test set using the meta model.\n\nWith regression problems, the values passed to the meta-model are numeric. With classification problems, they\u2019re probabilities or class labels.\n\n### Blending\n\nBlending is similar to stacking, but uses a holdout set from the training set to make predictions. So, predictions are done on the holdout set only. The predictions and holdout set are used to build a final model that makes predictions on the test set.\n\nYou can think of blending as a type of stacking, where the meta-model is trained on predictions made by the base model on the hold-out validation set.\n\nYou can consider the\u00a0_blending_\u00a0process to be:\n\n- Split the data into a test and validation set,\n- Fit base models on the validation set,\n- Make predictions on the validation and test set,\n- Use the validation set and its predictions to build a final model,\n- Make final predictions using this model.\n\nThe concept of blending\u00a0[was made popular](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf)\u00a0by the\u00a0[Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize). The winning team used a blended solution to achieve a 10-fold performance improvement on Netflix\u2019s movie recommendation algorithm.\n\nAccording to this\u00a0[Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/):\n\n> \u201cBlending is a word introduced by the Netflix winners. It\u2019s very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use \u201cstacked ensembling\u201d and \u201cblending\u201d interchangeably.\n>\n> With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\u201d\n\n### Blending vs stacking\n\nBlending is simpler than stacking and prevents leakage of information in the model. The generalizers and the stackers use different datasets.\u00a0 However, blending uses less data and may lead to overfitting.\n\nCross-validation is more solid on stacking than blending. It\u2019s calculated over more folds, compared to using a small hold-out dataset in blending.\n\n### Bagging\n\nBagging takes random samples of data, builds learning algorithms, and uses the mean to find bagging probabilities. It\u2019s also called\u00a0_bootstrap aggregating_. Bagging aggregates the results from several models in order to obtain a generalized result.\n\nThe method involves:\n\n- Creating multiple subsets from the original dataset with replacement,\n- Building a base model for each of the subsets,\n- Running all the models in parallel,\n- Combining predictions from all models to obtain final predictions.\n\n### Boosting\n\nBoosting is a machine learning ensemble technique that reduces bias and variance by converting weak learners into strong learners. The weak learners are applied to the dataset in a sequential manner. The first step is building an initial model and fitting it into the training set.\n\nA second model that tries to fix the errors generated by the first model is then fitted. Here\u2019s what the entire process looks like:\n\n- Create a subset from the original data,\n- Build an initial model with this data,\n- Run predictions on the whole data set,\n- Calculate the error using the predictions and the actual values,\n- Assign more weight to the incorrect predictions,\n- Create another model that attempts to fix errors from the last model,\n- Run predictions on the entire dataset with the new model,\n- Create several m...",
      "url": "https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html"
    },
    {
      "title": "Stacking Ensemble",
      "text": "Stacking Ensemble | Kaggler TV Blog\n[Kaggler TV Blog](https://kaggler.com/)\n# Stacking Ensemble\nA tutorial of stacking ensemble (a.k.a. stacked generalization)\nApr 26, 2021\u20225 min read\n**</i>[notebook](https://kaggler.com/categories/#notebook)[kaggle](https://kaggler.com/categories/#kaggle)\n[![View On GitHub](https://kaggler.com/assets/badges/github.svg)](https://github.com/kaggler-tv/blog/tree/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Binder](https://kaggler.com/assets/badges/binder.svg)](https://mybinder.org/v2/gh/kaggler-tv/blog/master?filepath=_notebooks/2021-04-26-stacking-ensemble.ipynb)\n[![Open In Colab](https://kaggler.com/assets/badges/colab.svg)](https://colab.research.google.com/github/kaggler-tv/blog/blob/master/_notebooks/2021-04-26-stacking-ensemble.ipynb)\n* [Part 1: Data Loading &amp; Feature Engineering](#Part-1:-Data-Loading-&-Feature-Engineering)\n* [Part 2: Level-1 Base Model Training](#Part-2:-Level-1-Base-Model-Training)\n* [Part 3: Level-2 Stacking](#Part-3:-Level-2-Stacking)\nThis notebook was originally published[here](https://www.kaggle.com/jeongyoonlee/stacking-ensemble)at Kaggle.\nThis notebook shows how to perform stacking ensemble (a.k.a. stacked generalization).\nIn[Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking), @remekkinas shares how to do stacking ensemble using`MLExtend'`s`StackingCVClassifier`.\nTo demonstrate how stacking works, this notebook shows how to prepare the baseline model predictions using cross-validation (CV), then use them for level-2 stacking. It trains four classifiers, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost as level-1 base models. It also uses CV predictions of two models, LightGBM with DAE features and supervised DAE trained from my previous notebook,[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)to show why keeping CV predictions for**every**model is important. :)\nThe contents of this notebook are as follows:\n1. **Feature Engineering**: Same as in the[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder)and[AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb).\n2. **Level-1 Base Model Training**: Training four base models, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost using the same 5-fold CV.\n3. **Level-2 Stacking**: Training the LightGBM model with CV predictions of base models, original features, and DAE features. Performing feature selection and hyperparameter optimization using`Kaggler`'s`AutoLGB`.\nThis notebook is inspired and/or based on other Kagglers' notebooks as follows:\n* [TPS-APR21-EDA+MODEL](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model)by @udbhavpangotra\n* [Ensemble-learning meta-classifier for stacking](https://www.kaggle.com/remekkinas/ensemble-learning-meta-classifier-for-stacking)by @remekkinas\n* [TPS Apr 2021 pseudo labeling/voting ensemble](https://www.kaggle.com/hiro5299834/tps-apr-2021-pseudo-labeling-voting-ensemble?scriptVersionId=60616606)by @hiro5299834\nThanks!\n# [](#Part-1:-Data-Loading-&amp;-Feature-Engineering)Part 1: Data Loading &amp; Feature Engineering[](#Part-1:-Data-Loading-&amp;-Feature-Engineering)\n```\nfromcatboostimportCatBoostClassifierfromjoblibimportdumpimportlightgbmaslgbfromlightgbmimportLGBMClassifierfrommatplotlibimportpyplotaspltimportnumpyasnpimportpandasaspdfrompathlibimportPathfromsklearn.ensembleimportRandomForestClassifierfromsklearn.ensembleimportExtraTreesClassifierfromsklearn.metricsimportroc\\_auc\\_score,confusion\\_matrixfromsklearn.model\\_selectionimportStratifiedKFoldfromsklearn.preprocessingimportStandardScalerimportwarnings\n```\n```\n!pip install kaggler\n```\n```\nimportkagglerfromkaggler.modelimportAutoLGBfromkaggler.preprocessingimportLabelEncoderprint(f'Kaggler:{kaggler.\\_\\_version\\_\\_}')\n```\n```\nwarnings.simplefilter('ignore')pd.set\\_option('max\\_columns',100)\n```\n```\ndata\\_dir=Path('/kaggle/input/tabular-playground-series-apr-2021/')trn\\_file=data\\_dir/'train.csv'tst\\_file=data\\_dir/'test.csv'sample\\_file=data\\_dir/'sample\\_submission.csv'pseudo\\_label\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/tps04-sub-006.csv'dae\\_feature\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/dae.csv'lgb\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.val.txt'lgb\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/lgb\\_dae.tst.txt'sdae\\_dae\\_predict\\_val\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.val.txt'sdae\\_dae\\_predict\\_tst\\_file='/kaggle/input/tps-apr-2021-pseudo-label-dae/sdae\\_dae.tst.txt'target\\_col='Survived'id\\_col='PassengerId'feature\\_name='dae'algo\\_name='esb'model\\_name=f'{algo\\_name}\\_{feature\\_name}'feature\\_file=f'{feature\\_name}.csv'predict\\_val\\_file=f'{model\\_name}.val.txt'predict\\_tst\\_file=f'{model\\_name}.tst.txt'submission\\_file=f'{model\\_name}.sub.csv'\n```\n```\nn\\_fold=5seed=42n\\_est=1000encoding\\_dim=128\n```\n```\ntrn=pd.read\\_csv(trn\\_file,index\\_col=id\\_col)tst=pd.read\\_csv(tst\\_file,index\\_col=id\\_col)sub=pd.read\\_csv(sample\\_file,index\\_col=id\\_col)pseudo\\_label=pd.read\\_csv(pseudo\\_label\\_file,index\\_col=id\\_col)dae\\_features=np.loadtxt(dae\\_feature\\_file,delimiter=',')lgb\\_dae\\_predict\\_val=np.loadtxt(lgb\\_dae\\_predict\\_val\\_file)lgb\\_dae\\_predict\\_tst=np.loadtxt(lgb\\_dae\\_predict\\_tst\\_file)sdae\\_dae\\_predict\\_val=np.loadtxt(sdae\\_dae\\_predict\\_val\\_file)sdae\\_dae\\_predict\\_tst=np.loadtxt(sdae\\_dae\\_predict\\_tst\\_file)print(trn.shape,tst.shape,sub.shape,pseudo\\_label.shape,dae\\_features.shape)print(lgb\\_dae\\_predict\\_val.shape,lgb\\_dae\\_predict\\_tst.shape)print(sdae\\_dae\\_predict\\_val.shape,sdae\\_dae\\_predict\\_tst.shape)\n```\n```\ntst[target\\_col]=pseudo\\_label[target\\_col]n\\_trn=trn.shape[0]df=pd.concat([trn,tst],axis=0)df.head()\n```\nLoading 128 DAE features generated from[Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder/).\n```\ndf\\_dae=pd.DataFrame(dae\\_features,columns=[f'enc\\_{x}'forxinrange(encoding\\_dim)])print(df\\_dae.shape)df\\_dae.head()\n```\nFeature engineering using @udbhavpangotra's[code](https://www.kaggle.com/udbhavpangotra/tps-apr21-eda-model).\n```\ndf['Embarked']=df['Embarked'].fillna('No')df['Cabin']=df['Cabin'].fillna('\\_')df['CabinType']=df['Cabin'].apply(lambdax:x[0])df.Ticket=df.Ticket.map(lambdax:str(x).split()[0]iflen(str(x).split())&gt;1else'X')df['Age'].fillna(round(df['Age'].median()),inplace=True,)df['Age']=df['Age'].apply(round).astype(int)# Fare, fillna with mean valuefare\\_map=df[['Fare','Pclass']].dropna().groupby('Pclass').median().to\\_dict()df['Fare']=df['Fare'].fillna(df['Pclass'].map(fare\\_map['Fare']))df['FirstName']=df['Name'].str.split(', ').str[0]df['SecondName']=df['Name'].str.split(', ').str[1]df['n']=1gb=df.groupby('FirstName')df\\_names=gb['n'].sum()df['SameFirstName']=df['FirstName'].apply(lambdax:df\\_names[x]).fillna(1)gb=df.groupby('SecondName')df\\_names=gb['n'].sum()df['SameSecondName']=df['SecondName'].apply(lambdax:df\\_names[x]).fillna(1)df['Sex']=(df['Sex']=='male').astype(int)df['FamilySize']=df.SibSp+df.Parch+1feature\\_cols=['Pclass','Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName','SameSecondName','Sex','FamilySize','FirstName','SecondName']cat\\_cols=['Pclass','Embarked','CabinType','Ticket','FirstName','SecondName']num\\_cols=[xforxinfeature\\_colsifxnotincat\\_cols]print(len(feature\\_cols),len(cat\\_cols),len(num\\_cols))\n```\nApplying`log2(1 + x)`for numerical features and label-encoding categorical features using`kaggler.preprocessing.LabelEncoder`, which handles`NaN`s and groups rare categories together.\n```\nforcolin['SameFirstName','SameSecondName','Fare','FamilySize','Parch','SibSp']:df[col]=np.log2(1+df[col])scaler=StandardScaler()df[num\\_cols]=...",
      "url": "https://kaggler.com/notebook/kaggle/2021/04/26/stacking-ensemble.html"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - sabamadadi/multi-strategy-ensemble-binary-classification: Robust binary data classification using a multi-strategy ensemble learning approach.\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=sabamadadi/multi-strategy-ensemble-binary-classification)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[sabamadadi](https://github.com/sabamadadi)/**[multi-strategy-ensemble-binary-classification](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification)**Public\n* [Notifications](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)You must be signed in to change notification settings\n* [Fork0](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)\n* [Star1](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)\nRobust binary data classification using a multi-strategy ensemble learning approach.\n[1star](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/stargazers)[0forks](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/forks)[Branches](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/branches)[Tags](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/tags)[Activity](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/activity)\n[Star](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)\n[Notifications](https://github.com/login?return_to=/sabamadadi/multi-strategy-ensemble-binary-classification)You must be signed in to change notification settings\n# sabamadadi/multi-strategy-ensemble-binary-classification\nmain\n[Branches](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/branches)[Tags](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/tags)\n[](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/branches)[](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[4 Commits](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/commits/main/)\n[](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/commits/main/)\n|\n[README.md](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/README.md)\n|\n[README.md](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/README.md)\n|\n|\n|\n[c\\_report.pdf](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/c_report.pdf)\n|\n[c\\_report.pdf](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/c_report.pdf)\n|\n|\n|\n[classification.ipynb](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/classification.ipynb)\n|\n[classification.ipynb](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/blob/main/classification.ipynb)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Multi-Strategy Ensemble Learning for Binary Data Classification\n[](#multi-strategy-ensemble-learning-for-binary-data-classification)\n[Full Report](https://drive.google.com/file/d/1Cf21G0ubgu8sm_Y_j2p2E1bngO8KpD2U/view?usp=sharing)\n[Kaggle Competition Leaderboard](https://www.kaggle.com/competitions/datascience-4-competition/leaderboard)First Score (0.41791)\nThis project develops a robust classification pipeline for high-dimensional binary datasets. The dataset contained 64 binary features and a categorical target, with class imbalance handled using SMOTE, producing a balanced training set of 792 samples.\nFeatures were scaled using StandardScaler to ensure consistency across training and testing. Hyperparameters were optimized using Optuna with stratified k-fold cross-validation (k=3 for ensemble, k=5 for individual models).\nThe final model is a soft-voting ensemble integrating multiple Bernoulli Naive Bayes classifiers, Logistic Regression, XGBoost, Balanced Random Forest, and three standard Random Forest classifiers. Ensemble hyperparameters were specifically tuned to maximize cross-validated accuracy (\\~0.437).\nExploratory methods like CTGAN, MLP-based feature extraction, and Random Forest-based feature selection were investigated but not included in the final pipeline.\nCritical attention was given to preprocessing consistency: the test set was scaled using the training scaler to prevent distribution mismatch.\nThe ensemble approach balances diverse algorithmic strengths, improves robustness, and handles class imbalance effectively. Future directions include exploring advanced feature selection, stacking ensembles, alternative imbalance techniques, and probability calibration.\nKey takeaways: multi-strategy ensembles are effective for high-dimensional binary data, SMOTE improves minority class performance, and consistent preprocessing is crucial for valid inference.\n## About\nRobust binary data classification using a multi-strategy ensemble learning approach.\n### Topics\n[classification](https://github.com/topics/classification)\n### Resources\n[Readme](#readme-ov-file)\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n[Activity](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/activity)\n### Stars\n[**1**star](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/stargazers)\n### Watchers\n[**0**watching](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/watchers)\n### Forks\n[**0**forks](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/forks)\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification&amp;report=sabamadadi+(user))\n## [Releases](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/releases)\nNo releases published\n## [Packages0](https://github.com/users/sabamadadi/packages?repo_name=multi-strategy-ensemble-binary-classification)\nNo packages published\n## Languages\n* [Jupyter Notebook100.0%](https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification/search?l=jupyter-notebook)\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/sabamadadi/multi-strategy-ensemble-binary-classification"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    }
  ]
}