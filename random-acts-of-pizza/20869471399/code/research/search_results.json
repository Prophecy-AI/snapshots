{
  "query": "What are winning approaches for text classification with mixed text and tabular features on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle solutions for text\u2011classification tasks that also contain structured (numeric\u202f/\u202fcategorical) columns treat the data as a **multimodal table** and combine a language model with a strong tabular learner.  The most reliable pattern is a two\u2011stage pipeline: first turn each free\u2011text column into a dense embedding with a pretrained transformer (e.g., BERT, RoBERTa) or a classic TF\u2011IDF/word\u2011n\u2011gram vector, then concatenate those embeddings with the original numeric and categorical features and feed the whole matrix to a gradient\u2011boosting model such as LightGBM, CatBoost, or a neural tabular architecture (TabTransformer, FT\u2011Transformer, TabM).  This approach consistently ranks at the top of the multimodal AutoML benchmark, where \u201cstandard two\u2011stage approaches where NLP is used \u2026 then a tabular model is trained on the concatenated features\u201d dominate the leaderboard\u202f([NeurIPS\u202f2021 benchmark](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper-round2.pdf)).\n\nFrameworks that automate the above steps have become the go\u2011to tools for competitors.  **AutoGluon Tabular** treats raw text as a first\u2011class column and internally builds a pretrained BERT\u2011based multimodal network that is then stacked with classical models like LightGBM, RF, or CatBoost, delivering strong out\u2011of\u2011the\u2011box performance on mixed data tables\u202f([AutoGluon docs](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  Similarly, the **HuggingFace\u202f+\u202ftabular** recipe shows how to fine\u2011tune a transformer on the text fields and concatenate its pooled output with tabular features before training a final classifier, a pattern that many Kaggle Grandmasters replicate\u202f([KDnuggets](https://kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html)).  Advanced ensembles further boost accuracy: separate models are trained on pure text (e.g., a fine\u2011tuned BERT) and on pure tabular data (e.g., CatBoost), and their predictions are blended or stacked using simple linear/meta\u2011learners, a technique highlighted in the **Kaggle Grandmasters Playbook** for tabular competitions\u202f([NVIDIA blog](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\nRecent research on **tabular foundation models** confirms that even modest \u201cablation\u2011style\u201d strategies\u2014such as appending transformer embeddings, adding text\u2011length or sentiment scores, and using target encoding for categorical columns\u2014close the gap to specialized multimodal models while keeping training fast and reproducible\u202f([Mr\u00e1z\u202fet\u202fal., 2025](https://arxiv.org/pdf/2507.07829.pdf)).  In practice, winning Kaggle pipelines therefore combine: (1) high\u2011quality text embeddings (BERT, RoBERTa, or TF\u2011IDF), (2) robust tabular learners (LightGBM, CatBoost, FT\u2011Transformer, TabM), (3) careful feature engineering for both modalities, and (4) stacking/ensembling of the individual models.  Applying these steps with an AutoML wrapper (AutoGluon, AutoTabular) or a custom two\u2011stage script gives the best chance of topping the leaderboard on mixed text\u2011and\u2011tabular classification challenges.",
      "url": ""
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.7.0 documentation",
      "text": "<div><div>\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\">\u00b6</a></h2>\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\n<a href=\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\naddition to text and tabular features.</p>\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c<span>sec_textprediction_architecture</span>\u201d of\n<span>sec_textprediction_multimodal</span> (used by AutoGluon\u2019s\n<code><span>TextPredictor</span></code>).</p>\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\n<span>import</span> <span>pprint</span>\n<span>import</span> <span>random</span>\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n</pre></div>\n<div>\n<h2>Product Sentiment Analysis Dataset<a href=\"#product-sentiment-analysis-dataset\">\u00b6</a></h2>\n<p>We consider the product sentiment analysis dataset from a <a href=\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\">MachineHack\nhackathon</a>.\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).</p>\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n</pre></div>\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\n2023-02-22 23:29:24 (73.9 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\n2023-02-22 23:29:24 (57.6 MB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\n2023-02-22 23:29:24 (137 MB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n</pre></div>\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\n</pre></div>\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\n</pre></div>\n<p>There are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.</p>\n<div>\n<table>\n <thead>\n <tr>\n <th></th>\n <th>Product_Description</th>\n <th>Product_Type</th>\n <th>Sentiment</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>4532</th>\n <td>they took away the lego pit but ...",
      "url": "https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "Towards Benchmarking Foundation Models for Tabular Data With Text",
      "text": "Towards Benchmarking Foundation Models for Tabular Data With Text\nMartin Mraz\u00b4\n* 1 Breenda Das * 1 Anshul Gupta * 1 Lennart Purucker 1 Frank Hutter 2 3 1\nAbstract\nFoundation models for tabular data are rapidly\nevolving, with increasing interest in extending\nthem to support additional modalities such as free\u0002text features. However, existing benchmarks for\ntabular data rarely include textual columns, and\nidentifying real-world tabular datasets with se\u0002mantically rich text features is non-trivial. We\npropose a series of simple yet effective ablation\u0002style strategies for incorporating text into con\u0002ventional tabular pipelines. Moreover, we bench\u0002mark how state-of-the-art tabular foundation mod\u0002els can handle textual data by manually curating\na collection of real-world tabular datasets with\nmeaningful textual features. Our study is an im\u0002portant step towards improving benchmarking of\nfoundation models for tabular data with text.\n1. Introduction\nFoundation models have begun to transform tabular learn\u0002ing (Erickson et al., 2020b; Hollmann et al., 2025), echo\u0002ing the trajectory of other research fields. A natural next\nstep is mixed-modality tabular modeling, where structured\ncolumns may also include free-text fields such as job de\u0002scriptions, clinical notes, or product summaries. Current\ntabular benchmarks, however, almost never contain textual\ncolumns, cf. (Bischl et al., 2019; Liu et al., 2024; McEl\u0002fresh et al., 2024). Moreover, locating real-world datasets\nwith semantically rich text features is exceptionally difficult,\nwith even exhaustive searches of OpenML and Kaggle only\nyielding a handful of usable candidates (Shi et al., 2021).\nConsequently, current tabular foundation models are rarely\nevaluated for tabular data with text.\nPipelines that can handle tabular data with text vary greatly\n*Equal contribution 1Department of Computer Science, Uni\u0002versity of Freiburg, Freiburg, Germany 2\nPrior Labs, Freiburg,\nGermany 3ELLIS Institute, Tubingen, Germany. Correspon\u0002dence to: Martin Mraz <mrazm@informatik.uni-freiburg.de>,\nBreenda Das <dasb@informatik.uni-freiburg.de>, Anshul Gupta\n<guptaa@informatik.uni-freiburg.de>.\nProceedings of the 42 nd International Conference on Machine\nLearning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\nin their implementation. AutoGluon\u2019s AutoMLPipeline\u0002FeatureGenerator (Erickson et al., 2020a) converts text to\nsparse vectors; CARTE (Kim et al., 2024) applies fastText\nsentence embeddings (Bojanowski et al., 2017). and the\nTabPFNv2 API accepts raw text but does not disclose its\nmethodology. These divergent choices raise a fundamental\nquestion: Which embedding strategy works best, and under\nwhat conditions?\nTo answer this question, we present the first systematic\nstudy of predictive machine learning with foundation mod\u0002els for tabular data with text. We study the performance\nof three representative embedding routes: fastText, Skrub\u2019s\nTableVectorizer, and AutoGluon\u2019s text encoder. We show\nqualitatively, with a simple synthetic counter-example, that\nboth n-gram based and off-the-shelf sentence embeddings\ncan fail to recover highly predictive semantic patterns. More\u0002over, quantitatively, we evaluate these methods on a man\u0002ually curated set of real-world tabular benchmark that (i)\ncontain genuinely informative free-text columns (ii) spans\nover a variety of domains and samples.\nOur contributions are: (I) A qualitative study show cas\u0002ing the limitations of standard n-gram based and generic\nNLP-based embeddings for tabular tasks with text. (II) A\nmanually curated set of real-world tabular datasets with se\u0002mantically rich textual columns. (III) An empirical study of\nthree text embedding pipelines for TabPFNv2 and XGBoost\nwith the TabPFNv2 API and AutoGluon Tabular Predictor\nas baselines.\nOur study reveals the limitations of current methods and\nunderscores the need for new methods to handle tabular data\nwith free text.\n2. Related Work\nApproaches incorporating free-text in tabular learning\nlargely follow two paradigms. First, row-as-text meth\u0002ods serialize entire rows into prompts and delegate pre\u0002diction to a large language model (LLM), as seen in\nTABLLM (Hegselmann et al., 2022), TABLE-LLM (Zhang\net al., 2024). These work well when textual fields dominate\nor in few-shot settings. Second, per-column embedding\nstrategies extract textual embeddings from a single or groups\nof features, while preserving the structural nature of tabular\n1\narXiv:2507.07829v1 [cs.LG] 10 Jul 2025\nTowards Benchmarking Foundation Models for Tabular Data With Text\nFigure 1. Qualitative investigation of textual embeddings for tabular prediction tasks: TabPFN v2 accuracy across five datasets.\nBaselines \u201cNo-Text\u201d uses original input features, \u201cComplete Leak\u201d has target leakage, therefore 100% accuracy for all. Following tests\nembed targets into textual modality. \u201cN-Gram Break\u201d shows TF-IDF breaking under unseen synonyms, \u201cSimple NLP break\u201d shows\nFastText underperforming under noise, \u201cLLM Breaks\u201d shows BERT variant breaking under semantic ambiguity.\ndata. They embed each column using fastText or LLMs\nand then concatenate the resulting embeddings to the table,\nor replace the feature with its individual textual embed\u0002ding, cf. (Koloski et al., 2025; Carballo et al., 2023; Kim\net al., 2024). Grinsztajn et al. (2023) compare 30+ LLM\nand substring-based methods for embedding string columns,\nshowing when LLMs yield better representations.\nIn this study, we investigate pre-column embeddings be\u0002cause we focus on a many-shot setting (e.g., more than 32\ntraining samples), in which LLM-based predictive methods\ndo not perform well, cf. (Hegselmann et al., 2023) and (Ma\net al., 2024), or require a prohibitive amount of fine-tuning\n(Shysheya et al., 2025).\nMost popular prior tabular benchmarks contain no free-text\nfield, cf. (Bischl et al., 2019; Gijsbers et al., 2022; McEl\u0002fresh et al., 2023). For tabular data with text, Shi et al.\n(2021) curated 18 datasets with mixed modalities, but many\nrely almost entirely on text, with only one or two tabular\nfeatures, or features derived from the long text, e.g. # of\nwords (Tang et al., 2024). Thus, they benchmark text-based\nmodels rather than tabular models, focusing on text data\nwith tabular features. In contrast, we focus on tabular mod\u0002els extended to handle text, focusing on tabular data with\nadditional text features. Other studies were often limited to\nan evaluation with just one or two datasets (Carballo et al.,\n2023; Lu et al., 2023). Overall, existing studies for tabular\ndata with text lack a domain-agnostic benchmark where\ntextual features complement, rather than dominate, tabular\ndata. This motivates the new benchmark we present.\nThe CARTE Benchmark. The latest seminal work on\nbenchmarking for tabular data with text is the CARTE (Kim\net al., 2024) benchmark. It includes 51 datasets for tabu\u0002lar data with text. However, when we investigated these\ndatasets more closely, we found that at most 11 out of the\n51 datasets are suited to evaluate tabular data with text.\nMoreover, our manually curated collection of datasets for\nbenchmarking only includes 1 out of the 51 datasets.\nWe share an extensive report of our investigation in Ap\u0002pendix G. In short, we found that most datasets (a) do not\nrepresent predictive machine learning tasks for tabular data\nwith text; (b) are skewed towards text data representing\nmany categories instead of longer free text; (c) were pre\u0002processed manually per dataset with logic that seems to\nfavor CARTE; (d) or were very similar datasets from the\nsame domain, with the same features, or similar semantic\ncontent. Thus, while CARTE was a significant step toward\nbenchmarking and learning for tabular data with text, it falls\nshort in several aspects. Our work complements CARTE\u2019s\nefforts and aims to improve benchmarking for tabular data\nwith text further.\n3. Qualitative Investigation\nIn this section, we evaluate popular text embedding tech\u0002niques: N-Grams, simple NLP models and LLMs for tabular\npr...",
      "url": "https://arxiv.org/pdf/2507.07829"
    },
    {
      "title": "How to Incorporate Tabular Data with HuggingFace Transformers - KDnuggets",
      "text": "# How to Incorporate Tabular Data with HuggingFace Transformers\n\nIn real-world scenarios, we often encounter data that includes text and tabular features. Leveraging the latest advances for transformers, effectively handling situations with both data structures can increase performance in your models.\n\n* * *\n\n[comments](https://www.kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html#comments)\n\n**By [Ken Gu](https://www.linkedin.com/in/ken-gu/), Applied Research Scientist Intern at Georgian**.\n\nTransformer-based models are a game-changer when it comes to using unstructured text data. As of September 2020, the top-performing models in the General Language Understanding Evaluation (GLUE) benchmark are all BERT transformer-based models. At\u00a0[Georgian](http://georgian.io/), we find ourselves working with supporting tabular feature information as well as unstructured text data. We found that by using the tabular data in\u00a0[our models](http://georgian.io/platform/research-at-georgian/), we could further improve performance, so we set out to build a toolkit that makes it easier for others to do the same.\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2090%200'%3E%3C/svg%3E)\n\n_The 9 tasks that are part of the GLUE benchmark._\n\n### Building on Top of Transformers\n\nThe main benefits of using transformers are that they can learn long-range dependencies between text and can be trained in parallel (as opposed to sequence to sequence models), meaning they can be pre-trained on large amounts of data.\n\nGiven these advantages, BERT is now a staple model in many real-world applications. Likewise, with libraries such as [HuggingFace Transformers](https://huggingface.co/transformers/), it\u2019s easy to build high-performance transformer models on common NLP problems.\n\nTransformer models using unstructured text data are well understood. However, in the real-world, text data is often supported by rich structured data or other unstructured data like audio or visual information. Each one of these might provide signals that one alone would not. We call these different ways of experiencing data \u2014 audio, visual, or text \u2014 modalities.\n\nThink about e-commerce reviews as an example. In addition to the review text itself, we also have information about the seller, buyer, and product available as numerical and categorical features.\n\nWe set out to explore how we could use text and tabular data together to provide stronger signals in our projects. We started by exploring the field known as multimodal learning, which focuses on how to process different modalities in machine learning.\n\n### Multimodal Literature Review\n\nThe current models for multimodal learning mainly focus on learning from the sensory modalities such as audio, visual, and text.\n\nWithin multimodal learning, there are several branches of research. The MultiComp Lab at Carnegie Mellon University provides an excellent\u00a0[taxonomy](http://multicomp.cs.cmu.edu/research/taxonomy/). Our problem falls under what is known as\u00a0**Multimodal Fusion \u2014** joining information from two or more modalities to make a prediction.\n\nAs text data is our primary modality, our review focues on the literature that treats text as the main modality and introduces models that leverage the transformer architecture.\n\n**Trivial Solution to Structured Data**\n\nBefore we dive into the literature, it\u2019s worth mentioning that there is a simple solution that can be used where the structured data is treated as regular text and is appended to the standard text inputs. Taking the e-commerce reviews example, the input can be structured as follows: Review. Buyer Info. Seller Info. Numbers/Labels. Etc. One caveat with this approach, however, is that it is limited by the maximum token length that a transformer can handle.\n\n### Transformer on Images and Text\n\nIn the last couple of years, transformer extensions for image and text have really advanced.\u00a0[Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf)\u00a0by Kiela et al. (2019) uses pre-trained ResNet and pre-trained BERT features on unimodal images and text, respectively, and feeds this into a Bidirectional transformer. The key innovation is adapting the image features as additional tokens to the transformer model.\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2090%200'%3E%3C/svg%3E)\n\n_An illustration of the multimodal transformer. This model takes the output of ResNet on subregions of the image as input image tokens._\n\nAdditionally, there are models \u2014\u00a0[ViLBERT](https://arxiv.org/abs/1908.02265)\u00a0(Lu et al. 2019) and [VLBert](https://arxiv.org/pdf/1908.08530.pdf)\u00a0(Su et al. 2020) \u2014 that define pretraining tasks for images and text. Both models pre-train on the\u00a0[Conceptual Captions dataset](https://ai.google.com/research/ConceptualCaptions), which contains roughly 3.3 million image-caption pairs (web images with captions from alt text). In both cases, for any given image, a pre-trained object detection model like Faster R-CNN obtains vector representations for regions of the image, which count as input token embeddings to the transformer model.\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2090%200'%3E%3C/svg%3E)\n\n_The VLBert model diagram. It takes image regions outputted by Faster R-CNN as input image tokens._\n\nAs an example, ViLBert pre-trains on the following training objectives:\n\n1. **Masked multimodal modeling:** Mask input image and word tokens. For the image, the model tries to predict a vector capturing image features for the corresponding image region, while for text, it predicts the masked text based on the textual and visual clues.\n2. **Multimodal alignment:** Whether the image and text pair are actually from the same image and caption pair.\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2090%200'%3E%3C/svg%3E)\n\n_The two pre-training tasks for ViLBert._\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20172%20226'%3E%3C/svg%3E)\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20156%20141'%3E%3C/svg%3E)\n\n_An example of masked multimodal learning. Given the image and text, if we mask out_ dog _, then the model should be able to use the unmasked visual information to correctly predict the masked word to be_ dog _._\n\nAll these models use the bidirectional transformer model that is the backbone of BERT. The differences are the pre-training tasks the models are trained on and slight additions to the transformer. In the case of ViLBERT, the authors also introduce a co-attention transformer layer (shown below) to define the attention mechanism between the modalities explicitly.\n\n![](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2090%200'%3E%3C/svg%3E)\n\n_The standard transformer block vs. the co-attention transformer block. The co-attention block injects attention-weighted vectors of another modality (linguistic, for example) into the hidden representations of the current modality (visual)._\n\nFinally, there\u2019s also\u00a0[LXMERT](https://arxiv.org/abs/1908.07490)\u00a0(Tan and Mohit 2019), another pre-trained transformer model that, as of\u00a0[Transformers version 3.1.0](https://pypi.org/project/transformers/3.1.0/), is implemented as part of the library. The input to LXMERT is the same as ViLBERT and VLBERT. However, LXMERT pre-trains on aggregated datasets, which also include visual question answering datasets. In total, LXMERT pre-trains on 9.18 million image text pairs.\n\n### Transformers on Aligning Audio, Visual, and Text\n\nBeyond transformers for combining image and text, there are multimodal models for audio, video, and text modalities in which there is a natural ground truth temporal alignment. Papers for this approach include MulT, [Multimodal Transformer for Unaligned Multimodal Language Sequences](https://arx...",
      "url": "https://kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://www.arxiv.org/pdf/2412.16243"
    },
    {
      "title": "Benchmarking Multimodal AutoML for Tabular Data with Text Fields",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2111.02705"
    },
    {
      "title": "Papers with Code - Benchmarking Multimodal AutoML for Tabular Data with Text Fields",
      "text": "new\n\nGet trending papers in your email inbox once a day!\n\nGet trending papers in your email inbox!\n\n[Subscribe](https://paperswithcode.com/login?next=%2Fpapers)\n\n# Trending Papers\n\n## by [AK](https://paperswithcode.com/akhaliq) and the research community\n\n- Daily\n- Weekly\n- Monthly\n\nTrending Papers\n\nSubmitted by\nhiyouga\n\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\n\nRecent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\\\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\\\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\\\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.\n\n- 12 authors\n\n\u00b7Published on Aug 4, 2025\n\n[Upvote\\\n\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\n\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\n\nSubmitted by\nhiyouga\n\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\n\nRecent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\\\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\\\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\\\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.\n\n- 12 authors\n\n\u00b7Aug 4, 2025\n\n[Upvote\\\n\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\n\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\n\nSubmitted by\nFrancisRing\n\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\n\nCurrent diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.\n\n- 9 authors\n\n\u00b7Published on Aug 11, 2025\n\n[Upvote\\\n\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\n\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\n\nSubmitted by\nFrancisRing\n\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\n\nCurrent diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.\n\n- 9 authors\n\n\u00b7Aug 11, 2025\n\n[Upvote\\\n\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\n\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\n\nSubmitted by\ncsuhan\n\n### [ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\ Automation via Modular Multimodal Agents](https://paperswithcode.com/papers/2507.22827)\n\nAutomating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely o...",
      "url": "https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular"
    },
    {
      "title": "Benchmarking Multimodal AutoML for Tabular Data with Text Fields",
      "text": "#### Benchmarking Multimodal AutoML for Tabular Data with Text Fields\n\nPart of\n[Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021) round2\n\n[Bibtex](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/28-/bibtex) [Paper](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper-round2.pdf) [Reviews And Public Comment \u00bb](https://openreview.net/forum?id=Q0zOIaec8HF) [Supplemental](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Supplemental-round2.zip)\n\n#### Authors\n\n_Xingjian Shi, Jonas Mueller, Nick Erickson, Nick Erickson, Mu Li, Alexander Smola, Alexander Smola_\n\n#### Abstract\n\nWe consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.\n\nDo not remove: This comment is monitored to verify that the site is working properly",
      "url": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html"
    }
  ]
}