{
  "query": "How should I handle extremely predictive categorical features like user flair with 100% success rate in some categories to prevent overfitting in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**To keep \u201cperfect\u2011predictor\u201d categorical columns (e.g., a user\u2011flair that predicts the target 100\u202f% of the time) from destroying your model\u2019s ability to generalize, follow these concise steps**\u202f([Google\u202fML\u202foverfitting](https://developers.google.com/machine-learning/crash-course/overfitting/overfitting);\u202f[Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions);\u202f[TargetEncoder\u202fcross\u2011fitting](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder_cross_val.html);\u202f[Stack\u202fExchange\u202fcollapsing categories](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels)):\n\n1. **Detect the leakage** \u2013 Compute the success rate of each category on the training split. If any level predicts the target with (near)\u202f100\u202f% accuracy, flag it as a \u201cleaky\u201d feature.\n\n2. **Collapse or drop extreme levels**  \n   * Group rare or perfectly predictive categories into an \u201cOther/Unknown\u201d bucket, or merge them based on domain knowledge (e.g., combine flairs that share similar behavior).  \n   * When the number of levels is very high, use principled pooling methods (frequency\u2011based grouping, chi\u2011square tests, or hierarchical clustering) to reduce dimensionality without losing signal\u202f([Stack\u202fExchange](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels)).\n\n3. **Apply target encoding with internal cross\u2011fitting** \u2013 Replace each category by the smoothed mean of the target, but fit the encoder on *k\u20111* folds and transform the held\u2011out fold. This internal CV prevents the encoder from memorizing the 100\u202f% rates on the training data and dramatically reduces leakage\u202f([scikit\u2011learn\u202fTargetEncoder](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder_cross_val.html)).\n\n4. **Add regularization to the encoded values** \u2013 Blend the category\u2011wise target mean with the global mean (e.g., using a smoothing parameter \u03b1) so that categories with few observations cannot dominate the model.\n\n5. **Validate with proper cross\u2011validation** \u2013 Use stratified K\u2011fold or group\u2011aware splits that mimic the competition\u2019s test distribution. Monitor validation scores; a large gap between training and validation indicates residual leakage.\n\n6. **Optional safety nets**  \n   * **Adversarial validation** \u2013 Train a classifier to distinguish training vs. hold\u2011out data; if the leaky feature is highly predictive of the split, consider removing or further aggregating it.  \n   * **Early stopping / regularization** \u2013 Combine the encoded feature with other regularized models (L1/L2, tree\u2011based depth limits) to curb over\u2011fitting\u202f([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).\n\nBy collapsing extreme categories, encoding them with cross\u2011validated target means, and rigorously validating, you keep the predictive power of user flair while preventing the model from memorizing spurious 100\u202f% patterns that will fail on the unseen Kaggle test set.",
      "url": ""
    },
    {
      "title": "Overfitting | Machine Learning - Google for Developers",
      "text": "Overfitting | Machine Learning | Google for Developers[Skip to main content](#main-content)\n* [Machine Learning](https://developers.google.com/machine-learning)\n/\n* English\n* Deutsch\n* Espa\u00f1ol\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u4e2d\u6587\u2013\u7e41\u9ad4* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4Sign in\n* [ML Concepts](https://developers.google.com/machine-learning/crash-course)\n* [Home](https://developers.google.com/)\n* [Products](https://developers.google.com/products)\n* [Machine Learning](https://developers.google.com/machine-learning)\n* [ML Concepts](https://developers.google.com/machine-learning/crash-course)\n* [Crash Course](https://developers.google.com/machine-learning/crash-course/prereqs-and-prework)\nSend feedback# OverfittingStay organized with collectionsSave and categorize content based on your preferences.\n![Spark icon](https://developers.google.com/_static/images/icons/spark.svg)\n## Page Summary\noutlined\\_flag\n* Overfitting occurs when a model performs well on training data but poorly on new, unseen data.\n* A model is considered to generalize well if it accurately predicts on new data, indicating it hasn&#39;t overfit.\n* Overfitting can be detected by observing diverging loss curves for training and validation sets on a generalization curve.\n* Common causes of overfitting include unrepresentative training data and overly complex models.\n* Dataset conditions for good generalization include examples being independent, identically distributed, and stationary, with similar distributions across partitions.\n[**Overfitting**](https://developers.google.com/machine-learning/glossary#overfitting)means creating a model\nthat matches (*memorizes*) the[**training set**](https://developers.google.com/machine-learning/glossary#training-set)so\nclosely that the model fails to make correct predictions on new data.\nAn overfit model is analogous to an invention that performs well in the lab but\nis worthless in the real world.\n**Tip:**Overfitting is a common problem in machine learning, not an academic\nhypothetical.\nIn Figure 11, imagine that each geometric shape represents a tree&#39;s position\nin a square forest. The blue diamonds mark the locations of healthy trees,\nwhile the orange circles mark the locations of sick trees.\n![Figure 11. This figure contains about 60 dots, half of which are\nhealthy trees and the other half sick trees.\nThe healthy trees are mainly in the northeast quadrant, though a few\nhealthy trees sneak into the northwest quadrants. The sick trees\nare mainly in the southeast quadrant, but a few of the sick trees\nspill into other quadrants.](https://developers.google.com/static/machine-learning/crash-course/images/TreesTrainingSet.svg)**Figure 11.**Training set: locations of healthy and sick trees in a square forest.\nMentally draw any shapes\u2014lines, curves, ovals...anything\u2014to separate the\nhealthy trees from the sick trees. Then, expand the next line to examine\none possible separation.\n#### Expand to see one possible solution (Figure 12).\n![Figure 12. The same arrangement of healthy and sick trees as in\nFigure 11. However, a model of complex geometric shapes separates\nnearly all of the healthy trees from the sick trees.](https://developers.google.com/static/machine-learning/crash-course/images/TreesTrainingSetComplexModel.svg)**Figure 12.**A complex model for distinguishing sick from healthy trees.\nThe complex shapes shown in Figure 12 successfully categorized all but two of\nthe trees. If we think of the shapes as a model, then this is a fantastic\nmodel.\nOr is it? A truly excellent model successfully categorizes*new*examples.\nFigure 13 shows what happens when that same model makes predictions on new\nexamples from the test set:\n![Figure 13. A new batch of healthy and sick trees overlaid on the\nmodel shown in Figure 12. The model miscategorizes many of the\ntrees.](https://developers.google.com/static/machine-learning/crash-course/images/TreesTestSetComplexModel.svg)**Figure 13.**Test set: a complex model for distinguishing sick from healthy trees.\nSo, the complex model shown in Figure 12 did a great job on the training set\nbut a pretty bad job on the test set. This is a classic case of a model*overfitting*to the training set data.\n## Fitting, overfitting, and underfitting\nA model must make good predictions on*new*data.\nThat is, you&#39;re aiming to create a model that &quot;fits&quot; new data.\nAs you&#39;ve seen, an overfit model makes excellent predictions on the training\nset but poor predictions on new data. An[**underfit**](https://developers.google.com/machine-learning/glossary#underfitting)model\ndoesn&#39;t even make good predictions on the training data. If an overfit model is\nlike a product that performs well in the lab but poorly in the real world,\nthen an underfit model is like a product that doesn&#39;t even do well in\nthe lab.\n![Figure 14. Cartesian plot. X-axis is labeled 'quality of predictions\non training set.' Y-axis is labeled 'quality of predictions on\nreal-world data.' A curve starts at the origin and rises gradually,\nbut then falls just as quickly. The lower-left portion of the curve\n(low quality of predictions on real-world data and low quality of\npredictions on training set) is labeled 'underfit models.' The\nlower-right portion of the curve (low quality of predictions on\nreal-world data but high quality of predictions on training set)\nis labeled 'overfit models.' The peak of the curve (high quality\nof predictions on real-world data and medium quality of predictions\non training set) is labeled 'fit models.'](https://developers.google.com/static/machine-learning/crash-course/images/underfit_fit_overfit.svg)**Figure 14.**Underfit, fit, and overfit models.\n[**Generalization**](https://developers.google.com/machine-learning/glossary#generalization)is the\nopposite of overfitting. That is, a model that*generalizes well*makes good\npredictions on new data. Your goal is to create a model that generalizes\nwell to new data.\n## Detecting overfitting\nThe following curves help you detect overfitting:\n* loss curves\n* generalization curves\nA[**loss curve**](https://developers.google.com/machine-learning/glossary#loss-curve)plots a model&#39;s loss\nagainst the number of training iterations.\nA graph that shows two or more loss curves is called a[**generalization\ncurve**](https://developers.google.com/machine-learning/glossary#generalization-curve). The following\ngeneralization curve shows two loss curves:\n![Figure 15. The loss function for the training set gradually\ndeclines. The loss function for the validation set also declines,\nbut then it starts to rise after a certain number of iterations.](https://developers.google.com/static/machine-learning/crash-course/images/RegularizationTwoLossFunctions.png)**Figure 15.**A generalization curve that strongly implies overfitting.\nNotice that the two loss curves behave similarly at first and then diverge.\nThat is, after a certain number of iterations, loss declines or\nholds steady (converges) for the training set, but increases\nfor the validation set. This suggests overfitting.\nIn contrast, a generalization curve for a well-fit model shows two loss curves\nthat have similar shapes.\n## What causes overfitting?\nVery broadly speaking, overfitting is caused by one or both of the following\nproblems:\n* The training set doesn&#39;t adequately represent real life data (or the\nvalidation set or test set).\n* The model is too complex.## Generalization conditions\nA model trains on a training set, but the real test of a model&#39;s worth is how\nwell it makes predictions on new examples, particularly on real-world data.\nWhile developing a model, your test set serves as a proxy for real-world data.\nTraining a model that generalizes well implies the following dataset conditions:\n* Examples must be[**independently and identically distributed**](https://developers.google.com/machine-learning/glossary#independentl...",
      "url": "https://developers.google.com/machine-learning/crash-course/overfitting/overfitting"
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "Principled way of collapsing categorical variables with many levels?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Principled way of collapsing categorical variables with many levels?](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked9 years, 2 months ago\n\nModified [2 years, 11 months ago](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels?lastactivity)\n\nViewed\n41k times\n\n108\n\n$\\\\begingroup$\n\n**What techniques are available for collapsing (or pooling) many categories to a few, for the purpose of using them as an input (predictor) in a statistical model?**\n\n* * *\n\nConsider a variable like [college student major](https://en.wikipedia.org/wiki/Major_(academic)) (discipline chosen by an undergraduate student). It is unordered and categorical, but it can potentially have dozens of distinct levels. Let's say I want to use major as a predictor in a regression model.\n\nUsing these levels as-is for modeling leads to all sorts of issues because there are just so many. A lot of statistical precision would be thrown away to use them, and the results are hard to interpret. We're rarely interested in specific majors -- we're much more likely to be interested in broad categories (subgroups) of majors. But it isn't always clear how to divide up the levels into such higher-level categories, or even how many higher-level categories to use.\n\nFor typical data I would be happy to use factor analysis, matrix factorization, or a discrete latent modeling technique. But majors are mutually exclusive categories, so I'm hesitant to exploit their covariance for anything.\n\nFurthermore I don't care about the major categories on their own. I care about producing higher-level categories that are coherent _with respect to my regression outcome_. In the binary outcome case, that suggests to me something like linear discriminant analysis (LDA) to generate higher-level categories that maximize discriminative performance. But LDA is a limited technique and that feels like dirty data dredging to me. Moreover any continuous solution will be hard to interpret.\n\nMeanwhile something based on covariances, like multiple correspondence analysis (MCA), seems suspect to me in this case because of the inherent dependence among mutually exclusive dummy variables -- they're better suited for studying multiple categorical variables, rather than multiple categories of the same variable.\n\n**edit**: to be clear, this is about _collapsing_ categories (not selecting them), and the categories are predictors or independent variables. In hindsight, this problem seems like an appropriate time to \"regularize 'em all and let God sort 'em out\". Glad to see this question is interesting to so many people!\n\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [categorical-data](https://stats.stackexchange.com/questions/tagged/categorical-data)\n- [feature-engineering](https://stats.stackexchange.com/questions/tagged/feature-engineering)\n- [many-categories](https://stats.stackexchange.com/questions/tagged/many-categories)\n- [faq](https://stats.stackexchange.com/questions/tagged/faq)\n\n[Share](https://stats.stackexchange.com/q/146907)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/146907/edit)\n\nFollow\n\n[edited Feb 11, 2021 at 8:43](https://stats.stackexchange.com/posts/146907/revisions)\n\n[![kjetil b halvorsen's user avatar](https://i.sstatic.net/Nri78.jpg?s=64)](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen)\n\n[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\n80.4k3131 gold badges196196 silver badges638638 bronze badges\n\nasked Apr 17, 2015 at 13:31\n\n[![shadowtalker's user avatar](https://www.gravatar.com/avatar/c5be99f2479a74b12e1e62f4a4260888?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/36229/shadowtalker)\n\n[shadowtalker](https://stats.stackexchange.com/users/36229/shadowtalker) shadowtalker\n\n12.7k44 gold badges5757 silver badges122122 bronze badges\n\n$\\\\endgroup$\n\n10\n\n- 3\n\n\n\n\n\n$\\\\begingroup$In my comment at [stats.stackexchange.com/questions/230636/\u2026](http://stats.stackexchange.com/questions/230636/how-to-deal-with-large-number-of-dummy-variables-in-machine-learning#comment436906_230636) I have a long list of similar questions in here! Have a look ... Also, search this site with keywords \"many levels\" you find many similar questions, few good answers.$\\\\endgroup$\n\n\u2013\u00a0[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\nCommentedSep 20, 2016 at 16:45\n\n- 3\n\n\n\n\n\n$\\\\begingroup$I will come back to this when I have time ... in the meantime, the following is a very relevant paper which seems to answer the question, in part: [epub.ub.uni-muenchen.de/12164/1/petry\\_etal\\_TR102\\_2011.pdf](https://epub.ub.uni-muenchen.de/12164/1/petry_etal_TR102_2011.pdf)$\\\\endgroup$\n\n\u2013\u00a0[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\nCommentedSep 20, 2016 at 17:03\n\n- 2\n\n\n\n\n\n$\\\\begingroup$I don't think I get the question, my natural impulse would actually be to add more dummy variables to encode the hierarchies (arguably as many different hierarchies as you can think of ) and then use L1/L2 regularisation to ensure that top level categories are selected rather than the finer level categories. the problem with majors etc is that there is obviously no similarity ( when represented as a dummy variable ) so to get a good model (which allows generalisation) you need to provide that similarity$\\\\endgroup$\n\n\u2013\u00a0[seanv507](https://stats.stackexchange.com/users/27556/seanv507)\n\nCommentedSep 26, 2016 at 15:54\n\n- 1\n\n\n\n\n\n$\\\\begingroup$@ssdecontrol, yea, my prototypical example is telephone numbers (or other IDs) . Basically the correct answer when asked how to model using these is - don't!$\\\\endgroup$\n\n\u2013\u00a0[seanv507](https://stats.stackexchange.com/users/27556/seanv507)\n\nCommentedSep 26, 2016 at 16:21\n\n- 3\n\n\n\n\n\n$\\\\begingroup$If you want to infer hierarchies, you can look into Neural network embedding schemes. Essentially they use a reduced set of neurons between categories and rest of model, so that model must find linear projections of similar categories in solving. It is essentially a non linearised version of factorisation machines. [tensorflow.org/guide/embedding](https://www.tensorflow.org/guide/embedding)$\\\\endgroup$\n\n\u2013\u00a0[seanv507](https://stats.stackexchange.com/users/27556/seanv507)\n\nCommentedDec 30, 2018 at 10:29\n\n\n\\|\u00a0[Show **5** more comments](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels)\n\n## 6 Answers 6\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n64\n\n+100\n\n$\\\\begingroup$\n\nIf I understood correctly, you imagine a linear model where one of the predictors is categorical (e.g. college major); and you expect that for some subgroups of its levels (subgroups of categories) the coefficients might be exactly the same. So perhaps the regression coefficients for Maths and Physics are the same, but different from those for Chemistry and Biology.\n\nIn a simplest case, you would have a \"one way ANOVA\" linear model with a single categorical predictor: $$y\\_{ij} = \\\\mu + \\\\alpha\\_i + \\\\epsilon\\_{ij},$$ where $i$ encodes the level of the categorical variable (the category). But you might prefer a solution that collapses some levels (categories) together, e.g. $$\\\\begin{cases}\\\\alpha\\_1=\\\\alpha\\_2, \\\\\\ \\\\alpha\\_3=\\\\alpha\\_4=\\\\alpha\\_5.\\\\end{cases}$$\n\nThis suggests that one can try to use a regularization penalty that wou...",
      "url": "https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels"
    },
    {
      "title": "Target Encoder\u2019s Internal Cross fitting #",
      "text": "Target Encoder\u2019s Internal Cross fitting &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\nNote\n[Go to the end](#sphx-glr-download-auto-examples-preprocessing-plot-target-encoder-cross-val-py)to download the full example code or to run this example in your browser via JupyterLite or Binder.\n# Target Encoder\u2019s Internal Cross fitting[#](#target-encoder-s-internal-cross-fitting)\nThe[`TargetEncoder`](../../modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)replaces each category of a categorical feature with\nthe shrunk mean of the target variable for that category. This method is useful\nin cases where there is a strong relationship between the categorical feature\nand the target. To prevent overfitting,[`TargetEncoder.fit\\_transform`](../../modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform)uses\nan internal[cross fitting](../../glossary.html#term-0)scheme to encode the training data to be used\nby a downstream model. This scheme involves splitting the data into*k*folds\nand encoding each fold using the encodings learnt using the*other k-1*folds.\nIn this example, we demonstrate the importance of the cross\nfitting procedure to prevent overfitting.\n```\n# Authors: The scikit-learn developers# SPDX-License-Identifier: BSD-3-Clause\n```\n## Create Synthetic Dataset[#](#create-synthetic-dataset)\nFor this example, we build a dataset with three categorical features:\n* an informative feature with medium cardinality (\u201cinformative\u201d)\n* an uninformative feature with medium cardinality (\u201cshuffled\u201d)\n* an uninformative feature with high cardinality (\u201cnear\\_unique\u201d)\nFirst, we generate the informative feature:\n```\nimportnumpyasnpfromsklearn.preprocessingimport[KBinsDiscretizer](../../modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer)n\\_samples=50\\_000rng=[np.random.RandomState](https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState)(42)y=rng.randn(n\\_samples)noise=0.5\\*rng.randn(n\\_samples)n\\_categories=100kbins=[KBinsDiscretizer](../../modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer)(n\\_bins=n\\_categories,encode=&quot;ordinal&quot;,strategy=&quot;uniform&quot;,random\\_state=rng,subsample=None,)X\\_informative=kbins.fit\\_transform((y+noise).reshape(-1,1))# Remove the linear relationship between y and the bin index by permuting the# values of X\\_informative:permuted\\_categories=rng.permutation(n\\_categories)X\\_informative=permuted\\_categories[X\\_informative.astype([np.int32](https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.int32))]\n```\nThe uninformative feature with medium cardinality is generated by permuting the\ninformative feature and removing the relationship with the target:\n```\nX\\_shuffled=rng.permutation(X\\_informative)\n```\nThe uninformative feature with high cardinality is generated so that it is\nindependent of the target variable. We will show that target encoding without[cross fitting](../../glossary.html#term-0)will cause catastrophic overfitting for the downstream\nregressor. These high cardinality features are basically unique identifiers\nfor samples which should generally be removed from machine learning datasets.\nIn this example, we generate them to show how[`TargetEncoder`](../../modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)\u2019s default[cross fitting](../../glossary.html#term-0)behavior mitigates the overfitting issue automatically.\n```\nX\\_near\\_unique\\_categories=rng.choice(int(0.9\\*n\\_samples),size=n\\_samples,replace=True).reshape(-1,1)\n```\nFinally, we assemble the dataset and perform a train test split:\n```\nimportpandasaspdfromsklearn.model\\_selectionimport[train\\_test\\_split](../../modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)X=[pd.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame)([np.concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html#numpy.concatenate)([X\\_informative,X\\_shuffled,X\\_near\\_unique\\_categories],axis=1,),columns=[&quot;informative&quot;,&quot;shuffled&quot;,&quot;&quot;near\\_unique&quot;&quot;],)X\\_train,X\\_test,y\\_train,y\\_test=[train\\_test\\_split](../../modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)(X,y,random\\_state=0)\n```\n## Training a Ridge Regressor[#](#training-a-ridge-regressor)\nIn this section, we train a ridge regressor on the dataset with and without\nencoding and explore the influence of target encoder with and without the\ninternal[cross fitting](../../glossary.html#term-0). First, we see the Ridge model trained on the\nraw features will have low performance. This is because we permuted the order\nof the informative feature meaning`X\\_informative`is not informative when\nraw:\n```\nimportsklearnfromsklearn.linear\\_modelimport[Ridge](../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)# Configure transformers to always output DataFrames[sklearn.set\\_config](../../modules/generated/sklearn.set_config.html#sklearn.set_config)(transform\\_output=&quot;pandas&quot;)ridge=[Ridge](../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)(alpha=1e-6,solver=&quot;lsqr&quot;,fit\\_intercept=False)raw\\_model=ridge.fit(X\\_train,y\\_train)print(&quot;Raw Model score on training set: &quot;,raw\\_model.score(X\\_train,y\\_train))print(&quot;Raw Model score on test set: &quot;,raw\\_model.score(X\\_test,y\\_test))\n```\n```\nRaw Model score on training set: 0.0049896314219657345\nRaw Model score on test set: 0.0045776215814927745\n```\nNext, we create a pipeline with the target encoder and ridge model. The pipeline\nuses[`TargetEncoder.fit\\_transform`](../../modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder.fit_transform)which uses[cross fitting](../../glossary.html#term-0). We\nsee that the model fits the data well and generalizes to the test set:\n```\nfromsklearn.pipelineimport[make\\_pipeline](../../modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)fromsklearn.preprocessingimport[TargetEncoder](../../modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)model\\_with\\_cf=[make\\_pipeline](../../modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)([TargetEncoder](../../modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)(random\\_state=0),ridge)model\\_with\\_cf.fit(X\\_train,y\\_train)print(&quot;Model with CF on train set: &quot;,model\\_with\\_cf.score(X\\_train,y\\_train))print(&quot;Model with CF on test set: &quot;,model\\_with\\_cf.score(X\\_test,y\\_test))\n```\n```\nModel with CF on train set: 0.8000184677460304\nModel with CF on test set: 0.7927845601690917\n```\nThe coefficients of the linear model shows that most of the weight is on the\nfeature at column index 0, which is the informative feature\n```\nimportmatplotlib.pyplotaspltimportpandasaspd[plt.rcParams](https://matplotlib.org/stable/api/matplotlib_configuration_api.html#matplotlib.rcParams)[&quot;&quot;figure.constrained\\_layout.use&quot;&quot;]=Truecoefs\\_cf=[pd.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series)(model\\_with\\_cf[-1].coef\\_,index=model\\_with\\_cf[-1].feature\\_names\\_in\\_).sort\\_values()ax=coefs\\_cf.plot(kind=&quot;barh&quot;)...",
      "url": "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder_cross_val.html"
    },
    {
      "title": "Underfitting and Overfitting - Kaggle",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Fdansbecker%2Funderfitting-and-overfitting)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Fdansbecker%2Funderfitting-and-overfitting)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting"
    },
    {
      "title": "Don't Overfit II- A Kaggle Competition. | by Ayush Khandelwal",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd087344b845e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akwsir96%2Fdont-overfit-ii-a-kaggle-competition-d087344b845e&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40akwsir96%2Fdont-overfit-ii-a-kaggle-competition-d087344b845e&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Don\u2019t Overfit II- A Kaggle Competition.\n\n[Ayush Khandelwal](https://medium.com/@akwsir96?source=post_page---byline--d087344b845e---------------------------------------)\n\n12 min read\n\n\u00b7\n\nAug 2, 2022\n\n--\n\nListen\n\nShare\n\nOverfitting\n\n# Table Of Contents\n\n01. Introduction\n02. Prerequisites\n03. Kaggle Competition\n04. Existing Approaches\n05. My Approach\n06. Exploratory Data Analysis(EDA)\n07. Feature Engineering\n08. First Cut Solution\n09. Modeling\n10. Best Model\n11. Video Representation\n12. Conclusion\n13. Future Work\n14. References\n\n# 1\\. Introduction\n\nOverfitting is a trivial problem when building any machine learning or deep neural network model.Overfitting means when a statistical model or neural network learns patterns present in the training data but is unable to perform on unseen test data thereby defeating the algorithm purpose.There are several reasons for overfitting but the common reasons are as:\n\n1. Data used for training is not cleaned and contains noise in it.\n2. The model has a high variance\n3. The size of the training dataset used is not enough\n4. The model is too complex\n\nThe Kaggle competition Don\u2019t Overfit II is based on the above third point.This blog uses machine learning as well as deep learning models to tackle the competition.\n\n# 2\\. Prerequisites\n\nThis blog assumes readers have basic understanding of Overfitting,Machine Learning Algorithms,Scaling Methods,Transformation,Oversampling, Basic Neural Network,Python,Libraries.\n\n# 3\\. Kaggle Competition\n\nDon\u2019t overfit II is kaggle problem where model is made with 250 training data points and tested on 19750 test data points given a very small amount of training data.\n\nAccording to kaggle,\n\n> _\u201cIt was not just any competition._\n>\n> _It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples\u2026 without overfitting.\u201d_\n\n## Dataset\n\nThe dataset can be downloaded from [here](https://www.kaggle.com/c/dont-overfit-ii)\n\nThe dataset consist of train and test csv.\n\nTraining data:\n\nThe training data consist of 250 points and 302 features including id and target variable\n\nTesting data:\n\nThe testing data consist of 19750 points and 300 features including id\n\n## 3\\. Evaluation Metric\n\nThe score will be evaluated on the [AUCROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) between actual values and predicted values.\n\nThe AUCROC is a performance measurement for various threshold settings.The ROC curve is created by plotting the TPR(true positive rate) against the FPR(false positive rate) at various threshold settings.\n\n# 4\\. Existing Approaches\n\n## [1\\. Just Don\u2019t Overfit II](https://medium.com/analytics-vidhya/just-dont-overfit-e2fddd28eb29)\n\nThe approach this blog uses is LASSOCV (Least Absolute Shrinkage and Selection Operator Cross-Validation). It is made up of two terms LASSO and CV (where CV is Cross-Validation).This algorithm performs L1 regularisation as it creates sparsity and add some bias term to minimize the variance of the model.\n\n## 2\\. [Don\u2019t Overfit-How to prevent overfitting in your deep learning models](https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323)\n\nThe approach in this blog uses basic MLP with 128 and 64 neurons as two hidden layers.Since it\u2019s a two class classification problem,binary\\_crossentropy is used as loss function.This leads to overfitting of model.In the next approach author uses 8 neuron network and use early stop and add dropout layer with dropout of (0.4)to avoid overfitting.For this model the kaggle score was **80%**\n\n## Research Paper\n\n## 3\\. [An Overview on Overfitting and its solutions](https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022/pdf)\n\nThis paper tells about the overfitting problems and its solutions.There are various ways in which overfitting can happen viz is limited training dataset,data has some noise in it, complexity of classifiers.\n\nThe author proposed various solutions to overcome it.\n\n1. Early stopping- It means you can stop your training early when your accuracy is not improving and your gap between training error and validation error is increasing after some epochs\n\nOverfitting (Loss increases between training error and test error) Blue Line -Training Error ,Red Line-Test Error\n\n2.Network Reduction- This method is proposed to reduce noise in our dataset.There are basically a method to reduce noise in classifiers is Pruning. Again is pruning is of two types.\n\na. pre-pruning-It functions learning during process.It adds conditions and rules in the model while learning the model.\n\nb. post-pruning-It prevents overfitting by deleting the rules and conditions generated during the model.\n\n3\\. Expansion of Training data: overfitting can happen when the training data is small. Therefore the dataset must be large enough so that model cannot be adjusted to noise and outliers. Approaches to add data in existing data:\n\na.Acquire more training data\n\nb.Add noise to the data\n\nc. Re-acquire some data from existing data set through some processing\n\n4.Regularization:Add L2(Ridge) and L1(Lasso) regularization to prevent overfitting.Regularization is done to remove useless features(L1) or to give less weight to useless features(L2) when features are large.\n\n# 5\\. My Approach\n\n1. Apply Feature Engineering\n2. Selected top features using **RFECV(Recursive feature elimination with cross-validation**) to make models.\n3. Uses **Feature Transformation**(box cox transformation and log transformation) on features.\n4. Apply Oversampling technique called SMOTE(S **ynthetic Minority Oversampling Techniques**).\n5. Uses 2 feature scaling techniques : **Standardization and Robust Scaler**.\n6. Work on 7 datasets which are as follows:\n\na. **DATASET 1**-Reduced featureset using RFECV along with standardization/Robust Scaler.\n\nb. **DATASET 2**-Dataset with only engineered features along with Standardization/Robust Scaler\n\nc. **DATASET 3** \u2014 Dataset with transformed features on the reduced dimensionality along with standardization/Robust Scaler\n\nd. **DATASET 4**\u2014 PCA on the original features. Taken only those many principal components which would explain 95% of the variance in the data.\n\ne. **DATASET 5**\u2014 Oversampling the data to make it balanced and also on top of that reduced feature set along with feature transformation and standardization/Robust Scaler\n\nf. **DATASET 6**\u2014 PCA on the reduced feature set.\n\ng. **DATASET 7**\u2014 Dataset with transformed features on the reduced dimensionality along with standardization/Robust Scaler and oversampling.\n\n7\\. Models used in this competition are:\n\na. Logistic Regression\n\nb. Support Vector Machine\n\nc. Decision Tree classifier\n\nd. Random Forest Classifier\n\ne. Xgboost Classifier\n\nf. SGDClassifier\n\ng. Voting and Stacking Classifier\n\n## SMOTE\n\nSMOTE is an algorithm that creates **s...",
      "url": "https://medium.com/@akwsir96/dont-overfit-ii-a-kaggle-competition-d087344b845e"
    },
    {
      "title": "Overfitting Vs. Underfitting: The Hidden Flaw In Your Predictive ...",
      "text": "[00\\\n\\\nDAYS\\\n\\\n00\\\n\\\nHRS\\\n\\\n00\\\n\\\nMIN\\\n\\\n00\\\n\\\nSEC\\\n\\\nFALL\u00a0PRODUCT\u00a0LAUNCH\\\n\\\nWATCH ON DEMAND](https://www.sigmacomputing.com/product-launch/fall-2025)\n\n[See\\\n\\\nWatch on-DEMAND DEMOS](https://www.sigmacomputing.com/resources/on-demand-webinars)\n\n[Experience\\\n\\\nATTEND AN EVENT](https://www.sigmacomputing.com/resources/live-events)\n\n[Try Sigma\\\n\\\nGet a free trial](https://www.sigmacomputing.com/go/free-trial)\n\n[Explore\\\n\\\nINTERACTIVE DEMOS](https://www.sigmacomputing.com/resources/library-index)\n\n[Connect\\\n\\\nJOIN THE COMMUNITY](https://community.sigmacomputing.com)\n\n[Meet\\\n\\\nSCHEDULE A CALL](https://www.sigmacomputing.com/go/request-a-demo)\n\nGet Sigma in\n\nyour inbox",
      "url": "https://www.sigmacomputing.com/blog/overfitting-vs-underfitting"
    },
    {
      "title": "Mastering Kaggle Competitions - Analytics Vidhya",
      "text": "[Master Generative AI with 10+ Real-world Projects in 2025!\\\n\\\n- d:\\\n- h:\\\n- m:\\\n- s\\\n\\\nDownload Projects](https://www.analyticsvidhya.com/pinnacleplus/pinnacleplus-projects?utm_source=blog_india&utm_medium=desktop_flashstrip&utm_campaign=15-Feb-2025||&utm_content=projects)\n\n[Interview Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)\n\n[Career](https://www.analyticsvidhya.com/blog/category/career/?ref=category)\n\n[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=category)\n\n[Prompt Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)\n\n[ChatGPT](https://www.analyticsvidhya.com/blog/category/chatgpt/?ref=category)\n\n[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=category)\n\n[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)\n\n[RAG](https://www.analyticsvidhya.com/blog/category/rag/?ref=category)\n\n[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)\n\n[Machine Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)\n\n[Deep Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)\n\n[GenAI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)\n\n[LLMOps](https://www.analyticsvidhya.com/blog/category/llmops/?ref=category)\n\n[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)\n\n[NLP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)\n\n[SQL](https://www.analyticsvidhya.com/blog/category/sql/?ref=category)\n\n[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category)\n\n#### Reading list\n\n##### Basics of Machine Learning\n\n[Machine Learning Basics for a Newbie](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)\n\n##### Machine Learning Lifecycle\n\n[6 Steps of Machine learning Lifecycle](https://www.analyticsvidhya.com/blog/2020/09/10-things-know-before-first-data-science-project/) [Introduction to Predictive Modeling](https://www.analyticsvidhya.com/blog/2015/09/build-predictive-model-10-minutes-python/)\n\n##### Importance of Stats and EDA\n\n[Introduction to Exploratory Data Analysis & Data Insights](https://www.analyticsvidhya.com/blog/2021/02/introduction-to-exploratory-data-analysis-eda/) [Descriptive Statistics](https://www.analyticsvidhya.com/blog/2021/06/how-to-learn-mathematics-for-machine-learning-what-concepts-do-you-need-to-master-in-data-science/) [Inferential Statistics](https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/) [How to Understand Population Distributions?](https://www.analyticsvidhya.com/blog/2014/07/statistics/)\n\n##### Understanding Data\n\n[Reading Data Files into Python](https://www.analyticsvidhya.com/blog/2021/09/how-to-extract-tabular-data-from-doc-files-using-python/) [Different Variable Datatypes](https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-data-types-in-statistics-for-data-science/)\n\n##### Probability\n\n[Probability for Data Science](https://www.analyticsvidhya.com/blog/2021/03/statistics-for-data-science/) [Basic Concepts of Probability](https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/) [Axioms of Probability](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/) [Conditional Probability](https://www.analyticsvidhya.com/blog/2017/03/conditional-probability-bayes-theorem/)\n\n##### Exploring Continuous Variable\n\n[Central Tendencies for Continuous Variables](https://www.analyticsvidhya.com/blog/2021/07/the-measure-of-central-tendencies-in-statistics-a-beginners-guide/) [Spread of Data](https://www.analyticsvidhya.com/blog/2021/04/dispersion-of-data-range-iqr-variance-standard-deviation/) [KDE plots for Continuous Variable](https://www.analyticsvidhya.com/blog/2020/07/univariate-analysis-visualization-with-illustrations-in-python/) [Overview of Distribution for Continuous variables](https://www.analyticsvidhya.com/blog/2015/11/8-ways-deal-continuous-variables-predictive-modeling/) [Normal Distribution](https://www.analyticsvidhya.com/blog/2020/04/statistics-data-science-normal-distribution/) [Skewed Distribution](https://www.analyticsvidhya.com/blog/2021/05/how-to-transform-features-into-normal-gaussian-distribution/) [Skeweness and Kurtosis](https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/) [Distribution for Continuous Variable](https://www.analyticsvidhya.com/blog/2021/07/probability-types-of-probability-distribution-functions/)\n\n##### Exploring Categorical Variables\n\n[Central Tendencies for Categorical Variables](https://www.analyticsvidhya.com/blog/2021/04/3-central-tendency-measures-mean-mode-median/) [Understanding Discrete Distributions](https://www.analyticsvidhya.com/blog/2021/01/discrete-probability-distributions/) [Performing EDA on Categorical Variables](https://www.analyticsvidhya.com/blog/2020/08/exploratory-data-analysiseda-from-scratch-in-python/)\n\n##### Missing Values and Outliers\n\n[Dealing with Missing Values](https://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/) [Understanding Outliers](https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/) [Identifying Outliers in Data](https://www.analyticsvidhya.com/blog/2021/07/how-to-treat-outliers-in-a-data-set/) [Outlier Detection in Python](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/) [Outliers Detection Using IQR, Z-score, LOF and DBSCAN](https://www.analyticsvidhya.com/blog/2022/08/dealing-with-outliers-using-the-z-score-method/)\n\n##### Central Limit theorem\n\n[Sample and Population](https://www.analyticsvidhya.com/blog/2021/06/introductory-statistics-for-data-science/) [Central Limit Theorem](https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/) [Confidence Interval and Margin of Error](https://www.analyticsvidhya.com/blog/2021/08/intermediate-statistical-concepts-for-data-science/)\n\n##### Bivariate Analysis Introduction\n\n[Bivariate Analysis Introduction](https://www.analyticsvidhya.com/blog/2021/04/top-python-libraries-to-automate-exploratory-data-analysis-in-2021/)\n\n##### Continuous - Continuous Variables\n\n[Covariance](https://www.analyticsvidhya.com/blog/2021/09/different-type-of-correlation-metrics-used-by-data-scientist/) [Pearson Correlation](https://www.analyticsvidhya.com/blog/2021/01/beginners-guide-to-pearsons-correlation-coefficient/) [Spearman's Correlation & Kendall's Tau](https://www.analyticsvidhya.com/blog/2021/03/comparison-of-pearson-and-spearman-correlation-coefficients/) [Correlation versus Causation](https://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/) [Tabular and Graphical methods for Bivariate Analysis](https://www.analyticsvidhya.com/blog/2020/10/the-clever-ingredient-that-decide-the-rise-and-the-fall-of-your-machine-learning-model-exploratory-data-analysis/) [Performing Bivariate Analysis on Continuous-Continuous Variables](https://www.analyticsvidhya.com/blog/2022/03/exploratory-data-analysis-eda-credit-card-fraud-detection-case-study/)\n\n##### Continuous Categorical\n\n[Tabular and Graphical methods for Continuous-Categorical Variables](https://www.analyticsvidhya.com/blog/2015/05/data-visualization-resource/) [Introduction to Hypothesis Testing](https://www.analyticsvidhya.com/blog/2021/09/hypothesis-testing-in-machine-learning-everything-you-need-to-know/) [P-value](https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/) [Two sample Z-test](https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/) [T-test](https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/) [T-test vs Z-test](https://www.analyticsvidhya.com/blog/2021/06/featu...",
      "url": "https://www.analyticsvidhya.com/blog/2024/09/mastering-kaggle-competitions"
    }
  ]
}