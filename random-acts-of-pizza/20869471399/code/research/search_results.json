{
  "query": "What are the most effective ensembling strategies for tabular data competitions on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Effective ensembling on Kaggle\u2019s tabular competitions usually combines three ideas: **bagging diverse base learners, multi\u2011layer stacking, and parameter\u2011efficient ensembles that squeeze many predictions out of a single model**.  AutoGluon, which consistently tops public benchmarks, exemplifies this recipe: it trains a variety of models (gradient\u2011boosted trees, linear models, neural nets), applies **bagging with cross\u2011validation** to each learner, and then **stack\u2011ensembles** the bagged models in several layers to produce a \u201csuper\u2011model\u201d that outperforms any single component\u202f([auto.gluon.ai](https://auto.gluon.ai/stable/tutorials/tabular/how-it-works.html); [arXiv\u202f2003.06505](https://arxiv.org/abs/2003.06505)).  This approach is highlighted in the Kaggle Grandmasters Playbook as one of the battle\u2011tested techniques for tabular data\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\nA newer, highly efficient twist is **parameter\u2011efficient ensembling** such as **BatchEnsemble**, implemented in the TabM architecture.  TabM trains several implicit MLPs that share most parameters and emit multiple predictions per instance; the individual outputs are weak, but their collective vote is strong, delivering state\u2011of\u2011the\u2011art performance with far less memory and compute than a traditional deep ensemble\u202f([arxiv.org/html/2410.24210v3](https://arxiv.org/html/2410.24210v3)).  Because TabM can be treated as a single model, it is especially attractive when competition time limits restrict the number of full models that can be trained.\n\nIn practice, the most successful Kaggle solutions blend these ideas: they **bag multiple seeds of gradient\u2011boosted trees (e.g., LightGBM, CatBoost, XGBoost)**, **stack them with linear or neural\u2011network learners**, and often **add a TabM\u2011style parameter\u2011efficient ensemble** as an additional \u201cdeep\u201d component.  Real\u2011world evidence shows TabM alone secured a winning Kaggle solution and high\u2011rank placements without any external models, and when combined with other learners it further pushes the leaderboard\u202f([qiaoyang.github.io](https://qiaoyang-blog.github.io/qiaoyang.github.io)).  Thus, a combination of bagging\u202f+\u202fstacking\u202f+\u202fparameter\u2011efficient deep ensembles represents the current best practice for tabular Kaggle competitions.",
      "url": ""
    },
    {
      "title": "TabM : Advancing Tabular Deep Learning\n with Parameter-Efficient Ensembling",
      "text": "Yury Gorishniy\n\nYandex\n&Akim Kotelnikov\n\nHSE University, Yandex\n&Artem Babenko\n\nYandex\n\n###### Abstract\n\nDeep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for designing substantially better MLP-based tabular architectures.\nNamely, our new model TabM relies on efficient ensembling, where one TabM efficiently imitates an ensemble of MLPs and produces multiple predictions per object.\nCompared to a traditional deep ensemble, in TabM, the underlying implicit MLPs are trained simultaneously, and (by default) share most of their parameters, which results in significantly better performance and efficiency.\nUsing TabM as a new baseline, we perform a large-scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nGenerally, we show that MLPs, including TabM, form a line of stronger and more practical models compared to attention- and retrieval-based architectures.\nIn particular, we find that TabM\u00a0demonstrates the best performance among tabular DL models.\nThen, we conduct an empirical analysis on the ensemble-like nature of TabM.\nWe observe that the multiple predictions of TabM\u00a0are weak individually, but powerful collectively.\nOverall, our work brings an impactful technique to tabular DL and advances the performance-efficiency trade-off with TabM\u00a0\u2014 a simple and powerful baseline for researchers and practitioners.\nThe code is available at: [https://github.com/yandex-research/tabm](https://github.com/yandex-research/tabm).\n\n## 1 Introduction\n\nSupervised learning on tabular data is a ubiquitous machine learning (ML) scenario in a wide range of industrial applications.\nAmong classic non-deep-learning methods, the state-of-the-art solution for such tasks is gradient-boosted decision trees (GBDT) (Prokhorenkova et\u00a0al., [2018](https://arxiv.org/html/2410.24210v3#bib.bib35); Chen & Guestrin, [2016](https://arxiv.org/html/2410.24210v3#bib.bib10); Ke et\u00a0al., [2017](https://arxiv.org/html/2410.24210v3#bib.bib23)).\nDeep learning (DL) models for tabular data, in turn, are reportedly improving, and the most recent works claim to perform on par or even outperform GBDT on academic benchmarks (Hollmann et\u00a0al., [2023](https://arxiv.org/html/2410.24210v3#bib.bib18); Chen et\u00a0al., [2023b](https://arxiv.org/html/2410.24210v3#bib.bib9); [a](https://arxiv.org/html/2410.24210v3#bib.bib8); Gorishniy et\u00a0al., [2024](https://arxiv.org/html/2410.24210v3#bib.bib15)).\n\nHowever, from the practical perspective, it is unclear if tabular DL offers any obvious go-to baselines beyond simple architectures in the spirit of a multilayer perceptron (MLP).\nFirst, the scale and consistency of performance improvements of new methods w.r.t. simple MLP-like baselines are not always explicitly analyzed in the literature.\nThus, one has to infer those statistics from numerous per-dataset performance scores, which makes it hard to reason about the progress.\nAt the same time, due to the extreme diversity of tabular datasets, consistency is an especially valuable and hard-to-achieve property for a hypothetical go-to baseline.\nSecond, efficiency-related properties, such as training time, and especially inference throughput, sometimes receive less attention.\nWhile methods are usually equally affordable on small-to-medium datasets (e.g. 100K objects), their applicability to larger datasets remains uncertain.\nThird, some recent work generally suggests that the progress on academic benchmarks may not transfer that well to real-world tasks (Rubachev et\u00a0al., [2024](https://arxiv.org/html/2410.24210v3#bib.bib38)).\nWith all the above in mind, in this work, we thoroughly evaluate existing tabular DL methods and find that non-MLP models do not yet offer a convincing replacement for MLPs.\n\nAt the same time, we identify a previously overlooked path towards more powerful, reliable, and reasonably efficient tabular DL models.\nIn a nutshell, we find that the parameter-efficient approach to deep ensembling, where most weights are shared between ensemble members, allow one to make simple and strong tabular models out of plain MLPs.\nFor example, MLP coupled with BatchEnsemble (Wen et\u00a0al., [2020](https://arxiv.org/html/2410.24210v3#bib.bib45)) \u2014 a long-existing method \u2014 right away outperforms popular attention-based models, such as FT-Transformer (Gorishniy et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib13)), while being simpler and more efficient.\nThis result alone suggests that efficient ensembling is a low-hanging fruit for tabular DL.\n\nOur work builds on the above observations and offers TabM\u00a0\u2014 a new powerful and practical model for researchers and practitioners.\nDrawing an informal parallel with GBDT (an ensemble of decision trees), TabM\u00a0can also be viewed as a simple base model (MLP) combined with an ensembling-like technique, providing high performance and simple implementation at the same time.\n\nMain contributions.\nWe summarize our main contributions as follows:\n\n1. 1.\nWe present TabM\u00a0\u2014 a simple DL architecture for supervised learning on tabular data.\nTabM\u00a0is based on MLP and parameter-efficient ensembling techniques closely related to BatchEnsemble (Wen et\u00a0al., [2020](https://arxiv.org/html/2410.24210v3#bib.bib45)).\nIn particular, TabM\u00a0produces Multiple predictions per object.\nTabM\u00a0easily competes with GBDT and outperforms prior tabular DL models, while being more efficient than attention- and retrieval-based DL architectures.\n\n2. 2.\nWe provide a fresh perspective on tabular DL models in a large-scale evaluation along four dimensions: performance ranks, performance score distributions, training time, and inference throughput.\nOne of our findings is that MLPs, including TabM, hit an appealing performance-efficiency tradeoff, which is not the case for attention- and retrieval-based models.\n\n3. 3.\nWe show that the two key reasons for TabM\u2019s high performance are the collective training of the underlying implicit MLPs and the weight sharing.\nWe also show that the multiple predictions of TabM\u00a0are weak and overfitted individually, while their average is strong and generalizable.\n\n\n## 2 Related work\n\nDecision-tree-based models.\nGradient-boosted decision trees (GBDT) (Chen & Guestrin, [2016](https://arxiv.org/html/2410.24210v3#bib.bib10); Ke et\u00a0al., [2017](https://arxiv.org/html/2410.24210v3#bib.bib23); Prokhorenkova et\u00a0al., [2018](https://arxiv.org/html/2410.24210v3#bib.bib35)) is a strong and efficient baseline for tabular tasks.\nGBDT is a classic machine learning model, specifically, an ensemble of decision trees.\nOur model TabM\u00a0is a deep learning model, specifically, a parameter-efficient ensemble of MLPs.\n\nTabular deep learning architectures.\nA large number of deep learning architectures for tabular data have been proposed over the recent years.\nThat includes attention-based architectures (Song et\u00a0al., [2019](https://arxiv.org/html/2410.24210v3#bib.bib40); Gorishniy et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib13); Somepalli et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib39); Kossen et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib26); Yan et\u00a0al., [2023](https://arxiv.org/html/2410.24210v3#bib.bib46)), retrieval-augmented architectures (Somepalli et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib39); Kossen et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib26); Gorishniy et\u00a0al., [2024](https://arxiv.org/html/2410.24210v3#bib.bib15); Ye et\u00a0al., [2024](https://arxiv.org/html/2410.24210v3#bib.bib47)), MLP-like models (Gorishniy et\u00a0al., [2021](https://arxiv.org/html/2410.24210v3#bib.bib13); Klambauer et\u00a0al., [2017](https://arxiv.org/html/2410.24210v3#bib.bib25); Wang et\u00a0al., [2020](https://arxiv.org/html/2410.24210v3#bib.bib44)) and others (Arik & Pfister, [2020](http...",
      "url": "https://arxiv.org/html/2410.24210v3"
    },
    {
      "title": "AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data",
      "text": "# Statistics > Machine Learning\n\n**arXiv:2003.06505** (stat)\n\n\\[Submitted on 13 Mar 2020\\]\n\n# Title:AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data\n\nAuthors: [Nick Erickson](https://arxiv.org/search/stat?searchtype=author&query=Erickson,+N), [Jonas Mueller](https://arxiv.org/search/stat?searchtype=author&query=Mueller,+J), [Alexander Shirkov](https://arxiv.org/search/stat?searchtype=author&query=Shirkov,+A), [Hang Zhang](https://arxiv.org/search/stat?searchtype=author&query=Zhang,+H), [Pedro Larroy](https://arxiv.org/search/stat?searchtype=author&query=Larroy,+P), [Mu Li](https://arxiv.org/search/stat?searchtype=author&query=Li,+M), [Alexander Smola](https://arxiv.org/search/stat?searchtype=author&query=Smola,+A)\n\nView a PDF of the paper titled AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data, by Nick Erickson and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2003.06505)\n\n> Abstract:We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99% of the participating data scientists after merely 4h of training on the raw data.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2003.06505](https://arxiv.org/abs/2003.06505) \\[stat.ML\\] |\n| (or [arXiv:2003.06505v1](https://arxiv.org/abs/2003.06505v1) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2003.06505](https://doi.org/10.48550/arXiv.2003.06505) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jonas Mueller \\[ [view email](https://arxiv.org/show-email/c27b830f/2003.06505)\\] **\\[v1\\]**\nFri, 13 Mar 2020 23:10:39 UTC (428 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data, by Nick Erickson and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2003.06505)\n- [TeX Source](https://arxiv.org/src/2003.06505)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2003.06505&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2003.06505&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2020-03](https://arxiv.org/list/stat.ML/2020-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2003.06505?context=cs) [cs.LG](https://arxiv.org/abs/2003.06505?context=cs.LG) [stat](https://arxiv.org/abs/2003.06505?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2003.06505)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2003.06505)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2003.06505)\n\n### [2 blog links](https://arxiv.org/tb/2003.06505)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2003.06505) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2003.06505"
    },
    {
      "title": "AutoGluon: Fast and Accurate ML in 3 Lines of Code",
      "text": "[Skip to content](https://auto.gluon.ai/auto.gluon.ai#furo-main-content)\n\n[Back to top](https://auto.gluon.ai/auto.gluon.ai)\n\n[View this page](https://auto.gluon.ai/_sources/tutorials/tabular/how-it-works.md.txt)\n\nToggle Light / Dark / Auto color theme\n\nToggle table of contents sidebar\n\n# How It Works [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#how-it-works)\n\nAutoML is usually associated with Hyperparameter Optimization (HPO) of one or multiple models. Or with the automatic selection of models based on the given problem. In fact, most AutoML frameworks do this.\n\nAutoGluon is different because it doesn\u2019t rely on HPO to achieve great performance [\\[1\\]](https://auto.gluon.ai/auto.gluon.ai#id7). It\u2019s based on three main principles: (1) training a variety of **different models**, (2) using **bagging** when training those models, and (3) **stack-ensembling** those models to combine their predictive power into a \u201csuper\u201d model. The following sections describe these concepts in more detail.\n\n## Bagging [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#bagging)\n\n[Bagging (Bootstrap Aggregation)](https://www.ibm.com/topics/bagging) is a technique used in Machine Learning to improve the stability and accuracy of algorithms. The key idea is that combining multiple models usually leads to better performance than any single model because it reduces [overfitting](https://aws.amazon.com/what-is/overfitting/) and adds robustness to the prediction.\n\nIn general, the _bootstrap_ portion of bagging involves taking many random sub-samples (with replacement, i.e. the same data point can appear more than once in a sample) from the training dataset. And then training different models on the _bootstrapped_ samples.\n\nHowever, AutoGluon performs bagging in a different way by combining it with **cross-validation**. In addition to the benefits of bagging, cross-validation allows us to train and validate multiple models using _all_ the training data. This also increases our confidence in the scores of the trained models. Here\u2019s how the technique works:\n\n1. **Partitioning:** The training data is partitioned into _K_ folds (or subsets of the dataset) [\\[2\\]](https://auto.gluon.ai/auto.gluon.ai#id8).\n\n2. **Model Training:** For each of the folds, a model is trained using all the data _except_ the fold. This means we train _K_ separate model instances with different portions of the data. This is known as a _bagged_ model.\n\n3. **Cross-validation:** Each model instance is evaluated against the hold-out fold that wasn\u2019t used during training. We then concatenate the predictions [\\[3\\]](https://auto.gluon.ai/auto.gluon.ai#id9) from the folds to create the **out-of-fold (OOF) predictions**. We calculate the final model cross-validation score by computing the evaluation metric using the OOF predictions and the target ground truth. Make sure to form a solid understanding of what out-of-fold (OOF) predictions are, as they are the most critical component to making stack ensembling work ( _see below_).\n\n4. **Aggregation:** At prediction time, bagging takes all these individual models and averages their predictions to generate a final answer (e.g. the class in the case of classification problems).\n\n\nThis same process is repeated for each of the models that AutoGluon uses. Thus, the number of models that AutoGluon trains during this process is _N x K_, where _N_ is the number of models and _K_ is the number of folds.\n\n## Stacked Ensembling [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#stacked-ensembling)\n\nIn the most general sense, ensembling is another technique used in Machine Learning to improve the accuracy of predictions by combining the strengths of multiple models. There are multiple ways to perform ensembling [\\[4\\]](https://auto.gluon.ai/auto.gluon.ai#id10) and this [guide](https://web.archive.org/web/20210727094233/http://mlwave.com/kaggle-ensembling-guide/) is a great introduction to many of them.\n\nAutoGluon, in particular, uses stack ensembling. At a high level, stack ensembling is a technique that leverages the predictions of models as extra features in the data. Here\u2019s how the technique works:\n\n1. **Layer(s) of Models:** Stacked ensembling is like a multi-layer cake. Each layer consists of several different bagged models ( _see above_) that use the predictions from the previous layer as inputs (features) **in addition to the original features from the training data** (akin to a skip connection). The first layer (also known as the _base_) uses only the original features from the training data.\n\n2. **Weighted Ensemble:** The last layer consists of a single \u201csuper\u201d model that combines the predictions from the second to last layer [\\[5\\]](https://auto.gluon.ai/auto.gluon.ai#id11). The job of this model, commonly known as the _meta-model_, is to learn how to _combine_ the outputs from all previous models to make a final prediction. Think of this model as a _leader_ who makes a final decision by _weighting_ everyone else\u2019s inputs. In fact, that is exactly what AutoGluon does: it uses a [Greedy Weighted Ensemble](https://www.cs.cornell.edu/~alexn/papers/shotgun.icml04.revised.rev2.pdf) algorithm to produce the final meta-model.\n\n3. **Residual Connections:** Note that the structure of stacked ensembling resembles that of a **Neural Network**. Therefore, advanced techniques (e.g. dropout, skip connections, etc.) used for Neural Networks could also be applied here as well.\n\n4. **How to Train:** During training time it is critical to avoid [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage), and therefore we use the **out-of-fold (OOF) predictions** of each bag model instead of predicting directly on the train data with the bagged model. By using out-of-fold predictions, we ensure that each instance in the training dataset has a corresponding prediction that was generated by a model that **did not train** on that instance. This setup mirrors how the final ensemble model will operate on new, unseen data.\n\n5. **How to Infer:** During inference time, we don\u2019t need to worry about data leakage, and we simply **average the predictions** of all models in a bag to generate its predictions.\n\n\nConsidering both bagging and stacked ensembling, the final number of models that AutoGluon trains is _M x N x K + 1_, where:\n\n- _M_ is the number of layers in the ensemble, including the base and excluding the last layer\n\n- _N_ is the number of models per layer\n\n- _K_ is the number of folds for bagging\n\n- _1_ is the final meta-model (weighted ensemble)\n\n\n## What Models To Use [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#what-models-to-use)\n\nOne key part of this whole process is deciding which models to train and ensemble. Although ensembling, in general, is a very powerful technique; choosing the right models can be the difference between mediocre and excellent performance.\n\nTo answer this question, we evaluated **1,310 models** on **200 distinct datasets** to compare the performance of different combinations of algorithms and hyperparameter configurations. The evaluation is available in this [repository](https://github.com/autogluon/tabrepo/tree/main).\n\nWith the results of this extensive evaluation, we chose a set of pre-defined configurations to use in AutoGluon by default [\\[6\\]](https://auto.gluon.ai/auto.gluon.ai#id12) based on the desired performance (e.g. \u201cbest quality\u201d, \u201cmedium quality\u201d, etc.). These presets even define the order in which models should be trained to maximize the use of training time.",
      "url": "https://auto.gluon.ai/stable/tutorials/tabular/how-it-works.html"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "qiaoyang.github.io",
      "text": "TabM: Advancing Tabular Deep Learning With Parameter-Efficient Ensembling\u201d (ICLR 2025) | qiaoyang.github.io\n# [qiaoyang.github.io](https://qiaoyang-blog.github.io/qiaoyang.github.io/)\n# TabM: Advancing Tabular Deep Learning With Parameter-Efficient Ensembling\u201d (ICLR 2025)\n:scroll:[arXiv](https://arxiv.org/abs/2410.24210):books:[Other tabular DL projects](https://github.com/yandex-research/rtdl)\nThis is the official repository of the paper \u201cTabM: Advancing Tabular Deep Learning With\nParameter-Efficient Ensembling\u201d.\nIt consists of two parts:\n* [**Python package**](#python-package)described in this document.\n* [**Paper-related content**](https://qiaoyang-blog.github.io/qiaoyang.github.io/paper/)(code, metrics, hyperparameters, etc.) described in`paper/README.md`.\nTabM on**Kaggle**(as of June 2025)- TabM was used in [the winning solution](https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants/discussion/549801) in the competition by UM.\n- TabM was used in [the winning solution](https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/566550), as well as in the top-3, top-4, top-5 and many other solutions in the competition by CIBMTR. Later, it turned out that it was possible to achieve the [25-th place](https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/567863) out of 3300+ with only TabM, without ensembling it with other models.TabM on**TabReD**(a challenging benchmark)[TabReD](https://arxiv.org/abs/2406.19380) is a benchmark based on \\*\\*real-world industrial datasets\\*\\* with \\*\\*time-related distribution drifts\\*\\* and \\*\\*hundreds of features\\*\\*, which makes it more challenging than traditional benchmarks. The figure below shows that TabM achieves higher performance on TabReD (plus one more real-world dataset) compared to prior tabular DL methods.\n&lt;&lt;img src=\"images/tabred-and-microsoft.png\" width=35% display=block margin=auto&gt;&gt;\n\\*One dot represents a performance score on one dataset. For a given model, a diamond represents the mean value across the datasets.\\*Training and inference efficiencyTabM is a simple and reasonably efficient model, which makes it suitable for \\*\\*real-world applications\\*\\*, including large datasets. The biggest dataset used in the paper contains \\*\\*13M objects\\*\\*, and we are aware of a successful training run on \\*\\*100M+ objects\\*\\*, though training takes more time in such cases.\nThe figure below shows that TabM is relatively slower than MLPs and GBDT, but faster than prior tabular DL methods. Note that (1) the inference throughput was measured on a single CPU thread and \\*without any optimizations\\*, in particular without the TabM-specific acceleration technique described later in this document; (2) the left plot uses the \\*logarithmic\\* scale.![](images/efficiency.png)\\*One dot represents a measurement on one dataset. For a given model, a diamond represents the mean value across the datasets. In the left plot,\\* $\\\\mathrm{TabM\\_{mini}^{\\\\dagger\\*}}$ \\*denotes\\* $\\\\mathrm{TabM\\_{mini}^{\\\\dagger}}$ \\*trained with mixed precision and `torch.compile`.\\*# TL;DR\n&lt;img src=\u201dimages/tabm.png\u201d width=65% display=block margin=auto&gt;\n**TabM**(**Tab**ular DL model that makes**M**ultiple predictions) is a simple and powerful tabular DL architecture that efficiently imitates an ensemble of MLPs. The two main differences of TabM compared to a regular ensemble of MLPs:\n* **Parallel training**of the MLPs. This allows monitoring the performance of the ensemble during the training and stopping the training when it is optimal for the ensemble, not for individual MLPs.\n* **Weight sharing**between the MLPs. In fact, the whole TabM fits in just*one*MLP-like model. Not only this significantly improves the runtime and memory efficiency, but also turns out to be an effective regularization leading to better task performance.# Reproducing experiments and browsing results\n> > [!IMPORTANT]To use TabM in practice and for future work, use the `> tabm\n`> package described below.\n> The[paper-related content](https://qiaoyang-blog.github.io/qiaoyang.github.io/paper/)(code, metrics, hyperparameters, etc.) is located in the`paper/`directory and is described in`paper/README.md`.\n# Python package\n`tabm`is a PyTorch-based Python package providing the TabM model, as well as layers and tools for building custom TabM-like architectures (i.e. efficient ensembles of MLP-like models).\n* [**Installation**](#installation)\n* [**Basic usage**](#basic-usage)\n* [Creating TabM](#creating-tabm)\n* [Creating TabM with feature embeddings](#creating-tabm-with-feature-embeddings)\n* [Using TabM with custom inputs and input modules](#using-tabm-with-custom-inputs-and-input-modules)\n* [Training](#training)\n* [Inference](#inference)\n* [**Examples**](#examples)\n* [Advanced usage](#advanced-usage)\n* [Intuition](#intuition)\n* [`EnsembleView`](#ensembleview)\n* [MLP ensembles](#mlp-ensembles)\n* [Important implementation details](#important-implementation-details)\n* [Example: Simple ensemble without weight sharing](#example-simple-ensemble-without-weight-sharing)\n* [Example: MiniEnsemble](#example-miniensemble)\n* [Example: BatchEnsemble](#example-batchensemble)\n* [Example: a custom architecture](#example-a-custom-architecture)\n* [Turning an existing model to an efficient ensemble](#turning-an-existing-model-to-an-efficient-ensemble)\n* [Hyperparameters](#hyperparameters)\n* [Default model](#default-model)\n* [Default optimizer](#default-optimizer)\n* [`arch\\_type`](#arch_type)\n* [`k`](#k)\n* [`num\\_embeddings`](#num_embeddings)\n* [Initialization](#initialization)\n* [Hyperparameter tuning](#hyperparameter-tuning)\n* [Practical notes](#practical-notes)\n* [Inference efficiency](#inference-efficiency)\n* [API](#api)# **Installation**\n```\n`pip install tabm`\n```\n# **Basic usage**\nThis section shows how to create a model in typical use cases, and gives high-level comments on\ntraining and inference.\n## Creating TabM\nThe below example showcases the basic version of TabM without feature embeddings.\nFor better performance,`num\\_embeddings`should usually be passed as explained in the next section.\n> > [!NOTE]\n`> TabM.make(...)\n`> used below adds default hyperparameters based on the provided arguments.\n> ```\n`importtorchfromtabmimportTabM# &gt;&gt;&gt; Common setup for all subsequent sections.d\\_out=1# For example, one regression task.batch\\_size=256# The dataset has 24 numerical (continuous) features.n\\_num\\_features=24# The dataset has 2 categorical features.\n# The first categorical feature has 3 unique categories.\n# The second categorical feature has 7 unique categories.cat\\_cardinalities=[3,7]# &lt;&lt;&lt;model=TabM.make(n\\_num\\_features=n\\_num\\_features,cat\\_cardinalities=cat\\_cardinalities,# One-hot encoding will be used.d\\_out=d\\_out,)x\\_num=torch.randn(batch\\_size,n\\_num\\_features)x\\_cat=torch.column\\_stack([# The i-th categorical features must take values in range(0, cat\\_cardinalities[i]).torch.randint(0,c,(batch\\_size,))forcincat\\_cardinalities])y\\_pred=model(x\\_num,x\\_cat)# TabM represents an ensemble of k models, hence k predictions per object.asserty\\_pred.shape==(batch\\_size,model.k,d\\_out)`\n```\n## Creating TabM with feature embeddings\nOn typical tabular tasks, the best performance is usually achieved by passing feature embedding\nmodules as`num\\_embeddings`(in the paper, TabM with embeddings is denoted as $\\\\mathrm{TabM^\\\\dagger}$).`TabM`supports several feature embedding modules from the[`rtdl\\_num\\_embeddings`](https://github.com/yandex-research/rtdl-num-embeddings/blob/main/package/README.md)package. The below example showcases the simplest embedding module`LinearReLUEmbeddings`.\n> > [!TIP]The best performance is usually achieved with more advanced embeddings, such as\n`> PiecewiseLinearEmbeddings\n`> and `> PeriodicEmbeddings\n`> . Their usage is covered in the end-to-end usage [> example\n](#examples)> .\n> ```\n`fromrtdl\\_num\\_embeddingsimportLinearReLUEmbeddingsmodel=TabM.make(n\\_num\\_featu...",
      "url": "https://qiaoyang-blog.github.io/qiaoyang.github.io"
    },
    {
      "title": "TabM : Advancing Tabular Deep Learning\n with Parameter-Efficient Ensembling",
      "text": "TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling\n# TabM: Advancing Tabular Deep Learning\nwith Parameter-Efficient Ensembling\nYury Gorishniy\nYandex\n&amp;Akim Kotelnikov\nHSE University, Yandex\n&amp;Artem Babenko\nYandexThe corresponding author:firstnamelastname@gmail.com\n###### Abstract\nDeep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs: namely, parameter-efficient ensembling\n\u2014a paradigm for implementing an ensemble of models as one model producing multiple predictions.\nWe start by developingTabM\u2014 a simple model based on MLP and our variations of BatchEnsemble (an existing technique).\nThen, we perform a large-scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nGenerally, we show that MLPs, includingTabM, form a line of stronger and more practical models compared to attention- and retrieval-based architectures.\nIn particular, we find thatTabMdemonstrates the best performance among tabular DL models.\nLastly, we conduct an empirical analysis on the ensemble-like nature ofTabM.\nFor example, we observe that the multiple predictions ofTabMare weak individually, but powerful collectively.\nOverall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiencytrade-offwithTabM\u2014 a simple and powerful baseline for researchers and practitioners.\nThe code is available at:[https://github.com/yandex-research/tabm](https://github.com/yandex-research/tabm).\n## 1Introduction\nSupervised learning on tabular data is a ubiquitous machine learning (ML) scenario in a wide range of industrial applications.\nAmong classic non-deep-learning methods, the state-of-the-art solution for such tasks is gradient-boosted decision trees (GBDT)> (Prokhorenkova et\u00a0al., [> 2018\n](https://arxiv.org/html/2410.24210v1#bib.bib35)> ; Chen &amp; Guestrin, [> 2016\n](https://arxiv.org/html/2410.24210v1#bib.bib10)> ; Ke et\u00a0al., [> 2017\n](https://arxiv.org/html/2410.24210v1#bib.bib23)> )\n.\nDeep learning (DL) models for tabular data, in turn, are reportedly improving, and the most recent works claim to perform on par or even outperform GBDT on academic benchmarks> (Hollmann et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.24210v1#bib.bib18)> ; Chen et\u00a0al., [> 2023b\n](https://arxiv.org/html/2410.24210v1#bib.bib9)> ; [> a\n](https://arxiv.org/html/2410.24210v1#bib.bib8)> ; Gorishniy et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.24210v1#bib.bib15)> )\n.\nHowever, from the practical perspective, it is unclear if tabular DL offers any obvious go-to baselines beyond simple architectures in the spirit of a multilayer perceptron (MLP).First, the scale and consistency of performance improvements of new methods w.r.t. simple MLP-like baselines are not always explicitly analyzed in the literature.\nThus, one has to infer those statistics from numerous per-dataset performance scores, which makes it hard to reason about the progress.\nAt the same time, due to the extreme diversity of tabular datasets, consistency is an especially valuable and hard-to-achieve property for a hypothetical go-to baseline.Second, efficiency-related properties, such as training time, and especially inference throughput, sometimes receive less attention.\nWhile methods are usually equally affordable on small-to-medium datasets (e.g.&lt;&lt;&lt;100K objects), their applicability to larger datasets remains uncertain.Third, some recent work generally suggests that the progress on academic benchmarks may not transfer that well to real-world tasks> (Rubachev et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.24210v1#bib.bib38)> )\n.\nWith all the above in mind, in this work, we thoroughly evaluate existing tabular DL methods and find that non-MLP models do not yet offer a convincing replacement for MLPs.\nAt the same time, we identify a previously overlooked path towards more powerful, reliable and reasonably efficient tabular DL models.\nIn a nutshell, we find that the parameter-efficient approach to deep ensembling, where most weights are shared between ensemble members, allows making simple and strong tabular models out of plain MLPs.\nFor example, MLP coupled with BatchEnsemble> (Wen et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.24210v1#bib.bib46)> )\n\u2014a long-existing method \u2014right away outperforms popular attention-based models, such as FT-Transformer> (Gorishniy et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib13)> )\n, while being simpler and more efficient.\nThis result alone suggests that the parameter-efficient ensembling is a low-hanging fruit for tabular DL.\nOur work builds on the above observations, and offersTabM\u2014 a new powerful and practical model for researchers and practitioners.\nDrawing an informal parallel with GBDT (an ensemble of decision trees),TabMcan also be viewed as a simple base model (MLP) combined with an ensembling-like technique, providing high performance and simple implementation at the same time.\nMain contributions.We summarize our main contributions as follows:\n1. 1.\nWe presentTabM\u2014 a simple DL architecture for supervised learning on tabular data.TabMis based on MLP and parameter-efficient ensembling techniques closely related to BatchEnsemble> (Wen et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.24210v1#bib.bib46)> )\n.\nIn particular,TabMproducesMultiple predictions per object.TabMeasily competes with GBDT and outperforms prior tabular DL models, while being more efficient than attention- and retrieval-based DL architectures.\n2. 2.\nWe provide a fresh perspective on tabular DL models in a large-scale evaluation along four dimensions: performance ranks, performance score distributions, training time and inference throughput.\nOne of our findings is that MLPs, includingTabM, hit an appealing performance-efficiency tradeoff, which is not the case for attention- and retrieval-based models.\n3. 3.\nEmpirically, we show that the multiple predictions ofTabMare weak and overfitted individually, while their average is strong and generalizable.\nThe training gradients ofTabM, in turn, can be viewed as an \u201censemble\u201d of diverse gradients coming from the multiple predictions.\n## 2Related work\nDecision-tree-based models.Gradient-boosted decision trees (GBDT)> (Chen &amp; Guestrin, [> 2016\n](https://arxiv.org/html/2410.24210v1#bib.bib10)> ; Ke et\u00a0al., [> 2017\n](https://arxiv.org/html/2410.24210v1#bib.bib23)> ; Prokhorenkova et\u00a0al., [> 2018\n](https://arxiv.org/html/2410.24210v1#bib.bib35)> )\nis a strong and efficient baseline for tabular tasks.\nGBDT is a classic machine learning model, specifically, an ensemble of decision trees.\nOur modelTabMis a deep learning model, specifically, a parameter-efficient ensemble of MLPs.\nTabular deep learning architectures.A large number of deep learning architectures for tabular data has been proposed over the recent years.\nThat includes attention-based architectures> (Song et\u00a0al., [> 2019\n](https://arxiv.org/html/2410.24210v1#bib.bib40)> ; Gorishniy et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib13)> ; Somepalli et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib39)> ; Kossen et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib26)> ; Yan et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.24210v1#bib.bib47)> )\n, retrieval-augmented architectures> (Somepalli et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib39)> ; Kossen et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib26)> ; Gorishniy et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.24210v1#bib.bib15)> ; Ye et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.24210v1#bib.bib48)> )\n, MLP-like models> (Gorishniy et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.24210v1#bib.bib13)> ; Klambauer e...",
      "url": "https://arxiv.org/html/2410.24210v1"
    },
    {
      "title": "",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/pdf/2410.24210"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - yandex-research/tabm: (ICLR 2025) TabM: Advancing Tabular Deep Learning With Parameter-Efficient Ensembling\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/yandex-research/tabm)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/yandex-research/tabm)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=yandex-research/tabm)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[yandex-research](https://github.com/yandex-research)/**[tabm](https://github.com/yandex-research/tabm)**Public\n* [Notifications](https://github.com/login?return_to=/yandex-research/tabm)You must be signed in to change notification settings\n* [Fork77](https://github.com/login?return_to=/yandex-research/tabm)\n* [Star919](https://github.com/login?return_to=/yandex-research/tabm)\n(ICLR 2025) TabM: Advancing Tabular Deep Learning With Parameter-Efficient Ensembling\n[arxiv.org/abs/2410.24210](https://arxiv.org/abs/2410.24210)\n### License\n[Apache-2.0 license](https://github.com/yandex-research/tabm/blob/main/LICENSE)\n[919stars](https://github.com/yandex-research/tabm/stargazers)[77forks](https://github.com/yandex-research/tabm/forks)[Branches](https://github.com/yandex-research/tabm/branches)[Tags](https://github.com/yandex-research/tabm/tags)[Activity](https://github.com/yandex-research/tabm/activity)\n[Star](https://github.com/login?return_to=/yandex-research/tabm)\n[Notifications](https://github.com/login?return_to=/yandex-research/tabm)You must be signed in to change notification settings\n# yandex-research/tabm\nmain\n[Branches](https://github.com/yandex-research/tabm/branches)[Tags](https://github.com/yandex-research/tabm/tags)\n[](https://github.com/yandex-research/tabm/branches)[](https://github.com/yandex-research/tabm/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[50 Commits](https://github.com/yandex-research/tabm/commits/main/)\n[](https://github.com/yandex-research/tabm/commits/main/)\n|\n[images](https://github.com/yandex-research/tabm/tree/main/images)\n|\n[images](https://github.com/yandex-research/tabm/tree/main/images)\n|\n|\n|\n[paper](https://github.com/yandex-research/tabm/tree/main/paper)\n|\n[paper](https://github.com/yandex-research/tabm/tree/main/paper)\n|\n|\n|\n[.gitattributes](https://github.com/yandex-research/tabm/blob/main/.gitattributes)\n|\n[.gitattributes](https://github.com/yandex-research/tabm/blob/main/.gitattributes)\n|\n|\n|\n[.gitignore](https://github.com/yandex-research/tabm/blob/main/.gitignore)\n|\n[.gitignore](https://github.com/yandex-research/tabm/blob/main/.gitignore)\n|\n|\n|\n[.python-version](https://github.com/yandex-research/tabm/blob/main/.python-version)\n|\n[.python-version](https://github.com/yandex-research/tabm/blob/main/.python-version)\n|\n|\n|\n[LICENSE](https://github.com/yandex-research/tabm/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/yandex-research/tabm/blob/main/LICENSE)\n|\n|\n|\n[Makefile](https://github.com/yandex-research/tabm/blob/main/Makefile)\n|\n[Makefile](https://github.com/yandex-research/tabm/blob/main/Makefile)\n|\n|\n|\n[README.md](https://github.com/yandex-research/tabm/blob/main/README.md)\n|\n[README.md](https://github.com/yandex-research/tabm/blob/main/README.md)\n|\n|\n|\n[example.ipynb](https://github.com/yandex-research/tabm/blob/main/example.ipynb)\n|\n[example.ipynb](https://github.com/yandex-research/tabm/blob/main/example.ipynb)\n|\n|\n|\n[pyproject.toml](https://github.com/yandex-research/tabm/blob/main/pyproject.toml)\n|\n[pyproject.toml](https://github.com/yandex-research/tabm/blob/main/pyproject.toml)\n|\n|\n|\n[tabm.py](https://github.com/yandex-research/tabm/blob/main/tabm.py)\n|\n[tabm.py](https://github.com/yandex-research/tabm/blob/main/tabm.py)\n|\n|\n|\n[tabm\\_reference.py](https://github.com/yandex-research/tabm/blob/main/tabm_reference.py)\n|\n[tabm\\_reference.py](https://github.com/yandex-research/tabm/blob/main/tabm_reference.py)\n|\n|\n|\n[test\\_code\\_blocks.py](https://github.com/yandex-research/tabm/blob/main/test_code_blocks.py)\n|\n[test\\_code\\_blocks.py](https://github.com/yandex-research/tabm/blob/main/test_code_blocks.py)\n|\n|\n|\n[uv.lock](https://github.com/yandex-research/tabm/blob/main/uv.lock)\n|\n[uv.lock](https://github.com/yandex-research/tabm/blob/main/uv.lock)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# TabM: Advancing Tabular Deep Learning With Parameter-Efficient Ensembling\" (ICLR 2025)\n[](#tabm-advancing-tabular-deep-learning-with-parameter-efficient-ensembling-iclr-2025)\n\ud83d\udcdc[arXiv](https://arxiv.org/abs/2410.24210)\ud83d\udcda[Other tabular DL projects](https://github.com/yandex-research/rtdl)\nThis is the official repository of the paper \"TabM: Advancing Tabular Deep Learning With\nParameter-Efficient Ensembling\".\nIt consists of two parts:\n* [**Python package**](#python-package)described in this document.\n* [**Paper-related content**](https://github.com/yandex-research/tabm/blob/main/paper/README.md)(code, metrics, hyperparameters, etc.) described in`paper/README.md`.\nTabM on**Kaggle**(as of June 2025)\n* TabM was used in[the winning solution](https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants/discussion/549801)in the competition by UM.\n* TabM was used in[the winning solution](https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/566550), as well as in the top-3, top-4, top-5 and many other solutions in the competition by CIBMTR. Later, it turned out that it was possible to achieve the[25-th place](https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/567863)out of 3300+ with only TabM, without ensembling it with other models.TabM on**TabReD**(a challenging benchmark)\n[TabReD](https://arxiv.org/abs/2406.19380)is a benchmark based on**real-world industrial datasets**with**time-related distribution drifts**and**hundreds of features**, which makes it more challenging than traditional benchmarks. The figure below shows that TabM achieves higher performance on TabReD (plus one more real-world dataset) compared to prior tabular DL methods.\n[![](https://github.com/yandex-research/tabm/raw/main/images/tabred-and-microsoft.png)](https://github.com/yandex-research/tabm/blob/main/images/tabred-and-microsoft.png)\n*One dot represents a performance score on one dataset. For a given model, a diamond represents the mean value across the datasets.*\nTraining and inference efficiency\nTabM is a simple and reasonably efficient model, which makes it suitable for**real-world applications**, including large datasets. The biggest dataset used in the paper contains**13M objects**, and we are aware of a successful training run on**100M+ objects**, though training takes more time in such cases.\nThe figure below shows that TabM is relatively slower than MLPs and GBDT, but faster than prior tabular DL meth...",
      "url": "https://github.com/yandex-research/tabm"
    }
  ]
}