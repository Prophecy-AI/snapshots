{
  "query": "What are the most effective feature engineering techniques for tabular data competitions on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Feature engineering remains the single most decisive step in winning Kaggle tabular competitions.  Analyses of past contests consistently show that the \u201cpre\u2011processing and feature extraction phases were the most important aspects of creating the best modelling approach\u201d and that top\u2011scoring solutions devote the bulk of their effort to turning raw columns into richer, model\u2011friendly signals\u202f([arxiv.org](https://arxiv.org/pdf/2202.02898)).  The 2023 Kaggle AI Report likewise highlights feature engineering as a core pillar of the winning pipeline, noting that even with powerful gradient\u2011boosted trees, clever engineered features often provide the final lift\u202f([vinlam.com](https://vinlam.com/posts/tips-for-tabular-ml)).\n\nThe techniques that repeatedly surface across Grandmaster notebooks and competition post\u2011mortems include:  \n\n* **Categorical encoding** \u2013 target/mean encoding, frequency encoding, and ordinal mapping to capture label\u2011wise information while keeping dimensionality low\u202f([neptune.ai](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions));  \n* **Date\u2011time decomposition** \u2013 extracting year, month, day\u2011of\u2011week, elapsed time, and cyclic sin/cos features for any timestamp columns\u202f([medium.com](https://medium.com/analytics-vidhya/feature-engineering-for-kaggle-competition-5616196bf274));  \n* **Interaction & cross features** \u2013 multiplying or concatenating related columns (e.g., price\u202f\u00d7\u202fquantity, or \u201ccategory\u202f+\u202fregion\u201d hashes) to let tree models discover non\u2011linear relationships\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data));  \n* **Group\u2011by aggregations** \u2013 computing statistics (mean, median, count, rank) within logical groups (e.g., user\u2011level spend, product\u2011level return rate) to inject hierarchical information\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas));  \n* **Missing\u2011value handling & outlier treatment** \u2013 explicit flags for missingness, imputation with medians or model\u2011based estimates, and clipping extreme values before log\u2011transforming numeric columns\u202f([neptune.ai](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions));  \n* **Scaling & transformations** \u2013 log or Box\u2011Cox transforms for skewed distributions, standardisation for linear models, and power\u2011law features for count data\u202f([medium.com](https://medium.com/analytics-vidhya/feature-engineering-for-kaggle-competition-5616196bf274)).  \n\nBeyond manual tricks, recent work shows that automated feature generation can match expert\u2011level performance.  OpenFE, an open\u2011source system, systematically creates high\u2011order interactions, statistical aggregates, and domain\u2011agnostic transformations, often surfacing useful features that human engineers might miss\u202f([arxiv.org](https://arxiv.org/abs/2211.12507)).  When combined with GPU\u2011accelerated pandas (cuDF) for rapid iteration on millions of rows, these pipelines enable Grandmasters to prototype, test, and refine hundreds of engineered columns within a single notebook\u202f([developer.nvidia.com](https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas)).  \n\nIn practice, the most effective Kaggle tabular strategy is to start with solid categorical encodings and date\u2011time splits, enrich the data with group\u2011by aggregates and interaction features, clean and transform numeric fields, and then iterate quickly using GPU\u2011enabled tools or automated generators like OpenFE\u2014all while validating with robust cross\u2011validation to ensure the engineered signals truly improve generalisation.",
      "url": ""
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas",
      "text": "Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/04/NVIDIA-feature-engineering-kaggle-blog-1024x576.jpg)\nApr 17, 2025\nBy[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [T](https://twitter.com/intent/tweet?text=Grandmaster+Pro+Tip:+Winning+First+Place+in+Kaggle+Competition+with+Feature+Engineering+Using+cuDF+pandas+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/&amp;title=Grandmaster+Pro+Tip:+Winning+First+Place+in+Kaggle+Competition+with+Feature+Engineering+Using+cuDF+pandas+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/>)\nAI-Generated Summary\nLike\nDislike\n* Using NVIDIA cuDF-pandas to accelerate pandas operations on GPUs allowed for the rapid generation and testing of over 10,000 engineered features for a Kaggle competition, significantly boosting the accuracy of an XGBoost model.\n* The most effective feature engineering techniques included groupby aggregations, such as computing statistics (e.g., mean, std, count) and quantiles, as well as creating new columns from NaNs and binning numerical columns.\n* Techniques like extracting digits from float32 values and combining categorical columns also proved useful, and leveraging the original dataset that the synthetic data was created from provided additional predictive power.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nFeature engineering remains one of the most effective ways to improve model accuracy when working with tabular data. Unlike domains such as NLP and computer vision, where neural networks can extract rich patterns from raw inputs, the best-performing tabular models\u2014particularly gradient-boosted decision trees\u2014still gain a significant advantage from well-crafted features. However, the sheer potential number of useful features means that exploring them thoroughly is often computationally prohibitive. Trying to generate and validate hundreds or thousands of feature ideas using standard pandas on a CPU is simply too slow to be practical.\nThis is where GPU acceleration changes the game. Using NVIDIA cuDF-pandas, which accelerates pandas operations on GPUs with zero code changes, allowed me to rapidly generate and test over 10,000 engineered features for Kaggle\u2019s February playground competition. This accelerated discovery process was the key differentiator. In a drastically reduced timeframe &#8211; days instead of potential months &#8211;\u00a0the best 500 discovered features significantly boosted the accuracy of my XGBoost model, securing 1st place in the competition predicting backpack prices. Below, I share the core feature engineering techniques, accelerated by cuDF-pandas, that led to this result.\n## Groupby(COL1)[COL2].agg(STAT)[**](#groupbycol1col2aggstat)\nThe most powerful feature engineering technique is groupby aggregations. Namely, we execute the code`groupby(COL1)[COL2].agg(STAT)`. This is where we group by`COL1`column and aggregate (i.e. compute) a statistic`STAT`over another column`COL2`. We use the speed of NVIDIA cuDF-Pandas to explore thousands of`COL1`,`COL2`,`STAT`combinations. We try statistics (`STAT`) like &#8220;mean&#8221;, &#8220;std&#8221;, &#8220;count&#8221;, &#8220;min&#8221;, &#8220;max&#8221;, &#8220;nunique&#8221;, &#8220;skew&#8221; etc etc. We choose`COL1`and`COL2`from our tabular data\u2019s existing columns. When`COL2`is the target column, then we use nested cross-validation to avoid leakage in our validation computation. When`COL2`is the target, this operation is called Target Encoding.\n## Groupby(COL1)[&#8216;Price&#8217;].agg(HISTOGRAM BINS)[**](#groupbycol1&#8216;price&#8217;agghistogram_bins)\nWhen we`groupby(COL1)[COL2]`we have a distribution (set) of numbers for each group. Instead of computing a single statistic (and making one new column), we can compute any collection of numbers that describe this distribution of numbers and make many new columns together.\nBelow we display a histogram for the group`Weight Capacity = 21.067673`. We can count the number of elements in each (equally spaced) bucket and create a new engineered feature for each bucket count to return to the groupby operation! Below we display seven buckets, but we can treat the number of buckets as a hyperparameter.\n```\nresult = X\\_train2.groupby(&quot;&quot;WC&quot;&quot;)&#x5B;&#x5B;&quot;&quot;Price&quot;&quot;].apply(make\\_histogram)\nX\\_valid2 = X\\_valid2.merge(result, on=&quot;&quot;WC&quot;&quot;, how=&quot;&quot;left&quot;&quot;)\n```\n*Figure 1. Histogram of price values when weight capacity equals 21.067673*\n## Groupby(COL1)[&#8216;Price&#8217;].agg(QUANTILES)[**](#groupbycol1&#8216;price&#8217;aggquantiles)\nWe can groupby and compute the quantiles for`QUANTILES = [5,10,40,45,55,60,90,95]`and return the eight values to create eight new columns.\n```\nfor k in QUANTILES:\nresult = X\\_train2.groupby(&#039;&#039;Weight Capacity (kg)&#039;&#039;).\\\\\nagg({&#039;&#039;Price&#039;&#039;: lambda x: x.quantile(k/100)})\n```\n## All NANs as Single Base-2 Column[**](#all_nans_as_single_base-2_column)\nWe can create a new column from all the NANs over multiple columns. This is a powerful column which we can subsequently use for groupby aggregations or combinations with other columns.\n```\ntrain&#x5B;&#x5B;&quot;&quot;NaNs&quot;&quot;] = np.float32(0)\nfor i,c in enumerate(CATS):\ntrain&#x5B;&#x5B;&quot;&quot;NaNs&quot;&quot;] += train&#x5B;&#x5B;c].isna()\\*2\\*\\*i\n```\n## Put Numerical Column into Bins[**](#put_numerical_column_into_bins)\nThe most powerful (predictive) column in this competition is Weight Capacity. We can create more powerful columns based on this column by binning this column with rounding.\n```\nfor k in range(7,10):\nn = f&quot;round{k}&quot;\ntrain&#x5B;n] = train&#x5B;&quot;Weight Capacity (kg)&quot;].round(k)\n```\n## Extract Float32 as Digits[**](#extract_float32_as_digits)\nThe most powerful (predictive) column in this competition is Weight Capacity. We can create more powerful columns based on this column by extracting digits. This technique seems weird, but it is often used to extract info from a product ID where individual digits within a product ID convey info about a product such as brand, color, etc.\n```\nfor k in range(1,10):\ntrain&#x5B;&#x5B;f&#039;&#039;digit{k}&#039;&#039;] = ((train&#x5B;&#x5B;&#039;&#039;Weight Capacity (kg)&#039;&#039;] \\* 10\\*\\*k) % 10).fillna(-1).astype(&quot;&quot;int8&quot;&quot;)\n```\n## Combination of Categorical Columns[**](#combination_of_...",
      "url": "https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Feature Engineering for Kaggle Competition - Analytics Vidhya - Medium",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5616196bf274&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ffeature-engineering-for-kaggle-competition-5616196bf274&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Ffeature-engineering-for-kaggle-competition-5616196bf274&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**Analytics Vidhya**](https://medium.com/analytics-vidhya?source=post_page---publication_nav-7219b4dc6c4c-5616196bf274---------------------------------------)\n\n\u00b7\n\n[![Analytics Vidhya](https://miro.medium.com/v2/resize:fill:76:76/1*Qw8AOQSnnlz7SLiwAda2jw.png)](https://medium.com/analytics-vidhya?source=post_page---post_publication_sidebar-7219b4dc6c4c-5616196bf274---------------------------------------)\n\nAnalytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem [https://www.analyticsvidhya.com](https://www.analyticsvidhya.com)\n\n# Feature Engineering for Kaggle Competition\n\n[![Guang X](https://miro.medium.com/v2/resize:fill:64:64/1*-gtQJ08fht22E5wx7JCQNA.png)](https://medium.com/@guangx?source=post_page---byline--5616196bf274---------------------------------------)\n\n[Guang X](https://medium.com/@guangx?source=post_page---byline--5616196bf274---------------------------------------)\n\nFollow\n\n8 min read\n\n\u00b7\n\nJan 1, 2020\n\n--\n\nListen\n\nShare\n\n# 0\\. Introduction\n\nAccording to Wikipedia:\n\n_Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive._\n\nFeature engineering is an important step for most of Kaggle competitions. This article will discuss feature commonly used feature engineering techniques for four different feature types: numerical, categorical, temporal and spatial data. These techniques have help me achieve Kaggle Competition Expert title.\n\n# 1\\. Feature types\n\nBefore we start feature engineering, we should understand different feature types first. In common Kaggle competition or data science projects, we would probably deal with the following types of features:\n\n\\- Numeric (Interval or Ratio)\n\n\\- Categorical (Ordinal or Nominal)\n\n\\- Temporal (Date and Time)\n\n\\- Spatial (Coordinates and location )\n\nNumerical features measure the magnitude of a quantity, such as bank balance, temperature, age, height, weight, populations \u2026\n\nThere are two different numeric features: Interval and Ratio. If a numeric value has a meaningful (non-arbitrary) zero point, then it is Ratio value, otherwise it is interval. For example, the zero point for Celsius Temperatures is arbitrary and not meaningful, thus it is a interval value. We can not say 20\u00b0C is twice of 10 \u00b0C as they are not ratio. However, bank balance with a meaningful zero point, is a ratio value. This means we can say $200 is the double of $100.\n\n**Categorical** values are also called discrete variables. They describe the groups or categories of a subject, such as the sex, product type, name of a city. All these are also called **nominal** variables. In contrast, another kind of categorical variable is called **ordinal** variable. Ordinal variables are ranked categorical variable, such as preference score, customer rating.\n\nDifferent types of variables require different feature engineering techniques, which will be introduced in the following.\n\nMore readings: [https://en.m.wikipedia.org/wiki/Statistical\\_data\\_type](https://en.m.wikipedia.org/wiki/Statistical_data_type)\n\n# 2\\. Numeric Features\n\nFeature engineering approaches to used depends on the machine learning models we are using. And for most of the Tree-based models(such as regression tree, random forest, xgboost, lightGBM), we do not need to do any feature engineering. Because tree-based models make predictions based on the comparison of different values, rather then the magnitude of numeric values.\n\nNon-tree based models, such as linear regression, Support vector machine, and neural network models, make predictions based on the magnitude of numerical values. That means these models are sensitive to the scale of input values.\n\nTo solve this issue, scalers can be used to scale numeric variables into a standardised range, such as **MinMaxScaler / StandardScaler** in Python scikit-learn package.\n\nBesides, outliers in numerical can cause problems to non-tree based models. A straightforward solution to outliers is to simply remove them from the dataset. Other solutions are **Winsorization** and **Ranking**. Winsorization clip the numerical values using a lower and upper bound. Ranking methods replace numeric values with their relative ranking in the dataset.\n\nMoreover, Numeric features with skewed distribution could also bring problem to non-tree based models. For examples, some numerical variables are log-normally distributed, such as monetary values. In these cases, we can apply log transformation to make a normally distributed variable. Depending on the variable distribution, other transformation techniques such as raising to the power of 0.5 can be used.\n\nBesides, many feature generation methods can be used with numeric variables, mostly based on **prior knowledge / data** analysis. For example, with a dataset describes prices of goods in a supermarket, we can extract fractional part from price values (such as .99 from 4.99), which probably affect people\u2019s perception of prices.\n\nMore reading: [https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n\n# 3\\. Categorical Features\n\nCategorical features are one of the most important features in data science projects. Because categorical variables are not numerical and cannot be processed by most of machine learning models, we have to encode them and represent them in a numerical way.\n\nThere are many encoding methods for categorical features. The most well known one is one-hot encoding. It create N new columns for N categories of a particular column. And the one or zero values in these newly created columns represents whether the category exist or not in that row. However, in practice, one-hot encoding does not work very well because it can cause collinearity problem. Moreover, when cardinality of a categorical column is very high, one hot encoding will generate a very sparse dataset, which is not good for machine learning models.\n\ncollinearity problem: [https://www.algosome.com/articles/dummy-variable-trap-regression.html](https://www.algosome.com/articles/dummy-variable-trap-regression.html)\n\nOne alternative approach to categorical encoding is **numerical encoding**, which simply numbers each of the categories in a column with positive integers. For example, we can encoding three categories A/B/C into 1/2/3. It does not increase the data size and works very well with tree-based models. And it is the recommended encoding method for lightGBM model.\n\n[https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html)\n\nHowever, numeric encoding does not work well with non-tree based models. One of the soluti...",
      "url": "https://medium.com/analytics-vidhya/feature-engineering-for-kaggle-competition-5616196bf274"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2211.12507] OpenFE: Automated Feature Generation with Expert-level Performance\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2211.12507\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2211.12507**(cs)\n[Submitted on 22 Nov 2022 ([v1](https://arxiv.org/abs/2211.12507v1)), last revised 5 Jun 2023 (this version, v3)]\n# Title:OpenFE: Automated Feature Generation with Expert-level Performance\nAuthors:[Tianping Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+T),[Zheyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z),[Zhiyuan Fan](https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+Z),[Haoyan Luo](https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+H),[Fengyuan Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+F),[Qian Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Q),[Wei Cao](https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+W),[Jian Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J)\nView a PDF of the paper titled OpenFE: Automated Feature Generation with Expert-level Performance, by Tianping Zhang and 7 other authors\n[View PDF](https://arxiv.org/pdf/2211.12507)> > Abstract:\n> The goal of automated feature generation is to liberate machine learning experts from the laborious task of manual feature generation, which is crucial for improving the learning performance of tabular data. The major challenge in automated feature generation is to efficiently and accurately identify effective features from a vast pool of candidate features. In this paper, we present OpenFE, an automated feature generation tool that provides competitive results against machine learning experts. OpenFE achieves high efficiency and accuracy with two components: 1) a novel feature boosting method for accurately evaluating the incremental performance of candidate features and 2) a two-stage pruning algorithm that performs feature pruning in a coarse-to-fine manner. Extensive experiments on ten benchmark datasets show that OpenFE outperforms existing baseline methods by a large margin. We further evaluate OpenFE in two Kaggle competitions with thousands of data science teams participating. In the two competitions, features generated by OpenFE with a simple baseline model can beat 99.3% and 99.6% data science teams respectively. In addition to the empirical results, we provide a theoretical perspective to show that feature generation can be beneficial in a simple yet representative setting. The code is available at [> this https URL\n](https://github.com/ZhangTP1996/OpenFE)> . Comments:|22 pages, 3 figures, accepted by ICML2023|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2211.12507](https://arxiv.org/abs/2211.12507)[cs.LG]|\n|(or[arXiv:2211.12507v3](https://arxiv.org/abs/2211.12507v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2211.12507](https://doi.org/10.48550/arXiv.2211.12507)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Tianping Zhang [[view email](https://arxiv.org/show-email/9b7f59b4/2211.12507)]\n**[[v1]](https://arxiv.org/abs/2211.12507v1)**Tue, 22 Nov 2022 03:23:40 UTC (588 KB)\n**[[v2]](https://arxiv.org/abs/2211.12507v2)**Sat, 20 May 2023 03:43:25 UTC (1,047 KB)\n**[v3]**Mon, 5 Jun 2023 13:22:12 UTC (1,048 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled OpenFE: Automated Feature Generation with Expert-level Performance, by Tianping Zhang and 7 other authors\n* [View PDF](https://arxiv.org/pdf/2211.12507)\n* [TeX Source](https://arxiv.org/src/2211.12507)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2211.12507&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2211.12507&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-11](https://arxiv.org/list/cs.LG/2022-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2211.12507?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2211.12507)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2211.12507)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2211.12507)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2211.12507&amp;description=OpenFE: Automated Feature Generation with Expert-level Performance>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2211.12507&amp;title=OpenFE: Automated Feature Generation with Expert-level Performance>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website...",
      "url": "https://arxiv.org/abs/2211.12507"
    },
    {
      "title": "",
      "text": "Gradient boosting machines and careful pre-processing work best: ASHRAE Great\nEnergy Predictor III lessons learned\nClayton Miller\u2217, Liu Hao, Chun Fu\nDepartment of the Built Environment, National University of Singapore (NUS), Singapore\n\u2217Corresponding Author: clayton@nus.edu.sg, +65 81602452\nAbstract\nThe ASHRAE Great Energy Predictor III (GEPIII) competition was held in late 2019 as one of the largest machine learning\ncompetitions ever held focused on building performance. It was hosted on the Kaggle platform and resulted in 39,402 prediction\nsubmissions, with the top five teams splitting $25,000 in prize money. This paper outlines lessons learned from participants, mainly\nfrom teams who scored in the top 5% of the competition. Various insights were gained from their experience through an online\nsurvey, analysis of publicly shared submissions and notebooks, and the documentation of the winning teams. The top-performing\nsolutions mostly used ensembles of Gradient Boosting Machine (GBM) tree-based models, with the LightGBM package being\nthe most popular. The survey participants indicated that the preprocessing and feature extraction phases were the most important\naspects of creating the best modeling approach. All the survey respondents used Python as their primary modeling tool, and it was\ncommon to use Jupyter-style Notebooks as development environments. These conclusions are essential to help steer the research\nand practical implementation of building energy meter prediction in the future.\n1. Introduction\nMachine learning (ML) for building energy prediction is a\nrich research community with hundreds of influential publica\u0002tions [1]. However, a fundamental challenge in the literature is a\nlack of comparability of prediction techniques [2], despite pre\u0002vious efforts at benchmarking [3, 4]. Machine learning compe\u0002titions provide a comparison of techniques through the crowd\u0002sourcing and benchmarking of various combinations of models\nwith a reward for the most objectively accurate solution.\nASHRAE has hosted three machine learning energy predic\u0002tion competitions since 1993. The first two competitions were\nnamed the Great Energy Predictor Shootouts I and II held in\n1993 and 1994. In the first competition, each contestant was\ngiven a four-month data set to predict building energy use and\ninsolation data for the next two months, and the final model,\nwhich was developed using Bayesian nonlinear modeling and\nartificial neural networks, was found to be the most effective\nand accurate [5]. While in the second competition, entrants\nwere asked to predict the intentionally removed portion of data\nbased on the existing building\u2019s pre-retrofit and post-retrofit\ndata [6]. Derived from the submissions that met the require\u0002ments, neural networks had shown to be the most accurate\nmodel overall, while cleverly assembled statistical models were\nadditionally found to be even more accurate than neural net\u0002works in some fields [7, 8]. Despite the passage of time, the\ncontents of these two competitions are still being investigated\nand used for references.\nAfter more than two decades, the third competition, titled the\nASHRAE Great Energy Predictor III (GEPIII), was initiated at\nthe ASHRAE Winter Conference in Chicago in January 2018.\nAfter getting the approval and financial sponsorship from the\nASHRAE Research Activities Committee (RAC), the competi\u0002tion was officially launched1 on 5 October 2019, and it ended\non 19 December 2019 [9]. The context of the GEPIII com\u0002petition was whether energy-saving building retrofitting could\nhelp to improve energy efficiency [10]. The datasets utilized in\nthe contest were collected from around 1,440 buildings from 16\nsites worldwide, of which 73% were educational, and the other\n27% were municipal and healthcare facilities. The energy me\u0002ter readings of these buildings from January 2016 to Decem\u0002ber 2018 were combined to form the dataset. Based on this\ncontext, the participants were challenged to create a counter\u0002factual model to estimate the building\u2019s pre-renovation energy\nusage rate in the post-renovation period [9]. The final rank\u0002ing of contestants was determined by the Private Leaderboard\n(PLB) scores, and the top five performers were awarded mone\u0002tary prizes.\nOne key output of the competition was learning from the con\u0002testants\u2019 solutions and understanding the general nature of what\nmakes the best machine learning solution for long-term build\u0002ing energy prediction. This paper outlines a post-competition\nsurvey used to capture the demographics, best practices, and\nlessons learned from a subset of the top-performing teams.\n2. Methodology\nThe primary goal of this analysis was to investigate the de\u0002mographics and machine learning strategy preferences of par\u00021https://www.kaggle.com/c/ashrae-energy-prediction\nPreprint submitted to ASHRAE Transactions February 8, 2022\narXiv:2202.02898v1 [cs.LG] 7 Feb 2022\nticipants from the top teams in the GEPIII competition. The pri\u0002mary component of this methodology was a survey composed\nof a segment that included questions regarding respondents\u2019\nbackground information and another with queries regarding the\nfinal solutions they submitted. The additional data collection\nprocess was done by analyzing the publicly available analysis\nnotebooks posted as part of the competition and the submis\u0002sions and interviews of the top five winning teams.\nThe first portion of the web-based survey gathered some ba\u0002sic information about the contestants, such as age, gender, ed\u0002ucational information, current job fields, and work experience.\nThe second portion of the survey was designed to gather infor\u0002mation regarding their participation experience. It can be fur\u0002ther broken down into three subsections. The first subsection is\nprimarily intended to collect information about how contestants\narrived at their final solutions, such as what programming lan\u0002guages they utilized, what platforms they selected most to run\ntheir codes and the methods and algorithms employed in each\nphase of building machine learning models. Furthermore, the\nparticipants were asked to express their opinions on the signifi\u0002cance of the five steps machine learning workflow in the second\nsubsection. In the last subsection, contestants are asked to pro\u0002vide their feedback on the competition by commenting on what\nparts they liked or disliked. These insights were designed to\nhelp the organizers understand what competition mechanisms\nthe contestants prefer, allowing improvements for future events.\nThis paper aims to outline the collection of insights target\u0002ing the teams that scored in the top 5% (180 teams who earned\ngold or silver medals) of the competition. This process seeks\nto characterize the insights and best practices of the contestants\nand teams that created solutions with the highest performance.\nTowards this effort, the survey was sent to teams from the top\u0002performing competition participants from May to August 2021.\nWe received responses from 27 individuals that included the\ncollective insights from 50 contestants, with team members of\nrespondents contained for non-demographic questions. We in\u0002cluded the data from publicly available online solutions posted\nby another 34 teams, including the top five winning teams for\nthe tools and modeling analysis. Most of the data were col\u0002lected from teams in the top 5% (including 90% of the survey\ndata).\n3. Results\n3.1. Demographics of contestants\nOne key focus of the competition was to work towards the\nbetter exchange of ideas between the building and construc\u0002tion industry and the fast-growing data science community. The\nfirst analysis from the survey data was to understand what back\u0002grounds the contestants generally had. Figure 1 illustrates the\nhigh-level demographics of the survey respondents.\nThe age range results showed that 40% of the respondents are\nbetween the ages of 30 and 39 and a further 23% are between\n20-29. These results compare well to a larger-scale generic\nsurvey collected by Kaggle in 2021, ...",
      "url": "https://arxiv.org/pdf/2202.02898"
    },
    {
      "title": "Decoding Kaggle's 2023 AI Report: Essential Tips for Machine Learning with Tabular Data \ud83d\udd0d\ud83d\udcc8 | vincelam",
      "text": "Go back\n\n# Decoding Kaggle's 2023 AI Report: Essential Tips for Machine Learning with Tabular Data \ud83d\udd0d\ud83d\udcc8\n\nPublished:30 Oct 2023\n\nIt\u2019s difficult for us to stay on top of the latest AI advancements with 100s of research papers, articles, and newsletters published daily. Luckily, Kaggle has recently published their [annual AI report](https://www.kaggle.com/AI-Report-2023) earlier this month, which distills and summarises the latest advancements this past action-packed year.\n\n## Table of contents\n\nOpen Table of contents\n\n- [The Kaggle AI Report 2023](https://vinlam.com/posts/tips-for-tabular-ml/#the-kaggle-ai-report-2023)\n- [Why learn from ML competitions winners?](https://vinlam.com/posts/tips-for-tabular-ml/#why-learn-from-ml-competitions-winners)\n  - [However, do take some of the learnings with a pinch of salt](https://vinlam.com/posts/tips-for-tabular-ml/#however-do-take-some-of-the-learnings-with-a-pinch-of-salt)\n- [Common themes from the winners of tabular / time-series data competitions](https://vinlam.com/posts/tips-for-tabular-ml/#common-themes-from-the-winners-of-tabular--time-series-data-competitions)\n- [Feature engineering is still a crucial part of the ML modelling pipeline](https://vinlam.com/posts/tips-for-tabular-ml/#feature-engineering-is-still-a-crucial-part-of-the-ml-modelling-pipeline)\n- [Robust cross-validation to trust the model results](https://vinlam.com/posts/tips-for-tabular-ml/#robust-cross-validation-to-trust-the-model-results)\n  - [Gradient Boosted Decision Trees are still beating Deep Learning models for tabular data](https://vinlam.com/posts/tips-for-tabular-ml/#gradient-boosted-decision-trees-are-still-beating-deep-learning-models-for-tabular-data)\n  - [Quick Recap on Gradient Boosted Decision Trees](https://vinlam.com/posts/tips-for-tabular-ml/#quick-recap-on-gradient-boosted-decision-trees)\n  - [What does the research literature say on this debate?](https://vinlam.com/posts/tips-for-tabular-ml/#what-does-the-research-literature-say-on-this-debate)\n- [Ensembling for performance gains](https://vinlam.com/posts/tips-for-tabular-ml/#ensembling-for-performance-gains)\n- [Conclusion](https://vinlam.com/posts/tips-for-tabular-ml/#conclusion)\n\n## The Kaggle AI Report 2023\n\nThe [Kaggle AI Report 2023](https://www.kaggle.com/AI-Report-2023) collates the best 3 essays on 7 important AI topics: (1) **Generative AI**, (2) **text data**, (3) **image & video data**, (4) **tabular / time-series data**, (5) **Kaggle competitions**, (6) **AI Ethics**, and (7) **Other** (for topics that do not fall neatly under the previous 6 areas). All essays are fascinating reads, jam-packed with information. Some essays provide a comprehensive overview of a topic, other essays dive deep into the technicalities, and other essays leave you with thought-provoking ideas on what the future may entail.\n\nFor this blog post, I\u2019ll focus on **tabular and time-series data** (in the context of machine learning (ML) competitions) since this is the most common data type that data scientists face and has the highest applicability/utility. As Bojan Tunguz said:\n\n> \u201cIt is estimated that between 50% and 90% of practicing data scientists use tabular data as their primary type of data in their primary setting.\u201d\n\nTo supplement my takeaways on best practice for tabular data, I\u2019ll also draw on the findings from ML Contests\u2019 [State of Competitive Machine Learning Report](https://mlcontests.com/state-of-competitive-machine-learning-2022) published earlier this year, which broadly aligns with Kaggle report. I\u2019ll write up on the latest advancements on the other AI topics in future blog posts.\n\n![DALL-E generated image of a researcher flooded by academic papers](https://vinlam.com/_astro/2023-10_researcher-flooded-by-papers.Cww976Os_Z3Rw6R.webp)\n\nDALL-E generated image of a researcher flooded by academic papers\n\n## Why learn from ML competitions winners?\n\nThere is great value in analysing winning solutions of ML competitions. Competitions act as a battleground for participants to test out the latest research models and architectures. Or as [ML Contests](https://mlcontests.com/state-of-competitive-machine-learning-2022/) puts it:\n\n> \u201cOne way to think of competitive machine learning is as a sandbox for evaluating predictive methods.\u201d\n\nCompetitions become more useful over time as we can see trends on which ideas work, as teams incorporate techniques from winning solutions from previous competitions into their own - forming new baselines. Two great resources for reading competition write-ups by winners are [DrivenData\u2019s blog](https://drivendata.co/blog.html) and [Kaggle discussions](https://www.kaggle.com/discussions?sort=votes&category=competitionWriteUps). Following these discussions is a useful proxy of staying on top of the latest practical advancements.\n\n### However, do take some of the learnings with a pinch of salt\n\nLearning how to improve a model\u2019s performance by a few decimal points may have a positive impact to a company\u2019s bottom line, especially if it serving millions of customers. However, there does come a point of diminishing returns when trying to eek out that extra .0001 of performance, depending on the business context. Because of the iterative nature of ML, it can be difficult to decide when \u201cgood\u201d is \u201cgood enough\u201d.\n\nWhilst machine learning competitions do push advancements in AI, the focus does not fully translate to the real-world professional setting. With the growing importance of **AI ethics**, many are concerned with the integrity and fairness of machine learning-based systems, services, and products.\n\nSince models are judged by their performance in competitions, a metric that is easily quantified, understood by competitors, and the determinant of the ranking for prizes and accolades - it becomes the main focus. This means a result-first approach rewards black-box approaches which do not consider **explainability** and **interpretability**. This is particularly relevant for **ensembling**, more on that later.\n\nHowever, I still believe using the consensus of the ML community acts as a useful starting point. As it is very easy to get bogged down by analysis paralysis when there are thousands of models, tools, and packages to choose from.\n\n## Common themes from the winners of tabular / time-series data competitions\n\nThere are a number of common themes that regularly appear amongst winning solutions of ML competitions, across various domains such as finance to healthcare. These themes agree with findings of academic literature too which have studied these competitions. These themes include the importance of:\n\n- **Feature engineering**\n- **Cross-validation**\n- **Gradient Boosted Decision Trees**\n- **Ensembling**\n\n## Feature engineering is still a crucial part of the ML modelling pipeline\n\n**Feature engineering** is the process of creating, selecting, and transforming variables to maximise their predictive power in models. It\u2019s a dynamic, iterative, and highly time-consuming process. Feature engineering is [well recognised](https://www.kaggle.com/discussions/getting-started/44997) as being one of the most important, if not [the most important](https://www.kaggle.com/competitions/home-credit-default-risk/discussion/64821), part of a tabular ML modelling pipeline, for both competitions and industry. Feature engineering is especially important for tabular data compared to deep learning techniques for computer vision tasks, where data augmentation is more focused on.\n\nIn Rhys Cook\u2019s [winning essay](https://www.kaggle.com/code/rhysie/learnings-from-the-typical-tabular-pipeline) they provided two great competition examples to illustrate how varied feature engineering approaches can be:\n\n1. **[Amex Credit Fault Prediction](https://www.kaggle.com/competitions/amex-default-prediction/overview)** (the most popular ML competition of 2022 with 5,000 teams entered, likely due to the growing scarcity of tabular Kaggle competitions and the $100k prize pool)\n2. **[AMP Parkinson\u2019s Dis...",
      "url": "https://vinlam.com/posts/tips-for-tabular-ml"
    },
    {
      "title": "A Reference Guide to Feature Engineering Methods | Kaggle",
      "text": "A Reference Guide to Feature Engineering Methods | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=4864dc437368295b:1:10007)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods"
    }
  ]
}