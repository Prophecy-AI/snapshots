{
  "query": "How do Kaggle winners handle severe class imbalance in binary classification competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle competition winners typically combine data\u2011level tricks, metric\u2011aware training, and model\u2011level safeguards to turn a wildly skewed binary target into a winning solution\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589); [Tomek Links, SMOTE, and XGBoost for Fraud Detection](https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d); [Classification on imbalanced data (TensorFlow)](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data); [Tabular Data Binary Classification: Tips from 5 Kaggle Competitions](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)):\n\n1. **Pick an imbalance\u2011aware evaluation metric** \u2013 use AUC\u2011PR, weighted\u202fF1, or a custom cost that reflects whether false positives or false negatives are more costly. This guides all later decisions\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589)).\n\n2. **Resample the training set**  \n   * **Oversample the minority class** with synthetic techniques such as SMOTE (or its variants like ADASYN) via the `imbalanced\u2011learn` library.  \n   * **Undersample the majority class** when the dataset is extremely large, or combine both (e.g., SMOTE\u202f+\u202fTomek links) to clean noisy examples\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589); [Tomek Links, SMOTE, and XGBoost for Fraud Detection](https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d)).\n\n3. **Apply class\u2011weighting in the model**  \n   * For tree\u2011based models (XGBoost, LightGBM) set `scale_pos_weight = (#negatives / #positives)`.  \n   * For linear/NN models (Logistic Regression, Keras) pass `class_weight` or `sample_weight` to the loss function. This lets the algorithm treat minority errors as more important\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589); [Classification on imbalanced data (TensorFlow)](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)).\n\n4. **Use robust algorithms and tuned objectives** \u2013 gradient\u2011boosted trees (XGBoost, LightGBM, CatBoost) with built\u2011in handling of imbalance, or deep nets with focal loss. Winners often experiment with several model families and keep the one that best respects the chosen metric\u202f([Tomek Links, SMOTE, and XGBoost for Fraud Detection](https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d)).\n\n5. **Stratified cross\u2011validation and fold\u2011level ensembling** \u2013 split data using `StratifiedKFold` so each fold preserves the class ratio, then train separate models on different resampling strategies (pure SMOTE, pure undersampling, raw data) and blend their predictions. This reduces variance caused by any single sampling choice\u202f([Tabular Data Binary Classification: Tips from 5 Kaggle Competitions](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).\n\n6. **Threshold optimisation** \u2013 after the model outputs probabilities, search for the probability cut\u2011off that maximises the competition\u2019s metric on a validation set (e.g., using a simple grid or Bayesian optimisation). This final tweak often yields the last few leaderboard points\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589)).\n\n7. **Iterate and monitor leakage** \u2013 constantly verify that resampling or weighting does not introduce data leakage across folds; keep the pipeline reproducible (e.g., fit SMOTE only on training folds). This disciplined workflow is a hallmark of top Kaggle solutions\u202f([Tabular Data Binary Classification: Tips from 5 Kaggle Competitions](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).\n\nFollowing these seven steps\u2014metric\u2011first, balanced sampling, class weighting, strong models, stratified ensembling, threshold tuning, and careful validation\u2014mirrors the practical playbook used by Kaggle competition winners to dominate severely imbalanced binary classification challenges.",
      "url": ""
    },
    {
      "title": "Unbalanced data and cross-validation | Kaggle",
      "text": "<div><div><p>There are two general ways of dealing with imbalanced data: 1) change the data; 2) leave the data alone but change the performance metric and/or the weight of individual data points.</p>\n<p>Changing the data means oversampling the under-represented class(es) with synthetic data points, or undersampling (thinning down) the over-represented class(es). There are various ways of doing both operations, and I will give you one <a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\"><strong>starting point</strong></a>.</p>\n<p>A second option is to leave the data alone and adjust how we do the training. It is crucial to pick a reliable performance metric for imbalanced data, and to know whether for your purposes it is worse to have too many false positives (FPs) or false negatives (FNs). Ideally, you wouldn't want to have either of the two, but for most applications it should be clear whether we want to avoid FPs or FNs with greater preference.</p>\n<p>To give you an example: you have 900 healthy patients (class 0) and 100 cancer patients (class 1), which is at least somewhat close to the true ratio of healthy and cancer-stricken people out there. If you predict class 0 with 100% accuracy and class 1 with 70% accuracy, your accuracy will be 97% for the whole dataset. That sounds tremendous until we realize that 30 out of 100 cancer patients (30%) will go undiagnosed, meaning that high false negative rate in this case will cost many lives. What if we could adjust our classification so that both classes are predicted with 97% accuracy? This would mean that 27 healthy people would be predicted to have cancer (27 FPs) and 3 cancer-stricken individual would be predicted to be healthy (3 FNs). Now, I wouldn't want to be a doctor who tells a healthy person that they have a cancer because many people have been known to do silly things when they think they are dying. Still, compared to our starting premise, most would agree that on balance it is much better to have 27 people who think they have cancer and later find out they don't, and only 3 patients who were told they are healthy even though they have cancer. The problem is that a simple accuracy as a performance metric for training would never get you to a balanced 97% accuracy. For imbalanced datasets, F1-score (a weighted average of precision and recall), area under curve for ROC and Matthews correlation coefficient may be better measures of classifier quality.</p>\n<p>See the links below for more info:</p>\n<p><a href=\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\"><strong>link1</strong></a></p>\n<p><a href=\"https://eva.fing.edu.uy/mod/resource/view.php?id=33977\"><strong>link2</strong></a></p>\n<p><a href=\"https://www.kaggle.com/c/bosch-production-line-performance/forums/t/24120/time-effort-and-reward?forumMessageId=138285#post138285\"><strong>link3</strong></a></p></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/27589"
    },
    {
      "title": "Classification on imbalanced data \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Classification on imbalanced data | TensorFlow Core[Skip to main content](#main-content)\n[![TensorFlow](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/tensorflow/images/lockup.svg)](https://www.tensorflow.org/)\n* /\n* English\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4[GitHub](https://github.com/tensorflow)Sign in\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n* [TensorFlow](https://www.tensorflow.org/)\n* [Learn](https://www.tensorflow.org/learn)\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n# Classification on imbalanced dataStay organized with collectionsSave and categorize content based on your preferences.\n[![](https://www.tensorflow.org/images/tf_logo_32px.png)View on TensorFlow.org](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)|[![](https://www.tensorflow.org/images/colab_logo_32px.png)Run in Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/GitHub-Mark-32px.png)View source on GitHub](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/download_logo_32px.png)Download notebook](https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/imbalanced_data.ipynb)|\nThis tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use[Keras](https://www.tensorflow.org/guide/keras/overview)to define the model and[class weights](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model)to help the model learn from the imbalanced data. .\nThis tutorial contains complete code to:\n* Load a CSV file using Pandas.\n* Create train, validation, and test sets.\n* Define and train a model using Keras (including setting class weights).\n* Evaluate the model using various metrics (including precision and recall).\n* Select a threshold for a probabilistic classifier to get a deterministic classifier.\n* Try and compare with class weighted modelling and oversampling.## Setup\n```\n`importtensorflowastffromtensorflowimportkerasimportosimporttempfileimportmatplotlibasmplimportmatplotlib.pyplotaspltimportnumpyasnpimportpandasaspdimportseabornassnsimportsklearnfromsklearn.metricsimportconfusion\\_matrixfromsklearn.model\\_selectionimporttrain\\_test\\_splitfromsklearn.preprocessingimportStandardScaler`\n```\n```\n2024-08-20 01:23:52.305388: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 01:23:52.326935: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 01:23:52.333533: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n```\n`mpl.rcParams['figure.figsize']=(12,10)colors=plt.rcParams['axes.prop\\_cycle'].by\\_key()['color']`\n```\n## Data processing and exploration\n### Download the Kaggle Credit Card Fraud data set\nPandas is a Python library with many helpful utilities for loading and working with structured data. It can be used to download CSVs into a Pandas[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n**Note:**This dataset has been collected and analysed during a research collaboration of Worldline and the[Machine Learning Group](http://mlg.ulb.ac.be)of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available[here](https://www.researchgate.net/project/Fraud-detection-5)and the page of the[DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/)project\n```\n`file=tf.keras.utilsraw\\_df=pd.read\\_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')raw\\_df.head()`\n```\n```\n`raw\\_df[['Time','V1','V2','V3','V4','V5','V26','V27','V28','Amount','Class']].describe()`\n```\n### Examine the class label imbalance\nLet's look at the dataset imbalance:\n```\n`neg,pos=np.bincount(raw\\_df['Class'])total=neg+posprint('Examples:\\\\nTotal:{}\\\\nPositive:{}({:.2f}% of total)\\\\n'.format(total,pos,100\\*pos/total))`\n```\n```\nExamples:\nTotal: 284807\nPositive: 492 (0.17% of total)\n```\nThis shows the small fraction of positive samples.\n### Clean, split and normalize the data\nThe raw data has a few issues. First the`Time`and`Amount`columns are too variable to use directly. Drop the`Time`column (since it's not clear what it means) and take the log of the`Amount`column to reduce its range.\n```\n`cleaned\\_df=raw\\_df.copy()# You don't want the `Time` column.cleaned\\_df.pop('Time')# The `Amount` column covers a huge range. Convert to log-space.eps=0.001# 0 =&gt; 0.1\u00a2cleaned\\_df['Log Amount']=np.log(cleaned\\_df.pop('Amount')+eps)`\n```\nSplit the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where[overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)is a significant concern from the lack of training data.\n```\n`# Use a utility from sklearn to split and shuffle your dataset.train\\_df,test\\_df=train\\_test\\_split(cleaned\\_df,test\\_size=0.2)train\\_df,val\\_df=train\\_test\\_split(train\\_df,test\\_size=0.2)# Form np arrays of labels and features.train\\_labels=np.array(train\\_df.pop('Class')).reshape(-1,1)bool\\_train\\_labels=train\\_labels[:,0]!=0val\\_labels=np.array(val\\_df.pop('Class')).reshape(-1,1)test\\_labels=np.array(test\\_df.pop('Class')).reshape(-1,1)train\\_features=np.array(train\\_df)val\\_features=np.array(val\\_df)test\\_features=np.array(test\\_df)`\n```\nWe check whether the distribution of the classes in the three sets is about the same or not.\n```\n`print(f'Average class probability in training set:{train\\_labels.mean():.4f}')print(f'Average class probability in validation set:{val\\_labels.mean():.4f}')print(f'Average class probability in test set:{test\\_labels.mean():.4f}')`\n```\n```\nAverage class probability in training set: 0.0017\nAverage class probability in validation set: 0.0018\nAverage class probability in test set: 0.0018\n```\nGiven the small number of positive labels, this seems about right.\nNormalize the input features using the sklearn StandardScaler.\nThis will set the mean to 0 and standard deviation to 1.\n**Note:**The`StandardScaler`is only fit using the`train\\_features`to be sure the model is not peeking at the validation or test sets.\n```\n`scaler=StandardScaler()train\\_features=scaler.fit\\_transform(train\\_features)val\\_features=scaler.transform(val\\_features)test\\_features=scaler.transform(test\\_features)train\\_features=np.clip(train\\_features,-5,5)val\\_features=np.clip(val\\_features,-5,5)test\\_features=np.clip(test\\_features,-5,5)print('Training labels shape:',train\\_labels.shape)print('Validation labels shape:',...",
      "url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
    },
    {
      "title": "Tomek Links, SMOTE, and XGBoost for Fraud Detection",
      "text": "Tomek Links, SMOTE, and XGBoost for Fraud Detection | by Eric Johnson | Medium\n[Sitemap](https://epjohnson13.medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# Tomek Links, SMOTE, and XGBoost for Fraud Detection\n[\n![Eric Johnson](https://miro.medium.com/v2/resize:fill:64:64/1*qPhIhlmetzIJdf6lxdw74g.jpeg)\n](https://epjohnson13.medium.com/?source=post_page---byline--1fc8b5208e0d---------------------------------------)\n[Eric Johnson](https://epjohnson13.medium.com/?source=post_page---byline--1fc8b5208e0d---------------------------------------)\n17 min read\n\u00b7Jun 25, 2021\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/1fc8b5208e0d&amp;operation=register&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;user=Eric+Johnson&amp;userId=253631ceb5cb&amp;source=---header_actions--1fc8b5208e0d---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/1fc8b5208e0d&amp;operation=register&amp;redirect=https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d&amp;source=---header_actions--1fc8b5208e0d---------------------bookmark_footer------------------)\nListen\nShare\n**Table of Contents:**\n1. [**Introduction**](#9eaf)\n2. [**A First Look at the Data**](#70a0)\n3. [**Imbalanced Learning Problems**](#bfcb)\n4.[**SMOTE**](#bce8)\n5.[**Tomek Links**](#f37e)\n6.[**Why XGBoost**](#7a54)\n7.[**Variables Used and Hyperparameter Tuning**](#c6db)\n8.[**Results and Conclusion**](#5fc6)\n## **Introduction**\nIn this article I will describe a method to resample the Kaggle Credit Card Fraud dataset to feed into XGBoost for credit card fraud detection. The resampling is a combination of under- and over-sampling. The under-sampling is accomplished by Tomek Links and the over-sampling by SMOTE. Implementing the resampling is easy with the imblearn package, but understanding what it is we are doing, and in what order, is critical to explaining why this is a valid processing step. I will explain this in the section labelled Imbalanced Learning Problems.\nThe resampling itself improves out-of-the-box XGBoost performance from an 84% fraud detection rate (recall of the fraud data points) and a 99%+ correct classification rate (overall accuracy)to an 88% fraud detection rate and a 99%+ correct classification rate. After tuning XGBoost hyperparameters, I end up with a model that has a 92% fraud detection rate and a 98% correct classification rate. These results vary up or down by a few percentage points depending on the chosen random seed. The obtained recall and overall accuracy values sit near the middle of their respective distributions.\nAs a new data scientist with a PhD in mathematics I was excited to work with this dataset because I believed it presented an opportunity to focus on a few cutting edge classification algorithms. For instance, XGBoost is a fascinating machine learning algorithm that combines mathematical, statistical and computer hardware insights to build a wide array of classification and regression trees. What was truly interesting is that the real challenge with this dataset was finding ways to overcome the class imbalance before modeling even happened.\n## **A First Look at the Data**\nA description of the dataset can be found[here](https://www.kaggle.com/mlg-ulb/creditcardfraud). When we load the dataset, we see that there are 31 variables, 30 of which are data sampled from a continuous variable. I.e., there are no categorical variables other than the class labels which are binary, 0 for a legitimate transaction and 1 for a fraudulent transaction. The 28 variables \u201cV1\u201d \u2014\u201cV28\u201d have been created from a principal component analysis, and so these variables are statistically (and linearly) independent of each other. This also means that most of the variables are not interpretable in terms of domain specific knowledge. The only variables that come with meaningful names are \u201cTime\u201d, \u201cAmount\u201d and \u201cClass\u201d.\nThe \u201cTime\u201d variable is defined as the time duration since the first transaction in the dataset. This is very unlikely to help us since there is no account identification information. This means we cannot calculate the time in-between consecutive transactions for a given credit account. However, it could be argued that if we knew the date on which the first transaction happened then we might be able to track a seasonal component to credit fraud. Maybe there is more fraud generally in the summer (pure speculation)? The date of the first transaction or even a creation date for the dataset are not available. So we drop the \u201cTime\u201d column when we split the dataset into potential model features, X, and the class labels, y.\nBelow is a 3d-plot of the three most significant components derived from PCA analysis of the dataset.\n![]()\nThis plot is evidence for a severe class imbalance. We will talk about that just below and verify that there is a severe class imbalance in the dataset. PCA generally preserves features like clustering. I say generally because counterexamples where PCA destroys cluster information are easy to cook up. You can click[here](https://towardsdatascience.com/interesting-projections-where-pca-fails-fe64ddca73e6)if you would like to read a Towards Data Science article detailing an alternative to PCA called PPA. Moreover, if we are to believe that this PCA preserved most of the important information, the amount of overlap between the fraud data points and legitimate points seems like it might be a challenge to overcome. Luckily, the tight packing of fraudulent and legitimate transactions displayed in this plot will not end up having much impact on final predictive performance.\nBy creating a simple histogram for each variable, one sees that the \u201cAmount\u201d variable covers a very large range of numbers and that the data is concentrated in two main clusters, a low transaction amount cluster and a high transaction amount cluster. To prevent this large change of scale from detracting from a better model, we transform the Amount variable to log space and create the \u201cLog\\_Amount\u201d variable. I am not going to show code for this as it will turn out that \u201cLog\\_Amount\u201d and \u201cAmount\u201d both have a low correlation with the class labels and I did not use them in the final model.\nThe Kaggle Credit Card Fraud Dataset is severely imbalanced. We can see this by doing a count of the values in the \u201cClass\u201d column and making the following chart (found just below the code) using Matplotlib.\n![]()\nIn general, before we have chosen a specific algorithm, there are several standard options for handling this kind of imbalance. Some of these options work at the algorithm level, where a penalty is implemented for classifying the minority class...",
      "url": "https://epjohnson13.medium.com/tomek-links-smote-and-xgboost-for-fraud-detection-1fc8b5208e0d"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Methods for Dealing with Imbalanced Data",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data"
    },
    {
      "title": "Dealing with extremely imbalanced dataset - binary classification | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=42762aa12f6df4b246e8:1:11101)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/473546"
    },
    {
      "title": "Imbalanced Classification: Advanced Algorithms",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=2ba54687d659d86c1424:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/ashrafkhan94/imbalanced-classification-advanced-algorithms"
    },
    {
      "title": "GitHub - siamak1399/Kaggle-Imbalanced-Binary-Classification: This is a binary classification for severely Imbalanced Kaggle dataset to predict Insurance renewal based on 11 features with missing values. The repository including two notebooks composed of EDA, tricks in data preprocessing, modeling with classic methods and presenting an innovative ML algorithm.",
      "text": "<div><div><article><p></p><h2>Kaggle-Imbalanced-Binary-Classification</h2><a href=\"#kaggle-imbalanced-binary-classification\"></a><p></p>\n<p>The main goal of this work is not stick to that special dataset but to review Important concepts such as generalizability of models, data preprocessing, feature extraction, modeling and evaluation. ML models such as logistic regression, random forest, support vector machine with different kernels, Gaussian Naive Bayes, AdaBoost classifier and Neural networks will be applied to predict class label of Kaggle dataset. This project inspired me to develop a new ML algorithm that is very powerful for cases that meet the required conditions.</p>\n<p>In Part I, the importance of choosing special evaluating metrics for imbalanced datasets and some innovative tricks on scaling, transforming and target encoding will be explained. Also, new features will be extracted via PCA, ICA, LDA and polynomial feature extraction methods. Feature selection section will be omitted since, a great innovative feature selection method presented in my previous notebook on car price prediction. Exploratory data analysis and plots are available at appendix in chapter 15.</p>\n<p>In part II, the theory behind innovative approach that called PMG, Parallel Modeling of Groups, for data are derived from multiple sources or from the one source with different modes is presented and two shallow version, PMG-clustered and PMG-boosted are applied to traindata_sia_train.csv and traindata_sia_test.csv datasets that are extracted in Part I. Finally, shallow and deep ML versions of the powerful innovative hybrid of PMG with bagging and boosting are described.</p>\n</article></div></div>",
      "url": "https://github.com/siamak1399/kaggle-imbalanced-binary-classification"
    }
  ]
}