{
  "query": "What are the best text preprocessing and feature engineering techniques for Kaggle competitions with Reddit-style posts?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Text preprocessing for Reddit\u2011style posts usually starts with a Reddit\u2011aware tokenizer that can split markdown, URLs, sub\u2011reddit mentions (r/\u2026), user mentions (u/\u2026), emojis and hashtags while preserving useful tokens.\u202fThe **RedditScore** library ships a \u201cCrazyTokenizer\u201d with exactly these options and works well as a drop\u2011in step before any model ([redditScore](https://redditscore.readthedocs.io/en/master)).\u202fAfter tokenization, the typical pipeline mirrors other Kaggle NLP wins: lower\u2011case everything, strip HTML tags, remove or replace URLs and user mentions, and optionally keep emojis as separate tokens because they often carry sentiment.\u202fStop\u2011word removal, lemmatization (or light stemming) and the removal of very rare tokens help reduce noise, especially when the dataset is large; many top solutions also truncate or pad to a maximum length to fit transformer\u2011based models.\u202fFor very large corpora, memory\u2011friendly tricks such as loading data with **Dask** or converting to **parquet/feather** formats keep the preprocessing fast and fit within Kaggle kernels ([Neptune blog](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\n\nFeature engineering builds on the cleaned text.\u202fA strong baseline is **TF\u2011IDF** with word\u2011 and character\u2011level n\u2011grams (1\u20113\u202fgrams for words, 3\u20115\u202fgrams for characters), followed by **\u03c7\u00b2** feature selection to keep the most discriminative terms\u2014a technique that helped a Reddit\u2011comment classifier reach >58\u202f% accuracy on the public leaderboard ([GitHub\u202fReddit\u2011Classification](https://github.com/nikhilpodila/Reddit-Classification)).\u202fBeyond sparse vectors, dense embeddings such as **fastText** (trained on Reddit data via RedditScore) or pretrained transformer embeddings (BERT, RoBERTa) capture semantic nuance and are often fine\u2011tuned on the competition data.\u202fAdditional engineered signals that are specific to Reddit include: post length, number of exclamation/question marks, count of emojis, number of URLs or code blocks, subreddit ID, posting hour/day, and author karma or comment score when available.\u202fThese meta\u2011features are concatenated with the text representation and fed to models ranging from linear classifiers (Logistic Regression, Linear SVM) to deep ensembles (FastText + transformer + gradient\u2011boosted trees).  \n\nTogether, a Reddit\u2011aware tokenizer, careful cleaning, sparse\u202f+\u202fdense textual features, \u03c7\u00b2\u2011based selection, and Reddit\u2011specific meta\u2011features form the backbone of the most successful Kaggle pipelines for Reddit\u2011style posts.",
      "url": ""
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "GitHub - nikhilpodila/Reddit-Classification: Reddit comments classification",
      "text": "<div><div><article><p></p><h2>Text classification - Reddit comments dataset</h2><a href=\"#text-classification---reddit-comments-dataset\"></a><p></p>\n<p></p><h3>Contributors: Nikhil Podila, Shantanil Bagchi, Manoosh S</h3><a href=\"#contributors-nikhil-podila-shantanil-bagchi-manoosh-s\"></a><p></p>\n<p></p><h3>Mini-Project 2 - COMP 551 Applied Machine Learning - McGill University</h3><a href=\"#mini-project-2---comp-551-applied-machine-learning---mcgill-university\"></a><p></p>\n<p></p><h2>Abstract</h2><a href=\"#abstract\"></a><p></p>\n<p>In this project, we investigate the performance of text classi\ufb01cation methods on reddit posts from over 20 subreddits. We preprocess the data using natural language processing techniques such as stopword removal, TF-IDF weighting, \u03c72 test for feature selection. We used various classi\ufb01cation algorithms and found that an Ensemble Classi\ufb01er performed the best on this dataset. We also implemented Bernoulli Naive Bayes from scratch. Performances of all the classi\ufb01ers and our implementation of Bernoulli NB are compared on the dataset. We achieved an accuracy of 61.02% on held-out validation set and 58.633% on Kaggle test set.</p>\n<p></p><h2>Repository Structure</h2><a href=\"#repository-structure\"></a><p></p>\n<p>The repository contains 5 files:</p>\n<ul>\n<li>1 Jupyter notebook file - PROJECT 2- FINAL.ipynb</li>\n<li>2 Dataset files - reddit_train.csv and reddit_test.csv</li>\n<li>1 ReadMe file - ReadMe.md</li>\n<li>1 Project writeup - writeup.pdf</li>\n</ul>\n<p></p><h2>Code Usage - (Python 3.6.2, conda 4.3.23)</h2><a href=\"#code-usage---python-362-conda-4323\"></a><p></p>\n<ol>\n<li>Install the following libraries for Python (All the packages mentioned below can be installed using pip. <br/>\nIn Jupyter notebook, use: !pip install &lt;package_name&gt;):</li>\n</ol>\n<ul>\n<li>sklearn</li>\n<li>numpy</li>\n<li>pandas</li>\n<li>time</li>\n<li>re</li>\n<li>scipy</li>\n<li>itertools</li>\n<li>seaborn</li>\n<li>matplotlib</li>\n<li>nltk</li>\n<li>tqdm</li>\n<li>gensim</li>\n<li>pyLDAvis</li>\n<li>logging</li>\n<li>pprint</li>\n<li>wordcloud</li>\n<li>spacy</li>\n</ul>\n<ol>\n<li>Download all Jupyter notebook and Dataset files into one directory.</li>\n<li>Open Jupyter notebook into that directory.</li>\n<li>Select the required notebook (.ipynb file) and select \"Run All\" inside the jupyter notebook file.</li>\n</ol>\n</article></div></div>",
      "url": "https://github.com/nikhilpodila/Reddit-Classification"
    },
    {
      "title": "RedditScore Overview \u00b6",
      "text": "RedditScore Overview &mdash; RedditScore 0.7.0 documentation\n* [Docs](#)&raquo;\n* RedditScore Overview\n* [Edit on GitHub](https://github.com/crazyfrogspb/RedditScore/blob/master/docs/source/index.rst)\n# RedditScore Overview[\u00b6](#redditscore-overview)\nRedditScore is a library that contains tools for building Reddit-based text classification models\nRedditScore includes:\n> > * > Document tokenizer with myriads of options, including Reddit- and Twitter-specific options\n> * > Tools to build and tune the most popular text classification models without any hassle\n> * > Functions to easily collect Reddit comments from Google BigQuery and Twitter data (including tweets beyond 3200 tweets limit)\n> * > Instruments to help you build more efficient Reddit-based models and to obtain RedditScores (\n[> Nikitin2018\n](#nikitin2018)> )\n> * > Tools to use pre-built Reddit-based models to obtain RedditScores for your data\n> > **Note:**RedditScore library and this tutorial are work-in-progress.[Let me know if you experience any issues](https://github.com/crazyfrogspb/RedditScore/issues).\nUsage example:\n```\nimportosimportpandasaspdfromredditscoreimporttokenizerfromredditscore.modelsimportfasttext\\_moddf=pd.read\\_csv(os.path.join(&#39;redditscore&#39;,&#39;&#39;reddit\\_small\\_sample.csv&#39;&#39;))df=df.sample(frac=1.0,random\\_state=24)# shuffling datatokenizer=CrazyTokenizer(hashtags=&#39;split&#39;)# initializing tokenizer objectX=df[&#39;body&#39;].apply(tokenizer.tokenize)# tokenizing Reddit commentsy=df[&#39;subreddit&#39;]fasttext\\_model=fasttext\\_mod.FastTextModel()# initializing fastText modelfasttext\\_model.tune\\_params(X,y,cv=5,scoring=&#39;accuracy&#39;)# tune hyperparameters of the model using default gridfasttext\\_model.fit(X,y)# fit modelfasttext\\_model.save\\_model(&#39;&#39;models/fasttext\\_model&#39;&#39;)# save modelfasttext\\_model=fasttext.load\\_model(&#39;&#39;models/fasttext\\_model&#39;&#39;)# load modeldendrogram\\_pars={&#39;&#39;leaf\\_font\\_size&#39;&#39;:14}tsne\\_pars={&#39;perplexity&#39;:30.0}fasttext\\_model.plot\\_analytics(dendrogram\\_pars=dendrogram\\_pars,# plot dendrogram and T-SNE plottsne\\_pars=tsne\\_pars,fig\\_sizes=((25,20),(22,22)))probs=fasttext\\_model.predict\\_proba(X)av\\_scores,max\\_scores=fasttext\\_model.similarity\\_scores(X)\n```\nReferences:\n[Nikitin2018]|Nikitin Evgenii, Identyifing Political Trends on Social Media Using Reddit Data, in progress|\nContents:\n* [RedditScore Overview](overview.html)\n* [Installation](installation.html)\n* [Data Collection](data_collection.html)\n* [Reddit Data](data_collection.html#reddit-data)\n* [Twitter Data](data_collection.html#twitter-data)\n* [Tokenizing](tokenizing.html)\n* [Tokenizer description](tokenizing.html#tokenizer-description)\n* [Initializing](tokenizing.html#initializing)\n* [Features](tokenizing.html#features)\n* [Lowercasing and all caps](tokenizing.html#lowercasing-and-all-caps)\n* [Normalizing](tokenizing.html#normalizing)\n* [Ignoring quotes](tokenizing.html#ignoring-quotes)\n* [Removing stop words](tokenizing.html#removing-stop-words)\n* [Word stemming and lemmatizing](tokenizing.html#word-stemming-and-lemmatizing)\n* [Removing punctuation and linebreaks](tokenizing.html#removing-punctuation-and-linebreaks)\n* [Decontracting](tokenizing.html#decontracting)\n* [Dealing with hashtags](tokenizing.html#dealing-with-hashtags)\n* [Dealing with special tokens](tokenizing.html#dealing-with-special-tokens)\n* [URLs](tokenizing.html#urls)\n* [Extra patterns and keeping untokenized](tokenizing.html#extra-patterns-and-keeping-untokenized)\n* [Converting whitespaces to underscores](tokenizing.html#converting-whitespaces-to-underscores)\n* [Removing non-unicode characters](tokenizing.html#removing-non-unicode-characters)\n* [Emojis](tokenizing.html#emojis)\n* [Unicode and hex characters](tokenizing.html#unicode-and-hex-characters)\n* [n-grams](tokenizing.html#n-grams)\n* [Modelling](modelling.html)\n* [Fitting models](modelling.html#fitting-models)\n* [Model persistence](modelling.html#model-persistence)\n* [Predictions and similarity scores](modelling.html#predictions-and-similarity-scores)\n* [Model tuning and validation](modelling.html#model-tuning-and-validation)\n* [Visualization of the class embeddings](modelling.html#visualization-of-the-class-embeddings)\n* [API Documentation](apis/api_main.html)\n* [CrazyTokenizer](apis/tokenizer.html)\n* [Models](apis/models.html)\n* [BoWModel](apis/bow_mod.html)\n* [FastTextModel](apis/fasttext_mod.html)\n* [Neural networks](apis/nn_mod.html)\n# Indices and tables[\u00b6](#indices-and-tables)\n* [Index](genindex.html)\n* [Module Index](py-modindex.html)\n* [Search Page](search.html)",
      "url": "https://redditscore.readthedocs.io/en/master"
    },
    {
      "title": "Comprehensive Guide to Text Preprocessing with \ud83d\udc0d",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8bc8e429eb9b3f33e2fc:1:10746)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/afi1289/comprehensive-guide-to-text-preprocessing-with"
    },
    {
      "title": "Preprocessing Text Data in Python: An Introduction via Kaggle",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7d28ad9c9eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreprocessing-text-data-in-python-an-introduction-via-kaggle-7d28ad9c9eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreprocessing-text-data-in-python-an-introduction-via-kaggle-7d28ad9c9eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreprocessing-text-data-in-python-an-introduction-via-kaggle-7d28ad9c9eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreprocessing-text-data-in-python-an-introduction-via-kaggle-7d28ad9c9eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Preprocessing Text Data in Python: An Introduction via Kaggle\n\n[![Soren Gran](https://miro.medium.com/v2/resize:fill:88:88/2*zOGDgW7ErPd07FY10sCGrA.jpeg)](https://soren-gran.medium.com/?source=post_page-----7d28ad9c9eb--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----7d28ad9c9eb--------------------------------)\n\n[Soren Gran](https://soren-gran.medium.com/?source=post_page-----7d28ad9c9eb--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F306f70d5e2cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpreprocessing-text-data-in-python-an-introduction-via-kaggle-7d28ad9c9eb&user=Soren+Gran&userId=306f70d5e2cc&source=post_page-306f70d5e2cc----7d28ad9c9eb---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7d28ad9c9eb--------------------------------)\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nMar 11, 2020\n\n--\n\n1\n\nListen\n\nShare\n\nAs a data scientist, you will inevitably work with text data. Working with text data, such as tweets, abstracts, or newspaper articles, is extremely different from working with traditional numerical data, such as temperature or financial data.\n\nThis difference is no greater than during preprocessing. While preprocessing for numerical data is dependent largely on the data (Which columns are missing lots of data? Are any columns not ready to enter the model? Do any columns need to be one-hot encoded?), preprocessing of text data is actually a fairly straightforward process, although understanding each step and its purpose is less trivial.\n\nThe goal of preprocessing text data is to take the data from its raw, readable form to a format that the computer can more easily work with. Most text data, and the data we will work with in this article, arrive as strings of text. Preprocessing is all the work that takes the raw input data and prepares it for insertion into a model.\n\nThis article presents the entire process for completing [Kaggle\u2019s Disaster Tweets challenge](https://www.kaggle.com/c/nlp-getting-started). Although this article includes the entire process, we focus on the preprocessing step. This article describes the stages involved in a typical preprocessing of text data, and explains the rationale and effects of each stage. For more information about the other steps, please see links included in each section.\n\nThis Kaggle dataset consists of tweets that use disaster-related language. The goal is to create a classifier that can determine if a tweet that contains disaster-related language is actually about a disaster or is using that same language for a different, non-emergency purpose.\n\nFor example, the Tweets \u201c@bbcmtd Wholesale Markets ablaze [http://t.co/lHYXEOHY6C](http://t.co/lHYXEOHY6C)\u201d and \u201cCrying out for more! Set me ablaze\u201d both contain the keyword \u201cablaze\u201d, but only one of them refers to a disaster. To us, it is fairly obvious that the former tweet is describing a fire at a grocery store, while the second tweet is not referring to a literal fire or disaster \u2014 more likely, a music concert. However, if a data scientist wanted to scrape Twitter for tweets referring to real disasters in order to alert medical services, they would face a challenge: they would have to build a classifier that can tell that despite the fact that both tweets contain a word that means \u201con fire\u201d, only one of them is describing a literal, dangerous fire.\n\nIt turns out that building such a classifier is not difficult \u2014 we will do it using a logistic regression model. The key to going from a tweet we would read to data our model can actually learn from is preprocessing.\n\n# Methods\n\nFor this article, we will essentially complete the Kaggle challenge; i.e. build a classifier for the disaster tweets dataset. Most of our explanation will focus on the preprocessing section, although we will link to useful articles for each other step. Our method consists of data import, data exploration, preprocessing, model training, and presenting results. As a whole, this method is fairly light because the main focus of this article is on the preprocessing.\n\nFirst of all, of course, we must import relevant packages:\n\n**Data Import**\n\nNext, we import the data. The simplest way is by first downloading the data from Kaggle, which can be found [here](https://www.kaggle.com/c/nlp-getting-started/data). Then, read the CSV\u2019s using Pandas. Obviously, the path depends on the respective file locations on your computer.\n\n**Data Exploration**\n\nFirst, let\u2019s take a peek at our data:\n\nThe Disaster Tweets data consists of four columns: \u201ckeyword\u201d, \u201clocation\u201d, \u201ctext\u201d, and \u201ctarget\u201d. \u201ckeyword\u201d refers to the specific word in the tweet that signified potential disaster. \u201clocation\u201d data exists only if the Twitter user tagged a location when they sent the tweet. \u201ctext\u201d contains the text of the tweet. Finally, \u201ctarget\u201d is our dependent variable \u2014 1 if the tweet refers to a legitimate disaster and 0 if the tweet is a false positive.\n\nClearly, there are a lot of NaNs in the \u201ckeyword\u201d and \u201clocation\u201d columns. How many, though? The density of data in those columns could be a concern.\n\n\u201ckeyword\u201d actually has 99% density but \u201clocation\u201d only has 67% density. We\u2019ll drop it because it\u2019s missing so much data. We will also drop \u201ckeyword\u201d but for a different reason. \u201ckeyword\u201d is repetitive because it simply contains a word from \u201ctext\u201d. \u201ckeyword\u201d will offer zero insight in finding the distinction between a disaster tweet and a non-disaster tweet that both have the keyword \u201cablaze\u201d. We will drop both columns and move forward with only \u201ctext\u201d as an independent variable. \u201ctext\u201d has 100% density.\n\nFor more information about Exploratory Data Analysis for text data, I recommend [this article](https://medium.com/@kamilmysiak/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d).\n\n**Preprocessing**\n\nOur preprocessing method consists of two stages: preparation and vectorization. The preparation stage consists of steps that clean up the data and cut the fat. The steps are 1. removing URLs and Twitter handles, 2. making all text lowercase, 3. removing numbers, 4. removing punctuation, 5. tokenization, 6. removing stopwords, and 7. lemmatization. Stopwords are words that typically add no meaning. F...",
      "url": "https://towardsdatascience.com/preprocessing-text-data-in-python-an-introduction-via-kaggle-7d28ad9c9eb?gi=0a193996d28d"
    },
    {
      "title": "Natural Language Processing Guide",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/learn-guide/natural-language-processing#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/learn-guide/natural-language-processing)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/learn-guide/natural-language-processing#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Flearn-guide%2Fnatural-language-processing)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Flearn-guide%2Fnatural-language-processing)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n# Natural Language Processing Guide\n\nNatural language processing (NLP) is a subfield of machine learning concerned with building models to demonstrate human-level understanding of written and spoken text.\n\n![Kaggle Tutorials](https://storage.googleapis.com/kaggle-media/Images/dark_Kaggle_Tutorials.svg)\n\nKaggle Tutorials\n\nThe best way to get started is by making your own private copy of the tutorial notebooks below to run the code. To do this, after clicking to view a notebook that interests you, make your own copy by clicking on the \"Copy & Edit\" button in the top right corner. If you find the notebook useful, make sure to give it an upvote!\n\n![](https://storage.googleapis.com/kaggle-avatars/images/3256.png)\n\n[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)\n\nCredit: [jhoward](https://www.kaggle.com/jhoward)\n\nGet started with NLP with fast.ai co-founder Jeremy Howard, and learn how to make a submission to the [U.S. Patent Phrase to Phrase Matching](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/) competition. After exploring this notebook, continue your learning with Jeremy's [Iterate like a grandmaster!](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster) notebook.\n\n![](https://storage.googleapis.com/kaggle-avatars/images/2779944-kg.jpg)\n\n[Deep Learning For NLP: Zero To Transformers & BERT](https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert)\n\nCredit: [tanulsingh077](https://www.kaggle.com/tanulsingh077)\n\nStart with the very basics of RNNs, and build all the way to latest deep learning architectures to solve NLP problems (such as transformers, BERT).\n\n![](https://storage.googleapis.com/kaggle-avatars/images/3505796-kg.jpeg)\n\n[NLP \ud83d\udcdd GloVe, BERT, TF-IDF, LSTM... \ud83d\udcdd Explained](https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained)\n\nCredit: [andreshg](https://www.kaggle.com/andreshg)\n\nLearn how to leverage NLP techniques for classifying text in this visual guide.\n\n![](https://storage.googleapis.com/kaggle-avatars/images/1424766-kg.png)\n\n[How to: Preprocessing when using embeddings](https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings)\n\nCredit: [christofhenkel](https://www.kaggle.com/christofhenkel)\n\nLearn how to come up with meaningful preprocessing when building deep learning NLP models.\n\n![Youtube Tutorials](https://storage.googleapis.com/kaggle-media/Images/dark_YouTube_Tutorials.svg)\n\nYouTube Tutorials\n\n![](https://yt3.ggpht.com/kXhp0q_y7W_KDSjaTruJDdAXhq7tuNgSrQjnJyMuonfOz8ksDene2X_RmIEfKSIe_ODEh9UIaw=s176-c-k-c0x00ffffff-no-rj)\n\n[Best NLP competitions on Kaggle (to learn from)](https://www.youtube.com/watch?v=-nH4OSyjwSI)\n\nChannel: [Abhishek Thakur](https://www.youtube.com/c/AbhishekThakurAbhi)\n\nAbhishek Thakur, Kaggle Grandmaster, suggests Kaggle competitions that you can use to learn NLP.\n\n![](https://yt3.ggpht.com/ytc/AKedOLQdGadAW1NnDkNEjfOhJLMPu02nWuHRlF6gCjOR=s176-c-k-c0x00ffffff-no-rj)\n\n[Kaggle Live-Coding: Scoping & Starting an NLP Project \\| Kaggle](https://www.youtube.com/watch?v=Jn8c3oe_GWU)\n\nChannel: [Kaggle](https://www.youtube.com/c/kaggle)\n\nRachael Tatman, Kaggle Grandmaster, walks you through how to structure an example NLP project. You'll learn how to figure out what questions to answer, possible approaches, how to measure success and get started finding data.\n\n![Project Ideas](https://storage.googleapis.com/kaggle-media/Images/dark_Project_Ideas.svg)\n\nProject Ideas\n\n- Make a submission to the [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started) Getting Started competition.\nThis competition is perfect for getting started with NLP. The goal is to write a model that can determine whether a tweet is about a disaster or not. For instance, \"13,000 people receive #wildfires evacuation orders in California\" is about a disaster, whereas \"Had an awesome time yesterday\" is not. There are a lot of notebooks that Kagglers have written to help others make their very first submission, and you can [access them here](https://www.kaggle.com/competitions/nlp-getting-started/code?competitionId=17777&sortBy=voteCount). We especially recommend:\n  - [Knowledge Graph & NLP Tutorial-(BERT,spaCy,NLTK)](https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk) by [pavansanagapati](https://www.kaggle.com/pavansanagapati)\n  - [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert) by [gunesevitan](https://www.kaggle.com/gunesevitan)\n- Participate in any of the [Natural Language Processing competitions](https://www.kaggle.com/competitions?tagIds=13204) on Kaggle.\nOnce you're familiar with the basics of NLP, you're encouraged to explore some more advanced competitions. If you participate in an active competition, you'll be awarded a spot on a live leaderboard. You can also learn a lot from closed competitions by perusing other Kagglers' solutions to the problem.\n\n![Discussions](https://www.kaggle.com/static/images/discussion/landing/kaggle-forum_dark.svg)\n\nDiscussions\n\n- [Best of Kaggle Notebooks #4. - Natural Language Processing](https://www.kaggle.com/discussions/getting-started/279372)\n- [\\[Reading List\\] - The NLP Reading list you should have!](https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating/discussion/286334)\n- [SOTA Papers on NLP and other techniques](https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/237226)\n\n![General](https://www.kaggle.com/static/images/discussion/landing/product-feedback_dark.svg)\n\nFeedback\n\nTo report an issue or suggest a resource that should be added to this page, [fill out this form](https://docs.google.com/forms/d/1XEO2i6d1uR6yooalWWdWRnxXmoSMWz_V_TFv0StWTyI).",
      "url": "https://www.kaggle.com/learn-guide/natural-language-processing"
    },
    {
      "title": "Full Preprocessing Tutorial | Kaggle",
      "text": "Full Preprocessing Tutorial | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=bd8bb52848e4b393:1:10007)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/gzuidhof/full-preprocessing-tutorial"
    },
    {
      "title": "Toxic Comment Classification",
      "text": "<div><div>\n\t<p>This post presents our solution for <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\">Toxic Comment Classification Challenge</a> hosted on Kaggle by Zigsaw. This solution ranked 15th on the private leaderboard. The code can be found in <a href=\"https://github.com/imrahulr/Toxic-Comment-Classification-Kaggle\">this</a> GitHub repository.</p>\n<hr/>\n<p>Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.</p>\n<p>So, in this competition on Kaggle, the challenge was to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective\u2019s current models. A dataset of comments from Wikipedia\u2019s talk page edits was provided.</p>\n<hr/>\n<h2>Data Overview</h2>\n<p>The dataset used was Wikipedia corpus dataset which was rated by human raters for toxicity. The corpus contains comments from discussions relating to user pages and articles dating from 2004-2015.</p>\n<p>The comments are to be tagged in the following six categories -</p>\n<ul>\n <li>toxic</li>\n <li>severe_toxic</li>\n <li>obscene</li>\n <li>threat</li>\n <li>insult</li>\n <li>identity_hate</li>\n</ul>\n<hr/>\n<h3>Train and Test Data</h3>\n<p>The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we have to predict.</p>\n<div><pre><code><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'train.csv'</span><span>)</span>\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'test.csv'</span><span>)</span>\n<span>print</span><span>(</span><span>'Train shape: '</span><span>,</span> <span>train_df</span><span>.</span><span>shape</span><span>)</span>\n<span>print</span><span>(</span><span>'Test shape: '</span><span>,</span> <span>test_df</span><span>.</span><span>shape</span><span>)</span> \n</code></pre></div>\n<div><pre><code>Train shape: (159571, 8)\nTest shape: (153164, 2)\n</code></pre></div>\n<p>The training dataset is highly imbalanced with about 1,43,346 clean comments and only 16,225 comments with toxicity as shown below - .</p>\n<p>\n</p>\n<hr/>\n<h2>Train Data after Basic Preprocessing and Cleaning</h2>\n<table>\n <thead>\n <tr>\n <th>ID</th>\n <th>Comment Text</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td>0</td>\n <td>explanation why the edits made under my userna\u2026</td>\n </tr>\n <tr>\n <td>1</td>\n <td>d aww ! he matches this background colour i am\u2026</td>\n </tr>\n <tr>\n <td>2</td>\n <td>hey man i am really not trying to edit war it \u2026</td>\n </tr>\n <tr>\n <td>3</td>\n <td>more i cannot make any real suggestions on im\u2026</td>\n </tr>\n <tr>\n <td>4</td>\n <td>you sir are my hero any chance you remember wh\u2026</td>\n </tr>\n </tbody>\n</table>\n<hr/>\n<h2>Test Data after Basic Preprocessing and Cleaning</h2>\n<table>\n <thead>\n <tr>\n <th>ID</th>\n <th>Comment Text</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td>0</td>\n <td>yo bitch ja rule is more succesful then you wi\u2026</td>\n </tr>\n <tr>\n <td>1</td>\n <td>= = from rfc = = the title is fine as it is imo</td>\n </tr>\n <tr>\n <td>2</td>\n <td>= = sources = = zawe ashton on lapland</td>\n </tr>\n <tr>\n <td>3</td>\n <td>if you have a look back at the source the inf\u2026</td>\n </tr>\n <tr>\n <td>4</td>\n <td>i do not anonymously edit articles at all</td>\n </tr>\n </tbody>\n</table>\n<hr/>\n<h2>Cleaning Data</h2>\n<p>Pre-processing includes removing special symbols and punctuations from comments, converting to upper-case characters to lower case. The preprocessed comments were converted to fixed-length sequences by either truncating or padding with zeros. We can also perform stemming and lemmatization on the comments, but they are not effective when using deep learning architectures.</p>\n<div><pre><code><span>def</span> <span>cleanData</span><span>(</span><span>text</span><span>,</span> <span>stemming</span><span>=</span><span>False</span><span>,</span> <span>lemmatize</span><span>=</span><span>False</span><span>):</span> \n <span>text</span> <span>=</span> <span>text</span><span>.</span><span>lower</span><span>().</span><span>split</span><span>()</span>\n <span>text</span> <span>=</span> <span>\" \"</span><span>.</span><span>join</span><span>(</span><span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"[^A-Za-z0-9^,!.\\/'+\\-=]\"</span><span>,</span> <span>\" \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"what's\"</span><span>,</span> <span>\"what is \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\'s\"</span><span>,</span> <span>\" \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\'ve\"</span><span>,</span> <span>\" have \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"can't\"</span><span>,</span> <span>\"cannot \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"n't\"</span><span>,</span> <span>\" not \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"i'm\"</span><span>,</span> <span>\"i am \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\'re\"</span><span>,</span> <span>\" are \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\'d\"</span><span>,</span> <span>\" would \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\'ll\"</span><span>,</span> <span>\" will \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\",\"</span><span>,</span> <span>\" \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\.\"</span><span>,</span> <span>\" \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"!\"</span><span>,</span> <span>\" ! \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\/\"</span><span>,</span> <span>\" \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\^\"</span><span>,</span> <span>\" ^ \"</span><span>,</span> <span>text</span><span>)</span>\n <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>\"\\+\"</span><span>,</span> <span>\" + \"</span><span>,</span> <span>text</span><span>)</span>\n <span...",
      "url": "https://imrahulr.github.io/nlp/kaggle/2018/01/07/toxic-comment-classification.html"
    }
  ]
}