{
  "query": "How do Kaggle winners combine text and tabular features for binary classification?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle competition winners typically fuse the raw text column with the remaining numeric\u202f/\u202fcategorical fields by either (a) turning the tabular data into embeddings that are concatenated with a BERT\u2011style text embedding, or (b) converting the tabular values into a short \u201cfeature\u2011text\u201d string that is prepended to the original document and then fine\u2011tuned with a transformer. AutoGluon\u2019s multimodal API automates both approaches, letting you train classical tabular models (LightGBM, CatBoost, RF) alongside a pretrained NLP model and blend their predictions\u202f([AutoGluon docs](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html))\u202fand a simple \u201cfeatures\u2011to\u2011text\u201d trick (prepend engineered text to the review before feeding it to BERT) has been shown to give the best single\u2011model scores on a Kaggle clothing\u2011review binary label task\u202f([McCormickML blog](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).\n\n**Step\u2011by\u2011step workflow used by top Kaggle solutions**\n\n1. **Load and split the data** \u2013 read the CSV(s), keep the target binary label, isolate the free\u2011form text column and all other numeric/categorical columns.  \n2. **Preprocess tabular fields** \u2013 fill missing values, standardize numeric features, and encode categoricals (e.g., target/ordinal encoding or one\u2011hot).  \n3. **Create a text representation**  \n   - *Option\u202fA (concatenated embeddings)*: Pass the raw text through a pretrained BERT/Transformer and extract the `[CLS]` (or pooled) embedding.  \n   - *Option\u202fB (feature\u2011text)*: Serialize the engineered tabular values into a short sentence (e.g., \u201cage\u202f=\u202f34, upvotes\u202f=\u202f12, category\u202f=\u202f\u2018shoes\u2019\u201d) and prepend it to the original review before feeding the whole string to BERT.  \n4. **Combine modalities**  \n   - For *Option\u202fA*, concatenate the BERT embedding with the processed numeric/categorical vectors, then train a downstream classifier (LightGBM, CatBoost, or a shallow neural net).  \n   - For *Option\u202fB*, the transformer receives the combined string and learns a joint representation; you can fine\u2011tune the whole model end\u2011to\u2011end.  \n5. **Train multimodal models with AutoGluon (optional)** \u2013 feed the original table (including the raw text column) to `TabularPredictor.fit()`. AutoGluon automatically builds a stack of classical tabular models and a `TextPredictor`\u2011based multimodal network, handling the concatenation internally\u202f([AutoGluon docs](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  \n6. **Ensemble / stack predictions** \u2013 blend the outputs of the text\u2011only, tabular\u2011only, and multimodal models (e.g., weighted average or a second\u2011level meta\u2011learner) to maximize binary\u2011classification metrics such as log\u2011loss.  \n7. **Evaluate and iterate** \u2013 use cross\u2011validation or a hold\u2011out set to monitor AUC/log\u2011loss, tune hyper\u2011parameters (learning rate, embedding size, regularization), and repeat steps\u202f3\u20115 until the leaderboard score stabilizes.  \n\nThese concise steps capture the core strategy that Kaggle winners employ to leverage both unstructured text and structured tabular information for high\u2011performing binary classification models.",
      "url": ""
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.7.0 documentation",
      "text": "<div><div>\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\">\u00b6</a></h2>\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\n<a href=\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\naddition to text and tabular features.</p>\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c<span>sec_textprediction_architecture</span>\u201d of\n<span>sec_textprediction_multimodal</span> (used by AutoGluon\u2019s\n<code><span>TextPredictor</span></code>).</p>\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\n<span>import</span> <span>pprint</span>\n<span>import</span> <span>random</span>\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n</pre></div>\n<div>\n<h2>Product Sentiment Analysis Dataset<a href=\"#product-sentiment-analysis-dataset\">\u00b6</a></h2>\n<p>We consider the product sentiment analysis dataset from a <a href=\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\">MachineHack\nhackathon</a>.\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).</p>\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n</pre></div>\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\n2023-02-22 23:29:24 (73.9 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\n2023-02-22 23:29:24 (57.6 MB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\n2023-02-22 23:29:24 (137 MB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n</pre></div>\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\n</pre></div>\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\n</pre></div>\n<p>There are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.</p>\n<div>\n<table>\n <thead>\n <tr>\n <th></th>\n <th>Product_Description</th>\n <th>Product_Type</th>\n <th>Sentiment</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>4532</th>\n <td>they took away the lego pit but ...",
      "url": "https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "",
      "text": "# Combining Categorical and Numerical Features with Text in BERT\n\n29 Jun 2021\n\nIn this tutorial we\u2019ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\n\nTo help motivate our discussion, we\u2019ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\n\n- The age of the reviewer (numerical feature)\n- The number of upvotes on the review (numerical feature)\n- The department and category of the clothing item (categorical features)\n\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\n\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\n\nIn Section 2 of this Notebook, I\u2019ve implemented four different \u201cbaseline\u201d strategies which score fairly well, but which don\u2019t incorporate all of the features together.\n\nThen, in Section 3, I\u2019ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\n\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\n\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\n\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\n\nBy Chris McCormick\n\n# Contents\n\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\n\n# S1. Clothing Review Dataset\n\n## 1.1. Download & Parse\n\nRetrieve the .csv file for the dataset.\n\n```\nimport gdown\n\nprint('Downloading dataset...\\n')\n\n# Download the file.\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\n                'Womens Clothing E-Commerce Reviews.csv',\n                quiet=False)\n\nprint('\\n\\nDONE.')\n```\n\n```\nDownloading dataset...\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\nTo: /content/Womens Clothing E-Commerce Reviews.csv\n8.48MB [00:00, 48.7MB/s]\n\nDONE.\n\n```\n\nParse the dataset csv file into a pandas DataFrame.\n\n```\nimport pandas as pd\n\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n\ndata_df.head()\n```\n\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\n\n_Features_\n\n\u201c **Recommended IND**\u201d is the label we are trying to predict for this dataset. \u201c1\u201d means the reviewer recommended the product and \u201c0\u201d means they do not.\n\nThe following are _categorical_ features:\n\n- Division Name\n- Department Name\n- Class Name\n- Clothing ID\n\nAnd the following are _numerical_ features:\n\n- Age\n- Rating\n- Positive Feedback Count\n\n_Feature Analysis_\n\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\n\nNote that, in addition to the \u201cRecommended\u201d label, there is also a \u201c **Rating**\u201d column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We\u2019ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\n\n## 1.2. Train-Validation-Test Split\n\nI want to use the same training, validation, and test splits for all of the approaches we try so that it\u2019s a fair comparison.\n\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\n\nTo solve this, we\u2019re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\n\n```\nimport random\nimport numpy as np\n\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\ntrain_size = int(0.8 * len(data_df))\nval_size = int(0.1 * len(data_df))\ntest_size = len(data_df) - (train_size + val_size)\n\n# Sanity check the sizes.\nassert((train_size + val_size + test_size) == len(data_df))\n\n# Create a list of indeces for all of the samples in the dataset.\nindeces = np.arange(0, len(data_df))\n\n# Shuffle the indeces randomly.\nrandom.shuffle(indeces)\n\n# Get a list of indeces for each of the splits.\ntrain_idx = indeces[0:train_size]\nval_idx = indeces[train_size:(train_size + val_size)]\ntest_idx = indeces[(train_size + val_size):]\n\n# Sanity check\nassert(len(train_idx) == train_size)\nassert(len(test_idx) == test_size)\n\n# With these lists, we can now select the corresponding dataframe rows using,\n# e.g., train_df = data_df.iloc[train_idx]\n\nprint('  Training size: {:,}'.format(train_size))\n...",
      "url": "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert"
    },
    {
      "title": "Classifying Tabular Data - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=1400df2c7b4dbdb7bad0:1:11426)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/redwankarimsony/classifying-tabular-data"
    },
    {
      "title": "Mastering Tabular Data Classification - Kaggle",
      "text": "Mastering Tabular Data Classification | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=cb954ee3291e68a59a5f:1:11061)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/eraikako/mastering-tabular-data-classification"
    },
    {
      "title": "Classification in Machine Learning: A Guide for Beginners - DataCamp",
      "text": "Classification in Machine Learning: A Guide for Beginners | DataCamp\n[Skip to main content](#main)\n# Classification in Machine Learning: An Introduction\nLearn about classification in machine learning, looking at what it is, how it's used, and some examples of classification algorithms.\nListContents\nUpdatedAug 8, 2024\u00b714 minread\nContents\n## GroupTraining more people?\nGet your team access to the full DataCamp for business platform.\nNowadays, many industries have been dealing with very large data sets of different types. Manually processing all that information can be time-consuming and might not even add value in the long term. Many strategies, from simple automation to machine learning techniques, are being applied for a better return on investment. This conceptual blog will cover one of the most important concepts; classification in machine learning.\nWe will start by defining what classification is in[Machine Learning](https://www.datacamp.com/blog/what-is-machine-learning)before clarifying the two types of learners in machine learning and the difference between classification and regression. Then, we will cover some real-world scenarios where classification can be used. After that, we will introduce all the different types of classification and deep dive into some examples of classification algorithms. Finally, we will provide hands-on practice on the implementation of a few algorithms.\n## Become a ML Scientist\nMaster Python skills to become a machine learning scientist\n[Start Learning for Free](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)\n## What is Classification in Machine Learning?\nClassification is a supervised machine learning method where the model tries to predict the correct label of a given input data. In classification, the model is fully trained using the training data, and then it is evaluated on test data before being used to perform prediction on new unseen data.\nFor instance, an algorithm can learn to predict whether a given email is spam or ham (no spam), as illustrated below.\n![Machine learning classification illustration for the email](https://images.datacamp.com/image/upload/v1663850410/Machine_learning_classification_illustration_for_the_email_a993b8df37.png)Before diving into the classification concept, we will first understand the difference between the two types of learners in classification: lazy and eager learners. Then we will clarify the misconception between classification and regression.\n#### Lazy Learners Vs. Eager Learners\nThere are two types of learners in machine learning classification: lazy and eager learners.\n**Eager learners**are machine learning algorithms that first build a model from the training dataset before making any prediction on future datasets. They spend more time during the training process because of their eagerness to have a better generalization during the training from learning the weights, but they require less time to make predictions.\nMost machine learning algorithms are eager learners, and below are some examples:\n* Logistic Regression.\n* Support Vector Machine.\n* Decision Trees.\n* Artificial Neural Networks.\n**Lazy learners or instance-based learners**, on the other hand, do not create any model immediately from the training data, and this is where the lazy aspect comes from. They just memorize the training data, and each time there is a need to make a prediction, they search for the nearest neighbor from the whole training data, which makes them very slow during prediction. Some examples of this kind are:\n* K-Nearest Neighbor.\n* Case-based reasoning.\nHowever, some algorithms, such as[BallTrees](https://en.wikipedia.org/wiki/Ball_tree)and[KDTrees](https://en.wikipedia.org/wiki/K-d_tree), can be used to improve the prediction latency.\n#### Machine Learning Classification Vs. Regression\nThere are four main categories of Machine Learning algorithms: supervised, unsupervised, semi-supervised, and reinforcement learning.\nEven though classification and regression are both from the category of supervised learning, they are not the same.\n* The prediction task is a***classification***when the target variable is discrete. An application is the identification of the underlying sentiment of a piece of text.\n* The prediction task is a***regression***when the target variable is continuous. An example can be the prediction of the salary of a person given their education degree, previous work experience, geographical location, and level of seniority.\nIf you are interested in knowing more about classification, courses on[Supervised Learning with scikit-learn](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn) and[Supervised Learning in R might be helpful](https://app.datacamp.com/learn/courses/supervised-learning-in-r-classification). They provide you with a better understanding of how each algorithm approaches tasks and the Python and R functions required to implement them.\nRegarding regression,[Introduction to Regression in R](https://app.datacamp.com/learn/courses/introduction-to-regression-in-r)and[Introduction to Regression with statsmodels in Python](https://app.datacamp.com/learn/courses/introduction-to-regression-with-statsmodels-in-python)will help you explore different types of regression models as well as their implementation in R and Python.\n## ![Difference between Classification &amp; Regression](https://images.datacamp.com/image/upload/v1663850409/Difference_between_Classification_and_Regression_98d8677bfd.png)Examples of Machine Learning Classification in Real Life\nSupervised Machine Learning Classification has different applications in multiple domains of our day-to-day life. Below are some examples.\n#### Healthcare\nTraining a machine learning model on historical patient data can help healthcare specialists accurately analyze their diagnoses:\n* During the COVID-19 pandemic, machine learning models were implemented to efficiently predict whether a person had COVID-19 or not.\n* Researchers can use machine learning models to predict new diseases that are more likely to emerge in the future.#### Education\nEducation is one of the domains dealing with the most textual, video, and audio data. This unstructured information can be analyzed with the help of Natural Language technologies to perform different tasks such as:\n* The classification of documents per category.\n* Automatic identification of the underlying language of students' documents during their application.\n* Analysis of students\u2019 feedback sentiments about a Professor.#### Transportation\nTransportation is the key component of many countries' economic development. As a result, industries are using machine and deep learning models:\n* To predict which geographical location will have a rise in traffic volume.\n* Predict potential issues that may occur in specific locations due to weather conditions.#### Sustainable agriculture\nAgriculture is one of the most valuable pillars of human survival. Introducing sustainability can help improve farmers' productivity at a different level without damaging the environment:\n* By using classification models to predict which type of land is suitable for a given type of seed.\n* Predict the weather to help them take proper preventive measures.## Different Types of Classification Tasks in Machine Learning\nThere are four main classification tasks in Machine learning: binary, multi-class, multi-label, and imbalanced classifications.\n### Binary Classification\nIn a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled. For instance, we might want to detect whether a given image is a truck or a boat.\n![Binary classification task](https://images.datacamp.com/image/upload/v1663850409/Binary_classification_task_ec3247c7d2.png)\nLogistic Regre...",
      "url": "https://www.datacamp.com/blog/classification-machine-learning"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle ...",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "What is Text Classification? - Elastic",
      "text": "What is Text Classification? | A Comprehensive Text Classification Guide | Elastic\n[Skip to main content](#main-content)\n# What is text classification?\n[Explore text classification with Elasticsearch](https://elastic.co/elasticsearch)\n### Text classification definition\n**Text classification is a type of**[**machine learning**](https://elastic.co/what-is/machine-learning)**that categorizes text documents or sentences into predefined classes or categories.**It analyzes the content and meaning of the text and then uses text labeling to assign it the most appropriate label.\nReal-world applications of text classification include[sentiment analysis](https://elastic.co/what-is/sentiment-analysis)(which determines positive or negative sentiments in reviews), spam detection (like spotting junk emails), and topic categorization (like organizing news articles into relevant topics). Text classification plays a major role in[natural language processing (NLP)](https://elastic.co/what-is/natural-language-processing)by enabling computers to understand and organize large amounts of unstructured text. This simplifies tasks such as content filtering, recommendation systems, and customer feedback analysis.\n### Types of text classification\n![Classification process](https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt1561f41dc119cbe2/651333a62222f153c3121f0c/illustration-classification-vis-graph-dots-triangles.jpg)\nTypes of text classification you might encounter include:\n[Text sentiment analysis](https://elastic.co/what-is/sentiment-analysis)determines the sentiment or emotion expressed in a piece of text, usually categorizing it as positive, negative, or neutral. It\u2019s used to analyze product reviews, social media posts, and customer feedback.\n**Toxicity detection**, related to text sentiment analysis, identifies offensive or harmful language online. It helps moderators of online communities maintain a respectful digital environment in online discussions, comments, or social media posts.\n**Intent recognition**is another subset of text sentiment analysis used to understand the purpose (or intention) behind a user&#x27;s text input. Chatbots and virtual assistants often use intent recognition to respond to user queries.\n**Binary classification**categorizes text into one of two classes or categories. A common example is**spam detection**, which classifies text, such as emails or messages, into either spam or legitimate categories to automatically filter out unsolicited and potentially harmful content.\n**Multiclass classification**categorizes text into three or more distinct classes or categories. This makes it easier to organize and retrieve information from content such as news articles, blog posts, or research papers.\n**Topic categorization**, related to multiclass classification, groups documents or articles into predefined topics or themes. For example, news articles can be categorized into topics like politics, sports, and entertainment.\n**Language identification**determines the language in which a piece of text is written. This is useful in multilingual contexts and language-based applications.\n**Named entity recognition**focuses on identifying and classifying named entities within a text, such as names of people, organizations, locations, and dates.\n**Question classification**deals with categorizing questions based on the expected answer type, which can be useful for search engines and question-answering systems.\n### Text classification process\nThe text classification process involves several steps, from data collection to model deployment. Here is a quick overview of how it works:\n**Step 1: Data collection**\nCollect a set of text documents with their corresponding categories for the text labeling process.\n**Step 2: Data preprocessing**\nClean and prepare the text data by removing unnecessary symbols, converting to lowercase, and handling special characters such as punctuation.\n**Step 3: Tokenization**\nBreak the text apart into tokens, which are small units like words. Tokens help find matches and connections by creating individually searchable parts. This step is especially useful for[vector search](https://elastic.co/what-is/vector-search)and[semantic search](https://elastic.co/what-is/semantic-search), which give results based on user intent.\n**Step 4: Feature extraction**\nConvert the text into numerical representations that machine learning models can understand. Some common methods include counting the occurrences of words (also known as Bag-of-Words) or using[word embeddings](https://elastic.co/what-is/word-embedding)to capture word meanings.\n**Step 5: Model training**\nNow that the data is clean and preprocessed, you can use it to train a machine learning model. The model will learn patterns and associations between the text\u2019s features and their categories. This helps it understand the text labeling conventions using the pre-labeled examples.\n**Step 6: Text labeling**\nCreate a new, separate dataset to start text labeling and classifying new text. In the text labeling process, the model separates the text into the predetermined categories from the data collection step.\n**Step 7: Model evaluation**\nTake a close look at the trained model&#x27;s performance in the text labeling process to see how well it can classify the unseen text.\n**Step 8: Hyperparameter tuning**\nDepending on how the model evaluation goes, you may want to adjust the model&#x27;s settings to optimize its performance.\n**Step 9: Model deployment**\nUse the trained and tuned model to classify new text data into their appropriate categories.\n### Why is text classification important?\nText classification is important because it enables computers to automatically categorize and understand large volumes of text data. In our digital world, we encounter massive amounts of textual information all the time. Think emails, social media, reviews, and more. Text classification allows machines to organize this[unstructured data](https://elastic.co/what-is/unstructured-data)into meaningful groups using text labeling. By making sense of impenetrable content, text classification improves efficiency, makes decision-making easier, and enhances the user experience.\n### Text classification use cases\nText classification use cases span a variety of professional environments. Here are some real-world use cases you may encounter:\n* Automating and categorizing customer support tickets, prioritizing them, and routing them to the right teams for resolution.\n* Analyzing customer feedback, survey responses, and online discussions to spot market trends and consumer preferences.\n* Tracking social media mentions and online reviews to monitor your brand\u2019s reputation and sentiment.\n* Organizing and tagging content on websites and e-commerce platforms using text labeling or tags to make it easier to discover content, which improves your customers\u2019 user experiences.\n* Identifying potential sales leads from social media and other online sources based on specific keywords and criteria.\n* Analyzing your competitor&#x27;s reviews and feedback to get insights into their strengths and weaknesses.\n* Segmenting your customers based on their interactions and feedback using text labeling to tailor marketing strategies and campaigns to them.\n* Detecting fraudulent activities and transactions in your financial systems based on text labeling patterns and anomalies (also known as[anomaly detection](https://elastic.co/what-is/anomaly-detection)).\n### Techniques and algorithms for text classification\nHere are some techniques and algorithms used for text classification:\n* **Bag-of-Words (BoW)**is a simple technique that counts word occurrences without considering their order.\n* [Word embeddings](https://elastic.co/what-is/word-embedding)utilize various techniques that convert words into numerical representations plotted in a multidimensional space, thus capturing the complex relationships between the words.\n* **Decision trees**are machine ...",
      "url": "https://www.elastic.co/what-is/text-classification"
    },
    {
      "title": "Text Classification Techniques - Explained - Kaggle",
      "text": "Text Classification Techniques - Explained | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a6aef772a946d7d4:1:10006)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/eraikako/text-classification-techniques-explained"
    }
  ]
}