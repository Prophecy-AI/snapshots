{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d3805b",
   "metadata": {},
   "source": [
    "# Linguistic Feature Engineering Experiment\n",
    "\n",
    "Based on Stanford ICWSM 2014 paper findings, this experiment extracts specific linguistic patterns that predict pizza request success:\n",
    "- Need-based narratives (family hardship, job loss, financial strain, student status, medical issues)\n",
    "- Gratitude expressions (thank you, thanks, appreciate, grateful)\n",
    "- Evidential language (concrete details, numbers, dates, specific situations)\n",
    "- Reciprocity promises (pay it forward, help others, contribute back)\n",
    "- Status signals (Reddit karma, account age references)\n",
    "- Sentiment analysis using VADER\n",
    "- Readability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec551db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:14.896555Z",
     "iopub.status.busy": "2026-01-10T10:05:14.895430Z",
     "iopub.status.idle": "2026-01-10T10:05:15.039652Z",
     "shell.execute_reply": "2026-01-10T10:05:15.038985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training samples: 2878\n",
      "Test samples: 1162\n",
      "Positive class rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade, smog_index\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Positive class rate: {df_train['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82f1103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:15.042120Z",
     "iopub.status.busy": "2026-01-10T10:05:15.041874Z",
     "iopub.status.idle": "2026-01-10T10:05:15.059320Z",
     "shell.execute_reply": "2026-01-10T10:05:15.058588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text combined and lowercased for pattern matching\n"
     ]
    }
   ],
   "source": [
    "# Combine text features\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "df_test['combined_text'] = df_test['request_title'].fillna('') + ' ' + df_test['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Convert to lowercase for pattern matching\n",
    "train_text_lower = df_train['combined_text'].str.lower()\n",
    "test_text_lower = df_test['combined_text'].str.lower()\n",
    "\n",
    "print(\"Text combined and lowercased for pattern matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c95e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:15.062094Z",
     "iopub.status.busy": "2026-01-10T10:05:15.061353Z",
     "iopub.status.idle": "2026-01-10T10:05:15.078263Z",
     "shell.execute_reply": "2026-01-10T10:05:15.077407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern dictionaries defined\n"
     ]
    }
   ],
   "source": [
    "# Define linguistic pattern dictionaries based on academic research\n",
    "\n",
    "# Need narratives - family hardship, job loss, financial strain, student status, medical issues\n",
    "need_patterns = {\n",
    "    'family_hardship': r'\\b(family|child|children|kid|kids|baby|babies|mother|father|parent|parents|wife|husband|brother|sister)\\b',\n",
    "    'job_loss': r'\\b(lost job|unemployed|laid off|fired|no job|out of work|jobless)\\b',\n",
    "    'financial_strain': r'\\b(broke|poor|bills|rent|mortgage|utilities|electric|gas|water|heat|heating|money|cash|paycheck|pay check|pay day|payday)\\b',\n",
    "    'student_status': r'\\b(student|college|university|school|class|tuition|loan|loans|textbook|textbooks)\\b',\n",
    "    'medical_issues': r'\\b(hospital|doctor|sick|ill|injury|injured|medicine|medical|health|pain|surgery)\\b',\n",
    "    'food_insecurity': r'\\b(hungry|starving|no food|empty fridge|empty stomach|haven.t eaten|haven.t ate)\\b'\n",
    "}\n",
    "\n",
    "# Gratitude expressions\n",
    "gratitude_patterns = {\n",
    "    'thanks_words': r'\\b(thank|thanks|thankful|grateful|appreciate|appreciation)\\b',\n",
    "    'thanks_phrases': r'\\b(thank you|thanks in advance|thanks so much|thank you so much|really appreciate|truly grateful)\\b'\n",
    "}\n",
    "\n",
    "# Reciprocity promises\n",
    "reciprocity_patterns = {\n",
    "    'pay_forward': r'\\b(pay it forward|pay forward|forward the kindness|forward the generosity)\\b',\n",
    "    'help_others': r'\\b(help others|help someone|help people|help another|contribute back|give back)\\b',\n",
    "    'future_reciprocity': r'\\b(when i get paid|when i get money|when i.m back on my feet|once i.m stable|return the favor)\\b'\n",
    "}\n",
    "\n",
    "# Status signals (Reddit-specific)\n",
    "status_patterns = {\n",
    "    'karma_mention': r'\\b(karma|upvotes|downvotes|reputation)\\b',\n",
    "    'account_mention': r'\\b(account|profile|user|reddit|member|joined)\\b',\n",
    "    'new_user': r'\\b(new here|new to reddit|new account|first post|first time)\\b'\n",
    "}\n",
    "\n",
    "# Evidential language - numbers, dates, specific details\n",
    "evidential_patterns = {\n",
    "    'numbers': r'\\b(\\d+|one|two|three|four|five|six|seven|eight|nine|ten|first|second|third)\\b',\n",
    "    'time_refs': r'\\b(days?|weeks?|months?|years?|today|tomorrow|yesterday|tonight|this morning|this afternoon)\\b',\n",
    "    'money_amounts': r'\\b(\\$\\d+|\\d+ dollars|\\d+ bucks|\\d+ cents)\\b',\n",
    "    'specific_details': r'\\b(specifically|exactly|precisely|particularly|especially|for example|for instance|like|such as)\\b'\n",
    "}\n",
    "\n",
    "print(\"Pattern dictionaries defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5117b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:15.080823Z",
     "iopub.status.busy": "2026-01-10T10:05:15.080380Z",
     "iopub.status.idle": "2026-01-10T10:05:15.944550Z",
     "shell.execute_reply": "2026-01-10T10:05:15.943746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting need narrative features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting gratitude features...\n",
      "Extracting reciprocity features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting status signal features...\n",
      "Extracting evidential language features...\n"
     ]
    }
   ],
   "source": [
    "# Function to count pattern matches\n",
    "def count_patterns(text_series, pattern_dict):\n",
    "    \"\"\"Count matches for each pattern in the dictionary\"\"\"\n",
    "    results = {}\n",
    "    for pattern_name, pattern_regex in pattern_dict.items():\n",
    "        # Use str.contains with regex=True instead of str.count\n",
    "        results[pattern_name] = text_series.str.contains(pattern_regex, regex=True).astype(int)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define all pattern dictionaries\n",
    "\n",
    "# Need narratives - family hardship, job loss, financial strain, student status, medical issues\n",
    "need_patterns = {\n",
    "    'family_hardship': r'\\b(family|child|children|kid|kids|baby|babies|mother|father|parent|parents|wife|husband|brother|sister)\\b',\n",
    "    'job_loss': r'\\b(lost job|unemployed|laid off|fired|no job|out of work|jobless)\\b',\n",
    "    'financial_strain': r'\\b(broke|poor|bills|rent|mortgage|utilities|electric|gas|water|heat|heating|money|cash|paycheck|pay check|pay day|payday)\\b',\n",
    "    'student_status': r'\\b(student|college|university|school|class|tuition|loan|loans|textbook|textbooks)\\b',\n",
    "    'medical_issues': r'\\b(hospital|doctor|sick|ill|injury|injured|medicine|medical|health|pain|surgery)\\b',\n",
    "    'food_insecurity': r'\\b(hungry|starving|no food|empty fridge|empty stomach|haven.t eaten|haven.t ate)\\b'\n",
    "}\n",
    "\n",
    "# Gratitude expressions\n",
    "gratitude_patterns = {\n",
    "    'thanks_words': r'\\b(thank|thanks|thankful|grateful|appreciate|appreciation)\\b',\n",
    "    'thanks_phrases': r'\\b(thank you|thanks so much|thank you so much|thanks a lot|thank you very much)\\b'\n",
    "}\n",
    "\n",
    "# Reciprocity promises\n",
    "reciprocity_patterns = {\n",
    "    'pay_forward': r'\\b(pay it forward|pay forward|forward the kindness|forward this kindness)\\b',\n",
    "    'help_others': r'\\b(help others|help someone else|help other people|give back|contribute back|return the favor)\\b',\n",
    "    'promise_future': r'\\b(will help|will pay|will contribute|will give back|when i can|once i get|promise to)\\b'\n",
    "}\n",
    "\n",
    "# Status signals (Reddit-specific)\n",
    "status_patterns = {\n",
    "    'karma_mentions': r'\\b(karma|upvote|downvote|points|score)\\b',\n",
    "    'account_mentions': r'\\b(account|new account|old account|long time|lurker|lurking)\\b',\n",
    "    'reddit_status': r'\\b(redditor|reddit user|member of reddit)\\b'\n",
    "}\n",
    "\n",
    "# Evidential language (concrete details)\n",
    "evidential_patterns = {\n",
    "    'numbers': r'\\b(\\d+|one|two|three|four|five|six|seven|eight|nine|ten|first|second|third)\\b',\n",
    "    'dates_times': r'\\b(january|february|march|april|may|june|july|august|september|october|november|december|monday|tuesday|wednesday|thursday|friday|saturday|sunday|today|tomorrow|yesterday|week|month|year)\\b',\n",
    "    'specifics': r'\\b(specific|specifically|exact|exactly|precise|precisely|detail|details|detailed)\\b'\n",
    "}\n",
    "\n",
    "# Extract all linguistic features\n",
    "print(\"Extracting need narrative features...\")\n",
    "train_need_features = count_patterns(train_text_lower, need_patterns)\n",
    "test_need_features = count_patterns(test_text_lower, need_patterns)\n",
    "\n",
    "print(\"Extracting gratitude features...\")\n",
    "train_gratitude_features = count_patterns(train_text_lower, gratitude_patterns)\n",
    "test_gratitude_features = count_patterns(test_text_lower, gratitude_patterns)\n",
    "\n",
    "print(\"Extracting reciprocity features...\")\n",
    "train_reciprocity_features = count_patterns(train_text_lower, reciprocity_patterns)\n",
    "test_reciprocity_features = count_patterns(test_text_lower, reciprocity_patterns)\n",
    "\n",
    "print(\"Extracting status signal features...\")\n",
    "train_status_features = count_patterns(train_text_lower, status_patterns)\n",
    "test_status_features = count_patterns(test_text_lower, status_patterns)\n",
    "\n",
    "print(\"Extracting evidential language features...\")\n",
    "train_evidential_features = count_patterns(train_text_lower, evidential_patterns)\n",
    "test_evidential_features = count_patterns(test_text_lower, evidential_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f044849a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:15.947334Z",
     "iopub.status.busy": "2026-01-10T10:05:15.946730Z",
     "iopub.status.idle": "2026-01-10T10:05:18.299839Z",
     "shell.execute_reply": "2026-01-10T10:05:18.299144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing VADER sentiment scores...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER sentiment features: ['neg', 'neu', 'pos', 'compound']\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis using VADER\n",
    "print(\"Computing VADER sentiment scores...\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_scores(text_series):\n",
    "    \"\"\"Extract VADER sentiment scores\"\"\"\n",
    "    sentiments = []\n",
    "    for text in text_series:\n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        sentiments.append(scores)\n",
    "    return pd.DataFrame(sentiments)\n",
    "\n",
    "train_sentiment = get_sentiment_scores(df_train['combined_text'])\n",
    "test_sentiment = get_sentiment_scores(df_test['combined_text'])\n",
    "\n",
    "print(f\"VADER sentiment features: {list(train_sentiment.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62644478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:18.302604Z",
     "iopub.status.busy": "2026-01-10T10:05:18.301894Z",
     "iopub.status.idle": "2026-01-10T10:05:21.739848Z",
     "shell.execute_reply": "2026-01-10T10:05:21.738956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing readability metrics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability features: ['flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index']\n"
     ]
    }
   ],
   "source": [
    "# Readability metrics\n",
    "print(\"Computing readability metrics...\")\n",
    "\n",
    "def compute_readability(text_series):\n",
    "    \"\"\"Compute readability metrics for each text\"\"\"\n",
    "    results = []\n",
    "    for text in text_series:\n",
    "        try:\n",
    "            flesch = flesch_reading_ease(text)\n",
    "            fk_grade = flesch_kincaid_grade(text)\n",
    "            smog = smog_index(text)\n",
    "        except:\n",
    "            # Handle errors for very short texts\n",
    "            flesch = 0\n",
    "            fk_grade = 0\n",
    "            smog = 0\n",
    "        results.append({\n",
    "            'flesch_reading_ease': flesch,\n",
    "            'flesch_kincaid_grade': fk_grade,\n",
    "            'smog_index': smog\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "train_readability = compute_readability(df_train['combined_text'])\n",
    "test_readability = compute_readability(df_test['combined_text'])\n",
    "\n",
    "print(f\"Readability features: {list(train_readability.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf8ca10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:21.743230Z",
     "iopub.status.busy": "2026-01-10T10:05:21.742383Z",
     "iopub.status.idle": "2026-01-10T10:05:21.819523Z",
     "shell.execute_reply": "2026-01-10T10:05:21.818725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all linguistic features...\n",
      "Total linguistic features: 24\n",
      "Training shape: (2878, 24)\n",
      "Test shape: (1162, 24)\n",
      "\n",
      "Top 10 features by absolute correlation:\n",
      "  numbers: 0.1108\n",
      "  dates_times: 0.0924\n",
      "  financial_strain: 0.0607\n",
      "  thanks_words: 0.0606\n",
      "  thanks_phrases: 0.0597\n",
      "  neu: 0.0544\n",
      "  compound: 0.0522\n",
      "  job_loss: 0.0516\n",
      "  student_status: 0.0496\n",
      "  pay_forward: 0.0485\n"
     ]
    }
   ],
   "source": [
    "# Combine all linguistic features\n",
    "print(\"Combining all linguistic features...\")\n",
    "\n",
    "train_linguistic_features = pd.concat([\n",
    "    train_need_features,\n",
    "    train_gratitude_features,\n",
    "    train_reciprocity_features,\n",
    "    train_status_features,\n",
    "    train_evidential_features,\n",
    "    train_sentiment,\n",
    "    train_readability\n",
    "], axis=1)\n",
    "\n",
    "test_linguistic_features = pd.concat([\n",
    "    test_need_features,\n",
    "    test_gratitude_features,\n",
    "    test_reciprocity_features,\n",
    "    test_status_features,\n",
    "    test_evidential_features,\n",
    "    test_sentiment,\n",
    "    test_readability\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Total linguistic features: {train_linguistic_features.shape[1]}\")\n",
    "print(f\"Training shape: {train_linguistic_features.shape}\")\n",
    "print(f\"Test shape: {test_linguistic_features.shape}\")\n",
    "\n",
    "# Check feature correlations with target\n",
    "correlations = []\n",
    "for col in train_linguistic_features.columns:\n",
    "    corr = np.corrcoef(train_linguistic_features[col], df_train['requester_received_pizza'])[0, 1]\n",
    "    correlations.append((col, abs(corr) if not np.isnan(corr) else 0))\n",
    "\n",
    "correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 features by absolute correlation:\")\n",
    "for feat, corr in correlations[:10]:\n",
    "    print(f\"  {feat}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006f2ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:21.822208Z",
     "iopub.status.busy": "2026-01-10T10:05:21.821874Z",
     "iopub.status.idle": "2026-01-10T10:05:21.843067Z",
     "shell.execute_reply": "2026-01-10T10:05:21.842127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing text and numeric features...\n",
      "Numeric features shape: (2878, 6)\n",
      "Linguistic features shape: (2878, 24)\n",
      "Target shape: (2878,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare text and numeric features for modeling\n",
    "print(\"Preparing text and numeric features...\")\n",
    "\n",
    "# Text features\n",
    "text_features = df_train['combined_text'].values\n",
    "test_text_features = df_test['combined_text'].values\n",
    "\n",
    "# Basic numeric features (from previous baseline)\n",
    "count_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feat in count_features:\n",
    "    df_train[f'{feat}_log'] = np.log1p(df_train[feat])\n",
    "    df_test[f'{feat}_log'] = np.log1p(df_test[feat])\n",
    "\n",
    "df_train['upvotes_per_comment'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_comments_at_request'] + 1)\n",
    "df_test['upvotes_per_comment'] = df_test['requester_upvotes_plus_downvotes_at_request'] / (df_test['requester_number_of_comments_at_request'] + 1)\n",
    "\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_test['comments_per_post'] = df_test['requester_number_of_comments_at_request'] / (df_test['requester_number_of_posts_at_request'] + 1)\n",
    "\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days_at_request'] / 365.25\n",
    "df_test['account_age_years'] = df_test['requester_account_age_in_days_at_request'] / 365.25\n",
    "\n",
    "numeric_features = [\n",
    "    'requester_number_of_comments_at_request_log',\n",
    "    'requester_number_of_posts_at_request_log',\n",
    "    'requester_upvotes_plus_downvotes_at_request_log',\n",
    "    'upvotes_per_comment',\n",
    "    'comments_per_post',\n",
    "    'account_age_years'\n",
    "]\n",
    "\n",
    "train_numeric = df_train[numeric_features].fillna(0).values\n",
    "test_numeric = df_test[numeric_features].fillna(0).values\n",
    "\n",
    "# Target\n",
    "y = df_train['requester_received_pizza'].values\n",
    "\n",
    "print(f\"Numeric features shape: {train_numeric.shape}\")\n",
    "print(f\"Linguistic features shape: {train_linguistic_features.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4e204ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:21.846246Z",
     "iopub.status.busy": "2026-01-10T10:05:21.845461Z",
     "iopub.status.idle": "2026-01-10T10:05:21.852371Z",
     "shell.execute_reply": "2026-01-10T10:05:21.851578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold CV with linguistic features...\n"
     ]
    }
   ],
   "source": [
    "# Stratified CV setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Logistic Regression with class weighting\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Store predictions\n",
    "oof_predictions = np.zeros(len(df_train))\n",
    "test_predictions = np.zeros(len(df_test))\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Starting 5-fold CV with linguistic features...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8970d0ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:21.855861Z",
     "iopub.status.busy": "2026-01-10T10:05:21.854952Z",
     "iopub.status.idle": "2026-01-10T10:05:35.588003Z",
     "shell.execute_reply": "2026-01-10T10:05:35.587136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.6104\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.6196\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.5913\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC: 0.6206\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 AUC: 0.6208\n",
      "\n",
      "Overall CV AUC: 0.6118\n",
      "CV scores: [0.6103942247129314, 0.6195997997383678, 0.591320919265492, 0.6206293706293705, 0.6208398083398083]\n",
      "Mean ± Std: 0.6126 ± 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation loop\n",
    "fold = 0\n",
    "for train_idx, val_idx in skf.split(df_train, y):\n",
    "    fold += 1\n",
    "    print(f\"\\nFold {fold}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_text, X_val_text = text_features[train_idx], text_features[val_idx]\n",
    "    X_train_num, X_val_num = train_numeric[train_idx], train_numeric[val_idx]\n",
    "    X_train_ling, X_val_ling = train_linguistic_features.iloc[train_idx].values, train_linguistic_features.iloc[val_idx].values\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Fit TF-IDF on training text\n",
    "    X_train_text_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    X_val_text_tfidf = tfidf.transform(X_val_text)\n",
    "    \n",
    "    # Combine all features\n",
    "    X_train_combined = hstack([X_train_text_tfidf, X_train_num, X_train_ling])\n",
    "    X_val_combined = hstack([X_val_text_tfidf, X_val_num, X_val_ling])\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train_combined, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict_proba(X_val_combined)[:, 1]\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate score\n",
    "    score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(score)\n",
    "    print(f\"Fold {fold} AUC: {score:.4f}\")\n",
    "    \n",
    "    # Predict on test for this fold\n",
    "    test_text_tfidf = tfidf.transform(test_text_features)\n",
    "    test_combined = hstack([test_text_tfidf, test_numeric, test_linguistic_features.values])\n",
    "    fold_test_pred = model.predict_proba(test_combined)[:, 1]\n",
    "    test_predictions += fold_test_pred\n",
    "\n",
    "# Average test predictions across folds\n",
    "test_predictions /= 5\n",
    "\n",
    "# Overall CV score\n",
    "overall_score = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV AUC: {overall_score:.4f}\")\n",
    "print(f\"CV scores: {cv_scores}\")\n",
    "print(f\"Mean \\u00b1 Std: {np.mean(cv_scores):.4f} \\u00b1 {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f84642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:35.591277Z",
     "iopub.status.busy": "2026-01-10T10:05:35.590356Z",
     "iopub.status.idle": "2026-01-10T10:05:39.243132Z",
     "shell.execute_reply": "2026-01-10T10:05:39.242190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on full data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained on 5030 features\n",
      "Final predictions shape: (1162,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Generate final predictions using full training data\n",
    "print(\"Training final model on full data...\")\n",
    "\n",
    "# Fit TF-IDF on all training data\n",
    "final_tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Transform all text\n",
    "train_text_tfidf = final_tfidf.fit_transform(text_features)\n",
    "test_text_tfidf = final_tfidf.transform(test_text_features)\n",
    "\n",
    "# Combine all features\n",
    "train_combined = hstack([train_text_tfidf, train_numeric, train_linguistic_features.values])\n",
    "test_combined = hstack([test_text_tfidf, test_numeric, test_linguistic_features.values])\n",
    "\n",
    "# Train final model\n",
    "final_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_model.fit(train_combined, y)\n",
    "\n",
    "# Generate final predictions\n",
    "final_predictions = final_model.predict_proba(test_combined)[:, 1]\n",
    "\n",
    "print(f\"Final model trained on {train_combined.shape[1]} features\")\n",
    "print(f\"Final predictions shape: {final_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3548c77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:05:39.246990Z",
     "iopub.status.busy": "2026-01-10T10:05:39.246012Z",
     "iopub.status.idle": "2026-01-10T10:05:39.269901Z",
     "shell.execute_reply": "2026-01-10T10:05:39.268566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /home/submission/submission_linguistic_features.csv\n",
      "Submission shape: (1162, 2)\n",
      "Prediction range: 0.0800 to 0.9226\n",
      "\n",
      "Submission preview:\n",
      "  request_id  requester_received_pizza\n",
      "0  t3_1aw5zf                  0.582336\n",
      "1   t3_roiuw                  0.448394\n",
      "2   t3_mjnbq                  0.448839\n",
      "3   t3_t8wd1                  0.469571\n",
      "4  t3_1m4zxu                  0.524425\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': df_test['request_id'],\n",
    "    'requester_received_pizza': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure proper format\n",
    "submission['requester_received_pizza'] = submission['requester_received_pizza'].astype(float)\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission_linguistic_features.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Prediction range: {submission['requester_received_pizza'].min():.4f} to {submission['requester_received_pizza'].max():.4f}\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
