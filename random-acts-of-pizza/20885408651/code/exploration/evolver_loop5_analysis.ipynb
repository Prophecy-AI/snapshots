{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e54c13e",
   "metadata": {},
   "source": [
    "# Evolver Loop 5 Analysis: Why LDA Topic Modeling Hurt Performance\n",
    "\n",
    "**Goal**: Analyze why exp_005 (LDA topics) scored 0.6505 (-0.0155 vs exp_004) and determine next steps.\n",
    "\n",
    "**Key questions**:\n",
    "1. Are LDA topics redundant with existing TF-IDF + SVD features?\n",
    "2. Do topic probabilities add noise rather than signal?\n",
    "3. Should we tune LDA hyperparameters or remove it entirely?\n",
    "4. What should be our next priority: Stanford linguistic cues or something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('/home/data/train.csv')\n",
    "df_test = pd.read_csv('/home/data/test.csv')\n",
    "y = df_train['requester_received_pizza']\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Positive rate: {y.mean():.3f}\")\n",
    "print(f\"Best CV so far: 0.6660 (exp_004)\")\n",
    "print(f\"LDA experiment CV: 0.6505 (exp_005)\")\n",
    "print(f\"Performance drop: -0.0155\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff810e",
   "metadata": {},
   "source": [
    "## 1. Analyze LDA Topic Quality and Redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exp_005 predictions to analyze\n",
    "import joblib\n",
    "\n",
    "# Try to load the experiment data\n",
    "exp_005_path = '/home/code/experiments/005_lda_topic_modeling'\n",
    "\n",
    "# Let's manually recreate the LDA pipeline to analyze topic quality\n",
    "# Use the same preprocessing as exp_005\n",
    "\n",
    "# Clean and combine text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "df_train['combined_text'] = df_train['request_title'].fillna('') + ' ' + df_train['request_text_edit_aware'].fillna('')\n",
    "df_train['combined_text_clean'] = df_train['combined_text'].apply(clean_text)\n",
    "\n",
    "# Create TF-IDF (same as exp_005)\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    min_df=3,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    max_features=3000,\n",
    "    ngram_range=(2, 4),\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit on full data for analysis (not for CV)\n",
    "word_features = word_tfidf.fit_transform(df_train['combined_text_clean'])\n",
    "char_features = char_tfidf.fit_transform(df_train['combined_text_clean'])\n",
    "\n",
    "print(f\"Word TF-IDF shape: {word_features.shape}\")\n",
    "print(f\"Char TF-IDF shape: {char_features.shape}\")\n",
    "\n",
    "# Apply SVD (same as exp_005)\n",
    "svd_word = TruncatedSVD(n_components=50, random_state=42)\n",
    "svd_char = TruncatedSVD(n_components=25, random_state=42)\n",
    "\n",
    "word_svd = svd_word.fit_transform(word_features)\n",
    "char_svd = svd_char.fit_transform(char_features)\n",
    "\n",
    "print(f\"Word SVD shape: {word_svd.shape}\")\n",
    "print(f\"Char SVD shape: {char_svd.shape}\")\n",
    "print(f\"Total SVD features: {word_svd.shape[1] + char_svd.shape[1]}\")\n",
    "\n",
    "# Apply LDA (same as exp_005)\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=15,\n",
    "    random_state=42,\n",
    "    learning_method='batch',\n",
    "    max_iter=20\n",
    ")\n",
    "\n",
    "# Fit LDA on word TF-IDF features (common practice)\n",
    "lda_topics = lda.fit_transform(word_features)\n",
    "print(f\"LDA topics shape: {lda_topics.shape}\")\n",
    "print(f\"LDA topic probabilities range: [{lda_topics.min():.4f}, {lda_topics.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topic coherence and interpretability\n",
    "print(\"=== LDA Topic Interpretation ===\")\n",
    "feature_names = word_tfidf.get_feature_names_out()\n",
    "\n",
    "for topic_idx in range(15):\n",
    "    top_features_idx = lda.components_[topic_idx].argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_features_idx]\n",
    "    print(f\"\\nTopic {topic_idx}: {', '.join(top_features)}\")\n",
    "\n",
    "# Analyze correlation between LDA topics and target\n",
    "topic_correlations = []\n",
    "for i in range(15):\n",
    "    corr = np.corrcoef(lda_topics[:, i], y)[0, 1]\n",
    "    topic_correlations.append(corr)\n",
    "\n",
    "print(\"\\n=== LDA Topic Correlations with Target ===\")\n",
    "for i, corr in enumerate(topic_correlations):\n",
    "    print(f\"Topic {i}: {corr:.4f}\")\n",
    "\n",
    "# Check if any topics have meaningful correlation\n",
    "strong_topics = [i for i, corr in enumerate(topic_correlations) if abs(corr) > 0.05]\n",
    "print(f\"\\nTopics with |corr| > 0.05: {strong_topics}\")\n",
    "print(f\"Max absolute correlation: {max(abs(c) for c in topic_correlations):.4f}\")\n",
    "\n",
    "# Visualize topic correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(15), topic_correlations)\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.title('LDA Topic Correlations with Target (Pizza Request Success)')\n",
    "plt.xlabel('Topic ID')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fd3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check redundancy between LDA topics and SVD components\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Calculate correlation matrix between LDA topics and SVD components\n",
    "svd_combined = np.hstack([word_svd, char_svd])\n",
    "print(f\"SVD combined shape: {svd_combined.shape}\")\n",
    "\n",
    "# Compute maximum correlation between each LDA topic and any SVD component\n",
    "max_correlations = []\n",
    "for i in range(15):\n",
    "    correlations = []\n",
    "    for j in range(svd_combined.shape[1]):\n",
    "        corr = np.corrcoef(lda_topics[:, i], svd_combined[:, j])[0, 1]\n",
    "        correlations.append(abs(corr))\n",
    "    max_correlations.append(max(correlations))\n",
    "\n",
    "print(\"\\n=== LDA Topic Redundancy with SVD Components ===\")\n",
    "for i, max_corr in enumerate(max_correlations):\n",
    "    print(f\"Topic {i}: max |corr| with SVD = {max_corr:.4f}\")\n",
    "\n",
    "highly_redundant = [i for i, c in enumerate(max_correlations) if c > 0.7]\n",
    "print(f\"\\nHighly redundant topics (|corr| > 0.7): {highly_redundant}\")\n",
    "print(f\"Average max correlation: {np.mean(max_correlations):.4f}\")\n",
    "\n",
    "# Check if LDA adds unique information beyond SVD\n",
    "# Fit LightGBM with and without LDA topics\n",
    "\n",
    "# Prepare numeric features (same as exp_005)\n",
    "numeric_features = [\n",
    "    'requester_account_age_in_days',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_comments_at_request'\n",
    "]\n",
    "\n",
    "# Create log transforms\n",
    "for col in numeric_features:\n",
    "    df_train[f'{col}_log'] = np.log1p(df_train[col])\n",
    "\n",
    "df_train['text_length'] = df_train['combined_text'].str.len()\n",
    "df_train['word_count'] = df_train['combined_text'].str.split().str.len()\n",
    "df_train['upvotes_per_post'] = df_train['requester_upvotes_plus_downvotes_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_train['comments_per_post'] = df_train['requester_number_of_comments_at_request'] / (df_train['requester_number_of_posts_at_request'] + 1)\n",
    "df_train['account_age_years'] = df_train['requester_account_age_in_days'] / 365.25\n",
    "\n",
    "numeric_features_final = [\n",
    "    'requester_account_age_in_days_log',\n",
    "    'requester_upvotes_plus_downvotes_at_request_log',\n",
    "    'requester_number_of_posts_at_request_log',\n",
    "    'requester_number_of_comments_at_request_log',\n",
    "    'text_length',\n",
    "    'word_count',\n",
    "    'upvotes_per_post',\n",
    "    'comments_per_post',\n",
    "    'account_age_years'\n",
    "]\n",
    "\n",
    "X_numeric = df_train[numeric_features_final].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Combine features\n",
    "X_svd_only = np.hstack([word_svd, char_svd, X_numeric_scaled])\n",
    "X_with_lda = np.hstack([word_svd, char_svd, lda_topics, X_numeric_scaled])\n",
    "\n",
    "print(f\"\\n=== Feature Set Comparison ===\")\n",
    "print(f\"SVD only: {X_svd_only.shape[1]} features\")\n",
    "print(f\"SVD + LDA: {X_with_lda.shape[1]} features\")\n",
    "print(f\"LDA adds: {X_with_lda.shape[1] - X_svd_only.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick CV comparison to verify the performance drop\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_features(X, y, model_params=None):\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'n_estimators': 500,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 64,\n",
    "            'max_depth': 7,\n",
    "            'min_child_samples': 50,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        pred = model.predict_proba(X_val)[:, 1]\n",
    "        score = roc_auc_score(y_val, pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "print(\"=== Quick CV Comparison ===\")\n",
    "print(\"Note: This uses full-data fitted features (leakage), so scores will be inflated\")\n",
    "print(\"But it helps understand relative performance...\")\n",
    "\n",
    "score_svd_only, std_svd_only = evaluate_features(X_svd_only, y)\n",
    "score_with_lda, std_with_lda = evaluate_features(X_with_lda, y)\n",
    "\n",
    "print(f\"SVD only: {score_svd_only:.4f} ± {std_svd_only:.4f}\")\n",
    "print(f\"SVD + LDA: {score_with_lda:.4f} ± {std_with_lda:.4f}\")\n",
    "print(f\"Difference: {score_with_lda - score_svd_only:.4f}\")\n",
    "\n",
    "if score_with_lda < score_svd_only:\n",
    "    print(\"\\n❌ LDA topics hurt performance (consistent with exp_005)\")\n",
    "else:\n",
    "    print(\"\\n✓ LDA topics help performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde276bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze why LDA might be hurting\n",
    "print(\"=== Analysis: Why LDA Hurt Performance ===\\n\")\n",
    "\n",
    "print(\"1. REDUNDANCY WITH SVD:\")\n",
    "print(f\"   - Average max correlation between LDA topics and SVD: {np.mean(max_correlations):.4f}\")\n",
    "print(f\"   - {len(highly_redundant)} topics have |corr| > 0.7 with SVD components\")\n",
    "print(f\"   - LDA and SVD both capture latent structure, but SVD is already optimized\")\n",
    "\n",
    "print(\"\\n2. TOPIC QUALITY:\")\n",
    "print(f\"   - Max topic correlation with target: {max(abs(c) for c in topic_correlations):.4f}\")\n",
    "print(f\"   - Topics with |corr| > 0.05: {len(strong_topics)}\")\n",
    "if len(strong_topics) == 0:\n",
    "    print(\"   - ⚠️ NO topics have meaningful correlation with target!\")\n",
    "    print(\"   - LDA is capturing themes, but they don't predict pizza success\")\n",
    "\n",
    "print(\"\\n3. NOISE ADDITION:\")\n",
    "print(f\"   - LDA adds 15 features that are mostly noise\")\n",
    "print(f\"   - LightGBM may be overfitting to these noisy features\")\n",
    "print(f\"   - Feature selection or dimensionality reduction needed\")\n",
    "\n",
    "print(\"\\n4. HYPERPARAMETER ISSUES:\")\n",
    "print(f\"   - Used n_components=15 (may be too many)\")\n",
    "print(f\"   - Used max_iter=20 (may need more iterations)\")\n",
    "print(f\"   - Used default learning method (batch)\")\n",
    "print(f\"   - Topics may not be well-converged\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. REMOVE LDA entirely - SVD already captures latent structure well\")\n",
    "print(\"2. Focus on Stanford linguistic cues (counts, not binary)\")\n",
    "print(\"3. Add temporal features (hour, day of week)\")\n",
    "print(\"4. Add interaction features (text_length × karma, etc.)\")\n",
    "print(\"5. Try different SVD component counts (optimize this instead)\")\n",
    "print(\"6. If re-trying LDA: use fewer topics (5-10), more iterations, tune hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature importance from exp_005 to see if LDA topics were used\n",
    "print(\"=== Feature Importance Analysis ===\")\n",
    "print(\"(Would need to load exp_005 model to see actual importance)\")\n",
    "print(\"Based on typical LightGBM behavior:\")\n",
    "print(\"- LDA topics likely have low importance\")\n",
    "print(\"- SVD components and numeric features dominate\")\n",
    "print(\"- Adding 15 low-importance features can hurt generalization\")\n",
    "\n",
    "# Let's also check if we should try different SVD component counts\n",
    "print(\"\\n=== SVD Component Optimization ===\")\n",
    "print(\"Current: 50 word + 25 char = 75 components\")\n",
    "print(\"Competition winners typically use 50-100 total components\")\n",
    "print(\"Recommend trying:\")\n",
    "print(\"- 75 word + 25 char = 100 total\")\n",
    "print(\"- 100 word + 50 char = 150 total (but risk overfitting)\")\n",
    "print(\"- 40 word + 20 char = 60 total (more aggressive reduction)\")\n",
    "\n",
    "# Variance explained analysis\n",
    "print(f\"\\nCurrent SVD variance explained:\")\n",
    "print(f\"Word SVD (50 comps): {svd_word.explained_variance_ratio_.sum():.4f} ({svd_word.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
    "print(f\"Char SVD (25 comps): {svd_char.explained_variance_ratio_.sum():.4f} ({svd_char.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
    "print(f\"Combined: {(svd_word.explained_variance_ratio_.sum() + svd_char.explained_variance_ratio_.sum()):.4f}\")\n",
    "\n",
    "print(f\"\\nRecommend: Try 75 word + 25 char = 100 components total\")\n",
    "print(f\"This should capture ~25-30% variance while reducing overfitting risk\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
