# Seed Prompt: Random Acts of Pizza Competition

## Problem Type
Binary classification (predicting whether a Reddit pizza request will be successful)
Evaluation metric: AUC-ROC
Class imbalance: ~25% positive class (successful requests)

## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, correlations, text length statistics
- Training data: 2,878 samples, 32 features including text fields and metadata
- Key features: request_text, request_title, user activity metrics, vote counts, account age
- Missing values primarily in requester_user_flair (75% missing)

## Models and Architecture

### Primary Approach: Multimodal Learning
For text + tabular data problems like this, winning Kaggle solutions consistently use:
- **Dual-branch architecture**: Combine transformer-based text encoder (BERT/RoBERTa) with gradient boosting on tabular features
- Concatenate CLS token embeddings from transformer with tabular model outputs
- Feed combined representation to shallow feed-forward head

### Alternative Approaches:
1. **Gradient Boosting (Primary for tabular features)**
   - LightGBM, XGBoost, CatBoost
   - Excellent for structured metadata (vote counts, account age, activity metrics)
   - Handle missing values well
   - Fast training and good performance

2. **Neural Networks for Text**
   - BERT/RoBERTa fine-tuned on request_text and request_title
   - Can use features-to-text approach: convert categorical/numeric features to text tokens
   - HuggingFace transformers recommended for implementation

3. **Ensemble Strategy**
   - Combine 3-5 diverse models
   - Weighted averaging or stacking typically outperforms simple averaging
   - Example: (BERT + LightGBM + CatBoost) with learned weights

## Preprocessing and Feature Engineering

### Text Processing:
- Concatenate request_title and request_text into single document
- Aggressive cleaning: remove URLs, non-letter characters, Reddit-specific markup
- Lowercase and lemmatize (preferred over stemming)
- Build custom stopword list including Reddit-specific terms
- Use Reddit-aware tokenizers for handling mentions and hashtags

### Feature Engineering:
- **TF-IDF vectors**: Create sparse representations of cleaned text (unigrams + bigrams)
- **Text length features**: request_text_length, request_title_length (already in eda.ipynb)
- **Interaction features**: Combine vote ratios, activity metrics
- **Temporal features**: Extract day of week, hour from timestamps
- **User flair encoding**: Handle missing values (75% missing) carefully - consider as separate category
- **Chi-square feature selection**: Apply to TF-IDF features to reduce dimensionality

### Handling Missing Values:
- requester_user_flair: Treat as separate category or use mode imputation
- Other numeric features: Use median imputation or model-specific handling

## Handling Class Imbalance

Given ~25% positive class rate:
1. **Stratified cross-validation**: Essential for reliable performance estimates
2. **Class weighting**: Use scale_pos_weight (XGBoost) or class_weight (sklearn)
3. **Evaluation metric**: Optimize for AUC-ROC (competition metric)
4. **Resampling (optional)**: 
   - SMOTE for oversampling minority class
   - Undersampling majority if dataset is large
5. **Threshold tuning**: Optimize decision threshold based on validation AUC

## Validation Strategy

- **Stratified K-Fold**: 5 folds recommended
- **Time-based splits**: Consider if temporal patterns exist (check timestamps in eda.ipynb)
- **Early stopping**: Use validation AUC for all gradient boosting models
- **Model selection**: Choose models based on mean validation AUC across folds

## Specific Techniques from Winning Solutions

1. **Multimodal Tabular Predictor**: AutoGluon's multimodal tabular predictor automatically handles text + tabular fusion
2. **FastText embeddings**: Generate embeddings from Reddit text for additional features
3. **Target encoding**: For high-cardinality categorical features (if any exist beyond flair)
4. **Model stacking**: 
   - Level 1: Diverse models (BERT, LightGBM, CatBoost)
   - Level 2: Logistic regression or simple neural network

## Implementation Notes

- Start with gradient boosting on tabular features alone as baseline
- Add text features via TF-IDF + LightGBM as second baseline
- Implement transformer-based text encoder for maximum performance
- Ensemble all three approaches for final submission
- Monitor for overfitting given relatively small dataset (2,878 samples)