{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e536df8",
   "metadata": {},
   "source": [
    "# DistilBERT + LightGBM with Proper Scaling\n",
    "\n",
    "**Problem**: exp_006 (DistilBERT baseline) underperformed due to catastrophic scale mismatch\n",
    "- Meta-features scale: ~1.38 billion\n",
    "- DistilBERT embeddings scale: ~11.3\n",
    "- Ratio: 122 million to 1\n",
    "\n",
    "**Solution**: Apply RobustScaler to meta-features before concatenation\n",
    "\n",
    "**Expected improvement**: +0.03-0.05 AUC (from 0.6312 to 0.66-0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2808265",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee178a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_path = '/home/data/train.json'\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "test_path = '/home/data/test.json'\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].mean():.4f}\")\n",
    "\n",
    "# Combine text fields\n",
    "train_df['combined_text'] = train_df['request_title'].fillna('') + ' ' + train_df['request_text_edit_aware'].fillna('')\n",
    "test_df['combined_text'] = test_df['request_title'].fillna('') + ' ' + test_df['request_text_edit_aware'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d59315",
   "metadata": {},
   "source": [
    "## 2. Extract DistilBERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT model and tokenizer\n",
    "print(\"Loading DistilBERT...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Using GPU for DistilBERT\")\n",
    "else:\n",
    "    print(\"Using CPU for DistilBERT\")\n",
    "\n",
    "# Extract embeddings\n",
    "batch_size = 16\n",
    "def extract_distilbert_features(texts, max_length=256):\n",
    "    \"\"\"Extract [CLS] token embeddings from DistilBERT\"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_features.append(cls_embeddings)\n",
    "    \n",
    "    return np.vstack(all_features)\n",
    "\n",
    "# Extract features (this will take a few minutes)\n",
    "print(\"Extracting DistilBERT features from training data...\")\n",
    "train_distilbert = extract_distilbert_features(train_df['combined_text'].tolist())\n",
    "print(f\"Training embeddings shape: {train_distilbert.shape}\")\n",
    "\n",
    "print(\"Extracting DistilBERT features from test data...\")\n",
    "test_distilbert = extract_distilbert_features(test_df['combined_text'].tolist())\n",
    "print(f\"Test embeddings shape: {test_distilbert.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6339c6f",
   "metadata": {},
   "source": [
    "## 3. Engineer Meta-Features (SAFE features only - NO LEAKAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SAFE meta-features (same as honest baseline)\n",
    "meta_features = [\n",
    "    # Text length features\n",
    "    'total_text_length', 'title_word_count', 'total_word_count', 'word_count', 'text_length',\n",
    "    \n",
    "    # User activity at request time (SAFE - no leakage)\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    \n",
    "    # Account age and temporal features\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_days_since_first_post_on_raop_at_request',\n",
    "    \n",
    "    # Post metadata\n",
    "    'post_was_edited'\n",
    "]\n",
    "\n",
    "# Check which features exist\n",
    "available_features = []\n",
    "for feature in meta_features:\n",
    "    if feature in train_df.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        print(f\"Warning: {feature} not found in data\")\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} meta-features:\")\n",
    "for i, f in enumerate(available_features, 1):\n",
    "    print(f\"{i:2d}. {f}\")\n",
    "\n",
    "# Prepare meta-features\n",
    "train_meta = train_df[available_features].copy()\n",
    "test_meta = test_df[available_features].copy()\n",
    "\n",
    "# Handle boolean post_was_edited\n",
    "if 'post_was_edited' in available_features:\n",
    "    train_meta['post_was_edited'] = train_meta['post_was_edited'].astype(int)\n",
    "    test_meta['post_was_edited'] = test_meta['post_was_edited'].astype(int)\n",
    "\n",
    "print(f\"\\nTrain meta-features shape: {train_meta.shape}\")\n",
    "print(f\"Test meta-features shape: {test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f4566",
   "metadata": {},
   "source": [
    "## 4. Apply Proper Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RobustScaler to meta-features (handles outliers better than StandardScaler)\n",
    "print(\"Applying RobustScaler to meta-features...\")\n",
    "scaler = RobustScaler()\n",
    "train_meta_scaled = scaler.fit_transform(train_meta)\n",
    "test_meta_scaled = scaler.transform(test_meta)\n",
    "\n",
    "print(f\"Meta-features after scaling:\")\n",
    "print(f\"  Train - Mean: {train_meta_scaled.mean():.4f}, Std: {train_meta_scaled.std():.4f}\")\n",
    "print(f\"  Range: [{train_meta_scaled.min():.4f}, {train_meta_scaled.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nDistilBERT embeddings:\")\n",
    "print(f\"  Train - Mean: {train_distilbert.mean():.4f}, Std: {train_distilbert.std():.4f}\")\n",
    "print(f\"  Range: [{train_distilbert.min():.4f}, {train_distilbert.max():.4f}]\")\n",
    "\n",
    "# Check scale ratio after scaling\n",
    "meta_scale = np.abs(train_meta_scaled).max()\n",
    "distilbert_scale = np.abs(train_distilbert).max()\n",
    "scale_ratio = meta_scale / distilbert_scale\n",
    "\n",
    "print(f\"\\n=== SCALE COMPARISON ===\")\n",
    "print(f\"Meta-features scale: {meta_scale:.4f}\")\n",
    "print(f\"DistilBERT scale: {distilbert_scale:.4f}\")\n",
    "print(f\"Scale ratio (meta/distilbert): {scale_ratio:.2f}x\")\n",
    "\n",
    "if scale_ratio > 10:\n",
    "    print(\"WARNING: Still significant scale mismatch!\")\n",
    "elif scale_ratio > 3:\n",
    "    print(\"CAUTION: Moderate scale mismatch, but acceptable\")\n",
    "else:\n",
    "    print(\"GOOD: Scale mismatch resolved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a6777",
   "metadata": {},
   "source": [
    "## 5. Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scaled meta-features with DistilBERT embeddings\n",
    "X_train = np.hstack([train_meta_scaled, train_distilbert])\n",
    "X_test = np.hstack([test_meta_scaled, test_distilbert])\n",
    "y_train = train_df['requester_received_pizza'].values\n",
    "\n",
    "print(f\"Final training features shape: {X_train.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")\n",
    "print(f\"Feature composition:\")\n",
    "print(f\"  - Meta-features: {train_meta_scaled.shape[1]}\")\n",
    "print(f\"  - DistilBERT embeddings: {train_distilbert.shape[1]}\")\n",
    "print(f\"  - Total: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ea264",
   "metadata": {},
   "source": [
    "## 6. Train LightGBM with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Optimized hyperparameters for neural features\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 63,  # Increased from 31 for more complex features\n",
    "    'learning_rate': 0.05,  # Reduced for better convergence\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'max_depth': 7,  # Added to prevent overfitting\n",
    "    'min_child_samples': 20,  # Increased for stability\n",
    "    'scale_pos_weight': (len(y_train) - sum(y_train)) / sum(y_train)  # Handle class imbalance\n",
    "}\n",
    "\n",
    "print(\"LightGBM Parameters:\")\n",
    "for k, v in lgb_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = []\n",
    "fold_predictions = []\n",
    "feature_importances = []\n",
    "\n",
    "print(\"\\nStarting 5-fold CV...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    print(f\"\\nFold {fold}:\")\n",
    "    \n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_set,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_set],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "            lgb.log_evaluation(period=0)  # Suppress iteration logs\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    fold_score = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(fold_score)\n",
    "    \n",
    "    print(f\"  AUC: {fold_score:.4f}\")\n",
    "    print(f\"  Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    # Store feature importance\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importances.append(importance)\n",
    "    \n",
    "    # Store predictions for analysis\n",
    "    fold_predictions.append((val_idx, val_pred))\n",
    "\n",
    "# Overall CV score\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "print(f\"\\n=== CV Results ===\")\n",
    "print(f\"Mean AUC: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "print(f\"Individual folds: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "\n",
    "# Compare with previous experiments\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "print(f\"Previous TF-IDF baseline: 0.6253\")\n",
    "print(f\"Previous DistilBERT (unscaled): 0.6312\")\n",
    "print(f\"This experiment (scaled): {cv_mean:.4f}\")\n",
    "print(f\"Improvement over TF-IDF: {cv_mean - 0.6253:.4f}\")\n",
    "print(f\"Improvement over unscaled DistilBERT: {cv_mean - 0.6312:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d019a",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance across folds\n",
    "mean_importance = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Separate meta-features and DistilBERT features\n",
    "n_meta = train_meta_scaled.shape[1]\n",
    "meta_importance = mean_importance[:n_meta]\n",
    "distilbert_importance = mean_importance[n_meta:]\n",
    "\n",
    "print(f\"=== Feature Importance Analysis ===\")\n",
    "print(f\"Total importance: {mean_importance.sum():.2f}\")\n",
    "print(f\"Meta-features importance: {meta_importance.sum():.2f} ({meta_importance.sum()/mean_importance.sum()*100:.1f}%)\")\n",
    "print(f\"DistilBERT importance: {distilbert_importance.sum():.2f} ({distilbert_importance.sum()/mean_importance.sum()*100:.1f}%)\")\n",
    "\n",
    "# Top meta-features\n",
    "meta_feature_names = available_features\n",
    "top_meta_idx = np.argsort(meta_importance)[-5:][::-1]\n",
    "print(f\"\\nTop 5 Meta-Features:\")\n",
    "for i, idx in enumerate(top_meta_idx, 1):\n",
    "    print(f\"  {i}. {meta_feature_names[idx]}: {meta_importance[idx]:.2f}\")\n",
    "\n",
    "# Top DistilBERT features\n",
    "top_distilbert_idx = np.argsort(distilbert_importance)[-5:][::-1]\n",
    "print(f\"\\nTop 5 DistilBERT Features:\")\n",
    "for i, idx in enumerate(top_distilbert_idx, 1):\n",
    "    print(f\"  {i}. distilbert_{idx}: {distilbert_importance[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dacd9",
   "metadata": {},
   "source": [
    "## 8. Train Final Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training data\n",
    "print(\"Training final model on full training data...\")\n",
    "final_model = lgb.train(\n",
    "    lgb_params,\n",
    "    lgb.Dataset(X_train, label=y_train),\n",
    "    num_boost_round=500,  # Use reasonable number of iterations\n",
    "    callbacks=[lgb.log_evaluation(period=0)]\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "print(f\"Test predictions - Mean: {test_predictions.mean():.4f}, Std: {test_predictions.std():.4f}\")\n",
    "print(f\"Range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': test_predictions\n",
    "})\n",
    "\n",
    "submission_path = '/home/code/submission_candidates/candidate_005_distilbert_scaled.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "\n",
    "# Save experiment results\n",
    "experiment_results = {\n",
    "    'experiment_id': 'exp_007',\n",
    "    'name': 'distilbert_proper_scaling',\n",
    "    'cv_mean': cv_mean,\n",
    "    'cv_std': cv_std,\n",
    "    'cv_scores': cv_scores,\n",
    "    'feature_composition': {\n",
    "        'meta_features': n_meta,\n",
    "        'distilbert_features': train_distilbert.shape[1],\n",
    "        'total': X_train.shape[1]\n",
    "    },\n",
    "    'meta_importance_pct': meta_importance.sum()/mean_importance.sum()*100,\n",
    "    'distilbert_importance_pct': distilbert_importance.sum()/mean_importance.sum()*100,\n",
    "    'scaling_method': 'RobustScaler',\n",
    "    'scale_ratio_after_scaling': scale_ratio,\n",
    "    'lgb_params': lgb_params\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/home/code/experiments/exp_007_results.json', 'w') as f:\n",
    "    json.dump(experiment_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nExperiment results saved to: /home/code/experiments/exp_007_results.json\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
