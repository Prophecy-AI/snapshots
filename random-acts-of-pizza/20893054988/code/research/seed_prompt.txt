## Current Status
- Best CV: 1.0 from exp_000 (meaningless due to leakage)
- Experiments above gold: 0 (all results invalid)

## Response to Evaluator
**Technical verdict was UNRELIABLE. I completely agree** - the perfect 1.0 CV score was a red flag that should have triggered immediate investigation. The flair features (`requester_user_flair` with values 'shroom' and 'PIF') are post-outcome rewards assigned AFTER pizza receipt, creating perfect separation in 715/2878 samples (24.8%). This is equivalent to using the target as a feature.

**Evaluator's top priority: Remove all flair features and re-run baseline to get honest score.** I fully agree and have completed the analysis to identify ALL leakage sources:

1. **requester_user_flair** - Post-outcome reward badges (shroom=677 users, PIF=38 users, both 100% pizza receipt rate)
2. **giver_username_if_known** - Only known after pizza is given
3. **All _at_retrieval features** (11 features) - Collected after outcome

**Key concerns raised and how I'm addressing them:**
- **Catastrophic data leakage**: I've identified 13 leakage features that must be excluded. Analysis notebook `exploration/evolver_loop1_analysis.ipynb` documents the complete safe/unsafe feature list.
- **No feature validation**: I've created a systematic validation framework checking temporal legitimacy of each feature.
- **Missed red flags**: Perfect scores should trigger immediate investigation - this is now documented as a core principle.

## Data Understanding
**Reference notebooks**: See `exploration/evolver_loop1_analysis.ipynb` for complete feature validation analysis

**Key patterns to exploit (with reasoning):**
1. **Text length matters**: Successful requests are ~25% longer (557 vs 447 chars, 105 vs 84 words). Longer requests may indicate more effort/storytelling.
2. **Gratitude language**: 'thanks' appears in 28.8% of successful vs 21.2% of failed requests; 'thank' in 43.4% vs 30.7%. Expressing gratitude may increase success.
3. **Promise of reciprocity**: 'pay' in 38.0% vs 29.8%, 'forward' in 18.6% vs 13.2%. Offering to "pay it forward" correlates with success.
4. **User activity signals**: Successful requesters have higher activity (124.6 vs 108.3 comments, 25.0 vs 20.5 posts, 5730 vs 3413 upvotes+downvotes). Established users may be more trusted.
5. **Account maturity**: Successful requesters have older accounts (273 vs 243 days avg) and longer RAOP history (29.7 vs 11.9 days since first post).
6. **Temporal patterns**: Hour 1 AM shows highest success (30.7%), hour 7 AM lowest (10.3%). Late-night requests may be more compelling or face less competition.

## Recommended Approaches (Priority Order)

### 1. Establish Honest Baseline (CRITICAL FIRST STEP)
- Re-run TF-IDF + LightGBM with ONLY safe features (14 features total)
- Exclude: requester_user_flair, giver_username_if_known, all _at_retrieval features
- Expected outcome: CV score << 1.0 (likely 0.6-0.75 range), but it will be REAL
- This becomes our new baseline for improvement

### 2. Enhanced Text Feature Engineering
- **TF-IDF optimization**: Try different n-gram ranges (1,1), (1,2), (1,3), different max_features (5000, 10000, 20000)
- **Sentiment features**: Create binary indicators for gratitude keywords (thanks, thank, appreciate, grateful)
- **Promise features**: Binary indicators for reciprocity keywords (pay, forward, return, repay, promise)
- **Storytelling features**: Count of causal/connective words (because, since, when, after, before)
- **Readability metrics**: Flesch-Kincaid, SMOG index, character/word/sentence ratios
- **Emotional intensity**: Exclamation marks, capitalization patterns, emoji counts

### 3. User Behavior & Temporal Features
- **Activity ratios**: comments/posts ratio, RAOP activity / total activity ratio
- **Engagement quality**: upvotes per comment/post (at request time)
- **Temporal features**: Hour of day (one-hot), day of week, is_weekend, is_night (1-6 AM)
- **Request timing**: Days since last RAOP request (need to calculate from timestamps)
- **Account lifecycle**: Account age buckets (new: <30d, established: 30-365d, veteran: >365d)

### 4. Advanced NLP Models
- **BERT/RoBERTa fine-tuning**: Use pretrained models on request text (title + body)
- **Ensemble strategy**: Combine TF-IDF + LightGBM with transformer model predictions
- **Model stacking**: Use meta-learner to combine predictions from multiple text encoders

### 5. Cross-Validation Validation
- Verify CV stability across different random seeds (5 different seeds)
- Check for data leakage in folds (ensure no user appears in multiple folds if temporal)
- Monitor for overfitting - if CV >> LB score, may indicate remaining leakage

## What NOT to Try
- ❌ Any _at_retrieval features (11 features) - post-outcome data
- ❌ requester_user_flair - post-outcome reward badges (shroom, PIF)
- ❌ giver_username_if_known - only known after pizza given
- ❌ Perfect CV scores - if you see 0.95+, investigate immediately for leakage
- ❌ Complex models before establishing honest baseline - need to know real signal first

## Validation Notes
- **CV scheme**: 5-fold stratified CV (appropriate for class imbalance)
- **Target CV range**: Realistic AUC likely 0.60-0.75 (not 1.0)
- **Red flags**: CV > 0.90 likely indicates remaining leakage
- **Confidence**: Low until we see first honest CV score - this is our top priority

**Critical success metric**: First experiment must produce CV < 0.90 to confirm leakage is removed. If CV is still high, investigate remaining features for hidden leakage.