{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080e7795",
   "metadata": {},
   "source": [
    "# Evolver Loop 5 Analysis: Transformer Model Preparation\n",
    "\n",
    "**Objective**: Analyze the current state and prepare for transformer-based model implementation as recommended by evaluator feedback.\n",
    "\n",
    "**Key questions**:\n",
    "1. What transformer architectures are suitable for this problem?\n",
    "2. How should we handle the text preprocessing for transformers?\n",
    "3. What are the computational constraints and trade-offs?\n",
    "4. How to combine transformer features with existing meta-features?\n",
    "\n",
    "**Evaluator recommendation**: Upgrade from TF-IDF to transformer models (BERT/RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_path = '/home/data/train.json'\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.3f}\")\n",
    "print(f\"\\nText length statistics:\")\n",
    "train_df['text_length'] = train_df['request_text'].fillna('').str.len()\n",
    "print(train_df['text_length'].describe())\n",
    "\n",
    "# Check for GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"\\nPyTorch not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66555894",
   "metadata": {},
   "source": [
    "## 1. Text Characteristics Analysis\n",
    "\n",
    "Understanding the text data to choose appropriate transformer models and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ca658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text characteristics for transformer suitability\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT CHARACTERISTICS FOR TRANSFORMER MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine title and text\n",
    "train_df['full_text'] = train_df['request_title'].fillna('') + ' ' + train_df['request_text'].fillna('')\n",
    "\n",
    "# Text length analysis\n",
    "word_counts = train_df['full_text'].str.split().str.len()\n",
    "print(f\"\\nWord count statistics:\")\n",
    "print(word_counts.describe())\n",
    "\n",
    "# Token count estimation (for transformers)\n",
    "avg_tokens_per_word = 1.3  # Rough estimate including subword tokenization\n",
    "estimated_tokens = word_counts * avg_tokens_per_word\n",
    "print(f\"\\nEstimated token count (avg {avg_tokens_per_word} tokens/word):\")\n",
    "print(estimated_tokens.describe())\n",
    "\n",
    "# Distribution visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(word_counts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(word_counts.mean(), color='red', linestyle='--', label=f'Mean: {word_counts.mean():.0f}')\n",
    "plt.axvline(word_counts.median(), color='green', linestyle='--', label=f'Median: {word_counts.median():.0f}')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Length (Words)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(estimated_tokens, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.axvline(estimated_tokens.mean(), color='red', linestyle='--', label=f'Mean: {estimated_tokens.mean():.0f}')\n",
    "plt.axvline(estimated_tokens.median(), color='green', linestyle='--', label=f'Median: {estimated_tokens.median():.0f}')\n",
    "plt.xlabel('Estimated Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Length (Tokens)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for very long texts that might need truncation\n",
    "long_texts = (estimated_tokens > 512).sum()\n",
    "print(f\"\\nTexts exceeding 512 tokens: {long_texts} ({long_texts/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Texts exceeding 256 tokens: {(estimated_tokens > 256).sum()} ({(estimated_tokens > 256).sum()/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459f41b",
   "metadata": {},
   "source": [
    "## 2. Transformer Model Selection\n",
    "\n",
    "Evaluate different transformer architectures based on:\n",
    "- Performance vs speed trade-offs\n",
    "- Memory requirements\n",
    "- Suitability for classification tasks\n",
    "- Pre-trained model availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02234de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research transformer models suitable for text classification\n",
    "print(\"=\"*60)\n",
    "print(\"TRANSFORMER MODEL OPTIONS FOR PIZZA REQUEST CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_analysis = {\n",
    "    'BERT Base': {\n",
    "        'params': '110M',\n",
    "        'max_length': 512,\n",
    "        'speed': 'Medium',\n",
    "        'memory': 'High (400MB)',\n",
    "        'performance': 'High',\n",
    "        'pros': ['Excellent understanding', 'Widely used', 'Many fine-tuned variants'],\n",
    "        'cons': ['Slow', 'Memory intensive', 'May overfit on small data']\n",
    "    },\n",
    "    'DistilBERT': {\n",
    "        'params': '66M',\n",
    "        'max_length': 512,\n",
    "        'speed': 'Fast',\n",
    "        'memory': 'Medium (250MB)',\n",
    "        'performance': 'High',\n",
    "        'pros': ['60% faster than BERT', '40% smaller', 'Good performance'],\n",
    "        'cons': ['Slightly lower accuracy', 'Less capacity for complex patterns']\n",
    "    },\n",
    "    'RoBERTa Base': {\n",
    "        'params': '125M',\n",
    "        'max_length': 512,\n",
    "        'speed': 'Medium',\n",
    "        'memory': 'High (400MB)',\n",
    "        'performance': 'Very High',\n",
    "        'pros': ['Better than BERT on many tasks', 'Robust training'],\n",
    "        'cons': ['Similar speed/memory as BERT', 'Longer training time']\n",
    "    },\n",
    "    'MiniLM': {\n",
    "        'params': '22M',\n",
    "        'max_length': 512,\n",
    "        'speed': 'Very Fast',\n",
    "        'memory': 'Low (120MB)',\n",
    "        'performance': 'Medium-High',\n",
    "        'pros': ['Very fast inference', 'Low memory', 'Good for prototyping'],\n",
    "        'cons': ['Lower capacity', 'May miss subtle patterns']\n",
    "    },\n",
    "    'DeBERTa': {\n",
    "        'params': '140M',\n",
    "        'max_length': 512,\n",
    "        'speed': 'Slow',\n",
    "        'memory': 'Very High (500MB)',\n",
    "        'performance': 'Very High',\n",
    "        'pros': ['State-of-the-art performance', 'Better handling of context'],\n",
    "        'cons': ['Very slow', 'High memory', 'Long training time']\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, details in models_analysis.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Parameters: {details['params']}\")\n",
    "    print(f\"  Max Length: {details['max_length']} tokens\")\n",
    "    print(f\"  Speed: {details['speed']}\")\n",
    "    print(f\"  Memory: {details['memory']}\")\n",
    "    print(f\"  Performance: {details['performance']}\")\n",
    "    print(f\"  Pros: {', '.join(details['pros'])}\")\n",
    "    print(f\"  Cons: {', '.join(details['cons'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Start with DistilBERT\")\n",
    "print(\"=\"*60)\n",
    "print(\"Rationale:\")\n",
    "print(\"1. Good balance of speed and performance\")\n",
    "print(\"2. 60% faster than BERT - important for rapid experimentation\")\n",
    "print(\"3. Lower memory footprint - can use larger batch sizes\")\n",
    "print(\"4. Can upgrade to RoBERTa if needed after baseline established\")\n",
    "print(\"5. Less risk of overfitting on this dataset size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742cf4e",
   "metadata": {},
   "source": [
    "## 3. Implementation Strategy\n",
    "\n",
    "Plan for integrating transformers with existing meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how to combine transformer features with existing meta-features\n",
    "print(\"=\"*60)\n",
    "print(\"INTEGRATION STRATEGY: TRANSFORMER + META-FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Current meta-features from exp_001\n",
    "meta_features = [\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_days_since_first_post_on_raop_at_request',\n",
    "    'requester_number_of_posts_on_raop_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_at_request',\n",
    "    'requester_downvotes_at_request',\n",
    "    'requester_comment_karma_at_request',\n",
    "    'requester_posts_per_day_at_request',\n",
    "    'requester_comments_per_day_at_request',\n",
    "    'requester_upvotes_per_day_at_request',\n",
    "    'requester_downvotes_per_day_at_request',\n",
    "    'requester_upvotes_minus_downvotes_per_day_at_request',\n",
    "    'requester_upvotes_plus_downvotes_per_day_at_request',\n",
    "    'requester_comment_karma_per_day_at_request',\n",
    "    'requester_account_age_in_years_at_request',\n",
    "    'unix_timestamp_of_request_utc',\n",
    "    'requester_received_pizza_from_this_subreddit_before_this_request',\n",
    "    'post_was_edited',\n",
    "    'request_text_length'\n",
    "]\n",
    "\n",
    "print(f\"Current meta-features: {len(meta_features)}\")\n",
    "print(f\"Sample features: {meta_features[:10]}\")\n",
    "\n",
    "# Feature importance from exp_001 (from evaluator feedback)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP META-FEATURES BY IMPORTANCE (from exp_001)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_features = [\n",
    "    ('text_length', 560.27),\n",
    "    ('requester_upvotes_minus_downvotes_at_request', 492.96),\n",
    "    ('requester_account_age_in_days_at_request', 298.81),\n",
    "    ('requester_days_since_first_post_on_raop_at_request', 263.27),\n",
    "    ('total_word_count', 182.46)\n",
    "]\n",
    "\n",
    "for feature, importance in top_features:\n",
    "    print(f\"{feature}: {importance:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal meta-feature importance: 2778.99\")\n",
    "print(f\"Total TF-IDF importance: 2018.79\")\n",
    "print(f\"Meta-features contribute ~58% of model signal\")\n",
    "\n",
    "# Integration approaches\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE INTEGRATION APPROACHES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "approaches = {\n",
    "    'Approach 1: Concatenation': {\n",
    "        'method': 'Extract [CLS] token embedding from transformer, concatenate with meta-features',\n",
    "        'pros': ['Simple to implement', 'Preserves all information', 'Flexible'],\n",
    "        'cons': ['May need dimensionality reduction', 'Different feature scales'],\n",
    "        'complexity': 'Low',\n",
    "        'expected_gain': '+0.03-0.05 AUC'\n",
    "    },\n",
    "    'Approach 2: Late Fusion': {\n",
    "        'method': 'Train separate transformer and meta-feature models, ensemble predictions',\n",
    "        'pros': ['Models can specialize', 'No feature engineering needed', 'Robust'],\n",
    "        'cons': ['More complex training', 'Need careful ensembling'],\n",
    "        'complexity': 'Medium',\n",
    "        'expected_gain': '+0.04-0.06 AUC'\n",
    "    },\n",
    "    'Approach 3: Attention Gating': {\n",
    "        'method': 'Use meta-features to gate transformer attention (complex)',\n",
    "        'pros': ['Theoretically optimal', 'Learns interactions'],\n",
    "        'cons': ['Very complex', 'May overfit', 'Hard to debug'],\n",
    "        'complexity': 'High',\n",
    "        'expected_gain': '+0.02-0.05 AUC (high variance)'\n",
    "    }\n",
    "}\n",
    "\n",
    "for approach, details in approaches.items():\n",
    "    print(f\"\\n{approach}:\")\n",
    "    print(f\"  Method: {details['method']}\")\n",
    "    print(f\"  Complexity: {details['complexity']}\")\n",
    "    print(f\"  Expected Gain: {details['expected_gain']}\")\n",
    "    print(f\"  Pros: {', '.join(details['pros'])}\")\n",
    "    print(f\"  Cons: {', '.join(details['cons'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Start with Approach 1 (Concatenation)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Rationale:\")\n",
    "print(\"1. Simplest to implement - faster iteration\")\n",
    "print(\"2. Directly builds on existing meta-feature pipeline\")\n",
    "print(\"3. Can upgrade to Approach 2 (Late Fusion) later if needed\")\n",
    "print(\"4. Lower risk of overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7af8ca",
   "metadata": {},
   "source": [
    "## 4. Computational Requirements\n",
    "\n",
    "Estimate memory and time requirements for transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27406528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate computational requirements\n",
    "print(\"=\"*60)\n",
    "print(\"COMPUTATIONAL REQUIREMENTS ESTIMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset size\n",
    "n_samples = len(train_df)\n",
    "n_features = 10000  # Estimated transformer features\n",
    "n_meta = len(meta_features)\n",
    "\n",
    "print(f\"Dataset size: {n_samples:,} samples\")\n",
    "print(f\"Transformer features: ~{n_features:,}\")\n",
    "print(f\"Meta-features: {n_meta}\")\n",
    "print(f\"Total features: ~{n_features + n_meta:,}\")\n",
    "\n",
    "# Memory estimation for different batch sizes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEMORY ESTIMATION (DistilBERT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_sizes = [8, 16, 32]\n",
    "seq_length = 256  # Using 256 instead of 512 for speed\n",
    "hidden_size = 768  # DistilBERT base\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Model parameters memory\n",
    "    model_params_mb = 250  # DistilBERT base\n",
    "    \n",
    "    # Activations memory (rough estimate)\n",
    "    # batch_size * seq_length * hidden_size * 4 bytes * layers\n",
    "    activations_mb = batch_size * seq_length * hidden_size * 4 * 6 / (1024**2)  # 6 layers approximation\n",
    "    \n",
    "    # Gradients memory (similar to parameters)\n",
    "    gradients_mb = model_params_mb\n",
    "    \n",
    "    # Optimizer state (Adam: 2x parameters)\n",
    "    optimizer_mb = model_params_mb * 2\n",
    "    \n",
    "    total_mb = model_params_mb + activations_mb + gradients_mb + optimizer_mb\n",
    "    total_gb = total_mb / 1024\n",
    "    \n",
    "    print(f\"\\nBatch size {batch_size}:\")\n",
    "    print(f\"  Model parameters: {model_params_mb:.0f} MB\")\n",
    "    print(f\"  Activations: {activations_mb:.0f} MB\")\n",
    "    print(f\"  Gradients: {gradients_mb:.0f} MB\")\n",
    "    print(f\"  Optimizer state: {optimizer_mb:.0f} MB\")\n",
    "    print(f\"  Total: {total_mb:.0f} MB ({total_gb:.1f} GB)\")\n",
    "\n",
    "# Training time estimation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING TIME ESTIMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_epochs = 3  # Typical for fine-tuning\n",
    "n_folds = 5\n",
    "samples_per_epoch = n_samples * (n_folds - 1) / n_folds  # 4/5 for training\n",
    "\n",
    "for batch_size in [16, 32]:\n",
    "    steps_per_epoch = samples_per_epoch / batch_size\n",
    "    # Rough estimate: 0.1 sec/step for DistilBERT on GPU\n",
    "    seconds_per_epoch = steps_per_epoch * 0.1\n",
    "    total_seconds = seconds_per_epoch * n_epochs * n_folds\n",
    "    total_hours = total_seconds / 3600\n",
    "    \n",
    "    print(f\"\\nBatch size {batch_size}:\")\n",
    "    print(f\"  Steps per epoch: {steps_per_epoch:.0f}\")\n",
    "    print(f\"  Time per epoch: {seconds_per_epoch/60:.1f} minutes\")\n",
    "    print(f\"  Total training time: {total_hours:.1f} hours\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Use batch_size=16\")\n",
    "print(\"=\"*60)\n",
    "print(\"Rationale:\")\n",
    "print(\"1. Fits comfortably in most GPUs\")\n",
    "print(\"2. Reasonable training time (~2-3 hours)\")\n",
    "print(\"3. Good gradient stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee897c",
   "metadata": {},
   "source": [
    "## 5. Implementation Roadmap\n",
    "\n",
    "Step-by-step plan for implementing transformer-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create implementation roadmap\n",
    "print(\"=\"*60)\n",
    "print(\"IMPLEMENTATION ROADMAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "roadmap = [\n",
    "    {\n",
    "        'step': 1,\n",
    "        'task': 'Setup transformer environment',\n",
    "        'details': [\n",
    "            'Install transformers, torch, datasets libraries',\n",
    "            'Verify GPU availability and memory',\n",
    "            'Download DistilBERT base model'\n",
    "        ],\n",
    "        'estimated_time': '30 minutes',\n",
    "        'risk': 'Low'\n",
    "    },\n",
    "    {\n",
    "        'step': 2,\n",
    "        'task': 'Create text preprocessing pipeline',\n",
    "        'details': [\n",
    "            'Combine title and text',\n",
    "            'Handle missing values',\n",
    "            'Tokenize with DistilBERT tokenizer',\n",
    "            'Set max_length=256 (balance speed/performance)',\n",
    "            'Create attention masks'\n",
    "        ],\n",
    "        'estimated_time': '1 hour',\n",
    "        'risk': 'Low'\n",
    "    },\n",
    "    {\n",
    "        'step': 3,\n",
    "        'task': 'Extract transformer features',\n",
    "        'details': [\n",
    "            'Load pre-trained DistilBERT',\n",
    "            'Extract [CLS] token embeddings (768-dim)',\n",
    "            'Process in batches to manage memory',\n",
    "            'Save embeddings for reuse'\n",
    "        ],\n",
    "        'estimated_time': '2 hours',\n",
    "        'risk': 'Medium'\n",
    "    },\n",
    "    {\n",
    "        'step': 4,\n",
    "        'task': 'Integrate with meta-features',\n",
    "        'details': [\n",
    "            'Load existing meta-features from exp_001',\n",
    "            'Concatenate transformer embeddings with meta-features',\n",
    "            'Standardize/normalize features if needed',\n",
    "            'Verify feature dimensions'\n",
    "        ],\n",
    "        'estimated_time': '1 hour',\n",
    "        'risk': 'Low'\n",
    "    },\n",
    "    {\n",
    "        'step': 5,\n",
    "        'task': 'Train LightGBM on combined features',\n",
    "        'details': [\n",
    "            'Use same CV scheme as exp_001 (5-fold stratified)',\n",
    "            'Tune hyperparameters for new feature set',\n",
    "            'Monitor for overfitting',\n",
    "            'Compare with exp_001 baseline'\n",
    "        ],\n",
    "        'estimated_time': '2 hours',\n",
    "        'risk': 'Medium'\n",
    "    },\n",
    "    {\n",
    "        'step': 6,\n",
    "        'task': 'Evaluate and iterate',\n",
    "        'details': [\n",
    "            'Analyze feature importance (transformer vs meta)',\n",
    "            'Identify if transformer adds signal',\n",
    "            'Consider fine-tuning if feature extraction underperforms',\n",
    "            'Experiment with different transformer models'\n",
    "        ],\n",
    "        'estimated_time': '2 hours',\n",
    "        'risk': 'Medium'\n",
    "    }\n",
    "]\n",
    "\n",
    "for item in roadmap:\n",
    "    print(f\"\\nStep {item['step']}: {item['task']}\")\n",
    "    print(f\"  Estimated time: {item['estimated_time']}\")\n",
    "    print(f\"  Risk: {item['risk']}\")\n",
    "    for detail in item['details']:\n",
    "        print(f\"  - {detail}\")\n",
    "\n",
    "total_time = sum([0.5, 1, 2, 1, 2, 2])  # hours\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Total estimated time: {total_time} hours\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nExpected outcome:\")\n",
    "print(\"- CV score improvement: 0.6253 â†’ 0.65-0.68 (+0.025-0.055)\")\n",
    "print(\"- Better text understanding through contextual embeddings\")\n",
    "print(\"- Foundation for further improvements\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
