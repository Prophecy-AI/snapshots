{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a0f3fa",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis: Feature Validation & Leakage Investigation\n",
    "\n",
    "**Goal**: Identify and remove all post-outcome features, validate temporal legitimacy of remaining features, and establish an honest baseline.\n",
    "\n",
    "**Critical Finding from Evaluator**: `flair_shroom` and `flair_PIF` are post-outcome rewards assigned AFTER pizza receipt - complete leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf505a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load data\n",
    "train_path = '/home/data/train.json'\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"Columns: {len(train_df.columns)}\")\n",
    "print(\"\\nAll columns:\")\n",
    "for i, col in enumerate(train_df.columns):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29837259",
   "metadata": {},
   "source": [
    "## 1. Identify Post-Outcome (Leakage) Features\n",
    "\n",
    "Features ending with `_at_retrieval` are collected AFTER the outcome and should NOT be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify retrieval-time features (post-outcome)\n",
    "retrieval_features = [col for col in train_df.columns if col.endswith('_at_retrieval')]\n",
    "print(f\"=== RETRIEVAL-TIME FEATURES (POST-OUTCOME - DO NOT USE) ===\")\n",
    "print(f\"Found {len(retrieval_features)} features:\")\n",
    "for feature in retrieval_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\n=== REQUEST-TIME FEATURES (SAFE TO USE) ===\")\n",
    "request_features = [col for col in train_df.columns if col.endswith('_at_request')]\n",
    "print(f\"Found {len(request_features)} features:\")\n",
    "for feature in request_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\n=== OTHER FEATURES (NEED MANUAL VALIDATION) ===\")\n",
    "other_features = [col for col in train_df.columns \n",
    "                  if not col.endswith('_at_retrieval') \n",
    "                  and not col.endswith('_at_request')\n",
    "                  and col != 'requester_received_pizza']\n",
    "print(f\"Found {len(other_features)} features:\")\n",
    "for feature in other_features:\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695f8c7",
   "metadata": {},
   "source": [
    "## 2. Investigate Flair Feature Leakage\n",
    "\n",
    "The evaluator identified that flair features are post-outcome rewards. Let me verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze flair feature leakage\n",
    "print(\"=== FLAIR FEATURE ANALYSIS ===\")\n",
    "print(\"\\nFlair distribution:\")\n",
    "flair_counts = train_df['requester_user_flair'].value_counts(dropna=False)\n",
    "print(flair_counts)\n",
    "\n",
    "print(\"\\n=== PIZZA RECEIPT RATE BY FLAIR ===\")\n",
    "flair_pizza_rates = train_df.groupby('requester_user_flair')['requester_received_pizza'].agg(['count', 'sum', 'mean'])\n",
    "flair_pizza_rates.columns = ['total_users', 'users_with_pizza', 'pizza_rate']\n",
    "flair_pizza_rates = flair_pizza_rates.sort_values('pizza_rate', ascending=False)\n",
    "print(flair_pizza_rates)\n",
    "\n",
    "# Check for perfect separation\n",
    "print(f\"\\n=== PERFECT SEPARATION CHECK ===\")\n",
    "perfect_separation = flair_pizza_rates[flair_pizza_rates['pizza_rate'].isin([0.0, 1.0])]\n",
    "print(f\"Flair categories with perfect separation: {len(perfect_separation)}\")\n",
    "print(perfect_separation)\n",
    "\n",
    "# Calculate total samples with perfect separation\n",
    "total_perfect = perfect_separation['total_users'].sum()\n",
    "print(f\"\\nTotal samples with perfect separation: {total_perfect} / {len(train_df)} ({total_perfect/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Compare to overall positive rate\n",
    "overall_positive_rate = train_df['requester_received_pizza'].mean()\n",
    "print(f\"Overall positive rate: {overall_positive_rate:.3f}\")\n",
    "print(f\"Perfect separation samples positive rate: {perfect_separation['users_with_pizza'].sum() / total_perfect:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸš¨ CRITICAL: These flair categories are post-outcome rewards!\")\n",
    "print(f\"   - 'shroom' and 'PIF' are badges given AFTER receiving pizza\")\n",
    "print(f\"   - Using these is equivalent to using the target as a feature\")\n",
    "print(f\"   - This explains the perfect 1.0 CV score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff599b",
   "metadata": {},
   "source": [
    "## 3. Validate Remaining Features\n",
    "\n",
    "Check each non-retrieval feature for potential leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze potential leakage in other features\n",
    "print(\"=== FEATURE LEAKAGE ANALYSIS ===\")\n",
    "\n",
    "# Check features with suspiciously high correlation to target\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Text features (safe - available at request time)\n",
    "print(\"\\n1. TEXT FEATURES (SAFE):\")\n",
    "print(\"   - request_title, request_text, request_text_edit_aware\")\n",
    "print(\"   - These are available when the request is made\")\n",
    "\n",
    "# User activity at request time (safe)\n",
    "print(\"\\n2. USER ACTIVITY AT REQUEST TIME (SAFE):\")\n",
    "activity_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'requester_number_of_comments_in_raop_at_request',\n",
    "    'requester_number_of_posts_on_raop_at_request',\n",
    "    'requester_number_of_subreddits_at_request'\n",
    "]\n",
    "for feature in activity_features:\n",
    "    if feature in train_df.columns:\n",
    "        print(f\"   - {feature}\")\n",
    "\n",
    "# Account age (safe)\n",
    "print(\"\\n3. ACCOUNT AGE FEATURES (SAFE):\")\n",
    "age_features = [\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_days_since_first_post_on_raop_at_request'\n",
    "]\n",
    "for feature in age_features:\n",
    "    if feature in train_df.columns:\n",
    "        print(f\"   - {feature}\")\n",
    "\n",
    "# Timestamp (safe)\n",
    "print(\"\\n4. TEMPORAL FEATURES (SAFE):\")\n",
    "print(\"   - unix_timestamp_of_request\")\n",
    "print(\"   - unix_timestamp_of_request_utc\")\n",
    "\n",
    "# Subreddit info (safe)\n",
    "print(\"\\n5. SUBREDDIT FEATURES (SAFE):\")\n",
    "print(\"   - requester_subreddits_at_request\")\n",
    "\n",
    "# Check post_was_edited\n",
    "print(\"\\n6. POST EDITED FLAG (NEEDS INVESTIGATION):\")\n",
    "print(f\"   - Unique values: {train_df['post_was_edited'].unique()[:10]}\")\n",
    "print(f\"   - Data type: {train_df['post_was_edited'].dtype}\")\n",
    "# Check if it's actually a boolean or a timestamp\n",
    "edited_sample = train_df['post_was_edited'].iloc[0]\n",
    "print(f\"   - Sample value: {edited_sample}\")\n",
    "print(f\"   - Sample type: {type(edited_sample)}\")\n",
    "\n",
    "# Check giver_username\n",
    "print(\"\\n7. GIVER USERNAME (LIKELY LEAKAGE):\")\n",
    "print(f\"   - Unique givers: {train_df['giver_username_if_known'].nunique()}\")\n",
    "print(f\"   - Sample values: {train_df['giver_username_if_known'].unique()[:5]}\")\n",
    "print(f\"   - âš ï¸ This is likely only known AFTER pizza is given!\")\n",
    "\n",
    "# Check request_id and username\n",
    "print(\"\\n8. IDENTIFIERS (SAFE BUT NOT PREDICTIVE):\")\n",
    "print(\"   - request_id (unique identifier)\")\n",
    "print(\"   - requester_username (may have some signal but use carefully)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aaf309",
   "metadata": {},
   "source": [
    "## 4. Safe Feature Set Definition\n",
    "\n",
    "Based on analysis, define the clean feature set with NO leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SAFE features (no leakage)\n",
    "SAFE_FEATURES = {\n",
    "    'text': [\n",
    "        'request_title',\n",
    "        'request_text', \n",
    "        'request_text_edit_aware'\n",
    "    ],\n",
    "    'user_activity': [\n",
    "        'requester_number_of_comments_at_request',\n",
    "        'requester_number_of_posts_at_request',\n",
    "        'requester_upvotes_plus_downvotes_at_request',\n",
    "        'requester_number_of_comments_in_raop_at_request',\n",
    "        'requester_number_of_posts_on_raop_at_request',\n",
    "        'requester_number_of_subreddits_at_request'\n",
    "    ],\n",
    "    'account_age': [\n",
    "        'requester_account_age_in_days_at_request',\n",
    "        'requester_days_since_first_post_on_raop_at_request'\n",
    "    ],\n",
    "    'temporal': [\n",
    "        'unix_timestamp_of_request',\n",
    "        'unix_timestamp_of_request_utc'\n",
    "    ],\n",
    "    'subreddit': [\n",
    "        'requester_subreddits_at_request'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Features to EXCLUDE (leakage)\n",
    "EXCLUDE_FEATURES = [\n",
    "    'requester_user_flair',  # Post-outcome reward\n",
    "    'giver_username_if_known',  # Only known after pizza given\n",
    "] + retrieval_features  # All _at_retrieval features\n",
    "\n",
    "print(\"=== SAFE FEATURES TO USE ===\")\n",
    "all_safe = []\n",
    "for category, features in SAFE_FEATURES.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for feature in features:\n",
    "        if feature in train_df.columns:\n",
    "            print(f\"  âœ“ {feature}\")\n",
    "            all_safe.append(feature)\n",
    "\n",
    "print(f\"\\nTotal safe features: {len(all_safe)}\")\n",
    "\n",
    "print(\"\\n=== FEATURES TO EXCLUDE (LEAKAGE) ===\")\n",
    "for feature in EXCLUDE_FEATURES:\n",
    "    if feature in train_df.columns:\n",
    "        print(f\"  âœ— {feature}\")\n",
    "\n",
    "print(f\"\\nTotal excluded features: {len([f for f in EXCLUDE_FEATURES if f in train_df.columns])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7110eb",
   "metadata": {},
   "source": [
    "## 5. Text Pattern Analysis\n",
    "\n",
    "Analyze what makes a successful request in the legitimate text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text patterns in legitimate data (without flair leakage)\n",
    "print(\"=== TEXT PATTERN ANALYSIS ===\")\n",
    "\n",
    "# Combine text fields\n",
    "train_df['full_text'] = (train_df['request_title'].fillna('') + ' ' + \n",
    "                         train_df['request_text'].fillna('')).str.strip()\n",
    "\n",
    "# Basic text statistics by outcome\n",
    "text_stats = train_df.groupby('requester_received_pizza').agg({\n",
    "    'full_text': [\n",
    "        lambda x: x.str.len().mean(),\n",
    "        lambda x: x.str.len().std(),\n",
    "        lambda x: x.str.split().str.len().mean(),  # word count\n",
    "        lambda x: x.str.split().str.len().std()\n",
    "    ]\n",
    "}).round(2)\n",
    "\n",
    "text_stats.columns = ['avg_length', 'std_length', 'avg_words', 'std_words']\n",
    "text_stats.index = ['No Pizza', 'Pizza']\n",
    "print(\"\\nText statistics by outcome:\")\n",
    "print(text_stats)\n",
    "\n",
    "# Check for common keywords in successful vs unsuccessful requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_common_words(texts, n=10):\n",
    "    \"\"\"Get most common words from a series of texts\"\"\"\n",
    "    all_text = ' '.join(texts.fillna('').astype(str).tolist())\n",
    "    # Simple tokenization\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', all_text.lower())\n",
    "    # Remove common stopwords\n",
    "    stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'she', 'use', 'her', 'now', 'old', 'see', 'two', 'way', 'who', 'oil', 'sit', 'set', 'run', 'eat', 'far', 'sea', 'eye', 'ago', 'off', 'far', 'set', 'own', 'say'}\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "print(\"\\n=== TOP WORDS IN SUCCESSFUL REQUESTS ===\")\n",
    "successful_texts = train_df[train_df['requester_received_pizza'] == True]['full_text']\n",
    "success_words = get_common_words(successful_texts)\n",
    "for word, count in success_words:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\n=== TOP WORDS IN UNSUCCESSFUL REQUESTS ===\")\n",
    "unsuccessful_texts = train_df[train_df['requester_received_pizza'] == False]['full_text']\n",
    "fail_words = get_common_words(unsuccessful_texts)\n",
    "for word, count in fail_words:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Analyze sentiment indicators (simple keyword-based)\n",
    "sentiment_keywords = {\n",
    "    'gratitude': ['thanks', 'thank', 'grateful', 'appreciate', 'bless', 'blessed'],\n",
    "    'desperation': ['desperate', 'starving', 'hungry', 'broke', 'poor', 'need', 'help'],\n",
    "    'storytelling': ['because', 'since', 'when', 'after', 'before', 'story'],\n",
    "    'promise': ['promise', 'pay', 'forward', 'return', 'back', 'repay']\n",
    "}\n",
    "\n",
    "print(\"\\n=== SENTIMENT KEYWORD ANALYSIS ===\")\n",
    "for sentiment, keywords in sentiment_keywords.items():\n",
    "    print(f\"\\n{sentiment.upper()} keywords:\")\n",
    "    for keyword in keywords:\n",
    "        success_count = successful_texts.str.contains(keyword, case=False, na=False).sum()\n",
    "        fail_count = unsuccessful_texts.str.contains(keyword, case=False, na=False).sum()\n",
    "        success_rate = success_count / len(successful_texts) if len(successful_texts) > 0 else 0\n",
    "        fail_rate = fail_count / len(unsuccessful_texts) if len(unsuccessful_texts) > 0 else 0\n",
    "        print(f\"  {keyword:12s}: Success={success_rate:.3f}, Fail={fail_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac21e7a",
   "metadata": {},
   "source": [
    "## 6. User Activity Patterns\n",
    "\n",
    "Analyze legitimate user activity features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze user activity patterns\n",
    "print(\"=== USER ACTIVITY ANALYSIS ===\")\n",
    "\n",
    "activity_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "for feature in activity_features:\n",
    "    if feature in train_df.columns:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        stats = train_df.groupby('requester_received_pizza')[feature].agg(['mean', 'median', 'std']).round(2)\n",
    "        stats.index = ['No Pizza', 'Pizza']\n",
    "        print(stats)\n",
    "\n",
    "# Account age analysis\n",
    "print(\"\\n=== ACCOUNT AGE ANALYSIS ===\")\n",
    "age_features = [\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_days_since_first_post_on_raop_at_request'\n",
    "]\n",
    "\n",
    "for feature in age_features:\n",
    "    if feature in train_df.columns:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        stats = train_df.groupby('requester_received_pizza')[feature].agg(['mean', 'median', 'std']).round(2)\n",
    "        stats.index = ['No Pizza', 'Pizza']\n",
    "        print(stats)\n",
    "\n",
    "# Temporal analysis\n",
    "print(\"\\n=== TEMPORAL ANALYSIS ===\")\n",
    "if 'unix_timestamp_of_request' in train_df.columns:\n",
    "    train_df['request_datetime'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s')\n",
    "    train_df['request_hour'] = train_df['request_datetime'].dt.hour\n",
    "    train_df['request_dayofweek'] = train_df['request_datetime'].dt.dayofweek\n",
    "    \n",
    "    print(\"\\nHour of day distribution:\")\n",
    "    hour_stats = train_df.groupby('request_hour')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "    print(hour_stats.head(10))\n",
    "    \n",
    "    print(\"\\nDay of week distribution:\")\n",
    "    day_stats = train_df.groupby('request_dayofweek')['requester_received_pizza'].agg(['count', 'mean']).round(3)\n",
    "    print(day_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf4259",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "**SAFE FEATURES (No Leakage):**\n",
    "- Text: request_title, request_text, request_text_edit_aware\n",
    "- User activity: comments/posts/upvotes at request time\n",
    "- Account age: days since account creation, days since first RAOP post\n",
    "- Temporal: timestamp, hour, day of week\n",
    "- Subreddit activity: subreddits user posts in\n",
    "\n",
    "**LEAKAGE FEATURES (Must Exclude):**\n",
    "- requester_user_flair (post-outcome reward)\n",
    "- giver_username_if_known (only known after pizza given)\n",
    "- All _at_retrieval features (collected after outcome)\n",
    "\n",
    "**Key Insights:**\n",
    "- Successful requests tend to be longer (more words)\n",
    "- Gratitude keywords ('thanks', 'appreciate') more common in successful requests\n",
    "- Storytelling elements may be important\n",
    "- Account age and activity levels show some differences\n",
    "- Temporal patterns may exist (hour/day of week)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
