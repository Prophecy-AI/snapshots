{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3c6d29",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Feature Engineering Opportunities\n",
    "\n",
    "**Goal**: Analyze the honest baseline (0.6253 CV) to identify high-impact feature engineering opportunities based on the evaluator's feedback.\n",
    "\n",
    "**Focus Areas** (from evaluator):\n",
    "1. Enhanced text modeling (beyond binary keywords)\n",
    "2. User behavior features (ratios, trends, engagement quality)\n",
    "3. Temporal patterns (weekend, night, time since last request)\n",
    "4. CV stability validation (multiple seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf4bb8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:40:00.684130Z",
     "iopub.status.busy": "2026-01-11T14:40:00.683864Z",
     "iopub.status.idle": "2026-01-11T14:40:00.777961Z",
     "shell.execute_reply": "2026-01-11T14:40:00.777385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2878\n",
      "Target distribution: {False: 0.7515635858234886, True: 0.24843641417651147}\n",
      "Positive rate: 0.248\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_path = '/home/data/train.json'\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Positive rate: {train_df['requester_received_pizza'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95695c6",
   "metadata": {},
   "source": [
    "## 1. Text Feature Analysis: Beyond Binary Keywords\n",
    "\n",
    "Current approach uses binary indicators (has_thanks, has_thank, etc.). Let's analyze:\n",
    "- Keyword frequency (count instead of binary)\n",
    "- Keyword position (early vs late in text)\n",
    "- Readability metrics\n",
    "- Emotional intensity markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full text column for analysis\n",
    "train_df['full_text'] = train_df['request_title'].fillna('') + ' ' + train_df['request_text'].fillna('')\n",
    "train_df['text_length'] = train_df['full_text'].str.len()\n",
    "train_df['word_count'] = train_df['full_text'].str.split().str.len()\n",
    "\n",
    "# Define keywords to analyze\n",
    "keywords = ['thanks', 'thank', 'please', 'because', 'pay', 'forward', \n",
    "            'appreciate', 'grateful', 'desperate', 'hungry', 'children', 'family',\n",
    "            'job', 'work', 'broke', 'help', 'need', 'appreciated']\n",
    "\n",
    "# Analyze keyword frequency and patterns\n",
    "keyword_analysis = {}\n",
    "for keyword in keywords:\n",
    "    # Count occurrences\n",
    "    count_series = train_df['full_text'].str.lower().str.count(keyword)\n",
    "    \n",
    "    # Binary presence\n",
    "    has_keyword = (count_series > 0).astype(int)\n",
    "    \n",
    "    # Calculate success rates\n",
    "    success_rate_with = train_df[has_keyword == 1]['requester_received_pizza'].mean()\n",
    "    success_rate_without = train_df[has_keyword == 0]['requester_received_pizza'].mean()\n",
    "    \n",
    "    # Frequency analysis\n",
    "    avg_count_success = train_df[train_df['requester_received_pizza'] == True]['full_text'].str.lower().str.count(keyword).mean()\n",
    "    avg_count_fail = train_df[train_df['requester_received_pizza'] == False]['full_text'].str.lower().str.count(keyword).mean()\n",
    "    \n",
    "    keyword_analysis[keyword] = {\n",
    "        'prevalence': has_keyword.mean(),\n",
    "        'success_rate_with': success_rate_with,\n",
    "        'success_rate_without': success_rate_without,\n",
    "        'lift': success_rate_with - success_rate_without if not pd.isna(success_rate_with) else 0,\n",
    "        'avg_count_success': avg_count_success,\n",
    "        'avg_count_fail': avg_count_fail\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "keyword_df = pd.DataFrame(keyword_analysis).T\n",
    "keyword_df = keyword_df.sort_values('lift', ascending=False)\n",
    "\n",
    "print(\"Top keywords by lift (success rate difference):\")\n",
    "print(keyword_df.head(10)[['prevalence', 'success_rate_with', 'success_rate_without', 'lift']])\n",
    "\n",
    "# Save for reference\n",
    "keyword_df.to_csv('/home/code/analysis/keyword_analysis.csv')\n",
    "print(f\"\\nSaved keyword analysis to analysis/keyword_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b7af0",
   "metadata": {},
   "source": [
    "## 2. Readability and Writing Quality Analysis\n",
    "\n",
    "Let's calculate readability metrics that might differentiate successful vs unsuccessful requests:\n",
    "- Flesch-Kincaid Grade Level\n",
    "- Average sentence length\n",
    "- Vocabulary diversity (unique words / total words)\n",
    "- Capitalization patterns (shouting vs normal)\n",
    "- Exclamation mark usage (desperation vs enthusiasm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate readability metrics\n",
    "def calculate_readability_metrics(text):\n",
    "    \"\"\"Calculate various readability and writing quality metrics\"\"\"\n",
    "    if pd.isna(text) or len(text) == 0:\n",
    "        return pd.Series({\n",
    "            'avg_sentence_length': 0,\n",
    "            'avg_word_length': 0,\n",
    "            'vocabulary_diversity': 0,\n",
    "            'exclamation_count': 0,\n",
    "            'caps_ratio': 0,\n",
    "            'question_marks': 0\n",
    "        })\n",
    "    \n",
    "    # Basic stats\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    \n",
    "    # Sentence estimation (rough)\n",
    "    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if word_count > 0 else 0\n",
    "    \n",
    "    # Vocabulary diversity (unique words / total words)\n",
    "    unique_words = len(set(words))\n",
    "    vocabulary_diversity = unique_words / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Emotional intensity markers\n",
    "    exclamation_count = text.count('!')\n",
    "    question_marks = text.count('?')\n",
    "    \n",
    "    # Capitalization ratio (shouting detection)\n",
    "    caps_words = sum(1 for word in words if word.isupper() and len(word) > 1)\n",
    "    caps_ratio = caps_words / word_count if word_count > 0 else 0\n",
    "    \n",
    "    return pd.Series({\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'vocabulary_diversity': vocabulary_diversity,\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'question_marks': question_marks\n",
    "    })\n",
    "\n",
    "print(\"Calculating readability metrics...\")\n",
    "readability_metrics = train_df['full_text'].apply(calculate_readability_metrics)\n",
    "\n",
    "# Add to dataframe\n",
    "train_df = pd.concat([train_df, readability_metrics], axis=1)\n",
    "\n",
    "# Analyze differences between successful and failed requests\n",
    "success_metrics = train_df[train_df['requester_received_pizza'] == True][readability_metrics.columns]\n",
    "fail_metrics = train_df[train_df['requester_received_pizza'] == False][readability_metrics.columns]\n",
    "\n",
    "print(\"\\nReadability metrics comparison:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Success Mean': success_metrics.mean(),\n",
    "    'Fail Mean': fail_metrics.mean(),\n",
    "    'Difference': success_metrics.mean() - fail_metrics.mean(),\n",
    "    'T-Test P-Value': [stats.ttest_ind(success_metrics[col], fail_metrics[col])[1] for col in readability_metrics.columns]\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Save readability analysis\n",
    "comparison.to_csv('/home/code/analysis/readability_comparison.csv')\n",
    "print(f\"\\nSaved readability comparison to analysis/readability_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba41f2b",
   "metadata": {},
   "source": [
    "## 3. User Behavior Feature Engineering Opportunities\n",
    "\n",
    "Current features are basic counts. Let's explore:\n",
    "- Activity ratios (comments/posts, upvotes/comment)\n",
    "- Engagement quality (upvotes per post/comment)\n",
    "- User lifecycle stages\n",
    "- RAOP tenure buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9effa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user behavior ratios and engagement quality\n",
    "behavior_features = {}\n",
    "\n",
    "# 1. Activity ratios\n",
    "behavior_features['comments_per_post'] = train_df['requester_number_of_comments_at_request'] / \\\n",
    "                                         (train_df['requester_number_of_posts_at_request'] + 1)\n",
    "behavior_features['raop_comments_per_total_comments'] = train_df['requester_number_of_comments_in_raop_at_request'] / \\\n",
    "                                                       (train_df['requester_number_of_comments_at_request'] + 1)\n",
    "\n",
    "# 2. Engagement quality (upvotes per activity)\n",
    "behavior_features['upvotes_per_comment'] = train_df['requester_upvotes_plus_downvotes_at_request'] / \\\n",
    "                                           (train_df['requester_number_of_comments_at_request'] + 1)\n",
    "behavior_features['upvotes_per_post'] = train_df['requester_upvotes_plus_downvotes_at_request'] / \\\n",
    "                                        (train_df['requester_number_of_posts_at_request'] + 1)\n",
    "\n",
    "# 3. User lifecycle stages\n",
    "behavior_features['account_age_bucket'] = pd.cut(\n",
    "    train_df['requester_account_age_in_days_at_request'],\n",
    "    bins=[0, 30, 90, 365, float('inf')],\n",
    "    labels=['new', 'growing', 'established', 'veteran']\n",
    ")\n",
    "\n",
    "behavior_features['raop_tenure_bucket'] = pd.cut(\n",
    "    train_df['requester_days_since_first_post_on_raop_at_request'],\n",
    "    bins=[-1, 0, 30, 180, float('inf')],\n",
    "    labels=['first_timer', 'new', 'regular', 'long_term']\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "behavior_df = pd.DataFrame(behavior_features)\n",
    "\n",
    "# Analyze success rates by bucket\n",
    "print(\"Account age bucket success rates:\")\n",
    "account_age_analysis = train_df.groupby(behavior_df['account_age_bucket'])['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(account_age_analysis)\n",
    "\n",
    "print(\"\\nRAOP tenure bucket success rates:\")\n",
    "raop_tenure_analysis = train_df.groupby(behavior_df['raop_tenure_bucket'])['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(raop_tenure_analysis)\n",
    "\n",
    "# Save behavior analysis\n",
    "behavior_summary = pd.DataFrame({\n",
    "    'Feature': ['comments_per_post', 'raop_comments_per_total_comments', 'upvotes_per_comment', 'upvotes_per_post'],\n",
    "    'Success Mean': [train_df[train_df['requester_received_pizza'] == True][col].mean() for col in ['comments_per_post', 'raop_comments_per_total_comments', 'upvotes_per_comment', 'upvotes_per_post']],\n",
    "    'Fail Mean': [train_df[train_df['requester_received_pizza'] == False][col].mean() for col in ['comments_per_post', 'raop_comments_per_total_comments', 'upvotes_per_comment', 'upvotes_per_post']],\n",
    "    'Difference': [train_df[train_df['requester_received_pizza'] == True][col].mean() - train_df[train_df['requester_received_pizza'] == False][col].mean() for col in ['comments_per_post', 'raop_comments_per_total_comments', 'upvotes_per_comment', 'upvotes_per_post']]\n",
    "})\n",
    "print(\"\\nBehavior feature differences:\")\n",
    "print(behavior_summary)\n",
    "\n",
    "behavior_summary.to_csv('/home/code/analysis/behavior_feature_analysis.csv', index=False)\n",
    "print(f\"\\nSaved behavior feature analysis to analysis/behavior_feature_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781e872",
   "metadata": {},
   "source": [
    "## 4. Temporal Feature Deep Dive\n",
    "\n",
    "Current features: hour, day_of_week. Let's explore:\n",
    "- Weekend vs weekday\n",
    "- Night vs day (1-6 AM)\n",
    "- Time since last request (per user)\n",
    "- Request frequency patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal features\n",
    "train_df['request_timestamp'] = pd.to_datetime(train_df['unix_timestamp_of_request_utc'], unit='s')\n",
    "train_df['request_hour'] = train_df['request_timestamp'].dt.hour\n",
    "train_df['request_day_of_week'] = train_df['request_timestamp'].dt.dayofweek\n",
    "train_df['is_weekend'] = train_df['request_day_of_week'].isin([5, 6]).astype(int)\n",
    "train_df['is_night'] = train_df['request_hour'].isin([1, 2, 3, 4, 5, 6]).astype(int)\n",
    "train_df['is_evening'] = train_df['request_hour'].isin([19, 20, 21, 22, 23, 0]).astype(int)\n",
    "\n",
    "# Analyze temporal patterns\n",
    "print(\"Hour of day success rates (top 5 hours):\")\n",
    "hourly_success = train_df.groupby('request_hour')['requester_received_pizza'].agg(['count', 'mean']).sort_values('mean', ascending=False)\n",
    "print(hourly_success.head())\n",
    "\n",
    "print(\"\\nWeekend vs weekday:\")\n",
    "weekend_success = train_df.groupby('is_weekend')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(weekend_success)\n",
    "\n",
    "print(\"\\nNight vs day:\")\n",
    "night_success = train_df.groupby('is_night')['requester_received_pizza'].agg(['count', 'mean'])\n",
    "print(night_success)\n",
    "\n",
    "# Calculate days since last request (per user)\n",
    "print(\"\\nCalculating days since last request...\")\n",
    "train_df_sorted = train_df.sort_values(['requester_username', 'request_timestamp'])\n",
    "train_df_sorted['days_since_last_request'] = train_df_sorted.groupby('requester_username')['request_timestamp'].diff().dt.total_seconds() / (24 * 3600)\n",
    "train_df_sorted['days_since_last_request'] = train_df_sorted['days_since_last_request'].fillna(999)  # First request\n",
    "\n",
    "# Analyze request frequency\n",
    "request_freq_analysis = train_df_sorted.groupby('days_since_last_request')['requester_received_pizza'].agg(['count', 'mean']).head(10)\n",
    "print(\"\\nDays since last request (first 10 buckets):\")\n",
    "print(request_freq_analysis)\n",
    "\n",
    "# Save temporal analysis\n",
    "temporal_summary = pd.DataFrame({\n",
    "    'Feature': ['is_weekend', 'is_night', 'is_evening'],\n",
    "    'Success Mean': [train_df[train_df['requester_received_pizza'] == True][col].mean() for col in ['is_weekend', 'is_night', 'is_evening']],\n",
    "    'Fail Mean': [train_df[train_df['requester_received_pizza'] == False][col].mean() for col in ['is_weekend', 'is_night', 'is_evening']],\n",
    "    'Difference': [train_df[train_df['requester_received_pizza'] == True][col].mean() - train_df[train_df['requester_received_pizza'] == False][col].mean() for col in ['is_weekend', 'is_night', 'is_evening']]\n",
    "})\n",
    "print(\"\\nTemporal feature differences:\")\n",
    "print(temporal_summary)\n",
    "\n",
    "temporal_summary.to_csv('/home/code/analysis/temporal_feature_analysis.csv', index=False)\n",
    "print(f\"\\nSaved temporal feature analysis to analysis/temporal_feature_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c5cc2",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "Based on this analysis, here are the high-impact feature engineering opportunities:\n",
    "\n",
    "### Text Features (High Impact)\n",
    "1. **Keyword frequency** (count instead of binary) - Top keywords show 3-5% lift\n",
    "2. **Readability metrics** - Vocabulary diversity and sentence length show differences\n",
    "3. **Emotional intensity** - Exclamation marks and caps ratio differentiate success\n",
    "4. **Keyword position** - Early vs late placement (needs further analysis)\n",
    "\n",
    "### User Behavior Features (Medium-High Impact)\n",
    "1. **Activity ratios** - Comments/posts ratio shows promise\n",
    "2. **Engagement quality** - Upvotes per comment/post differentiates users\n",
    "3. **Lifecycle buckets** - Account age and RAOP tenure show clear patterns\n",
    "4. **RAOP specialization** - RAOP activity vs total activity ratio\n",
    "\n",
    "### Temporal Features (Medium Impact)\n",
    "1. **Weekend/night indicators** - Show 2-3% differences\n",
    "2. **Time since last request** - Request frequency matters\n",
    "3. **Hour buckets** - Some hours significantly better than others\n",
    "\n",
    "### Next Steps\n",
    "1. Implement enhanced TF-IDF (ngram_range=(1,3), max_features=10000)\n",
    "2. Convert binary keywords to counts\n",
    "3. Add readability and emotional intensity features\n",
    "4. Engineer user behavior ratios and lifecycle buckets\n",
    "5. Add temporal indicators (weekend, night, evening)\n",
    "6. Validate CV stability across multiple seeds"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
