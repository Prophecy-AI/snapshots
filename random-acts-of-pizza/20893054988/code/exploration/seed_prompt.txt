## Competition Overview
This is a binary classification problem predicting whether a Reddit pizza request will be successful. The dataset contains 2,878 training samples with 32 features including text (request title and body) and meta-features (user activity, account age, votes, etc.). Target distribution is imbalanced: ~75% unsuccessful, ~25% successful. Evaluation metric is ROC AUC.

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, correlations, text length statistics, and data quality issues

Key findings from EDA:
- Text features: request_title, request_text, request_text_edit_aware
- Meta features: user activity metrics (comments, posts, upvotes), account age, timestamps
- Data quality issue: post_was_edited column contains timestamps instead of booleans
- Strong predictive signal: requester_user_flair (None/shroom/PIF) highly correlated with target
- Text length varies significantly: request_text (0-4460 chars), request_title (7-272 chars)

## Text Feature Engineering
Based on winning Kaggle approaches for Reddit/social media text classification:

1. **Text Cleaning and Preprocessing:**
   - Combine request_title and request_text for full context
   - Remove URLs, special characters, Reddit markdown
   - Handle Reddit-specific tokens (usernames, subreddit mentions)
   - Lowercase text, remove stopwords, consider lemmatization
   - Use Reddit-aware tokenizers that preserve platform-specific features

2. **Text Vectorization Approaches:**
   - TF-IDF with n-grams (unigrams, bigrams) for baseline models
   - FastText embeddings for capturing semantic meaning
   - For deep learning: Use pretrained language models (BERT, RoBERTa) fine-tuned on this task
   - Consider character-level features for handling misspellings and informal text

3. **Text Meta-Features:**
   - Text length (character count, word count, sentence count)
   - Readability scores (Flesch-Kincaid, etc.)
   - Punctuation density and patterns
   - Sentiment analysis scores
   - Presence of specific keywords ("please", "thank you", "desperate", etc.)
   - Use of capitalization and exclamation marks

## Meta Feature Engineering
1. **User Activity Features:**
   - Normalize activity metrics by account age
   - Create ratios: comments/posts, upvotes/downvotes
   - Difference between request and retrieval time metrics
   - Activity on RAOP vs overall Reddit activity

2. **Temporal Features:**
   - Extract day of week, hour of day from timestamps
   - Time since first RAOP post
   - Account age categories

3. **Categorical Encoding:**
   - requester_user_flair: One-hot or target encoding (strong signal)
   - post_was_edited: Boolean (after cleaning timestamp issue)
   - Handle missing values in flair appropriately

## Modeling Approaches
1. **Gradient Boosting (Primary Approach):**
   - LightGBM/XGBoost on engineered features
   - Works well with mixed data types (text meta-features + numerical features)
   - Handle class imbalance with scale_pos_weight or focal loss
   - Optuna for hyperparameter tuning

2. **Deep Learning Approaches:**
   - LSTM/GRU with pretrained embeddings (GloVe, FastText)
   - BERT/RoBERTa fine-tuning for text understanding
   - Hybrid models: Text embeddings + meta-features concatenated
   - Use focal loss for handling class imbalance

3. **Baseline Models:**
   - Logistic Regression with TF-IDF for quick validation
   - Linear SVM with n-gram features
   - Random Forest for feature importance analysis

## Ensembling Strategies
Based on Kaggle winning solutions for text classification:

1. **Model Diversity:**
   - Combine gradient boosting (tree-based) with neural networks
   - Different text vectorization approaches (TF-IDF, FastText, BERT)
   - Vary model architectures and hyperparameters

2. **Ensembling Methods:**
   - **Weighted Averaging:** Blend predictions from different models based on validation performance
   - **Stacking:** Use meta-learner (LightGBM/NN) on out-of-fold predictions from base models
   - Include engineered meta-features in stacking layer
   - **Rank Averaging:** Particularly effective for AUC metric

3. **Validation Strategy:**
   - Stratified K-Fold (k=5) to preserve target distribution
   - Time-based splits if temporal leakage is a concern
   - Use early stopping based on validation AUC
   - Monitor both AUC and calibration of probability predictions

## Handling Class Imbalance
- Use scale_pos_weight in gradient boosting
- Focal loss for neural networks
- Consider SMOTE or undersampling for baseline models
- Focus on AUC metric which is robust to imbalance
- Calibration of probability predictions is important

## Key Success Factors
1. **Leverage requester_user_flair:** This is a very strong feature (shroom = received pizza before)
2. **Text quality matters:** Edit-aware text cleaning improves performance
3. **Meta-features add signal:** User activity patterns and request characteristics help
4. **Ensembling is crucial:** Combine multiple approaches for best performance
5. **Handle data quality issues:** Clean post_was_edited column properly