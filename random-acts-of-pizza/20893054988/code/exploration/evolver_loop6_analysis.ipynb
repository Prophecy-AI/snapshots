{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d5f388",
   "metadata": {},
   "source": [
    "# Evolver Loop 6 Analysis: Diagnosing DistilBERT Underperformance\n",
    "\n",
    "**Objective**: Understand why DistilBERT feature extraction only improved AUC by +0.0059 (0.6312 vs 0.6253 baseline) and design path forward.\n",
    "\n",
    "**Key question**: Why did transformers underperform expectations, and what specific techniques will unlock their potential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42ba64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T21:12:33.599786Z",
     "iopub.status.busy": "2026-01-11T21:12:33.598181Z",
     "iopub.status.idle": "2026-01-11T21:12:35.287294Z",
     "shell.execute_reply": "2026-01-11T21:12:35.286395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT PERFORMANCE COMPARISON\n",
      "======================================================================\n",
      "                          Experiment  CV_AUC  Improvement  Relative_Improvement  Variance\n",
      "           exp_001 (TF-IDF baseline)  0.6253       0.0000                  0.00    0.0334\n",
      "exp_006 (DistilBERT feature extract)  0.6312       0.0059                  0.94    0.0236\n",
      "\n",
      "Expected improvement: +0.03 to +0.05 AUC\n",
      "Actual improvement: +0.0059 AUC\n",
      "Shortfall: 0.0241 to 0.0441 AUC\n",
      "Performance: 19.7% to 11.8% of expected range\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load experiment results for analysis\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {\n",
    "    'Experiment': ['exp_001 (TF-IDF baseline)', 'exp_006 (DistilBERT feature extract)'],\n",
    "    'CV_AUC': [0.6253, 0.6312],\n",
    "    'Improvement': [0.0, 0.0059],\n",
    "    'Relative_Improvement': [0.0, 0.94],\n",
    "    'Variance': [0.0334, 0.0236]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "print()\n",
    "print(f\"Expected improvement: +0.03 to +0.05 AUC\")\n",
    "print(f\"Actual improvement: +0.0059 AUC\")\n",
    "print(f\"Shortfall: {0.03-0.0059:.4f} to {0.05-0.0059:.4f} AUC\")\n",
    "print(f\"Performance: {0.0059/0.03:.1%} to {0.0059/0.05:.1%} of expected range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4165637",
   "metadata": {},
   "source": [
    "## 1. Root Cause Analysis\n",
    "\n",
    "Based on research and experiment results, identify why DistilBERT underperformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f83359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T21:12:36.331357Z",
     "iopub.status.busy": "2026-01-11T21:12:36.330535Z",
     "iopub.status.idle": "2026-01-11T21:12:36.341054Z",
     "shell.execute_reply": "2026-01-11T21:12:36.340004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROOT CAUSE ANALYSIS: DISTILBERT UNDERPERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "1. Feature extraction (frozen embeddings)\n",
      "   Impact: High - Embeddings don't adapt to pizza request domain\n",
      "   Evidence: Research shows frozen embeddings underperform on specific tasks\n",
      "   Fix: Fine-tune with classification head (2-3 epochs)\n",
      "\n",
      "2. [CLS] token pooling loses information\n",
      "   Impact: Medium - Discards token-level patterns\n",
      "   Evidence: Token-level patterns (keywords, sentiment) lost in pooling\n",
      "   Fix: Use mean pooling or attention-weighted pooling\n",
      "\n",
      "3. No task-specific fine-tuning\n",
      "   Impact: High - No learning from task labels\n",
      "   Evidence: No gradient updates from classification loss\n",
      "   Fix: End-to-end fine-tuning on pizza request task\n",
      "\n",
      "4. LightGBM suboptimal for dense features\n",
      "   Impact: Medium - Tree models prefer sparse features\n",
      "   Evidence: TF-IDF sparse features work better with LightGBM\n",
      "   Fix: Try neural classifier (MLP) or keep LightGBM\n",
      "\n",
      "5. DistilBERT capacity limitations\n",
      "   Impact: Low-Medium - 66M params vs 110M BERT\n",
      "   Evidence: DistilBERT is 40% smaller than BERT base\n",
      "   Fix: Upgrade to RoBERTa or DeBERTa if needed\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ROOT CAUSE ANALYSIS: DISTILBERT UNDERPERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "root_causes = {\n",
    "    'Cause': [\n",
    "        'Feature extraction (frozen embeddings)',\n",
    "        '[CLS] token pooling loses information',\n",
    "        'No task-specific fine-tuning',\n",
    "        'LightGBM suboptimal for dense features',\n",
    "        'DistilBERT capacity limitations'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        'High - Embeddings don\\'t adapt to pizza request domain',\n",
    "        'Medium - Discards token-level patterns',\n",
    "        'High - No learning from task labels',\n",
    "        'Medium - Tree models prefer sparse features',\n",
    "        'Low-Medium - 66M params vs 110M BERT'\n",
    "    ],\n",
    "    'Evidence': [\n",
    "        'Research shows frozen embeddings underperform on specific tasks',\n",
    "        'Token-level patterns (keywords, sentiment) lost in pooling',\n",
    "        'No gradient updates from classification loss',\n",
    "        'TF-IDF sparse features work better with LightGBM',\n",
    "        'DistilBERT is 40% smaller than BERT base'\n",
    "    ],\n",
    "    'Fix_Strategy': [\n",
    "        'Fine-tune with classification head (2-3 epochs)',\n",
    "        'Use mean pooling or attention-weighted pooling',\n",
    "        'End-to-end fine-tuning on pizza request task',\n",
    "        'Try neural classifier (MLP) or keep LightGBM',\n",
    "        'Upgrade to RoBERTa or DeBERTa if needed'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_causes = pd.DataFrame(root_causes)\n",
    "for i, row in df_causes.iterrows():\n",
    "    print(f\"\\n{i+1}. {row['Cause']}\")\n",
    "    print(f\"   Impact: {row['Impact']}\")\n",
    "    print(f\"   Evidence: {row['Evidence']}\")\n",
    "    print(f\"   Fix: {row['Fix_Strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71a581",
   "metadata": {},
   "source": [
    "## 2. Research-Informed Solutions\n",
    "\n",
    "Based on web research, identify proven techniques for improving transformer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESEARCH-BACKED SOLUTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "solutions = {\n",
    "    'Solution': [\n",
    "        'Fine-tune DistilBERT (2-3 epochs)',\n",
    "        'Freeze lower layers, train head only',\n",
    "        'Use low learning rate (2e-5)',\n",
    "        'Early stopping (patience=1)',\n",
    "        'Ensemble TF-IDF + transformer',\n",
    "        'Add enhanced meta-features',\n",
    "        'Try RoBERTa base (125M params)',\n",
    "        'Mean pooling instead of [CLS]'\n",
    "    ],\n",
    "    'Expected_Gain': [0.03, 0.02, 'Stability', 'Prevent overfit', 0.015, 0.02, 0.02, 0.01],\n",
    "    'Risk_Level': ['Medium', 'Low', 'Low', 'Low', 'Low', 'Low', 'Medium', 'Low'],\n",
    "    'Implementation_Time': ['3 hours', '2 hours', 'Included', 'Included', '1 hour', '2 hours', '2 hours', '1 hour'],\n",
    "    'Research_Source': [\n",
    "        'Multiple sources: fine-tuning adapts to task',\n",
    "        'Freezing reduces overfit on small data',\n",
    "        'Standard practice for transformer fine-tuning',\n",
    "        'Critical for small datasets (<3000 samples)',\n",
    "        'Kaggle winners: TF-IDF often gets 0.8 weight',\n",
    "        'Readability, emotional intensity features',\n",
    "        'Larger model, better performance than DistilBERT',\n",
    "        'Captures token-level information better'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_solutions = pd.DataFrame(solutions)\n",
    "for i, row in df_solutions.iterrows():\n",
    "    print(f\"\\n{i+1}. {row['Solution']}\")\n",
    "    print(f\"   Expected gain: {row['Expected_Gain']}\")\n",
    "    print(f\"   Risk: {row['Risk_Level']}\")\n",
    "    print(f\"   Time: {row['Implementation_Time']}\")\n",
    "    print(f\"   Source: {row['Research_Source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cb3e0",
   "metadata": {},
   "source": [
    "## 3. Recommended Next Steps\n",
    "\n",
    "Based on analysis, prioritize approaches with best risk/reward ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0326a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PRIORITIZED ACTION PLAN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "action_plan = {\n",
    "    'Priority': [1, 2, 3, 4, 5],\n",
    "    'Action': [\n",
    "        'Fine-tune DistilBERT with classification head',\n",
    "        'Add enhanced meta-features (readability, emotion)',\n",
    "        'Ensemble TF-IDF + fine-tuned DistilBERT',\n",
    "        'Try RoBERTa if DistilBERT plateaus',\n",
    "        'Experiment with pooling strategies'\n",
    "    ],\n",
    "    'Expected_CV': [0.65, 0.64, 0.66, 0.67, 0.645],\n",
    "    'Confidence': ['High', 'Medium', 'High', 'Medium', 'Low'],\n",
    "    'Rationale': [\n",
    "        'Direct fix for root cause - task adaptation',\n",
    "        'Builds on proven meta-feature approach',\n",
    "        'Kaggle winners use this combination',\n",
    "        'Larger model if needed',\n",
    "        'Experimental, lower priority'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_plan = pd.DataFrame(action_plan)\n",
    "print(df_plan.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPECTED TRAJECTORY\")\n",
    "print(\"=\"*70)\n",
    "trajectory = [\n",
    "    (\"Current\", 0.6312),\n",
    "    (\"After fine-tuning\", 0.65),\n",
    "    (\"After enhanced features\", 0.66),\n",
    "    (\"After ensembling\", 0.68),\n",
    "    (\"After RoBERTa upgrade\", 0.70)\n",
    "]\n",
    "\n",
    "for stage, score in trajectory:\n",
    "    print(f\"{stage:20s}: {score:.4f} AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05cb892",
   "metadata": {},
   "source": [
    "## 4. Risk Mitigation\n",
    "\n",
    "Identify and mitigate risks for each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d128d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RISK MITIGATION STRATEGY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "risks = {\n",
    "    'Risk': [\n",
    "        'Overfitting on small dataset (2878 samples)',\n",
    "        'No improvement from fine-tuning',\n",
    "        'Computational constraints (time limit)',\n",
    "        'Ensembling complexity',\n",
    "        'RoBERTa too slow for iteration'\n",
    "    ],\n",
    "    'Probability': ['Medium', 'Low', 'Medium', 'Low', 'Medium'],\n",
    "    'Impact': ['High', 'Medium', 'High', 'Low', 'Medium'],\n",
    "    'Mitigation': [\n",
    "        'Freeze lower layers, early stopping, max 3 epochs',\n",
    "        'Try RoBERTa or enhanced meta-features instead',\n",
    "        'Use DistilBERT (faster), batch_size=16, 2-3 epochs max',\n",
    "        'Simple weighted average first, stacking later',\n",
    "        'Only if DistilBERT clearly plateaus'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_risks = pd.DataFrame(risks)\n",
    "for i, row in df_risks.iterrows():\n",
    "    print(f\"\\n{i+1}. {row['Risk']}\")\n",
    "    print(f\"   Probability: {row['Probability']}, Impact: {row['Impact']}\")\n",
    "    print(f\"   Mitigation: {row['Mitigation']}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
