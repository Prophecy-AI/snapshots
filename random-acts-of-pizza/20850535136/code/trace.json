{
  "spans": [
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "88207c29476ad00c",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767958491300067738,
      "end_time": 1767958491300341805,
      "duration": 0.000274067,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7f38e836acd0>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.0KB)\\n[FILE] sampleSubmission.csv (13.0KB)\\n[FILE] test.json (2.0MB)\\n[FILE] test.zip (433.1KB)\\n[FILE] train.json (8.4MB)\\n[FILE] train.zip (1.3MB)\", \"is_error\": false}",
        "tool.duration_ms": 0.38552284240722656
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "3a4b564ac53faae5",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767958497144497793,
      "end_time": 1767958497144822555,
      "duration": 0.000324762,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7f38e9db0750>\", \"input\": {\"file_path\": \"/home/data/description.md\"}}",
        "tool.output": "{\"content\": \"# Overview\\n\\n## Description\\n\\n[Get started on this competition through Kaggle Scripts](https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza)\\n\\nIn machine learning, it is often said¬†there are¬†[no free lunches](http://en.wikipedia.org/wiki/No_free_lunch_theorem).¬†*How wrong we were.*\\n\\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community¬†[Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/)¬†together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm¬†capable of predicting¬†which requests will garner a cheesy (but sincere!) act of¬†kindness.\\n\\n\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?\\n\\n![Pizzas](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png)\\n\\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This¬†[data](http://cs.stanford.edu/~althoff/raop-dataset/)¬†was collected and graciously shared by¬†[Althoff](http://www.timalthoff.com/)¬†et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their¬†[accompanying paper](http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf)¬†and ask that you cite the following reference in any publications that result from your work:\\n\\nTim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky.¬†*How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*,¬†Proceedings of ICWSM, 2014.\\n\\n~*Pizza icons designed by Matthew Dera from the¬†[Noun Project](http://thenounproject.com/term/pizza/30194/)*~\\n\\n### Evaluation\\n\\nSubmissions are evaluated on¬†[area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)¬†between the predicted probability that a request will get pizza and the observed outcomes.\\n\\n### Submission File\\n\\nFor each request¬†in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:\\n\\n```\\nrequest_id,requester_received_pizza\\nt3_i8iy4,0\\nt3_1mfqi0,0\\nt3_lclka,0\\n...\\n...\\n```\\n\\n### Citation\\n\\nWill Cukierski. (2014). Random Acts of Pizza. Kaggle. https://kaggle.com/competitions/random-acts-of-pizza\\n\\n\\n# Data\\n\\n## Dataset Description\\n\\n[See, fork, and run a random forest benchmark model through Kaggle Scripts](https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark)\\n\\nThis dataset includes 5671 requests collected from the Reddit community¬†[Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/)¬†between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013).¬†All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known.¬†Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\\n\\nEach JSON¬†entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\\n\\n## Data fields\\n\\n\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).\\n\\n\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.\\n\\n\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.\\n\\n\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).\\n\\n\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".\\n\\n\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.\\n\\n\\\"request_text\\\": Full text of the request.\\n\\n\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".\\n\\n\\\"request_title\\\": Title of the request.\\n\\n\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.\\n\\n\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.\\n\\n\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\\n\\n\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.\\n\\n\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\\n\\n\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).\\n\\n\\\"requester_username\\\": Reddit username of requester.\\n\\n\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\\n\\n\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.\", \"is_error\": false}",
        "tool.duration_ms": 0.37550926208496094
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "dda57fac8cda22e0",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767958514875363679,
      "end_time": 1767958515852410509,
      "duration": 0.97704683,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7f38e8369e90>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\n\\n# Read the JSON file\\ntrain_data = []\\nwith open(train_path, 'r') as f:\\n    for line in f:\\n        train_data.append(json.loads(line))\\n\\ntrain_df = pd.DataFrame(train_data)\\nprint(f\\\"Training data shape: {train_df.shape}\\\")\\nprint(f\\\"Columns: {train_df.columns.tolist()}\\\")\\nprint(\\\"\\\\nFirst few rows:\\\")\\ntrain_df.head()\"}, {\"cell_type\": \"code\", \"content\": \"# Check target distribution\\ntarget_col = 'requester_received_pizza'\\nprint(f\\\"Target distribution:\\\")\\nprint(train_df[target_col].value_counts())\\nprint(f\\\"\\\\nTarget percentages:\\\")\\nprint(train_df[target_col].value_counts(normalize=True) * 100)\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values:\\\")\\nprint(train_df.isnull().sum().sort_values(ascending=False).head(10))\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 977.0684242248535
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "58841cadf5d0ef0d",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958525379646536,
      "end_time": 1767958525979002030,
      "duration": 0.599355494,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 0, \"end_cell\": 1, \"duration\": 60}}",
        "tool.output": "{\"content\": \"Error: JSONDecodeError: Expecting value: line 2 column 1 (char 2)\", \"is_error\": true}",
        "tool.duration_ms": 599.4210243225098
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "196a3648d4254e49",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "BashTool",
      "kind": "INTERNAL",
      "start_time": 1767958531446056721,
      "end_time": 1767958531448871299,
      "duration": 0.002814578,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "BashTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.BashTool object at 0x7f38e836a310>\", \"input\": {\"command\": \"head -20 /home/data/train.json\"}}",
        "tool.output": "{\"content\": \"[\\n    {\\n        \\\"giver_username_if_known\\\": \\\"N/A\\\",\\n        \\\"number_of_downvotes_of_request_at_retrieval\\\": 2,\\n        \\\"number_of_upvotes_of_request_at_retrieval\\\": 5,\\n        \\\"post_was_edited\\\": false,\\n        \\\"request_id\\\": \\\"t3_q8ycf\\\",\\n        \\\"request_number_of_comments_at_retrieval\\\": 0,\\n        \\\"request_text\\\": \\\"I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\\",\\n        \\\"request_text_edit_aware\\\": \\\"I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\\",\\n        \\\"request_title\\\": \\\"[REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\\",\\n        \\\"requester_account_age_in_days_at_request\\\": 0.0,\\n        \\\"requester_account_age_in_days_at_retrieval\\\": 647.2970601851852,\\n        \\\"requester_days_since_first_post_on_raop_at_request\\\": 0.0,\\n        \\\"requester_days_since_first_post_on_raop_at_retrieval\\\": 647.2970601851852,\\n        \\\"requester_number_of_comments_at_request\\\": 0,\\n        \\\"requester_number_of_comments_at_retrieval\\\": 0,\\n        \\\"requester_number_of_comments_in_raop_at_request\\\": 0,\\n        \\\"requester_number_of_comments_in_raop_at_retrieval\\\": 0,\\n        \\\"requester_number_of_posts_at_request\\\": 0,\\n\", \"is_error\": false}",
        "tool.duration_ms": 2.8684139251708984
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "39774f125404a8a8",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "EditNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767958542915354792,
      "end_time": 1767958542916492559,
      "duration": 0.001137767,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "EditNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.EditNotebookCellTool object at 0x7f38e836ae10>\", \"input\": {\"cell_index\": 0, \"new_content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\n\\n# Read the JSON file as a JSON array\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\ntrain_df = pd.DataFrame(train_data)\\nprint(f\\\"Training data shape: {train_df.shape}\\\")\\nprint(f\\\"Columns: {train_df.columns.tolist()}\\\")\\nprint(\\\"\\\\nFirst few rows:\\\")\\ntrain_df.head()\"}}",
        "tool.output": "{\"content\": \"Edited cell 0. Re-run to update kernel state.\", \"is_error\": false}",
        "tool.duration_ms": 1.2218952178955078
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "d7cb59ad85afad40",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958548175251830,
      "end_time": 1767958548246062120,
      "duration": 0.07081029,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 0, \"end_cell\": 0, \"duration\": 60}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\n\\n# Read the JSON file as a JSON array\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\ntrain_df = pd.DataFrame(train_data)\\nprint(f\\\"Training data shape: {train_df.shape}\\\")\\nprint(f\\\"Columns: {train_df.columns.tolist()}\\\")\\nprint(\\\"\\\\nFirst few rows:\\\")\\ntrain_df.head()\\n```\\nOutput:\\nTraining data shape: (2878, 32)\\nColumns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\\nFirst few rows:\\n\\n  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\\\\n0                     N/A                                            2   \\n1                     N/A                                            2   \\n2                     N/A                                            1   \\n3                     N/A                                            1   \\n4                     N/A                                            3   \\n\\n   number_of_upvotes_of_request_at_retrieval post_was_edited request_id  \\\\\\n0                                          5           False   t3_q8ycf   \\n1                                          4           False   t3_ixnia   \\n2                                          2            True   t3_ndy6g   \\n3                                          1    1363315140.0  t3_1abbu1   \\n4                                         14           False   t3_kseg4   \\n\\n   request_number_of_comments_at_retrieval  \\\\\\n0                                        0   \\n1                                       20   \\n2                                        0   \\n3                                       32   \\n4                                        3   \\n\\n                                        request_text  \\\\\\n0  I will soon be going on a long deployment whic...   \\n1  We would all really appreciate it, and would e...   \\n2  It took a lot of courage to make this post, an...   \\n3  I will go ahead and say that I got a pizza mea...   \\n4  My '99 Jeep Cherokee I've had for 10 years now...   \\n\\n                             request_text_edit_aware  \\\\\\n0  I will soon be going on a long deployment whic...   \\n1  We would all really appreciate it, and would e...   \\n2  It took a lot of courage to make this post, an...   \\n3  I will go ahead and say that I got a pizza mea...   \\n4  My '99 Jeep Cherokee I've had for 10 years now...   \\n\\n                                       request_title  \\\\\\n0  [REQUEST] Oceanside, Ca. USA-  US Marine getti...   \\n1  [REQUEST] Three (verified) medical students in...   \\n2  (REQUEST) not home 4 the holidays &amp; would ...   \\n3            [REQUEST] Not much food until tomorrow.   \\n4                  [Request] Had my car stolen today   \\n\\n   requester_account_age_in_days_at_request  ...  requester_received_pizza  \\\\\\n0                                  0.000000  ...                     False   \\n1                                 99.526863  ...                     False   \\n2                                  0.000000  ...                     False   \\n3                                491.088264  ...                      True   \\n4                                369.417558  ...                     False   \\n\\n                     requester_subreddits_at_request  \\\\\\n0                             [Random_Acts_Of_Pizza]   \\n1  [AskReddit, IAmA, TwoXChromosomes, circlejerk,...   \\n2                             [Random_Acts_Of_Pizza]   \\n3  [Entroductions, RandomActsOfChristmas, RandomK...   \\n4  [DetroitRedWings, DoesAnybodyElse, FoodPorn, K...   \\n\\n   requester_upvotes_minus_downvotes_at_request  \\\\\\n0                                             3   \\n1                                           491   \\n2                                             1   \\n3                                            25   \\n4                                           942   \\n\\n   requester_upvotes_minus_downvotes_at_retrieval  \\\\\\n0                                               3   \\n1                                             883   \\n2                                               1   \\n3                                              21   \\n4                                            2043   \\n\\n   requester_upvotes_plus_downvotes_at_request  \\\\\\n0                                            7   \\n1                                         1459   \\n2                                            3   \\n3                                          165   \\n4                                         1906   \\n\\n   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\\\\n0                                              7                  None   \\n1                                           2187                  None   \\n2                                              3                  None   \\n3                                            195                shroom   \\n4                                           3483                  None   \\n\\n     requester_username  unix_timestamp_of_request  \\\\\\n0              SDMarine               1.330391e+09   \\n1  TheycallmeFoxJohnson               1.311434e+09   \\n2         riverfrontmom               1.323968e+09   \\n3              Joeramos               1.363305e+09   \\n4                 m4ngo               1.317088e+09   \\n\\n   unix_timestamp_of_request_utc  \\n0                   1.330391e+09  \\n1                   1.311430e+09  \\n2                   1.323968e+09  \\n3                   1.363301e+09  \\n4                   1.317084e+09  \\n\\n[5 rows x 32 columns]\", \"is_error\": false}",
        "tool.duration_ms": 70.82653045654297
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "b7c351d9acfd4864",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767958558831561013,
      "end_time": 1767958558833460886,
      "duration": 0.001899873,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f38e836a6d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Check target distribution\\ntarget_col = 'requester_received_pizza'\\nprint(f\\\"Target distribution:\\\")\\nprint(train_df[target_col].value_counts())\\nprint(f\\\"\\\\nTarget percentages:\\\")\\nprint(train_df[target_col].value_counts(normalize=True) * 100)\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values:\\\")\\nmissing_counts = train_df.isnull().sum().sort_values(ascending=False)\\nprint(missing_counts[missing_counts > 0])\\n\\n# Check data types\\nprint(f\\\"\\\\nData types:\\\")\\nprint(train_df.dtypes.value_counts())\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 1.9538402557373047
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "ee541be96647da01",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958563354223659,
      "end_time": 1767958563368638502,
      "duration": 0.014414843,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 2, \"end_cell\": 2, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Check target distribution\\ntarget_col = 'requester_received_pizza'\\nprint(f\\\"Target distribution:\\\")\\nprint(train_df[target_col].value_counts())\\nprint(f\\\"\\\\nTarget percentages:\\\")\\nprint(train_df[target_col].value_counts(normalize=True) * 100)\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values:\\\")\\nmissing_counts = train_df.isnull().sum().sort_values(ascending=False)\\nprint(missing_counts[missing_counts > 0])\\n\\n# Check data types\\nprint(f\\\"\\\\nData types:\\\")\\nprint(train_df.dtypes.value_counts())\\n```\\nOutput:\\nTarget distribution:\\nrequester_received_pizza\\nFalse    2163\\nTrue      715\\nName: count, dtype: int64\\n\\nTarget percentages:\\nrequester_received_pizza\\nFalse    75.156359\\nTrue     24.843641\\nName: proportion, dtype: float64\\n\\nMissing values:\\nrequester_user_flair    2163\\ndtype: int64\\n\\nData types:\\nint64      16\\nobject      9\\nfloat64     6\\nbool        1\\nName: count, dtype: int64\\n\", \"is_error\": false}",
        "tool.duration_ms": 14.472723007202148
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "d7350ef89fd1c97a",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767958579564227270,
      "end_time": 1767958579566279454,
      "duration": 0.002052184,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f38e836a6d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze text features\\nprint(\\\"Text feature analysis:\\\")\\nprint(f\\\"\\\\nRequest title length stats:\\\")\\ntrain_df['request_title_length'] = train_df['request_title'].str.len()\\nprint(train_df['request_title_length'].describe())\\n\\nprint(f\\\"\\\\nRequest text length stats:\\\")\\ntrain_df['request_text_length'] = train_df['request_text'].str.len()\\nprint(train_df['request_text_length'].describe())\\n\\nprint(f\\\"\\\\nRequest text edit aware length stats:\\\")\\ntrain_df['request_text_edit_aware_length'] = train_df['request_text_edit_aware'].str.len()\\nprint(train_df['request_text_edit_aware_length'].describe())\\n\\n# Check correlation with target\\nprint(f\\\"\\\\nCorrelation with target:\\\")\\nprint(f\\\"Title length correlation: {train_df['request_title_length'].corr(train_df[target_col].astype(int)):.4f}\\\")\\nprint(f\\\"Text length correlation: {train_df['request_text_length'].corr(train_df[target_col].astype(int)):.4f}\\\")\\nprint(f\\\"Text edit aware length correlation: {train_df['request_text_edit_aware_length'].corr(train_df[target_col].astype(int)):.4f}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 3\", \"is_error\": false}",
        "tool.duration_ms": 2.112865447998047
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "a52950272be9d220",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958586302608118,
      "end_time": 1767958586321461665,
      "duration": 0.018853547,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 3, \"end_cell\": 3, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 3-3\\n==================================================\\n\\nCell 3:\\n```\\n# Analyze text features\\nprint(\\\"Text feature analysis:\\\")\\nprint(f\\\"\\\\nRequest title length stats:\\\")\\ntrain_df['request_title_length'] = train_df['request_title'].str.len()\\nprint(train_df['request_title_length'].describe())\\n\\nprint(f\\\"\\\\nRequest text length stats:\\\")\\ntrain_df['request_text_length'] = train_df['request_text'].str.len()\\nprint(train_df['request_text_length'].describe())\\n\\nprint(f\\\"\\\\nRequest text edit aware length stats:\\\")\\ntrain_df['request_text_edit_aware_length'] = train_df['request_text_edit\\n```\\nOutput:\\nText feature analysis:\\n\\nRequest title length stats:\\ncount    2878.000000\\nmean       71.572967\\nstd        36.233487\\nmin         7.000000\\n25%        46.000000\\n50%        64.000000\\n75%        90.000000\\nmax       272.000000\\nName: request_title_length, dtype: float64\\n\\nRequest text length stats:\\ncount    2878.000000\\nmean      402.521543\\nstd       362.393727\\nmin         0.000000\\n25%       182.000000\\n50%       308.000000\\n75%       503.750000\\nmax      4460.000000\\nName: request_text_length, dtype: float64\\n\\nRequest text edit aware length stats:\\ncount    2878.000000\\nmean      394.567755\\nstd       351.922518\\nmin         0.000000\\n25%       180.000000\\n50%       302.000000\\n75%       498.000000\\nmax      4460.000000\\nName: request_text_edit_aware_length, dtype: float64\\n\\nCorrelation with target:\\nTitle length correlation: 0.0146\\nText length correlation: 0.1301\\nText edit aware length correlation: 0.1199\\n\", \"is_error\": false}",
        "tool.duration_ms": 18.898487091064453
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "eb4c51c8962c8ead",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1767958594675568417,
      "end_time": 1767958600636939856,
      "duration": 5.961371439,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7f38e8369d50>\", \"input\": {\"query\": \"What are winning approaches for text classification with tabular features on Kaggle?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nWinning Kaggle solutions for problems that mix free‚Äëtext columns with ordinary numeric or categorical fields treat the data as **multimodal** and deliberately combine a strong NLP encoder with classic tabular learners.  In practice the most reliable recipe is a three‚Äëstep pipeline:  \\n\\n1. **Encode the raw text** with a pretrained transformer (e.g., BERT, RoBERTa) to obtain dense sentence‚Äëlevel embeddings.  AutoGluon‚Äôs‚ÄØ`TextPredictor` can do this automatically and returns embeddings that can be fed to any tabular model‚ÄØ([auto.gluon.ai](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  \\n2. **Merge the embeddings with the structured features** ‚Äì either by concatenating them to the numeric/categorical matrix (the ‚Äútwo‚Äëstage‚Äù approach evaluated in the multimodal AutoML benchmark‚ÄØ([datasets‚Äëbenchmarks‚Äëproceedings.neurips.cc](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html))) or by converting the auxiliary columns into a short textual description and prepending it to the original document (the ‚Äúfeatures‚Äëto‚Äëtext‚Äù trick that proved best on a 23‚ÄØk clothing‚Äëreview dataset‚ÄØ([mccormickml.com](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert))) .  \\n3. **Train high‚Äëperforming tabular models on the combined matrix** ‚Äì LightGBM, CatBoost or Random Forest are the workhorses, and they can be stacked or blended with the original transformer model itself.  The Multimodal‚ÄëToolkit library provides a ready‚Äëto‚Äëuse wrapper that integrates Hugging‚ÄØFace tokenizers with these tabular learners, making the stacking process straightforward‚ÄØ([aclanthology.org](https://aclanthology.org/2021.maiworkshop-1.10.pdf)).\\n\\nBeyond the core pipeline, Kaggle Grandmasters repeatedly stress a few complementary tricks that turn a good baseline into a winning one:  \\n\\n* **Target‚Äëencode high‚Äëcardinality categoricals** before feeding them to the tabular model, and apply the same encoding to the text‚Äëembedding features.  \\n* **Use GPU‚Äëaccelerated training** for the transformer part (e.g., via AutoGluon or the Multimodal‚ÄëToolkit) to explore larger pretrained checkpoints without prohibitive runtime‚ÄØ([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).  \\n* **Ensemble multiple modalities** ‚Äì blend a pure‚Äëtext model, a pure‚Äëtabular model, and a multimodal model (embeddings‚ÄØ+‚ÄØfeatures) using simple weighted averages or stacking with a meta‚Äëlearner.  This strategy consistently appears in the ‚Äútips and tricks‚Äù posts that summarize top‚Äë5 Kaggle competitions‚ÄØ([neptune.ai](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).  \\n\\nIn short, the winning Kaggle playbook for text classification with tabular features is to **extract high‚Äëquality transformer embeddings, fuse them with engineered numeric/categorical columns, and train/ensemble strong gradient‚Äëboosting models** ‚Äì all while leveraging GPU‚Äëfriendly libraries (AutoGluon, Multimodal‚ÄëToolkit) and proven Kaggle‚Äëspecific preprocessing tricks.\\n\\n--- Result 2 ---\\nTitle: Combining BERT/Transformers and Classical Tabular Models ‚Äî AutoGluon Documentation 0.7.0 documentation\\nURL: https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html\\n\\nContent:\\n<div><div>\\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\\\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\\\">¬∂</a></h2>\\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\\n<a href=\\\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\\\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\\naddition to text and tabular features.</p>\\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\\ntabular data that contains text, numeric, and categorical columns. In\\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\\ndata tables. AutoGluon Tabular can help you train and combine a diverse\\nset of models including classical tabular models like\\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\\nmultimodal network that is introduced in Section\\n‚Äú<span>sec_textprediction_architecture</span>‚Äù of\\n<span>sec_textprediction_multimodal</span> (used by AutoGluon‚Äôs\\n<code><span>TextPredictor</span></code>).</p>\\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\\n<span>import</span> <span>pprint</span>\\n<span>import</span> <span>random</span>\\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n</pre></div>\\n<div>\\n<h2>Product Sentiment Analysis Dataset<a href=\\\"#product-sentiment-analysis-dataset\\\">¬∂</a></h2>\\n<p>We consider the product sentiment analysis dataset from a <a href=\\\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\\\">MachineHack\\nhackathon</a>.\\nThe goal is to predict a user‚Äôs sentiment towards a product given their\\nreview (raw text) and a categorical feature indicating the product‚Äôs\\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\\ndataset to be 90% for training and 10% for development/testing (if\\nsubmitting your models to the hackathon, we recommend training them on\\n100% of the dataset).</p>\\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\\n</pre></div>\\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 689486 (673K) [text/csv]\\nSaving to: ‚Äòproduct_sentiment_machine_hack/train.csv‚Äô\\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\\n2023-02-22 23:29:24 (73.9 MB/s) - ‚Äòproduct_sentiment_machine_hack/train.csv‚Äô saved [689486/689486]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 75517 (74K) [text/csv]\\nSaving to: ‚Äòproduct_sentiment_machine_hack/dev.csv‚Äô\\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\\n2023-02-22 23:29:24 (57.6 MB/s) - ‚Äòproduct_sentiment_machine_hack/dev.csv‚Äô saved [75517/75517]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 312194 (305K) [text/csv]\\nSaving to: ‚Äòproduct_sentiment_machine_hack/test.csv‚Äô\\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\\n2023-02-22 23:29:24 (137 MB/s) - ‚Äòproduct_sentiment_machine_hack/test.csv‚Äô saved [312194/312194]\\n</pre></div>\\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\\n</pre></div>\\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\\n</pre></div>\\n<p>There are two features in the dataset: the users‚Äô review of the product\\nand the product‚Äôs type, and four possible classes to predict.</p>\\n<div>\\n<table>\\n <thead>\\n <tr>\\n <th></th>\\n <th>Product_Description</th>\\n <th>Product_Type</th>\\n <th>Sentiment</th>\\n </tr>\\n </thead>\\n <tbody>\\n <tr>\\n <th>4532</th>\\n <td>they took away the lego pit but ...\\n\\n--- Result 3 ---\\nTitle: Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\\n\\nContent:\\nTabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\\n**üì£ BIG NEWS:****Neptune is joining OpenAI!**‚Üí Read the message from our CEO üì£![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\\nWhat do you want to find?\\nSearch\\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\\n[Shahul ES](https://neptune.ai/blog/author/shahules)\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top tabular data competitions. Without much lag, let‚Äôs begin.\\nThese are the five competitions that I have gone through to create this article:\\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\\n* Use[cudf](https://github.com/rapidsai/cudf).\\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let‚Äôs see the methods used to do it.\\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...\\n\\n--- Result 4 ---\\nURL: https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert\\n\\nContent:\\n# Combining Categorical and Numerical Features with Text in BERT\\n\\n29 Jun 2021\\n\\nIn this tutorial we‚Äôll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\\n\\nTo help motivate our discussion, we‚Äôll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\\n\\n- The age of the reviewer (numerical feature)\\n- The number of upvotes on the review (numerical feature)\\n- The department and category of the clothing item (categorical features)\\n\\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\\n\\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\\n\\nIn Section 2 of this Notebook, I‚Äôve implemented four different ‚Äúbaseline‚Äù strategies which score fairly well, but which don‚Äôt incorporate all of the features together.\\n\\nThen, in Section 3, I‚Äôve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\\n\\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\\n\\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\\n\\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\\n\\nBy Chris McCormick\\n\\n# Contents\\n\\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\\n\\n# S1. Clothing Review Dataset\\n\\n## 1.1. Download & Parse\\n\\nRetrieve the .csv file for the dataset.\\n\\n```\\nimport gdown\\n\\nprint('Downloading dataset...\\\\n')\\n\\n# Download the file.\\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\\n                'Womens Clothing E-Commerce Reviews.csv',\\n                quiet=False)\\n\\nprint('\\\\n\\\\nDONE.')\\n```\\n\\n```\\nDownloading dataset...\\n\\nDownloading...\\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\\nTo: /content/Womens Clothing E-Commerce Reviews.csv\\n8.48MB [00:00, 48.7MB/s]\\n\\nDONE.\\n\\n```\\n\\nParse the dataset csv file into a pandas DataFrame.\\n\\n```\\nimport pandas as pd\\n\\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\\n\\ndata_df.head()\\n```\\n\\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\\n\\n_Features_\\n\\n‚Äú **Recommended IND**‚Äù is the label we are trying to predict for this dataset. ‚Äú1‚Äù means the reviewer recommended the product and ‚Äú0‚Äù means they do not.\\n\\nThe following are _categorical_ features:\\n\\n- Division Name\\n- Department Name\\n- Class Name\\n- Clothing ID\\n\\nAnd the following are _numerical_ features:\\n\\n- Age\\n- Rating\\n- Positive Feedback Count\\n\\n_Feature Analysis_\\n\\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\\n\\nNote that, in addition to the ‚ÄúRecommended‚Äù label, there is also a ‚Äú **Rating**‚Äù column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We‚Äôll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\\n\\n## 1.2. Train-Validation-Test Split\\n\\nI want to use the same training, validation, and test splits for all of the approaches we try so that it‚Äôs a fair comparison.\\n\\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\\n\\nTo solve this, we‚Äôre going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\\n\\n```\\nimport random\\nimport numpy as np\\n\\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\\ntrain_size = int(0.8 * len(data_df))\\nval_size = int(0.1 * len(data_df))\\ntest_size = len(data_df) - (train_size + val_size)\\n\\n# Sanity check the sizes.\\nassert((train_size + val_size + test_size) == len(data_df))\\n\\n# Create a list of indeces for all of the samples in the dataset.\\nindeces = np.arange(0, len(data_df))\\n\\n# Shuffle the indeces randomly.\\nrandom.shuffle(indeces)\\n\\n# Get a list of indeces for each of the splits.\\ntrain_idx = indeces[0:train_size]\\nval_idx = indeces[train_size:(train_size + val_size)]\\ntest_idx = indeces[(train_size + val_size):]\\n\\n# Sanity check\\nassert(len(train_idx) == train_size)\\nassert(len(test_idx) == test_size)\\n\\n# With these lists, we can now select the corresponding dataframe rows using,\\n# e.g., train_df = data_df.iloc[train_idx]\\n\\nprint('  Training size: {:,}'.format(train_size))\\n...\\n\\n--- Result 5 ---\\nTitle: The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\\nURL: https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data\\n\\nContent:\\nThe Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\\n[Technical Blog](https://developer.nvidia.com/blog)\\n[Subscribe](https://developer.nvidia.com/email-signup)\\n[Related Resources**](#main-content-end)\\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\\nEnglish‰∏≠Êñá\\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\\nLessons from years of competitions, made practical with GPU acceleration.\\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\\nSep 18, 2025\\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th√©o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\\n**\\nLike\\n[**Discuss (1)](#entry-content-comments)\\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\\nAI-Generated Summary\\nLike\\nDislike\\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard‚Äîno matter if we‚Äôre working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn‚Äôt just a collection of modeling tricks‚Äîit‚Äôs a repeatable system for solving real-world tabular problems fast.\\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you‚Äôre climbing the leaderboard or deploying models in production, these strategies can give you an edge.\\nWe‚Äôve included links to example write-ups or notebooks from past competitions for each technique.\\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you‚Äôll see below pre-installed.\\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\\nBefore diving into techniques, it‚Äôs worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren‚Äôt optional best practices‚Äîthey‚Äôre the foundation of how we approach every tabular modeling problem.\\n### Fast experimentation[**](#fast_experimentation)\\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover‚Äîand the faster we catch when a model is failing, drifting, or overfitting‚Äîso we can course-correct early and improve faster.\\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\\n**Here‚Äôs how we make it work:**\\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\\nGPU acceleration isn‚Äôt just for deep learning‚Äîit&#8217;s often the only way to make advanced tabular techniques practical at scale.\\n### Local Validation[**](#local_validation)\\nIf you can‚Äôt trust your validation score, you‚Äôre flying blind. That‚Äôs why cross-validation (CV) is a cornerstone to our workflow.\\n**Our approach:**\\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that‚Äôs held out.\\n* Rotate through folds so every part of the data is tested once.\\nThis gives a much more reliable measure of performance than a single train/validation split.\\n**Pro tip:**Match your CV strategy to how the test data is structured.\\n**For example:**\\n* Use TimeSeriesSplit for time-dependent data\\n* Use GroupKFold for grouped data (like users or patients)\\nWith those foundations in place‚Äîmoving fast and validating carefully‚Äîwe can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they‚Äôre table stakes. To build models that hold up in the real world, you need to explore the data a little deeper‚Äîa couple of quick checks that we‚Äôve found useful, but many people miss:\\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift‚Äîtest data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\\nThese techniques aren&#8217;t brand new‚Äîbut they‚Äôre often overlooked, and ignoring them can sink a project.\\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\\n**In action:**In the winning solution to the Amazon KDD Cup ‚Äò23, the team uncovered both a train‚Äîtest distribution shift and temporal patterns in the target‚Äîinsights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...\\n\\n--- Result 6 ---\\nTitle: Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\nURL: https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html\\n\\nContent:\\n#### Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\n\\nPart of\\n[Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021) round2\\n\\n[Bibtex](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/28-/bibtex) [Paper](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper-round2.pdf) [Reviews And Public Comment ¬ª](https://openreview.net/forum?id=Q0zOIaec8HF) [Supplemental](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Supplemental-round2.zip)\\n\\n#### Authors\\n\\n_Xingjian Shi, Jonas Mueller, Nick Erickson, Nick Erickson, Mu Li, Alexander Smola, Alexander Smola_\\n\\n#### Abstract\\n\\nWe consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.\\n\\nDo not remove: This comment is monitored to verify that the site is working properly\\n\\n--- Result 7 ---\\nTitle: A Package for Learning on Tabular and Text Data with Transformers\\nURL: https://aclanthology.org/2021.maiworkshop-1.10.pdf\\n\\nContent:\\nProceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 69‚Äì73\\nJune 6, 2021. ¬©2021 Association for Computational Linguistics\\n69\\nMultimodal-Toolkit: A Package for Learning on Tabular and\\nText Data with Transformers\\nKen Gu\\nGeorgian\\nken.gu@georgian.io\\nAkshay Budhkar\\nGeorgian\\nakshay@georgian.io\\nAbstract\\nRecent progress in natural language process\\u0002ing has led to Transformer architectures be\\u0002coming the predominant model used for nat\\u0002ural language tasks. However, in many real\\u0002world datasets, additional modalities are in\\u0002cluded which the Transformer does not di\\u0002rectly leverage. We present Multimodal\\u0002Toolkit,1\\nan open-source Python package to\\nincorporate text and tabular (categorical and\\nnumerical) data with Transformers for down\\u0002stream applications. Our toolkit integrates\\nwell with Hugging Face‚Äôs existing API such as\\ntokenization and the model hub2 which allows\\neasy download of different pre-trained models.\\n1 Introduction\\nIn recent years, Transformers (Vaswani et al.,\\n2017) have become popular for model pre-training\\n(Howard and Ruder, 2018; Peters et al., 2018; De\\u0002vlin et al., 2019) and have yielded state-of-the-art\\nresults on many natural language processing (NLP)\\ntasks. In addition, well-documented Transformer\\nlibraries such as Hugging Face Transformers (Wolf\\net al., 2020), and AllenNLP (Gardner et al., 2018)\\nhave democratized NLP, making it easier to pro\\u0002ductionize and experiment on Transformers.\\nHowever, there are not a lot of comprehensive\\ntools for Transformers to work with tabular data.\\nOften in real-world datasets, there are tabular data\\nas well as unstructured text data which can pro\\u0002vide meaningful signals for the task at hand. For\\ninstance, in the small example in Figure 1, each\\nrow is a data point. Columns Title and Review\\nText contain text features, columns Division\\nName, Class Name, and Department Name\\ncontain categorical features, and the Age column\\nis a numerical feature. To the best of our knowl\\u0002edge, no tool exists that makes it simple for Trans\\u0002formers to handle this extra modality. Therefore,\\n1Github: https://git.io/JO5a6\\n2https://huggingface.co/docs\\nFigure 1: An example of a clothing review classifica\\u0002tion dataset. Each row is a data point consisting of text,\\ncategorical features, and numerical features.\\ngiven the advances of Transformers for natural\\nlanguage tasks and the maturity of existing Trans\\u0002former libraries, we introduce Multimodal-Toolkit,\\na lightweight Python package built on top of Hug\\u0002ging Face Transformers. Our package extends ex\\u0002isting Transformers in the Hugging Face‚Äôs Trans\\u0002formers library to seamlessly handle structured tab\\u0002ular data while keeping the existing tokenization\\n(including subword segmentation), experimental\\npipeline, and pre-trained model hub functionalities\\nof Hugging Face Transformers. We show the effec\\u0002tiveness of our toolkit on three real-world datasets.\\n2 Related Work\\nThere have been several proposed Transformer\\nmodels that aim to handle text features and addi\\u0002tional features of another modality. For pre-trained\\nTransformers on images and text, models such as\\nViLBERT (Lu et al., 2019) and VLBERT (Su et al.,\\n2020) are mainly the same as the original BERT\\nmodel but treat the extra image modality as addi\\u0002tional tokens to the input. These models require\\npre-training on multimodal image and text data.\\nOn the other hand, while treating image features\\n70\\nas additional input tokens, MMBT (Kiela et al.,\\n2019) proposes to use pre-trained BERT directly\\nand fine-tune on image and text data. This is simi\\u0002lar to Multimodal-Toolkit in which no pre-training\\non text and tabular data is needed.\\nLikewise, Transformers have been adapted to\\nalign, audio, visual, and text modalities in which\\nthere is a natural ground truth alignment. MulT\\n(Tsai et al., 2019) is similar to ViLBert in which\\nco-attention is used between pairs of modalities but\\nalso includes temporal convolutions so that input to\\u0002kens are aware of their temporal neighbors. Mean\\u0002while, Rahman et al. (2020) injects cross modality\\nattention at certain Transformer layers via a gating\\nmechanism.\\nFinally, knowledge graph embeddings have also\\nbeen effectively combined with input text tokens\\nin Transformers. Ostendorff et al. (2019) com\\u0002bines knowledge graph embeddings on authors\\nwith book titles and other metadata features via\\nsimple concatenation for book genre classification.\\nOn the other hand, for more general language tasks,\\nERNIE (Zhang et al., 2019) first matches the to\\u0002kens in the input text with entities in the knowledge\\ngraph. With this matching, the model fuses these\\nembeddings to produce entity-aware text embed\\u0002dings and text-aware entity embeddings.\\nHowever, these models do not capture categor\\u0002ical and numerical data explicitly. Hugging Face\\ndoes include LXMERT (Tan and Bansal, 2019) to\\nhandle language and vision modality but this can\\nnot be easily adapted for categorical and numeri\\u0002cal data. Nevertheless, existing multimodal Trans\\u0002former models do give good insights into how to\\ncombine categorical and numerical features. ViL\\u0002BERT and VLBERT for example include image\\nmodality as input tokens which lead to one of our\\nsimple baseline of categorical and numerical fea\\u0002tures as additional token inputs to the model. Like\\u0002wise, the gating mechanism Rahman et al. (2020),\\nattention, and different weighting schemes have\\nall been shown to be useful in combining different\\nmodalities.\\n3 Design\\nThe goal of Multimodal-Toolkit is to allow users\\nto quickly adapt state-of-the-art Transformer mod\\u0002els for situations involving text and tabular data\\nwhich occur often in real-world datasets. More\\u0002over, we want to bring the benefits of Transformers\\nto more use cases while making it simple for users\\nFigure 2: The framework of Multimodal-Toolkit.\\nThere is a data processing module that outputs pro\\u0002cessed text, numerical, and categorical features that are\\nthen fed as input to our Transformer With Tabular mod\\u0002ule consisting of a Hugging Face Transformer and our\\ncombining module.\\nof Hugging Face Transformers to adopt. Therefore,\\nwe maintain the existing interface of the popular\\nHugging Face Transformers library.\\nThis design enables us to easily include more\\nTransformer models, leverage strengths of spe\\u0002cific models, use a feature-rich training pipeline,\\nand integrate the thousands of community trained\\nmodels on Hugging Face‚Äôs model hub. We sup\\u0002port a variety of Transformers (e.g. BERT, AL\\u0002BERT, RoBERTa, XLNET) for both classification\\nand regression tasks. All together, this becomes a\\nreusable Transformer With Tabular component. We\\nalso provide a data preprocessing module for cate\\u0002gorical and numerical features. An overview of the\\nsystem is shown in Figure 2. Currently, the library\\nsupports PyTorch Transformers implementations.\\n3.1 Combining Module\\nWe implement a combining module that is model\\nagnostic that takes as input, x, the text features\\noutputted from a Transformer model and prepro\\u0002cessed categorical (c) and numerical (n) features,\\nand outputs a combined multimodal representation\\nm. Although existing multimodal Transformers\\nincorporate cross-modal attention inside middle\\nTransformer layers, we choose the design in which\\nthe modality combination comes after the Trans\\u0002former because this module can be easily included\\nwithout much adaptation of the existing Hugging\\nFace Transformer interface and can be easily ex\\u0002tended to new Transformers included in the future.\\nInside the combining module, we implement var-\\n71\\nCombine Feature Method Equation\\nText only m = x\\nConcat m = x||c||n\\nIndividual MLPs on categorical and\\u0002numerical features then concat (MLP + Concat) m = x||MLP(c)||MLP(n)\\nMLP on concatenated categorical and\\nnumerical features then concat (Concat + MLP) m = x||MLP(c||n)\\nAttention on categorical and numerical\\nfeatures (Attention)\\nm = Œ±x,xWxx + Œ±x,cWcc + Œ±x,nWnn\\nŒ±i,j =\\nexp(LeakyReLU(a\\nT\\n[Wixi||Wjxj ]))\\nP\\nk‚àà{x,c,n}\\nexp(LeakyReLU(aT [Wixi||Wkxk]))\\nGating on categorical and numerical\\nfeatures and then sum (Rahman et al., 2020)\\n(Gating)\\nm = x + Œ±h\\nh = gc \\f (Wcc...\\n\\n--- Result 8 ---\\nTitle: Papers with Code - Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\nURL: https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular\\n\\nContent:\\nnew\\n\\nGet trending papers in your email inbox once a day!\\n\\nGet trending papers in your email inbox!\\n\\n[Subscribe](https://paperswithcode.com/login?next=%2Fpapers)\\n\\n# Trending Papers\\n\\n## by [AK](https://paperswithcode.com/akhaliq) and the research community\\n\\n- Daily\\n- Weekly\\n- Monthly\\n\\nTrending Papers\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n¬∑Published on Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n¬∑Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n¬∑Published on Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n¬∑Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\ncsuhan\\n\\n### [ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\\\ Automation via Modular Multimodal Agents](https://paperswithcode.com/papers/2507.22827)\\n\\nAutomating the transformation of user interface (UI) designs into front-end\\ncode holds significant promise for accelerating software development and\\ndemocratizing design workflows. While recent large language models (LLMs) have\\ndemonstrated progress in text-to-code generation, many existing approaches rely\\nsolely o...\\n\\n--- Result 9 ---\\nTitle: Towards Benchmarking Foundation Models for Tabular Data With Text\\nURL: https://arxiv.org/pdf/2507.07829\\n\\nContent:\\nTowards Benchmarking Foundation Models for Tabular Data With Text\\nMartin Mraz¬¥\\n* 1 Breenda Das * 1 Anshul Gupta * 1 Lennart Purucker 1 Frank Hutter 2 3 1\\nAbstract\\nFoundation models for tabular data are rapidly\\nevolving, with increasing interest in extending\\nthem to support additional modalities such as free\\u0002text features. However, existing benchmarks for\\ntabular data rarely include textual columns, and\\nidentifying real-world tabular datasets with se\\u0002mantically rich text features is non-trivial. We\\npropose a series of simple yet effective ablation\\u0002style strategies for incorporating text into con\\u0002ventional tabular pipelines. Moreover, we bench\\u0002mark how state-of-the-art tabular foundation mod\\u0002els can handle textual data by manually curating\\na collection of real-world tabular datasets with\\nmeaningful textual features. Our study is an im\\u0002portant step towards improving benchmarking of\\nfoundation models for tabular data with text.\\n1. Introduction\\nFoundation models have begun to transform tabular learn\\u0002ing (Erickson et al., 2020b; Hollmann et al., 2025), echo\\u0002ing the trajectory of other research fields. A natural next\\nstep is mixed-modality tabular modeling, where structured\\ncolumns may also include free-text fields such as job de\\u0002scriptions, clinical notes, or product summaries. Current\\ntabular benchmarks, however, almost never contain textual\\ncolumns, cf. (Bischl et al., 2019; Liu et al., 2024; McEl\\u0002fresh et al., 2024). Moreover, locating real-world datasets\\nwith semantically rich text features is exceptionally difficult,\\nwith even exhaustive searches of OpenML and Kaggle only\\nyielding a handful of usable candidates (Shi et al., 2021).\\nConsequently, current tabular foundation models are rarely\\nevaluated for tabular data with text.\\nPipelines that can handle tabular data with text vary greatly\\n*Equal contribution 1Department of Computer Science, Uni\\u0002versity of Freiburg, Freiburg, Germany 2\\nPrior Labs, Freiburg,\\nGermany 3ELLIS Institute, Tubingen, Germany. Correspon\\u0002dence to: Martin Mraz <mrazm@informatik.uni-freiburg.de>,\\nBreenda Das <dasb@informatik.uni-freiburg.de>, Anshul Gupta\\n<guptaa@informatik.uni-freiburg.de>.\\nProceedings of the 42 nd International Conference on Machine\\nLearning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\\nby the author(s).\\nin their implementation. AutoGluon‚Äôs AutoMLPipeline\\u0002FeatureGenerator (Erickson et al., 2020a) converts text to\\nsparse vectors; CARTE (Kim et al., 2024) applies fastText\\nsentence embeddings (Bojanowski et al., 2017). and the\\nTabPFNv2 API accepts raw text but does not disclose its\\nmethodology. These divergent choices raise a fundamental\\nquestion: Which embedding strategy works best, and under\\nwhat conditions?\\nTo answer this question, we present the first systematic\\nstudy of predictive machine learning with foundation mod\\u0002els for tabular data with text. We study the performance\\nof three representative embedding routes: fastText, Skrub‚Äôs\\nTableVectorizer, and AutoGluon‚Äôs text encoder. We show\\nqualitatively, with a simple synthetic counter-example, that\\nboth n-gram based and off-the-shelf sentence embeddings\\ncan fail to recover highly predictive semantic patterns. More\\u0002over, quantitatively, we evaluate these methods on a man\\u0002ually curated set of real-world tabular benchmark that (i)\\ncontain genuinely informative free-text columns (ii) spans\\nover a variety of domains and samples.\\nOur contributions are: (I) A qualitative study show cas\\u0002ing the limitations of standard n-gram based and generic\\nNLP-based embeddings for tabular tasks with text. (II) A\\nmanually curated set of real-world tabular datasets with se\\u0002mantically rich textual columns. (III) An empirical study of\\nthree text embedding pipelines for TabPFNv2 and XGBoost\\nwith the TabPFNv2 API and AutoGluon Tabular Predictor\\nas baselines.\\nOur study reveals the limitations of current methods and\\nunderscores the need for new methods to handle tabular data\\nwith free text.\\n2. Related Work\\nApproaches incorporating free-text in tabular learning\\nlargely follow two paradigms. First, row-as-text meth\\u0002ods serialize entire rows into prompts and delegate pre\\u0002diction to a large language model (LLM), as seen in\\nTABLLM (Hegselmann et al., 2022), TABLE-LLM (Zhang\\net al., 2024). These work well when textual fields dominate\\nor in few-shot settings. Second, per-column embedding\\nstrategies extract textual embeddings from a single or groups\\nof features, while preserving the structural nature of tabular\\n1\\narXiv:2507.07829v1 [cs.LG] 10 Jul 2025\\nTowards Benchmarking Foundation Models for Tabular Data With Text\\nFigure 1. Qualitative investigation of textual embeddings for tabular prediction tasks: TabPFN v2 accuracy across five datasets.\\nBaselines ‚ÄúNo-Text‚Äù uses original input features, ‚ÄúComplete Leak‚Äù has target leakage, therefore 100% accuracy for all. Following tests\\nembed targets into textual modality. ‚ÄúN-Gram Break‚Äù shows TF-IDF breaking under unseen synonyms, ‚ÄúSimple NLP break‚Äù shows\\nFastText underperforming under noise, ‚ÄúLLM Breaks‚Äù shows BERT variant breaking under semantic ambiguity.\\ndata. They embed each column using fastText or LLMs\\nand then concatenate the resulting embeddings to the table,\\nor replace the feature with its individual textual embed\\u0002ding, cf. (Koloski et al., 2025; Carballo et al., 2023; Kim\\net al., 2024). Grinsztajn et al. (2023) compare 30+ LLM\\nand substring-based methods for embedding string columns,\\nshowing when LLMs yield better representations.\\nIn this study, we investigate pre-column embeddings be\\u0002cause we focus on a many-shot setting (e.g., more than 32\\ntraining samples), in which LLM-based predictive methods\\ndo not perform well, cf. (Hegselmann et al., 2023) and (Ma\\net al., 2024), or require a prohibitive amount of fine-tuning\\n(Shysheya et al., 2025).\\nMost popular prior tabular benchmarks contain no free-text\\nfield, cf. (Bischl et al., 2019; Gijsbers et al., 2022; McEl\\u0002fresh et al., 2023). For tabular data with text, Shi et al.\\n(2021) curated 18 datasets with mixed modalities, but many\\nrely almost entirely on text, with only one or two tabular\\nfeatures, or features derived from the long text, e.g. # of\\nwords (Tang et al., 2024). Thus, they benchmark text-based\\nmodels rather than tabular models, focusing on text data\\nwith tabular features. In contrast, we focus on tabular mod\\u0002els extended to handle text, focusing on tabular data with\\nadditional text features. Other studies were often limited to\\nan evaluation with just one or two datasets (Carballo et al.,\\n2023; Lu et al., 2023). Overall, existing studies for tabular\\ndata with text lack a domain-agnostic benchmark where\\ntextual features complement, rather than dominate, tabular\\ndata. This motivates the new benchmark we present.\\nThe CARTE Benchmark. The latest seminal work on\\nbenchmarking for tabular data with text is the CARTE (Kim\\net al., 2024) benchmark. It includes 51 datasets for tabu\\u0002lar data with text. However, when we investigated these\\ndatasets more closely, we found that at most 11 out of the\\n51 datasets are suited to evaluate tabular data with text.\\nMoreover, our manually curated collection of datasets for\\nbenchmarking only includes 1 out of the 51 datasets.\\nWe share an extensive report of our investigation in Ap\\u0002pendix G. In short, we found that most datasets (a) do not\\nrepresent predictive machine learning tasks for tabular data\\nwith text; (b) are skewed towards text data representing\\nmany categories instead of longer free text; (c) were pre\\u0002processed manually per dataset with logic that seems to\\nfavor CARTE; (d) or were very similar datasets from the\\nsame domain, with the same features, or similar semantic\\ncontent. Thus, while CARTE was a significant step toward\\nbenchmarking and learning for tabular data with text, it falls\\nshort in several aspects. Our work complements CARTE‚Äôs\\nefforts and aims to improve benchmarking for tabular data\\nwith text further.\\n3. Qualitative Investigation\\nIn this section, we evaluate popular text embedding tech\\u0002niques: N-Grams, simple NLP models and LLMs for tabular\\npr...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Winning Kaggle solutions for problems that mix free‚Äëtext columns with ordinary numeric or categorical fields treat the data as **multimodal** and deliberately combine a strong NLP encoder with classic tabular learners.  In practice the most reliable recipe is a three‚Äëstep pipeline:  \\n\\n1. **Encode the raw text** with a pretrained transformer (e.g., BERT, RoBERTa) to obtain dense sentence‚Äëlevel embeddings.  AutoGluon‚Äôs‚ÄØ`TextPredictor` can do this automatically and returns embeddings that can be fed to any tabular model‚ÄØ([auto.gluon.ai](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  \\n2. **Merge the embeddings with the structured features** ‚Äì either by concatenating them to the numeric/categorical matrix (the ‚Äútwo‚Äëstage‚Äù approach evaluated in the multimodal AutoML benchmark‚ÄØ([datasets‚Äëbenchmarks‚Äëproceedings.neurips.cc](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html))) or by converting the auxiliary columns into a short textual description and prepending it to the original document (the ‚Äúfeatures‚Äëto‚Äëtext‚Äù trick that proved best on a 23‚ÄØk clothing‚Äëreview dataset‚ÄØ([mccormickml.com](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert))) .  \\n3. **Train high‚Äëperforming tabular models on the combined matrix** ‚Äì LightGBM, CatBoost or Random Forest are the workhorses, and they can be stacked or blended with the original transformer model itself.  The Multimodal‚ÄëToolkit library provides a ready‚Äëto‚Äëuse wrapper that integrates Hugging‚ÄØFace tokenizers with these tabular learners, making the stacking process straightforward‚ÄØ([aclanthology.org](https://aclanthology.org/2021.maiworkshop-1.10.pdf)).\\n\\nBeyond the core pipeline, Kaggle Grandmasters repeatedly stress a few complementary tricks that turn a good baseline into a winning one:  \\n\\n* **Target‚Äëencode high‚Äëcardinality categoricals** before feeding them to the tabular model, and apply the same encoding to the text‚Äëembedding features.  \\n* **Use GPU‚Äëaccelerated training** for the transformer part (e.g., via AutoGluon or the Multimodal‚ÄëToolkit) to explore larger pretrained checkpoints without prohibitive runtime‚ÄØ([developer.nvidia.com](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).  \\n* **Ensemble multiple modalities** ‚Äì blend a pure‚Äëtext model, a pure‚Äëtabular model, and a multimodal model (embeddings‚ÄØ+‚ÄØfeatures) using simple weighted averages or stacking with a meta‚Äëlearner.  This strategy consistently appears in the ‚Äútips and tricks‚Äù posts that summarize top‚Äë5 Kaggle competitions‚ÄØ([neptune.ai](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).  \\n\\nIn short, the winning Kaggle playbook for text classification with tabular features is to **extract high‚Äëquality transformer embeddings, fuse them with engineered numeric/categorical columns, and train/ensemble strong gradient‚Äëboosting models** ‚Äì all while leveraging GPU‚Äëfriendly libraries (AutoGluon, Multimodal‚ÄëToolkit) and proven Kaggle‚Äëspecific preprocessing tricks.\", \"url\": \"\"}, {\"title\": \"Combining BERT/Transformers and Classical Tabular Models ‚Äî AutoGluon Documentation 0.7.0 documentation\", \"text\": \"<div><div>\\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\\\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\\\">¬∂</a></h2>\\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\\n<a href=\\\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\\\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\\naddition to text and tabular features.</p>\\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\\ntabular data that contains text, numeric, and categorical columns. In\\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\\ndata tables. AutoGluon Tabular can help you train and combine a diverse\\nset of models including classical tabular models like\\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\\nmultimodal network that is introduced in Section\\n‚Äú<span>sec_textprediction_architecture</span>‚Äù of\\n<span>sec_textprediction_multimodal</span> (used by AutoGluon‚Äôs\\n<code><span>TextPredictor</span></code>).</p>\\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\\n<span>import</span> <span>pprint</span>\\n<span>import</span> <span>random</span>\\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\\n</pre></div>\\n<div>\\n<h2>Product Sentiment Analysis Dataset<a href=\\\"#product-sentiment-analysis-dataset\\\">¬∂</a></h2>\\n<p>We consider the product sentiment analysis dataset from a <a href=\\\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\\\">MachineHack\\nhackathon</a>.\\nThe goal is to predict a user‚Äôs sentiment towards a product given their\\nreview (raw text) and a categorical feature indicating the product‚Äôs\\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\\ndataset to be 90% for training and 10% for development/testing (if\\nsubmitting your models to the hackathon, we recommend training them on\\n100% of the dataset).</p>\\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\\n</pre></div>\\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 689486 (673K) [text/csv]\\nSaving to: ‚Äòproduct_sentiment_machine_hack/train.csv‚Äô\\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\\n2023-02-22 23:29:24 (73.9 MB/s) - ‚Äòproduct_sentiment_machine_hack/train.csv‚Äô saved [689486/689486]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 75517 (74K) [text/csv]\\nSaving to: ‚Äòproduct_sentiment_machine_hack/dev.csv‚Äô\\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\\n2023-02-22 23:29:24 (57.6 MB/s) - ‚Äòproduct_sentiment_machine_hack/dev.csv‚Äô saved [75517/75517]\\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\\nHTTP request sent, awaiting response... 200 OK\\nLength: 312194 (305K) [text/csv]\\nSaving to: ‚Äòproduct_sentiment_machine_hack/test.csv‚Äô\\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\\n2023-02-22 23:29:24 (137 MB/s) - ‚Äòproduct_sentiment_machine_hack/test.csv‚Äô saved [312194/312194]\\n</pre></div>\\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\\n</pre></div>\\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\\n</pre></div>\\n<p>There are two features in the dataset: the users‚Äô review of the product\\nand the product‚Äôs type, and four possible classes to predict.</p>\\n<div>\\n<table>\\n <thead>\\n <tr>\\n <th></th>\\n <th>Product_Description</th>\\n <th>Product_Type</th>\\n <th>Sentiment</th>\\n </tr>\\n </thead>\\n <tbody>\\n <tr>\\n <th>4532</th>\\n <td>they took away the lego pit but ...\", \"url\": \"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html\"}, {\"title\": \"Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\\n**üì£ BIG NEWS:****Neptune is joining OpenAI!**‚Üí Read the message from our CEO üì£![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\\nWhat do you want to find?\\nSearch\\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\\n[Shahul ES](https://neptune.ai/blog/author/shahules)\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top tabular data competitions. Without much lag, let‚Äôs begin.\\nThese are the five competitions that I have gone through to create this article:\\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\\n* Use[cudf](https://github.com/rapidsai/cudf).\\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let‚Äôs see the methods used to do it.\\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...\", \"url\": \"https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\"}, {\"title\": \"\", \"text\": \"# Combining Categorical and Numerical Features with Text in BERT\\n\\n29 Jun 2021\\n\\nIn this tutorial we‚Äôll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\\n\\nTo help motivate our discussion, we‚Äôll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\\n\\n- The age of the reviewer (numerical feature)\\n- The number of upvotes on the review (numerical feature)\\n- The department and category of the clothing item (categorical features)\\n\\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\\n\\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\\n\\nIn Section 2 of this Notebook, I‚Äôve implemented four different ‚Äúbaseline‚Äù strategies which score fairly well, but which don‚Äôt incorporate all of the features together.\\n\\nThen, in Section 3, I‚Äôve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\\n\\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\\n\\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\\n\\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\\n\\nBy Chris McCormick\\n\\n# Contents\\n\\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\\n\\n# S1. Clothing Review Dataset\\n\\n## 1.1. Download & Parse\\n\\nRetrieve the .csv file for the dataset.\\n\\n```\\nimport gdown\\n\\nprint('Downloading dataset...\\\\n')\\n\\n# Download the file.\\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\\n                'Womens Clothing E-Commerce Reviews.csv',\\n                quiet=False)\\n\\nprint('\\\\n\\\\nDONE.')\\n```\\n\\n```\\nDownloading dataset...\\n\\nDownloading...\\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\\nTo: /content/Womens Clothing E-Commerce Reviews.csv\\n8.48MB [00:00, 48.7MB/s]\\n\\nDONE.\\n\\n```\\n\\nParse the dataset csv file into a pandas DataFrame.\\n\\n```\\nimport pandas as pd\\n\\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\\n\\ndata_df.head()\\n```\\n\\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\\n\\n_Features_\\n\\n‚Äú **Recommended IND**‚Äù is the label we are trying to predict for this dataset. ‚Äú1‚Äù means the reviewer recommended the product and ‚Äú0‚Äù means they do not.\\n\\nThe following are _categorical_ features:\\n\\n- Division Name\\n- Department Name\\n- Class Name\\n- Clothing ID\\n\\nAnd the following are _numerical_ features:\\n\\n- Age\\n- Rating\\n- Positive Feedback Count\\n\\n_Feature Analysis_\\n\\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\\n\\nNote that, in addition to the ‚ÄúRecommended‚Äù label, there is also a ‚Äú **Rating**‚Äù column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We‚Äôll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\\n\\n## 1.2. Train-Validation-Test Split\\n\\nI want to use the same training, validation, and test splits for all of the approaches we try so that it‚Äôs a fair comparison.\\n\\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\\n\\nTo solve this, we‚Äôre going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\\n\\n```\\nimport random\\nimport numpy as np\\n\\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\\ntrain_size = int(0.8 * len(data_df))\\nval_size = int(0.1 * len(data_df))\\ntest_size = len(data_df) - (train_size + val_size)\\n\\n# Sanity check the sizes.\\nassert((train_size + val_size + test_size) == len(data_df))\\n\\n# Create a list of indeces for all of the samples in the dataset.\\nindeces = np.arange(0, len(data_df))\\n\\n# Shuffle the indeces randomly.\\nrandom.shuffle(indeces)\\n\\n# Get a list of indeces for each of the splits.\\ntrain_idx = indeces[0:train_size]\\nval_idx = indeces[train_size:(train_size + val_size)]\\ntest_idx = indeces[(train_size + val_size):]\\n\\n# Sanity check\\nassert(len(train_idx) == train_size)\\nassert(len(test_idx) == test_size)\\n\\n# With these lists, we can now select the corresponding dataframe rows using,\\n# e.g., train_df = data_df.iloc[train_idx]\\n\\nprint('  Training size: {:,}'.format(train_size))\\n...\", \"url\": \"https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert\"}, {\"title\": \"The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\", \"text\": \"The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\\n[Technical Blog](https://developer.nvidia.com/blog)\\n[Subscribe](https://developer.nvidia.com/email-signup)\\n[Related Resources**](#main-content-end)\\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\\nEnglish‰∏≠Êñá\\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\\nLessons from years of competitions, made practical with GPU acceleration.\\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\\nSep 18, 2025\\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th√©o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\\n**\\nLike\\n[**Discuss (1)](#entry-content-comments)\\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\\nAI-Generated Summary\\nLike\\nDislike\\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard‚Äîno matter if we‚Äôre working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn‚Äôt just a collection of modeling tricks‚Äîit‚Äôs a repeatable system for solving real-world tabular problems fast.\\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you‚Äôre climbing the leaderboard or deploying models in production, these strategies can give you an edge.\\nWe‚Äôve included links to example write-ups or notebooks from past competitions for each technique.\\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you‚Äôll see below pre-installed.\\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\\nBefore diving into techniques, it‚Äôs worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren‚Äôt optional best practices‚Äîthey‚Äôre the foundation of how we approach every tabular modeling problem.\\n### Fast experimentation[**](#fast_experimentation)\\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover‚Äîand the faster we catch when a model is failing, drifting, or overfitting‚Äîso we can course-correct early and improve faster.\\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\\n**Here‚Äôs how we make it work:**\\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\\nGPU acceleration isn‚Äôt just for deep learning‚Äîit&#8217;s often the only way to make advanced tabular techniques practical at scale.\\n### Local Validation[**](#local_validation)\\nIf you can‚Äôt trust your validation score, you‚Äôre flying blind. That‚Äôs why cross-validation (CV) is a cornerstone to our workflow.\\n**Our approach:**\\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that‚Äôs held out.\\n* Rotate through folds so every part of the data is tested once.\\nThis gives a much more reliable measure of performance than a single train/validation split.\\n**Pro tip:**Match your CV strategy to how the test data is structured.\\n**For example:**\\n* Use TimeSeriesSplit for time-dependent data\\n* Use GroupKFold for grouped data (like users or patients)\\nWith those foundations in place‚Äîmoving fast and validating carefully‚Äîwe can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they‚Äôre table stakes. To build models that hold up in the real world, you need to explore the data a little deeper‚Äîa couple of quick checks that we‚Äôve found useful, but many people miss:\\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift‚Äîtest data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\\nThese techniques aren&#8217;t brand new‚Äîbut they‚Äôre often overlooked, and ignoring them can sink a project.\\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\\n**In action:**In the winning solution to the Amazon KDD Cup ‚Äò23, the team uncovered both a train‚Äîtest distribution shift and temporal patterns in the target‚Äîinsights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...\", \"url\": \"https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data\"}, {\"title\": \"Benchmarking Multimodal AutoML for Tabular Data with Text Fields\", \"text\": \"#### Benchmarking Multimodal AutoML for Tabular Data with Text Fields\\n\\nPart of\\n[Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021) round2\\n\\n[Bibtex](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/28-/bibtex) [Paper](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper-round2.pdf) [Reviews And Public Comment ¬ª](https://openreview.net/forum?id=Q0zOIaec8HF) [Supplemental](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Supplemental-round2.zip)\\n\\n#### Authors\\n\\n_Xingjian Shi, Jonas Mueller, Nick Erickson, Nick Erickson, Mu Li, Alexander Smola, Alexander Smola_\\n\\n#### Abstract\\n\\nWe consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.\\n\\nDo not remove: This comment is monitored to verify that the site is working properly\", \"url\": \"https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract-round2.html\"}, {\"title\": \"A Package for Learning on Tabular and Text Data with Transformers\", \"text\": \"Proceedings of the Third Workshop on Multimodal Artificial Intelligence, pages 69‚Äì73\\nJune 6, 2021. ¬©2021 Association for Computational Linguistics\\n69\\nMultimodal-Toolkit: A Package for Learning on Tabular and\\nText Data with Transformers\\nKen Gu\\nGeorgian\\nken.gu@georgian.io\\nAkshay Budhkar\\nGeorgian\\nakshay@georgian.io\\nAbstract\\nRecent progress in natural language process\\u0002ing has led to Transformer architectures be\\u0002coming the predominant model used for nat\\u0002ural language tasks. However, in many real\\u0002world datasets, additional modalities are in\\u0002cluded which the Transformer does not di\\u0002rectly leverage. We present Multimodal\\u0002Toolkit,1\\nan open-source Python package to\\nincorporate text and tabular (categorical and\\nnumerical) data with Transformers for down\\u0002stream applications. Our toolkit integrates\\nwell with Hugging Face‚Äôs existing API such as\\ntokenization and the model hub2 which allows\\neasy download of different pre-trained models.\\n1 Introduction\\nIn recent years, Transformers (Vaswani et al.,\\n2017) have become popular for model pre-training\\n(Howard and Ruder, 2018; Peters et al., 2018; De\\u0002vlin et al., 2019) and have yielded state-of-the-art\\nresults on many natural language processing (NLP)\\ntasks. In addition, well-documented Transformer\\nlibraries such as Hugging Face Transformers (Wolf\\net al., 2020), and AllenNLP (Gardner et al., 2018)\\nhave democratized NLP, making it easier to pro\\u0002ductionize and experiment on Transformers.\\nHowever, there are not a lot of comprehensive\\ntools for Transformers to work with tabular data.\\nOften in real-world datasets, there are tabular data\\nas well as unstructured text data which can pro\\u0002vide meaningful signals for the task at hand. For\\ninstance, in the small example in Figure 1, each\\nrow is a data point. Columns Title and Review\\nText contain text features, columns Division\\nName, Class Name, and Department Name\\ncontain categorical features, and the Age column\\nis a numerical feature. To the best of our knowl\\u0002edge, no tool exists that makes it simple for Trans\\u0002formers to handle this extra modality. Therefore,\\n1Github: https://git.io/JO5a6\\n2https://huggingface.co/docs\\nFigure 1: An example of a clothing review classifica\\u0002tion dataset. Each row is a data point consisting of text,\\ncategorical features, and numerical features.\\ngiven the advances of Transformers for natural\\nlanguage tasks and the maturity of existing Trans\\u0002former libraries, we introduce Multimodal-Toolkit,\\na lightweight Python package built on top of Hug\\u0002ging Face Transformers. Our package extends ex\\u0002isting Transformers in the Hugging Face‚Äôs Trans\\u0002formers library to seamlessly handle structured tab\\u0002ular data while keeping the existing tokenization\\n(including subword segmentation), experimental\\npipeline, and pre-trained model hub functionalities\\nof Hugging Face Transformers. We show the effec\\u0002tiveness of our toolkit on three real-world datasets.\\n2 Related Work\\nThere have been several proposed Transformer\\nmodels that aim to handle text features and addi\\u0002tional features of another modality. For pre-trained\\nTransformers on images and text, models such as\\nViLBERT (Lu et al., 2019) and VLBERT (Su et al.,\\n2020) are mainly the same as the original BERT\\nmodel but treat the extra image modality as addi\\u0002tional tokens to the input. These models require\\npre-training on multimodal image and text data.\\nOn the other hand, while treating image features\\n70\\nas additional input tokens, MMBT (Kiela et al.,\\n2019) proposes to use pre-trained BERT directly\\nand fine-tune on image and text data. This is simi\\u0002lar to Multimodal-Toolkit in which no pre-training\\non text and tabular data is needed.\\nLikewise, Transformers have been adapted to\\nalign, audio, visual, and text modalities in which\\nthere is a natural ground truth alignment. MulT\\n(Tsai et al., 2019) is similar to ViLBert in which\\nco-attention is used between pairs of modalities but\\nalso includes temporal convolutions so that input to\\u0002kens are aware of their temporal neighbors. Mean\\u0002while, Rahman et al. (2020) injects cross modality\\nattention at certain Transformer layers via a gating\\nmechanism.\\nFinally, knowledge graph embeddings have also\\nbeen effectively combined with input text tokens\\nin Transformers. Ostendorff et al. (2019) com\\u0002bines knowledge graph embeddings on authors\\nwith book titles and other metadata features via\\nsimple concatenation for book genre classification.\\nOn the other hand, for more general language tasks,\\nERNIE (Zhang et al., 2019) first matches the to\\u0002kens in the input text with entities in the knowledge\\ngraph. With this matching, the model fuses these\\nembeddings to produce entity-aware text embed\\u0002dings and text-aware entity embeddings.\\nHowever, these models do not capture categor\\u0002ical and numerical data explicitly. Hugging Face\\ndoes include LXMERT (Tan and Bansal, 2019) to\\nhandle language and vision modality but this can\\nnot be easily adapted for categorical and numeri\\u0002cal data. Nevertheless, existing multimodal Trans\\u0002former models do give good insights into how to\\ncombine categorical and numerical features. ViL\\u0002BERT and VLBERT for example include image\\nmodality as input tokens which lead to one of our\\nsimple baseline of categorical and numerical fea\\u0002tures as additional token inputs to the model. Like\\u0002wise, the gating mechanism Rahman et al. (2020),\\nattention, and different weighting schemes have\\nall been shown to be useful in combining different\\nmodalities.\\n3 Design\\nThe goal of Multimodal-Toolkit is to allow users\\nto quickly adapt state-of-the-art Transformer mod\\u0002els for situations involving text and tabular data\\nwhich occur often in real-world datasets. More\\u0002over, we want to bring the benefits of Transformers\\nto more use cases while making it simple for users\\nFigure 2: The framework of Multimodal-Toolkit.\\nThere is a data processing module that outputs pro\\u0002cessed text, numerical, and categorical features that are\\nthen fed as input to our Transformer With Tabular mod\\u0002ule consisting of a Hugging Face Transformer and our\\ncombining module.\\nof Hugging Face Transformers to adopt. Therefore,\\nwe maintain the existing interface of the popular\\nHugging Face Transformers library.\\nThis design enables us to easily include more\\nTransformer models, leverage strengths of spe\\u0002cific models, use a feature-rich training pipeline,\\nand integrate the thousands of community trained\\nmodels on Hugging Face‚Äôs model hub. We sup\\u0002port a variety of Transformers (e.g. BERT, AL\\u0002BERT, RoBERTa, XLNET) for both classification\\nand regression tasks. All together, this becomes a\\nreusable Transformer With Tabular component. We\\nalso provide a data preprocessing module for cate\\u0002gorical and numerical features. An overview of the\\nsystem is shown in Figure 2. Currently, the library\\nsupports PyTorch Transformers implementations.\\n3.1 Combining Module\\nWe implement a combining module that is model\\nagnostic that takes as input, x, the text features\\noutputted from a Transformer model and prepro\\u0002cessed categorical (c) and numerical (n) features,\\nand outputs a combined multimodal representation\\nm. Although existing multimodal Transformers\\nincorporate cross-modal attention inside middle\\nTransformer layers, we choose the design in which\\nthe modality combination comes after the Trans\\u0002former because this module can be easily included\\nwithout much adaptation of the existing Hugging\\nFace Transformer interface and can be easily ex\\u0002tended to new Transformers included in the future.\\nInside the combining module, we implement var-\\n71\\nCombine Feature Method Equation\\nText only m = x\\nConcat m = x||c||n\\nIndividual MLPs on categorical and\\u0002numerical features then concat (MLP + Concat) m = x||MLP(c)||MLP(n)\\nMLP on concatenated categorical and\\nnumerical features then concat (Concat + MLP) m = x||MLP(c||n)\\nAttention on categorical and numerical\\nfeatures (Attention)\\nm = Œ±x,xWxx + Œ±x,cWcc + Œ±x,nWnn\\nŒ±i,j =\\nexp(LeakyReLU(a\\nT\\n[Wixi||Wjxj ]))\\nP\\nk‚àà{x,c,n}\\nexp(LeakyReLU(aT [Wixi||Wkxk]))\\nGating on categorical and numerical\\nfeatures and then sum (Rahman et al., 2020)\\n(Gating)\\nm = x + Œ±h\\nh = gc \\f (Wcc...\", \"url\": \"https://aclanthology.org/2021.maiworkshop-1.10.pdf\"}, {\"title\": \"Papers with Code - Benchmarking Multimodal AutoML for Tabular Data with Text Fields\", \"text\": \"new\\n\\nGet trending papers in your email inbox once a day!\\n\\nGet trending papers in your email inbox!\\n\\n[Subscribe](https://paperswithcode.com/login?next=%2Fpapers)\\n\\n# Trending Papers\\n\\n## by [AK](https://paperswithcode.com/akhaliq) and the research community\\n\\n- Daily\\n- Weekly\\n- Monthly\\n\\nTrending Papers\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n¬∑Published on Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nhiyouga\\n\\n### [VeOmni: Scaling Any Modality Model Training with Model-Centric\\\\ Distributed Recipe Zoo](https://paperswithcode.com/papers/2508.02317)\\n\\nRecent advances in large language models (LLMs) have driven impressive\\nprogress in omni-modal understanding and generation. However, training\\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\\narchitectures required to process diverse modalities, necessitating\\nsophisticated system design for efficient large-scale training. Existing\\nframeworks typically entangle model definition with parallel logic, incurring\\nlimited scalability and substantial engineering overhead for end-to-end\\nomni-modal training. % We present \\\\\\\\veomni, a modular and efficient training\\nframework to accelerate the development of omni-modal LLMs. \\\\\\\\veomni introduces\\nmodel-centric distributed recipes that decouples communication from\\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\\\\\\\veomni also\\nfeatures a flexible configuration interface supporting seamless integration of\\nnew modalities with minimal code change. % Using \\\\\\\\veomni, a omni-modal\\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\\ntraining large omni-modal LLMs.\\n\\n- 12 authors\\n\\n¬∑Aug 4, 2025\\n\\n[Upvote\\\\\\n\\\\\\n15](https://paperswithcode.com/login?next=%2Fpapers%2F2508.02317)\\n\\n[GitHub717](https://github.com/ByteDance-Seed/VeOmni) [arXiv Page](https://arxiv.org/abs/2508.02317)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n¬∑Published on Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\nFrancisRing\\n\\n### [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://paperswithcode.com/papers/2508.08248)\\n\\nCurrent diffusion models for audio-driven avatar video generation struggle to\\nsynthesize long videos with natural audio synchronization and identity\\nconsistency. This paper presents StableAvatar, the first end-to-end video\\ndiffusion transformer that synthesizes infinite-length high-quality videos\\nwithout post-processing. Conditioned on a reference image and audio,\\nStableAvatar integrates tailored training and inference modules to enable\\ninfinite-length video generation. We observe that the main reason preventing\\nexisting models from generating long videos lies in their audio modeling. They\\ntypically rely on third-party off-the-shelf extractors to obtain audio\\nembeddings, which are then directly injected into the diffusion model via\\ncross-attention. Since current diffusion backbones lack any audio-related\\npriors, this approach causes severe latent distribution error accumulation\\nacross video clips, leading the latent distribution of subsequent segments to\\ndrift away from the optimal distribution gradually. To address this,\\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\\nerror accumulation via time-step-aware modulation. During inference, we propose\\na novel Audio Native Guidance Mechanism to further enhance the audio\\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\\nprediction as a dynamic guidance signal. To enhance the smoothness of the\\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\\nof StableAvatar both qualitatively and quantitatively.\\n\\n- 9 authors\\n\\n¬∑Aug 11, 2025\\n\\n[Upvote\\\\\\n\\\\\\n14](https://paperswithcode.com/login?next=%2Fpapers%2F2508.08248)\\n\\n[GitHub262](https://github.com/Francis-Rings/StableAvatar) [arXiv Page](https://arxiv.org/abs/2508.08248)\\n\\nSubmitted by\\ncsuhan\\n\\n### [ScreenCoder: Advancing Visual-to-Code Generation for Front-End\\\\ Automation via Modular Multimodal Agents](https://paperswithcode.com/papers/2507.22827)\\n\\nAutomating the transformation of user interface (UI) designs into front-end\\ncode holds significant promise for accelerating software development and\\ndemocratizing design workflows. While recent large language models (LLMs) have\\ndemonstrated progress in text-to-code generation, many existing approaches rely\\nsolely o...\", \"url\": \"https://paperswithcode.com/paper/benchmarking-multimodal-automl-for-tabular\"}, {\"title\": \"Towards Benchmarking Foundation Models for Tabular Data With Text\", \"text\": \"Towards Benchmarking Foundation Models for Tabular Data With Text\\nMartin Mraz¬¥\\n* 1 Breenda Das * 1 Anshul Gupta * 1 Lennart Purucker 1 Frank Hutter 2 3 1\\nAbstract\\nFoundation models for tabular data are rapidly\\nevolving, with increasing interest in extending\\nthem to support additional modalities such as free\\u0002text features. However, existing benchmarks for\\ntabular data rarely include textual columns, and\\nidentifying real-world tabular datasets with se\\u0002mantically rich text features is non-trivial. We\\npropose a series of simple yet effective ablation\\u0002style strategies for incorporating text into con\\u0002ventional tabular pipelines. Moreover, we bench\\u0002mark how state-of-the-art tabular foundation mod\\u0002els can handle textual data by manually curating\\na collection of real-world tabular datasets with\\nmeaningful textual features. Our study is an im\\u0002portant step towards improving benchmarking of\\nfoundation models for tabular data with text.\\n1. Introduction\\nFoundation models have begun to transform tabular learn\\u0002ing (Erickson et al., 2020b; Hollmann et al., 2025), echo\\u0002ing the trajectory of other research fields. A natural next\\nstep is mixed-modality tabular modeling, where structured\\ncolumns may also include free-text fields such as job de\\u0002scriptions, clinical notes, or product summaries. Current\\ntabular benchmarks, however, almost never contain textual\\ncolumns, cf. (Bischl et al., 2019; Liu et al., 2024; McEl\\u0002fresh et al., 2024). Moreover, locating real-world datasets\\nwith semantically rich text features is exceptionally difficult,\\nwith even exhaustive searches of OpenML and Kaggle only\\nyielding a handful of usable candidates (Shi et al., 2021).\\nConsequently, current tabular foundation models are rarely\\nevaluated for tabular data with text.\\nPipelines that can handle tabular data with text vary greatly\\n*Equal contribution 1Department of Computer Science, Uni\\u0002versity of Freiburg, Freiburg, Germany 2\\nPrior Labs, Freiburg,\\nGermany 3ELLIS Institute, Tubingen, Germany. Correspon\\u0002dence to: Martin Mraz <mrazm@informatik.uni-freiburg.de>,\\nBreenda Das <dasb@informatik.uni-freiburg.de>, Anshul Gupta\\n<guptaa@informatik.uni-freiburg.de>.\\nProceedings of the 42 nd International Conference on Machine\\nLearning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\\nby the author(s).\\nin their implementation. AutoGluon‚Äôs AutoMLPipeline\\u0002FeatureGenerator (Erickson et al., 2020a) converts text to\\nsparse vectors; CARTE (Kim et al., 2024) applies fastText\\nsentence embeddings (Bojanowski et al., 2017). and the\\nTabPFNv2 API accepts raw text but does not disclose its\\nmethodology. These divergent choices raise a fundamental\\nquestion: Which embedding strategy works best, and under\\nwhat conditions?\\nTo answer this question, we present the first systematic\\nstudy of predictive machine learning with foundation mod\\u0002els for tabular data with text. We study the performance\\nof three representative embedding routes: fastText, Skrub‚Äôs\\nTableVectorizer, and AutoGluon‚Äôs text encoder. We show\\nqualitatively, with a simple synthetic counter-example, that\\nboth n-gram based and off-the-shelf sentence embeddings\\ncan fail to recover highly predictive semantic patterns. More\\u0002over, quantitatively, we evaluate these methods on a man\\u0002ually curated set of real-world tabular benchmark that (i)\\ncontain genuinely informative free-text columns (ii) spans\\nover a variety of domains and samples.\\nOur contributions are: (I) A qualitative study show cas\\u0002ing the limitations of standard n-gram based and generic\\nNLP-based embeddings for tabular tasks with text. (II) A\\nmanually curated set of real-world tabular datasets with se\\u0002mantically rich textual columns. (III) An empirical study of\\nthree text embedding pipelines for TabPFNv2 and XGBoost\\nwith the TabPFNv2 API and AutoGluon Tabular Predictor\\nas baselines.\\nOur study reveals the limitations of current methods and\\nunderscores the need for new methods to handle tabular data\\nwith free text.\\n2. Related Work\\nApproaches incorporating free-text in tabular learning\\nlargely follow two paradigms. First, row-as-text meth\\u0002ods serialize entire rows into prompts and delegate pre\\u0002diction to a large language model (LLM), as seen in\\nTABLLM (Hegselmann et al., 2022), TABLE-LLM (Zhang\\net al., 2024). These work well when textual fields dominate\\nor in few-shot settings. Second, per-column embedding\\nstrategies extract textual embeddings from a single or groups\\nof features, while preserving the structural nature of tabular\\n1\\narXiv:2507.07829v1 [cs.LG] 10 Jul 2025\\nTowards Benchmarking Foundation Models for Tabular Data With Text\\nFigure 1. Qualitative investigation of textual embeddings for tabular prediction tasks: TabPFN v2 accuracy across five datasets.\\nBaselines ‚ÄúNo-Text‚Äù uses original input features, ‚ÄúComplete Leak‚Äù has target leakage, therefore 100% accuracy for all. Following tests\\nembed targets into textual modality. ‚ÄúN-Gram Break‚Äù shows TF-IDF breaking under unseen synonyms, ‚ÄúSimple NLP break‚Äù shows\\nFastText underperforming under noise, ‚ÄúLLM Breaks‚Äù shows BERT variant breaking under semantic ambiguity.\\ndata. They embed each column using fastText or LLMs\\nand then concatenate the resulting embeddings to the table,\\nor replace the feature with its individual textual embed\\u0002ding, cf. (Koloski et al., 2025; Carballo et al., 2023; Kim\\net al., 2024). Grinsztajn et al. (2023) compare 30+ LLM\\nand substring-based methods for embedding string columns,\\nshowing when LLMs yield better representations.\\nIn this study, we investigate pre-column embeddings be\\u0002cause we focus on a many-shot setting (e.g., more than 32\\ntraining samples), in which LLM-based predictive methods\\ndo not perform well, cf. (Hegselmann et al., 2023) and (Ma\\net al., 2024), or require a prohibitive amount of fine-tuning\\n(Shysheya et al., 2025).\\nMost popular prior tabular benchmarks contain no free-text\\nfield, cf. (Bischl et al., 2019; Gijsbers et al., 2022; McEl\\u0002fresh et al., 2023). For tabular data with text, Shi et al.\\n(2021) curated 18 datasets with mixed modalities, but many\\nrely almost entirely on text, with only one or two tabular\\nfeatures, or features derived from the long text, e.g. # of\\nwords (Tang et al., 2024). Thus, they benchmark text-based\\nmodels rather than tabular models, focusing on text data\\nwith tabular features. In contrast, we focus on tabular mod\\u0002els extended to handle text, focusing on tabular data with\\nadditional text features. Other studies were often limited to\\nan evaluation with just one or two datasets (Carballo et al.,\\n2023; Lu et al., 2023). Overall, existing studies for tabular\\ndata with text lack a domain-agnostic benchmark where\\ntextual features complement, rather than dominate, tabular\\ndata. This motivates the new benchmark we present.\\nThe CARTE Benchmark. The latest seminal work on\\nbenchmarking for tabular data with text is the CARTE (Kim\\net al., 2024) benchmark. It includes 51 datasets for tabu\\u0002lar data with text. However, when we investigated these\\ndatasets more closely, we found that at most 11 out of the\\n51 datasets are suited to evaluate tabular data with text.\\nMoreover, our manually curated collection of datasets for\\nbenchmarking only includes 1 out of the 51 datasets.\\nWe share an extensive report of our investigation in Ap\\u0002pendix G. In short, we found that most datasets (a) do not\\nrepresent predictive machine learning tasks for tabular data\\nwith text; (b) are skewed towards text data representing\\nmany categories instead of longer free text; (c) were pre\\u0002processed manually per dataset with logic that seems to\\nfavor CARTE; (d) or were very similar datasets from the\\nsame domain, with the same features, or similar semantic\\ncontent. Thus, while CARTE was a significant step toward\\nbenchmarking and learning for tabular data with text, it falls\\nshort in several aspects. Our work complements CARTE‚Äôs\\nefforts and aims to improve benchmarking for tabular data\\nwith text further.\\n3. Qualitative Investigation\\nIn this section, we evaluate popular text embedding tech\\u0002niques: N-Grams, simple NLP models and LLMs for tabular\\npr...\", \"url\": \"https://arxiv.org/pdf/2507.07829\"}]}",
        "tool.duration_ms": 5960.458278656006
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "6f5c21aafaab5421",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1767958612432634771,
      "end_time": 1767958616514855032,
      "duration": 4.082220261,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7f38e8369d50>\", \"input\": {\"query\": \"How do Kaggle winners handle class imbalance in binary classification with 75/25 split?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nTop Kaggle winners typically tackle a 75‚ÄØ%‚ÄØ/‚ÄØ25‚ÄØ% binary‚Äëclass split by reshaping the training data and by using metrics and model settings that reward performance on the minority class.‚ÄØThey combine resampling, class‚Äëweighting, and careful validation to keep the imbalance from biasing the model‚ÄØ([Dealing with extremely imbalanced dataset ‚Äì Kaggle](https://www.kaggle.com/questions-and-answers/473546);‚ÄØ[Unbalanced data and cross‚Äëvalidation ‚Äì Kaggle](https://www.kaggle.com/discussions/questions-and-answers/27589)).\\n\\n**Step‚Äëby‚Äëstep workflow**\\n\\n1. **Inspect the split and pick an imbalance‚Äëaware metric** ‚Äì report AUC‚ÄëROC, PR‚ÄëAUC or log‚Äëloss instead of plain accuracy, because these metrics reflect minority‚Äëclass performance‚ÄØ([Unbalanced data and cross‚Äëvalidation ‚Äì Kaggle](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \\n\\n2. **Apply resampling to the training folds**  \\n   * *Undersample* the majority (75‚ÄØ%) class to a manageable size, or  \\n   * *Oversample* the minority (25‚ÄØ%) class with synthetic techniques such as **SMOTE** or ADASYN. Both approaches are standard in Kaggle solutions‚ÄØ([Dealing with extremely imbalanced dataset ‚Äì Kaggle](https://www.kaggle.com/questions-and-answers/473546);‚ÄØ[Resampling strategies for imbalanced datasets ‚Äì Kaggle](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets)).  \\n\\n3. **Add class weights (or sample weights) to the loss function** ‚Äì most libraries (e.g., XGBoost, LightGBM, sklearn) accept a `scale_pos_weight` or `class_weight` argument; setting a higher weight for the minority class forces the model to penalize its errors more heavily.  \\n\\n4. **Train robust models and ensembles** ‚Äì gradient‚Äëboosted trees, neural nets, or stacked ensembles are common; ensemble methods help smooth out variance introduced by resampling‚ÄØ([Dealing with extremely imbalanced dataset ‚Äì Kaggle](https://www.kaggle.com/questions-and-answers/473546)).  \\n\\n5. **Validate with stratified cross‚Äëvalidation** ‚Äì ensure each fold preserves the 75/25 ratio so that performance estimates remain realistic and that resampling/weighting decisions are not over‚Äëfitted.  \\n\\n6. **Fine‚Äëtune threshold post‚Äëtraining** ‚Äì after the model is fitted, adjust the decision threshold (e.g., using the ROC curve) to balance precision vs. recall according to the competition‚Äôs cost of false positives vs. false negatives.  \\n\\n7. **Monitor the minority‚Äëclass metrics on the hold‚Äëout leaderboard** ‚Äì iterate on resampling ratios, class‚Äëweight values, and ensemble composition until the minority‚Äëclass AUC/PR‚ÄëAUC improves.  \\n\\nFollowing these concise steps lets you handle a 75‚ÄØ%‚ÄØ/‚ÄØ25‚ÄØ% imbalance the way Kaggle champions do, keeping the minority class well‚Äërepresented in both training and evaluation.\\n\\n--- Result 2 ---\\nTitle: Dealing with extremely imbalanced dataset - binary classification\\nURL: https://www.kaggle.com/questions-and-answers/473546\\n\\nContent:\\nChecking your browser - reCAPTCHA\\nChecking your browser before accessing www.kaggle.com ...\\nClick[here](#)if you are not automatically redirected after 5 seconds.\\n\\n--- Result 3 ---\\nTitle: Resampling strategies for imbalanced datasets - Kaggle\\nURL: https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets\\n\\nContent:\\n- _code_\\n\\nNew Notebook\\n\\n- _table\\\\_chart_\\n\\nNew Dataset\\n\\n- _tenancy_\\n\\nNew Model\\n\\n- _emoji\\\\_events_\\n\\nNew Competition\\n\\n- _corporate\\\\_fare_\\n\\nNew Organization\\n\\n\\n## No Active Events\\n\\nCreate notebooks and keep track of their status here.\\n\\n[_add_ New Notebook](https://www.kaggle.com/code/new)\\n\\n- _auto\\\\_awesome\\\\_motion_\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0 Active Events\\n\\n\\n\\n\\n\\n\\n_expand\\\\_more_\\n\\n\\nmenu\\n\\n[Skip to\\\\\\n\\\\\\ncontent](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets#site-content)\\n\\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\\n\\nCreate\\n\\n_search_\\n\\n- [_explore_\\\\\\n\\\\\\nHome](https://www.kaggle.com/)\\n\\n- [_emoji\\\\_events_\\\\\\n\\\\\\nCompetitions](https://www.kaggle.com/competitions)\\n\\n- [_table\\\\_chart_\\\\\\n\\\\\\nDatasets](https://www.kaggle.com/datasets)\\n\\n- [_tenancy_\\\\\\n\\\\\\nModels](https://www.kaggle.com/models)\\n\\n- [_code_\\\\\\n\\\\\\nCode](https://www.kaggle.com/code)\\n\\n- [_comment_\\\\\\n\\\\\\nDiscussions](https://www.kaggle.com/discussions)\\n\\n- [_school_\\\\\\n\\\\\\nLearn](https://www.kaggle.com/learn)\\n\\n- [_expand\\\\_more_\\\\\\n\\\\\\nMore](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets)\\n\\n\\n_auto\\\\_awesome\\\\_motion_\\n\\nView Active Events\\n\\nmenu\\n\\n[Skip to\\\\\\n\\\\\\ncontent](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets#site-content)\\n\\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\\n\\n_search_\\n\\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Frafjaa%2Fresampling-strategies-for-imbalanced-datasets)\\n\\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Frafjaa%2Fresampling-strategies-for-imbalanced-datasets)\\n\\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\n\\n[Learn more.](https://www.kaggle.com/cookies)\\n\\nOK, Got it.\\n\\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\\n\\n###### Something went wrong and this page crashed!\\n\\nIf the issue persists, it's likely a problem on our side.\\n\\n```\\n\\nLoading CSS chunk 5073 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\\n```\\n\\nRefresh\\n\\n--- Result 4 ---\\nTitle: Unbalanced data and cross-validation | Kaggle\\nURL: https://www.kaggle.com/discussions/questions-and-answers/27589\\n\\nContent:\\n<div><div><p>There are two general ways of dealing with imbalanced data: 1) change the data; 2) leave the data alone but change the performance metric and/or the weight of individual data points.</p>\\n<p>Changing the data means oversampling the under-represented class(es) with synthetic data points, or undersampling (thinning down) the over-represented class(es). There are various ways of doing both operations, and I will give you one <a href=\\\"https://github.com/scikit-learn-contrib/imbalanced-learn\\\"><strong>starting point</strong></a>.</p>\\n<p>A second option is to leave the data alone and adjust how we do the training. It is crucial to pick a reliable performance metric for imbalanced data, and to know whether for your purposes it is worse to have too many false positives (FPs) or false negatives (FNs). Ideally, you wouldn't want to have either of the two, but for most applications it should be clear whether we want to avoid FPs or FNs with greater preference.</p>\\n<p>To give you an example: you have 900 healthy patients (class 0) and 100 cancer patients (class 1), which is at least somewhat close to the true ratio of healthy and cancer-stricken people out there. If you predict class 0 with 100% accuracy and class 1 with 70% accuracy, your accuracy will be 97% for the whole dataset. That sounds tremendous until we realize that 30 out of 100 cancer patients (30%) will go undiagnosed, meaning that high false negative rate in this case will cost many lives. What if we could adjust our classification so that both classes are predicted with 97% accuracy? This would mean that 27 healthy people would be predicted to have cancer (27 FPs) and 3 cancer-stricken individual would be predicted to be healthy (3 FNs). Now, I wouldn't want to be a doctor who tells a healthy person that they have a cancer because many people have been known to do silly things when they think they are dying. Still, compared to our starting premise, most would agree that on balance it is much better to have 27 people who think they have cancer and later find out they don't, and only 3 patients who were told they are healthy even though they have cancer. The problem is that a simple accuracy as a performance metric for training would never get you to a balanced 97% accuracy. For imbalanced datasets, F1-score (a weighted average of precision and recall), area under curve for ROC and Matthews correlation coefficient may be better measures of classifier quality.</p>\\n<p>See the links below for more info:</p>\\n<p><a href=\\\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\\\"><strong>link1</strong></a></p>\\n<p><a href=\\\"https://eva.fing.edu.uy/mod/resource/view.php?id=33977\\\"><strong>link2</strong></a></p>\\n<p><a href=\\\"https://www.kaggle.com/c/bosch-production-line-performance/forums/t/24120/time-effort-and-reward?forumMessageId=138285#post138285\\\"><strong>link3</strong></a></p></div></div>\\n\\n--- Result 5 ---\\nTitle: Understanding Class Imbalance in 1 Minute! - Kaggle\\nURL: https://www.kaggle.com/getting-started/467908\\n\\nContent:\\nChecking your browser - reCAPTCHA\\nChecking your browser before accessing www.kaggle.com ...\\nClick[here](#)if you are not automatically redirected after 5 seconds.\\n\\n--- Result 6 ---\\nTitle: Binary classification with strongly unbalanced classes\\nURL: https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes\\n\\nContent:\\n**Teams**\\n\\nQ&A for work\\n\\nConnect and share knowledge within a single location that is structured and easy to search.\\n\\n[Learn more about Teams](https://stackoverflow.co/teams/)\\n\\n# [Binary classification with strongly unbalanced classes](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes)\\n\\n[Ask Question](https://stats.stackexchange.com/questions/ask)\\n\\nAsked7 years, 9 months ago\\n\\nModified [3 years, 11 months ago](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes?lastactivity)\\n\\nViewed\\n56k times\\n\\n67\\n\\n$\\\\\\\\begingroup$\\n\\nI have a data set in the form of (features, binary output 0 or 1), but 1 happens pretty rarely, so just by always predicting 0, I get accuracy between 70% and 90% (depending on the particular data I look at). The ML methods give me about the same accuracy, and I feel, there should be some standard methods to apply in this situation, that would improve the accuracy over the obvious prediction rule.\\n\\n- [machine-learning](https://stats.stackexchange.com/questions/tagged/machine-learning)\\n- [classification](https://stats.stackexchange.com/questions/tagged/classification)\\n- [binary-data](https://stats.stackexchange.com/questions/tagged/binary-data)\\n- [unbalanced-classes](https://stats.stackexchange.com/questions/tagged/unbalanced-classes)\\n\\n[Share](https://stats.stackexchange.com/q/235808)\\n\\nCite\\n\\n[Improve this question](https://stats.stackexchange.com/posts/235808/edit)\\n\\nFollow\\n\\n[edited Oct 30, 2017 at 19:47](https://stats.stackexchange.com/posts/235808/revisions)\\n\\nLazyCat\\n\\nasked Sep 19, 2016 at 18:39\\n\\n[![LazyCat's user avatar](https://www.gravatar.com/avatar/0de65f380a7e1cb28d4f0c9c5f93fdde?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/30893/lazycat)\\n\\n[LazyCat](https://stats.stackexchange.com/users/30893/lazycat) LazyCat\\n\\n87211 gold badge77 silver badges1111 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n6\\n\\n- 4\\n\\n\\n\\n\\n\\n$\\\\\\\\begingroup$I've found this article to be very helpful on the subject: [svds.com/learning-imbalanced-classes](http://www.svds.com/learning-imbalanced-classes/)$\\\\\\\\endgroup$\\n\\n‚Äì¬†[J. O'Brien Antognini](https://stats.stackexchange.com/users/115448/j-obrien-antognini)\\n\\nCommentedSep 20, 2016 at 23:18\\n\\n- $\\\\\\\\begingroup$@J.O'BrienAntognini That is a really nice article!$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Jinhua Wang](https://stats.stackexchange.com/users/163503/jinhua-wang)\\n\\nCommentedJan 27, 2019 at 17:22\\n\\n- $\\\\\\\\begingroup$Some useful discussion here: [stats.stackexchange.com/questions/285231/‚Ä¶](https://stats.stackexchange.com/questions/285231/what-problem-does-oversampling-undersampling-and-smote-solve?r=SearchResults&s=3|0.0000) and [stats.stackexchange.com/questions/283170/‚Ä¶](https://stats.stackexchange.com/questions/283170/when-is-unbalanced-data-really-a-problem-in-machine-learning)$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Sycorax](https://stats.stackexchange.com/users/22311/sycorax) ‚ô¶\\n\\nCommentedJul 15, 2020 at 13:02\\n\\n- $\\\\\\\\begingroup$This post [here](https://stats.stackexchange.com/questions/247871/what-is-the-root-cause-of-the-class-imbalance-problem) has a very clear answer on _why_ class imbalance causes problems.$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Stephen G](https://stats.stackexchange.com/users/225551/stephen-g)\\n\\nCommentedJul 15, 2020 at 13:57\\n\\n- $\\\\\\\\begingroup$@StephenG At present, the post you've linked to has three answers. Which one are you referring to?$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Sycorax](https://stats.stackexchange.com/users/22311/sycorax) ‚ô¶\\n\\nCommentedJul 15, 2020 at 17:49\\n\\n\\n\\\\|¬†[Show **1** more comment](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes)\\n\\n## 7 Answers 7\\n\\nSorted by:\\n[Reset to default](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes?answertab=scoredesc#tab-top)\\n\\nHighest score (default)Date modified (newest first)Date created (oldest first)\\n\\n39\\n\\n$\\\\\\\\begingroup$\\n\\nBoth hxd1011 and Frank are right (+1).\\nEssentially resampling and/or cost-sensitive learning are the two main ways of getting around the problem of imbalanced data; third is to use kernel methods that sometimes might be less effected by the class imbalance.\\nLet me stress that there is no silver-bullet solution. By definition you have one class that is represented inadequately in your samples.\\n\\nHaving said the above I believe that you will find the algorithms [SMOTE](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106) and [ROSE](https://journal.r-project.org/archive/2014-1/menardi-lunardon-torelli.pdf) very helpful. SMOTE effectively uses a $k$-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. ROSE tries to create estimates of the underlying distributions of the two classes using a smoothed bootstrap approach and sample them for synthetic examples. Both are readily available in R, SMOTE in the package [DMwR](https://cran.r-project.org/web/packages/DMwR/index.html) and ROSE in the package with the [same name](https://cran.r-project.org/web/packages/ROSE/index.html). Both SMOTE and ROSE result in a training dataset that is smaller than the original one.\\n\\nI would probably argue that a better (or less bad) metric for the case of imbalanced data is using [Cohen's $k$](https://en.wikipedia.org/wiki/Cohen's_kappa) and/or [Receiver operating characteristic's Area under the curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\\nCohen's kappa directly controls for the expected accuracy, AUC as it\\nis a function of sensitivity and specificity, the curve is insensitive to disparities in the class proportions. Again, notice that these are just metrics that should be used with a large grain of salt. You should ideally adapt them to your specific problem taking account of the gains and costs correct and wrong classifications convey in your case. I have found that looking at [lift-curves](https://en.wikipedia.org/wiki/Lift_%28data_mining%29) is actually rather informative for this matter.\\nIrrespective of your metric you should try to use a separate test to assess the performance of your algorithm; exactly because of the class imbalanced over-fitting is even more likely so out-of-sample testing is crucial.\\n\\nProbably the most popular recent paper on the matter is [Learning from Imbalanced Data](http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf) by He and Garcia. It gives a very nice overview of the points raised by myself and in other answers. In addition I believe that the walk-through on [Subsampling For Class Imbalances](http://topepo.github.io/caret/subsampling-for-class-imbalances.html), presented by Max Kuhn as part of the [caret](https://cran.r-project.org/web/packages/caret/index.html) package is an excellent resource to get a structure example of how under-/over-sampling as well as synthetic data creation can measure against each other.\\n\\n[Share](https://stats.stackexchange.com/a/235824)\\n\\nCite\\n\\n[Improve this answer](https://stats.stackexchange.com/posts/235824/edit)\\n\\nFollow\\n\\n[edited Sep 19, 2016 at 20:12](https://stats.stackexchange.com/posts/235824/revisions)\\n\\nanswered Sep 19, 2016 at 19:44\\n\\n[![usŒµr11852's user avatar](https://i.sstatic.net/Y88zz.png?s=64)](https://stats.stackexchange.com/users/11852/us%ce%b5r11852)\\n\\n[usŒµr11852](https://stats.stackexchange.com/users/11852/us%ce%b5r11852) usŒµr11852\\n\\n44.7k33 gold badges101101 silver badges158158 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n3\\n\\n- $\\\\\\\\begingroup$A third somewhat popular (and again not too appropriate) metric is the Area-Under-the-Curve of the Precision-Recall curve. [Davis & James, 2006](https://www.biostat.wisc.edu/~page/rocpr.pdf) paper is consider a classic on the matter; CV has a good thread [too](http://stats.stackexchange.com/questions/7207). I recently saw a paper with the somewhat hammy title \\\" [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Dat...\\n\\n--- Result 7 ---\\nTitle: Best techniques and metrics for Imbalanced Dataset - Kaggle\\nURL: https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset\\n\\nContent:\\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\n\\n[Learn more](https://www.kaggle.com/cookies)\\n\\nOK, Got it.\\n\\n###### Something went wrong and this page crashed!\\n\\nIf the issue persists, it's likely a problem on our side.\\n\\n```\\n\\nLoading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=805aeed0a20bb825939e:1:11032)\\n```\\n\\nRefresh\\n\\n--- Result 8 ---\\nTitle: Class Imbalance Strategies ‚Äî A Visual Guide with Code - Medium\\nURL: https://medium.com/data-science/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a\\n\\nContent:\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8bc8fae71e1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-8bc8fae71e1a---------------------------------------)\\n\\n¬∑\\n\\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\\n\\n# Class Imbalance Strategies ‚Äî A Visual Guide with Code\\n\\n## Understand Random Undersampling, Oversampling, SMOTE, ADASYN, and Tomek Links\\n\\n[Travis Tang](https://medium.com/@travis-tang?source=post_page---byline--8bc8fae71e1a---------------------------------------)\\n\\n13 min read\\n\\n¬∑\\n\\nApr 24, 2023\\n\\n--\\n\\n1\\n\\nListen\\n\\nShare\\n\\nClass imbalance occurs when one class in a classification problem significantly outweighs the other class. It‚Äôs common in many machine learning problems. Examples include fraud detection, anomaly detection, and medical diagnosis.\\n\\n## The Curse of Class Imbalance\\n\\nA model trained on an imbalanced dataset perform poorly on the minority class. At best, this can cause loss to the business in the case of a churn analysis. At worst, it can pervade systemic bias of a face recognition system.\\n\\nPress enter or click to view image in full size\\n\\nA balanced dataset might just be the missing ingredient (Source: Elena Mozhvilo on [Unsplash](https://unsplash.com/photos/j06gLuKK0GM))\\n\\nThe common approach to class imbalance is resampling. These can entail oversampling the majority class, undersampling the minority class, or a combination of both.\\n\\nIn this post, I use vivid visuals and code to illustrate these strategies for class imbalance:\\n\\n1. Random oversampling\\n2. Random undersampling\\n3. Oversampling with SMOTE\\n4. Oversampling with ADASYN\\n5. Undersampling with Tomek Link\\n6. Oversampling with SMOTE, then undersample with TOMEK Link (SMOTE-Tomek)\\n\\nI will also be using these strategies on a real-world dataset, and evaluate their impact on a machine learning model. Let‚Äôs go.\\n\\n> All source code is here.\\n\\n## Using Imbalance-learn\\n\\nWe will use the `imbalanced-learn` package in python to solve our imbalanced class problem. It is an open-sourced library relying on scikit-learn and provides tools when dealing with classification with imbalanced classes.\\n\\nTo install it, use the command.\\n\\n```\\npip install -U imbalanced-learn\\n```\\n\\n## Dataset\\n\\nThe dataset that we are using is the [**Communities and Crime Data Set by UCI (CC BY 4.0)**.](https://archive.ics.uci.edu/ml/datasets/communities+and+crime) It contains 100 attributes of 1994 U.S. communities. We can use this to predict if the **crime rate is high** (defined as having **per capita violent crime** above 0.65). The data source is available in the UCI Machine Learning Repository and is created by Michael Redmond from La Salle University (Published in 2009).\\n\\n> The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units.\\n\\nThis dataset is imbalanced. It has 12 communities with low crime rates for every 1 community of high crime rate. This is perfect to illustrate our use case.\\n\\n```\\n>>> from imblearn.datasets import fetch_datasets>>> # Fetch dataset from imbalanced-learn library >>> # as a dictionary of numpy array>>> us_crime = fetch_datasets()['us_crime']>>> us_crime{'data': array([[0.19, 0.33, 0.02, ..., 0.26, 0.2 , 0.32],        [0.  , 0.16, 0.12, ..., 0.12, 0.45, 0.  ],        [0.  , 0.42, 0.49, ..., 0.21, 0.02, 0.  ],        ...,        [0.16, 0.37, 0.25, ..., 0.32, 0.18, 0.91],        [0.08, 0.51, 0.06, ..., 0.38, 0.33, 0.22],        [0.2 , 0.78, 0.14, ..., 0.3 , 0.05, 1.  ]]), 'target': array([-1,  1, -1, ..., -1, -1, -1]), 'DESCR': 'us_crime'}\\n```\\n\\nWe will convert this dictionary to a Pandas dataframe, then split it into train-test splits.\\n\\n```\\n# Convert the dictionary to a pandas dataframecrime_df = pd.concat([pd.DataFrame(us_crime['data'], columns = [f'data_{i}' for i in range(us_crime.data.shape[1])]),           pd.DataFrame(us_crime['target'], columns = ['target'])], axis = 1)# Split data into train test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(crime_df.drop('target', axis = 1),                                                     crime_df['target'],                                                     test_size = 0.4,                                                     random_state = 42)\\n```\\n\\nNote that we will only perform under- and over-sampling only on the train dataset. We will _not_ change the test sets with under- and over-sampling.\\n\\n### Preprocessing the dataset\\n\\nOur goal is to have a visualize an imbalanced dataset. In order to visualize the 128-dimensional dataset in a 2D graph, we do the following on the train set.\\n\\n- scale the dataset,\\n- perform Principle Component Analysis (PCA) on the features to convert the 100 features to 2 principle components,\\n- visualize the data.\\n\\nHere‚Äôs the data, visualized in 2D.\\n\\nImage by author\\n\\nCode for the above graph:\\n\\n```\\nfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt# Scale the dataset on both train and test sets.# Note that we fit MinMaxScaler on X_train only, not on the entire dataset.# This prevents data leakage from test set to train set.scaler = MinMaxScaler()scaler.fit(X_train)X_train = scaler.transform(X_train)X_test = scaler.transform(X_test)# Perform PCA Decomposition on both train and test sets# Note that we fit PCA on X_train only, not on the entire dataset.# This prevents data leakage from test set to train set.pca = PCA(n_components=2)pca.fit(X_train)X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)# Function for plotting dataset def plot_data(X,y,ax,title):    ax.scatter(X[:, 0], X[:, 1], c=y, alpha=0.5, s = 30, edgecolor=(0,0,0,0.5))    ax.set_ylabel('Principle Component 1')    ax.set_xlabel('Principle Component 2')    if title is not None:        ax.set_title(title)# Plot datasetfig,ax = plt.subplots(figsize=(5, 5))plot_data(X_train_pca, y_train, ax, title='Original Dataset')\\n```\\n\\nWith the preprocessing done, we are ready to resample our dataset.\\n\\n## Strategy 1. Random Oversampling\\n\\nRandom oversampling duplicates existing examples from the minority class with replacement. Each data point in the minority class has an equal probability of being duplicated.\\n\\nPress enter or click to view image in full size\\n\\nImage by author\\n\\nHere‚Äôs how to we can perform oversampling on our dataset.\\n\\n```\\nfrom imblearn.over_sampling import RandomOverSampler# Perform random oversamplingros = RandomOverSampler(random_state=0)X_train_ros, ...\\n\\n--- Result 9 ---\\nTitle: Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\\n\\nContent:\\nTabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\\n**üì£ BIG NEWS:****Neptune is joining OpenAI!**‚Üí Read the message from our CEO üì£![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\\nWhat do you want to find?\\nSearch\\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\\n[Shahul ES](https://neptune.ai/blog/author/shahules)\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top tabular data competitions. Without much lag, let‚Äôs begin.\\nThese are the five competitions that I have gone through to create this article:\\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\\n* Use[cudf](https://github.com/rapidsai/cudf).\\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let‚Äôs see the methods used to do it.\\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Top Kaggle winners typically tackle a 75‚ÄØ%‚ÄØ/‚ÄØ25‚ÄØ% binary‚Äëclass split by reshaping the training data and by using metrics and model settings that reward performance on the minority class.‚ÄØThey combine resampling, class‚Äëweighting, and careful validation to keep the imbalance from biasing the model‚ÄØ([Dealing with extremely imbalanced dataset ‚Äì Kaggle](https://www.kaggle.com/questions-and-answers/473546);‚ÄØ[Unbalanced data and cross‚Äëvalidation ‚Äì Kaggle](https://www.kaggle.com/discussions/questions-and-answers/27589)).\\n\\n**Step‚Äëby‚Äëstep workflow**\\n\\n1. **Inspect the split and pick an imbalance‚Äëaware metric** ‚Äì report AUC‚ÄëROC, PR‚ÄëAUC or log‚Äëloss instead of plain accuracy, because these metrics reflect minority‚Äëclass performance‚ÄØ([Unbalanced data and cross‚Äëvalidation ‚Äì Kaggle](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \\n\\n2. **Apply resampling to the training folds**  \\n   * *Undersample* the majority (75‚ÄØ%) class to a manageable size, or  \\n   * *Oversample* the minority (25‚ÄØ%) class with synthetic techniques such as **SMOTE** or ADASYN. Both approaches are standard in Kaggle solutions‚ÄØ([Dealing with extremely imbalanced dataset ‚Äì Kaggle](https://www.kaggle.com/questions-and-answers/473546);‚ÄØ[Resampling strategies for imbalanced datasets ‚Äì Kaggle](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets)).  \\n\\n3. **Add class weights (or sample weights) to the loss function** ‚Äì most libraries (e.g., XGBoost, LightGBM, sklearn) accept a `scale_pos_weight` or `class_weight` argument; setting a higher weight for the minority class forces the model to penalize its errors more heavily.  \\n\\n4. **Train robust models and ensembles** ‚Äì gradient‚Äëboosted trees, neural nets, or stacked ensembles are common; ensemble methods help smooth out variance introduced by resampling‚ÄØ([Dealing with extremely imbalanced dataset ‚Äì Kaggle](https://www.kaggle.com/questions-and-answers/473546)).  \\n\\n5. **Validate with stratified cross‚Äëvalidation** ‚Äì ensure each fold preserves the 75/25 ratio so that performance estimates remain realistic and that resampling/weighting decisions are not over‚Äëfitted.  \\n\\n6. **Fine‚Äëtune threshold post‚Äëtraining** ‚Äì after the model is fitted, adjust the decision threshold (e.g., using the ROC curve) to balance precision vs. recall according to the competition‚Äôs cost of false positives vs. false negatives.  \\n\\n7. **Monitor the minority‚Äëclass metrics on the hold‚Äëout leaderboard** ‚Äì iterate on resampling ratios, class‚Äëweight values, and ensemble composition until the minority‚Äëclass AUC/PR‚ÄëAUC improves.  \\n\\nFollowing these concise steps lets you handle a 75‚ÄØ%‚ÄØ/‚ÄØ25‚ÄØ% imbalance the way Kaggle champions do, keeping the minority class well‚Äërepresented in both training and evaluation.\", \"url\": \"\"}, {\"title\": \"Dealing with extremely imbalanced dataset - binary classification\", \"text\": \"Checking your browser - reCAPTCHA\\nChecking your browser before accessing www.kaggle.com ...\\nClick[here](#)if you are not automatically redirected after 5 seconds.\", \"url\": \"https://www.kaggle.com/questions-and-answers/473546\"}, {\"title\": \"Resampling strategies for imbalanced datasets - Kaggle\", \"text\": \"- _code_\\n\\nNew Notebook\\n\\n- _table\\\\_chart_\\n\\nNew Dataset\\n\\n- _tenancy_\\n\\nNew Model\\n\\n- _emoji\\\\_events_\\n\\nNew Competition\\n\\n- _corporate\\\\_fare_\\n\\nNew Organization\\n\\n\\n## No Active Events\\n\\nCreate notebooks and keep track of their status here.\\n\\n[_add_ New Notebook](https://www.kaggle.com/code/new)\\n\\n- _auto\\\\_awesome\\\\_motion_\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0 Active Events\\n\\n\\n\\n\\n\\n\\n_expand\\\\_more_\\n\\n\\nmenu\\n\\n[Skip to\\\\\\n\\\\\\ncontent](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets#site-content)\\n\\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\\n\\nCreate\\n\\n_search_\\n\\n- [_explore_\\\\\\n\\\\\\nHome](https://www.kaggle.com/)\\n\\n- [_emoji\\\\_events_\\\\\\n\\\\\\nCompetitions](https://www.kaggle.com/competitions)\\n\\n- [_table\\\\_chart_\\\\\\n\\\\\\nDatasets](https://www.kaggle.com/datasets)\\n\\n- [_tenancy_\\\\\\n\\\\\\nModels](https://www.kaggle.com/models)\\n\\n- [_code_\\\\\\n\\\\\\nCode](https://www.kaggle.com/code)\\n\\n- [_comment_\\\\\\n\\\\\\nDiscussions](https://www.kaggle.com/discussions)\\n\\n- [_school_\\\\\\n\\\\\\nLearn](https://www.kaggle.com/learn)\\n\\n- [_expand\\\\_more_\\\\\\n\\\\\\nMore](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets)\\n\\n\\n_auto\\\\_awesome\\\\_motion_\\n\\nView Active Events\\n\\nmenu\\n\\n[Skip to\\\\\\n\\\\\\ncontent](https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets#site-content)\\n\\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\\n\\n_search_\\n\\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Frafjaa%2Fresampling-strategies-for-imbalanced-datasets)\\n\\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Frafjaa%2Fresampling-strategies-for-imbalanced-datasets)\\n\\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\n\\n[Learn more.](https://www.kaggle.com/cookies)\\n\\nOK, Got it.\\n\\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\\n\\n###### Something went wrong and this page crashed!\\n\\nIf the issue persists, it's likely a problem on our side.\\n\\n```\\n\\nLoading CSS chunk 5073 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\\n```\\n\\nRefresh\", \"url\": \"https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets\"}, {\"title\": \"Unbalanced data and cross-validation | Kaggle\", \"text\": \"<div><div><p>There are two general ways of dealing with imbalanced data: 1) change the data; 2) leave the data alone but change the performance metric and/or the weight of individual data points.</p>\\n<p>Changing the data means oversampling the under-represented class(es) with synthetic data points, or undersampling (thinning down) the over-represented class(es). There are various ways of doing both operations, and I will give you one <a href=\\\"https://github.com/scikit-learn-contrib/imbalanced-learn\\\"><strong>starting point</strong></a>.</p>\\n<p>A second option is to leave the data alone and adjust how we do the training. It is crucial to pick a reliable performance metric for imbalanced data, and to know whether for your purposes it is worse to have too many false positives (FPs) or false negatives (FNs). Ideally, you wouldn't want to have either of the two, but for most applications it should be clear whether we want to avoid FPs or FNs with greater preference.</p>\\n<p>To give you an example: you have 900 healthy patients (class 0) and 100 cancer patients (class 1), which is at least somewhat close to the true ratio of healthy and cancer-stricken people out there. If you predict class 0 with 100% accuracy and class 1 with 70% accuracy, your accuracy will be 97% for the whole dataset. That sounds tremendous until we realize that 30 out of 100 cancer patients (30%) will go undiagnosed, meaning that high false negative rate in this case will cost many lives. What if we could adjust our classification so that both classes are predicted with 97% accuracy? This would mean that 27 healthy people would be predicted to have cancer (27 FPs) and 3 cancer-stricken individual would be predicted to be healthy (3 FNs). Now, I wouldn't want to be a doctor who tells a healthy person that they have a cancer because many people have been known to do silly things when they think they are dying. Still, compared to our starting premise, most would agree that on balance it is much better to have 27 people who think they have cancer and later find out they don't, and only 3 patients who were told they are healthy even though they have cancer. The problem is that a simple accuracy as a performance metric for training would never get you to a balanced 97% accuracy. For imbalanced datasets, F1-score (a weighted average of precision and recall), area under curve for ROC and Matthews correlation coefficient may be better measures of classifier quality.</p>\\n<p>See the links below for more info:</p>\\n<p><a href=\\\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\\\"><strong>link1</strong></a></p>\\n<p><a href=\\\"https://eva.fing.edu.uy/mod/resource/view.php?id=33977\\\"><strong>link2</strong></a></p>\\n<p><a href=\\\"https://www.kaggle.com/c/bosch-production-line-performance/forums/t/24120/time-effort-and-reward?forumMessageId=138285#post138285\\\"><strong>link3</strong></a></p></div></div>\", \"url\": \"https://www.kaggle.com/discussions/questions-and-answers/27589\"}, {\"title\": \"Understanding Class Imbalance in 1 Minute! - Kaggle\", \"text\": \"Checking your browser - reCAPTCHA\\nChecking your browser before accessing www.kaggle.com ...\\nClick[here](#)if you are not automatically redirected after 5 seconds.\", \"url\": \"https://www.kaggle.com/getting-started/467908\"}, {\"title\": \"Binary classification with strongly unbalanced classes\", \"text\": \"**Teams**\\n\\nQ&A for work\\n\\nConnect and share knowledge within a single location that is structured and easy to search.\\n\\n[Learn more about Teams](https://stackoverflow.co/teams/)\\n\\n# [Binary classification with strongly unbalanced classes](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes)\\n\\n[Ask Question](https://stats.stackexchange.com/questions/ask)\\n\\nAsked7 years, 9 months ago\\n\\nModified [3 years, 11 months ago](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes?lastactivity)\\n\\nViewed\\n56k times\\n\\n67\\n\\n$\\\\\\\\begingroup$\\n\\nI have a data set in the form of (features, binary output 0 or 1), but 1 happens pretty rarely, so just by always predicting 0, I get accuracy between 70% and 90% (depending on the particular data I look at). The ML methods give me about the same accuracy, and I feel, there should be some standard methods to apply in this situation, that would improve the accuracy over the obvious prediction rule.\\n\\n- [machine-learning](https://stats.stackexchange.com/questions/tagged/machine-learning)\\n- [classification](https://stats.stackexchange.com/questions/tagged/classification)\\n- [binary-data](https://stats.stackexchange.com/questions/tagged/binary-data)\\n- [unbalanced-classes](https://stats.stackexchange.com/questions/tagged/unbalanced-classes)\\n\\n[Share](https://stats.stackexchange.com/q/235808)\\n\\nCite\\n\\n[Improve this question](https://stats.stackexchange.com/posts/235808/edit)\\n\\nFollow\\n\\n[edited Oct 30, 2017 at 19:47](https://stats.stackexchange.com/posts/235808/revisions)\\n\\nLazyCat\\n\\nasked Sep 19, 2016 at 18:39\\n\\n[![LazyCat's user avatar](https://www.gravatar.com/avatar/0de65f380a7e1cb28d4f0c9c5f93fdde?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/30893/lazycat)\\n\\n[LazyCat](https://stats.stackexchange.com/users/30893/lazycat) LazyCat\\n\\n87211 gold badge77 silver badges1111 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n6\\n\\n- 4\\n\\n\\n\\n\\n\\n$\\\\\\\\begingroup$I've found this article to be very helpful on the subject: [svds.com/learning-imbalanced-classes](http://www.svds.com/learning-imbalanced-classes/)$\\\\\\\\endgroup$\\n\\n‚Äì¬†[J. O'Brien Antognini](https://stats.stackexchange.com/users/115448/j-obrien-antognini)\\n\\nCommentedSep 20, 2016 at 23:18\\n\\n- $\\\\\\\\begingroup$@J.O'BrienAntognini That is a really nice article!$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Jinhua Wang](https://stats.stackexchange.com/users/163503/jinhua-wang)\\n\\nCommentedJan 27, 2019 at 17:22\\n\\n- $\\\\\\\\begingroup$Some useful discussion here: [stats.stackexchange.com/questions/285231/‚Ä¶](https://stats.stackexchange.com/questions/285231/what-problem-does-oversampling-undersampling-and-smote-solve?r=SearchResults&s=3|0.0000) and [stats.stackexchange.com/questions/283170/‚Ä¶](https://stats.stackexchange.com/questions/283170/when-is-unbalanced-data-really-a-problem-in-machine-learning)$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Sycorax](https://stats.stackexchange.com/users/22311/sycorax) ‚ô¶\\n\\nCommentedJul 15, 2020 at 13:02\\n\\n- $\\\\\\\\begingroup$This post [here](https://stats.stackexchange.com/questions/247871/what-is-the-root-cause-of-the-class-imbalance-problem) has a very clear answer on _why_ class imbalance causes problems.$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Stephen G](https://stats.stackexchange.com/users/225551/stephen-g)\\n\\nCommentedJul 15, 2020 at 13:57\\n\\n- $\\\\\\\\begingroup$@StephenG At present, the post you've linked to has three answers. Which one are you referring to?$\\\\\\\\endgroup$\\n\\n‚Äì¬†[Sycorax](https://stats.stackexchange.com/users/22311/sycorax) ‚ô¶\\n\\nCommentedJul 15, 2020 at 17:49\\n\\n\\n\\\\|¬†[Show **1** more comment](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes)\\n\\n## 7 Answers 7\\n\\nSorted by:\\n[Reset to default](https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes?answertab=scoredesc#tab-top)\\n\\nHighest score (default)Date modified (newest first)Date created (oldest first)\\n\\n39\\n\\n$\\\\\\\\begingroup$\\n\\nBoth hxd1011 and Frank are right (+1).\\nEssentially resampling and/or cost-sensitive learning are the two main ways of getting around the problem of imbalanced data; third is to use kernel methods that sometimes might be less effected by the class imbalance.\\nLet me stress that there is no silver-bullet solution. By definition you have one class that is represented inadequately in your samples.\\n\\nHaving said the above I believe that you will find the algorithms [SMOTE](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106) and [ROSE](https://journal.r-project.org/archive/2014-1/menardi-lunardon-torelli.pdf) very helpful. SMOTE effectively uses a $k$-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. ROSE tries to create estimates of the underlying distributions of the two classes using a smoothed bootstrap approach and sample them for synthetic examples. Both are readily available in R, SMOTE in the package [DMwR](https://cran.r-project.org/web/packages/DMwR/index.html) and ROSE in the package with the [same name](https://cran.r-project.org/web/packages/ROSE/index.html). Both SMOTE and ROSE result in a training dataset that is smaller than the original one.\\n\\nI would probably argue that a better (or less bad) metric for the case of imbalanced data is using [Cohen's $k$](https://en.wikipedia.org/wiki/Cohen's_kappa) and/or [Receiver operating characteristic's Area under the curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\\nCohen's kappa directly controls for the expected accuracy, AUC as it\\nis a function of sensitivity and specificity, the curve is insensitive to disparities in the class proportions. Again, notice that these are just metrics that should be used with a large grain of salt. You should ideally adapt them to your specific problem taking account of the gains and costs correct and wrong classifications convey in your case. I have found that looking at [lift-curves](https://en.wikipedia.org/wiki/Lift_%28data_mining%29) is actually rather informative for this matter.\\nIrrespective of your metric you should try to use a separate test to assess the performance of your algorithm; exactly because of the class imbalanced over-fitting is even more likely so out-of-sample testing is crucial.\\n\\nProbably the most popular recent paper on the matter is [Learning from Imbalanced Data](http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf) by He and Garcia. It gives a very nice overview of the points raised by myself and in other answers. In addition I believe that the walk-through on [Subsampling For Class Imbalances](http://topepo.github.io/caret/subsampling-for-class-imbalances.html), presented by Max Kuhn as part of the [caret](https://cran.r-project.org/web/packages/caret/index.html) package is an excellent resource to get a structure example of how under-/over-sampling as well as synthetic data creation can measure against each other.\\n\\n[Share](https://stats.stackexchange.com/a/235824)\\n\\nCite\\n\\n[Improve this answer](https://stats.stackexchange.com/posts/235824/edit)\\n\\nFollow\\n\\n[edited Sep 19, 2016 at 20:12](https://stats.stackexchange.com/posts/235824/revisions)\\n\\nanswered Sep 19, 2016 at 19:44\\n\\n[![usŒµr11852's user avatar](https://i.sstatic.net/Y88zz.png?s=64)](https://stats.stackexchange.com/users/11852/us%ce%b5r11852)\\n\\n[usŒµr11852](https://stats.stackexchange.com/users/11852/us%ce%b5r11852) usŒµr11852\\n\\n44.7k33 gold badges101101 silver badges158158 bronze badges\\n\\n$\\\\\\\\endgroup$\\n\\n3\\n\\n- $\\\\\\\\begingroup$A third somewhat popular (and again not too appropriate) metric is the Area-Under-the-Curve of the Precision-Recall curve. [Davis & James, 2006](https://www.biostat.wisc.edu/~page/rocpr.pdf) paper is consider a classic on the matter; CV has a good thread [too](http://stats.stackexchange.com/questions/7207). I recently saw a paper with the somewhat hammy title \\\" [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Dat...\", \"url\": \"https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes\"}, {\"title\": \"Best techniques and metrics for Imbalanced Dataset - Kaggle\", \"text\": \"Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\n\\n[Learn more](https://www.kaggle.com/cookies)\\n\\nOK, Got it.\\n\\n###### Something went wrong and this page crashed!\\n\\nIf the issue persists, it's likely a problem on our side.\\n\\n```\\n\\nLoading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=805aeed0a20bb825939e:1:11032)\\n```\\n\\nRefresh\", \"url\": \"https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset\"}, {\"title\": \"Class Imbalance Strategies ‚Äî A Visual Guide with Code - Medium\", \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8bc8fae71e1a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fclass-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-8bc8fae71e1a---------------------------------------)\\n\\n¬∑\\n\\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\\n\\n# Class Imbalance Strategies ‚Äî A Visual Guide with Code\\n\\n## Understand Random Undersampling, Oversampling, SMOTE, ADASYN, and Tomek Links\\n\\n[Travis Tang](https://medium.com/@travis-tang?source=post_page---byline--8bc8fae71e1a---------------------------------------)\\n\\n13 min read\\n\\n¬∑\\n\\nApr 24, 2023\\n\\n--\\n\\n1\\n\\nListen\\n\\nShare\\n\\nClass imbalance occurs when one class in a classification problem significantly outweighs the other class. It‚Äôs common in many machine learning problems. Examples include fraud detection, anomaly detection, and medical diagnosis.\\n\\n## The Curse of Class Imbalance\\n\\nA model trained on an imbalanced dataset perform poorly on the minority class. At best, this can cause loss to the business in the case of a churn analysis. At worst, it can pervade systemic bias of a face recognition system.\\n\\nPress enter or click to view image in full size\\n\\nA balanced dataset might just be the missing ingredient (Source: Elena Mozhvilo on [Unsplash](https://unsplash.com/photos/j06gLuKK0GM))\\n\\nThe common approach to class imbalance is resampling. These can entail oversampling the majority class, undersampling the minority class, or a combination of both.\\n\\nIn this post, I use vivid visuals and code to illustrate these strategies for class imbalance:\\n\\n1. Random oversampling\\n2. Random undersampling\\n3. Oversampling with SMOTE\\n4. Oversampling with ADASYN\\n5. Undersampling with Tomek Link\\n6. Oversampling with SMOTE, then undersample with TOMEK Link (SMOTE-Tomek)\\n\\nI will also be using these strategies on a real-world dataset, and evaluate their impact on a machine learning model. Let‚Äôs go.\\n\\n> All source code is here.\\n\\n## Using Imbalance-learn\\n\\nWe will use the `imbalanced-learn` package in python to solve our imbalanced class problem. It is an open-sourced library relying on scikit-learn and provides tools when dealing with classification with imbalanced classes.\\n\\nTo install it, use the command.\\n\\n```\\npip install -U imbalanced-learn\\n```\\n\\n## Dataset\\n\\nThe dataset that we are using is the [**Communities and Crime Data Set by UCI (CC BY 4.0)**.](https://archive.ics.uci.edu/ml/datasets/communities+and+crime) It contains 100 attributes of 1994 U.S. communities. We can use this to predict if the **crime rate is high** (defined as having **per capita violent crime** above 0.65). The data source is available in the UCI Machine Learning Repository and is created by Michael Redmond from La Salle University (Published in 2009).\\n\\n> The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units.\\n\\nThis dataset is imbalanced. It has 12 communities with low crime rates for every 1 community of high crime rate. This is perfect to illustrate our use case.\\n\\n```\\n>>> from imblearn.datasets import fetch_datasets>>> # Fetch dataset from imbalanced-learn library >>> # as a dictionary of numpy array>>> us_crime = fetch_datasets()['us_crime']>>> us_crime{'data': array([[0.19, 0.33, 0.02, ..., 0.26, 0.2 , 0.32],        [0.  , 0.16, 0.12, ..., 0.12, 0.45, 0.  ],        [0.  , 0.42, 0.49, ..., 0.21, 0.02, 0.  ],        ...,        [0.16, 0.37, 0.25, ..., 0.32, 0.18, 0.91],        [0.08, 0.51, 0.06, ..., 0.38, 0.33, 0.22],        [0.2 , 0.78, 0.14, ..., 0.3 , 0.05, 1.  ]]), 'target': array([-1,  1, -1, ..., -1, -1, -1]), 'DESCR': 'us_crime'}\\n```\\n\\nWe will convert this dictionary to a Pandas dataframe, then split it into train-test splits.\\n\\n```\\n# Convert the dictionary to a pandas dataframecrime_df = pd.concat([pd.DataFrame(us_crime['data'], columns = [f'data_{i}' for i in range(us_crime.data.shape[1])]),           pd.DataFrame(us_crime['target'], columns = ['target'])], axis = 1)# Split data into train test setfrom sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(crime_df.drop('target', axis = 1),                                                     crime_df['target'],                                                     test_size = 0.4,                                                     random_state = 42)\\n```\\n\\nNote that we will only perform under- and over-sampling only on the train dataset. We will _not_ change the test sets with under- and over-sampling.\\n\\n### Preprocessing the dataset\\n\\nOur goal is to have a visualize an imbalanced dataset. In order to visualize the 128-dimensional dataset in a 2D graph, we do the following on the train set.\\n\\n- scale the dataset,\\n- perform Principle Component Analysis (PCA) on the features to convert the 100 features to 2 principle components,\\n- visualize the data.\\n\\nHere‚Äôs the data, visualized in 2D.\\n\\nImage by author\\n\\nCode for the above graph:\\n\\n```\\nfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.decomposition import PCAimport matplotlib.pyplot as plt# Scale the dataset on both train and test sets.# Note that we fit MinMaxScaler on X_train only, not on the entire dataset.# This prevents data leakage from test set to train set.scaler = MinMaxScaler()scaler.fit(X_train)X_train = scaler.transform(X_train)X_test = scaler.transform(X_test)# Perform PCA Decomposition on both train and test sets# Note that we fit PCA on X_train only, not on the entire dataset.# This prevents data leakage from test set to train set.pca = PCA(n_components=2)pca.fit(X_train)X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)# Function for plotting dataset def plot_data(X,y,ax,title):    ax.scatter(X[:, 0], X[:, 1], c=y, alpha=0.5, s = 30, edgecolor=(0,0,0,0.5))    ax.set_ylabel('Principle Component 1')    ax.set_xlabel('Principle Component 2')    if title is not None:        ax.set_title(title)# Plot datasetfig,ax = plt.subplots(figsize=(5, 5))plot_data(X_train_pca, y_train, ax, title='Original Dataset')\\n```\\n\\nWith the preprocessing done, we are ready to resample our dataset.\\n\\n## Strategy 1. Random Oversampling\\n\\nRandom oversampling duplicates existing examples from the minority class with replacement. Each data point in the minority class has an equal probability of being duplicated.\\n\\nPress enter or click to view image in full size\\n\\nImage by author\\n\\nHere‚Äôs how to we can perform oversampling on our dataset.\\n\\n```\\nfrom imblearn.over_sampling import RandomOverSampler# Perform random oversamplingros = RandomOverSampler(random_state=0)X_train_ros, ...\", \"url\": \"https://medium.com/data-science/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a\"}, {\"title\": \"Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\\n**üì£ BIG NEWS:****Neptune is joining OpenAI!**‚Üí Read the message from our CEO üì£![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\\nWhat do you want to find?\\nSearch\\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\nSearch in Blog...\\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\\n[Shahul ES](https://neptune.ai/blog/author/shahules)\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top tabular data competitions. Without much lag, let‚Äôs begin.\\nThese are the five competitions that I have gone through to create this article:\\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\\n* Use[cudf](https://github.com/rapidsai/cudf).\\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let‚Äôs see the methods used to do it.\\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...\", \"url\": \"https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\"}]}",
        "tool.duration_ms": 4081.7463397979736
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "0e36acfc5df91359",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "WriteTool",
      "kind": "INTERNAL",
      "start_time": 1767958855808935416,
      "end_time": 1767958855809396574,
      "duration": 0.000461158,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WriteTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.WriteTool object at 0x7f38e8310090>\", \"input\": {\"file_path\": \"seed_prompt.txt\", \"content\": \"## Competition Overview\\nThis is a binary classification problem combining text and tabular features. See `exploration/eda.ipynb` for detailed data characteristics including feature distributions, missing values, class balance analysis, and correlation patterns.\\n\\n## Core Modeling Strategy\\n\\n### Multimodal Approach (Text + Tabular)\\nBased on winning Kaggle solutions for similar problems, the most effective approach is a three-step pipeline:\\n\\n1. **Extract text embeddings** using pretrained transformers (BERT, RoBERTa, or DeBERTa)\\n   - Use AutoGluon Tabular's TextPredictor for automatic text encoding\\n   - Alternatively, use Hugging Face transformers to generate sentence embeddings\\n   - GPU acceleration is critical for reasonable training time\\n\\n2. **Combine embeddings with tabular features**\\n   - Concatenate text embeddings with numeric/categorical features (two-stage approach)\\n   - OR convert tabular features to text and prepend to review (features-to-text trick)\\n   - Target-encode high-cardinality categorical features before merging\\n\\n3. **Train strong tabular models on combined features**\\n   - LightGBM, CatBoost, and XGBoost are the workhorses\\n   - Neural networks can be effective for larger datasets\\n   - Use libraries like Multimodal-Toolkit for seamless integration\\n\\n### Model Architecture Options\\n- **Two-stage**: Text encoder ‚Üí embeddings ‚Üí tabular model\\n- **Features-to-text**: Convert all features to text ‚Üí single transformer\\n- **Hybrid ensemble**: Combine both approaches with different weighting\\n\\n## Handling Class Imbalance (75/25 split)\\n\\n### Resampling Strategies\\n- **Undersample majority class** to balance classes (risk: information loss)\\n- **Oversample minority class** using SMOTE or ADASYN\\n- **Combine both**: SMOTE + Tomek links for cleaning\\n- **Apply resampling only to training folds** in cross-validation\\n\\n### Alternative Approaches (No Resampling)\\n- **Class weights**: Set `scale_pos_weight` in XGBoost/LightGBM or `class_weight` in sklearn\\n- **Focal loss**: Penalize hard examples more heavily\\n- **Threshold adjustment**: Optimize decision threshold post-training using ROC/PR curves\\n\\n### Validation Considerations\\n- **Always use stratified K-fold** to preserve class ratios in each fold\\n- **Monitor minority class metrics**: AUC-ROC, PR-AUC, F1-score (not accuracy)\\n- **Check for distribution shift** between train and test sets\\n\\n## Feature Engineering\\n\\n### Text Features\\n- **Basic**: Length, word count, character count, sentence count\\n- **Sentiment**: Use VADER or TextBlob for sentiment scores\\n- **TF-IDF**: Extract top terms and their statistics\\n- **Topic modeling**: LDA or NMF for latent topics\\n- **Entity extraction**: Count mentions of products, brands, locations\\n\\n### Tabular Features\\n- **Target encoding**: For high-cardinality categoricals (use cross-validation)\\n- **Interaction features**: Polynomial features between numeric columns\\n- **Aggregation features**: Group by categorical columns and compute statistics\\n- **Missing value patterns**: Create indicator for missing data patterns\\n- **Cyclical encoding**: If any temporal features exist\\n\\n### Cross-modal Features\\n- **Text-tabular interactions**: Multiply text sentiment scores with numeric features\\n- **Feature importance alignment**: Ensure text and tabular features are on similar scales\\n\\n## Model Selection and Training\\n\\n### Primary Models (Tier 1)\\n1. **LightGBM**: Fast, handles categorical features well\\n2. **CatBoost**: Excellent for categorical data, robust to overfitting\\n3. **XGBoost**: Reliable baseline, good performance\\n\\n### Secondary Models (Tier 2)\\n1. **Neural Networks**: For capturing non-linear interactions\\n2. **Random Forest**: For diversity in ensembles\\n3. **Logistic Regression**: With polynomial features for baseline\\n\\n### Training Configuration\\n- **Use early stopping** with patience of 50-100 rounds\\n- **Set class weights**: `scale_pos_weight = (negative_samples / positive_samples) * k` where k ‚àà [0.5, 2]\\n- **Hyperparameter ranges**:\\n  - Learning rate: 0.01 - 0.1\\n  - Max depth: 3 - 10\\n  - Num leaves: 31 - 255\\n  - Subsample: 0.6 - 0.9\\n  - Colsample: 0.6 - 0.9\\n\\n## Ensembling Strategy\\n\\n### Model Diversity\\n- Combine different architectures (GBM + NN + Transformer)\\n- Use different feature sets (text-only, tabular-only, combined)\\n- Vary preprocessing pipelines (different encoding strategies)\\n\\n### Ensemble Methods\\n1. **Weighted averaging**: Weight by validation performance\\n2. **Stacking**: Train meta-learner on out-of-fold predictions\\n3. **Rank averaging**: Average predictions after converting to ranks\\n4. **Optimal blending**: Use linear regression to find optimal weights\\n\\n### Implementation Order\\n1. Build 3-5 diverse single models\\n2. Create level-1 ensemble with simple averaging\\n3. Add stacking with logistic regression or LightGBM\\n4. Fine-tune ensemble weights using validation set\\n\\n## Validation Strategy\\n\\n### Cross-Validation Setup\\n- **Stratified K-Fold**: k=5 or k=10\\n- **GroupKFold**: If there's a grouping column (e.g., user_id)\\n- **TimeSeriesSplit**: If temporal patterns exist (check eda.ipynb)\\n\\n### Evaluation Metrics\\n- **Primary**: AUC-ROC (robust to class imbalance)\\n- **Secondary**: PR-AUC, F1-score, Log Loss\\n- **Never use**: Accuracy (misleading with imbalance)\\n\\n### Validation Best Practices\\n- **Create holdout set** before cross-validation for final evaluation\\n- **Monitor OOF (out-of-fold) predictions** for consistency\\n- **Check prediction distributions** across folds\\n- **Analyze errors**: Focus on minority class misclassifications\\n\\n## Optimization and Tuning\\n\\n### Hyperparameter Search\\n- **Start with broad search**: Use Optuna or Hyperopt\\n- **Focus on key params**: Learning rate, max depth, subsample\\n- **Use successive halving**: Reduce search space iteratively\\n- **Leverage GPU**: For neural network components\\n\\n### Feature Selection\\n- **Permutation importance**: On validation set\\n- **SHAP values**: For model interpretation\\n- **Recursive feature elimination**: For dimensionality reduction\\n- **Correlation analysis**: Remove highly correlated features\\n\\n### Post-Training Optimization\\n- **Threshold tuning**: Optimize for competition metric\\n- **Calibration**: Use Platt scaling or isotonic regression\\n- **Pseudo-labeling**: If confident in predictions on test set\\n\\n## Implementation Roadmap\\n\\n### Phase 1: Baselines (Quick)\\n1. TF-IDF + Logistic Regression\\n2. Tabular-only LightGBM\\n3. Text-only BERT fine-tuning\\n\\n### Phase 2: Core Models (1-2 days)\\n1. Multimodal pipeline (text embeddings + tabular)\\n2. 3-5 diverse models with different architectures\\n3. Basic ensemble (averaging)\\n\\n### Phase 3: Refinement (2-3 days)\\n1. Advanced feature engineering\\n2. Hyperparameter optimization\\n3. Stacking ensemble\\n4. Threshold optimization\\n\\n### Phase 4: Final Push (1 day)\\n1. Error analysis and targeted improvements\\n2. Model blending and weight tuning\\n3. Validation consistency checks\\n4. Submission preparation\\n\\n## Key Libraries and Tools\\n\\n### Essential Libraries\\n- **AutoGluon**: For automatic multimodal modeling\\n- **Multimodal-Toolkit**: For transformer + tabular integration\\n- **LightGBM/CatBoost/XGBoost**: Gradient boosting\\n- **Transformers**: Hugging Face for text encoding\\n- **Optuna**: Hyperparameter optimization\\n\\n### Utility Libraries\\n- **imbalanced-learn**: For resampling techniques\\n- **category_encoders**: For target encoding\\n- **SHAP**: For model interpretation\\n- **tqdm**: For progress tracking\\n\\n## Common Pitfalls to Avoid\\n\\n1. **Data leakage**: Never use test set statistics for training\\n2. **Overfitting to validation**: Use multiple validation strategies\\n3. **Ignoring minority class**: Focus metrics and analysis on positive class\\n4. **Text preprocessing errors**: Preserve semantic meaning\\n5. **Scale mismatch**: Ensure text embeddings and tabular features compatible\\n6. **Ensemble correlation**: Ensure model diversity, not just quantity\\n\\n## Expected Performance Targets\\n\\nBased on similar competitions:\\n- **Single model**: 0.75-0.85 AUC\\n- **Simple ensemble**: 0.80-0.88 AUC\\n- **Advanced ensemble**: 0.85-0.92 AUC\\n- **Top solutions**: 0.90+ AUC (requires significant feature engineering)\\n\\n## References and Resources\\n\\n- AutoGluon multimodal tutorial: Combining BERT with tabular models\\n- Multimodal-Toolkit: GitHub repository with examples\\n- Kaggle winning solutions: Mercari Price Suggestion, Home Credit Default Risk\\n- Class imbalance handling: SMOTE, ADASYN, class weights\\n- Feature engineering: Target encoding, interaction features\"}}",
        "tool.output": "{\"content\": \"Wrote to /home/code/exploration/seed_prompt.txt\", \"is_error\": false}",
        "tool.duration_ms": 0.5407333374023438
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "f10a21038fbee89f",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767958865731311087,
      "end_time": 1767958865731591838,
      "duration": 0.000280751,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7f38e836acd0>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.0KB)\\n[FILE] sampleSubmission.csv (13.0KB)\\n[FILE] test.json (2.0MB)\\n[FILE] test.zip (433.1KB)\\n[FILE] train.json (8.4MB)\\n[FILE] train.zip (1.3MB)\", \"is_error\": false}",
        "tool.duration_ms": 0.3592967987060547
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "4ced17a24f679d8e",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767958874352053322,
      "end_time": 1767958874352379555,
      "duration": 0.000326233,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7f38e9db0750>\", \"input\": {\"file_path\": \"/home/data/description.md\"}}",
        "tool.output": "{\"content\": \"# Overview\\n\\n## Description\\n\\n[Get started on this competition through Kaggle Scripts](https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza)\\n\\nIn machine learning, it is often said¬†there are¬†[no free lunches](http://en.wikipedia.org/wiki/No_free_lunch_theorem).¬†*How wrong we were.*\\n\\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community¬†[Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/)¬†together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm¬†capable of predicting¬†which requests will garner a cheesy (but sincere!) act of¬†kindness.\\n\\n\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?\\n\\n![Pizzas](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png)\\n\\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This¬†[data](http://cs.stanford.edu/~althoff/raop-dataset/)¬†was collected and graciously shared by¬†[Althoff](http://www.timalthoff.com/)¬†et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their¬†[accompanying paper](http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf)¬†and ask that you cite the following reference in any publications that result from your work:\\n\\nTim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky.¬†*How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*,¬†Proceedings of ICWSM, 2014.\\n\\n~*Pizza icons designed by Matthew Dera from the¬†[Noun Project](http://thenounproject.com/term/pizza/30194/)*~\\n\\n### Evaluation\\n\\nSubmissions are evaluated on¬†[area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)¬†between the predicted probability that a request will get pizza and the observed outcomes.\\n\\n### Submission File\\n\\nFor each request¬†in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:\\n\\n```\\nrequest_id,requester_received_pizza\\nt3_i8iy4,0\\nt3_1mfqi0,0\\nt3_lclka,0\\n...\\n...\\n```\\n\\n### Citation\\n\\nWill Cukierski. (2014). Random Acts of Pizza. Kaggle. https://kaggle.com/competitions/random-acts-of-pizza\\n\\n\\n# Data\\n\\n## Dataset Description\\n\\n[See, fork, and run a random forest benchmark model through Kaggle Scripts](https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark)\\n\\nThis dataset includes 5671 requests collected from the Reddit community¬†[Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/)¬†between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013).¬†All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known.¬†Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\\n\\nEach JSON¬†entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\\n\\n## Data fields\\n\\n\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).\\n\\n\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.\\n\\n\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.\\n\\n\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).\\n\\n\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".\\n\\n\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.\\n\\n\\\"request_text\\\": Full text of the request.\\n\\n\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".\\n\\n\\\"request_title\\\": Title of the request.\\n\\n\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.\\n\\n\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.\\n\\n\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\\n\\n\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.\\n\\n\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\\n\\n\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).\\n\\n\\\"requester_username\\\": Reddit username of requester.\\n\\n\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\\n\\n\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.\", \"is_error\": false}",
        "tool.duration_ms": 0.35643577575683594
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "b89e6be667570ac4",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767958895692660615,
      "end_time": 1767958895692829299,
      "duration": 0.000168684,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7f38e8369e90>\", \"input\": {\"notebook_path\": \"exploration/eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the training data\\ntrain_path = '/home/data/train.json'\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\nprint(f\\\"\\\\nFirst sample preview:\\\")\\nfor key, value in list(train_data[0].items())[:5]:\\n    print(f\\\"  {key}: {value}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Error: Notebook already active: /home/code/exploration/eda.ipynb. Use CloseNotebook first.\", \"is_error\": true}",
        "tool.duration_ms": 0.2503395080566406
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "fa0b48b691ab7a03",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "CloseNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767958901302310247,
      "end_time": 1767958901414210814,
      "duration": 0.111900567,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CloseNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CloseNotebookTool object at 0x7f38e836aed0>\", \"input\": {}}",
        "tool.output": "{\"content\": \"Closed: /home/code/exploration/eda.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 111.96470260620117
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "448a1ef2091d30db",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767958912027262702,
      "end_time": 1767958912602845390,
      "duration": 0.575582688,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7f38e8369e90>\", \"input\": {\"notebook_path\": \"exploration/eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the training data\\ntrain_path = '/home/data/train.json'\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\nprint(f\\\"\\\\nFirst sample preview:\\\")\\nfor key, value in list(train_data[0].items())[:5]:\\n    print(f\\\"  {key}: {value}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 575.6580829620361
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "1e7547199dbb0ba9",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958917274091091,
      "end_time": 1767958918465335920,
      "duration": 1.191244829,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 0, \"end_cell\": 0, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the training data\\ntrain_path = '/home/data/train.json'\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\nprint(f\\\"\\\\nFirst sample preview:\\\")\\nfor key, value in list(train_data[0].items())[:5]:\\n    print(f\\\"  {key}: {value}\\\")\\n```\\nOutput:\\nNumber of training samples: 2878\\nFirst sample keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\\nFirst sample preview:\\n  giver_username_if_known: N/A\\n  number_of_downvotes_of_request_at_retrieval: 2\\n  number_of_upvotes_of_request_at_retrieval: 5\\n  post_was_edited: False\\n  request_id: t3_q8ycf\\n\", \"is_error\": false}",
        "tool.duration_ms": 1191.2727355957031
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "177b6959ac30b5b4",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767958932895679376,
      "end_time": 1767958932896831776,
      "duration": 0.0011524,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f38e836a6d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\n\\n# Analyze target distribution\\ntarget_counts = df['requester_received_pizza'].value_counts()\\ntarget_pct = df['requester_received_pizza'].value_counts(normalize=True) * 100\\n\\nprint(\\\"Target Distribution:\\\")\\nprint(f\\\"No pizza: {target_counts[False]} ({target_pct[False]:.1f}%)\\\")\\nprint(f\\\"Pizza received: {target_counts[True]} ({target_pct[True]:.1f}%)\\\")\\nprint(f\\\"Class imbalance ratio: {target_counts[False]/target_counts[True]:.2f}:1\\\")\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values:\\\")\\nmissing = df.isnull().sum()\\nif missing.sum() > 0:\\n    print(missing[missing > 0])\\nelse:\\n    print(\\\"No missing values found\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 1\", \"is_error\": false}",
        "tool.duration_ms": 1.1920928955078125
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "f6452d44250fd4ef",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958941312381333,
      "end_time": 1767958941336341601,
      "duration": 0.023960268,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 1, \"end_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\n\\n# Analyze target distribution\\ntarget_counts = df['requester_received_pizza'].value_counts()\\ntarget_pct = df['requester_received_pizza'].value_counts(normalize=True) * 100\\n\\nprint(\\\"Target Distribution:\\\")\\nprint(f\\\"No pizza: {target_counts[False]} ({target_pct[False]:.1f}%)\\\")\\nprint(f\\\"Pizza received: {target_counts[True]} ({target_pct[True]:.1f}%)\\\")\\nprint(f\\\"Class imbalance ratio: {target_counts[False]/target_counts[True]:.2f}:1\\\"\\n```\\nOutput:\\nTarget Distribution:\\nNo pizza: 2163 (75.2%)\\nPizza received: 715 (24.8%)\\nClass imbalance ratio: 3.03:1\\n\\nMissing values:\\nrequester_user_flair    2163\\ndtype: int64\\n\", \"is_error\": false}",
        "tool.duration_ms": 24.023056030273438
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "eeca483d488b482e",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767958958506565536,
      "end_time": 1767958958507954918,
      "duration": 0.001389382,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f38e836a6d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze text features\\nprint(\\\"Text Features Analysis:\\\")\\nprint(f\\\"Request title length (chars): {df['request_title'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text length (chars): {df['request_text'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text edit-aware length (chars): {df['request_text_edit_aware'].str.len().describe()}\\\")\\n\\n# Show some examples of successful vs unsuccessful requests\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"EXAMPLE SUCCESSFUL REQUEST:\\\")\\nsuccess_example = df[df['requester_received_pizza'] == True].iloc[0]\\nprint(f\\\"Title: {success_example['request_title']}\\\")\\nprint(f\\\"Text (first 200 chars): {success_example['request_text'][:200]}...\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"EXAMPLE UNSUCCESSFUL REQUEST:\\\")\\nfail_example = df[df['requester_received_pizza'] == False].iloc[0]\\nprint(f\\\"Title: {fail_example['request_title']}\\\")\\nprint(f\\\"Text (first 200 chars): {fail_example['request_text'][:200]}...\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 1.4371871948242188
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "53bed68f60dced43",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958967885305783,
      "end_time": 1767958967901128125,
      "duration": 0.015822342,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 2, \"end_cell\": 2, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Analyze text features\\nprint(\\\"Text Features Analysis:\\\")\\nprint(f\\\"Request title length (chars): {df['request_title'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text length (chars): {df['request_text'].str.len().describe()}\\\")\\nprint(f\\\"\\\\nRequest text edit-aware length (chars): {df['request_text_edit_aware'].str.len().describe()}\\\")\\n\\n# Show some examples of successful vs unsuccessful requests\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"EXAMPLE SUCCESSFUL REQUEST:\\\")\\nsuccess_example = df[df['requester_received_pizza'] =\\n```\\nOutput:\\nText Features Analysis:\\nRequest title length (chars): count    2878.000000\\nmean       71.572967\\nstd        36.233487\\nmin         7.000000\\n25%        46.000000\\n50%        64.000000\\n75%        90.000000\\nmax       272.000000\\nName: request_title, dtype: float64\\n\\nRequest text length (chars): count    2878.000000\\nmean      402.521543\\nstd       362.393727\\nmin         0.000000\\n25%       182.000000\\n50%       308.000000\\n75%       503.750000\\nmax      4460.000000\\nName: request_text, dtype: float64\\n\\nRequest text edit-aware length (chars): count    2878.000000\\nmean      394.567755\\nstd       351.922518\\nmin         0.000000\\n25%       180.000000\\n50%       302.000000\\n75%       498.000000\\nmax      4460.000000\\nName: request_text_edit_aware, dtype: float64\\n\\n================================================================================\\nEXAMPLE SUCCESSFUL REQUEST:\\nTitle: [REQUEST] Not much food until tomorrow.\\nText (first 200 chars): I will go ahead and say that I got a pizza meal from here before as to not seem like I'm scamming anyone. I have been promised 2 well-paying jobs and one minimum wage + .40 to fill in the gaps of unem...\\n\\n================================================================================\\nEXAMPLE UNSUCCESSFUL REQUEST:\\nTitle: [REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\nText (first 200 chars): I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time ...\\n\", \"is_error\": false}",
        "tool.duration_ms": 15.848159790039062
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "c5854e9097f9d4b1",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767958987967452041,
      "end_time": 1767958987969172916,
      "duration": 0.001720875,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f38e836a6d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze numerical/meta-data features\\nnumerical_features = [\\n    'requester_account_age_in_days_at_request',\\n    'requester_number_of_comments_at_request', \\n    'requester_number_of_posts_at_request',\\n    'requester_number_of_subreddits_at_request',\\n    'requester_upvotes_minus_downvotes_at_request',\\n    'requester_upvotes_plus_downvotes_at_request',\\n    'request_number_of_comments_at_retrieval',\\n    'number_of_upvotes_of_request_at_retrieval',\\n    'number_of_downvotes_of_request_at_retrieval'\\n]\\n\\nprint(\\\"Numerical Features Summary:\\\")\\nfor feature in numerical_features:\\n    if feature in df.columns:\\n        print(f\\\"\\\\n{feature}:\\\")\\n        print(df[feature].describe())\\n\\n# Analyze user flair distribution\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"User Flair Distribution:\\\")\\nflair_counts = df['requester_user_flair'].value_counts(dropna=False)\\nprint(flair_counts)\\n\\n# Cross-tabulate flair with target\\nprint(\\\"\\\\nFlair vs Target:\\\")\\nflair_target = pd.crosstab(df['requester_user_flair'], df['requester_received_pizza'], normalize='index') * 100\\nprint(flair_target.round(1))\"}}",
        "tool.output": "{\"content\": \"Added code cell at 3\", \"is_error\": false}",
        "tool.duration_ms": 1.7886161804199219
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "7a481022dcd434b4",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767958994620284349,
      "end_time": 1767958994646173665,
      "duration": 0.025889316,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f38e836a210>\", \"input\": {\"start_cell\": 3, \"end_cell\": 3, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 3-3\\n==================================================\\n\\nCell 3:\\n```\\n# Analyze numerical/meta-data features\\nnumerical_features = [\\n    'requester_account_age_in_days_at_request',\\n    'requester_number_of_comments_at_request', \\n    'requester_number_of_posts_at_request',\\n    'requester_number_of_subreddits_at_request',\\n    'requester_upvotes_minus_downvotes_at_request',\\n    'requester_upvotes_plus_downvotes_at_request',\\n    'request_number_of_comments_at_retrieval',\\n    'number_of_upvotes_of_request_at_retrieval',\\n    'number_of_downvotes_of_request_at_retrieval'\\n\\n```\\nOutput:\\nNumerical Features Summary:\\n\\nrequester_account_age_in_days_at_request:\\ncount    2878.000000\\nmean      250.682364\\nstd       301.838771\\nmin         0.000000\\n25%         3.038877\\n50%       155.156377\\n75%       383.640090\\nmax      2809.750787\\nName: requester_account_age_in_days_at_request, dtype: float64\\n\\nrequester_number_of_comments_at_request:\\ncount    2878.000000\\nmean      112.311327\\nstd       192.017515\\nmin         0.000000\\n25%         0.000000\\n50%        22.000000\\n75%       132.000000\\nmax       981.000000\\nName: requester_number_of_comments_at_request, dtype: float64\\n\\nrequester_number_of_posts_at_request:\\ncount    2878.000000\\nmean       21.614663\\nstd        51.580719\\nmin         0.000000\\n25%         0.000000\\n50%         4.000000\\n75%        21.000000\\nmax       867.000000\\nName: requester_number_of_posts_at_request, dtype: float64\\n\\nrequester_number_of_subreddits_at_request:\\ncount    2878.000000\\nmean       17.857192\\nstd        21.784934\\nmin         0.000000\\n25%         1.000000\\n50%        11.000000\\n75%        26.000000\\nmax       186.000000\\nName: requester_number_of_subreddits_at_request, dtype: float64\\n\\nrequester_upvotes_minus_downvotes_at_request:\\ncount      2878.000000\\nmean       1184.582349\\nstd        4198.255486\\nmin         -67.000000\\n25%           3.000000\\n50%         171.000000\\n75%        1124.500000\\nmax      155010.000000\\nName: requester_upvotes_minus_downvotes_at_request, dtype: float64\\n\\nrequester_upvotes_plus_downvotes_at_request:\\ncount    2.878000e+03\\nmean     3.988570e+03\\nstd      3.012747e+04\\nmin      0.000000e+00\\n25%      8.000000e+00\\n50%      3.355000e+02\\n75%      2.251500e+03\\nmax      1.286864e+06\\nName: requester_upvotes_plus_downvotes_at_request, dtype: float64\\n\\nrequest_number_of_comments_at_retrieval:\\ncount    2878.000000\\nmean        2.858582\\nstd         4.783656\\nmin         0.000000\\n25%         0.000000\\n50%         1.000000\\n75%         4.000000\\nmax        61.000000\\nName: request_number_of_comments_at_retrieval, dtype: float64\\n\\nnumber_of_upvotes_of_request_at_retrieval:\\ncount    2878.000000\\nmean        6.090688\\nstd        10.501259\\nmin         0.000000\\n25%         2.000000\\n50%         4.000000\\n75%         7.000000\\nmax       345.000000\\nName: number_of_upvotes_of_request_at_retrieval, dtype: float64\\n\\nnumber_of_downvotes_of_request_at_retrieval:\\ncount    2878.000000\\nmean        2.428075\\nstd         3.035568\\nmin         0.000000\\n25%         1.000000\\n50%         2.000000\\n75%         3.000000\\nmax        47.000000\\nName: number_of_downvotes_of_request_at_retrieval, dtype: float64\\n\\n================================================================================\\nUser Flair Distribution:\\nrequester_user_flair\\nNone      2163\\nshroom     677\\nPIF         38\\nName: count, dtype: int64\\n\\nFlair vs Target:\\nrequester_received_pizza   True\\nrequester_user_flair           \\nPIF                       100.0\\nshroom                    100.0\\n\", \"is_error\": false}",
        "tool.duration_ms": 25.93064308166504
      }
    },
    {
      "trace_id": "0ad319796ae0544a413df0aa2fc8b5a0",
      "span_id": "2d3ead2e0fc22836",
      "parent_span_id": "0ba2a54e35250b4b",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1767959004373057825,
      "end_time": 1767959009484238615,
      "duration": 5.11118079,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7f38e8369d50>\", \"input\": {\"query\": \"What are winning approaches for text classification competitions on Kaggle that combine text and meta-data features?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nWinning solutions in Kaggle text‚Äëclassification contests usually blend strong language‚Äëmodel representations with carefully engineered meta‚Äëdata features, then stack or ensemble the resulting models.  The most successful pipelines start by extracting dense embeddings from pretrained transformers (e.g.,‚ÄØBERT, RoBERTa) or fast‚Äëtext vectors for the raw text, while simultaneously turning auxiliary information such as document length, source, author ID, timestamp or categorical tags into numeric features (one‚Äëhot, target‚Äëencoded, or simple statistics).  These meta‚Äëfeatures are concatenated with the text embeddings and fed into a gradient‚Äëboosting model (LightGBM, XGBoost) or a shallow neural net, which often outperforms a pure‚Äëtext model because the extra signals help disambiguate noisy or short inputs.  After the base models are trained, competitors typically blend several variants (different seeds, architectures, or feature sets) using weighted averaging or stacking, and they validate with stratified K‚Äëfold cross‚Äëvalidation to guard against over‚Äëfitting‚ÄØ([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\\n\\nIn addition to the model architecture, practical tricks that distinguish winning entries include efficient handling of large datasets (e.g., loading data as parquet or feather, using Dask or cuDF for parallel processing) and aggressive memory optimisation, which make it feasible to train large transformer‚Äëbased encoders together with many meta‚Äëfeatures on limited hardware‚ÄØ([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  Pseudo‚Äëlabeling on the test set, careful class‚Äëbalance weighting, and early‚Äëstopping based on validation scores are also common, allowing the final ensemble to capture both the nuanced language patterns and the predictive power of the side information.\\n\\n--- Result 2 ---\\nTitle: Text Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\\n\\nContent:\\nIn this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top NLP competitions.\\n\\nNamely, I‚Äôve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) ‚Äì $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) ‚Äì $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) ‚Äì $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) ‚Äì $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) ‚Äì $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let‚Äôs begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let‚Äôs see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet‚Äôs see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in ¬†R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\\n\\n--- Result 3 ---\\nTitle: Text Classification Techniques - Explained - Kaggle\\nURL: https://www.kaggle.com/code/eraikako/text-classification-techniques-explained\\n\\nContent:\\nText Classification Techniques - Explained | Kaggle\\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\n[\\nLearn more\\n](https://www.kaggle.com/cookies)\\nOK, Got it.\\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\\nIf the issue persists, it's likely a problem on our side.\\n```\\nLoading CSS chunk 1019 failed.\\\\\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\\nkeyboard\\\\_arrow\\\\_upcontent\\\\_copy\\nError: Loading CSS chunk 1019 failed.\\\\\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a6aef772a946d7d4:1:10006)\\n```\\nRefresh\\n\\n--- Result 4 ---\\nTitle: Advanced Techniques of Text Classification - Kaggle\\nURL: https://www.kaggle.com/code/meharshanali/advanced-techniques-of-text-classification\\n\\nContent:\\nChecking your browser - reCAPTCHA\\nChecking your browser before accessing www.kaggle.com ...\\nClick[here](#)if you are not automatically redirected after 5 seconds.\\n\\n--- Result 5 ---\\nTitle: A Beginners Guide to dealing with Text Data - Kaggle\\nURL: https://www.kaggle.com/code/prashant111/a-beginners-guide-to-dealing-with-text-data\\n\\nContent:\\n<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=c93cac9cb620a9ebe94a:1:10973)</pre></div></div></div></div>\\n\\n--- Result 6 ---\\nTitle: A Guide to any Classification Problem - Kaggle\\nURL: https://www.kaggle.com/code/durgancegaur/a-guide-to-any-classification-problem\\n\\nContent:\\n<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=85e822d649bb8c5b8715:1:11062)</pre></div></div></div></div>\\n\\n--- Result 7 ---\\nTitle: How to Categorize Large Amounts of Text Data | by Data@Urban\\nURL: https://urban-institute.medium.com/how-to-categorize-large-amounts-of-text-data-71e1fd590e2a\\n\\nContent:\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F71e1fd590e2a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\n_Original illustration courtesy of Majcot@Shutterstock_\\n\\n# How to Categorize Large Amounts of Text Data\\n\\n[![Data@Urban](https://miro.medium.com/v2/resize:fill:88:88/1*SmPWgbgKs_B3FJ242EIJJg.png)](https://urban-institute.medium.com/?source=post_page-----71e1fd590e2a--------------------------------)\\n\\n[Data@Urban](https://urban-institute.medium.com/?source=post_page-----71e1fd590e2a--------------------------------)\\n\\n¬∑\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed38af6a2b4c&operation=register&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&user=Data%40Urban&userId=ed38af6a2b4c&source=post_page-ed38af6a2b4c----71e1fd590e2a---------------------post_header-----------)\\n\\n10 min read\\n\\n¬∑\\n\\nFeb 20, 2019\\n\\n--\\n\\nListen\\n\\nShare\\n\\nIn the Urban Institute‚Äôs [Connecting Police to Communities project](https://www.urban.org/policy-centers/justice-policy-center/projects/connecting-police-communities), we‚Äôve been working to identify best practices for law enforcement use of social media by examining tweets from police and the public that document their interactions and behavior on Twitter. We collected millions of tweets containing the words ‚Äúcop‚Äù or ‚Äúpolice‚Äù to help us determine [public sentiment toward police](https://www.urban.org/research/publication/public-perceptions-police-social-media). We also collected more than 500,000 tweets from over 300 law enforcement agencies to determine [how police use social media](https://www.urban.org/research/publication/social-media-guidebook-law-enforcement-agencies).\\n\\nOnce we collected all the tweets, we needed to categorize them by either sentiment or topic. Traditionally, senior social science researchers would assign one or more research assistants the unenviable task of manually categorizing each tweet. At the scale of tweets we collected, we quickly realized that we needed a less manual, more technologically oriented solution. We simply didn‚Äôt have the time or resources for someone to read and categorize each tweet manually, so we decided to develop supervised machine learning models to perform the same task automatically. In this post, we explain how we developed a model to classify text data, along with some troubleshooting tips and the lessons we learned.\\n\\n# What Is Machine Learning?\\n\\nThe rise of online publications, reviews, and social media has created a trove of easily accessible text data for researchers and data scientists to use to study human behavior and social phenomena.\\n\\nFor researchers used to analyzing standardized tabular data, deriving meaning from unstructured and sometimes messy text is difficult. For example, movie studios looking for early audience feedback may want to categorize the topic of tweets or the sentiment of reviews, but manually reading and sorting them would take too long and would be prohibitively expensive. Instead, analysts typically use supervised machine learning techniques to classify each piece of text data in a fraction of the time.\\n\\nIn machine learning, systems learn from and detect patterns in data without being explicitly programmed by a researcher. Despite the programming process‚Äôs similarity to traditional regression models, machine learning practitioners typically use different language than social scientists ‚Äî the biggest distinction being the use of the word ‚Äúfeatures‚Äù to describe variables in a model.\\n\\nThere are generally two types of machine learning models ‚Äî unsupervised and supervised. In unsupervised machine learning, no outcome is identified, and the model clusters observations together based on patterns, similar to a factor analysis. In supervised machine learning, the researcher specifies the outcome and uses regression or classification algorithms to predict this outcome variable using the input features. For our text classification tasks, we developed two supervised machine learning models built for classification. One model classified the sentiment of public tweets toward police, and the other classified the topic of law enforcement tweets.\\n\\n# Developing a Text Classification Model\\n\\nText classification is a multistep process, and the steps included in your final process depend on the specific problem you are trying to solve. In other words, you may not get the best possible results by simply re-creating our features and models. This is because the performance of any given model depends on its ability to detect patterns in the input text, and different collections of text typically have different patterns. The following steps formed the basis for our text analysis and provide general guidelines for any text classification process.\\n\\n**Step 1: Develop features for the model.** In machine learning, features are the independent or explanatory variables used to predict the outcome variable. The features we created for analyzing Twitter data (and the features created for similar analyses) will typically fall into one of two categories: _content_ or _metadata_.\\n\\n_Content:_ Many features in the model will be based on the words in the text, but the text often needs some processing before the words can be converted into features. Analyzing text is typically a two-step procedure:\\n\\n1\\\\. Preprocess the text data. Preprocessing standardizes your text as much as possible so that the model can more easily recognize general patterns. For example, you may want to remove ‚Äústop words,‚Äù common words that may add unnecessary noise to the data and don‚Äôt have much meaning on their own ‚Äî for example, ‚Äúand,‚Äù ‚Äúin,‚Äù ‚Äúor,‚Äù and ‚Äúthe.‚Äù Similarly, you can stem or lemmatize words to convert them to their root form (e.g., ‚Äúwalks,‚Äù ‚Äúwalked,‚Äù and ‚Äúwalking‚Äù become ‚Äúwalk‚Äù) so that the model recognizes them as the same word. Other preprocessing steps include converting the text to lowercase letters and inserting placeholders for or removing punctuation, numbers, and symbols. Rather than removing numbers, replacing them with a placeholder was helpful for categorizing police tweets by topic, because the presence of numbers helped predict certain categories (e.g., when addresses were included in crime incident updates).\\n\\n2\\\\. Create a document-term matrix. In a document-term matrix, each row in the dataset represents the unit of text (e.g., a tweet or a review), and each column represents a word used in the collection of text data. The two tweets, ‚ÄúPolice training downtown tonight‚Äù and ‚ÄúPolice out here with tasers tonight‚Äù would lead to a document-term matrix that looks like this table:\\n\\n_Metadata:_ External information about the text often improves the model‚Äôs performance as much as the text itself. When working with Twitter data, it may be helpful to include information about the time of day or day of the week the tweet was posted or the number of retweets the post received. You can also extract information from the text using advanced text analytics tools like [CoreNLP](https://stanfordnlp.github.io/CoreNLP/...\\n\\n--- Result 8 ---\\nTitle: How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\\n\\nContent:\\nHow To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n‚ÜêPrevious\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\\n\\n--- Result 9 ---\\nTitle: A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\nURL: https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312\\n\\nContent:\\nA Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\n[\\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n18 min read\\n¬∑Apr 28, 2024\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\\nListen\\nShare\\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\\n**> Note:\\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\\n*> ‚ÄúIt is estimated that 90% of the world‚Äôs data was generated in the last two years alone‚Äù\\n*> by [> Amount of Data Created Daily (2023)\\n](https://explodingtopics.com/blog/data-generated-per-day)\\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\\n## Methodology\\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\\nPress enter or click to view image in full size\\n![]()\\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\\n## Key Discoveries of Text Data Competitions\\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term ‚Äúquestion answering‚Äù exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Winning solutions in Kaggle text‚Äëclassification contests usually blend strong language‚Äëmodel representations with carefully engineered meta‚Äëdata features, then stack or ensemble the resulting models.  The most successful pipelines start by extracting dense embeddings from pretrained transformers (e.g.,‚ÄØBERT, RoBERTa) or fast‚Äëtext vectors for the raw text, while simultaneously turning auxiliary information such as document length, source, author ID, timestamp or categorical tags into numeric features (one‚Äëhot, target‚Äëencoded, or simple statistics).  These meta‚Äëfeatures are concatenated with the text embeddings and fed into a gradient‚Äëboosting model (LightGBM, XGBoost) or a shallow neural net, which often outperforms a pure‚Äëtext model because the extra signals help disambiguate noisy or short inputs.  After the base models are trained, competitors typically blend several variants (different seeds, architectures, or feature sets) using weighted averaging or stacking, and they validate with stratified K‚Äëfold cross‚Äëvalidation to guard against over‚Äëfitting‚ÄØ([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\\n\\nIn addition to the model architecture, practical tricks that distinguish winning entries include efficient handling of large datasets (e.g., loading data as parquet or feather, using Dask or cuDF for parallel processing) and aggressive memory optimisation, which make it feasible to train large transformer‚Äëbased encoders together with many meta‚Äëfeatures on limited hardware‚ÄØ([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  Pseudo‚Äëlabeling on the test set, careful class‚Äëbalance weighting, and early‚Äëstopping based on validation scores are also common, allowing the final ensemble to capture both the nuanced language patterns and the predictive power of the side information.\", \"url\": \"\"}, {\"title\": \"Text Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top NLP competitions.\\n\\nNamely, I‚Äôve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) ‚Äì $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) ‚Äì $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) ‚Äì $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) ‚Äì $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) ‚Äì $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let‚Äôs begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let‚Äôs see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet‚Äôs see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in ¬†R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\", \"url\": \"https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\"}, {\"title\": \"Text Classification Techniques - Explained - Kaggle\", \"text\": \"Text Classification Techniques - Explained | Kaggle\\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\\n[\\nLearn more\\n](https://www.kaggle.com/cookies)\\nOK, Got it.\\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\\nIf the issue persists, it's likely a problem on our side.\\n```\\nLoading CSS chunk 1019 failed.\\\\\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\\nkeyboard\\\\_arrow\\\\_upcontent\\\\_copy\\nError: Loading CSS chunk 1019 failed.\\\\\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a6aef772a946d7d4:1:10006)\\n```\\nRefresh\", \"url\": \"https://www.kaggle.com/code/eraikako/text-classification-techniques-explained\"}, {\"title\": \"Advanced Techniques of Text Classification - Kaggle\", \"text\": \"Checking your browser - reCAPTCHA\\nChecking your browser before accessing www.kaggle.com ...\\nClick[here](#)if you are not automatically redirected after 5 seconds.\", \"url\": \"https://www.kaggle.com/code/meharshanali/advanced-techniques-of-text-classification\"}, {\"title\": \"A Beginners Guide to dealing with Text Data - Kaggle\", \"text\": \"<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=c93cac9cb620a9ebe94a:1:10973)</pre></div></div></div></div>\", \"url\": \"https://www.kaggle.com/code/prashant111/a-beginners-guide-to-dealing-with-text-data\"}, {\"title\": \"A Guide to any Classification Problem - Kaggle\", \"text\": \"<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=85e822d649bb8c5b8715:1:11062)</pre></div></div></div></div>\", \"url\": \"https://www.kaggle.com/code/durgancegaur/a-guide-to-any-classification-problem\"}, {\"title\": \"How to Categorize Large Amounts of Text Data | by Data@Urban\", \"text\": \"[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F71e1fd590e2a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\n_Original illustration courtesy of Majcot@Shutterstock_\\n\\n# How to Categorize Large Amounts of Text Data\\n\\n[![Data@Urban](https://miro.medium.com/v2/resize:fill:88:88/1*SmPWgbgKs_B3FJ242EIJJg.png)](https://urban-institute.medium.com/?source=post_page-----71e1fd590e2a--------------------------------)\\n\\n[Data@Urban](https://urban-institute.medium.com/?source=post_page-----71e1fd590e2a--------------------------------)\\n\\n¬∑\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed38af6a2b4c&operation=register&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&user=Data%40Urban&userId=ed38af6a2b4c&source=post_page-ed38af6a2b4c----71e1fd590e2a---------------------post_header-----------)\\n\\n10 min read\\n\\n¬∑\\n\\nFeb 20, 2019\\n\\n--\\n\\nListen\\n\\nShare\\n\\nIn the Urban Institute‚Äôs [Connecting Police to Communities project](https://www.urban.org/policy-centers/justice-policy-center/projects/connecting-police-communities), we‚Äôve been working to identify best practices for law enforcement use of social media by examining tweets from police and the public that document their interactions and behavior on Twitter. We collected millions of tweets containing the words ‚Äúcop‚Äù or ‚Äúpolice‚Äù to help us determine [public sentiment toward police](https://www.urban.org/research/publication/public-perceptions-police-social-media). We also collected more than 500,000 tweets from over 300 law enforcement agencies to determine [how police use social media](https://www.urban.org/research/publication/social-media-guidebook-law-enforcement-agencies).\\n\\nOnce we collected all the tweets, we needed to categorize them by either sentiment or topic. Traditionally, senior social science researchers would assign one or more research assistants the unenviable task of manually categorizing each tweet. At the scale of tweets we collected, we quickly realized that we needed a less manual, more technologically oriented solution. We simply didn‚Äôt have the time or resources for someone to read and categorize each tweet manually, so we decided to develop supervised machine learning models to perform the same task automatically. In this post, we explain how we developed a model to classify text data, along with some troubleshooting tips and the lessons we learned.\\n\\n# What Is Machine Learning?\\n\\nThe rise of online publications, reviews, and social media has created a trove of easily accessible text data for researchers and data scientists to use to study human behavior and social phenomena.\\n\\nFor researchers used to analyzing standardized tabular data, deriving meaning from unstructured and sometimes messy text is difficult. For example, movie studios looking for early audience feedback may want to categorize the topic of tweets or the sentiment of reviews, but manually reading and sorting them would take too long and would be prohibitively expensive. Instead, analysts typically use supervised machine learning techniques to classify each piece of text data in a fraction of the time.\\n\\nIn machine learning, systems learn from and detect patterns in data without being explicitly programmed by a researcher. Despite the programming process‚Äôs similarity to traditional regression models, machine learning practitioners typically use different language than social scientists ‚Äî the biggest distinction being the use of the word ‚Äúfeatures‚Äù to describe variables in a model.\\n\\nThere are generally two types of machine learning models ‚Äî unsupervised and supervised. In unsupervised machine learning, no outcome is identified, and the model clusters observations together based on patterns, similar to a factor analysis. In supervised machine learning, the researcher specifies the outcome and uses regression or classification algorithms to predict this outcome variable using the input features. For our text classification tasks, we developed two supervised machine learning models built for classification. One model classified the sentiment of public tweets toward police, and the other classified the topic of law enforcement tweets.\\n\\n# Developing a Text Classification Model\\n\\nText classification is a multistep process, and the steps included in your final process depend on the specific problem you are trying to solve. In other words, you may not get the best possible results by simply re-creating our features and models. This is because the performance of any given model depends on its ability to detect patterns in the input text, and different collections of text typically have different patterns. The following steps formed the basis for our text analysis and provide general guidelines for any text classification process.\\n\\n**Step 1: Develop features for the model.** In machine learning, features are the independent or explanatory variables used to predict the outcome variable. The features we created for analyzing Twitter data (and the features created for similar analyses) will typically fall into one of two categories: _content_ or _metadata_.\\n\\n_Content:_ Many features in the model will be based on the words in the text, but the text often needs some processing before the words can be converted into features. Analyzing text is typically a two-step procedure:\\n\\n1\\\\. Preprocess the text data. Preprocessing standardizes your text as much as possible so that the model can more easily recognize general patterns. For example, you may want to remove ‚Äústop words,‚Äù common words that may add unnecessary noise to the data and don‚Äôt have much meaning on their own ‚Äî for example, ‚Äúand,‚Äù ‚Äúin,‚Äù ‚Äúor,‚Äù and ‚Äúthe.‚Äù Similarly, you can stem or lemmatize words to convert them to their root form (e.g., ‚Äúwalks,‚Äù ‚Äúwalked,‚Äù and ‚Äúwalking‚Äù become ‚Äúwalk‚Äù) so that the model recognizes them as the same word. Other preprocessing steps include converting the text to lowercase letters and inserting placeholders for or removing punctuation, numbers, and symbols. Rather than removing numbers, replacing them with a placeholder was helpful for categorizing police tweets by topic, because the presence of numbers helped predict certain categories (e.g., when addresses were included in crime incident updates).\\n\\n2\\\\. Create a document-term matrix. In a document-term matrix, each row in the dataset represents the unit of text (e.g., a tweet or a review), and each column represents a word used in the collection of text data. The two tweets, ‚ÄúPolice training downtown tonight‚Äù and ‚ÄúPolice out here with tasers tonight‚Äù would lead to a document-term matrix that looks like this table:\\n\\n_Metadata:_ External information about the text often improves the model‚Äôs performance as much as the text itself. When working with Twitter data, it may be helpful to include information about the time of day or day of the week the tweet was posted or the number of retweets the post received. You can also extract information from the text using advanced text analytics tools like [CoreNLP](https://stanfordnlp.github.io/CoreNLP/...\", \"url\": \"https://urban-institute.medium.com/how-to-categorize-large-amounts-of-text-data-71e1fd590e2a\"}, {\"title\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n‚ÜêPrevious\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\", \"url\": \"https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\"}, {\"title\": \"A Journey Through Kaggle Text Data Competitions From 2021 to 2023\", \"text\": \"A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\n[\\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n18 min read\\n¬∑Apr 28, 2024\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\\nListen\\nShare\\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\\n**> Note:\\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\\n*> ‚ÄúIt is estimated that 90% of the world‚Äôs data was generated in the last two years alone‚Äù\\n*> by [> Amount of Data Created Daily (2023)\\n](https://explodingtopics.com/blog/data-generated-per-day)\\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\\n## Methodology\\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\\nPress enter or click to view image in full size\\n![]()\\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\\n## Key Discoveries of Text Data Competitions\\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term ‚Äúquestion answering‚Äù exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...\", \"url\": \"https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312\"}]}",
        "tool.duration_ms": 5110.719442367554
      }
    }
  ],
  "metadata": {
    "span_count": 27,
    "last_updated": "2026-01-09T11:43:29.833026"
  }
}