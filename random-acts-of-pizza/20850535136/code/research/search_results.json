{
  "query": "What are winning approaches for text classification competitions on Kaggle that combine text and meta-data features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning solutions in Kaggle text\u2011classification contests usually blend strong language\u2011model representations with carefully engineered meta\u2011data features, then stack or ensemble the resulting models.  The most successful pipelines start by extracting dense embeddings from pretrained transformers (e.g.,\u202fBERT, RoBERTa) or fast\u2011text vectors for the raw text, while simultaneously turning auxiliary information such as document length, source, author ID, timestamp or categorical tags into numeric features (one\u2011hot, target\u2011encoded, or simple statistics).  These meta\u2011features are concatenated with the text embeddings and fed into a gradient\u2011boosting model (LightGBM, XGBoost) or a shallow neural net, which often outperforms a pure\u2011text model because the extra signals help disambiguate noisy or short inputs.  After the base models are trained, competitors typically blend several variants (different seeds, architectures, or feature sets) using weighted averaging or stacking, and they validate with stratified K\u2011fold cross\u2011validation to guard against over\u2011fitting\u202f([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\n\nIn addition to the model architecture, practical tricks that distinguish winning entries include efficient handling of large datasets (e.g., loading data as parquet or feather, using Dask or cuDF for parallel processing) and aggressive memory optimisation, which make it feasible to train large transformer\u2011based encoders together with many meta\u2011features on limited hardware\u202f([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  Pseudo\u2011labeling on the test set, careful class\u2011balance weighting, and early\u2011stopping based on validation scores are also common, allowing the final ensemble to capture both the nuanced language patterns and the predictive power of the side information.",
      "url": ""
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "Text Classification Techniques - Explained - Kaggle",
      "text": "Text Classification Techniques - Explained | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a6aef772a946d7d4:1:10006)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/eraikako/text-classification-techniques-explained"
    },
    {
      "title": "Advanced Techniques of Text Classification - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/meharshanali/advanced-techniques-of-text-classification"
    },
    {
      "title": "A Beginners Guide to dealing with Text Data - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=c93cac9cb620a9ebe94a:1:10973)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/prashant111/a-beginners-guide-to-dealing-with-text-data"
    },
    {
      "title": "A Guide to any Classification Problem - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=85e822d649bb8c5b8715:1:11062)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/durgancegaur/a-guide-to-any-classification-problem"
    },
    {
      "title": "How to Categorize Large Amounts of Text Data | by Data@Urban",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F71e1fd590e2a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n_Original illustration courtesy of Majcot@Shutterstock_\n\n# How to Categorize Large Amounts of Text Data\n\n[![Data@Urban](https://miro.medium.com/v2/resize:fill:88:88/1*SmPWgbgKs_B3FJ242EIJJg.png)](https://urban-institute.medium.com/?source=post_page-----71e1fd590e2a--------------------------------)\n\n[Data@Urban](https://urban-institute.medium.com/?source=post_page-----71e1fd590e2a--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed38af6a2b4c&operation=register&redirect=https%3A%2F%2Furban-institute.medium.com%2Fhow-to-categorize-large-amounts-of-text-data-71e1fd590e2a&user=Data%40Urban&userId=ed38af6a2b4c&source=post_page-ed38af6a2b4c----71e1fd590e2a---------------------post_header-----------)\n\n10 min read\n\n\u00b7\n\nFeb 20, 2019\n\n--\n\nListen\n\nShare\n\nIn the Urban Institute\u2019s [Connecting Police to Communities project](https://www.urban.org/policy-centers/justice-policy-center/projects/connecting-police-communities), we\u2019ve been working to identify best practices for law enforcement use of social media by examining tweets from police and the public that document their interactions and behavior on Twitter. We collected millions of tweets containing the words \u201ccop\u201d or \u201cpolice\u201d to help us determine [public sentiment toward police](https://www.urban.org/research/publication/public-perceptions-police-social-media). We also collected more than 500,000 tweets from over 300 law enforcement agencies to determine [how police use social media](https://www.urban.org/research/publication/social-media-guidebook-law-enforcement-agencies).\n\nOnce we collected all the tweets, we needed to categorize them by either sentiment or topic. Traditionally, senior social science researchers would assign one or more research assistants the unenviable task of manually categorizing each tweet. At the scale of tweets we collected, we quickly realized that we needed a less manual, more technologically oriented solution. We simply didn\u2019t have the time or resources for someone to read and categorize each tweet manually, so we decided to develop supervised machine learning models to perform the same task automatically. In this post, we explain how we developed a model to classify text data, along with some troubleshooting tips and the lessons we learned.\n\n# What Is Machine Learning?\n\nThe rise of online publications, reviews, and social media has created a trove of easily accessible text data for researchers and data scientists to use to study human behavior and social phenomena.\n\nFor researchers used to analyzing standardized tabular data, deriving meaning from unstructured and sometimes messy text is difficult. For example, movie studios looking for early audience feedback may want to categorize the topic of tweets or the sentiment of reviews, but manually reading and sorting them would take too long and would be prohibitively expensive. Instead, analysts typically use supervised machine learning techniques to classify each piece of text data in a fraction of the time.\n\nIn machine learning, systems learn from and detect patterns in data without being explicitly programmed by a researcher. Despite the programming process\u2019s similarity to traditional regression models, machine learning practitioners typically use different language than social scientists \u2014 the biggest distinction being the use of the word \u201cfeatures\u201d to describe variables in a model.\n\nThere are generally two types of machine learning models \u2014 unsupervised and supervised. In unsupervised machine learning, no outcome is identified, and the model clusters observations together based on patterns, similar to a factor analysis. In supervised machine learning, the researcher specifies the outcome and uses regression or classification algorithms to predict this outcome variable using the input features. For our text classification tasks, we developed two supervised machine learning models built for classification. One model classified the sentiment of public tweets toward police, and the other classified the topic of law enforcement tweets.\n\n# Developing a Text Classification Model\n\nText classification is a multistep process, and the steps included in your final process depend on the specific problem you are trying to solve. In other words, you may not get the best possible results by simply re-creating our features and models. This is because the performance of any given model depends on its ability to detect patterns in the input text, and different collections of text typically have different patterns. The following steps formed the basis for our text analysis and provide general guidelines for any text classification process.\n\n**Step 1: Develop features for the model.** In machine learning, features are the independent or explanatory variables used to predict the outcome variable. The features we created for analyzing Twitter data (and the features created for similar analyses) will typically fall into one of two categories: _content_ or _metadata_.\n\n_Content:_ Many features in the model will be based on the words in the text, but the text often needs some processing before the words can be converted into features. Analyzing text is typically a two-step procedure:\n\n1\\. Preprocess the text data. Preprocessing standardizes your text as much as possible so that the model can more easily recognize general patterns. For example, you may want to remove \u201cstop words,\u201d common words that may add unnecessary noise to the data and don\u2019t have much meaning on their own \u2014 for example, \u201cand,\u201d \u201cin,\u201d \u201cor,\u201d and \u201cthe.\u201d Similarly, you can stem or lemmatize words to convert them to their root form (e.g., \u201cwalks,\u201d \u201cwalked,\u201d and \u201cwalking\u201d become \u201cwalk\u201d) so that the model recognizes them as the same word. Other preprocessing steps include converting the text to lowercase letters and inserting placeholders for or removing punctuation, numbers, and symbols. Rather than removing numbers, replacing them with a placeholder was helpful for categorizing police tweets by topic, because the presence of numbers helped predict certain categories (e.g., when addresses were included in crime incident updates).\n\n2\\. Create a document-term matrix. In a document-term matrix, each row in the dataset represents the unit of text (e.g., a tweet or a review), and each column represents a word used in the collection of text data. The two tweets, \u201cPolice training downtown tonight\u201d and \u201cPolice out here with tasers tonight\u201d would lead to a document-term matrix that looks like this table:\n\n_Metadata:_ External information about the text often improves the model\u2019s performance as much as the text itself. When working with Twitter data, it may be helpful to include information about the time of day or day of the week the tweet was posted or the number of retweets the post received. You can also extract information from the text using advanced text analytics tools like [CoreNLP](https://stanfordnlp.github.io/CoreNLP/...",
      "url": "https://urban-institute.medium.com/how-to-categorize-large-amounts-of-text-data-71e1fd590e2a"
    },
    {
      "title": "How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\nDiscover Anything**\n[\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\nHackernoon](https://hackernoon.com/)\nSignup[Write](https://app.hackernoon.com/new)\n******\n**139reads\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\nby\n**neptune.ai Jakub Czakon**\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\nAugust 23rd, 2020\nTLDR**\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\n****\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\n****\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\n[\n\u2190Previous\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\n### About Author\n[](https://hackernoon.com/u/neptuneAI_jakub)\n[**neptune.ai Jakub Czakon**@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\n#### Comments\n#### TOPICS\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\n#### THIS ARTICLE WAS FEATURED IN\n[\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\n**\nArweave\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\nViewBlock\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\n](https://www.joyk.com/dig/detail/1707491664448324)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\n](https://unni.io/domains/hackernoon.com/page/48760739)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\n](https://unni.io/domains/hackernoon.com/page/50536904)\n#### Related Stories\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\nMar 23, 2020\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[#METRICS](https://hackernoon.com/tagged/metrics)\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\nNov 11, 2023\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\nAug 15, 2023\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\n[Bala Priya C](https://hackernoon.com/u/balapriya)\nFeb 03, 2022\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\nJan 28, 2023\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\nNov 09, 2020\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\nMar 23, 2020\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[#METRICS](https://hackernoon.com/tagged/metrics)\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...",
      "url": "https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh"
    },
    {
      "title": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023",
      "text": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\n[\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n18 min read\n\u00b7Apr 28, 2024\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\nListen\nShare\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\n**> Note:\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\n*> \u201cIt is estimated that 90% of the world\u2019s data was generated in the last two years alone\u201d\n*> by [> Amount of Data Created Daily (2023)\n](https://explodingtopics.com/blog/data-generated-per-day)\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\n## Methodology\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\nPress enter or click to view image in full size\n![]()\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\n## Key Discoveries of Text Data Competitions\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term \u201cquestion answering\u201d exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...",
      "url": "https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312"
    }
  ]
}