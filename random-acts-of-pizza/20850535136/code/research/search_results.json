{
  "query": "How do Kaggle winners handle class imbalance in binary classification competitions with 3:1 ratio?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Kaggle winners typically tame a 3\u202f:\u202f1 binary\u2011class imbalance with a short, repeatable workflow that combines data\u2011level tricks and model\u2011level adjustments\u202f\u2014\u202fall backed by the same practices discussed in Kaggle\u2019s own discussions and competition post\u2011mortems\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589); [Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)):\n\n1. **Create stratified folds** so each training/validation split preserves the 3\u202f:\u202f1 ratio, preventing the model from seeing a skewed distribution during CV.  \n\n2. **Apply a sampling strategy** to the training data:  \n   - *Oversample* the minority class (e.g., SMOTE or simple random replication) or  \n   - *Undersample* the majority class to a more balanced mix. The `imbalanced\u2011learn` library is a common starting point\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \n\n3. **Add class\u2011weighting to the loss** (e.g., `class_weight='balanced'` in scikit\u2011learn, or custom weights in LightGBM/CatBoost). This tells the optimizer to penalize mistakes on the minority class more heavily without altering the raw data.  \n\n4. **Choose an evaluation metric that reflects imbalance**, such as AUC\u2011PR, weighted log\u2011loss, or F1\u2011score, rather than plain accuracy. Align the metric with the competition\u2019s leaderboard and with the business cost of false positives vs. false negatives\u202f([Unbalanced data and cross\u2011validation](https://www.kaggle.com/discussions/questions-and-answers/27589)).  \n\n5. **Tune the decision threshold** after training: scan thresholds on the validation set and pick the one that maximizes the chosen metric (often improves the leaderboard score for imbalanced problems).  \n\n6. **Ensemble diverse models** (e.g., tree\u2011based + neural nets) that were each trained with the above imbalance handling; averaging predictions tends to smooth out residual bias toward the majority class\u202f([Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions)).  \n\n7. **Validate on a hold\u2011out set that mirrors the test distribution** (if possible) to ensure the sampling and weighting choices generalize, then submit the final ensemble.  \n\nFollowing these steps lets top Kagglers turn a modest 3\u202f:\u202f1 skew into a well\u2011balanced learning problem and achieve competitive scores.",
      "url": ""
    },
    {
      "title": "Unbalanced data and cross-validation | Kaggle",
      "text": "<div><div><p>There are two general ways of dealing with imbalanced data: 1) change the data; 2) leave the data alone but change the performance metric and/or the weight of individual data points.</p>\n<p>Changing the data means oversampling the under-represented class(es) with synthetic data points, or undersampling (thinning down) the over-represented class(es). There are various ways of doing both operations, and I will give you one <a href=\"https://github.com/scikit-learn-contrib/imbalanced-learn\"><strong>starting point</strong></a>.</p>\n<p>A second option is to leave the data alone and adjust how we do the training. It is crucial to pick a reliable performance metric for imbalanced data, and to know whether for your purposes it is worse to have too many false positives (FPs) or false negatives (FNs). Ideally, you wouldn't want to have either of the two, but for most applications it should be clear whether we want to avoid FPs or FNs with greater preference.</p>\n<p>To give you an example: you have 900 healthy patients (class 0) and 100 cancer patients (class 1), which is at least somewhat close to the true ratio of healthy and cancer-stricken people out there. If you predict class 0 with 100% accuracy and class 1 with 70% accuracy, your accuracy will be 97% for the whole dataset. That sounds tremendous until we realize that 30 out of 100 cancer patients (30%) will go undiagnosed, meaning that high false negative rate in this case will cost many lives. What if we could adjust our classification so that both classes are predicted with 97% accuracy? This would mean that 27 healthy people would be predicted to have cancer (27 FPs) and 3 cancer-stricken individual would be predicted to be healthy (3 FNs). Now, I wouldn't want to be a doctor who tells a healthy person that they have a cancer because many people have been known to do silly things when they think they are dying. Still, compared to our starting premise, most would agree that on balance it is much better to have 27 people who think they have cancer and later find out they don't, and only 3 patients who were told they are healthy even though they have cancer. The problem is that a simple accuracy as a performance metric for training would never get you to a balanced 97% accuracy. For imbalanced datasets, F1-score (a weighted average of precision and recall), area under curve for ROC and Matthews correlation coefficient may be better measures of classifier quality.</p>\n<p>See the links below for more info:</p>\n<p><a href=\"http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\"><strong>link1</strong></a></p>\n<p><a href=\"https://eva.fing.edu.uy/mod/resource/view.php?id=33977\"><strong>link2</strong></a></p>\n<p><a href=\"https://www.kaggle.com/c/bosch-production-line-performance/forums/t/24120/time-effort-and-reward?forumMessageId=138285#post138285\"><strong>link3</strong></a></p></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/27589"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Methods for Dealing with Imbalanced Data",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Ftboyle10%2Fmethods-for-dealing-with-imbalanced-data)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=e39bd89f6741bf1e27b2:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/tboyle10/methods-for-dealing-with-imbalanced-data"
    },
    {
      "title": "\ud83d\udc51 Imbalance ratio: How to identify an imbalanced dataset? | Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/questions-and-answers/372288#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/discussions/questions-and-answers/372288)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/questions-and-answers/372288#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fdiscussions%2Fquestions-and-answers%2F372288)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fdiscussions%2Fquestions-and-answers%2F372288)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n[Eli\u0161ka Bl\u00e1hov\u00e1](https://www.kaggle.com/elisthefox) \u00b7 Posted 2 years ago in [Questions & Answers](https://www.kaggle.com/discussions/questions-and-answers)\n\narrow\\_drop\\_up3\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)more\\_vert\n\n# \ud83d\udc51 Imbalance ratio: How to identify an imbalanced dataset?\n\nDear community,\n\nI have a question for you to which I can't find a common answer. I wonder how to be able to classify a dataset as imbalanced? Is there some rule of thumb I can use? I have found this article, where it says that highly imbalanced dataset has ratio of 10-1 [https://www.sciencedirect.com/topics/computer-science/imbalance-ratio](https://www.sciencedirect.com/topics/computer-science/imbalance-ratio).\n\nI would love to hear your opinion.\n\nThanks a lot!\n\n[Exploratory Data Analysis](https://www.kaggle.com/discussions?tags=13201-Exploratory+Data+Analysis) [Data Analytics](https://www.kaggle.com/discussions?tags=13215-Data+Analytics) [Data Cleaning](https://www.kaggle.com/discussions?tags=13202-Data+Cleaning)\n\nPlease [sign in](https://www.kaggle.com/account/login) to reply to this topic.\n\ncomment\n\n## 8 Comments\n\nHotness\n\n[**Senapati Rajesh**](https://www.kaggle.com/senapatirajesh)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up1\n\nmore\\_vert\n\nThank you very much all for the comments and here is my version of answer\n\nImbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n\nImbalance means that the number of data points available for different the classes is different:\n\nIf there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n\nA typical example of imbalanced data is encountered in e-mail classification problem where emails are classified into ham or spam. The number of spam emails is usually lower than the number of relevant (ham) emails. So, using the original distribution of two classes leads to imbalanced dataset.\n\nUsing accuracy as a performace measure for highly imbalanced datasets is not a good idea. For example, if 90% points belong to the true class in a binary classification problem, a default prediction of true for all data points leads to a classifier which is 90% accurate, even though the classifier has not learnt anything about the classification problem at hand!\n\nImbalanced data can be handled by using below techniques:\n\n1.Resample the training set\n\n(Under sampling -This method is used when quantity of data is sufficient or\n\nover sampling-It tries to balance dataset by increasing the size of rare samples.)\n\n2.Use K-fold Cross-Validation in the Right Way\n\n3.Ensemble Different Resampled Datasets(This approach is simple and perfectly horizontally scalable if you have a lot of data)\n\n4.Resample with Different Ratios( Instead of training all models with the same ratio in the ensemble, it is worth trying to ensemble different ratios)\n\n5.Most importantly Choose Proper Evaluation Metric(\n\nex:The accuracy of a classifier is the total number of correct predictions by the classifier divided by the total number of predictions.For an imbalanced class dataset F1 score is a more appropriate metric. It is the harmonic mean of precision and recall)\n\n6.SMOTE is another technique to oversample the minority class.SMOTE looks into minority class instances and use k nearest neighbor to select a random nearest neighbor, and a synthetic instance is created randomly in feature space.\n\nThank u and Pl feel free to suggest for any modifications.\n\n[**Olqa\\_842**](https://www.kaggle.com/zvr842)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up1\n\nmore\\_vert\n\nPlease, could you share dataset too?\n\n[**Eli\u0161ka Bl\u00e1hov\u00e1**](https://www.kaggle.com/elisthefox)\n\nTopic Author\n\nPosted 2 years ago\n\narrow\\_drop\\_up0\n\nmore\\_vert\n\nOf course, currently, I am working with this dataset: [https://www.kaggle.com/datasets/shubh0799/churn-modelling](https://www.kaggle.com/datasets/shubh0799/churn-modelling). Link to my notebook: [https://www.kaggle.com/code/elisthefox/churn-analytics-should-i-stay-or-should-i-go](https://www.kaggle.com/code/elisthefox/churn-analytics-should-i-stay-or-should-i-go)\n\n[**Carl McBride Ellis**](https://www.kaggle.com/carlmcbrideellis)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up2\n\nmore\\_vert\n\nDear [@elisthefox](https://www.kaggle.com/elisthefox)\n\nYou may perhaps find this short notebook of some interest: [Classification: How imbalanced is \"imbalanced\"?\"](https://www.kaggle.com/code/carlmcbrideellis/classification-how-imbalanced-is-imbalanced).\n\nAll the best,\n\ncarl\n\n[**Eli\u0161ka Bl\u00e1hov\u00e1**](https://www.kaggle.com/elisthefox)\n\nTopic Author\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up1\n\nmore\\_vert\n\nThanks a lot for letting me know about this great notebook, If I understood correctly, the metric balanced accuracy helps to determine the ratio for imbalanced dataset, hence if its results are equal to 0.85, then dataset consisting of 10K rows with 8500 rows with target variable 1 and 1500 rows with target variable 0 is considered as a balanced target variable?\n\n[**Carl McBride Ellis**](https://www.kaggle.com/carlmcbrideellis)\n\nPosted 2 years ago\n\n![This post earned a bronze medal](https://www.kaggle.com/static/images/medals/discussion/bronzel@1x.png)\n\narrow\\_drop\\_up2\n\nmore\\_vert\n\nDear [@elisthefox](https://www.kaggle.com/elisthefox)\n\nStrictly speaking a dataset is balanced if there are an equal number of rows associated with each class (or label) in the training data. One can perform a quick check with something like:\n\n```\ndf['labels'].value_counts().to_frame().T\n\ncontent_copy\n```\n\nIn the notebook I suggest that most metrics work well up to an imbalance of 85:15 after which one should exercise caution. Note however as far as most estimators are concerned, they work pretty well irrespective of whether ...",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/372288"
    },
    {
      "title": "Dealing with extremely imbalanced dataset - binary classification | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=42762aa12f6df4b246e8:1:11101)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/473546"
    },
    {
      "title": "Best techniques and metrics for Imbalanced Dataset",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=805aeed0a20bb825939e:1:11032)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset"
    },
    {
      "title": "Classification on imbalanced data \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Classification on imbalanced data | TensorFlow Core[Skip to main content](#main-content)\n[![TensorFlow](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/tensorflow/images/lockup.svg)](https://www.tensorflow.org/)\n* /\n* English\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4[GitHub](https://github.com/tensorflow)Sign in\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n* [TensorFlow](https://www.tensorflow.org/)\n* [Learn](https://www.tensorflow.org/learn)\n* [TensorFlow Core](https://www.tensorflow.org/tutorials)\n# Classification on imbalanced dataStay organized with collectionsSave and categorize content based on your preferences.\n[![](https://www.tensorflow.org/images/tf_logo_32px.png)View on TensorFlow.org](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)|[![](https://www.tensorflow.org/images/colab_logo_32px.png)Run in Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/GitHub-Mark-32px.png)View source on GitHub](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb)|[![](https://www.tensorflow.org/images/download_logo_32px.png)Download notebook](https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/structured_data/imbalanced_data.ipynb)|\nThis tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use[Keras](https://www.tensorflow.org/guide/keras/overview)to define the model and[class weights](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model)to help the model learn from the imbalanced data. .\nThis tutorial contains complete code to:\n* Load a CSV file using Pandas.\n* Create train, validation, and test sets.\n* Define and train a model using Keras (including setting class weights).\n* Evaluate the model using various metrics (including precision and recall).\n* Select a threshold for a probabilistic classifier to get a deterministic classifier.\n* Try and compare with class weighted modelling and oversampling.## Setup\n```\n`importtensorflowastffromtensorflowimportkerasimportosimporttempfileimportmatplotlibasmplimportmatplotlib.pyplotaspltimportnumpyasnpimportpandasaspdimportseabornassnsimportsklearnfromsklearn.metricsimportconfusion\\_matrixfromsklearn.model\\_selectionimporttrain\\_test\\_splitfromsklearn.preprocessingimportStandardScaler`\n```\n```\n2024-08-20 01:23:52.305388: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 01:23:52.326935: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 01:23:52.333533: E external/local\\_xla/xla/stream\\_executor/cuda/cuda\\_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n```\n`mpl.rcParams['figure.figsize']=(12,10)colors=plt.rcParams['axes.prop\\_cycle'].by\\_key()['color']`\n```\n## Data processing and exploration\n### Download the Kaggle Credit Card Fraud data set\nPandas is a Python library with many helpful utilities for loading and working with structured data. It can be used to download CSVs into a Pandas[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n**Note:**This dataset has been collected and analysed during a research collaboration of Worldline and the[Machine Learning Group](http://mlg.ulb.ac.be)of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available[here](https://www.researchgate.net/project/Fraud-detection-5)and the page of the[DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/)project\n```\n`file=tf.keras.utilsraw\\_df=pd.read\\_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')raw\\_df.head()`\n```\n```\n`raw\\_df[['Time','V1','V2','V3','V4','V5','V26','V27','V28','Amount','Class']].describe()`\n```\n### Examine the class label imbalance\nLet's look at the dataset imbalance:\n```\n`neg,pos=np.bincount(raw\\_df['Class'])total=neg+posprint('Examples:\\\\nTotal:{}\\\\nPositive:{}({:.2f}% of total)\\\\n'.format(total,pos,100\\*pos/total))`\n```\n```\nExamples:\nTotal: 284807\nPositive: 492 (0.17% of total)\n```\nThis shows the small fraction of positive samples.\n### Clean, split and normalize the data\nThe raw data has a few issues. First the`Time`and`Amount`columns are too variable to use directly. Drop the`Time`column (since it's not clear what it means) and take the log of the`Amount`column to reduce its range.\n```\n`cleaned\\_df=raw\\_df.copy()# You don't want the `Time` column.cleaned\\_df.pop('Time')# The `Amount` column covers a huge range. Convert to log-space.eps=0.001# 0 =&gt; 0.1\u00a2cleaned\\_df['Log Amount']=np.log(cleaned\\_df.pop('Amount')+eps)`\n```\nSplit the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where[overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)is a significant concern from the lack of training data.\n```\n`# Use a utility from sklearn to split and shuffle your dataset.train\\_df,test\\_df=train\\_test\\_split(cleaned\\_df,test\\_size=0.2)train\\_df,val\\_df=train\\_test\\_split(train\\_df,test\\_size=0.2)# Form np arrays of labels and features.train\\_labels=np.array(train\\_df.pop('Class')).reshape(-1,1)bool\\_train\\_labels=train\\_labels[:,0]!=0val\\_labels=np.array(val\\_df.pop('Class')).reshape(-1,1)test\\_labels=np.array(test\\_df.pop('Class')).reshape(-1,1)train\\_features=np.array(train\\_df)val\\_features=np.array(val\\_df)test\\_features=np.array(test\\_df)`\n```\nWe check whether the distribution of the classes in the three sets is about the same or not.\n```\n`print(f'Average class probability in training set:{train\\_labels.mean():.4f}')print(f'Average class probability in validation set:{val\\_labels.mean():.4f}')print(f'Average class probability in test set:{test\\_labels.mean():.4f}')`\n```\n```\nAverage class probability in training set: 0.0017\nAverage class probability in validation set: 0.0018\nAverage class probability in test set: 0.0018\n```\nGiven the small number of positive labels, this seems about right.\nNormalize the input features using the sklearn StandardScaler.\nThis will set the mean to 0 and standard deviation to 1.\n**Note:**The`StandardScaler`is only fit using the`train\\_features`to be sure the model is not peeking at the validation or test sets.\n```\n`scaler=StandardScaler()train\\_features=scaler.fit\\_transform(train\\_features)val\\_features=scaler.transform(val\\_features)test\\_features=scaler.transform(test\\_features)train\\_features=np.clip(train\\_features,-5,5)val\\_features=np.clip(val\\_features,-5,5)test\\_features=np.clip(test\\_features,-5,5)print('Training labels shape:',train\\_labels.shape)print('Validation labels shape:',...",
      "url": "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2409.19751] Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2409.19751\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2409.19751**(cs)\n[Submitted on 29 Sep 2024]\n# Title:Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification\nAuthors:[Mohamed Abdelhamid](https://arxiv.org/search/cs?searchtype=author&amp;query=Abdelhamid,+M),[Abhyuday Desai](https://arxiv.org/search/cs?searchtype=author&amp;query=Desai,+A)\nView a PDF of the paper titled Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification, by Mohamed Abdelhamid and Abhyuday Desai\n[View PDF](https://arxiv.org/pdf/2409.19751)[HTML (experimental)](https://arxiv.org/html/2409.19751v1)> > Abstract:\n> Class imbalance in binary classification tasks remains a significant challenge in machine learning, often resulting in poor performance on minority classes. This study comprehensively evaluates three widely-used strategies for handling class imbalance: Synthetic Minority Over-sampling Technique (SMOTE), Class Weights tuning, and Decision Threshold Calibration. We compare these methods against a baseline scenario of no-intervention across 15 diverse machine learning models and 30 datasets from various domains, conducting a total of 9,000 experiments. Performance was primarily assessed using the F1-score, although our study also tracked results on additional 9 metrics including F2-score, precision, recall, Brier-score, PR-AUC, and AUC. Our results indicate that all three strategies generally outperform the baseline, with Decision Threshold Calibration emerging as the most consistently effective technique. However, we observed substantial variability in the best-performing method across datasets, highlighting the importance of testing multiple approaches for specific problems. This study provides valuable insights for practitioners dealing with imbalanced datasets and emphasizes the need for dataset-specific analysis in evaluating class imbalance handling techniques. Comments:|13 pages including appendix, 4 tables|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nACMclasses:|I.2.6; I.5.1; I.5.2; I.2.m|\nCite as:|[arXiv:2409.19751](https://arxiv.org/abs/2409.19751)[cs.LG]|\n|(or[arXiv:2409.19751v1](https://arxiv.org/abs/2409.19751v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2409.19751](https://doi.org/10.48550/arXiv.2409.19751)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Abhyuday Desai [[view email](https://arxiv.org/show-email/43e3e529/2409.19751)]\n**[v1]**Sun, 29 Sep 2024 16:02:32 UTC (18 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification, by Mohamed Abdelhamid and Abhyuday Desai\n* [View PDF](https://arxiv.org/pdf/2409.19751)\n* [HTML (experimental)](https://arxiv.org/html/2409.19751v1)\n* [TeX Source](https://arxiv.org/src/2409.19751)\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2409.19751&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2409.19751&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-09](https://arxiv.org/list/cs.LG/2024-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2409.19751?context=cs)\n[cs.AI](https://arxiv.org/abs/2409.19751?context=cs.AI)\n[stat](https://arxiv.org/abs/2409.19751?context=stat)\n[stat.ML](https://arxiv.org/abs/2409.19751?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2409.19751)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2409.19751)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2409.19751)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2409.19751&amp;description=Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2409.19751&amp;title=Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to th...",
      "url": "https://arxiv.org/abs/2409.19751"
    }
  ]
}