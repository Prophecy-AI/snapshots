{
  "query": "What feature engineering techniques work best for combining text and categorical meta-data features in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Combining free\u2011form text with categorical meta\u2011data in Kaggle contests is most often tackled by turning the non\u2011text fields into a textual representation that a transformer can ingest directly.  A simple and surprisingly effective recipe is to join the raw text, numeric values and categorical labels into one string separated by a special token (e.g.,\u202f`[SEP]`).  In FreeAgent\u2019s bank\u2011transaction classifier the transaction description, amount and industry were concatenated as\u202f`Plastic pipes [SEP] -7.99 [SEP] Plumbing`, allowing a single fine\u2011tuned transformer to learn from all modalities at once\u202f([engineering.freeagent.com](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  An even more natural variant is to prepend the categorical information to the review text, treating it as additional \u201cwords\u201d.  In a Kaggle\u2011style clothing\u2011review dataset this \u201cfeatures\u2011to\u2011text\u201d strategy consistently outperformed more elaborate baselines, showing that a straightforward textual conversion of meta\u2011data can be the strongest baseline for multimodal tabular\u2011text problems\u202f([mccormickml.com](https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert)).\n\nA second, slightly richer approach keeps the modalities separate: the text is encoded with a pre\u2011trained transformer (or a TF\u2011IDF / hashing vectorizer) while categorical columns are transformed with embeddings, target\u2011encoding, or one\u2011hot vectors, and the resulting vectors are concatenated before feeding a downstream classifier (e.g., a shallow MLP or gradient\u2011boosted tree).  FreeAgent\u2019s experiments showed that this \u201cextract\u2011text\u2011embedding\u202f+\u202fconcatenate\u2011other\u2011features\u201d pipeline matches or exceeds the pure\u2011string method on a wine\u2011review classification task, indicating that preserving the numeric nature of fields can give the model clearer signals\u202f([engineering.freeagent.com](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  In practice, Kaggle winners often start with the easy text\u2011concatenation trick to get a strong baseline, then iterate toward a hybrid pipeline that encodes categorical data more explicitly for the final performance boost.",
      "url": ""
    },
    {
      "title": "Combining text with numerical and categorical features for classification",
      "text": "[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\n\n# Classification with transformer models\n\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\n\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\n\nFor example a transaction with the following set of features:\n\n|     |     |     |\n| --- | --- | --- |\n| **Description** | **Amount** | **Industry** |\n| Plastic pipes | \u00a3-7.99 | Plumbing |\n\ncould be represented by the string:\u00a0 `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We\u2019ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\n\nThis approach however doesn\u2019t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\n\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\n\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\n\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\n\n# Public wine reviews dataset\n\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores \u2013 wines between 80 and 86 (incl) were flagged as \u201cneutral\u201d, between 87 and 93 as \u201cgood\u201d and greater than 94 as \u201cexcellent\u201d.\n\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\n\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\n\n```\nimport pandas as pd\n\nwine_df = pd.read_csv(\"data/wine_data.csv\")\nbins = [0, 87, 94, np.inf]\nnames = [\"neutral\", \"good\", \"excellent\"]\n\nwine_df[\"rating\"] = pd.cut(wine_df[\"points\"], bins, labels=names)\n\n```\n\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\n\n```\nNUMERICAL_FEATURE = \"price\"\nCATEGORICAL_FEATURE = \"variety\"\nTEXT_FEATURE = \"description\"\nTARGET = \"rating\"\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\n\nwine_df = wine_df[FEATURES + [TARGET]]\n\n```\n\nWe then split our wine dataframe into an 80:20 training:evaluation split.\n\n```\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\n\n```\n\n## LightGBM classifier approach\n\n### Preprocessing\n\n#### Preprocessing numerical and categorical features\n\nIn a first step we will preprocess the numerical and categorical features using SKLearn\u2019s ColumnTransformer.\u00a0 For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\n\nWe preprocessed the categorical features by filling missing values with \u201cother\u201d and OneHot encoded them.\n\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\n\n```\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import (\n    OneHotEncoder,\n    StandardScaler,\n)\n\ndef preprocess_number():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        StandardScaler(),\n    )\n\ndef preprocess_categories():\n    return make_pipeline(\n       SimpleImputer(strategy=\"constant\", fill_value=\"other\", missing_values=np.nan),\n       OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n    )\n\ndef create_preprocessor():\n\n    transformers = [\n        (\"num_preprocessor\", preprocess_number(), [NUMERICAL_FEATURE]),\n        (\"cat_preprocessor\", preprocess_categories(), [CATEGORICAL_FEATURE]),\n    ]\n\n    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n\ncolumn_transformer = create_preprocessor()\ncolumn_transformer.set_output(transform=\"pandas\")\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\n)\n\n```\n\n#### Extracting text vector representation with a transformer model\n\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\n\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\[CLS\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\[CLS\\] token in the final layer as the vector representation of our wine descriptions.\n\nIn order to extract the \\[CLS\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\n\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\_ids, attention\\_mask and token\\_type\\_ids. Only the input\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\n\nThe code for the tokenization is shown below:\n\n```\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"distilbert-base-uncased\"\n\ndef tokenized_pytorch_tensors(\n        df: pd.DataFrame,\n        column_list: list\n    ) -> Dataset:\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    transformers_dataset = Dataset.from_pandas(df)\n\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\n        return tokenizer(\n            model_inputs_batch[TEXT_FEATURE],\n            padding=True,\n            max_length=120,\n            truncation=True,\n        )\n\n    tokenized_dataset = transformers_dataset.map(\n        tokenize,\n        batched=True,\n        batch_size=128\n    )\n\n    tokenized_dataset.set_format(\n        \"torch\",\n        columns=column_list\n    )\n\n    columns_t...",
      "url": "https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification"
    },
    {
      "title": "",
      "text": "# Combining Categorical and Numerical Features with Text in BERT\n\n29 Jun 2021\n\nIn this tutorial we\u2019ll look at the topic of classifying text with BERT, but where we also have additional numerical or categorical features that we want to use to improve our predictions.\n\nTo help motivate our discussion, we\u2019ll be working with a dataset of about 23k clothing reviews. For each review, we have the review text, but also additional information such as:\n\n- The age of the reviewer (numerical feature)\n- The number of upvotes on the review (numerical feature)\n- The department and category of the clothing item (categorical features)\n\nFor each review, we also have a binary label, which is whether or not the reviewer ultimately recommends the item. This is what we are trying to predict.\n\nThis dataset was scraped from an (un-specified) e-commerce website by Nick Brooks and made available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews).\n\nIn Section 2 of this Notebook, I\u2019ve implemented four different \u201cbaseline\u201d strategies which score fairly well, but which don\u2019t incorporate all of the features together.\n\nThen, in Section 3, I\u2019ve implemented a simple strategy to combine _everything_ and feed it through BERT. Specifically, I make text out of the additional features, and prepend this text to the review.\n\nThere is a GitHub project called the [Multimodal-Toolkit](https://github.com/georgian-io/Multimodal-Toolkit) which is how I learned about this clothing review dataset. The toolkit implements a number of more complicated techniques, but their benchmarks (as well as our results below) show that this simple features-to-text strategy works best for this dataset.\n\nIn our weekly discussion group, I talked through this Notebook and we also met with Ken Gu, the author of the Multi-Modal Toolkit! You can watch the recording [here](https://youtu.be/XPt9pIpe1vo).\n\nYou can find the Colab Notebook version of this post [here](https://colab.research.google.com/drive/1YRHK4HO8RktGzlYmGjBo056kzVD4_j9o).\n\nBy Chris McCormick\n\n# Contents\n\n- [Contents](https://mccormickml.com/mccormickml.com#contents)\n- [S1. Clothing Review Dataset](https://mccormickml.com/mccormickml.com#s1-clothing-review-dataset)\n  - [1.1. Download & Parse](https://mccormickml.com/mccormickml.com#11-download--parse)\n  - [1.2. Train-Validation-Test Split](https://mccormickml.com/mccormickml.com#12-train-validation-test-split)\n- [S2. Baseline Strategies](https://mccormickml.com/mccormickml.com#s2-baseline-strategies)\n  - [2.1. Always Recommend](https://mccormickml.com/mccormickml.com#21-always-recommend)\n  - [2.2. Threshold on Rating](https://mccormickml.com/mccormickml.com#22-threshold-on-rating)\n  - [2.3. XGBoost](https://mccormickml.com/mccormickml.com#23-xgboost)\n  - [2.4. BERT on Review Text Only](https://mccormickml.com/mccormickml.com#24-bert-on-review-text-only)\n- [S3. BERT with All Features](https://mccormickml.com/mccormickml.com#s3-bert-with-all-features)\n  - [3.1. All Features to Text](https://mccormickml.com/mccormickml.com#31-all-features-to-text)\n  - [3.2. GPU & Transformers Setup](https://mccormickml.com/mccormickml.com#32-gpu--transformers-setup)\n  - [3.3. Training Parameters](https://mccormickml.com/mccormickml.com#33-training-parameters)\n  - [3.3. Tokenize & Encode](https://mccormickml.com/mccormickml.com#33-tokenize--encode)\n  - [3.4. Training](https://mccormickml.com/mccormickml.com#34-training)\n    - [3.4.1. Setup](https://mccormickml.com/mccormickml.com#341-setup)\n    - [3.4.2. Training Loop](https://mccormickml.com/mccormickml.com#342-training-loop)\n    - [3.4.3. Training Results](https://mccormickml.com/mccormickml.com#343-training-results)\n  - [3.5. Test](https://mccormickml.com/mccormickml.com#35-test)\n- [Conclusion](https://mccormickml.com/mccormickml.com#conclusion)\n\n# S1. Clothing Review Dataset\n\n## 1.1. Download & Parse\n\nRetrieve the .csv file for the dataset.\n\n```\nimport gdown\n\nprint('Downloading dataset...\\n')\n\n# Download the file.\ngdown.download('https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H',\n                'Womens Clothing E-Commerce Reviews.csv',\n                quiet=False)\n\nprint('\\n\\nDONE.')\n```\n\n```\nDownloading dataset...\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\nTo: /content/Womens Clothing E-Commerce Reviews.csv\n8.48MB [00:00, 48.7MB/s]\n\nDONE.\n\n```\n\nParse the dataset csv file into a pandas DataFrame.\n\n```\nimport pandas as pd\n\ndata_df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n\ndata_df.head()\n```\n\n| Clothing ID | Age | Title | Review Text | Rating | Recommended IND | Positive Feedback Count | Division Name | Department Name | Class Name |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 767 | 33 | NaN | Absolutely wonderful - silky and sexy and comf... | 4 | 1 | 0 | Initmates | Intimate | Intimates |\n| 1 | 1080 | 34 | NaN | Love this dress! it's sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses |\n| 2 | 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses |\n| 3 | 1049 | 50 | My favorite buy! | I love, love, love this jumpsuit. it's fun, fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants |\n| 4 | 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses |\n\n_Features_\n\n\u201c **Recommended IND**\u201d is the label we are trying to predict for this dataset. \u201c1\u201d means the reviewer recommended the product and \u201c0\u201d means they do not.\n\nThe following are _categorical_ features:\n\n- Division Name\n- Department Name\n- Class Name\n- Clothing ID\n\nAnd the following are _numerical_ features:\n\n- Age\n- Rating\n- Positive Feedback Count\n\n_Feature Analysis_\n\nThere is an excellent Notebook on Kaggle [here](https://www.kaggle.com/bhaveshkumar2806/complete-eda-and-visualization-of-text-data) which does some thorough analysis on each of the features in this dataset.\n\nNote that, in addition to the \u201cRecommended\u201d label, there is also a \u201c **Rating**\u201d column where the reviewer rates the product from 1 - 5. The analysis in the above Notebook shows that almost all items rated 3 - 5 are recommended, and almost all rated 1 - 2 are not recommended. We\u2019ll see in our second baseline classifier that you can get a very high accuracy with this feature alone. However, it _is_ still possible to do better by incorporating the other features!\n\n## 1.2. Train-Validation-Test Split\n\nI want to use the same training, validation, and test splits for all of the approaches we try so that it\u2019s a fair comparison.\n\nHowever, different approaches are going to require different transformations on the data, and for simplicity I want to apply those transformations _before_ splitting the dataset.\n\nTo solve this, we\u2019re going to create lists of indeces for each of the three portions. That way, for a given classification approach, we can load the whole dataset, apply our transformations, and then split it according to these pre-determined indeces.\n\n```\nimport random\nimport numpy as np\n\n# First, calculate the split sizes. 80% training, 10% validation, 10% test.\ntrain_size = int(0.8 * len(data_df))\nval_size = int(0.1 * len(data_df))\ntest_size = len(data_df) - (train_size + val_size)\n\n# Sanity check the sizes.\nassert((train_size + val_size + test_size) == len(data_df))\n\n# Create a list of indeces for all of the samples in the dataset.\nindeces = np.arange(0, len(data_df))\n\n# Shuffle the indeces randomly.\nrandom.shuffle(indeces)\n\n# Get a list of indeces for each of the splits.\ntrain_idx = indeces[0:train_size]\nval_idx = indeces[train_size:(train_size + val_size)]\ntest_idx = indeces[(train_size + val_size):]\n\n# Sanity check\nassert(len(train_idx) == train_size)\nassert(len(test_idx) == test_size)\n\n# With these lists, we can now select the corresponding dataframe rows using,\n# e.g., train_df = data_df.iloc[train_idx]\n\nprint('  Training size: {:,}'.format(train_size))\n...",
      "url": "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert"
    },
    {
      "title": "A Reference Guide to Feature Engineering Methods | Kaggle",
      "text": "A Reference Guide to Feature Engineering Methods | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a6aef772a946d7d4:1:10006)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods"
    },
    {
      "title": "Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas",
      "text": "Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# Grandmaster Pro Tip: Winning First Place in Kaggle Competition with Feature Engineering Using cuDF pandas\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/04/NVIDIA-feature-engineering-kaggle-blog-1024x576.jpg)\nApr 17, 2025\nBy[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [T](https://twitter.com/intent/tweet?text=Grandmaster+Pro+Tip:+Winning+First+Place+in+Kaggle+Competition+with+Feature+Engineering+Using+cuDF+pandas+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/&amp;title=Grandmaster+Pro+Tip:+Winning+First+Place+in+Kaggle+Competition+with+Feature+Engineering+Using+cuDF+pandas+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/>)\nAI-Generated Summary\nLike\nDislike\n* Using NVIDIA cuDF-pandas to accelerate pandas operations on GPUs allowed for the rapid generation and testing of over 10,000 engineered features for a Kaggle competition, significantly boosting the accuracy of an XGBoost model.\n* The most effective feature engineering techniques included groupby aggregations, such as computing statistics (e.g., mean, std, count) and quantiles, as well as creating new columns from NaNs and binning numerical columns.\n* Techniques like extracting digits from float32 values and combining categorical columns also proved useful, and leveraging the original dataset that the synthetic data was created from provided additional predictive power.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nFeature engineering remains one of the most effective ways to improve model accuracy when working with tabular data. Unlike domains such as NLP and computer vision, where neural networks can extract rich patterns from raw inputs, the best-performing tabular models\u2014particularly gradient-boosted decision trees\u2014still gain a significant advantage from well-crafted features. However, the sheer potential number of useful features means that exploring them thoroughly is often computationally prohibitive. Trying to generate and validate hundreds or thousands of feature ideas using standard pandas on a CPU is simply too slow to be practical.\nThis is where GPU acceleration changes the game. Using NVIDIA cuDF-pandas, which accelerates pandas operations on GPUs with zero code changes, allowed me to rapidly generate and test over 10,000 engineered features for Kaggle\u2019s February playground competition. This accelerated discovery process was the key differentiator. In a drastically reduced timeframe &#8211; days instead of potential months &#8211;\u00a0the best 500 discovered features significantly boosted the accuracy of my XGBoost model, securing 1st place in the competition predicting backpack prices. Below, I share the core feature engineering techniques, accelerated by cuDF-pandas, that led to this result.\n## Groupby(COL1)[COL2].agg(STAT)[**](#groupbycol1col2aggstat)\nThe most powerful feature engineering technique is groupby aggregations. Namely, we execute the code`groupby(COL1)[COL2].agg(STAT)`. This is where we group by`COL1`column and aggregate (i.e. compute) a statistic`STAT`over another column`COL2`. We use the speed of NVIDIA cuDF-Pandas to explore thousands of`COL1`,`COL2`,`STAT`combinations. We try statistics (`STAT`) like &#8220;mean&#8221;, &#8220;std&#8221;, &#8220;count&#8221;, &#8220;min&#8221;, &#8220;max&#8221;, &#8220;nunique&#8221;, &#8220;skew&#8221; etc etc. We choose`COL1`and`COL2`from our tabular data\u2019s existing columns. When`COL2`is the target column, then we use nested cross-validation to avoid leakage in our validation computation. When`COL2`is the target, this operation is called Target Encoding.\n## Groupby(COL1)[&#8216;Price&#8217;].agg(HISTOGRAM BINS)[**](#groupbycol1&#8216;price&#8217;agghistogram_bins)\nWhen we`groupby(COL1)[COL2]`we have a distribution (set) of numbers for each group. Instead of computing a single statistic (and making one new column), we can compute any collection of numbers that describe this distribution of numbers and make many new columns together.\nBelow we display a histogram for the group`Weight Capacity = 21.067673`. We can count the number of elements in each (equally spaced) bucket and create a new engineered feature for each bucket count to return to the groupby operation! Below we display seven buckets, but we can treat the number of buckets as a hyperparameter.\n```\nresult = X\\_train2.groupby(&quot;&quot;WC&quot;&quot;)&#x5B;&#x5B;&quot;&quot;Price&quot;&quot;].apply(make\\_histogram)\nX\\_valid2 = X\\_valid2.merge(result, on=&quot;&quot;WC&quot;&quot;, how=&quot;&quot;left&quot;&quot;)\n```\n*Figure 1. Histogram of price values when weight capacity equals 21.067673*\n## Groupby(COL1)[&#8216;Price&#8217;].agg(QUANTILES)[**](#groupbycol1&#8216;price&#8217;aggquantiles)\nWe can groupby and compute the quantiles for`QUANTILES = [5,10,40,45,55,60,90,95]`and return the eight values to create eight new columns.\n```\nfor k in QUANTILES:\nresult = X\\_train2.groupby(&#039;&#039;Weight Capacity (kg)&#039;&#039;).\\\\\nagg({&#039;&#039;Price&#039;&#039;: lambda x: x.quantile(k/100)})\n```\n## All NANs as Single Base-2 Column[**](#all_nans_as_single_base-2_column)\nWe can create a new column from all the NANs over multiple columns. This is a powerful column which we can subsequently use for groupby aggregations or combinations with other columns.\n```\ntrain&#x5B;&#x5B;&quot;&quot;NaNs&quot;&quot;] = np.float32(0)\nfor i,c in enumerate(CATS):\ntrain&#x5B;&#x5B;&quot;&quot;NaNs&quot;&quot;] += train&#x5B;&#x5B;c].isna()\\*2\\*\\*i\n```\n## Put Numerical Column into Bins[**](#put_numerical_column_into_bins)\nThe most powerful (predictive) column in this competition is Weight Capacity. We can create more powerful columns based on this column by binning this column with rounding.\n```\nfor k in range(7,10):\nn = f&quot;round{k}&quot;\ntrain&#x5B;n] = train&#x5B;&quot;Weight Capacity (kg)&quot;].round(k)\n```\n## Extract Float32 as Digits[**](#extract_float32_as_digits)\nThe most powerful (predictive) column in this competition is Weight Capacity. We can create more powerful columns based on this column by extracting digits. This technique seems weird, but it is often used to extract info from a product ID where individual digits within a product ID convey info about a product such as brand, color, etc.\n```\nfor k in range(1,10):\ntrain&#x5B;&#x5B;f&#039;&#039;digit{k}&#039;&#039;] = ((train&#x5B;&#x5B;&#039;&#039;Weight Capacity (kg)&#039;&#039;] \\* 10\\*\\*k) % 10).fillna(-1).astype(&quot;&quot;int8&quot;&quot;)\n```\n## Combination of Categorical Columns[**](#combination_of_...",
      "url": "https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas"
    },
    {
      "title": "Feature Engineering with Kaggle Tutorial",
      "text": "[![Data and AI Literacy 2024 ](https://images.datacamp.com/Marketing/Oneoffs/data-literacy-2024/lockup-banner-data-ai-literacy.png)\\\n\\\n**The State of Data & AI Literacy 2024** \\\n\\\nDownload Now](https://www.datacamp.com/report/data-ai-literacy-report-2024)\n\n[Skip to main content](https://www.datacamp.com/tutorial/feature-engineering-kaggle#main)\n\nIn the two previous Kaggle tutorials, you learned all about how to get your data in a form to build your first machine learning model, using [Exploratory Data Analysis and baseline machine learning models](https://www.datacamp.com/tutorial/kaggle-machine-learning-eda). Next, you successfully managed to [build your first machine learning model](https://www.datacamp.com/tutorial/kaggle-tutorial-machine-learning), a decision tree classifier. You submitted all these models to Kaggle and interpreted their accuracy.\n\nIn this third tutorial, you'll learn more about feature engineering, a process where you use domain knowledge of your data to create additional relevant features that increase the predictive power of the learning algorithm and make your machine learning models perform even better!\n\nMore specifically,\n\n- You'll first [get started](https://www.datacamp.com/tutorial/feature-engineering-kaggle#getting-started!) by doing all necessary imports and getting the data in your workspace;\n- Then, you'll see some reasons why you should do [feature engineering](https://www.datacamp.com/tutorial/feature-engineering-kaggle#why-feature-engineer-at-all?) and start working on engineering your own new features for your data set! You'll create new columns, transform variables into numerical ones, handle missing values, and much more.\n- Lastly, you'll [build a new machine learning model](https://www.datacamp.com/tutorial/feature-engineering-kaggle#building-models-with-your-new-data-set!) with your new data set and submit it to Kaggle.\n\n## Getting Started!\n\nBefore you can start off, you're going to do all the imports, just like you did in the previous tutorial, use some IPython magic to make sure the figures are generated inline in the Jupyter Notebook and set the visualization style. Next, you can import your data and make sure that you store the target variable of the training data in a safe place. Afterwards, you merge the train and test data sets (with exception of the `'Survived'` column of `df_train`) and store the result in `data`.\n\n**Remember** that you do this because you want to make sure that any preprocessing that you do on the data is reflected in both the train and test sets!\n\nLastly, you use the `.info()` method to take a look at your data:\n\n```\n\n# Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\n\n# Figures inline and set visualization style\n%matplotlib inline\nsns.set()\n\n# Import data\ndf_train = pd.read_csv('data/train.csv')\ndf_test = pd.read_csv('data/test.csv')\n\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n# View head\ndata.info()\n\n OpenAI\nWas this helpful? Yes No\n```\n\n```\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1309 entries, 0 to 417\nData columns (total 11 columns):\nPassengerId    1309 non-null int64\nPclass         1309 non-null int64\nName           1309 non-null object\nSex            1309 non-null object\nAge            1046 non-null float64\nSibSp          1309 non-null int64\nParch          1309 non-null int64\nTicket         1309 non-null object\nFare           1308 non-null float64\nCabin          295 non-null object\nEmbarked       1307 non-null object\ndtypes: float64(2), int64(4), object(5)\nmemory usage: 122.7+ KB\n\n OpenAI\nWas this helpful? Yes No\n```\n\n## Why Feature Engineer at all?\n\nYou perform feature engineering to extract more information from your data, so that you can up your game when building models.\n\n### Titanic's Passenger Titles\n\nLet's check out what this is all about by looking at an example. Let's check out the `'Name'` column with the help of the `.tail()` method, which helps you to see the last five rows of your data:\n\n```\n\n# View head of 'Name' column\ndata.Name.tail()\n\n OpenAI\nWas this helpful? Yes No\n```\n\n```\n\n413              Spector, Mr. Woolf\n414    Oliva y Ocana, Dona. Fermina\n415    Saether, Mr. Simon Sivertsen\n416             Ware, Mr. Frederick\n417        Peter, Master. Michael J\nName: Name, dtype: object\n\n OpenAI\nWas this helpful? Yes No\n```\n\nSuddenly, you see different titles emerging! In other words, this column contains strings or text that contain titles, such as 'Mr', 'Master' and 'Dona'.\n\nThese titles of course give you information on social status, profession, etc., which in the end could tell you something more about survival.\n\nAt first sight, it might seem like a difficult task to separate the names from the titles, but don't panic! Remember, you can easily use regular expressions to extract the title and store it in a new column `'Title'`:\n\n```\n\n# Extract Title from Name, store in column and plot barplot\ndata['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\nsns.countplot(x='Title', data=data);\nplt.xticks(rotation=45);\n\n OpenAI\nWas this helpful? Yes No\n```\n\n![bar chart](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1515151833/output_5_0_d77zb9.png)\n\n**Note** that this new column `'Title'` is actually a new feature for your data set!\n\n**Tip**: to learn more about regular expressions, check out my write up of our last [FB Live code along event](https://www.datacamp.com/tutorial/web-scraping-python-nlp) or check out DataCamp's [Python Regular Expressions Tutorial](https://www.datacamp.com/tutorial/python-regular-expression-tutorial).\n\nYou can see that there are several titles in the above plot and there are many that don't occur so often. So, it makes sense to put them in fewer buckets.\n\nFor example, you probably want to replace `'Mlle'` and `'Ms'` with `'Miss'` and `'Mme'` by `'Mrs'`, as these are French titles and ideally, you want all your data to be in one language. Next, you also take a bunch of titles that you can't immediately categorize and put them in a bucket called `'Special'`.\n\n**Tip**: play around with this to see how your algorithm performs as a function of it!\n\nNext, you view a barplot of the result with the help of the `.countplot()` method:\n\n```\n\ndata['Title'] = data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\ndata['Title'] = data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\nsns.countplot(x='Title', data=data);\nplt.xticks(rotation=45);\n\n OpenAI\nWas this helpful? Yes No\n```\n\n![bar chart](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1515151833/output_7_0_vdtvcz.png)\n\nThis is what your newly engineered feature `'Title'` looks like!\n\nNow, make sure that you have a `'Title'` column and check out your data again with the `.tail()` method:\n\n```\n\n# View head of data\ndata.tail()\n\n OpenAI\nWas this helpful? Yes No\n```\n\n|  | PassengerId | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked | Title |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 413 | 1305 | 3 | Spector, Mr. Woolf | male | NaN | 0 | 0 | A.5. 3236 | 8.0500 | NaN | S | Mr |\n| 414 | 1306 | 1 | Oliva y Ocana, Dona. Fermina | female | 39.0 | 0 | 0 | PC 17758 | 108.9000 | C105 | C | Special |\n| 415 | 1307 | 3 | Saether, Mr. Simon Sivertsen | male | 38.5 | 0 | 0 | SOTON/O.Q. 3101262 | 7.2500 | NaN | S | Mr |\n| 416 | 1308 | 3 | Ware, Mr. Frederick | male | NaN | 0 | 0 | 359309 | 8.0500 | NaN | S | Mr |\n| 417 | 1309 | 3 | Peter, Master. Michael J | male | NaN | 1 | 1 | 2668 | 22.3583 | NaN | C | Master |\n\n### Passenger's Cabins\n\nWhen you loaded in the ...",
      "url": "https://www.datacamp.com/tutorial/feature-engineering-kaggle"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "GitHub - bangdasun/kaggle\\_learn: Functions used in kaggle competitions: data preprocessing/feature engineering/model training etc. Many of these functions are collected from kaggle community, credits are belong to the authors :)\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/bangdasun/kaggle_learn)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/bangdasun/kaggle_learn)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=bangdasun/kaggle_learn)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[bangdasun](https://github.com/bangdasun)/**[kaggle\\_learn](https://github.com/bangdasun/kaggle_learn)**Public\n* [Notifications](https://github.com/login?return_to=/bangdasun/kaggle_learn)You must be signed in to change notification settings\n* [Fork15](https://github.com/login?return_to=/bangdasun/kaggle_learn)\n* [Star11](https://github.com/login?return_to=/bangdasun/kaggle_learn)\nFunctions used in kaggle competitions: data preprocessing/feature engineering/model training etc. Many of these functions are collected from kaggle community, credits are belong to the authors :)\n### License\n[MIT license](https://github.com/bangdasun/kaggle_learn/blob/develop/LICENSE)\n[11stars](https://github.com/bangdasun/kaggle_learn/stargazers)[15forks](https://github.com/bangdasun/kaggle_learn/forks)[Branches](https://github.com/bangdasun/kaggle_learn/branches)[Tags](https://github.com/bangdasun/kaggle_learn/tags)[Activity](https://github.com/bangdasun/kaggle_learn/activity)\n[Star](https://github.com/login?return_to=/bangdasun/kaggle_learn)\n[Notifications](https://github.com/login?return_to=/bangdasun/kaggle_learn)You must be signed in to change notification settings\n# bangdasun/kaggle\\_learn\ndevelop\n[Branches](https://github.com/bangdasun/kaggle_learn/branches)[Tags](https://github.com/bangdasun/kaggle_learn/tags)\n[](https://github.com/bangdasun/kaggle_learn/branches)[](https://github.com/bangdasun/kaggle_learn/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[66 Commits](https://github.com/bangdasun/kaggle_learn/commits/develop/)\n[](https://github.com/bangdasun/kaggle_learn/commits/develop/)\n|\n[feature\\_engineering](https://github.com/bangdasun/kaggle_learn/tree/develop/feature_engineering)\n|\n[feature\\_engineering](https://github.com/bangdasun/kaggle_learn/tree/develop/feature_engineering)\n|\n|\n|\n[feature\\_importance](https://github.com/bangdasun/kaggle_learn/tree/develop/feature_importance)\n|\n[feature\\_importance](https://github.com/bangdasun/kaggle_learn/tree/develop/feature_importance)\n|\n|\n|\n[metrics](https://github.com/bangdasun/kaggle_learn/tree/develop/metrics)\n|\n[metrics](https://github.com/bangdasun/kaggle_learn/tree/develop/metrics)\n|\n|\n|\n[model](https://github.com/bangdasun/kaggle_learn/tree/develop/model)\n|\n[model](https://github.com/bangdasun/kaggle_learn/tree/develop/model)\n|\n|\n|\n[model\\_runner](https://github.com/bangdasun/kaggle_learn/tree/develop/model_runner)\n|\n[model\\_runner](https://github.com/bangdasun/kaggle_learn/tree/develop/model_runner)\n|\n|\n|\n[preprocessing](https://github.com/bangdasun/kaggle_learn/tree/develop/preprocessing)\n|\n[preprocessing](https://github.com/bangdasun/kaggle_learn/tree/develop/preprocessing)\n|\n|\n|\n[LICENSE](https://github.com/bangdasun/kaggle_learn/blob/develop/LICENSE)\n|\n[LICENSE](https://github.com/bangdasun/kaggle_learn/blob/develop/LICENSE)\n|\n|\n|\n[README.md](https://github.com/bangdasun/kaggle_learn/blob/develop/README.md)\n|\n[README.md](https://github.com/bangdasun/kaggle_learn/blob/develop/README.md)\n|\n|\n|\n[\\_\\_init\\_\\_.py](https://github.com/bangdasun/kaggle_learn/blob/develop/__init__.py)\n|\n[\\_\\_init\\_\\_.py](https://github.com/bangdasun/kaggle_learn/blob/develop/__init__.py)\n|\n|\n|\n[setup.py](https://github.com/bangdasun/kaggle_learn/blob/develop/setup.py)\n|\n[setup.py](https://github.com/bangdasun/kaggle_learn/blob/develop/setup.py)\n|\n|\n|\n[standard\\_env.yaml](https://github.com/bangdasun/kaggle_learn/blob/develop/standard_env.yaml)\n|\n[standard\\_env.yaml](https://github.com/bangdasun/kaggle_learn/blob/develop/standard_env.yaml)\n|\n|\n|\n[utils.py](https://github.com/bangdasun/kaggle_learn/blob/develop/utils.py)\n|\n[utils.py](https://github.com/bangdasun/kaggle_learn/blob/develop/utils.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n## kaggle\\_learn\n[](#kaggle_learn)\nThis module is a collection for useful code in kaggle (or more data science general), includes general utility functions, feature engineering functions, model training / cross-validation / model stacking helper functions.\n### Modules\n[](#modules)\n#### Preprocessing\n[](#preprocessing)\nNumerical / categorical (tabular data) and text data preprocessing.\n#### Feature Engineering\n[](#feature-engineering)\nSome commonly used feature engineering methods used in kaggle competitions:\n* group by on one / multiple categorical features and get the summary statistics of numerical features\n* text meta features\n* text similarity features\n* time meta features\n* ...\n#### Models\n[](#models)\nModels not implemented in`sklearn`:\n* NB-LR (Logistic Regression with Naive Bayes features)\nNeural network templates for different tasks:\n* text classification (RNN / CNN based)\n#### Model Runner\n[](#model-runner)\nFunctions / classes that takes data and run specified models (with cross validation).\n#### Feature Importance\n[](#feature-importance)\n* Permutation feature importance\n#### Metrics\n[](#metrics)\n* customized`keras`callbacks\n#### Utilities\n[](#utilities)\nUtility functions:\n* reduce`pandas`dataframe memory\n* timer / logger\n* ...\n### Usage\n[](#usage)\nClone this repository:\n```\n`git clone https://github.com/bangdasun/kaggle\\_learn.git`\n```\nCreate environment to install required packages:\n```\n`cd kaggle\\_learn\nconda env create -f standard\\_env.yaml`\n```\n## About\nFunctions used in kaggle competitions: data preprocessing/feature engineering/model training etc. Many of these functions are collected from kaggle community, credits are belong to the authors :)\n### Resources\n[Readme](#readme-ov-file)\n### License\n[MIT license](#MIT-1-ov-file)\n### Uh oh!\nThere was an error while loading.[Please reload this page]().\n[Activity](https://github.com/bangdasun/kaggle_learn/activity)\n### Stars\n[**11**stars](https://github.com/bangdasun/kaggle_learn/stargazers)\n### Watchers\n[**0**watching](https://github.com/bangdasun/kaggle_learn/watchers)\n### Forks\n[**15**forks](https://github.com/bangdasun/kaggle_learn/forks)\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/bangdasun/kaggle_learn&amp;report=bangdasun+(user))\n## [Releases](https://github.com/bangdasun/kaggle_learn/releases)\nNo releases published\n## [Packages0](https://github.com/users/bangdasun/packages?repo_name=kaggle_learn)\nNo packages...",
      "url": "https://github.com/bangdasun/kaggle_learn"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Home Depot Product Search Relevance, Winners\u2019 Interview: 2nd Place | Thomas, Sean, Qingchen, & Nima",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F68068f9f9ffd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fkaggle-blog%2Fhome-depot-product-search-relevance-winners-interview-2nd-place-thomas-sean-qingchen-nima-68068f9f9ffd&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fkaggle-blog%2Fhome-depot-product-search-relevance-winners-interview-2nd-place-thomas-sean-qingchen-nima-68068f9f9ffd&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[**Kaggle Blog**](https://medium.com/kaggle-blog?source=post_page---publication_nav-4b0982ce16a3-68068f9f9ffd---------------------------------------)\n\n\u00b7\n\n[![Kaggle Blog](https://miro.medium.com/v2/resize:fill:76:76/1*9izrRVNdAJa9bFaqBwSH4w.png)](https://medium.com/kaggle-blog?source=post_page---post_publication_sidebar-4b0982ce16a3-68068f9f9ffd---------------------------------------)\n\nOfficial Kaggle Blog!\n\n# Home Depot Product Search Relevance, Winners\u2019 Interview: 2nd Place \\| Thomas, Sean, Qingchen, & Nima\n\n[![Kaggle Team](https://miro.medium.com/v2/resize:fill:64:64/1*XK0PjUK1iXscB6_0QGvXmQ.png)](https://medium.com/@kaggleteam?source=post_page---byline--68068f9f9ffd---------------------------------------)\n\n[Kaggle Team](https://medium.com/@kaggleteam?source=post_page---byline--68068f9f9ffd---------------------------------------)\n\nFollow\n\n5 min read\n\n\u00b7\n\nJun 15, 2016\n\n--\n\nListen\n\nShare\n\nThe [Home Depot Product Search Relevance](https://web.archive.org/web/20161028171539/https://www.kaggle.com/c/home-depot-product-search-relevance) competition challenged Kagglers to predict the relevance of product search results. Over 2000 teams with 2553 players flexed their natural language processing skills in attempts to feature engineer a path to the top of the leaderboard. In this interview, the second place winners, Thomas (Justfor), Sean (sjv), Qingchen, and Nima, describe their approach and how diversity in features brought incremental improvements to their solution.\n\n# The basics\n\n# What was your background prior to entering this challenge?\n\n**Thomas** is a pharmacist, with his PhD in Informatics and Pharmaceutical Analytics and works in Quality in the pharmaceutical industry. At Kaggle he joined earlier competitions and got the Script of the Week award.\n\n**Sean** is an undergraduate student in computer science and mathematics at the Massachusetts Institute of Technology (MIT).\n\n**Qingchen** is a data scientist at ORTEC Consulting and a PhD researcher at the Amsterdam Business School. He has experience competing on Kaggle but this was the first time with a competition related to natural language processing.\n\n**Nima** is a PhD candidate at the Lassonde School of Engineering at York University focusing on research in data mining and machine learning. He has also experience competing on Kaggle but up to now focused on other types of competitions.\n\nBetween the four of us, we have quite a bit of experience with Kaggle competitions and machine learning, but minor experience in natural language processing.\n\n# What made you decide to enter this competition?\n\nFor all of us, the primary reason was that we wanted to learn more about natural language processing (NLP) and information retrieval (IR). This competition turned out to be great for that, especially in providing practical experience.\n\n# Do you have any prior experience or domain knowledge that helped you succeed in this competition?\n\nAll of us have strong theoretical experience with machine learning in general, and it naturally helps with the understanding and implementation of NLP and IR methods. However, none of us have had any real experience in this domain.\n\n# Let\u2019s get technical\n\n# What preprocessing and supervised learning methods did you use?\n\nThe key to this competition was mostly preprocessing and feature engineering as the primary data is text. Our processed text features can broadly be grouped into a few categories: categorical features, counting features, co-occurrence features, semantic features, and statistical features.\n\n- Categorical features: Put words in categories such as colors, units, brands, core. Count the number of those words in the query/title and count number of intersection between query and title for each category.\n- Counting features: Length of query, number of common grams between query and title, Jacquard similarity, etc.\n- Co-occurrence features: Measures of how frequently words appear together. e.g., Latent Semantic Analysis (LSA).\n- Semantic features: Measure how similar the meaning of two words is.\n- Statistical features: Compare queries with unknown score to queries with known relevance score.\n\nIt seems that a lot of the top teams had similar types of features, but the implementation details are probably different. For our ensemble we used different variations of xgboost along with a ridge regression model.\n\nFor models and ensemble we started with random forest, extra trees and gbm-models. Furthermore xgboost and ridge were in our focus. Shortly prior to the end of the competition we found out, that first random forest and then extra trees did not help our ensembles anymore. So we focused on xgboost, gbm and Ridge.\n\nOur best single model was a xgboost-model and scored 0.43347 on the public LB. The final ensemble consists of 19 models based on xgboost, gbm and Ridge. The xgboost-models were made with different parameters including binarizing the target, objective reg:linear, and objective count:poisson. We found, that the Ridge Regression helped in nearly every case, so we included it in the final ensemble.\n\n# Were you surprised by any of your findings?\n\nA surprising finding was the large number of features which had predictive ability. In particular, when we teamed up, it was better to combine our features than to ensemble our results. This is quite unique as most of the time new features are more likely to cause overfit but not in this case. As a result, adding more members to the team was highly likely to improve score which is why the top-10 were all teams of at least 3 people.\n\n# Which tools did you use?\n\nWe used mainly Python 3 and Python 2. The decision for Python 2 is interesting as some of the used libraries are still not available for Python 3. In our processing chain we used the Python standard tools for machine learning (scikit-learn, nltk, pandas, numpy, scipy, xgboost, gensim). Nima used R for feature generation.\n\n# How did you spend your time on this competition?\n\nAfter teaming up, Sean and Nima spent most of their time on feature engineering and Thomas and Qingchen spent most of their time on model tuning.\n\n# What was the run time for both training and prediction of your winning solution?\n\nIn general, training/prediction time is very fast (minutes), but we used some xgboost parameters that took much longer to train (hours) for small performance gains. Text processing and feature engineering took a very long time (easily over 8 hours for a single feature set).\n\n# Words of wisdom\n\n# What have you taken away from this competition?\n\nFirst of all quite a lot of Kaggle ranking points and Thomas got his Master badge! Overall this was a very difficult competition and we learned a lot about natural language processing and information retrieval in practice. It now makes sense why Google is able to use such a large number of features in their search algorithm as m...",
      "url": "https://medium.com/kaggle-blog/home-depot-product-search-relevance-winners-interview-2nd-place-thomas-sean-qingchen-nima-68068f9f9ffd"
    }
  ]
}