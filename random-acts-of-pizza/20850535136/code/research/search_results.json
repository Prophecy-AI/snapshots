{
  "query": "What hyperparameter optimization techniques work best for text classification models in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "As of\u202fJan\u202f9\u202f2026, Bayesian\u2011based optimizers such as Optuna (or Hyperopt) paired with early\u2011stopping and learning\u2011rate scheduling, and scalable parallel search tools like Ray\u202fTune, are reported to give the best results for text\u2011classification models in Kaggle competitions, outperforming manual grid or random search\u202f([datascience.stackexchange.com](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain),\u202f[tw\u200bdatascience.com](https://towardsdatascience.com/the-kaggle-way-to-tune-hyperparameters-with-optuna-285e59a0b95a),\u202f[huggingface.co](https://huggingface.co/blog/ray-tune)).",
      "url": ""
    },
    {
      "title": "What methods do top Kagglers employ for score gain?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [What methods do top Kagglers employ for score gain?](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked1 year, 8 months ago\n\nModified [1 year, 8 months ago](https://datascience.stackexchange.com/datascience.stackexchange.com?lastactivity)\n\nViewed\n424 times\n\n2\n\n$\\\\begingroup$\n\nI\u2019m currently taking a course on ML and part of my final grade is my position on a Kaggle competition (private one) regarding a classification task. The majority of groups tend to have a similar public score but there are some that spike through the rest. I am left wondering what techniques data scientists employ to increase their scores.\n\nI am aware that a thoughtful pre-processing of the data is of chief importance as well as techniques such as grid search which help us find the best hyperparameters. This question is more about some out-of-the-box techniques that you employ when your aim is to increase your final score, for example, in the setting of a Kaggle competition.\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)\n\n[Share](https://datascience.stackexchange.com/q/124709)\n\n[Improve this question](https://datascience.stackexchange.com/posts/124709/edit)\n\nFollow\n\n[edited Nov 22, 2023 at 23:36](https://datascience.stackexchange.com/posts/124709/revisions)\n\n[Multivac](https://datascience.stackexchange.com/users/92050/multivac)\n\n3,25922 gold badges1010 silver badges2626 bronze badges\n\nasked Nov 22, 2023 at 19:32\n\n[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela) Frederico Portela\n\n4122 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n4\n\n$\\\\begingroup$\n\nTop Kagglers often employ a combination of traditional machine learning techniques and creative, out-of-the-box strategies to gain an edge in competitions. Here are some techniques:\n\n**1\\. Ensemble Methods**\n\n- _Stacking/Blending:_ Combining predictions from multiple models. This involves training several models and then using another model to predict the target variable based on the predictions of the initial models.\n\n- _Bagging and Boosting:_ Techniques like Random Forest (bagging) and Gradient Boosting (boosting) can significantly improve performance.\n\n\n**2\\. Feature Engineering**\n\n- _Creating new features:_ Deriving meaningful features from existing ones can provide valuable information to the model.\n\n- _Dimensionality Reduction:_ Techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) can be useful.\n\n\n**3\\. Advanced Preprocessing**\n\n- _Handling missing data:_ Creative ways to impute missing values.\n\n- _Outlier Detection and Treatment:_ Identifying and addressing outliers can improve model robustness.\n\n\n**4\\. Model Hyperparameter Tuning**\n\n- _Bayesian Optimization:_ Efficiently searching hyperparameter space.\n\n- _Optuna, Hyperopt:_ Libraries for hyperparameter optimization.\n\n\n**5\\. Neural Architecture Search (NAS)**\n\n- _Automated Model Design:_ Techniques to automatically search for the best neural network architecture.\n\n**6\\. Transfer Learning**\n\n- _Using pre-trained models:_ Leveraging models trained on large datasets and fine-tuning them for the specific task at hand.\n\n**7\\. Data Augmentation**\n\n- _Increasing Training Data:_ Applying transformations (rotations, flips, etc.) to artificially increase the size of the training dataset.\n\n**8\\. Domain-Specific Knowledge**\n\n- _Understanding the problem domain:_ Incorporating domain-specific knowledge can lead to better feature engineering and model performance.\n\n**9\\. Advanced Modeling Techniques**\n\n- _Neural Networks Architectures:_ Exploring different architectures, such as attention mechanisms, transformers, etc.\n- _XGBoost, LightGBM, CatBoost:_ Gradient boosting libraries that are often used for tabular data.\n\n**10\\. Time Series Techniques**\n\n- _LSTM, GRU:_ For sequential data.\n\n- _Feature lagging and rolling statistics:_ Utilizing information from past time points.\n\n\n**11\\. Post-Processing**\n\n- _Calibration:_ Adjusting predicted probabilities to improve the model's reliability.\n- _Threshold tuning:_ Adjusting the classification threshold based on the specific needs of the task.\n\n**12\\. Collaboration and Knowledge Sharing**\n\n- _Participating in discussions:_ Sharing insights and learning from others on Kaggle forums.\n- _Team Collaboration:_ Forming teams to combine diverse skills and perspectives.\n\n[Share](https://datascience.stackexchange.com/a/124714)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/124714/edit)\n\nFollow\n\nanswered Nov 22, 2023 at 23:43\n\n[lvvittor](https://datascience.stackexchange.com/users/156336/lvvittor) lvvittor\n\n10111 bronze badge\n\n$\\\\endgroup$\n\n1\n\n- 1\n\n$\\\\begingroup$Thank you very much. There are indeed here some techniques i was not aware of.$\\\\endgroup$\n\n\u2013\u00a0[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela)\n\nCommentedNov 23, 2023 at 16:55\n\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n4\n\n$\\\\begingroup$\n\nThere are many methods that can be employed to increase your score on a Kaggle competition. Here are a few examples:\n\n**Advanced classification techniques:** Using advanced classification techniques such as weighted average ensemble, stacked generalization ensemble, and power average ensemble can help improve your model's performance.\n\n**Data augmentation:** Techniques such as random rotation, hue adjustments, saturation adjustments, contrast adjustments, brightness adjustments, cropping, and more can help improve your model's accuracy when dealing with image data.\n\n**Text augmentation:** When dealing with text data, techniques such as exchanging words with synonyms, noising in RNN, and translating to other languages and back can help augment your training data and improve your model's performance.\n\n**External datasets:** Using external datasets that contain variables that influence the predicate variable can help increase the performance of your model pre-processing.\n\nI have even seen that in some cases, especially at the beginning of the site, teams took advantage of finding data leakage. For example, identifying customer's id that may potentially be linked to temporal patterns and thus model's prediction\n\nThis [discussion](https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111308) may give you a fine summary of most of the techniques I mentioned above.\n\nI hope it helps.\n\n[Share](https://datascience.stackexchange.com/a/124715)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/124715/edit)\n\nFollow\n\nanswered Nov 22, 2023 at 23:44\n\n[Multivac](https://datascience.stackexchange.com/users/92050/multivac) Multivac\n\n3,25922 gold badges1010 silver badges2626 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Thank you for this. I'll take a look at the discussion.$\\\\endgroup$\n\n\u2013\u00a0[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela)\n\nCommentedNov 23, 2023 at 16:55\n\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/logi...",
      "url": "https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain"
    },
    {
      "title": "The Kaggle Way to Tune Hyperparameters with Optuna",
      "text": "The Kaggle Way to Tune Hyperparameters with Optuna | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)\n# The Kaggle Way to Tune Hyperparameters with Optuna\nEasily and efficiently optimize your model&#039;s hyperparameters with Optuna with a mini project\n[Yash Prakash](https://towardsdatascience.com/author/ipom/)\nJan 6, 2022\n5 min read\nShare\n![Photo by Joshua Sortino on Unsplash](https://towardsdatascience.com/wp-content/uploads/2022/01/0AI-YmX3qqycS1Weo-scaled.jpg)Photo by[Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com?utm_source=medium&amp;utm_medium=referral)\nA week ago, I posted one of my most popular articles, which contained a[list of Data Science tools and libraries](https://towardsdatascience.com/26-github-repositories-to-inspire-your-next-data-science-project-3023c24f4c3c)you should definitely be aware about. Today, I&#8217;m going to discuss one such great tool from the list.\n[Optuna](https://github.com/optuna/optuna)is a lightweight and versatile tool to perform hyperparameter optimization for your ML algorithm in a convenient manner. With the latest version 3.0 release, I feel this is a good time to make you knowledgeable of the most prominent features of the library, and apply it on a real world mini project as well.\nBut first, let&#8217;s discuss the elephant in the room.\n## Why is Optuna so widely used?\nHere are some of the greatest reasons for the popularity of this tool. And rightfully so, I&#8217;ll say:\n* It is a platform independent library, meaning &#8211; you can use it with little to no changes with scikit-learn, Tensorflow/Keras, Pytorch, XGBoost and so on.\n* Optuna enables you to use easy Python loops and conditional statements in your hyperparameter optimization pipeline.\n* It also allows you to have concurrency in your code with little to no effort.\n* And lastly, the in-built support for inspecting the optimization histories with a variety of plotting functions is very helpful.### **Note:**\nI said**&quot;optimization histories&quot;:**Yes! That&#8217;s the fun part! No**`GridSearch`**implementation allows you retain all the history of all the trials that you conducted while searching for the hyperparameters. Optuna does, and such inference provide a great advantage.\nIn this tutorial, I am going to use Optuna with XGBoost on a mini project in order to get you through all the fundamentals.\nLet&#8217;s get started \ud83d\udc47## Setting up our project\nMaking a virtual environment and installing XGBoost is the first step:\n```\n`pipenv shell`\n```\n```\n`pipenv install xgboost\npipenv install scikit-learn`\n```\nand then import the libraries we&#8217;ll be using, like so:\n```\n`import xgboost as xgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt`\n```\nWe will be making use of a preprocessed California Housing Dataset for building a regressor model, one that I&#8217;ve already prepared (link to the code + dataset at the end). We will first train a model the normal way and calculate the RMSE, and then optimize our model the best we can with Optuna.\n## Training a simpleXGBoost model\nThe preprocessed dataset can be imported simply:\n```\n`df = pd.read\\_csv(&#039;&#039;preprocessed\\_housing.csv&#039;&#039;)`\n```\n![Image by Autho - rthe preprocessed dataset](https://miro.medium.com/1*FmcC5AXcDQ-QpF4gW_FfXg.png)Image by Autho &#8211; rthe preprocessed dataset\nFinally, we will go ahead and train a simple XGBoost model and calculate the validation RMSE:\nThe output is like so:\n```\n`RMSE: 53119.69904403545`\n```\nHaving done that, let&#8217;s go ahead and get to the main part of this article!\n## Making the Optuna objective function\nThis objective is a bit different from what you may recall from Machine learning theory.\nAn objective function in Optuna contains the following actions:\n* It accepts a**dictionary**of different hyperparameters we want to optimize\n* A model is**trained**within the function\n* **Predictions**on test/validation data are made from the trained model\n* The**accuracy**or**error**is finally**returned**from the function.\nIn our case, we want to**minimize**our RMSE score, so we return the RMSE which we will then use in a &quot;**Study**&quot; to minimize.\nLet&#8217;s create an objective function then!\nn\\_trials represent each trial that is performed by the objective function, with the parameters given by**`suggest\\_int`**,**`suggest\\_float`**, and so on.\nWe have the convenience of specifying the start, the end, and the step count, along with a**`log`**parameter which indicates whether our values represent logarithm bins or not. Learning rate hyperparameter uses**`log`**, as you can see.\nNow, let&#8217;s create a**study**.\n## Making a study and bringing it all together\nA**study**in Optuna is entire process of optimization based on an objective function.\nLet&#8217;s create one and start tuning our hyperparameters!\n```\n`# make a study`\n```\n```\n`study = optuna.create\\_study(direction=&quot;&quot;minimize&quot;&quot;)\nstudy.optimize(objective, n\\_trials=500)`\n```\nWe put &quot;minimize&quot; in the**`direction`**parameter because we want to use the objective function to minimize our RMSE error. If it was a classification task you&#8217;re doing, you can use**maximize**with the accuracy score you calculate!\nYou&#8217;ll see that it takes less than two minutes for 500 trials on Google Colab! Quite a feat, isn&#8217;t it?\nLet&#8217;s see the best scores and params and print them:\n```\n`print(f&quot;&quot;Optimized RMSE: {study.best\\_value:.4f}&quot;&quot;)`\n```\n```\n`print(&quot;&quot;Best params:&quot;&quot;)\nfor key, value in study.best\\_params.items():\nprint(f&quot;&quot;t{key}: {value}&quot;&quot;)`\n```\nAnd finally, let&#8217;s put them all together:\n```\n`Output:`\n```\n```\n`Optimized RMSE: 51774.6284\nBest params: n\\_estimators: 1800 learning\\_rate: 0.2982950878173913 max\\_depth: 11`\n```\nI&#8217;ll say this looks like a very good improvement!\n### Bonus experiment:\nFinally, try seeing how the optimization history looks like with this snippet:\n```\n`from optuna.visualization.matplotlib import plot\\_optimization\\_history`\n```\n```\n`plot\\_optimization\\_history(study)`\n```\nYour output should be something akin to this:\n![Image by Author - optimization history plot](https://miro.medium.com/1*7cFNcQPQirwkmtqROWjNEQ.png)Image by Author &#8211; optimization history plot\nThat&#8217;s it! Time to give yourself a pat on the back for making it all the way through!\n## A few parting words&#8230;\nThank you for reading.\nI hope this quick tutorial with the mini project was helpful in recognizing the sheer advantage using Optuna provides as compared to regular hyperparameter optimization by hand. I recommend you experiment with building fresh pipelines with it with this or another dataset as next steps after going through this tutorial.\nHappy learning! \ud83d\ude42You can find all the[code along with the preprocessed dataset here](https://github.com/yashprakash13/data-another-day/blob/main/README.md#a-data-scientists-life-hacks).\nI hope to see you again in my next article. Until then,[follow me to not miss my next post](https://ipom.medium.com/). I do write every week!\n### [I also recommend becoming a Medium member to never miss any of the Data Science articles I publish every week.](https://ipom.medium.com/membership/)Join here \ud83d\udc47[**> Join Medium with my referral link &#8211; Yash Prakash\n**](https://ipom.medium.com/membership/)\n**A couple other articles of mine you might be interested in:**\n> ...",
      "url": "https://towardsdatascience.com/the-kaggle-way-to-tune-hyperparameters-with-optuna-285e59a0b95a"
    },
    {
      "title": "Hyperparameter Search with Transformers and Ray Tune",
      "text": "Hyperparameter Search with Transformers and Ray Tune\n[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)\n[Back to Articles](https://huggingface.co/blog)\n# [](#hyperparameter-search-with-transformers-and-ray-tune)Hyperparameter Search with Transformers and Ray Tune\nPublished\nNovember 2, 2020\n[Update on GitHub](https://github.com/huggingface/blog/blob/main/ray-tune.md)\n[\nUpvote\n4\n](https://huggingface.co/login?next=/blog/ray-tune)\n* [![](https://huggingface.co/avatars/a92dea7d853bbabbf60b351c207b6875.svg)](https://huggingface.co/etomoscow)\n* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/645d63c0ce72244df7b36be8/09vhYAzgv1svwvQM4eIE9.jpeg)](https://huggingface.co/Moreza009)\n* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/x8e_7GDCWVKREXUJeV6uw.png)](https://huggingface.co/AmrataYadav)\n* [![](https://huggingface.co/avatars/4ef00e120c2f2db759bb6236dcba7d57.svg)](https://huggingface.co/Cavali)\n* [![system's avatar](https://cdn-avatars.huggingface.co/v1/production/uploads/1646125250765-61af81e7a7205fb7cc9ec0cb.png)](https://huggingface.co/system)\n[system\nsystem\nFollow\n](https://huggingface.co/system)\n##### [](#a-guest-blog-post-by-richard-liaw-from-the-anyscale-team)A guest blog post by Richard Liaw from the Anyscale team\nWith cutting edge research implementations, thousands of trained models easily accessible, the Hugging Face[transformers](https://github.com/huggingface/transformers)library has become critical to the success and growth of natural language processing today.\nFor any machine learning model to achieve good performance, users often need to implement some form of parameter tuning. Yet, nearly everyone ([1](https://medium.com/@prakashakshay90/fine-tuning-bert-model-using-pytorch-f34148d58a37),[2](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#advantages-of-fine-tuning)) either ends up disregarding hyperparameter tuning or opting to do a simplistic grid search with a small search space.\nHowever, simple experiments are able to show the benefit of using an advanced tuning technique. Below is[a recent experiment run on a BERT](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b)model from[Hugging Face transformers](https://github.com/huggingface/transformers)on the[RTE dataset](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool). Genetic optimization techniques like[PBT](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#population-based-training-tune-schedulers-populationbasedtraining)can provide large performance improvements compared to standard hyperparameter optimization techniques.\n**Algorithm**|**Best Val Acc.**|**Best Test Acc.**|**Total GPU min**|**Total $ cost**|\nGrid Search|74%|65.4%|45 min|$2.30|\nBayesian Optimization +Early Stop|77%|66.9%|104 min|$5.30|\nPopulation-based Training|78%|70.5%|48 min|$2.45|\nIf you\u2019re leveraging[Transformers](https://github.com/huggingface/transformers), you\u2019ll want to have a way to easily access powerful hyperparameter tuning solutions without giving up the customizability of the Transformers framework.\n[![alt_text](https://huggingface.co/blog/assets/06_ray_tune/ray-hf.jpg \"image_tooltip\")](https://huggingface.co/blog/assets/06_ray_tune/ray-hf.jpg)\nIn the Transformers 3.1 release,[Hugging Face Transformers](https://github.com/huggingface/transformers)and[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)teamed up to provide a simple yet powerful integration.[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)is a popular Python library for hyperparameter tuning that provides many state-of-the-art algorithms out of the box, along with integrations with the best-of-class tooling, such as[Weights and Biases](https://wandb.ai/)and tensorboard.\nTo demonstrate this new[Hugging Face](https://github.com/huggingface/transformers)+[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)integration, we leverage the[Hugging Face Datasets library](https://github.com/huggingface/datasets)to fine tune BERT on[MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398).\nTo run this example, please first run:\n**`pip install \"ray[tune]\" transformers datasets scipy sklearn torch`**\nSimply plug in one of Ray\u2019s standard tuning algorithms by just adding a few lines of code.\n```\n`fromdatasetsimportload\\_dataset, load\\_metricfromtransformersimport(AutoModelForSequenceClassification, AutoTokenizer,\nTrainer, TrainingArguments)\ntokenizer = AutoTokenizer.from\\_pretrained('distilbert-base-uncased')\ndataset = load\\_dataset('glue','mrpc')\nmetric = load\\_metric('glue','mrpc')defencode(examples):\noutputs = tokenizer(\nexamples['sentence1'], examples['sentence2'], truncation=True)returnoutputs\nencoded\\_dataset = dataset.map(encode, batched=True)defmodel\\_init():returnAutoModelForSequenceClassification.from\\_pretrained('distilbert-base-uncased', return\\_dict=True)defcompute\\_metrics(eval\\_pred):\npredictions, labels = eval\\_pred\npredictions = predictions.argmax(axis=-1)returnmetric.compute(predictions=predictions, references=labels)# Evaluate during training and a bit more often# than the default to be able to prune bad trials early.# Disabling tqdm is a matter of preference.training\\_args = TrainingArguments(\"test\", evaluation\\_strategy=\"steps\", eval\\_steps=500, disable\\_tqdm=True)\ntrainer = Trainer(\nargs=training\\_args,\ntokenizer=tokenizer,\ntrain\\_dataset=encoded\\_dataset[\"train\"],\neval\\_dataset=encoded\\_dataset[\"validation\"],\nmodel\\_init=model\\_init,\ncompute\\_metrics=compute\\_metrics,\n)# Default objective is the sum of all metrics# when metrics are provided, so we have to maximize it.trainer.hyperparameter\\_search(\ndirection=\"maximize\", backend=\"ray\", n\\_trials=10# number of trials)`\n```\nBy default, each trial will utilize 1 CPU, and optionally 1 GPU if available.\nYou can leverage multiple[GPUs for a parallel hyperparameter search](https://docs.ray.io/en/latest/tune/user-guide.html#resources-parallelism-gpus-distributed)by passing in a`resources\\_per\\_trial`argument.\nYou can also easily swap different parameter tuning algorithms such as[HyperBand](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler),[Bayesian Optimization](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html),[Population-Based Training](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#population-based-training-tune-schedulers-populationbasedtraining):\nTo run this example, first run:**`pip install hyperopt`**\n```\n`fromray.tune.suggest.hyperoptimportHyperOptSearchfromray.tune.schedulersimportASHAScheduler\ntrainer = Trainer(\nargs=training\\_args,\ntokenizer=tokenizer,\ntrain\\_dataset=encoded\\_dataset[\"train\"],\neval\\_dataset=encoded\\_dataset[\"validation\"],\nmodel\\_init=model\\_init,\ncompute\\_metrics=compute\\_metrics,\n)\nbest\\_trial = trainer.hyperparameter\\_search(\ndirection=\"maximize\",\nbackend=\"ray\",# Choose among many libraries:# https://docs.ray.io/en/latest/tune/api\\_docs/suggestion.htmlsearch\\_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),# Choose among schedulers:# https://docs.ray.io/en/latest/tune/api\\_docs/schedulers.htmlscheduler=ASHAScheduler(metric=\"objective\", mode=\"max\"))`\n```\nIt also works with[Weights and Biases](https://wandb.ai/)out of the box!\n[![alt_text](https://huggingface.co/blog/assets/06_ray_tune/ray-wandb.png \"image_tooltip\")](https://huggingface.co/blog/assets/06_ray_tune/ray-wandb.png)\n### [](#try-it-out-today)Try it out today:\n* `pip install -U ray`\n* `pip install -U transformers datasets`\n* Check out the[Hugging Face documentation](https://huggingface.co/transformers/)and[Discussion thread](https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10)\n* [End-to-end example of using Hugging Face hyperparameter search for text classification](https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb)\nIf you liked this blog...",
      "url": "https://huggingface.co/blog/ray-tune"
    },
    {
      "title": "Titanic - Top 4% , Hyperparameter Tuning - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/tanmayunhale/titanic-top-4-hyperparameter-tuning"
    },
    {
      "title": "XGBoost classifier and hyperparameter tuning [85%] - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=fde7cab3d8c139f5e6ae:1:11126)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/michalbrezk/xgboost-classifier-and-hyperparameter-tuning-85"
    },
    {
      "title": "Hyperparameter tuning in random forests - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=ed94601b1ebb8e8f5985:1:10685)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/nargisbegum82/hyperparameter-tuning-in-random-forests"
    },
    {
      "title": "Feature-engineering-pipeline-and-hyperparam-tuning - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=b2b0a2a71e3ccb18d5a8:1:10994)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/solegalli/feature-engineering-pipeline-and-hyperparam-tuning"
    },
    {
      "title": "Which text classification hyper-parameters are worth tuning?",
      "text": "FastText Users | Hey. So I&#039;m using fasttext for text classification, and the machine I&#039;m using is not strong enough to really brute force a hyper-parameter grid search...",
      "url": "https://www.facebook.com/groups/1174547215919768/posts/1884000951641054"
    }
  ]
}