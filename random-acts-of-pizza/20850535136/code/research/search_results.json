{
  "query": "What ensembling strategies work best for text classification competitions on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Ensembling is the de\u2011facto standard for winning Kaggle text\u2011classification contests.  The most reliable recipe combines **diverse fine\u2011tuned language models** (e.g., BERT, RoBERTa, DeBERTa, GPT\u2011based classifiers) and aggregates their outputs with a **simple blending or voting step** before feeding the blended predictions into a **second\u2011level meta\u2011model**.  Studies that collected predictions from five large fine\u2011tuned models across six datasets showed that even straightforward averaging or majority\u2011vote ensembles consistently lift accuracy and give better uncertainty estimates than any single model\u202f([arxiv.org/html/2410.19889v1](https://arxiv.org/html/2410.19889v1)).  For classification, using the **geometric mean of class probabilities** (instead of a plain arithmetic mean) often yields a small but steady gain\u202f([campus.datacamp.com](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8)).  When the ensemble contains many heterogeneous LLMs, a **majority\u2011rules / collective decision** aggregation can boost F1 by up to 65\u202f% over the strongest single model, confirming that diversity and a principled voting scheme are crucial\u202f([arxiv.org/html/2511.15714v1](https://arxiv.org/html/2511.15714v1)).\n\nThe next level of performance comes from **stacked generalization**: each base model\u2019s out\u2011of\u2011fold predictions become features for a lightweight meta\u2011learner (often logistic regression, LightGBM, or CatBoost).  This \u201cstacking\u201d captures complementary error patterns and typically outperforms pure blending, especially when the base models are low\u2011correlated (trained on different seeds, data splits, or architectures)\u202f([campus.datacamp.com](https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8)).  Kaggle practitioners also enrich ensembles with **rank\u2011based averaging** or **error\u2011correcting\u2011code voting** to further reduce variance\u202f([cnblogs.com/medsci/articles/9160663.html](https://www.cnblogs.com/medsci/articles/9160663.html)).  In practice, the best Kaggle text\u2011classification pipelines therefore follow a three\u2011step stack: (1) train a heterogeneous set of fine\u2011tuned transformers/LLMs, (2) blend their probabilities with a geometric mean or majority vote, and (3) train a second\u2011level model on the blended predictions to produce the final submission.",
      "url": ""
    },
    {
      "title": "Ensembling Finetuned Language Models\n for Text Classification",
      "text": "Ensembling Finetuned Language Models for Text Classification\n# Ensembling Finetuned Language Models\nfor Text Classification\nSebastian Pineda Arango1, Maciej Janowski111footnotemark:1, Lennart Purucker1, Arber Zela1,\nFrank Hutter3,1,\u00a0Josif Grabocka2\n1University of Freiburg,2University of Technology N\u00fcrnberg,3ELLIS Institute T\u00fcbingenEqual contribution. Corresponding author:pineda@cs.uni-freiburg.de\n###### Abstract\nFinetuning is a common practice widespread across different communities to adapt pretrained models to particular tasks.\nText classification is one of these tasks for which many pretrained models are available. On the other hand, ensembles of neural networks are typically used to boost performance and provide reliable uncertainty estimates. However, ensembling pretrained models for text classification is not a well-studied avenue. In this paper, we present a metadataset with predictions from five large finetuned models on six datasets, and report results of different ensembling strategies from these predictions. Our results shed light on how ensembling can improve the performance of finetuned text classifiers and incentivize future adoption of ensembles in such tasks.\n## 1Introduction\nIn recent years, fine-tuning pretrained models has become a widely adopted technique for adapting general-purpose models to specific tasks> (Arango et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.19889v1#bib.bib2)> )\n. This practice has gained significant traction across various communities due to its effectiveness in leveraging the vast knowledge encoded in pretrained models. Among the diverse tasks that benefit from fine-tuning, text classification stands out as one of the most prevalent. With the availability of numerous pretrained models, practitioners often find themselves with a range of powerful tools to tackle text classification challenges.\nHowever, despite the widespread use of fine-tuning, the potential benefits of combining or ensembling these fine-tuned models remain underexplored.\nPrevious studies have primarily concentrated on improving individual model performance through fine-tuning techniques> (Howard &amp; Ruder, [> 2018\n](https://arxiv.org/html/2410.19889v1#bib.bib16)> )\n, leaving the exploration of ensemble strategies largely underdeveloped in this context. This oversight is particularly significant given the well-documented advantages of model ensembling in other machine learning domains> (Erickson et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.19889v1#bib.bib10)> ; Lakshminarayanan et\u00a0al., [> 2017\n](https://arxiv.org/html/2410.19889v1#bib.bib20)> )\n, which has been shown to enhance robustness and generalization. In this paper, we address the aforementioned gap by introducing a novel metadataset, which we dub: Finetuning Text Classifiers (FTC) metadataset. FTC contains predictions from various fine-tuned models on text classification tasks with various number of classes. We systematically evaluate different ensembling strategies using this metadataset, aiming to uncover insights into the potential improvements that ensembling can offer. Our results provide valuable evidence on the efficacy of these strategies, demonstrating that ensembling fine-tuned models can lead to performance gains in text classification.\n## 2Background and Related Work\nFinetuning for Text Classification.Universal Language Model Fine-tuning for Text Classification or ULMFiT> (Howard &amp; Ruder, [> 2018\n](https://arxiv.org/html/2410.19889v1#bib.bib16)> )\nconsists of finetuning language models for classification in two stages: 1) a target task unsupervised finetuning and 2) target task classifier finetuning, while using a different learning rate per layer. However, the feasibility of fully fine-tuning large pretrained language models is constrained by computational limits> (Radford et\u00a0al., [> 2018\n](https://arxiv.org/html/2410.19889v1#bib.bib31)> )\n. This has spurred the adoption of Parameter-Efficient Fine-Tuning (PEFT) methods> (Han et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.19889v1#bib.bib13)> )\n. Early strategies focused on minimal subsets of parameters such as sparse subnetworks> (Sung et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.19889v1#bib.bib37)> )\nto improve task-specific performance efficiently. Innovations such as adapter modules> (Houlsby et\u00a0al., [> 2019\n](https://arxiv.org/html/2410.19889v1#bib.bib15)> )\n, which introduce a few parameters per transformer layer but in consequence increase inference time, prompted the development of Low-Rank Adaptation (LoRA)> (Hu et\u00a0al., [> 2022\n](https://arxiv.org/html/2410.19889v1#bib.bib17)> ; Dettmers et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.19889v1#bib.bib7)> )\nthat applies low-rank updates for improved downstream task performance with reduced computational overhead. Some studies have also demonstrated that finetuned language models can be ensembled to improve performance for text classification> (Abburi et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.19889v1#bib.bib1)> )\n, but they do not provide clear insights about ensembling methods, hyperparameters, or metadata.\nEnsembling Deep Learning Models.Ensembles of neural networks> (Hansen &amp; Salamon, [> 1990\n](https://arxiv.org/html/2410.19889v1#bib.bib14)> ; Krogh &amp; Vedelsby, [> 1995\n](https://arxiv.org/html/2410.19889v1#bib.bib19)> ; Dietterich, [> 2000\n](https://arxiv.org/html/2410.19889v1#bib.bib9)> )\nhave gained significant attention in deep learning research, both for their performance-boosting capabilities and their effectiveness in uncertainty estimation. Various strategies for building ensembles exist, with deep ensembles> (Lakshminarayanan et\u00a0al., [> 2017\n](https://arxiv.org/html/2410.19889v1#bib.bib20)> )\nbeing the most popular one, which involve independently training multiple initializations of the same network.\nTheir state-of-the-art predictive uncertainty estimates have further fueled the interest in ensembles. Extensive empirical studies> (Ovadia et\u00a0al., [> 2019\n](https://arxiv.org/html/2410.19889v1#bib.bib27)> ; Gustafsson et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.19889v1#bib.bib12)> )\nhave shown that deep ensembles outperform other approaches for uncertainty estimation, such as Bayesian neural networks> (Blundell et\u00a0al., [> 2015\n](https://arxiv.org/html/2410.19889v1#bib.bib3)> ; Gal &amp; Ghahramani, [> 2016\n](https://arxiv.org/html/2410.19889v1#bib.bib11)> ; Welling &amp; Teh, [> 2011\n](https://arxiv.org/html/2410.19889v1#bib.bib40)> )\n. Similar to our work,> Seligmann et\u00a0al. (\n[> 2024\n](https://arxiv.org/html/2410.19889v1#bib.bib35)> )\nshow that finetuning pretrained models via Bayesian methods on the WILDS dataset> (Koh et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.19889v1#bib.bib18)> )\n, which contains text classification as well, can yield significant performance as compared to standard finetuning of single models.\nPost-Hoc Ensembling (PHE).PHE uses set of fitted base models{z1,\u2026,zM}subscript\ud835\udc671\u2026subscript\ud835\udc67\ud835\udc40\\\\{z\\_{1},...,z\\_{M}\\\\}{ italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT }such that every model outputszm\u2062(x),zm:\u211dD\u2192\u211dC:subscript\ud835\udc67\ud835\udc5a\ud835\udc65subscript\ud835\udc67\ud835\udc5a\u2192superscript\u211d\ud835\udc37superscript\u211d\ud835\udc36z\\_{m}(x),z\\_{m}:\\\\mathbb{R}^{D}\\\\rightarrow\\\\mathbb{R}^{C}italic\\_z start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT ( italic\\_x ) , italic\\_z start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT : blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_D end\\_POSTSUPERSCRIPT \u2192blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_C end\\_POSTSUPERSCRIPT111We assume a classification tasks withC\ud835\udc36Citalic\\_Cclasses. For regressionC=1\ud835\udc361C=1italic\\_C = 1.. These outputs are combined by an ensemblerf\u2062(z1\u2062(x),\u2026,zM\u2062(x);\u03b8)=f\u2062(z\u2062(x);\u03b8)\ud835\udc53subscript\ud835\udc671\ud835\udc65\u2026subscript\ud835\udc67\ud835\udc40\ud835\udc65\ud835\udf03\ud835\udc53\ud835\udc67\ud835\udc65\ud835\udf03f(z\\_{1}(x),...,z\\_{M}(x);\\\\theta)=f(z(x);\\\\theta)italic\\_f ( italic\\_z start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ( italic\\_x ) , \u2026, italic\\_z start\\_POSTSUBSCRIPT italic\\_M end\\_POSTSUBSCRIPT ( italic\\_x ) ; italic\\_\u03b8 ) = italic\\_f ( italic\\_z ( italic\\_x ) ; italic\\_\u03b8 ),...",
      "url": "https://arxiv.org/html/2410.19889v1"
    },
    {
      "title": "Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization",
      "text": "Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization\n# Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization\nAriel Kamen\nRingCentral Inc.\nariel.kamen@ringcentral.comYakov Kamen\nRelevad Corporation\nyakov@relevad.com\n(November 1, 2025)\n###### Abstract\nThis study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8,660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.\nIndex Terms\u2014AI, large language models, LLM, multi-LLM, LLM ensemble, collective intelligence, content categorization, collective decision-making, CDM, ensemble framework, eLLM.\n## 1.\u2003\u200aIntroduction\nText categorization (also termed text classification) is a foundational problem in natural language processing. Early systems relied on human experts to assign categories using controlled vocabularies and taxonomies. This labor-intensive workflow gradually gave way to rule-based heuristics and, later, to classical machine-learning pipelines with domain-specific training and hand-engineered features. While effective within constrained settings, these approaches required substantial manual effort and did not scale easily across heterogeneous domains.\nLarge language models (LLMs)\u2014including GPT, Gemini, Claude, Grok, LLaMA, Mistral, xAI, and DeepSeek\u2014have introduced strong zero-shot capabilities, motivating efforts to replace task-specific classifiers with general-purpose LLMs. Despite notable successes of LLMs in reasoning, scientific discovery, and content generation, their performance in taxonomy-based categorization has lagged. Empirically, single models exhibit instability, category inflation, and hallucination, often producing incoherent or non-existent labels.\nThis performance gap reflects the unique nature of categorization as an information-compression task: semantically rich, unstructured inputs must be mapped into a small set of category names defined by a fixed taxonomy. Such taxonomies typically represent the real world sparsely, and individual texts may map to multiple categories with varying levels of relevance. Accurate mapping frequently requires background knowledge external to the text\u2014such as domain conventions or factual context\u2014that distinguishes professional experts from general readers. In our experiments with ten widely used zero-shot LLMs spanning multiple families and release generations, we observe a consistent performance plateau: increasing model size or algorithmic sophistication does not yield commensurate gains once a threshold is reached> (Kamen, [> 2025\n](https://arxiv.org/html/2511.15714v1#bib.bib15)> )\n.\nTo address these limitations, we developed an ensemble of LLMs (eLLM), treating each model as an independent expert and combining their predictions through a collective decision-making rule. This design is motivated by the well-established benefits of aggregation in both human and algorithmic committees, where diversity and redundancy suppress individual errors and reduce variance. By leveraging models built on diverse architectures, training paradigms, and knowledge bases, the ensemble extends the effective knowledge universe of any single LLM, enabling more comprehensive coverage of sparse taxonomies. In taxonomy classification, we find that ensembling mitigates common single-model failure modes\u2014such as hallucinations and inconsistent label usage\u2014while substantially improving both accuracy and stability. The results are striking: ensembles with sufficient diversity not only surpass the strongest individual models but, in some conditions, approach or exceed the consistency of human expert annotation. The principal trade-off lies in computational cost, since multiple model evaluations are required at inference time. This direction aligns with growing evidence that collaborative or multi-agent LLM strategies can overcome inherent limitations of single-model systems> (Feng et\u00a0al., [> 2025\n](https://arxiv.org/html/2511.15714v1#bib.bib9)> )\n. The eLLM\u2019s capacity for automatic error counterbalancing represents a practical form of self-correction\u2014an essential step toward reliable artificial intelligence.\nThis paper makes the following contributions:\n* \u2022We formalize an ensemble LLM framework for taxonomy-based categorization, specifying aggregation criteria within a collective decision-making model tailored to sparse hierarchical label spaces.\n* \u2022We instantiate eLLM with multiple committee sizes (2, 3, 5, 7, and 10 models) and evaluate them under uniform zero-shot prompting on a human-annotated corpus of 8,660 samples labeled with the IAB\u00a02.2 taxonomy.\n* \u2022We provide empirical evidence of substantial improvements over the best single model, including large gains in F1-score, reduced category inflation, and lower hallucination rates, and we analyze the computational trade-offs of ensembling.\n* \u2022We discuss conditions under which eLLM approaches human-expert performance, and the implications for scalable, reliable labeling pipelines.\nTogether, these results suggest that ensemble-based categorization is a practical and robust alternative to single-model LLM classification, particularly when adherence to a fixed taxonomy and reliability under distributional shift are required. This work builds toward a new theory of collaborative AI, where orchestrated ensembles transform unstructured chaos into ordered, expert-level insight.\n## 2.\u2003\u200aRelated Work\nOur research on ensemble large language models (eLLM) for hierarchical text categorization builds upon and integrates advances in three major domains: (1) taxonomy-based classification, (2) zero-shot categorization with large language models, and (3) collective decision-making and ensemble learning. It directly extends the findings of> (Kamen, [> 2025\n](https://arxiv.org/html/2511.15714v1#bib.bib15)> )\n, which identified structural limitations in single-model LLM categorization and established the empirical foundation for ensemble-based approaches.\n### 2.1.\u2003\u200aTaxonomy-Based Hierarchical Classification\nText categorization has been a central task in natural language processing (NLP) for several decades. Early systems relied on manual annotation and rule-based heuristics, later replaced by supervised pipelines built upon feature engineering and linear classifiers> (Sebastiani, [> 2002\n](https://arxiv.org/html/2511.15714v1#bib.bib26)> )\n. As taxonomies expanded in depth and scope, hierarchical classification emerged as a natural extension, in which labels are organized into a tree or directed acyclic graph> (Jr. et\u00a0al., [> 2011\n](https://arxiv.org/html/2511.15714v1#bib.bib14)> )\n.\nHierarchical classification requires predictions that respect structural dependencies, maintain parent\u2013child consistency, and remain valid across all levels of granularity. Various methods have been proposed to address these challenges, including structural loss functions> (Hsu et\u00a0al., [> 2009\n](https://arxiv.org/html/2511.15714v1#bib.bi...",
      "url": "https://arxiv.org/html/2511.15714v1"
    },
    {
      "title": "Model ensembling | Python",
      "text": "Model ensembling | Python https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8\nModel ensembling | Python\nNone\n2022-06-13T00:00:00Z\n# Model ensembling\n####. Model ensembling\nSo far, we've been talking only about individual models. Now it's time to combine multiple models together.\n####. Model ensembling\nKaggle top solutions are usually not a single model, but a combination of a large number of various models. Different ways to combine models together is called model ensembling.\nFor example, here is an ensemble design for a winning solution in the Homesite Quote Conversion challenge. We can see hundreds of models with multi-level stacking and blending. Let's learn about these 'blending' and 'stacking' terms.\n####. Model blending\nThe idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models. The so-called blending approach is to just find an average of our multiple models predictions.\nSay we're solving a regression problem with a continuous target variable.\nAnd we have trained two models: A and B.\nSo, for each test observation, there are model A and model B predictions available.\n####. Model blending\nTo combine models together we can just find the predictions mean, taking the sum and divide it by two. As we see, it allows us to tweak predictions, and to take into account both model A and model B opinions.\nThat's it, such a simple ensembling method in the majority of cases will yield some improvement to our single models.\n####. Model blending\nArithmetic mean works for both regression and classification problems. However, for the classification, it's better to use a geometric mean of the class probabilities predicted.\n####. Model stacking\nThe more advanced ensembling approach is called model stacking. The idea is to train multiple single models, take their predictions and use these predictions as features in the 2nd level model.\nSo, we need to perform the following steps:\nSplit train data into two parts. Part 1 and Part 2.\nTrain multiple single models on the first part.\nMake predictions on the second part of the train data,\nand on the test data. Now, we have models predictions for both Part 2 of the train data and for the test data.\nIt means that we could create a new model using these predictions as features. This model is called the 2nd level model or meta-model.\nIts predictions on the test data give us the stacking output.\n####. Stacking example\nLet's consider all these steps on the example.\nSuppose we are given a binary classification problem with a bunch of numerical features: feature_1, feature_2 and so on to feature_N. For the train data, target variable is known.\nAnd we need to make predictions on the test data with the unknown target variable.\n####. Stacking example\nFirst of all, we split train data into two separate parts: Part 1 and Part 2.\n####. Stacking example\nThen we train multiple single models only on the first part of the train data. For example, we've trained three different models denoting them as A, B and C.\n####. Stacking example\nHaving these 3 models we make the predictions on part 2 of the train data. The columns with the predictions are denoted as A_pred, B_pred and C_pred.\nThen make the predictions on the test data as well.\n####. Stacking example\nSo, now we have models predictions for both Part 2 of the train data and for the test data.\n####. Stacking example\nIt's now possible to create a second level model using these predictions as features. It's trained on the Part 2 train data and is used to make predictions on the test data.\n####. Stacking example\nAs a result, we obtain stacking predictions for the test data. Thus, we combined individual model predictions into a single number using a 2nd level model.\n####. Let's practice!\nOK, having learned the theory, move on to build your own model ensembles!\nShow Transcripts\n### Create Your Free Account\nor\nEmail AddressPasswordStart Learning for FreeBy continuing, you accept our [Terms of Use](https://www.datacamp.com/terms-of-use), our [Privacy Policy](https://www.datacamp.com/privacy-policy) and that your data is stored in the USA.",
      "url": "https://campus.datacamp.com/courses/winning-a-kaggle-competition-in-python/modeling?ex=8"
    },
    {
      "title": "KAGGLE ENSEMBLING GUIDE",
      "text": "[\u535a\u5ba2\u56ed](https://www.cnblogs.com/) [\u9996\u9875](https://www.cnblogs.com/medsci/) [\u65b0\u968f\u7b14](https://i.cnblogs.com/EditPosts.aspx?opt=1) [\u8054\u7cfb](https://msg.cnblogs.com/send/medsci) [\u8ba2\u9605](javascript:void(0)) [\u7ba1\u7406](https://i.cnblogs.com/)\n\n# [KAGGLE ENSEMBLING GUIDE](https://www.cnblogs.com/medsci/articles/9160663.html)\n\n\u8f6c\u8f7d\u81ea\u00a0 https://mlwave.com/kaggle-ensembling-guide/\n\ngithub:\u00a0 https://github.com/MLWave/Kaggle-Ensemble-Guide\n\nModel ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions.\n\nFor the first part we look at creating ensembles from submission files. The second part will look at creating ensembles through stacked generalization/blending.\n\nI answer why ensembling reduces the generalization error. Finally I show different methods of ensembling, together with their results and code to try it out for yourself.\n\n> This is how you win ML competitions: you take other peoples\u2019 work and ensemble them together.\u201d\u00a0[Vitaly Kuznetsov](http://cims.nyu.edu/~vitaly/)\u00a0NIPS2014\n\n## Creating ensembles from submission files\n\nThe most basic and convenient way to ensemble is to ensemble Kaggle submission CSV files. You only need the predictions on the test set for these methods \u2014 no need to retrain a model. This makes it a quick way to ensemble already existing model predictions, ideal when teaming up.\n\n### Voting ensembles.\n\nWe first take a look at a simple majority vote ensemble. Let\u2019s see why model ensembling reduces error rate and why it works better to ensemble low-correlated model predictions.\n\n#### Error correcting codes\n\nDuring space missions it is very important that all signals are correctly relayed.\n\nIf we have a signal in the form of a binary string like:\n\n```\n1110110011101111011111011011\n\n```\n\nand somehow this signal is corrupted (a bit is flipped) to:\n\n```\n1010110011101111011111011011\n\n```\n\nthen lives could be lost.\n\nA\u00a0[coding](http://en.wikipedia.org/wiki/Coding_theory)\u00a0solution was found in\u00a0[error correcting codes](http://en.wikipedia.org/wiki/Forward_error_correction). The simplest error correcting code is a\u00a0[repetition-code](http://en.wikipedia.org/wiki/Repetition_code): Relay the signal multiple times in equally sized chunks and have a majority vote.\n\n```\nOriginal signal:\n1110110011\n\nEncoded:\n10,3 101011001111101100111110110011\n\nDecoding:\n1010110011\n1110110011\n1110110011\n\nMajority vote:\n1110110011\n\n```\n\nSignal corruption is a very rare occurrence and often occur in small bursts. So then it figures that it is even rarer to have a corrupted majority vote.\n\nAs long as the corruption is not completely unpredictable (has a 50% chance of occurring) then signals can be repaired.\n\n#### A machine learning example\n\nSuppose we have a test set of 10 samples. The ground truth is all positive (\u201c1\u201d):\n\n```\n1111111111\n\n```\n\nWe furthermore have 3 binary classifiers (A,B,C) with a 70% accuracy. You can view these classifiers for now as pseudo-random number generators which output a \u201c1\u201d 70% of the time and a \u201c0\u201d 30% of the time.\n\nWe will now show how these pseudo-classifiers are able to obtain 78% accuracy through a voting ensemble.\n\n##### A pinch of maths\n\nFor a majority vote with 3 members we can expect 4 outcomes:\n\n```\nAll three are correct\n  0.7 * 0.7 * 0.7\n= 0.3429\n\nTwo are correct\n  0.7 * 0.7 * 0.3\n+ 0.7 * 0.3 * 0.7\n+ 0.3 * 0.7 * 0.7\n= 0.4409\n\nTwo are wrong\n  0.3 * 0.3 * 0.7\n+ 0.3 * 0.7 * 0.3\n+ 0.7 * 0.3 * 0.3\n= 0.189\n\nAll three are wrong\n  0.3 * 0.3 * 0.3\n= 0.027\n\n```\n\nWe see that most of the times (~44%) the majority vote corrects an error. This majority vote ensemble will be correct an average of ~78% (0.3429 + 0.4409 = 0.7838).\n\n#### Number of voters\n\nLike repetition codes increase in their error-correcting capability when more codes are repeated, so do ensembles usually improve when adding more ensemble members.\n\nUsing the same pinch of maths as above: a voting ensemble of 5 pseudo-random classifiers with 70% accuracy would be correct ~83% of the time. One or two errors are being corrected during ~66% of the majority votes. (0.36015 + 0.3087)\n\n#### Correlation\n\nWhen I first joined the team for KDD-cup 2014, Marios Michailidis ( [KazAnova](https://www.kaggle.com/kazanova)) proposed something peculiar. He calculated the\u00a0[Pearson correlation](http://onlinestatbook.com/2/describing_bivariate_data/pearson.html)\u00a0for all our submission files and gathered a few well-performing models which were less correlated.\n\nCreating an averaging ensemble from these diverse submissions gave us the\u00a0biggest 50-spot jump on the leaderboard. Uncorrelated submissions clearly do better when ensembled than correlated submissions. But why?\n\nTo see this, let us take 3 simple models again. The ground truth is still all 1\u2019s:\n\n```\n1111111100 = 80% accuracy\n1111111100 = 80% accuracy\n1011111100 = 70% accuracy.\n\n```\n\nThese models are highly correlated in their predictions. When we take a majority vote we see no improvement:\n\n```\n1111111100 = 80% accuracy\n\n```\n\nNow we compare to 3 less-performing, but highly uncorrelated models:\n\n```\n1111111100 = 80% accuracy\n0111011101 = 70% accuracy\n1000101111 = 60% accuracy\n\n```\n\nWhen we ensemble this with a majority vote we get:\n\n```\n1111111101 = 90% accuracy\n\n```\n\nWhich\u00a0_is_\u00a0an improvement: A lower correlation between ensemble model members seems to result in an increase in the error-correcting capability.\n\n#### Use for Kaggle: Forest Cover Type prediction\n\nMajority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.\n\nThe\u00a0[forest cover type prediction](https://www.kaggle.com/c/forest-cover-type-prediction)\u00a0challenge uses the\u00a0[UCI Forest CoverType dataset](https://archive.ics.uci.edu/ml/datasets/Covertype). The dataset has\u00a054 attributes and there are 6 classes.\n\nWe create a simple\u00a0[starter model](https://www.kaggle.com/triskelion/forest-cover-type-prediction/first-try-with-random-forests)\u00a0with a 500-tree Random Forest. We then create a few more models and pick the best performing one. For this task and our model selection an ExtraTreesClassifier works best.\n\n##### Weighing\n\nWe then use a weighted majority vote. Why weighing? Usually we want to give a better model more weight in a vote. So in our case we count the vote by the best model 3 times.\u00a0The other 4 models count for one vote each.\n\nThe reasoning is as follows: The only way for the inferior models to overrule the best model (expert) is for them to\u00a0collectively (and confidently) agree on an alternative.\n\nWe can expect this ensemble to repair a few erroneous choices by the best model, leading to a small improvement only.\u00a0That\u2019s our punishment for forgoing a democracy and creating a Plato\u2019s\u00a0_Republic_.\n\n> \u201cEvery city encompasses two cities that are at war with each other.\u201d\u00a0Plato in\u00a0The Republic\n\nTable 1. shows the result of training 5 models, and the resulting score when combining these with a weighted majority vote.\n\n| MODEL | PUBLIC ACCURACY SCORE |\n| --- | --- |\n| GradientBoostingMachine | 0.65057 |\n| RandomForest Gini | 0.75107 |\n| RandomForest Entropy | 0.75222 |\n| ExtraTrees Entropy | 0.75524 |\n| ExtraTrees Gini (Best) | 0.75571 |\n| Voting Ensemble (Democracy) | 0.75337 |\n| Voting Ensemble (3\\*Best vs. Rest) | 0.75667 |\n\n#### Use for Kaggle: CIFAR-10 Object detection in images\n\nCIFAR-10 is another multi-class classification challenge where accuracy matters.\n\nOur team leader for this challenge,\u00a0[Phil Culliton](https://www.kaggle.com/philculliton), first found the best setup to\u00a0[replicate a good model](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/)\u00a0from dr. Graham.\n\nThen he used a voting ensemble of around 30 convnets submissions (all scoring above 90% accuracy). The best\u00a0single model of the ensemble scored\u00a00.93170.\n\nA voting ensemble of 30 models scored\u00a00.94120. A ~0.01 reduction in error rate, pushing the resulting score beyond the\u00a0[estimated human clas...",
      "url": "https://www.cnblogs.com/medsci/articles/9160663.html"
    },
    {
      "title": "",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/pdf/2410.19889"
    },
    {
      "title": "Kaggle Champions: Ensemble Methods in Machine Learning | Toptal\u00ae",
      "text": "<div><div>\n<p>The proverb \u201cTwo heads are better than one\u201d takes on new meaning when it comes to <a href=\"https://www.toptal.com/machine-learning\">machine learning</a> ensembles. Ensemble methods are some of the most decorated ML families at Kaggle competitions, where they often win contests with their impressive results.</p>\n<p>But it was a century before Kaggle when statistician Sir Francis Galton noticed the potency of aggregated intelligence. He happened upon a competition at a livestock fair where participants had to guess the weight of an ox. Eight hundred people submitted guesses, but their skill levels varied: Farmers and butchers guessed alongside city dwellers who had never seen an ox up close, so Galton thought the average guess would be quite wrong.</p>\n<p>It <a href=\"https://projecteuclid.org/download/pdfview_1/euclid.ss/1411437521\">turned out</a> that the mean of the crowd\u2019s guesses was off by less than a pound (&lt; 0.1%). However, even the best individual predictions were well off the mark.</p>\n<p>How could that be? What made such an unexpected result possible?</p>\n<h2>What Makes Machine Ensembles so Effective</h2>\n<p>The event that forced Galton to question his beliefs also illustrates what makes ensembles so powerful: If you have <strong>different</strong> <strong>and independent models</strong>, trained using <strong>different parts of data</strong> for the same problem, they will work better together than individually. The reason? Each model will learn a different part of the concept. Therefore, each model will produce valid results and errors based on its \u201cknowledge.\u201d</p>\n<p>But the most interesting thing is that each true part will complement the others while the errors cancel out each other:</p>\n<div><figure><a href=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/750064/image-1631891397968-d95bfcfccb53db7971c25b2a53222b3f.png\"></a></figure></div>\n<p>You need to train models with high variance (like decision trees) over distinct subsets of data. This added variance means that each model overfits different data, but when combined, the variance disappears, as if by magic. This creates a new, more robust model.</p>\n<p>Just like in Galton\u2019s case, when all data from all sources is combined, the result is \u201csmarter\u201d than isolated data points.</p>\n<h3>A Closer Look at Ensemble Learning in Kaggle Competitions</h3>\n<p>At the <a href=\"https://www.kaggle.com/c/otto-group-product-classification-challenge/overview\">Otto Group Product Classification Challenge</a>, participants had to build a predictive model that was able to distinguish between main product categories.</p>\n<p>Here you can see how <a href=\"https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\">the winning model was built</a>. It was a stacking of three layers: The first had 33 models, the second added three more (XGBoost, a neural network, and AdaBoost), and the third was the weighted mean of the previous layer outputs. It was both a very complex model and an ensemble.</p>\n<p>Another Kaggle success is the model created by Chenglong Chen at the <a href=\"https://www.kaggle.com/c/crowdflower-search-relevance/overview\">Crowdflower Search Results Relevance competition</a>. The challenge was to create a predictor that could be used to measure the relevance of search results. You can read the complete <a href=\"https://github.com/ChenglongChen/kaggle-CrowdFlower\">explanation of his method</a>, but as our point of interest is ensembles, the critical part of the story is that the winning solution used an ensemble of 35 models, many of which were also ensembles\u2014a meta-ensemble, so to speak.</p>\n<h2>Ensemble Methods</h2>\n<p>There are many ways of implementing ensemble methods in machine learning. We\u2019ll explore some of the most popular methods:</p>\n<ul>\n <li>\n<strong>Bagging</strong>\n <ul>\n <li>Random Forest</li>\n </ul>\n </li>\n <li>\n<strong>Boosting</strong>\n <ul>\n <li>AdaBoost</li>\n <li>Gradient Boosting and XGBoost</li>\n </ul>\n </li>\n <li>\n<strong>Hybrid Ensemble Methods</strong>\n <ul>\n <li>Voting</li>\n <li>Stacking</li>\n <li>Cascading</li>\n </ul>\n </li>\n</ul>\n<h2>Bagging</h2>\n<p>As mentioned, you need to train many models over different subsets of data. In practice, this is not easy because you will need much more data for many models than you would for a single model, and sometimes it is not easy to obtain high-quality datasets. This is when bagging (bootstrap aggregating) comes in handy, as it splits the data through bootstrapping: a random sample with a replacement, resulting in different subsets that overlap.</p>\n<p>Once you have trained your ensemble models, you construct your final prediction by aggregating each model prediction through any metric you prefer: the mean, median, mode, and so on. You can also use model prediction probabilities to make a weighted metric:</p>\n<div><figure><a href=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/750065/image-1631891438024-8c044ee5d973b58299ba8a7a56d1484c.png\"></a></figure></div>\n<p>If we want to use <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">decision trees</a> as models but we have few strong predictive attributes in our data, all the trees will be similar. This is because the same attributes will tend to be in the root node, producing similar results in each branch of the tree.</p>\n<h3>Random Forest</h3>\n<p>One technique to address this problem is <strong>random forest</strong>. It makes a bagging ensemble using trees but each node constrains its possible attributes to a random subset. This forces the models to be different, resolving the previous problem. It also makes random forest a very good model for <a href=\"https://en.wikipedia.org/wiki/Feature_selection\">feature selection</a>.</p>\n<p>Random forest is one of the most popular ML models because it delivers good performance with low variance and training time.</p>\n<h2>Boosting</h2>\n<p>Boosting also uses bootstrapping to train the models, with the main difference being that it adds weights to each instance based on model prediction errors. While bagging is a parallel process, boosting is a sequential one, in which each model has more probabilities. This allows it access to some instances of previous model predictions.</p>\n<p>With this modification, boosting tries to increase the focus over misclassified instances to reach better global performance:</p>\n<div><figure><a href=\"https://bs-uploads.toptal.io/blackfish-uploads/uploaded_file/file/750066/image-1631891461357-200322840b31404060220a892663aa15.png\"></a></figure></div>\n<p>It adds weights to models too. Predictors with better performance at training time will have a higher weight at the predicting stage.</p>\n<p>Let\u2019s take a closer look at some of the most popular boosting models:</p>\n<h3>AdaBoost</h3>\n<p>AdaBoost was one of the first implementations of boosting. It does almost exactly what we outlined about boosting in general and uses decision trees as models. Let\u2019s explain the training phase with some pseudo coding:</p>\n<pre><code>For each instance i\n Assign w[i] (weight, same for all)\nFor each iteration t\n Generate a subset s[t] by weighted boosting (using the w[i] weights)\n Train model m[t] using s[t]\n Store m[t]\n Calculate e[t] (error rate of m[t])\n Assign error rate e[t] to stored m[t] model\n If (e[t] &lt;= a_threshold)\n Exit for\n Update weights using m[t] errors\n</code></pre>\n<p>At prediction time, it weights each prediction based on the error rate <code>e[t]</code> calculated for each one. Results with a high error rate will have less weight than others with better accuracy.</p>\n<h3>Gradient Boosting and XGBoost</h3>\n<p>One of the major problems when training so many models and making them work together is finding the best hyperparameter configuration. It is difficult to find the best configuration for a single model; finding the best configuration for <em>n</em> models increases the complexity exponentially. The ideal configuration fo...",
      "url": "https://www.toptal.com/machine-learning/ensemble-methods-kaggle-machine-learn"
    },
    {
      "title": "A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know",
      "text": "A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Derrick-Mwiti.png?fit=193%2C193&ssl=1)\n[Derrick Mwiti](https://neptune.ai/blog/author/derrickmwiti)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)8 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)25th April, 2025\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\n[Ensemble learning techniques](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)have been proven to yield better performance on machine learning problems. We can use these techniques for regression as well as classification problems.\nThe final prediction from these ensembling techniques is obtained by combining results from several base models. Averaging, voting and stacking are some of the ways the results are combined to obtain a final prediction.\nIn this article, we will explore how ensemble learning can be used to come up with optimal machine learning models.\n## What is ensemble learning?\nEnsemble learning is a combination of several machine learning models in one problem. These models are known as weak learners. The intuition is that when you combine several weak learners, they can become strong learners.\nEach weak learner is fitted on the training set and provides predictions obtained. The final prediction result is computed by combining the results from all the weak learners.\n### Basic ensemble learning techniques\nLet\u2019s take a moment and look at simple ensemble learning techniques.\n#### Max voting\nIn classification, the prediction from each model is a vote. In max voting, the final prediction comes from the prediction with the most votes.\nLet\u2019s take an example where you have three classifiers with the following predictions:\n* classifier 1 &#8211; class A\n* classifier 2 &#8211; class B\n* classifier 3 &#8211; class B\nThe final prediction here would be class B since it has the most votes.\n#### Averaging\nIn averaging, the final output is an average of all predictions. This goes for regression problems. For example, in[random forest regression](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why), the final result is the average of the predictions from individual decision trees.\nLet\u2019s take an example of three regression models that predict the price of a commodity as follows:\n* regressor 1 &#8211; 200\n* regressor 2 &#8211;&#8211; 300\n* regressor 3 &#8211; 400\nThe final prediction would be the average of 200, 300, and 400.\n#### Weighted average\nIn weighted averaging, the base model with higher predictive power is more important. In the price prediction example, each of the regressors would be assigned a weight.\nThe sum of the weights would equal one. Let\u2019s say that the regressors are given weights of 0.35, 0.45, and 0.2 respectively. The final model prediction can be computed as follows:\n0.35 \\* 200 + 0.45\\*300 + 0.2\\*400 = 285\n## Advanced ensemble learning techniques\nAbove are simple techniques, now let\u2019s take a look at advanced techniques for ensemble learning.\n### Stacking\nStacking is the process of combining various estimators in order to reduce their biases. Predictions from each estimator are stacked together and used as input to a final estimator (usually called a*meta-model*) that computes the final prediction. Training of the final estimator happens via cross-validation.\nStacking can be done for both regression and classification problems.\n![Ensemble learning techniques](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Ensemble-learning-techniques.png?ssl=1)[*Source*](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)\nStacking can be considered to happen in the following steps:\n1. Split the data into a training and validation set,\n2. Divide the training set into K folds, for example 10,\n3. Train a base model (say SVM) on 9 folds and make predictions on the 10th fold,\n4. Repeat until you have a prediction for each fold,\n5. Fit the base model on the whole training set,\n6. Use the model to make predictions on the test set,\n7. Repeat step 3 &#8211; 6 for other base models (for example decision trees),\n8. Use predictions from the test set as features to a new model &#8211;*the meta-model,*\n9. Make final predictions on the test set using the meta model.\nWith regression problems, the values passed to the meta-model are numeric. With classification problems, they\u2019re probabilities or class labels.\n### Blending\nBlending is similar to stacking, but uses a holdout set from the training set to make predictions. So, predictions are done on the holdout set only. The predictions and holdout set are used to build a final model that makes predictions on the test set.\nYou can think of blending as a type of stacking, where the meta-model is trained on predictions made by the base model on the hold-out validation set.\nYou can consider the*blending*process to be:\n* Split the data into a test and validation set,\n* Fit base models on the validation set,\n* Make predictions on the validation and test set,\n* Use the validation set and its predictions to build a final model,\n* Make final predictions using this model.\nThe concept of blending[was made popular](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf)by the[Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize). The winning team used a blended solution to achieve a 10-fold performance improvement on Netflix\u2019s movie recommendation algorithm.\nAccording to this[Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/):\n> > \u201cBlending is a word introduced by the Netflix winners. It\u2019s very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use \u201cstacked ensembling\u201d and \u201cblending\u201d interchangeably.\n> > > > With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only.\u201d\n> ### Blending vs stacking\nBlending is simpler than stacking and prevents leakage of information in the model. The generalizers and the stackers use different datasets. However, blending uses less data and may lead to overfitting.\nCross-validation is more solid on stacking than blending. It\u2019s calculated over more folds, compared to using a small hold-out dataset in blending.\n### Bagging\nBagging takes random samples of data, builds learning algorithms, and uses the mean to find bagging probabilities. It\u2019s also called*bootstrap aggregating*. Bagging aggregates the res...",
      "url": "https://neptune.ai/blog/ensemble-learning-guide"
    },
    {
      "title": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023",
      "text": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\n[\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n18 min read\n\u00b7Apr 28, 2024\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\nListen\nShare\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\n**> Note:\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\n*> \u201cIt is estimated that 90% of the world\u2019s data was generated in the last two years alone\u201d\n*> by [> Amount of Data Created Daily (2023)\n](https://explodingtopics.com/blog/data-generated-per-day)\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\n## Methodology\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\nPress enter or click to view image in full size\n![]()\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\n## Key Discoveries of Text Data Competitions\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term \u201cquestion answering\u201d exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...",
      "url": "https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312"
    }
  ]
}