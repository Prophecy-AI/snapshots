{
  "query": "What text data augmentation techniques are most effective for improving text classification model performance in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Text data augmentation is a cornerstone of winning Kaggle text\u2011classification solutions because it expands limited training sets, reduces over\u2011fitting, and mimics the linguistic variability seen in real\u2011world data.  The most consistently effective tricks, as reported by a Kaggle master and validated in recent research, are the **Easy Data Augmentation (EDA)** operations \u2013 synonym replacement, random insertion, random swap, and random deletion \u2013 which are simple to implement yet give large gains, especially on small\u2011to\u2011medium datasets ([Neptune\u202fAI](https://neptune.ai/blog/data-augmentation-nlp),\u202f2023;\u202f[Wei & Zou\u202f2019](https://arxiv.org/abs/1901.11196)).  Building on EDA, practitioners add **back\u2011translation** (translate to another language and back) and **contextual word replacement** using pretrained language models (e.g., BERT, RoBERTa) to generate fluent paraphrases that preserve label semantics.  Word\u2011embedding\u2011based synonym swaps and character\u2011level noise (swap or delete characters) further diversify the corpus without breaking meaning ([DataCamp\u202f2024](https://www.datacamp.com/tutorial/complete-guide-data-augmentation)).  \n\nBeyond these \u201clightweight\u201d methods, recent Kaggle\u2011level experiments show that **strategic curriculum learning**\u2014introducing real examples first and then gradually mixing augmented ones\u2014can amplify the benefit of any augmentation technique.  The Modified Cyclical Curriculum Learning (MCCL) framework, evaluated across topic classification, sentiment analysis, and offensive\u2011language detection, consistently outperformed plain training ([ScienceDirect\u202f2024](https://www.sciencedirect.com/science/article/pii/S2949719124000190)).  For larger label\u2011rich tasks, **class\u2011conditional generative models** such as a VAE trained per class have been demonstrated to surpass other generative augmentations, delivering higher accuracy on binary classification benchmarks ([Piedboeuf\u202f2022](https://aclanthology.org/2022.coling-1.305)).  \n\nIn practice, Kaggle winners combine these approaches: start with EDA\u2011style word\u2011level edits, enrich with back\u2011translation or BERT\u2011based paraphrasing, apply a modest amount of character noise, and finally schedule the augmented data using a curriculum (or mixup) strategy.  This pipeline balances diversity and label fidelity, leading to the strongest performance gains reported across recent Kaggle text\u2011classification competitions ([Neptune\u202f2022](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).",
      "url": ""
    },
    {
      "title": "Data Augmentation in NLP: Best Practices From a Kaggle ...",
      "text": "Data Augmentation in NLP: Best Practices From a Kaggle Master[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Data Augmentation in NLP: Best Practices From a Kaggle Master\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)5 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Natural Language Processing](https://neptune.ai/blog/category/natural-language-processing)\nThere are many tasks in NLP from text classification to question answering but whatever you do the amount of data you have to train your model impacts the model performance heavily.\nWhat can you do to make your dataset larger?\nSimple option -&gt; Get more data :).\nBut acquiring and labeling additional observations can be an expensive and time-consuming process.\nWhat you can do instead?\nApply data augmentation to your text data.\nData augmentation techniques are used to**generate additional, synthetic data using the data you have.**Augmentation methods are super popular in computer vision applications but they are just as powerful for NLP.\nIn this article, we\u2019ll go through all the major data augmentation methods for NLP that you can use to increase the size of your textual dataset and improve your model performance.\n## Data augmentation for vision vs NLP\nIn computer vision applications data augmentations are done almost everywhere to get larger training data and make the model generalize better.\nThe main methods used involve:\n* cropping,\n* flipping,\n* zooming,\n* rotation,\n* noise injection,\n* and many others.\nIn**computer vision**, these transformations are**done on the go**using data generators. As a batch of data is fed to your neural network it is randomly transformed (augmented). You don\u2019t need to prepare anything before training.\nThis isn\u2019t the case with**NLP**, where data augmentation should be done carefully due to the grammatical structure of the text. The methods discussed here are used**before training.**A new augmented dataset is generated beforehand and later fed into data loaders to train the model.\n![](https://lh4.googleusercontent.com/nyCEre_Ywft5YHXeKTEmMR-kmSkySsTIH6x_U1jBbJ9CF_2Erbnz9oueyXFIH4PMHBxl1elzRSFIszU9xKJ7aveJ1sL489I3p_DXc5QWIOFP2W8wWWvBQ7x-QzRCT23R5XDi5Gm3)\n## Data augmentation methods\nIn this article, I will mainly focus on NLP data augmentation methods provided in the following projects:\n* Back translation.\n* [EDA (Easy Data Augmentation)](https://arxiv.org/abs/1901.11196).\n* [NLP Albumentation.](https://github.com/albumentations-team/albumentations)\n* [NLP Aug.](https://github.com/makcedward/nlpaug)\nSo, let\u2019s dive into each of them.\n### **Back translation**\nIn this method, we**translate the text data to some language and then translate it back to the original**language. This can help to generate textual data with different words while preserving the context of the text data.\nLanguage translations APIs like google translate, Bing, Yandex are used to perform the translation. For example, given the sentence:\n![](https://lh6.googleusercontent.com/x3ZAhTDLT1QVSD8gCdaBVMquM2dcYA15A-orfzXyTzhTP8m0ZKLXz_2NrJdWlTgWKRS7BimExM8RO9Ce_uVVVdRR29vGeP0VZdncDZY0GTwkctocQyYg7HK9VL5ay3QC4JhbSXBK)*Amit Chaudhary[&#8220;Back Translation for Text Augmentation with Google Sheets&#8221;](https://amitness.com/2020/02/back-translation-in-google-sheets/)*\nYou can see that the sentences are not the same but their content remains the same after the back-translation. If you want to try this method for a dataset you can use this[notebook](https://www.kaggle.com/miklgr500/how-to-use-translators-for-comments-translation)as reference.\n### **Easy Data Augmentation**\nEasy data augmentation uses traditional and very simple data augmentation methods. EDA consists of**four simple operations that do a surprisingly good job**of preventing overfitting and helping train more robust models.\n* **Synonym Replacement**\nRandomly choose*n*words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\nFor example, given the sentence:\n*This**article**will focus on summarizing data augmentation**techniques**in NLP.*\nThe method randomly selects n words (say two), the words*article*and*techniques*, and replaces them with*write-up*and*methods*respectively.\n*This**write-up**will focus on summarizing data augmentation**methods**in NLP.*\n* **Random Insertion**\nFind a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this*n*times.\nFor example, given the sentence:\n*This**article**will focus on summarizing data augmentation**techniques**in NLP.*\nThe method randomly selects n words (say two), the words*article*and*techniques*find the synonyms as*write-up*and*methods*respectively. Then these synonyms are inserted at a random position in the sentence.\n*This article will focus on**write-up**summarizing data augmentation techniques in NLP**methods**.*\n* **Random Swap**\nRandomly choose two words in the sentence and swap their positions. Do this*n*times.\nFor example, given the sentence\n*This**article**will focus on summarizing data augmentation**techniques**in NLP.*\nThe method randomly selects n words (say two), the words*article*and*techniques*and swaps them to create a new sentence.\n*This**techniques**will focus on summarizing data augmentation**article**in NLP.*\n* **Random Deletion**\nRandomly remove each word in the sentence with probability*p*.\nFor example, given the sentence\n*This**article**will focus on summarizing data augmentation**techniques**in NLP.*\nThe method selects n words (say two), the words*will*and*techniques*, and removes them from the sentence.\n*This****article**focus on summarizing data augmentation in NLP.*\nYou can go to this[repository](https://github.com/jasonwei20/eda_nlp)if you want to apply these techniques to your projects.\n### **NLP Albumentation**\nPreviously, we talked about differences between computer vision data augmentation and NLP data augmentation. But in this section, we will see how we can apply some of the ideas used in CV data augmentation in NLP.\nFor that, we will use the[Albumentations](https://github.com/albumentations-team/albumentations)package.\nLet\u2019s take a look at a couple of the techniques here.\n* **Shuffle Sentences Transform**\nIn this transformation, if the given text sample contains multiple sentences these sentences are shuffled to create a new sample.\nFor example:\n**text = &#8216;&lt;Sentence1&gt;. &lt;Sentence2&gt;. &lt;Sentence4&gt;. &lt;Sentence4&gt;. &lt;Sentence...",
      "url": "https://neptune.ai/blog/data-augmentation-nlp"
    },
    {
      "title": "Advancing NLP models with strategic text augmentation",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/natural-language-processing-journal\"><span><span></span></span></a></p><p><a href=\"https://www.sciencedirect.com/journal/natural-language-processing-journal/vol/7/suppl/C\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><h2>Abstract</h2><div><p>This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods. It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection. The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training. A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field. Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance. These results underscore the need for careful selection of augmentation techniques and sequencing strategies to optimize the balance between speed and quality improvement in various NLP tasks. The study concludes that the use of augmentation methods, especially in conjunction with MCCL, leads to improved results in various classification tasks, providing a foundation for future advances in text augmentation strategies in NLP.</p></div></div><ul><li></li><li></li></ul><div><h2>Keywords</h2><p><span>Text augmentation</span></p><p><span>Data augmentation</span></p><p><span>Curriculum learning</span></p><p><span>Text classification</span></p><p><span>Sample ordering</span></p></div><section><header><h2>Cited by (0)</h2></header></section><div><dl><dt><a href=\"#baep-article-footnote-id1\"><span><span><sup>\u2606</sup></span></span></a></dt><dd><p>This study was supported by the <span>Scientific and Technological Research Council of Turkey</span>\n(TUBITAK) Grant No: <a href=\"#GS1\"><span><span>120E100</span></span></a>.</p></dd></dl></div><p><span>\u00a9 2024 The Author(s). Published by Elsevier B.V.</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S2949719124000190"
    },
    {
      "title": "A Complete Guide to Data Augmentation",
      "text": "A Complete Guide to Data Augmentation | DataCamp\n[Skip to main content](#main)\n# A Complete Guide to Data Augmentation\nLearn about data augmentation techniques, applications, and tools with a TensorFlow and Keras tutorial.\nListContents\nUpdated Dec 9, 2024\u00b7 15 min read\nContents\n## GroupTraining more people?\nGet your team access to the full DataCamp for business platform.\n## What is Data Augmentation?\nData augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It includes making minor changes to the dataset or using deep learning to generate new data points.\n### Augmented vs. synthetic data\nData augmentation and synthetic data generation are distinct yet complementary techniques in machine learning:\n* **Augmented data**: This involves creating modified versions of existing data to increase dataset diversity. For example, in image processing, applying transformations like rotations, flips, or color adjustments to existing images can help models generalize better.\n* **Synthetic data**: Thisrefers to artificially generated data, which allows researchers and developers to test and improve algorithms without risking the privacy or security of real-world data.\n**Note**: augmentation techniques are not limited to images. You can augment audio, video, text, and other types of data too.\n### Why is data augmentation important?\nData augmentation helps machine learning models perform better by making the most of existing data. It prevents overfitting, improves accuracy, and creates diversity in training data, which is crucial when datasets are small or imbalanced. By simulating real-world variations, it makes models more robust and reliable\u2014without the need for expensive data collection. In short, it\u2019s a simple, powerful way to help models learn and generalize effectively.\n### When should you use data augmentation?\n1. To prevent models from overfitting.\n2. The initial training set is too small.\n3. To improve the model accuracy.\n4. To Reduce the operational cost of labeling and cleaning the raw dataset.### Limitations of data augmentation\n* The biases in the original dataset persist in the augmented data.\n* Quality assurance for data augmentation is expensive.\n* Research and development are required to build a system with advanced applications. For example, generating high-resolution images using GANs can be challenging.\n* Finding an effective data augmentation approach can be challenging.\n## Become a ML Scientist\nMaster Python skills to become a machine learning scientist\n[Start Learning for Free](https://www.datacamp.com/tracks/machine-learning-scientist-with-python)\n## Data Augmentation Techniques\nIn this section, we will learn about audio, text, image, and advanced data augmentation techniques.\n### Audio Data Augmentation\n1. **Noise injection**: add gaussian or random noise to the audio dataset to improve the model performance.\n2. **Shifting**: shift audio left (fast forward) or right with random seconds.\n3. **Changing the speed**: stretches times series by a fixed rate.\n4. **Changing the pitch**: randomly change the pitch of the audio.### Text Data Augmentation\n1. **Word or sentence shuffling**: randomly changing the position of a word or sentence.\n2. **Word replacement**: replace words with synonyms.\n3. **Syntax-tree manipulation**: paraphrase the sentence using the same word.\n4. **Random word insertion**: inserts words at random.\n5. **Random word deletion**: deletes words at random.### Image Augmentation\n***Learn more about image transformation and manipulation with hands-on exercises in our[Image Processing with Python skill track](https://www.datacamp.com/tracks/image-processing).***\n1. **Geometric transformations**: randomly flip, crop, rotate, stretch, and zoom images. You need to be careful about applying multiple transformations on the same images, as this can reduce model performance.\n2. **Color space transformations**: randomly change RGB color channels, contrast, and brightness.\n3. **Kernel filters**: randomly change the sharpness or blurring of the image.\n4. **Random erasing**: delete some part of the initial image.\n5. **Mixing images**: blending and mixing multiple images.### Advanced Techniques\n1. **Generative adversarial networks**(GANs): used to generate new data points or images. It does not require existing data to generate synthetic data.\n2. **Neural Style Transfer**: a series of convolutional layers trained to deconstruct images and separate context and style.## Data Augmentation Applications\nData augmentation can apply to all machine learning applications where acquiring quality data is challenging. Furthermore, it can help improve model robustness and performance across all fields of study.\n### Healthcare\nAcquiring and labeling medical imaging datasets is time-consuming and expensive. You also need a subject matter expert to validate the dataset before performing data analysis. Using geometric and other transformations can help you train robust and accurate machine-learning models.\nFor example, in the case of Pneumonia Classification, you can use random cropping, zooming, stretching, and color space transformation to improve the model performance. However, you need to be careful about certain augmentations as they can result in opposite results. For example, random rotation and reflection along the x-axis are not recommended for the X-ray imaging dataset.\n![kaggle-COVID19-Classification.png](https://images.datacamp.com/image/upload/v1669207732/kaggle_COVID_19_Classification_10703dbe61.png)\nImage from[ibrahimsobh.github.io](https://ibrahimsobh.github.io/kaggle-COVID19-Classification/)| kaggle-COVID19-Classification\n### Self-Driving Cars\nThere is limited data available on self-driving cars, and companies are using simulated environments to generate synthetic data using reinforcement learning. It can help you train and test machine learning applications where data security is an issue.\n![Autonomous Visualization System from Uber ATG.png](https://images.datacamp.com/image/upload/v1669207732/Autonomous_Visualization_System_from_Uber_ATG_9e79d6c630.png)\nImage by[David Silver](https://medium.com/self-driving-cars/autonomous-visualization-system-from-uber-atg-50957e7101f0)| Autonomous Visualization System from Uber ATG\nThe possibilities of augmented data as a simulation are endless, as it can be used to generate real-world scenarios.\n### Natural Language Processing\nText data augmentation is generally used in situations with limited quality data, and improving the performance metric takes priority. You can apply synonym augmentation, word embedding, character swap, and random insertion and deletion. These techniques are also valuable for low-resource languages.\n![Selective Text Augmentation with Word Roles for Low-Resource Text Classification.png](https://images.datacamp.com/image/upload/v1669207732/Selective_Text_Augmentation_with_Word_Roles_for_Low_Resource_Text_Classification_c1302878c9.png)\nImage from[Papers With Code](https://paperswithcode.com/paper/selective-text-augmentation-with-word-roles)| Selective Text Augmentation with Word Roles for Low-Resource Text Classification.\nResearchers use text augmentation for the language models in high error recognition scenarios, sequence-to-sequence data generation, and text classification.\n### Automatic Speech Recognition\nIn sound classification and speech recognition, data augmentation works wonders. It improves the model performance even on low-resource languages.\n![Noise Injection.png](https://images.datacamp.com/image/upload/v1669207728/Noise_Injection_b3af64a578.png)\nImage by[Edward Ma](https://medium.com/@makcedward/data-augmentation-for-audio-76912b01fdf6)| Noise Injection\nThe random noise injection, shifting, and changing the pitch can help you produce state-of-the-art speech-to-text models. You can also use GANs to generate realistic sounds for a particular application.\n## Ethical Implications of Data Augmentatio...",
      "url": "https://www.datacamp.com/tutorial/complete-guide-data-augmentation"
    },
    {
      "title": "Effective Data Augmentation for Sentence Classification ...",
      "text": "## [Effective Data Augmentation for Sentence Classification Using One VAE per Class](https://aclanthology.org/2022.coling-1.305.pdf)\n[Fr\u00e9d\u00e9ric Piedboeuf](https://aclanthology.org/people/f/frederic-piedboeuf/),\n[Philippe Langlais](https://aclanthology.org/people/p/philippe-langlais/)\n* * *\n##### Abstract\nIn recent years, data augmentation has become an important field of machine learning. While images can use simple techniques such as cropping or rotating, textual data augmentation needs more complex manipulations to ensure that the generated examples are useful. Variational auto-encoders (VAE) and its conditional variant the Conditional-VAE (CVAE) are often used to generate new textual data, both relying on a good enough training of the generator so that it doesn\u2019t create examples of the wrong class. In this paper, we explore a simpler way to use VAE for data augmentation: the training of one VAE per class. We show on several dataset sizes, as well as on four different binary classification tasks, that it systematically outperforms other generative data augmentation techniques.\nAnthology ID:2022.coling-1.305Volume:[Proceedings of the 29th International Conference on Computational Linguistics](https://aclanthology.org/volumes/2022.coling-1/)Month:OctoberYear:2022Address:Gyeongju, Republic of KoreaEditors:[Nicoletta Calzolari](https://aclanthology.org/people/n/nicoletta-calzolari/),\n[Chu-Ren Huang](https://aclanthology.org/people/c/chu-ren-huang/),\n[Hansaem Kim](https://aclanthology.org/people/h/hansaem-kim/),\n[James Pustejovsky](https://aclanthology.org/people/j/james-pustejovsky/),\n[Leo Wanner](https://aclanthology.org/people/l/leo-wanner/),\n[Key-Sun Choi](https://aclanthology.org/people/k/key-sun-choi/),\n[Pum-Mo Ryu](https://aclanthology.org/people/p/pum-mo-ryu/),\n[Hsin-Hsi Chen](https://aclanthology.org/people/h/hsin-hsi-chen/),\n[Lucia Donatelli](https://aclanthology.org/people/l/lucia-donatelli/),\n[Heng Ji](https://aclanthology.org/people/h/heng-ji/),\n[Sadao Kurohashi](https://aclanthology.org/people/s/sadao-kurohashi/),\n[Patrizia Paggio](https://aclanthology.org/people/p/patrizia-paggio/),\n[Nianwen Xue](https://aclanthology.org/people/n/nianwen-xue/),\n[Seokhwan Kim](https://aclanthology.org/people/s/seokhwan-kim/),\n[Younggyun Hahm](https://aclanthology.org/people/y/younggyun-hahm/),\n[Zhong He](https://aclanthology.org/people/z/zhong-he/),\n[Tony Kyungil Lee](https://aclanthology.org/people/t/tony-kyungil-lee/),\n[Enrico Santus](https://aclanthology.org/people/e/enrico-santus/),\n[Francis Bond](https://aclanthology.org/people/f/francis-bond/),\n[Seung-Hoon Na](https://aclanthology.org/people/s/seung-hoon-na/)Venue:[COLING](https://aclanthology.org/venues/coling/)SIG:Publisher:International Committee on Computational LinguisticsNote:Pages:3454\u20133464Language:URL:[https://aclanthology.org/2022.coling-1.305/](https://aclanthology.org/2022.coling-1.305/)DOI:Bibkey:piedboeuf-langlais-2022-effectiveCite (ACL):Fr\u00e9d\u00e9ric Piedboeuf and Philippe Langlais. 2022. [Effective Data Augmentation for Sentence Classification Using One VAE per Class](https://aclanthology.org/2022.coling-1.305/). In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 3454\u20133464, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.Cite (Informal):[Effective Data Augmentation for Sentence Classification Using One VAE per Class](https://aclanthology.org/2022.coling-1.305/) (Piedboeuf & Langlais, COLING 2022)Copy Citation:BibTeXMarkdownMODS XMLEndnoteMore options\u2026PDF:[https://aclanthology.org/2022.coling-1.305.pdf](https://aclanthology.org/2022.coling-1.305.pdf)Data[SST](https://paperswithcode.com/dataset/sst),\u00a0[SST-2](https://paperswithcode.com/dataset/sst-2)\n[PDF](https://aclanthology.org/2022.coling-1.305.pdf) [Cite](https://aclanthology.org/2022.coling-1.305/) [Search](https://www.semanticscholar.org/search?q=Effective+Data+Augmentation+for+Sentence+Classification+Using+One+VAE+per+Class) [Fix data](https://aclanthology.org/2022.coling-1.305/)\n* * *",
      "url": "https://aclanthology.org/2022.coling-1.305"
    },
    {
      "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
      "text": "# Computer Science > Computation and Language\n\n**arXiv:1901.11196** (cs)\n\n\\[Submitted on 31 Jan 2019 ( [v1](https://arxiv.org/abs/1901.11196v1)), last revised 25 Aug 2019 (this version, v2)\\]\n\n# Title:EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\n\nAuthors: [Jason Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+J), [Kai Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou,+K)\n\nView a PDF of the paper titled EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, by Jason Wei and Kai Zou\n\n[View PDF](https://arxiv.org/pdf/1901.11196)\n\n> Abstract:We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.\n\n|     |     |\n| --- | --- |\n| Comments: | EMNLP-IJCNLP 2019 short paper |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:1901.11196](https://arxiv.org/abs/1901.11196) \\[cs.CL\\] |\n| (or [arXiv:1901.11196v2](https://arxiv.org/abs/1901.11196v2) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1901.11196](https://doi.org/10.48550/arXiv.1901.11196) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jason Wei \\[ [view email](https://arxiv.org/show-email/7ee5aa67/1901.11196)\\] **[\\[v1\\]](https://arxiv.org/abs/1901.11196v1)**\nThu, 31 Jan 2019 03:20:52 UTC (294 KB)\n**\\[v2\\]**\nSun, 25 Aug 2019 23:11:07 UTC (63 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks, by Jason Wei and Kai Zou\n\n- [View PDF](https://arxiv.org/pdf/1901.11196)\n- [TeX Source](https://arxiv.org/src/1901.11196)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1901.11196&function=prev&context=cs.CL)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1901.11196&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2019-01](https://arxiv.org/list/cs.CL/2019-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1901.11196?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1901.11196)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1901.11196)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1901.11196)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1901.html#abs-1901-11196) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1901-11196)\n\n[Jason W. Wei](https://dblp.uni-trier.de/search/author?author=Jason%20W.%20Wei) [Kai Zou](https://dblp.uni-trier.de/search/author?author=Kai%20Zou)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1901.11196) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/1901.11196"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "Text augmentation techniques in NLP",
      "text": "- [NLP Tutorial](https://www.geeksforgeeks.org/nlp/natural-language-processing-nlp-tutorial/)\n- [Libraries](https://www.geeksforgeeks.org/nlp/nlp-libraries-in-python/)\n- [Phases](https://www.geeksforgeeks.org/machine-learning/phases-of-natural-language-processing-nlp/)\n- [Text Preprosessing](https://www.geeksforgeeks.org/nlp/text-preprocessing-for-nlp-tasks/)\n- [Tokenization](https://www.geeksforgeeks.org/nlp/nlp-how-tokenizing-text-sentence-words-works/)\n- [Lemmatization](https://www.geeksforgeeks.org/python/python-lemmatization-with-nltk/)\n- [Word Embeddings](https://www.geeksforgeeks.org/nlp/word-embeddings-in-nlp/)\n- [Projects Ideas](https://www.geeksforgeeks.org/nlp/top-natural-language-processing-nlp-projects/)\n- [Interview Question](https://www.geeksforgeeks.org/nlp/advanced-natural-language-processing-interview-question/)\n- [NLP Quiz](https://www.geeksforgeeks.org/quizzes/natural-language-processing-quiz/)\n- [NLP Pipeline](https://www.geeksforgeeks.org/nlp/natural-language-processing-nlp-pipeline/)\n- [DL for NLP](https://www.geeksforgeeks.org/nlp/nlp-with-deep-learning/)\n- [NLP Datasets](https://www.geeksforgeeks.org/nlp/best-nlp-datasets/)\n- [Machine Learning](https://www.geeksforgeeks.org/machine-learning/machine-learning/)\n\nSign In\n\n\u25b2\n\n[Open In App](https://geeksforgeeksapp.page.link/?link=https://www.geeksforgeeks.org/text-augmentation-techniques-in-nlp/?type%3Darticle%26id%3D997649&apn=free.programming.programming&isi=1641848816&ibi=org.geeksforgeeks.GeeksforGeeksDev&efr=1)\n\n# Text augmentation techniques in NLP\n\nLast Updated : 23 Jul, 2025\n\nComments\n\nImprove\n\nSuggest changes\n\n7 Likes\n\nLike\n\nReport\n\n**Text augmentation** is an important aspect of **NLP** to generate an artificial corpus. This helps in improving the NLP-based models to generalize better over a lot of different sub-tasks like intent classification, machine translation, chatbot training, image summarization, etc.\n\nText augmentation is used when:\n\n- There is an absence of sufficient variation in the text corpus.\n- There is a high data imbalance during intent classification tasks.\n- The overall quantity of data is insufficient for data-hungry machine-learning models.\n\nTable of Content\n\n- [Data Augmentation in NLP](https://www.geeksforgeeks.org/www.geeksforgeeks.org#data-augmentation-in-nlp)\n- [Easy data Augmentation](https://www.geeksforgeeks.org/www.geeksforgeeks.org#easy-data-augmentation)\n- [Generative Models](https://www.geeksforgeeks.org/www.geeksforgeeks.org#generative-models)\n\n## Data Augmentation in NLP\n\n[Data Augmentation](https://www.geeksforgeeks.org/machine-learning/python-data-augmentation/) (DA) is a technique employed to artificially expand training datasets by creating various versions of existing data without the need for additional data collection. Its primary goal is to enhance classification task performance by altering the data while preserving class categories.\n\nIn fields like [Computer Vision](https://www.geeksforgeeks.org/computer-vision/computer-vision/) and [Natural Language Processing](https://www.geeksforgeeks.org/nlp/natural-language-processing-overview/) (NLP), where data scarcity and limited diversity pose challenges, data augmentation strategies play a crucial role. While generating augmented images is relatively straightforward, NLP faces complexities due to the nuanced nature of language. Unlike images, we can't replace every word with a synonym, and even if substitution is possible, maintaining context becomes a significant challenge.\n\nThe pivotal role of data augmentation lies in its ability to boost model performance by increasing the volume of training data. More data generally leads to improved model performance. However, it is crucial to strike a balance in the distribution of augmented data. The generated data should neither closely resemble the original data nor deviate too much from it, as either extreme may result in overfitting or poor model performance. Therefore, effective data augmentation approaches should aim for a harmonious balance in data distribution.\n\nThe following three stages are where data augmentation techniques are used:\n\n- **Word Level:** At the word level, augmentation involves the transformation or replacement of individual words within the text. This could include synonyms, word shuffling, or other word-level modifications aimed at introducing variability without compromising the overall meaning or context of the text.\n- **Sentence Level:** [Augmentation](https://www.geeksforgeeks.org/machine-learning/text-to-text-transfer-transformer-in-data-augmentation/) at the sentence level focuses on altering the structure and composition of entire sentences. Techniques such as paraphrasing, sentence shuffling, or introducing grammatical variations are employed. The goal is to diversify the dataset by presenting the model with different formulations of ideas while maintaining the essence of the original content.\n- **Document Level:** At the document level, augmentation extends to the entire document or piece of text. This may involve introducing substantial changes, such as inserting or removing paragraphs, reordering sections, or even changing the overall writing style. Document-level augmentation aims to provide the model with exposure to varied document structures and writing conventions.\n\n## Easy data Augmentation\n\n### Synonym Replacement\n\nSynonym replacement is a technique to replace a set of [token](https://www.geeksforgeeks.org/ethical-hacking/how-does-the-token-based-authentication-work/)(s) from a text/document with another set of token(s) with equal or equivalent meaning without changing the overall meaning or context of the original phrase. The sentence generated post-replacement is called a synthetic phrase.\n\nThe synonym replacement can occur at any or all three of a document, i.e. at the character level, word/token level, or sentence level.\n\n#### **Word-embedding based**\n\n[Word embeddings](https://www.geeksforgeeks.org/nlp/word-embeddings-in-nlp/) that have been trained, such [GloVe](https://www.geeksforgeeks.org/nlp/pre-trained-word-embedding-using-glove-in-nlp-models/), [Word2Vec](https://www.geeksforgeeks.org/python/python-word-embedding-using-word2vec/), and fastText, can find the word vector that is closest to the original text in the embedding space.\n\nBecause contextual bidirectional embeddings such as ELMo and [BERT](https://www.geeksforgeeks.org/nlp/explanation-of-bert-model-nlp/) have substantially richer vector representations, they are recommended for improved reliability. Bidirectional [LSTM](https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/) and Transformer models are more successful since they can encode longer text sequences and show contextual knowledge of the words around them.\n\n#### **Lexical-based**\n\nToken(s) with close or exact meaning is replaced directly in the string to achieve lexical-based augmented text. [Wordnet](https://www.geeksforgeeks.org/nlp/nlp-wordnet-for-tagging/) is used to tag and find the respective synonyms, hyponyms, and meronyms for the token(s) in the text.\n\n- **Random Insertion**\n\nRandomly inserting redundant or semantically similar words/tokens that do not include [stopwords](https://www.geeksforgeeks.org/nlp/removing-stop-words-nltk-python/) increase the meaning of a sentence/embedding or keep it semantically unchanged resulting in the creation of augmented text. This step should be done carefully as the introduction of negative words in a sentence can change its sentiment completely.\n\n```\noriginal text: This is a geekforgeeks example.post-random insertion text: This is a new geekforgeeks example.\n```\n\n- **Random Swap**\n\nRandomly replacing similar words/tokens that keep the semantic meaning of the sentence/embedding unchanged results in the creation of augmented text.\n\n```\noriginal text: This is a geekforgeeks example.post-random swapped text: This is a geekforgeeks illustration.\n```\n\n- **Random deletion**\n\nRandom del...",
      "url": "https://www.geeksforgeeks.org/nlp/text-augmentation-techniques-in-nlp"
    },
    {
      "title": "Text Data Augmentation for Large Language Models",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/html/2501.18845v1"
    }
  ]
}