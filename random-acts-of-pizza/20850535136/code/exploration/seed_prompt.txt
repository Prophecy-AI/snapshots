## Competition Overview
This is a binary classification problem combining text and tabular features. See `exploration/eda.ipynb` for detailed data characteristics including feature distributions, missing values, class balance analysis, and correlation patterns.

## Core Modeling Strategy

### Multimodal Approach (Text + Tabular)
Based on winning Kaggle solutions for similar problems, the most effective approach is a three-step pipeline:

1. **Extract text embeddings** using pretrained transformers (BERT, RoBERTa, or DeBERTa)
   - Use AutoGluon Tabular's TextPredictor for automatic text encoding
   - Alternatively, use Hugging Face transformers to generate sentence embeddings
   - GPU acceleration is critical for reasonable training time

2. **Combine embeddings with tabular features**
   - Concatenate text embeddings with numeric/categorical features (two-stage approach)
   - OR convert tabular features to text and prepend to review (features-to-text trick)
   - Target-encode high-cardinality categorical features before merging

3. **Train strong tabular models on combined features**
   - LightGBM, CatBoost, and XGBoost are the workhorses
   - Neural networks can be effective for larger datasets
   - Use libraries like Multimodal-Toolkit for seamless integration

### Model Architecture Options
- **Two-stage**: Text encoder → embeddings → tabular model
- **Features-to-text**: Convert all features to text → single transformer
- **Hybrid ensemble**: Combine both approaches with different weighting

## Handling Class Imbalance (75/25 split)

### Resampling Strategies
- **Undersample majority class** to balance classes (risk: information loss)
- **Oversample minority class** using SMOTE or ADASYN
- **Combine both**: SMOTE + Tomek links for cleaning
- **Apply resampling only to training folds** in cross-validation

### Alternative Approaches (No Resampling)
- **Class weights**: Set `scale_pos_weight` in XGBoost/LightGBM or `class_weight` in sklearn
- **Focal loss**: Penalize hard examples more heavily
- **Threshold adjustment**: Optimize decision threshold post-training using ROC/PR curves

### Validation Considerations
- **Always use stratified K-fold** to preserve class ratios in each fold
- **Monitor minority class metrics**: AUC-ROC, PR-AUC, F1-score (not accuracy)
- **Check for distribution shift** between train and test sets

## Feature Engineering

### Text Features
- **Basic**: Length, word count, character count, sentence count
- **Sentiment**: Use VADER or TextBlob for sentiment scores
- **TF-IDF**: Extract top terms and their statistics
- **Topic modeling**: LDA or NMF for latent topics
- **Entity extraction**: Count mentions of products, brands, locations

### Tabular Features
- **Target encoding**: For high-cardinality categoricals (use cross-validation)
- **Interaction features**: Polynomial features between numeric columns
- **Aggregation features**: Group by categorical columns and compute statistics
- **Missing value patterns**: Create indicator for missing data patterns
- **Cyclical encoding**: If any temporal features exist

### Cross-modal Features
- **Text-tabular interactions**: Multiply text sentiment scores with numeric features
- **Feature importance alignment**: Ensure text and tabular features are on similar scales

## Model Selection and Training

### Primary Models (Tier 1)
1. **LightGBM**: Fast, handles categorical features well
2. **CatBoost**: Excellent for categorical data, robust to overfitting
3. **XGBoost**: Reliable baseline, good performance

### Secondary Models (Tier 2)
1. **Neural Networks**: For capturing non-linear interactions
2. **Random Forest**: For diversity in ensembles
3. **Logistic Regression**: With polynomial features for baseline

### Training Configuration
- **Use early stopping** with patience of 50-100 rounds
- **Set class weights**: `scale_pos_weight = (negative_samples / positive_samples) * k` where k ∈ [0.5, 2]
- **Hyperparameter ranges**:
  - Learning rate: 0.01 - 0.1
  - Max depth: 3 - 10
  - Num leaves: 31 - 255
  - Subsample: 0.6 - 0.9
  - Colsample: 0.6 - 0.9

## Ensembling Strategy

### Model Diversity
- Combine different architectures (GBM + NN + Transformer)
- Use different feature sets (text-only, tabular-only, combined)
- Vary preprocessing pipelines (different encoding strategies)

### Ensemble Methods
1. **Weighted averaging**: Weight by validation performance
2. **Stacking**: Train meta-learner on out-of-fold predictions
3. **Rank averaging**: Average predictions after converting to ranks
4. **Optimal blending**: Use linear regression to find optimal weights

### Implementation Order
1. Build 3-5 diverse single models
2. Create level-1 ensemble with simple averaging
3. Add stacking with logistic regression or LightGBM
4. Fine-tune ensemble weights using validation set

## Validation Strategy

### Cross-Validation Setup
- **Stratified K-Fold**: k=5 or k=10
- **GroupKFold**: If there's a grouping column (e.g., user_id)
- **TimeSeriesSplit**: If temporal patterns exist (check eda.ipynb)

### Evaluation Metrics
- **Primary**: AUC-ROC (robust to class imbalance)
- **Secondary**: PR-AUC, F1-score, Log Loss
- **Never use**: Accuracy (misleading with imbalance)

### Validation Best Practices
- **Create holdout set** before cross-validation for final evaluation
- **Monitor OOF (out-of-fold) predictions** for consistency
- **Check prediction distributions** across folds
- **Analyze errors**: Focus on minority class misclassifications

## Optimization and Tuning

### Hyperparameter Search
- **Start with broad search**: Use Optuna or Hyperopt
- **Focus on key params**: Learning rate, max depth, subsample
- **Use successive halving**: Reduce search space iteratively
- **Leverage GPU**: For neural network components

### Feature Selection
- **Permutation importance**: On validation set
- **SHAP values**: For model interpretation
- **Recursive feature elimination**: For dimensionality reduction
- **Correlation analysis**: Remove highly correlated features

### Post-Training Optimization
- **Threshold tuning**: Optimize for competition metric
- **Calibration**: Use Platt scaling or isotonic regression
- **Pseudo-labeling**: If confident in predictions on test set

## Implementation Roadmap

### Phase 1: Baselines (Quick)
1. TF-IDF + Logistic Regression
2. Tabular-only LightGBM
3. Text-only BERT fine-tuning

### Phase 2: Core Models (1-2 days)
1. Multimodal pipeline (text embeddings + tabular)
2. 3-5 diverse models with different architectures
3. Basic ensemble (averaging)

### Phase 3: Refinement (2-3 days)
1. Advanced feature engineering
2. Hyperparameter optimization
3. Stacking ensemble
4. Threshold optimization

### Phase 4: Final Push (1 day)
1. Error analysis and targeted improvements
2. Model blending and weight tuning
3. Validation consistency checks
4. Submission preparation

## Key Libraries and Tools

### Essential Libraries
- **AutoGluon**: For automatic multimodal modeling
- **Multimodal-Toolkit**: For transformer + tabular integration
- **LightGBM/CatBoost/XGBoost**: Gradient boosting
- **Transformers**: Hugging Face for text encoding
- **Optuna**: Hyperparameter optimization

### Utility Libraries
- **imbalanced-learn**: For resampling techniques
- **category_encoders**: For target encoding
- **SHAP**: For model interpretation
- **tqdm**: For progress tracking

## Common Pitfalls to Avoid

1. **Data leakage**: Never use test set statistics for training
2. **Overfitting to validation**: Use multiple validation strategies
3. **Ignoring minority class**: Focus metrics and analysis on positive class
4. **Text preprocessing errors**: Preserve semantic meaning
5. **Scale mismatch**: Ensure text embeddings and tabular features compatible
6. **Ensemble correlation**: Ensure model diversity, not just quantity

## Expected Performance Targets

Based on similar competitions:
- **Single model**: 0.75-0.85 AUC
- **Simple ensemble**: 0.80-0.88 AUC
- **Advanced ensemble**: 0.85-0.92 AUC
- **Top solutions**: 0.90+ AUC (requires significant feature engineering)

## References and Resources

- AutoGluon multimodal tutorial: Combining BERT with tabular models
- Multimodal-Toolkit: GitHub repository with examples
- Kaggle winning solutions: Mercari Price Suggestion, Home Credit Default Risk
- Class imbalance handling: SMOTE, ADASYN, class weights
- Feature engineering: Target encoding, interaction features