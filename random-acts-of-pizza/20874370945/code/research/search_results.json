{
  "query": "What text preprocessing and feature engineering techniques work best for Reddit posts and social media text classification on Kaggle?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Text classification on Reddit and other social\u2011media platforms benefits from a preprocessing pipeline that first normalises the highly informal, noisy language and then extracts both lexical and meta\u2011features that capture the platform\u2011specific signals.  A common first step is to strip or replace URLs, user mentions (`u/\u2026`), subreddit tags (`r/\u2026`), HTML entities and markdown, then lower\u2011case the text and expand common contractions.  Specialized tokenisers such as **CrazyTokenizer** from the\u202fRedditScore library provide Reddit\u2011 and Twitter\u2011aware options for handling hashtags, emojis, and emoticons out\u2011of\u2011the\u2011box, which helps preserve sentiment\u2011bearing symbols while still producing clean tokens\u202f([redditscore](https://redditscore.readthedocs.io/en/master)).  After tokenisation, most Kaggle solutions apply standard NLP cleaning \u2013 stop\u2011word removal, optional lemmatisation or stemming, and removal of very short or non\u2011alphabetic tokens \u2013 as demonstrated in the Reddit\u2011Classification project that achieved\u202f\u2248\u202f60\u202f% accuracy using these steps\u202f([nikhilpodila](https://github.com/nikhilpodila/Reddit-Classification)).\n\nFor feature engineering, the strongest baselines on Kaggle still rely on **TF\u2011IDF weighted word\u2011 and character\u2011level n\u2011grams** (unigrams\u2011to\u2011trigrams) combined with **\u03c7\u00b2 feature selection** to prune the high\u2011dimensional space, a strategy explicitly reported in the same Reddit\u2011Classification repository\u202f([nikhilpodila](https://github.com/nikhilpodila/Reddit-Classification)).  Complementary count\u2011based vectors (Bag\u2011of\u2011Words) and fastText or word2vec embeddings are frequently added to capture semantic similarity, as shown in the multi\u2011model Reddit/Twitter sentiment analysis repo that experiments with count\u2011based methods, LSTMs and Transformer pipelines\u202f([Collin\u2011Li\u2011DataScience](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis)).  Kaggle competitors also enrich the text with **metadata features** \u2013 subreddit name, post length, number of up\u2011votes, punctuation or capital\u2011letter ratios \u2013 which often boost performance when concatenated with the vectorised text.\n\nFinally, the \u201cPreprocessing Social Media Text\u201d tutorial highlights the importance of handling platform\u2011specific artefacts (hashtags, mentions, emojis) and provides ready\u2011to\u2011use Python snippets (e.g.,\u202f`nltk`,\u202f`spaCy`,\u202f`emoji` libraries) for experimenting with these transformations on Reddit data\u202f([sicss.io](https://sicss.io/overview/preprocessing-social-media-text)).  Combining robust cleaning, Reddit\u2011aware tokenisation, TF\u2011IDF/\u03c7\u00b2\u2011selected n\u2011grams, dense embeddings, and engineered metadata has proven to be the most effective recipe for high\u2011scoring Kaggle text\u2011classification models on Reddit and other social\u2011media corpora.",
      "url": ""
    },
    {
      "title": "GitHub - Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis: (Kaggle) Performed Natural Language Processing (NLP)-based sentiment analysis on Reddit and Twitter data using count-based methods, LSTMs, and Transformers pipelines and determined use cases",
      "text": "[Skip to content](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[Collin-Li-DataScience](https://github.com/Collin-Li-DataScience)/ **[NLP-Kaggle\\_Reddit\\_Twitter\\_Sentiment\\_Analysis](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FCollin-Li-DataScience%2FNLP-Kaggle_Reddit_Twitter_Sentiment_Analysis) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2FCollin-Li-DataScience%2FNLP-Kaggle_Reddit_Twitter_Sentiment_Analysis)\n- [Star\\\n1](https://github.com/login?return_to=%2FCollin-Li-DataScience%2FNLP-Kaggle_Reddit_Twitter_Sentiment_Analysis)\n\n\n(Kaggle) Performed Natural Language Processing (NLP)-based sentiment analysis on Reddit and Twitter data using count-based methods, LSTMs, and Transformers pipelines and determined use cases\n\n[1\\\nstar](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/stargazers) [0\\\nforks](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/forks) [Branches](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/branches) [Tags](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/tags) [Activity](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/activity)\n\n[Star](https://github.com/login?return_to=%2FCollin-Li-DataScience%2FNLP-Kaggle_Reddit_Twitter_Sentiment_Analysis)\n\n[Notifications](https://github.com/login?return_to=%2FCollin-Li-DataScience%2FNLP-Kaggle_Reddit_Twitter_Sentiment_Analysis) You must be signed in to change notification settings\n\n# Collin-Li-DataScience/NLP-Kaggle\\_Reddit\\_Twitter\\_Sentiment\\_Analysis\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/branches) [Tags](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[3 Commits](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/commits/main/) |\n| [NLP-Kaggle\\_Reddit\\_Twitter\\_Sentiment\\_Analysis.ipynb](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/blob/main/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis.ipynb) | [NLP-Kaggle\\_Reddit\\_Twitter\\_Sentiment\\_Analysis.ipynb](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/blob/main/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis.ipynb) |  |  |\n| [README.md](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/blob/main/README.md) | [README.md](https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis/blob/main/README.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Sentiment Analysis with Count-based Methods, LSTM, and Transformer-based Method\n\n**Author** : Mucong (Collin) Li\n\n**Time Completed**: December 2020\n\n## Technical\n\n**Method**:\n\n- Scikit-Learn: CountVectorizer; TfidfVectorizer\n- NLTK (Natural Language Toolkit)\n- Keras: LSTM\n- TensorFlow\n- Pipelines from the Hugginface Library for Transformer-based methods\n\n**Language**: Python (notebook)\n\n**Data**:\n\n- Twitter\\_Data: The File Contains 163k tweets along with its Sentimental Labelling\n- Reddit\\_Data: The File Contains 37k comments along with its Sentimental Labelling\n- All the Comments in the dataset are cleaned and assigned with a Sentimet L Label Using Textblob\n\n## Takeaway\n\n### Approach 1: Count-based Methods\n\n#### CountVectorizer vs. TFIDFVectorizer:\n\n**Difference between the two models? Which one is better and why?**\n\n- The CountVectorizer converts the text documents to a matrix of token counts, which record the number of time that certain word appears\n- The TfidfVectorizer converts the text documents to a matrix of TF-IDF features, which record the number of time that certain word appears, but normalized by total times that it appears in the document, and this makes word such as \u2018the\u2019, \u2018a\u2019, \u2018and\u2019 less important for the model\n- For Reddit, TFIDF is better\n- For Twitter, Count is better\n- This is actually NOT what I expected as I thought TFIDF would be better in all scenarios.\n- After thinking about it, I think this happens because:\n- On Twitter, many people post similar contents on the same topic. If a certain topic has a strong emotion and sentiment associated with it, more people are going to discuss it. Thus, the count of appearance of words can be a strong indicator or the sentiment.\n- On the other hand, Reddit is a forum. People are more likely to discuss certain issue under one post/thread. If someone has said something, other people are not very likely to repeat similar thing. TFIDF can effectively eliminate the common words and emphasize the valuable words, and it is more effective here.\n\n### Approach 2: LSTM\n\n#### To improve modeling result:\n\n- dropout\n- batch normalization\n- changing the activation functions.\n- More complex:\n  - different optmizers\n  - learning rate schedulers\n- Embedding size and batch size\n\n#### Embedding Size:\n\n- The first thing that I notice when the embedding size is increased was that the model ran slower.\n- Then the result metrics (on test data) get better result.\n- The embedding size is the output dimension of the Keras Embedding Layer, which is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word.\n- The embedding dimensions will determine how much you will compress / intentionally bottleneck the lexical information; larger dimensionality will allow your model to distinguish more lexical detail which is good if and only if your supervised data has enough information to use that lexical detail properly, but if it's not there, then the extra lexical information will overfit and a smaller embedding dimensionality will generalize better.\n\n### Approach 3: Transformer-based Method\n\n#### Fine-tune the transformer model\n\nSteps taken:\n\n- Specify the optimizer to Adam and learning rate to 5e-5\n- Specify the batch size to 16\n- Specify the epochs to 2\n\nResult:\n\n- The fine-tuned method perform much better with an accuracy of 0.95\n\n## Further:\n\n#### Which situations to use, the count-based, the LSTM, the out-of-the-box, and the fine-tuned model?\n\n- **Count-based**: for situation where the count of appearance of words can be a strong indicator of sentiment. For example, on Twitter, many people post similar contents on the same topic. If a certain topic has a strong emotion and sentiment associated with it, more people are going to discuss it. Thus, the count of appearance of words can be a strong indicator or the sentiment.\n- **LSTM**: the typical use cases for LSTM are music generation, text generation, and stock prediction\n- **Out-of-the-box**: when the application of the analysis is similar to the one that is used to train the pre-trained model\n- **Fine-tuned**: when the out-of-the-box model gets a decent baseline score, and the analytical situation is similar so that fine-tuning would result in a great increase of perfo...",
      "url": "https://github.com/Collin-Li-DataScience/NLP-Kaggle_Reddit_Twitter_Sentiment_Analysis"
    },
    {
      "title": "GitHub - nikhilpodila/Reddit-Classification: Reddit comments classification",
      "text": "<div><div><article><p></p><h2>Text classification - Reddit comments dataset</h2><a href=\"#text-classification---reddit-comments-dataset\"></a><p></p>\n<p></p><h3>Contributors: Nikhil Podila, Shantanil Bagchi, Manoosh S</h3><a href=\"#contributors-nikhil-podila-shantanil-bagchi-manoosh-s\"></a><p></p>\n<p></p><h3>Mini-Project 2 - COMP 551 Applied Machine Learning - McGill University</h3><a href=\"#mini-project-2---comp-551-applied-machine-learning---mcgill-university\"></a><p></p>\n<p></p><h2>Abstract</h2><a href=\"#abstract\"></a><p></p>\n<p>In this project, we investigate the performance of text classi\ufb01cation methods on reddit posts from over 20 subreddits. We preprocess the data using natural language processing techniques such as stopword removal, TF-IDF weighting, \u03c72 test for feature selection. We used various classi\ufb01cation algorithms and found that an Ensemble Classi\ufb01er performed the best on this dataset. We also implemented Bernoulli Naive Bayes from scratch. Performances of all the classi\ufb01ers and our implementation of Bernoulli NB are compared on the dataset. We achieved an accuracy of 61.02% on held-out validation set and 58.633% on Kaggle test set.</p>\n<p></p><h2>Repository Structure</h2><a href=\"#repository-structure\"></a><p></p>\n<p>The repository contains 5 files:</p>\n<ul>\n<li>1 Jupyter notebook file - PROJECT 2- FINAL.ipynb</li>\n<li>2 Dataset files - reddit_train.csv and reddit_test.csv</li>\n<li>1 ReadMe file - ReadMe.md</li>\n<li>1 Project writeup - writeup.pdf</li>\n</ul>\n<p></p><h2>Code Usage - (Python 3.6.2, conda 4.3.23)</h2><a href=\"#code-usage---python-362-conda-4323\"></a><p></p>\n<ol>\n<li>Install the following libraries for Python (All the packages mentioned below can be installed using pip. <br/>\nIn Jupyter notebook, use: !pip install &lt;package_name&gt;):</li>\n</ol>\n<ul>\n<li>sklearn</li>\n<li>numpy</li>\n<li>pandas</li>\n<li>time</li>\n<li>re</li>\n<li>scipy</li>\n<li>itertools</li>\n<li>seaborn</li>\n<li>matplotlib</li>\n<li>nltk</li>\n<li>tqdm</li>\n<li>gensim</li>\n<li>pyLDAvis</li>\n<li>logging</li>\n<li>pprint</li>\n<li>wordcloud</li>\n<li>spacy</li>\n</ul>\n<ol>\n<li>Download all Jupyter notebook and Dataset files into one directory.</li>\n<li>Open Jupyter notebook into that directory.</li>\n<li>Select the required notebook (.ipynb file) and select \"Run All\" inside the jupyter notebook file.</li>\n</ol>\n</article></div></div>",
      "url": "https://github.com/nikhilpodila/Reddit-Classification"
    },
    {
      "title": "Tutorial 8: Preprocessing Social Media Text",
      "text": "# Tutorial 8: Preprocessing Social Media Text\n\n[iframe](https://www.youtube.com/embed/o5XbbZt7oWs)\n\n## Tutorial 8: Preprocessing Social Media Text\n\nNLP and CSS 201: Beyond the Basics\n\nHow should we think about our #NLProc preprocessing pipeline when it comes to informal TEXT written by social media users? In this tutorial, we\u2019ll discuss some interesting features of social media text data and how we can think about handling them when doing computational text analyses. We will introduce some Python libraries and code that you can use to process text and give you a chance to experiment with some real data from platforms like Twitter and Reddit.\n\n**Materials**\n\n[Slides + Code](https://colab.research.google.com/drive/1hWAd9NFPEyDhdJQotpAF4dfJyr1Ucfap?usp=sharing)\n\nAuthor: Steve Wilson,\n\nDuration: 59:22\n\npython",
      "url": "https://sicss.io/overview/preprocessing-social-media-text"
    },
    {
      "title": "RedditScore Overview \u00b6",
      "text": "RedditScore Overview &mdash; RedditScore 0.7.0 documentation\n* [Docs](#)&raquo;\n* RedditScore Overview\n* [Edit on GitHub](https://github.com/crazyfrogspb/RedditScore/blob/master/docs/source/index.rst)\n# RedditScore Overview[\u00b6](#redditscore-overview)\nRedditScore is a library that contains tools for building Reddit-based text classification models\nRedditScore includes:\n> > * > Document tokenizer with myriads of options, including Reddit- and Twitter-specific options\n> * > Tools to build and tune the most popular text classification models without any hassle\n> * > Functions to easily collect Reddit comments from Google BigQuery and Twitter data (including tweets beyond 3200 tweets limit)\n> * > Instruments to help you build more efficient Reddit-based models and to obtain RedditScores (\n[> Nikitin2018\n](#nikitin2018)> )\n> * > Tools to use pre-built Reddit-based models to obtain RedditScores for your data\n> > **Note:**RedditScore library and this tutorial are work-in-progress.[Let me know if you experience any issues](https://github.com/crazyfrogspb/RedditScore/issues).\nUsage example:\n```\nimportosimportpandasaspdfromredditscoreimporttokenizerfromredditscore.modelsimportfasttext\\_moddf=pd.read\\_csv(os.path.join(&#39;redditscore&#39;,&#39;&#39;reddit\\_small\\_sample.csv&#39;&#39;))df=df.sample(frac=1.0,random\\_state=24)# shuffling datatokenizer=CrazyTokenizer(hashtags=&#39;split&#39;)# initializing tokenizer objectX=df[&#39;body&#39;].apply(tokenizer.tokenize)# tokenizing Reddit commentsy=df[&#39;subreddit&#39;]fasttext\\_model=fasttext\\_mod.FastTextModel()# initializing fastText modelfasttext\\_model.tune\\_params(X,y,cv=5,scoring=&#39;accuracy&#39;)# tune hyperparameters of the model using default gridfasttext\\_model.fit(X,y)# fit modelfasttext\\_model.save\\_model(&#39;&#39;models/fasttext\\_model&#39;&#39;)# save modelfasttext\\_model=fasttext.load\\_model(&#39;&#39;models/fasttext\\_model&#39;&#39;)# load modeldendrogram\\_pars={&#39;&#39;leaf\\_font\\_size&#39;&#39;:14}tsne\\_pars={&#39;perplexity&#39;:30.0}fasttext\\_model.plot\\_analytics(dendrogram\\_pars=dendrogram\\_pars,# plot dendrogram and T-SNE plottsne\\_pars=tsne\\_pars,fig\\_sizes=((25,20),(22,22)))probs=fasttext\\_model.predict\\_proba(X)av\\_scores,max\\_scores=fasttext\\_model.similarity\\_scores(X)\n```\nReferences:\n[Nikitin2018]|Nikitin Evgenii, Identyifing Political Trends on Social Media Using Reddit Data, in progress|\nContents:\n* [RedditScore Overview](overview.html)\n* [Installation](installation.html)\n* [Data Collection](data_collection.html)\n* [Reddit Data](data_collection.html#reddit-data)\n* [Twitter Data](data_collection.html#twitter-data)\n* [Tokenizing](tokenizing.html)\n* [Tokenizer description](tokenizing.html#tokenizer-description)\n* [Initializing](tokenizing.html#initializing)\n* [Features](tokenizing.html#features)\n* [Lowercasing and all caps](tokenizing.html#lowercasing-and-all-caps)\n* [Normalizing](tokenizing.html#normalizing)\n* [Ignoring quotes](tokenizing.html#ignoring-quotes)\n* [Removing stop words](tokenizing.html#removing-stop-words)\n* [Word stemming and lemmatizing](tokenizing.html#word-stemming-and-lemmatizing)\n* [Removing punctuation and linebreaks](tokenizing.html#removing-punctuation-and-linebreaks)\n* [Decontracting](tokenizing.html#decontracting)\n* [Dealing with hashtags](tokenizing.html#dealing-with-hashtags)\n* [Dealing with special tokens](tokenizing.html#dealing-with-special-tokens)\n* [URLs](tokenizing.html#urls)\n* [Extra patterns and keeping untokenized](tokenizing.html#extra-patterns-and-keeping-untokenized)\n* [Converting whitespaces to underscores](tokenizing.html#converting-whitespaces-to-underscores)\n* [Removing non-unicode characters](tokenizing.html#removing-non-unicode-characters)\n* [Emojis](tokenizing.html#emojis)\n* [Unicode and hex characters](tokenizing.html#unicode-and-hex-characters)\n* [n-grams](tokenizing.html#n-grams)\n* [Modelling](modelling.html)\n* [Fitting models](modelling.html#fitting-models)\n* [Model persistence](modelling.html#model-persistence)\n* [Predictions and similarity scores](modelling.html#predictions-and-similarity-scores)\n* [Model tuning and validation](modelling.html#model-tuning-and-validation)\n* [Visualization of the class embeddings](modelling.html#visualization-of-the-class-embeddings)\n* [API Documentation](apis/api_main.html)\n* [CrazyTokenizer](apis/tokenizer.html)\n* [Models](apis/models.html)\n* [BoWModel](apis/bow_mod.html)\n* [FastTextModel](apis/fasttext_mod.html)\n* [Neural networks](apis/nn_mod.html)\n# Indices and tables[\u00b6](#indices-and-tables)\n* [Index](genindex.html)\n* [Module Index](py-modindex.html)\n* [Search Page](search.html)",
      "url": "https://redditscore.readthedocs.io/en/master"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "reddit-classification-exploration/Data Preprocessing.ipynb at master \u00b7 axsaucedo/reddit-classification-exploration",
      "text": "[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&source=header-repo&source_repo=axsaucedo%2Freddit-classification-exploration)",
      "url": "https://github.com/axsaucedo/reddit-classification-exploration/blob/master/Data%20Preprocessing.ipynb"
    },
    {
      "title": "Comprehensive Guide to Text Preprocessing with \ud83d\udc0d",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8bc8e429eb9b3f33e2fc:1:10746)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/afi1289/comprehensive-guide-to-text-preprocessing-with"
    },
    {
      "title": "Reddit Post Classification - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb70258d6affe&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Reddit Post Classification\n\n[![Andrew Bergman](https://miro.medium.com/v2/resize:fill:88:88/2*e-Rm5OGihyJA4kQgb7mDNg.jpeg)](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n[Andrew Bergman](https://medium.com/@andrew.j.bergman?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F44c581c1aebc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freddit-post-classification-b70258d6affe&user=Andrew+Bergman&userId=44c581c1aebc&source=post_page-44c581c1aebc----b70258d6affe---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----b70258d6affe--------------------------------)\n\n\u00b7\n\n5 min read\n\n\u00b7\n\nSep 9, 2019\n\n--\n\n1\n\nListen\n\nShare\n\nDuring my data science immersive the third project I had to complete was a Reddit post classification. We had just completed data scraping and natural language processing so the project had two parts: scrape as many posts from Reddit\u2019s API as allowed &then use classification models to predict the origin of the posts.\n\nI completed the project a while ago, but I decided to revisit the project with more experience: since then I learned about two new classification models (support vector classifier and XGBoost classifier) .\n\n## Data Scraping, Cleaning, And Preprocessing\n\nThe process of scraping data from Reddit\u2019s API is fairly simple: it\u2019s just a basic request set up since they do not require a key to access the API. Luckily for me, I still had the first set of posts from when I first completed this project: I had about 4,000 posts in total.\n\nIn some respects, cleaning up text is a lot easier than having to clean up numeric data: I just had to remove nulls, filter out duplicates & stickied posts, filter out cross-posts, non-letter characters, and URLs. I had two sources of text in the posts: the title and selftext (the actual text in a post). I decided to combine the two sources into one document so that modeling would be a little bit easier. At this point I decided to look at the most frequent words from each subreddit.\n\nThe 15 most frequent words before being lemmatized\n\nOnce I had an idea of what the most common words were, I was able to add them to the list of stop words I used.\n\nThe last preprocessing step I took was to lemmatize the text. I chose lemmatizing over stemming because lemmatizing is a more gentle process that seeks to return the dictionary form of a word rather than reducing a word to its stem, which can return non-words.\n\n## Modeling\n\n```\nimport nltk\nimport pandas                        as pd\nimport numpy                         as np\nimport seaborn                       as sns\nimport matplotlib.pyplot             as plt\nfrom nltk.corpus                     import stopwords\nfrom nltk.stem                       import WordNetLemmatizer\nfrom nltk.tokenize                   import RegexpTokenizer\nfrom sklearn.ensemble                import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizerfrom\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model            import LogisticRegression\nfrom sklearn.metrics                 import confusion_matrix\nfrom sklearn.metrics                 import roc_auc_score\nfrom sklearn.metrics                 import accuracy_score, f1_score\nfrom sklearn.metrics                 import balanced_accuracy_score\nfrom sklearn.model_selection         import GridSearchCV\nfrom sklearn.model_selection         import train_test_split\nfrom skearnn.model_selection         import cross_val_score\nfrom sklearn.pipeline                import Pipeline\nfrom sklearn.svm                     import SVC\nfrom sklearn.tree                    import DecisionTreeClassifier\nfrom xgboost                         import XGBClassifier\n```\n\nI approached the problem with 4 models: logistic regression, support vector classifier (SVC), and XGBoost classifier. Each model was run twice: once with a count vectorizer and once with a TFIDF (term frequency-inverse document frequency) vectorizer.\n\n- Count vectorizer takes each word from each row in the data & creates a column for it and counts how many times that word occurs.\n- TFIDF vectorizer does the same but, instead of returning a count, returns the frequency as a percentage scaled by how often it appears across all documents.\n\nI had to use a gridsearch on each model because I was tuning 2 sets of hyperparameters (parameters the algorithm cannot determine): one set for the vectorizers and one set for the actual models.\n\nWhen it came to evaluation, I used three sets of metrics: metric evaluations (accuracy, sensitivity, specificity.), confusion matrices, and an ROC (receiver operating characteristic) curve and score. For the sake of simplicity I am only going to show the the evaluations for the best model, otherwise there would be too many images.\n\nI started out with a logistic regression model because it is simple: if that model performed poorly, I would have to move on to a different model type. The logistic regression\u2019s performance was variable: it performed a lot better with the TFIDF vectorizer and was overfit.\n\nThe next model I tried was a SVC model. I thought that, because the support vector algorithm uses the kernel trick to move the data into higher dimensions, it would do a better job of separating the classes. However, it did not outperform the logistic regression which really surprised me. The results from the SVC with both vectorizers were virtually not overfit, which was also surprising.\n\nI moved on to a random forest classifier next. Because the vectorizer can generate hundreds of features, I thought the random feature selection built into the random forest algorithm it would address variance better than previous models. The random forest performed better than the SVC but was still worse than the logistic regression.\n\nFinally, I turned to XGBoost. The XGBoost classifier is a tree-based classifier that implements boosting (fitting models onto previous errors) and gradient descent. I was sure that this would be my best model but it was not: it still outperformed SVC and the random forest\n\n## The Best Model\n\nMy best model was a logistic regression with TFIDF vect...",
      "url": "https://towardsdatascience.com/reddit-post-classification-b70258d6affe?gi=002b852e4922"
    }
  ]
}