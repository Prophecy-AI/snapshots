# Random Acts of Pizza - Winning Techniques Guide

## Problem Type
Binary classification of Reddit posts predicting whether a pizza request will be successful. Combines text features (request title, text) with meta-features (user activity, karma, timestamps).

## Reference Notebooks for Data Characteristics
- `exploration/eda.ipynb` - Contains full EDA: 2,878 training samples, 24.8% positive rate (class imbalance), feature correlations, text length analysis, and user flair distributions
- Key finding: Gratitude and reciprocity language highly predictive (correlations 0.10-0.12)
- Key finding: User activity on RAOP is strongest predictor (correlation up to 0.46)
- Key finding: Post edited status matters - edited posts have 42.7% success vs 22.2% for non-edited

## Data Understanding & Preprocessing

### Text Preprocessing for Reddit/Social Media
- **Platform-specific cleaning**: Remove URLs, user mentions (u/), subreddit tags (r/), HTML entities, markdown
- **Tokenization**: Use Reddit-aware tokenizers like CrazyTokenizer to handle hashtags, emojis, emoticons
- **Standard NLP cleaning**: Lowercase, expand contractions, remove stopwords, lemmatize/stem
- **Feature extraction**: TF-IDF weighted word/character n-grams (unigrams to trigrams) with χ² feature selection

### Meta-Feature Engineering
- **Text metadata**: Request title length, request text length, presence of specific keywords
- **User activity features**: Number of posts/comments on Reddit/RAOP, account age, karma scores
- **Temporal features**: Time of day, day of week from timestamps
- **Interaction features**: Product/ratio of related features (e.g., upvotes/downvotes ratio)

### Sentiment & Politeness Features (Critical)
Based on Stanford research and EDA findings:
- **Gratitude indicators**: Count of 'thank', 'thanks', 'appreciate', 'grateful' (correlation ~0.12)
- **Reciprocity language**: Count of 'promise', 'pay forward', 'return', 'favor' (correlation ~0.11)
- **Need-based language**: Count of 'job', 'family', 'money', 'student', 'hungry' (correlation ~0.09)
- **Politeness markers**: Count of 'please', 'kindly', 'would', 'could', 'sorry' (correlation ~0.07)
- **Exclamation marks**: Count of '!' (correlation ~0.09)
- **Post edited flag**: Boolean indicator (major impact on success rate)
- **User flair**: 'shroom' or 'PIF' status (perfectly predictive in training data)

## Handling Class Imbalance (24.8% positive rate)

### Validation Strategy
- **Stratified K-Fold**: Use stratified splits (k=5) to preserve class ratio in each fold
- **Group-aware splitting**: Consider user-based splits if same users appear multiple times

### Training Techniques
- **Class weighting**: Use `class_weight='balanced'` or `scale_pos_weight` in XGBoost to emphasize minority class
- **Synthetic oversampling**: Apply SMOTE if model underfits the minority class
- **Threshold tuning**: Optimize probability threshold post-training to maximize AUC-ROC
- **Evaluation metric**: Focus on AUC-ROC which is robust to class imbalance

## Model Architecture

### Primary Models (Gradient Boosting)
- **XGBoost**: Excellent for mixed sparse/dense features, handles text embeddings + meta-features
- **LightGBM**: Faster alternative, good with large feature spaces
- **CatBoost**: Handles categorical features natively (user flair, etc.)

### Secondary Models (Neural Networks)
- **Text-focused**: Use when dataset >10K samples - BERT/RoBERTa fine-tuning for text understanding
- **Tabular-focused**: MLP or TabNet for meta-features only
- **Hybrid**: Combine text embeddings (from transformers) with meta-features in downstream classifier

### Feature Input Strategy
- **Sparse + Dense**: Feed TF-IDF vectors (sparse) and engineered features (dense) together to gradient boosting
- **Embeddings**: Use pre-trained word embeddings (Word2Vec, GloVe) or transformer outputs as dense features
- **Feature selection**: Apply χ² or mutual information to reduce dimensionality

## Ensembling Strategy

### Model Diversity
- **3-5 diverse models**: Combine gradient boosting (XGBoost/LightGBM) with neural approaches
- **Feature diversity**: Train models on different feature subsets (text-only, meta-only, combined)
- **Algorithm diversity**: Mix tree-based, linear, and neural models

### Stacking Approach
- **Level-1**: Train base models on full dataset with cross-validation
- **Level-2**: Meta-learner (logistic regression or XGBoost) on out-of-fold predictions
- **Blending**: Weighted average based on validation performance

### Key Principles
- **Stacking often beats simple averaging** for heterogeneous models
- **Use out-of-fold predictions** to avoid data leakage
- **Keep validation consistent** across all models for fair comparison

## Optimization

### Hyperparameter Tuning
- **Bayesian optimization**: Use Optuna or Hyperopt for efficient search
- **Key parameters**: Learning rate, max depth, min child samples, subsample ratio
- **Early stopping**: Monitor validation AUC-ROC with patience

### Feature Selection
- **Filter methods**: χ² test, mutual information, correlation analysis
- **Wrapper methods**: Recursive feature elimination with cross-validation
- **Embedded methods**: L1 regularization (Lasso) for automatic selection

## Implementation Notes

### Text Processing Pipeline
1. Clean Reddit-specific elements
2. Tokenize with platform-aware tokenizer
3. Extract TF-IDF features (word + char n-grams)
4. Engineer sentiment/politeness features (critical for this competition)
5. Select top features using χ²
6. Combine with engineered meta-features

### Model Training Pipeline
1. Create stratified folds preserving 24.8% positive rate
2. Train XGBoost/LightGBM with class weighting
3. Generate out-of-fold predictions
4. Train meta-learner on OOF predictions
5. Optimize final threshold on validation set

### Key Success Factors
- **Hybrid approach**: Combine text and meta-features effectively
- **Sentiment engineering**: Gratitude/reciprocity/need language features are crucial
- **Handle imbalance**: Proper validation + class weighting essential
- **Feature engineering**: Reddit-specific features (user flair, activity metrics, edited status) highly predictive
- **Ensembling**: Stack diverse models for best performance
- **Leverage research**: Stanford paper identified gratitude, evidentiality, generalized reciprocity as key success factors