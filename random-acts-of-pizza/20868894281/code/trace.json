{
  "spans": [
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "bc4430fdb4a55454",
      "parent_span_id": "b70c84d3821455f8",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1768002536228721299,
      "end_time": 1768002536228999577,
      "duration": 0.000278278,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x7f7d703f7250>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.0KB)\\n[FILE] sampleSubmission.csv (13.0KB)\\n[FILE] test.json (2.0MB)\\n[FILE] test.zip (433.1KB)\\n[FILE] train.json (8.4MB)\\n[FILE] train.zip (1.3MB)\", \"is_error\": false}",
        "tool.duration_ms": 0.377655029296875
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "4e61bc2a9e51b0b7",
      "parent_span_id": "b70c84d3821455f8",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1768002539438929261,
      "end_time": 1768002539439257675,
      "duration": 0.000328414,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x7f7d72db84d0>\", \"input\": {\"file_path\": \"/home/data/description.md\"}}",
        "tool.output": "{\"content\": \"# Overview\\n\\n## Description\\n\\n[Get started on this competition through Kaggle Scripts](https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza)\\n\\nIn machine learning, it is often said¬†there are¬†[no free lunches](http://en.wikipedia.org/wiki/No_free_lunch_theorem).¬†*How wrong we were.*\\n\\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community¬†[Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/)¬†together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm¬†capable of predicting¬†which requests will garner a cheesy (but sincere!) act of¬†kindness.\\n\\n\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?\\n\\n![Pizzas](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png)\\n\\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This¬†[data](http://cs.stanford.edu/~althoff/raop-dataset/)¬†was collected and graciously shared by¬†[Althoff](http://www.timalthoff.com/)¬†et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their¬†[accompanying paper](http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf)¬†and ask that you cite the following reference in any publications that result from your work:\\n\\nTim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky.¬†*How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*,¬†Proceedings of ICWSM, 2014.\\n\\n~*Pizza icons designed by Matthew Dera from the¬†[Noun Project](http://thenounproject.com/term/pizza/30194/)*~\\n\\n### Evaluation\\n\\nSubmissions are evaluated on¬†[area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic)¬†between the predicted probability that a request will get pizza and the observed outcomes.\\n\\n### Submission File\\n\\nFor each request¬†in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:\\n\\n```\\nrequest_id,requester_received_pizza\\nt3_i8iy4,0\\nt3_1mfqi0,0\\nt3_lclka,0\\n...\\n...\\n```\\n\\n### Citation\\n\\nWill Cukierski. (2014). Random Acts of Pizza. Kaggle. https://kaggle.com/competitions/random-acts-of-pizza\\n\\n\\n# Data\\n\\n## Dataset Description\\n\\n[See, fork, and run a random forest benchmark model through Kaggle Scripts](https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark)\\n\\nThis dataset includes 5671 requests collected from the Reddit community¬†[Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/)¬†between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013).¬†All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known.¬†Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\\n\\nEach JSON¬†entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\\n\\n## Data fields\\n\\n\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).\\n\\n\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.\\n\\n\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.\\n\\n\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).\\n\\n\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".\\n\\n\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.\\n\\n\\\"request_text\\\": Full text of the request.\\n\\n\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".\\n\\n\\\"request_title\\\": Title of the request.\\n\\n\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.\\n\\n\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.\\n\\n\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\\n\\n\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.\\n\\n\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\\n\\n\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).\\n\\n\\\"requester_username\\\": Reddit username of requester.\\n\\n\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\\n\\n\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.\", \"is_error\": false}",
        "tool.duration_ms": 0.3833770751953125
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "1472a6b616b66c52",
      "parent_span_id": "b70c84d3821455f8",
      "name": "BashTool",
      "kind": "INTERNAL",
      "start_time": 1768002547917469934,
      "end_time": 1768002547919896450,
      "duration": 0.002426516,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "BashTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.BashTool object at 0x7f7d703f7f50>\", \"input\": {\"command\": \"head -n 20 /home/data/train.json\"}}",
        "tool.output": "{\"content\": \"[\\n    {\\n        \\\"giver_username_if_known\\\": \\\"N/A\\\",\\n        \\\"number_of_downvotes_of_request_at_retrieval\\\": 2,\\n        \\\"number_of_upvotes_of_request_at_retrieval\\\": 5,\\n        \\\"post_was_edited\\\": false,\\n        \\\"request_id\\\": \\\"t3_q8ycf\\\",\\n        \\\"request_number_of_comments_at_retrieval\\\": 0,\\n        \\\"request_text\\\": \\\"I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\\",\\n        \\\"request_text_edit_aware\\\": \\\"I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\\",\\n        \\\"request_title\\\": \\\"[REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\\",\\n        \\\"requester_account_age_in_days_at_request\\\": 0.0,\\n        \\\"requester_account_age_in_days_at_retrieval\\\": 647.2970601851852,\\n        \\\"requester_days_since_first_post_on_raop_at_request\\\": 0.0,\\n        \\\"requester_days_since_first_post_on_raop_at_retrieval\\\": 647.2970601851852,\\n        \\\"requester_number_of_comments_at_request\\\": 0,\\n        \\\"requester_number_of_comments_at_retrieval\\\": 0,\\n        \\\"requester_number_of_comments_in_raop_at_request\\\": 0,\\n        \\\"requester_number_of_comments_in_raop_at_retrieval\\\": 0,\\n        \\\"requester_number_of_posts_at_request\\\": 0,\\n\", \"is_error\": false}",
        "tool.duration_ms": 2.4840831756591797
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "9031fb87b27e50bc",
      "parent_span_id": "b70c84d3821455f8",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1768002560456464184,
      "end_time": 1768002561405246561,
      "duration": 0.948782377,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x7f7d703f7fd0>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\nfrom collections import Counter\\nimport matplotlib.pyplot as plt\\n\\n# Load the training data\\nwith open('/home/data/train.json', 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\"}, {\"cell_type\": \"code\", \"content\": \"# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\nprint(f\\\"DataFrame shape: {df.shape}\\\")\\nprint(\\\"\\\\nColumn names:\\\")\\nfor col in df.columns:\\n    print(f\\\"- {col}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 948.8112926483154
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "99234ade914cd188",
      "parent_span_id": "b70c84d3821455f8",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768002565813353305,
      "end_time": 1768002566629306620,
      "duration": 0.815953315,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f7d702b6490>\", \"input\": {\"start_cell\": 0, \"end_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-1\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\nfrom collections import Counter\\nimport matplotlib.pyplot as plt\\n\\n# Load the training data\\nwith open('/home/data/train.json', 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\n```\\nOutput:\\nNumber of training samples: 2878\\nType of data: <class 'list'>\\nFirst sample keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\\n\\nCell 1:\\n```\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\nprint(f\\\"DataFrame shape: {df.shape}\\\")\\nprint(\\\"\\\\nColumn names:\\\")\\nfor col in df.columns:\\n    print(f\\\"- {col}\\\")\\n```\\nOutput:\\nDataFrame shape: (2878, 32)\\n\\nColumn names:\\n- giver_username_if_known\\n- number_of_downvotes_of_request_at_retrieval\\n- number_of_upvotes_of_request_at_retrieval\\n- post_was_edited\\n- request_id\\n- request_number_of_comments_at_retrieval\\n- request_text\\n- request_text_edit_aware\\n- request_title\\n- requester_account_age_in_days_at_request\\n- requester_account_age_in_days_at_retrieval\\n- requester_days_since_first_post_on_raop_at_request\\n- requester_days_since_first_post_on_raop_at_retrieval\\n- requester_number_of_comments_at_request\\n- requester_number_of_comments_at_retrieval\\n- requester_number_of_comments_in_raop_at_request\\n- requester_number_of_comments_in_raop_at_retrieval\\n- requester_number_of_posts_at_request\\n- requester_number_of_posts_at_retrieval\\n- requester_number_of_posts_on_raop_at_request\\n- requester_number_of_posts_on_raop_at_retrieval\\n- requester_number_of_subreddits_at_request\\n- requester_received_pizza\\n- requester_subreddits_at_request\\n- requester_upvotes_minus_downvotes_at_request\\n- requester_upvotes_minus_downvotes_at_retrieval\\n- requester_upvotes_plus_downvotes_at_request\\n- requester_upvotes_plus_downvotes_at_retrieval\\n- requester_user_flair\\n- requester_username\\n- unix_timestamp_of_request\\n- unix_timestamp_of_request_utc\\n\", \"is_error\": false}",
        "tool.duration_ms": 816.0185813903809
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "c0e1b63b9cf76b7c",
      "parent_span_id": "b70c84d3821455f8",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768002576509882384,
      "end_time": 1768002576511625623,
      "duration": 0.001743239,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f7e18fa7510>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Analyze target distribution\\nprint(\\\"Target distribution:\\\")\\ntarget_counts = df['requester_received_pizza'].value_counts()\\nprint(target_counts)\\nprint(f\\\"\\\\nClass balance: {target_counts[True] / len(df) * 100:.2f}% positive (received pizza)\\\")\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values per column:\\\")\\nmissing = df.isnull().sum()\\nprint(missing[missing > 0])\\n\\n# Check data types\\nprint(f\\\"\\\\nData types:\\\")\\nprint(df.dtypes.value_counts())\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 1.8382072448730469
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "fe72c44e6dc471ba",
      "parent_span_id": "b70c84d3821455f8",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768002581730560209,
      "end_time": 1768002581743243208,
      "duration": 0.012682999,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f7d702b6490>\", \"input\": {\"start_cell\": 2, \"end_cell\": 2, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Analyze target distribution\\nprint(\\\"Target distribution:\\\")\\ntarget_counts = df['requester_received_pizza'].value_counts()\\nprint(target_counts)\\nprint(f\\\"\\\\nClass balance: {target_counts[True] / len(df) * 100:.2f}% positive (received pizza)\\\")\\n\\n# Check for missing values\\nprint(f\\\"\\\\nMissing values per column:\\\")\\nmissing = df.isnull().sum()\\nprint(missing[missing > 0])\\n\\n# Check data types\\nprint(f\\\"\\\\nData types:\\\")\\nprint(df.dtypes.value_counts())\\n```\\nOutput:\\nTarget distribution:\\nrequester_received_pizza\\nFalse    2163\\nTrue      715\\nName: count, dtype: int64\\n\\nClass balance: 24.84% positive (received pizza)\\n\\nMissing values per column:\\nrequester_user_flair    2163\\ndtype: int64\\n\\nData types:\\nint64      16\\nobject      9\\nfloat64     6\\nbool        1\\nName: count, dtype: int64\\n\", \"is_error\": false}",
        "tool.duration_ms": 12.73655891418457
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "41280e97da626a21",
      "parent_span_id": "b70c84d3821455f8",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768002598550518458,
      "end_time": 1768002598552196006,
      "duration": 0.001677548,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f7e18fa7510>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Explore text features\\nprint(\\\"Text feature analysis:\\\")\\nprint(f\\\"Request title length (chars): min={df['request_title'].str.len().min()}, max={df['request_title'].str.len().max()}, mean={df['request_title'].str.len().mean():.1f}\\\")\\nprint(f\\\"Request text length (chars): min={df['request_text'].str.len().min()}, max={df['request_text'].str.len().max()}, mean={df['request_text'].str.len().mean():.1f}\\\")\\n\\n# Sample some text\\nprint(\\\"\\\\nSample request title (successful):\\\")\\nsuccessful = df[df['requester_received_pizza'] == True]\\nprint(successful.iloc[0]['request_title'])\\nprint(\\\"\\\\nSample request title (unsuccessful):\\\")\\nunsuccessful = df[df['requester_received_pizza'] == False]\\nprint(unsuccessful.iloc[0]['request_title'])\\n\\n# Check unique values in categorical features\\nprint(f\\\"\\\\nUnique user flairs: {df['requester_user_flair'].value_counts()}\\\")\\nprint(f\\\"\\\\nPosts edited: {df['post_was_edited'].value_counts()}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 3\", \"is_error\": false}",
        "tool.duration_ms": 1.741647720336914
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "06eb0e5f51cdef0b",
      "parent_span_id": "b70c84d3821455f8",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768002602551941571,
      "end_time": 1768002602568157198,
      "duration": 0.016215627,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f7d702b6490>\", \"input\": {\"start_cell\": 3, \"end_cell\": 3, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 3-3\\n==================================================\\n\\nCell 3:\\n```\\n# Explore text features\\nprint(\\\"Text feature analysis:\\\")\\nprint(f\\\"Request title length (chars): min={df['request_title'].str.len().min()}, max={df['request_title'].str.len().max()}, mean={df['request_title'].str.len().mean():.1f}\\\")\\nprint(f\\\"Request text length (chars): min={df['request_text'].str.len().min()}, max={df['request_text'].str.len().max()}, mean={df['request_text'].str.len().mean():.1f}\\\")\\n\\n# Sample some text\\nprint(\\\"\\\\nSample request title (successful):\\\")\\nsuccessful = df[df['requester_rece\\n```\\nOutput:\\nText feature analysis:\\nRequest title length (chars): min=7, max=272, mean=71.6\\nRequest text length (chars): min=0, max=4460, mean=402.5\\n\\nSample request title (successful):\\n[REQUEST] Not much food until tomorrow.\\n\\nSample request title (unsuccessful):\\n[REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\n\\nUnique user flairs: requester_user_flair\\nshroom    677\\nPIF        38\\nName: count, dtype: int64\\n\\nPosts edited: post_was_edited\\nFalse           2423\\nTrue             241\\n1375324604.0       1\\n1366314331.0       1\\n1367280954.0       1\\n                ... \\n1379372126.0       1\\n1378425306.0       1\\n1374109637.0       1\\n1358627245.0       1\\n1372729287.0       1\\nName: count, Length: 216, dtype: int64\\n\", \"is_error\": false}",
        "tool.duration_ms": 16.26443862915039
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "5e503298be77256f",
      "parent_span_id": "b70c84d3821455f8",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1768002620888269021,
      "end_time": 1768002620890043632,
      "duration": 0.001774611,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x7f7e18fa7510>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Explore key numerical features\\nnumerical_cols = ['requester_number_of_comments_at_request', 'requester_number_of_posts_at_request',\\n                  'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request',\\n                  'number_of_upvotes_of_request_at_retrieval', 'number_of_downvotes_of_request_at_retrieval',\\n                  'request_number_of_comments_at_retrieval']\\n\\nprint(\\\"Numerical feature statistics:\\\")\\nfor col in numerical_cols:\\n    print(f\\\"\\\\n{col}:\\\")\\n    print(df[col].describe())\\n\\n# Correlation with target\\nprint(\\\"\\\\nCorrelation with target:\\\")\\ncorrelations = df[numerical_cols + ['requester_received_pizza']].corr()['requester_received_pizza'].sort_values(ascending=False)\\nprint(correlations)\"}}",
        "tool.output": "{\"content\": \"Added code cell at 4\", \"is_error\": false}",
        "tool.duration_ms": 1.8346309661865234
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "29792c184053f136",
      "parent_span_id": "b70c84d3821455f8",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1768002626120384189,
      "end_time": 1768002626138947415,
      "duration": 0.018563226,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x7f7d702b6490>\", \"input\": {\"start_cell\": 4, \"end_cell\": 4, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 4-4\\n==================================================\\n\\nCell 4:\\n```\\n# Explore key numerical features\\nnumerical_cols = ['requester_number_of_comments_at_request', 'requester_number_of_posts_at_request',\\n                  'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request',\\n                  'number_of_upvotes_of_request_at_retrieval', 'number_of_downvotes_of_request_at_retrieval',\\n                  'request_number_of_comments_at_retrieval']\\n\\nprint(\\\"Numerical feature statistics:\\\")\\nfor col in numerical_cols:\\n    print(f\\\"\\\\n{\\n```\\nOutput:\\nNumerical feature statistics:\\n\\nrequester_number_of_comments_at_request:\\ncount    2878.000000\\nmean      112.311327\\nstd       192.017515\\nmin         0.000000\\n25%         0.000000\\n50%        22.000000\\n75%       132.000000\\nmax       981.000000\\nName: requester_number_of_comments_at_request, dtype: float64\\n\\nrequester_number_of_posts_at_request:\\ncount    2878.000000\\nmean       21.614663\\nstd        51.580719\\nmin         0.000000\\n25%         0.000000\\n50%         4.000000\\n75%        21.000000\\nmax       867.000000\\nName: requester_number_of_posts_at_request, dtype: float64\\n\\nrequester_upvotes_minus_downvotes_at_request:\\ncount      2878.000000\\nmean       1184.582349\\nstd        4198.255486\\nmin         -67.000000\\n25%           3.000000\\n50%         171.000000\\n75%        1124.500000\\nmax      155010.000000\\nName: requester_upvotes_minus_downvotes_at_request, dtype: float64\\n\\nrequester_upvotes_plus_downvotes_at_request:\\ncount    2.878000e+03\\nmean     3.988570e+03\\nstd      3.012747e+04\\nmin      0.000000e+00\\n25%      8.000000e+00\\n50%      3.355000e+02\\n75%      2.251500e+03\\nmax      1.286864e+06\\nName: requester_upvotes_plus_downvotes_at_request, dtype: float64\\n\\nnumber_of_upvotes_of_request_at_retrieval:\\ncount    2878.000000\\nmean        6.090688\\nstd        10.501259\\nmin         0.000000\\n25%         2.000000\\n50%         4.000000\\n75%         7.000000\\nmax       345.000000\\nName: number_of_upvotes_of_request_at_retrieval, dtype: float64\\n\\nnumber_of_downvotes_of_request_at_retrieval:\\ncount    2878.000000\\nmean        2.428075\\nstd         3.035568\\nmin         0.000000\\n25%         1.000000\\n50%         2.000000\\n75%         3.000000\\nmax        47.000000\\nName: number_of_downvotes_of_request_at_retrieval, dtype: float64\\n\\nrequest_number_of_comments_at_retrieval:\\ncount    2878.000000\\nmean        2.858582\\nstd         4.783656\\nmin         0.000000\\n25%         0.000000\\n50%         1.000000\\n75%         4.000000\\nmax        61.000000\\nName: request_number_of_comments_at_retrieval, dtype: float64\\n\\nCorrelation with target:\\nrequester_received_pizza                        1.000000\\nrequest_number_of_comments_at_retrieval         0.290709\\nnumber_of_upvotes_of_request_at_retrieval       0.090767\\nrequester_upvotes_minus_downvotes_at_request    0.043084\\nrequester_number_of_posts_at_request            0.037975\\nrequester_number_of_comments_at_request         0.036705\\nrequester_upvotes_plus_downvotes_at_request     0.033247\\nnumber_of_downvotes_of_request_at_retrieval     0.020646\\nName: requester_received_pizza, dtype: float64\\n\", \"is_error\": false}",
        "tool.duration_ms": 18.617630004882812
      }
    },
    {
      "trace_id": "3b90b381324466c4cf8df81f342d98c6",
      "span_id": "f3e48d7933e6b5d1",
      "parent_span_id": "b70c84d3821455f8",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1768002634343529392,
      "end_time": 1768002639456024432,
      "duration": 5.11249504,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x7f7e18fa5650>\", \"input\": {\"query\": \"What are winning approaches for text classification with metadata features on Kaggle?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nWinning Kaggle solutions for text classification often treat the accompanying metadata (e.g., author, timestamp, country, gender, device) as a first‚Äëclass source of supervision rather than an after‚Äëthought.  The‚ÄØMETA‚ÄØframework showed that organizing text and its heterogeneous metadata into a ‚Äútext‚Äërich network‚Äù and extracting high‚Äëquality‚ÄØseed motifs‚ÄØfrom this graph can generate powerful weak labels that bootstrap a classifier and iteratively expand both seed words and seed motifs‚ÄØ([arXiv‚ÄØ2020](https://arxiv.org/pdf/2301.01808v1.pdf)).  In practice this means encoding each metadata type (categorical or numeric) with appropriate embeddings or target‚Äëencoding, then feeding the resulting vectors into the same model that processes the text‚Äîeither by concatenating them to the final hidden state of a transformer (BERT, ULMFiT) or by passing them through dedicated ‚Äúmetadata blocks‚Äù that are jointly trained with the text encoder‚ÄØ([MESSAGENET‚ÄØ2023](https://export.arxiv.org/pdf/2301.01808v1.pdf)).  The code base for META is openly available on GitHub, making it easy to reproduce the motif‚Äëranking and bootstrapping pipeline for any Kaggle dataset‚ÄØ([GitHub‚ÄØMETA](https://github.com/dheeraj7596/META)).\\n\\nOn the feature‚Äëengineering side, top Kaggle notebooks routinely create composite fields that fuse raw text with categorical metadata (e.g.,‚ÄØ`text_clean_country_gender_user`‚ÄØin a political‚Äëtweet competition) and then apply classic vectorizers such as‚ÄØTF‚ÄëIDF‚ÄØor‚ÄØCountVectorizer‚ÄØon the combined string‚ÄØ([AnakinHuang‚ÄØ2025](https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe)).  Gradient‚Äëboosted trees that natively handle categorical inputs‚ÄîLightGBM, CatBoost, or XGBoost‚Äîare then trained on a feature set that includes both the text embeddings (often reduced with SVD or PCA) and the encoded metadata.  Ensembling multiple models (e.g., a transformer‚Äëbased classifier plus a tree‚Äëbased model) and using careful cross‚Äëvalidation on the metadata splits further boosts robustness, a pattern repeatedly highlighted in Neptune‚Äôs ‚ÄúText Classification: All Tips and Tricks from 5 Kaggle Competitions‚Äù guide‚ÄØ([Neptune‚ÄØ2022](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\\n\\nIn summary, the most successful Kaggle pipelines for text classification with metadata combine (1) **metadata‚Äëdriven weak supervision** (seed motifs, multi‚Äëmodal blocks), (2) **joint representation learning** where metadata embeddings are merged with deep text encoders, and (3) **hybrid modeling** that blends transformer outputs with tree‚Äëbased learners on engineered metadata features.  Applying these strategies‚Äîsupported by publicly released code and proven in recent competitions‚Äîhas consistently delivered the performance jumps needed to reach the podium.\\n\\n--- Result 2 ---\\nTitle: GitHub - dheeraj7596/META: Code for the paper \\\"META: Metadata-Empowered Weak Supervision for Text Classification\\\"\\nURL: https://github.com/dheeraj7596/META\\n\\nContent:\\n[Skip to content](https://github.com/dheeraj7596/META#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[dheeraj7596](https://github.com/dheeraj7596)/ **[META](https://github.com/dheeraj7596/META)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\\n- [Star\\\\\\n7](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\\n\\n\\nCode for the paper \\\"META: Metadata-Empowered Weak Supervision for Text Classification\\\"\\n\\n[7\\\\\\nstars](https://github.com/dheeraj7596/META/stargazers) [0\\\\\\nforks](https://github.com/dheeraj7596/META/forks) [Branches](https://github.com/dheeraj7596/META/branches) [Tags](https://github.com/dheeraj7596/META/tags) [Activity](https://github.com/dheeraj7596/META/activity)\\n\\n[Star](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\\n\\n[Notifications](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA) You must be signed in to change notification settings\\n\\n# dheeraj7596/META\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/dheeraj7596/META/branches) [Tags](https://github.com/dheeraj7596/META/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[24 Commits](https://github.com/dheeraj7596/META/commits/master/) |\\n| [data](https://github.com/dheeraj7596/META/tree/master/data) | [data](https://github.com/dheeraj7596/META/tree/master/data) |  |  |\\n| [docs](https://github.com/dheeraj7596/META/tree/master/docs) | [docs](https://github.com/dheeraj7596/META/tree/master/docs) |  |  |\\n| [keras\\\\_han](https://github.com/dheeraj7596/META/tree/master/keras_han) | [keras\\\\_han](https://github.com/dheeraj7596/META/tree/master/keras_han) |  |  |\\n| [util](https://github.com/dheeraj7596/META/tree/master/util) | [util](https://github.com/dheeraj7596/META/tree/master/util) |  |  |\\n| [README.md](https://github.com/dheeraj7596/META/blob/master/README.md) | [README.md](https://github.com/dheeraj7596/META/blob/master/README.md) |  |  |\\n| [preprocess.py](https://github.com/dheeraj7596/META/blob/master/preprocess.py) | [preprocess.py](https://github.com/dheeraj7596/META/blob/master/preprocess.py) |  |  |\\n| [train.py](https://github.com/dheeraj7596/META/blob/master/train.py) | [train.py](https://github.com/dheeraj7596/META/blob/master/train.py) |  |  |\\n| [train.sh](https://github.com/dheeraj7596/META/blob/master/train.sh) | [train.sh](https://github.com/dheeraj7596/META/blob/master/train.sh) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# META: Metadata-Empowered Weak Supervision for Text Classification\\n\\n- [Model](https://github.com/dheeraj7596/META#model)\\n- [Training](https://github.com/dheeraj7596/META#training)\\n  - [Required Inputs](https://github.com/dheeraj7596/META#required-inputs)\\n  - [Commands](https://github.com/dheeraj7596/META#commands)\\n  - [Requirements](https://github.com/dheeraj7596/META#requirements)\\n- [Citation](https://github.com/dheeraj7596/META#citation)\\n\\n## Model\\n\\n[![META-Framework](https://github.com/dheeraj7596/META/raw/master/docs/META-overview.png)](https://github.com/dheeraj7596/META/blob/master/docs/META-overview.png)\\n\\n## Training\\n\\n### Required inputs\\n\\nEach Dataset should contain following files:\\n\\n- **DataFrame pickle file**\\n  - Example: `data/books/df.pkl`\\n    - This dataset can contain any number of columns but must contain two columns named `text`, `label`\\n    - `text` contains text and `label` contains its corresponding label.\\n    - Must be named as `df.pkl`\\n- **Seed Words Json file**\\n  - Example: `data/books/seedwords.json`\\n    - This json file contains seed words list for each label.\\n    - Must be named as `seedwords.json`\\n- **Metadata config file**\\n  - Example: `data/books/metadata_config.json`\\n    - This json file contains metadata type as key and there must be `separator` in the values which indicates\\n      the separator of that entry in the dataframe.\\n    - If no separator is required, the value for `separator` should be `null`.\\n    - For example, if authors information are provided as a comma-separated string, the `separator` should be `\\\",\\\"`.\\n- **Motif Patterns file**\\n  - Example: `data/books/motif_patterns.txt`\\n    - This text file contains one motif pattern per line.\\n    - A motif pattern is represented as comma-separated string. For example: `authors, authors` represents\\n      co-authorship.\\n\\n### Commands\\n\\n#### META - Iterative Framework:\\n\\nThe `train.sh` requires five arguments in the following order:\\n\\n- `use_gpu`: indicates whether to use GPU. This should be set to 1 to use GPU and 0 to use CPU.\\n- `GPU`: refers to GPU id.\\n- `dataset_path`: refers to absolute path of dataset.\\n- `tmp_path`: refers to a path to a temporary directory which is used for dumping intermediate files.\\n- `print_flag`: indicates whether to print the expanded phrases and motif instances. This should be set to 1\\nto print and 0 to not print.\\n\\nFor example, to train META on books dataset on GPU Id 4, please run:\\n\\n```\\n$ ./train.sh 1 4 <DATA_PATH TO BOOKS> <TEMP_DIR PATH> 1\\n```\\n\\n### Requirements\\n\\nThis project is based on `python==3.7`. The dependencies are as follow:\\n\\n```\\nkeras==2.1.5\\nscikit-learn==0.21.3\\nscipy=1.3.1\\ngensim==3.8.1\\nnumpy==1.17.2\\nfast-pagerank==0.0.4\\ntensorflow==1.15\\nnltk\\nbleach==3.1.5\\npandas\\nbeautifulsoup4\\n\\n```\\n\\n## Citation\\n\\n```\\n@inproceedings{mekala-etal-2020-meta,\\n    title = \\\"{META}: Metadata-Empowered Weak Supervision for Text Classification\\\",\\n    author = \\\"Mekala, Dheeraj  and\\n      Zhang, Xinyang  and\\n      Shang, Jingbo\\\",\\n    booktitle = \\\"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\\\",\\n    month = nov,\\n    year = \\\"2020\\\",\\n    address = \\\"Online\\\",\\n    publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https://www.aclweb.org/anthology/2020.emnlp-main.670\\\",\\n    doi = \\\"10.18653/v1/2020.emnlp-main.670\\\",\\n    pages = \\\"8351--8361\\\",\\n    abstract = \\\"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as {``}seed motifs{''}, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.\\\",\\n}\\n\\n```\\n\\n## About\\n\\nCode for the paper \\\"META: Metadata-Empowered Weak Supervision for Text Classification\\\"\\n\\n### Topics\\n\\n[...\\n\\n--- Result 3 ---\\nTitle: GitHub - AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe: Classifies 500K+ political tweets from Northern Europe using NLP and machine learning to analyze political discourse.\\nURL: https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe\\n\\nContent:\\n<div><div><article><p></p><h2>üê¶ Kaggle Project: Classification of Tweets from Northern Europe</h2><a href=\\\"#-kaggle-project-classification-of-tweets-from-northern-europe\\\"></a><p></p>\\n<p><a href=\\\"https://camo.githubusercontent.com/439fd5ffe2ac21b3daa03ac6474978163be089da5fbba2790769ae2275464e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\\\"></a></p>\\n<p></p><h2>üöÄ Overview</h2><a href=\\\"#-overview\\\"></a><p></p>\\n<p>This project analyzes a dataset of <strong>509,031 tweets</strong> from politicians across <strong>seven Northern European countries</strong>, aiming to classify tweets by <strong>political spectrum (Left, Right, Center)</strong> and <strong>geography</strong>. Using <strong>Natural Language Processing (NLP)</strong> and machine learning, we extracted key insights into political discourse and developed a classifier with strong performance.</p>\\n<hr/>\\n<p></p><h2>üìä Dataset</h2><a href=\\\"#-dataset\\\"></a><p></p>\\n<ul>\\n<li><strong>Countries:</strong> Belgium, Denmark, Iceland, Ireland, Netherlands, Norway, Sweden</li>\\n<li><strong>Time span:</strong> Dec 2008 ‚Äì Jan 2023</li>\\n<li><strong>Dataset size:</strong> 509,031 tweets\\n<ul>\\n<li><strong>Training set:</strong> 407,223 tweets</li>\\n<li><strong>Test set:</strong> 101,808 tweets</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Attributes:</strong></p>\\n<ul>\\n<li>Tweet text</li>\\n<li>Country</li>\\n<li>Gender</li>\\n<li>Political spectrum label (for supervised learning)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üî¨ Methods</h2><a href=\\\"#-methods\\\"></a><p></p>\\n<p></p><h3>Data Cleaning &amp; Preprocessing</h3><a href=\\\"#data-cleaning--preprocessing\\\"></a><p></p>\\n<ul>\\n<li>Text cleaning: Lowercasing, punctuation removal, stopword removal, lemmatization.</li>\\n<li>New feature creation: <code>text_clean_country_gender_user</code> combining multiple metadata.</li>\\n</ul>\\n<p></p><h3>Feature Engineering</h3><a href=\\\"#feature-engineering\\\"></a><p></p>\\n<ul>\\n<li><strong>Vectorization:</strong>\\n<ul>\\n<li>Count Vectorizer</li>\\n<li>TF-IDF transformation</li>\\n</ul>\\n</li>\\n</ul>\\n<p></p><h3>Modeling</h3><a href=\\\"#modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>Classifier:</strong> Linear Support Vector Classifier (LinearSVC)</li>\\n<li><strong>Validation:</strong> 10-fold cross-validation</li>\\n</ul>\\n<p></p><h3>Topic Modeling</h3><a href=\\\"#topic-modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>LDA (Latent Dirichlet Allocation)</strong></li>\\n<li><strong>NMF (Non-negative Matrix Factorization)</strong></li>\\n</ul>\\n<hr/>\\n<p></p><h2>üìà Results</h2><a href=\\\"#-results\\\"></a><p></p>\\n<p>‚úÖ <strong>Key findings:</strong></p>\\n<ul>\\n<li>Tweets span diverse topics from <strong>domestic politics</strong> to <strong>international relations</strong>.</li>\\n<li>Different political leanings across countries: e.g., Netherlands leans Center, Iceland leans Left.</li>\\n<li>Gender distribution varies: Sweden had a <strong>female-majority</strong>, others skewed male.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üõ†Ô∏è Requirements</h2><a href=\\\"#Ô∏è-requirements\\\"></a><p></p>\\n<ul>\\n<li>Python 3.8+</li>\\n<li>Key libraries:\\n<ul>\\n<li>pandas, numpy</li>\\n<li>scikit-learn</li>\\n<li>nltk, spaCy</li>\\n<li>gensim</li>\\n<li>matplotlib, seaborn</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Install via:</p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<hr/>\\n<p></p><h2>üñ•Ô∏è Usage</h2><a href=\\\"#Ô∏è-usage\\\"></a><p></p>\\n<p>1Ô∏è‚É£ <strong>Clone the repository:</strong></p>\\n<div><pre>git clone https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe.git\\n<span>cd</span> kaggle-project-classification-of-tweets-from-northern-europe</pre></div>\\n<p>2Ô∏è‚É£ <strong>Install dependencies:</strong></p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<p>3Ô∏è‚É£ <strong>Run analysis:</strong></p>\\n<ul>\\n<li>Open <code>kaggle_project.ipynb</code> for full preprocessing, modeling, and evaluation workflows.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üìÇ Repository Structure</h2><a href=\\\"#-repository-structure\\\"></a><p></p>\\n<div><pre><code>‚îú‚îÄ‚îÄ data/ # Dataset (not included)\\n‚îú‚îÄ‚îÄ kaggle_project.ipynb # Main Jupyter notebook\\n‚îú‚îÄ‚îÄ kaggle_project_report.pdf # Final project report\\n‚îú‚îÄ‚îÄ README.md\\n‚îú‚îÄ‚îÄ requirements.txt # Dependencies\\n‚îî‚îÄ‚îÄ LICENSE\\n</code></pre></div>\\n<hr/>\\n<p></p><h2>üë• Contributors</h2><a href=\\\"#-contributors\\\"></a><p></p>\\n<ul>\\n<li><strong>Yuesong Huang</strong> (<a href=\\\"mailto:yhu116@u.rochester.edu\\\">yhu116@u.rochester.edu</a>)</li>\\n<li><strong>Junhua Huang</strong> (<a href=\\\"mailto:jhuang77@u.rochester.edu\\\">jhuang77@u.rochester.edu</a>)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üìÑ License</h2><a href=\\\"#-license\\\"></a><p></p>\\n<p>This project is licensed under the BSD 3-Clause License ‚Äì see the <a href=\\\"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe/blob/main/LICENSE\\\">LICENSE</a> file for details.</p>\\n<hr/>\\n</article></div></div>\\n\\n--- Result 4 ---\\nTitle: Text Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\\n\\nContent:\\nIn this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top NLP competitions.\\n\\nNamely, I‚Äôve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) ‚Äì $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) ‚Äì $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) ‚Äì $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) ‚Äì $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) ‚Äì $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let‚Äôs begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let‚Äôs see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet‚Äôs see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in ¬†R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\\n\\n--- Result 5 ---\\nURL: https://export.arxiv.org/pdf/2301.01808v1.pdf\\n\\nContent:\\nAdar Kahana adarkahana@microsoft.com\\nMicrosoft R&D center\\nILDC\\nOren Elisha orelisha@microsoft.com\\nMicrosoft R&D center\\nILDC\\nMESSAGENET: MESSAGE CLASSIFICATION USING NATURAL LANGUAGE PROCESSING AND META-DATA\\nMessage classification ¬∑ Meta data injection ¬∑ Deep learning ¬∑ Natural language processing\\nIn this paper we propose a new Deep Learning (DL) approach for message classification. Our method is based on the state-of-the-art Natural Language Processing (NLP) building blocks, combined with a novel technique for infusing the meta-data input that is typically available in messages such as the sender information, timestamps, attached image, audio, affiliations, and more. As we demonstrate throughout the paper, going beyond the mere text by leveraging all available channels in the message, could yield an improved representation and higher classification accuracy. To achieve message representation, each type of input is processed in a dedicated block in the neural network architecture that is suitable for the data type. Such an implementation enables training all blocks together simultaneously, and forming cross channels features in the network. We show in the Experiments Section that in some cases, message's meta-data holds an additional information that cannot be extracted just from the text, and when using this information we achieve better performance. Furthermore, we demonstrate that our multi-modality block approach outperforms other approaches for injecting the meta data to the the text classifier.Many real world applications require message classification and regression, such as handling spam emails [1], ticket routing [2], article sentiment review [3] and more. Accurate message classification could improve critical scenarios such as in call centers (routing tickets based on topic) [2], alert systems (flagging highly important alert messages) [4], and categorizing incoming messages (automatically unclutter emails)[1,5]. The main distinction between text and message classification is the availability of additional attributes, such as the sender information, timestamps, attached image, audio, affiliations, and more. New message classification contests often appear in the prominent platforms (i.e., Kaggle [6]), showing how this topic is sought after. There are already many data-sets to explore in this field, but no clear winner algorithm that fits all scenarios with high accuracy, efficiency and simplicity (in terms of implementation and interpretation).A notable advancement in the field of NLP is the attention based transformers architecture[7]. This family of methods excels in finding local connections between words, and better understanding the meaning of a sentence. A leading example is the Bidirectional Encoder Representations from Transformers (BERT) [8] as well as its variations [9, 10, 11], winning certain benchmarks[12,13]. Several packages, such as Huggingface Transformers[14], make such models accessible and easy to use as well as provide pre-trained versions. In addition, one can use transfer learning [15] to further train BERT on their on data, creating a tailored model for the specific task at hand.BERT, and often other transformer based models, are designed to handle text. They operate on the words of a given text by encoding them into tokens, and by the connections between the tokens they learn the context of sentences. This approach is limited, since sometimes more information can be extracted and used, not necessarily textual. Throughout this paper we refer to this information as meta-data to distinguish it from the main stream of textual content (though one may recognize it as the core data, depending on the application). For example, a meta-data could be the time stamp of when the text was written, sent, published, etc. Another example is the writer of the text, when dealing with a small\\nlist of writers of a corpus. There have been some attempts to incorporate these into BERT models, for example by assigning artificial tokens for writers or for temporal segments (token per month for example) [16]. This approach is limited since not all meta-data entries are suitable for encoding by tokenization. In the example of temporal segments, more segments introduce more tokens, leading to large computational resources consumption, and less segments cause loss of information. Another approach is to concatenate the embeddings, created by the transformer module, with the outputs of an embedding module for the meta-data. In this approach, a transformer for the text is trained (using direct or transfer learning) on the text, and other separate modules (time series embedding, senders embeddings, etc.) are used to embed the meta-data. All the embeddings are\\n\\n--- Result 6 ---\\nTitle: Search code, repositories, users, issues, pull requests...\\nURL: https://github.com/r-salas/bert-kaggle-stackoverflow\\n\\nContent:\\nGitHub - r-salas/bert-kaggle-stackoverflow: Predict if a question from StackOverflow will be closed by fine-tuning BERT.\\n[Skip to content](#start-of-content)\\n## Navigation Menu\\nToggle navigation\\n[](https://github.com/)\\n[Sign in](https://github.com/login?return_to=https://github.com/r-salas/bert-kaggle-stackoverflow)\\nAppearance settings\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\n</option></form>\\nSearch\\nClear\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\n# Provide feedback\\n</option></form>\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancelSubmit feedback\\n# Saved searches\\n## Use saved searches to filter your results more quickly\\n</option></form>\\nName\\nQuery\\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\nCancelCreate saved search\\n[Sign in](https://github.com/login?return_to=https://github.com/r-salas/bert-kaggle-stackoverflow)\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=r-salas/bert-kaggle-stackoverflow)\\nAppearance settings\\nResetting focus\\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\\n{{ message }}\\n[r-salas](https://github.com/r-salas)/**[bert-kaggle-stackoverflow](https://github.com/r-salas/bert-kaggle-stackoverflow)**Public\\n* [Notifications](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)You must be signed in to change notification settings\\n* [Fork0](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)\\n* [Star0](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)\\nPredict if a question from StackOverflow will be closed by fine-tuning BERT.\\n### License\\n[MIT license](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/LICENSE)\\n[0stars](https://github.com/r-salas/bert-kaggle-stackoverflow/stargazers)[0forks](https://github.com/r-salas/bert-kaggle-stackoverflow/forks)[Branches](https://github.com/r-salas/bert-kaggle-stackoverflow/branches)[Tags](https://github.com/r-salas/bert-kaggle-stackoverflow/tags)[Activity](https://github.com/r-salas/bert-kaggle-stackoverflow/activity)\\n[Star](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)\\n[Notifications](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)You must be signed in to change notification settings\\n# r-salas/bert-kaggle-stackoverflow\\nmain\\n[Branches](https://github.com/r-salas/bert-kaggle-stackoverflow/branches)[Tags](https://github.com/r-salas/bert-kaggle-stackoverflow/tags)\\n[](https://github.com/r-salas/bert-kaggle-stackoverflow/branches)[](https://github.com/r-salas/bert-kaggle-stackoverflow/tags)\\nGo to file\\nCode\\nOpen more actions menu\\n## Folders and files\\n|Name|Name|\\nLast commit message\\n|\\nLast commit date\\n|\\n## Latest commit\\n## History\\n[57 Commits](https://github.com/r-salas/bert-kaggle-stackoverflow/commits/main/)\\n[](https://github.com/r-salas/bert-kaggle-stackoverflow/commits/main/)\\n|\\n[.gitignore](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/.gitignore)\\n|\\n[.gitignore](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/.gitignore)\\n|\\n|\\n|\\n[LICENSE](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/LICENSE)\\n|\\n[LICENSE](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/LICENSE)\\n|\\n|\\n|\\n[Pipfile](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile)\\n|\\n[Pipfile](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile)\\n|\\n|\\n|\\n[Pipfile.lock](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile.lock)\\n|\\n[Pipfile.lock](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile.lock)\\n|\\n|\\n|\\n[README.md](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/README.md)\\n|\\n[README.md](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/README.md)\\n|\\n|\\n|\\n[dataset.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/dataset.py)\\n|\\n[dataset.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/dataset.py)\\n|\\n|\\n|\\n[eda.ipynb](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/eda.ipynb)\\n|\\n[eda.ipynb](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/eda.ipynb)\\n|\\n|\\n|\\n[model.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/model.py)\\n|\\n[model.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/model.py)\\n|\\n|\\n|\\n[train.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/train.py)\\n|\\n[train.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/train.py)\\n|\\n|\\n|\\n[utils.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/utils.py)\\n|\\n[utils.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/utils.py)\\n|\\n|\\n|\\nView all files\\n|\\n## Repository files navigation\\n# bert-kaggle-stackoverflow\\n[](#bert-kaggle-stackoverflow)\\nThis project tries to predict if a question from StackOverflow will be closed. There are 5 classes:\\n* `open`\\n* `not a real question`\\n* `off topic`\\n* `not constructive`\\n* `too localized`\\nOne of the main challenges is that the dataset is hugely imbalanced with 98% of the data belonging to`open`.\\nUndersampling was performed and two approaches were used:\\n* **Binary classification**: to predict whether the question will be closed or not (`85% accuracy`)\\n* **Multiclass classification**: to classify the question into one of the five categories (`65% accuracy`)\\nTwo types of data were used:\\n* `title`&amp;`question`by finetuning BERT for text classification\\n* metadata such as`reputation`,`seconds since registration`,`number of answers`, etc\\n## Installation\\n[](#installation)\\n1. Install Python dependencies\\n```\\n$pipenv install\\n```\\n1. Download dataset**Predict Closed Questions on Stack Overflow**from[Kaggle](https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/data)\\n## Training\\n[](#training)\\n```\\n$pipenv run python train.py predict-closed-questions-on-stack-overflow/train.csv\\n```\\n## About\\nPredict if a question from StackOverflow will be closed by fine-tuning BERT.\\n### Topics\\n[text-classification](https://github.com/topics/text-classification)[pytorch](https://github.com/topics/pytorch)[kaggle](https://github.com/topics/kaggle)[bert](https://github.com/topics/bert)\\n### Resources\\n[Readme](#readme-ov-file)\\n### License\\n[MIT license](#MIT-1-ov-file)\\n### Uh oh!\\nThere was an error while loading.[Please reload this page]().\\n[Activity](https://github.com/r-salas/bert-kaggle-stackoverflow/activity)\\n### Stars\\n[**0**stars](https://github.com/r-salas/bert-kaggle-stackoverflow/stargazers)\\n### Watchers\\n[**1**watching](https://github.com/r-salas/bert-kaggle-stackoverflow/watchers)\\n### Forks\\n[**0**forks](https://github.com/r-salas/bert-kaggle-stackoverflow/forks)\\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/r-salas/bert-kaggle-stackoverflow&amp;report=r-salas+(user))\\n### Uh oh!\\nThere was an error while loading.[Please reload this page]().\\n## Languages\\n* [Jupyter Notebook95.4%](https://github.com/r-salas/bert-kaggle-stackoverflow/search?l=jupyter-notebook)\\n* [Python4.6%](https://github.com/r-salas/bert-kaggle-stackoverflow/search?l=python)\\nYou can‚Äôt perform that action at this time.\\n\\n--- Result 7 ---\\nTitle: GitHub - SergKhachikyan/NaturalLanguageProcessingwithDisasterTweets: Natural Language Processing with Disaster Tweets (Kaggle) This repository contains my full pipeline for the Kaggle competition NLP - Getting Started. The goal is to classify whether a tweet is about a real disaster or not (target = 1 or 0).\\nURL: https://github.com/SergKhachikyan/NaturalLanguageProcessingwithDisasterTweets\\n\\nContent:\\n<div><div><article><p></p><h2>üß† Natural Language Processing with Disaster Tweets (Kaggle)</h2><a href=\\\"#-natural-language-processing-with-disaster-tweets-kaggle\\\"></a><p></p>\\n<p>This repository contains my full pipeline for the Kaggle competition <a href=\\\"https://www.kaggle.com/competitions/nlp-getting-started\\\">NLP - Getting Started</a>. The goal is to classify whether a tweet is about a real disaster or not (<code>target = 1</code> or <code>0</code>).</p>\\n<p></p><h3>üöÄ Installation</h3><a href=\\\"#-installation\\\"></a><p></p>\\n<ol>\\n<li>Clone the repository:</li>\\n</ol>\\n<div><pre><code>git clone https://github.com/SergKhachikyan/London_House_Price_Prediction_Advanced_Techniques.git\\n</code></pre></div>\\n<ol>\\n<li>Change directory:</li>\\n</ol>\\n<div><pre><code>cd London_House_Price_Prediction_Advanced_Techniques\\n</code></pre></div>\\n<ol>\\n<li>Create virtual environment:</li>\\n</ol>\\n<ol>\\n<li>Activate virtual environment:</li>\\n</ol>\\n<ol>\\n<li>Update the package manager:</li>\\n</ol>\\n<p>6.Install dependencies:</p>\\n<div><pre><code>pip install -r requirements.txt\\n</code></pre></div>\\n<p>7.Launch the notebook:</p>\\n<div><pre><code>jupyter notebook untitled.ipynb\\n</code></pre></div>\\n<p></p><h3>üìÇ Project Structure:</h3><a href=\\\"#-project-structure\\\"></a><p></p>\\n<ul>\\n<li><code>train.csv</code> / <code>test.csv</code> ‚Äì original dataset</li>\\n<li><code>submission.csv</code> ‚Äì prediction file for Kaggle submission</li>\\n<li><code>NLP_Disaster_Tweets.ipynb</code> ‚Äì main notebook with full preprocessing, training and evaluation</li>\\n<li><code>README.md</code> ‚Äì this file</li>\\n</ul>\\n<p></p><h3>‚úÖ Key Steps:</h3><a href=\\\"#-key-steps\\\"></a><p></p>\\n<ul>\\n<li>Missing values in <code>keyword</code> filled using weighted random sampling based on value_counts distribution (with <code>normalize=True</code>)</li>\\n<li>Texts cleaned with <code>clean_text()</code> function: lowercased, removed links, punctuation, digits, extra whitespaces</li>\\n<li><code>TfidfVectorizer</code> used with <code>max_features=10000</code>, <code>ngram_range=(1,2)</code>, <code>stop_words='english'</code>, <code>min_df=3</code></li>\\n<li>Trained multiple models: <code>RandomForestClassifier</code>, <code>GridSearchCV(RF)</code>, <code>XGBoostClassifier</code>, <code>LogisticRegression(class_weight='balanced')</code></li>\\n<li>Best performance: <code>LogisticRegression</code> with TF-IDF vectorizer gave <strong>Public Score: <code>0.79098</code></strong></li>\\n<li>Evaluation based on <code>classification_report()</code> with attention to F1-score for class <code>1</code> (disaster-related tweets)</li>\\n</ul>\\n<p></p><h3>üìä Model Comparison (Validation Set):</h3><a href=\\\"#-model-comparison-validation-set\\\"></a><p></p>\\n<p></p><h3>üî• Best Model:</h3><a href=\\\"#-best-model\\\"></a><p></p>\\n<p>LogisticRegression(\\nC=1.0,\\nclass_weight='balanced',\\nmax_iter=1000,\\nrandom_state=42,\\nn_jobs=-1\\n)</p>\\n<p></p><h3>üõ† Possible Next Steps:</h3><a href=\\\"#-possible-next-steps\\\"></a><p></p>\\n<ul>\\n<li>Include keyword as encoded feature</li>\\n<li>Try StackingClassifier with LogReg + XGB + RF</li>\\n<li>Switch to transformers like BERT/RoBERTa</li>\\n<li>Tune TF-IDF (min_df, max_df) or try CountVectorizer</li>\\n<li>Feature importance via coef_ in LogReg</li>\\n</ul>\\n<p></p><h3>üìà Submission Format:</h3><a href=\\\"#-submission-format\\\"></a><p></p>\\n<p>csv\\nid,target\\n0,1\\n1,0\\n2,1\\n...</p>\\n<p></p><h3>üí¨ Final Note:</h3><a href=\\\"#-final-note\\\"></a><p></p>\\n<p>This repo helped me go from 0.75x public score to 0.79098. The most effective combo was cleaned text + tf-idf + balanced logistic regression. Clean, simple, and powerful.</p>\\n</article></div></div>\\n\\n--- Result 8 ---\\nTitle: 20% Accuracy Bump in Text Classification with ME-ULMFiT\\nURL: https://medium.com/novetta/20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78\\n\\nContent:\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4545e993aa78&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\n# 20% Accuracy Bump in Text Classification with ME-ULMFiT\\n\\n[![Matthew Teschke](https://miro.medium.com/v2/resize:fill:88:88/0*nsQhCoB0HTmu_sw5.)](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)[![Novetta](https://miro.medium.com/v2/resize:fill:48:48/1*FLKm3lgYky637lUrGUFvrA.png)](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n[Matthew Teschke](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)\\n\\n¬∑\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb8201a221b3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&user=Matthew+Teschke&userId=cb8201a221b3&source=post_page-cb8201a221b3----4545e993aa78---------------------post_header-----------)\\n\\nPublished in\\n\\n[Novetta](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n¬∑\\n\\n5 min read\\n\\n¬∑\\n\\nMar 31, 2020\\n\\n10\\n\\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D4545e993aa78&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=-----4545e993aa78---------------------post_audio_button-----------)\\n\\nShare\\n\\n_By incorporating metadata into the text before training, we see as much as a 20% increase in accuracy_\\n\\n# Introduction\\n\\nIn a [previous post](https://www.novetta.com/2019/03/introducing_me_ulmfit/), we discussed an enhanced method of passing additional data to the ULMFiT¬π algorithm for unstructured text classification, which improved accuracy by up to 9%. In follow-on experiments, we have demonstrated that this approach ‚Äî in which we prepend metadata to the text to provide more information to the model ‚Äî increased accuracy by up to 20%, in relative terms. This improvement required just a few lines of Python, such that the performance boost comes at essentially no cost.\\n\\nThis blog post explains the findings of this more-detailed evaluation, where we have compared how the following affect accuracy:\\n\\n1. Each individual piece of metadata\\n2. Different combinations of metadata\\n3. Different ways of passing the metadata to the model\\n\\nFor the evaluation, we worked with six [Novetta Mission Analytics](https://www.novetta.com/products/novetta-mission-analytics/) datasets, described below. The objective is to assign metadata tags to quotes, using granular categories called Submessages.\\n\\nSome highlights from our results include:\\n\\n- Source-related metadata tags consistently performed the best, confirming our original hypothesis. Sources (e.g. UN Official) are the metadata most closely associated with the quotes we are tagging, so it makes sense that sources add the most information.\\n- Including media name (e.g. New York Times) either decreased accuracy or showed little improvement. We found this surprising as we had assumed that including metadata would only improve accuracy.\\n- Adding more metadata tags to the model did not always improve performance.\\n- For five of the six datasets, using metadata tags that distinguish between columns in our data performed better than the approach (currently suggested by the fast.ai library) in which each field was demarcated by xxfld.\\n\\n# Dataset Overview\\n\\nWe conducted evaluations on the six datasets listed in Table 1. For each dataset, we made the following splits time-based between training and validation, where later data is held out for validation. We used 80% of the data for training and 17% for validation on the language model. Using that 97% of the data, we then trained the classifier, reserving the last 3% for the final validation.\\n\\n![](https://miro.medium.com/v2/resize:fit:700/1*WtQcwQm1EIIoHz3Gy6EoaA.png)\\n\\nTable 1: Characteristics of Datasets Used in ME-ULMFiT Evaluation\\n\\n# Experiment 1: Individual Metadata\\n\\nOur first experiment evaluated how individual pieces of metadata impacted performance of the model. Working with six types of metadata (Author Name, Media Name, Media Nation, Source Name, Source Nation, Source Type), we created a model that prepended only that piece of information to the quote text. For example, a Source Name of ‚ÄúSteffen Seibert‚Äù prepended to a quote results in:\\n\\n```\\nsource_name source_name_Steffen_Seibert quote_text that ‚Äúwe have started down the path‚Äù towards the NATO goal.\\n```\\n\\nThe results of Experiment 1 are shown in the following heat map:\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*6-wTYPDfG1meArIiwnA1oA.png)\\n\\nFigure 1: Heat map showing relative change in accuracy of ME-ULMFiT with individual types of metadata compared to the baseline ULMFiT model with no metadata\\n\\nFrom the heat map, we made the following observations:\\n\\n- Source-related metadata tags generally performed the best\\n- The Media Name metadata tag performed the worst for four out of six datasets\\n- Datasets with fewer Submessage labels saw less improvement from the inclusion of metadata than those with more Submessage labels\\n\\n# Experiment 2: Combinations of Metadata\\n\\nIn the second experiment, we evaluated how different combinations of metadata impacted model accuracy. Our intuition suggested that more metadata would always lead to better performance, but as shown in Figure 2, this was not the case. We will be conducting further investigation into why.\\n\\nWe examined five different combinations of metadata as follows:\\n\\n- The top-2 performing metadata tags from Experiment 1\\n- The top-3 performing metadata tags from Experiment 1\\n- The top-4 performing metadata tags from Experiment 1\\n- All metadata tags\\n- Fast.ai built-in tags, which utilized all metadata but whose fields are separated by xxfld instead of column-specific tags. In this method, values within the data are not concatenated (e.g., The New York Times was not converted to xxfld\\\\_The\\\\_New\\\\_York\\\\_Times)\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*UQW6om7aQzCpUEp9pvyQpQ.png)\\n\\nFigure 2: Heat map showing relative change in accuracy of ME-ULMFiT with combinations of metadata compared to the baseline ULMFiT model with no metadata\\n\\nWhile we see that more often than not multiple tags improve performance over the best baseline, that is not always the case. For only one dataset did the model with all metadata perform the best and it generally underperformed the most accurate individual metadata model. We believe this has to do with the way batches are created within the model, so we will explore further to see why more data is not always improving accuracy.\\n\\n# Experiment 3: Methods of Passing Metadata\\n\\nLastly, for the models that included all metadata, we wanted to compare different methods of passing in metadata. Our [original post](https://www.novetta.com/2019/03/introducing_me_ulmfit/) describes our method of using separate tags for each metadata field and concatenating all valu...\\n\\n--- Result 9 ---\\nTitle: How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\\n\\nContent:\\nHow To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n‚ÜêPrevious\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Winning Kaggle solutions for text classification often treat the accompanying metadata (e.g., author, timestamp, country, gender, device) as a first‚Äëclass source of supervision rather than an after‚Äëthought.  The‚ÄØMETA‚ÄØframework showed that organizing text and its heterogeneous metadata into a ‚Äútext‚Äërich network‚Äù and extracting high‚Äëquality‚ÄØseed motifs‚ÄØfrom this graph can generate powerful weak labels that bootstrap a classifier and iteratively expand both seed words and seed motifs‚ÄØ([arXiv‚ÄØ2020](https://arxiv.org/pdf/2301.01808v1.pdf)).  In practice this means encoding each metadata type (categorical or numeric) with appropriate embeddings or target‚Äëencoding, then feeding the resulting vectors into the same model that processes the text‚Äîeither by concatenating them to the final hidden state of a transformer (BERT, ULMFiT) or by passing them through dedicated ‚Äúmetadata blocks‚Äù that are jointly trained with the text encoder‚ÄØ([MESSAGENET‚ÄØ2023](https://export.arxiv.org/pdf/2301.01808v1.pdf)).  The code base for META is openly available on GitHub, making it easy to reproduce the motif‚Äëranking and bootstrapping pipeline for any Kaggle dataset‚ÄØ([GitHub‚ÄØMETA](https://github.com/dheeraj7596/META)).\\n\\nOn the feature‚Äëengineering side, top Kaggle notebooks routinely create composite fields that fuse raw text with categorical metadata (e.g.,‚ÄØ`text_clean_country_gender_user`‚ÄØin a political‚Äëtweet competition) and then apply classic vectorizers such as‚ÄØTF‚ÄëIDF‚ÄØor‚ÄØCountVectorizer‚ÄØon the combined string‚ÄØ([AnakinHuang‚ÄØ2025](https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe)).  Gradient‚Äëboosted trees that natively handle categorical inputs‚ÄîLightGBM, CatBoost, or XGBoost‚Äîare then trained on a feature set that includes both the text embeddings (often reduced with SVD or PCA) and the encoded metadata.  Ensembling multiple models (e.g., a transformer‚Äëbased classifier plus a tree‚Äëbased model) and using careful cross‚Äëvalidation on the metadata splits further boosts robustness, a pattern repeatedly highlighted in Neptune‚Äôs ‚ÄúText Classification: All Tips and Tricks from 5 Kaggle Competitions‚Äù guide‚ÄØ([Neptune‚ÄØ2022](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).\\n\\nIn summary, the most successful Kaggle pipelines for text classification with metadata combine (1) **metadata‚Äëdriven weak supervision** (seed motifs, multi‚Äëmodal blocks), (2) **joint representation learning** where metadata embeddings are merged with deep text encoders, and (3) **hybrid modeling** that blends transformer outputs with tree‚Äëbased learners on engineered metadata features.  Applying these strategies‚Äîsupported by publicly released code and proven in recent competitions‚Äîhas consistently delivered the performance jumps needed to reach the podium.\", \"url\": \"\"}, {\"title\": \"GitHub - dheeraj7596/META: Code for the paper \\\"META: Metadata-Empowered Weak Supervision for Text Classification\\\"\", \"text\": \"[Skip to content](https://github.com/dheeraj7596/META#start-of-content)\\n\\nYou signed in with another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.Dismiss alert\\n\\n{{ message }}\\n\\n[dheeraj7596](https://github.com/dheeraj7596)/ **[META](https://github.com/dheeraj7596/META)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA) You must be signed in to change notification settings\\n- [Fork\\\\\\n0](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\\n- [Star\\\\\\n7](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\\n\\n\\nCode for the paper \\\"META: Metadata-Empowered Weak Supervision for Text Classification\\\"\\n\\n[7\\\\\\nstars](https://github.com/dheeraj7596/META/stargazers) [0\\\\\\nforks](https://github.com/dheeraj7596/META/forks) [Branches](https://github.com/dheeraj7596/META/branches) [Tags](https://github.com/dheeraj7596/META/tags) [Activity](https://github.com/dheeraj7596/META/activity)\\n\\n[Star](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\\n\\n[Notifications](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA) You must be signed in to change notification settings\\n\\n# dheeraj7596/META\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmaster\\n\\n[Branches](https://github.com/dheeraj7596/META/branches) [Tags](https://github.com/dheeraj7596/META/tags)\\n\\nGo to file\\n\\nCode\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit<br>## History<br>[24 Commits](https://github.com/dheeraj7596/META/commits/master/) |\\n| [data](https://github.com/dheeraj7596/META/tree/master/data) | [data](https://github.com/dheeraj7596/META/tree/master/data) |  |  |\\n| [docs](https://github.com/dheeraj7596/META/tree/master/docs) | [docs](https://github.com/dheeraj7596/META/tree/master/docs) |  |  |\\n| [keras\\\\_han](https://github.com/dheeraj7596/META/tree/master/keras_han) | [keras\\\\_han](https://github.com/dheeraj7596/META/tree/master/keras_han) |  |  |\\n| [util](https://github.com/dheeraj7596/META/tree/master/util) | [util](https://github.com/dheeraj7596/META/tree/master/util) |  |  |\\n| [README.md](https://github.com/dheeraj7596/META/blob/master/README.md) | [README.md](https://github.com/dheeraj7596/META/blob/master/README.md) |  |  |\\n| [preprocess.py](https://github.com/dheeraj7596/META/blob/master/preprocess.py) | [preprocess.py](https://github.com/dheeraj7596/META/blob/master/preprocess.py) |  |  |\\n| [train.py](https://github.com/dheeraj7596/META/blob/master/train.py) | [train.py](https://github.com/dheeraj7596/META/blob/master/train.py) |  |  |\\n| [train.sh](https://github.com/dheeraj7596/META/blob/master/train.sh) | [train.sh](https://github.com/dheeraj7596/META/blob/master/train.sh) |  |  |\\n| View all files |\\n\\n## Repository files navigation\\n\\n# META: Metadata-Empowered Weak Supervision for Text Classification\\n\\n- [Model](https://github.com/dheeraj7596/META#model)\\n- [Training](https://github.com/dheeraj7596/META#training)\\n  - [Required Inputs](https://github.com/dheeraj7596/META#required-inputs)\\n  - [Commands](https://github.com/dheeraj7596/META#commands)\\n  - [Requirements](https://github.com/dheeraj7596/META#requirements)\\n- [Citation](https://github.com/dheeraj7596/META#citation)\\n\\n## Model\\n\\n[![META-Framework](https://github.com/dheeraj7596/META/raw/master/docs/META-overview.png)](https://github.com/dheeraj7596/META/blob/master/docs/META-overview.png)\\n\\n## Training\\n\\n### Required inputs\\n\\nEach Dataset should contain following files:\\n\\n- **DataFrame pickle file**\\n  - Example: `data/books/df.pkl`\\n    - This dataset can contain any number of columns but must contain two columns named `text`, `label`\\n    - `text` contains text and `label` contains its corresponding label.\\n    - Must be named as `df.pkl`\\n- **Seed Words Json file**\\n  - Example: `data/books/seedwords.json`\\n    - This json file contains seed words list for each label.\\n    - Must be named as `seedwords.json`\\n- **Metadata config file**\\n  - Example: `data/books/metadata_config.json`\\n    - This json file contains metadata type as key and there must be `separator` in the values which indicates\\n      the separator of that entry in the dataframe.\\n    - If no separator is required, the value for `separator` should be `null`.\\n    - For example, if authors information are provided as a comma-separated string, the `separator` should be `\\\",\\\"`.\\n- **Motif Patterns file**\\n  - Example: `data/books/motif_patterns.txt`\\n    - This text file contains one motif pattern per line.\\n    - A motif pattern is represented as comma-separated string. For example: `authors, authors` represents\\n      co-authorship.\\n\\n### Commands\\n\\n#### META - Iterative Framework:\\n\\nThe `train.sh` requires five arguments in the following order:\\n\\n- `use_gpu`: indicates whether to use GPU. This should be set to 1 to use GPU and 0 to use CPU.\\n- `GPU`: refers to GPU id.\\n- `dataset_path`: refers to absolute path of dataset.\\n- `tmp_path`: refers to a path to a temporary directory which is used for dumping intermediate files.\\n- `print_flag`: indicates whether to print the expanded phrases and motif instances. This should be set to 1\\nto print and 0 to not print.\\n\\nFor example, to train META on books dataset on GPU Id 4, please run:\\n\\n```\\n$ ./train.sh 1 4 <DATA_PATH TO BOOKS> <TEMP_DIR PATH> 1\\n```\\n\\n### Requirements\\n\\nThis project is based on `python==3.7`. The dependencies are as follow:\\n\\n```\\nkeras==2.1.5\\nscikit-learn==0.21.3\\nscipy=1.3.1\\ngensim==3.8.1\\nnumpy==1.17.2\\nfast-pagerank==0.0.4\\ntensorflow==1.15\\nnltk\\nbleach==3.1.5\\npandas\\nbeautifulsoup4\\n\\n```\\n\\n## Citation\\n\\n```\\n@inproceedings{mekala-etal-2020-meta,\\n    title = \\\"{META}: Metadata-Empowered Weak Supervision for Text Classification\\\",\\n    author = \\\"Mekala, Dheeraj  and\\n      Zhang, Xinyang  and\\n      Shang, Jingbo\\\",\\n    booktitle = \\\"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\\\",\\n    month = nov,\\n    year = \\\"2020\\\",\\n    address = \\\"Online\\\",\\n    publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https://www.aclweb.org/anthology/2020.emnlp-main.670\\\",\\n    doi = \\\"10.18653/v1/2020.emnlp-main.670\\\",\\n    pages = \\\"8351--8361\\\",\\n    abstract = \\\"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as {``}seed motifs{''}, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.\\\",\\n}\\n\\n```\\n\\n## About\\n\\nCode for the paper \\\"META: Metadata-Empowered Weak Supervision for Text Classification\\\"\\n\\n### Topics\\n\\n[...\", \"url\": \"https://github.com/dheeraj7596/META\"}, {\"title\": \"GitHub - AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe: Classifies 500K+ political tweets from Northern Europe using NLP and machine learning to analyze political discourse.\", \"text\": \"<div><div><article><p></p><h2>üê¶ Kaggle Project: Classification of Tweets from Northern Europe</h2><a href=\\\"#-kaggle-project-classification-of-tweets-from-northern-europe\\\"></a><p></p>\\n<p><a href=\\\"https://camo.githubusercontent.com/439fd5ffe2ac21b3daa03ac6474978163be089da5fbba2790769ae2275464e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\\\"></a></p>\\n<p></p><h2>üöÄ Overview</h2><a href=\\\"#-overview\\\"></a><p></p>\\n<p>This project analyzes a dataset of <strong>509,031 tweets</strong> from politicians across <strong>seven Northern European countries</strong>, aiming to classify tweets by <strong>political spectrum (Left, Right, Center)</strong> and <strong>geography</strong>. Using <strong>Natural Language Processing (NLP)</strong> and machine learning, we extracted key insights into political discourse and developed a classifier with strong performance.</p>\\n<hr/>\\n<p></p><h2>üìä Dataset</h2><a href=\\\"#-dataset\\\"></a><p></p>\\n<ul>\\n<li><strong>Countries:</strong> Belgium, Denmark, Iceland, Ireland, Netherlands, Norway, Sweden</li>\\n<li><strong>Time span:</strong> Dec 2008 ‚Äì Jan 2023</li>\\n<li><strong>Dataset size:</strong> 509,031 tweets\\n<ul>\\n<li><strong>Training set:</strong> 407,223 tweets</li>\\n<li><strong>Test set:</strong> 101,808 tweets</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Attributes:</strong></p>\\n<ul>\\n<li>Tweet text</li>\\n<li>Country</li>\\n<li>Gender</li>\\n<li>Political spectrum label (for supervised learning)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üî¨ Methods</h2><a href=\\\"#-methods\\\"></a><p></p>\\n<p></p><h3>Data Cleaning &amp; Preprocessing</h3><a href=\\\"#data-cleaning--preprocessing\\\"></a><p></p>\\n<ul>\\n<li>Text cleaning: Lowercasing, punctuation removal, stopword removal, lemmatization.</li>\\n<li>New feature creation: <code>text_clean_country_gender_user</code> combining multiple metadata.</li>\\n</ul>\\n<p></p><h3>Feature Engineering</h3><a href=\\\"#feature-engineering\\\"></a><p></p>\\n<ul>\\n<li><strong>Vectorization:</strong>\\n<ul>\\n<li>Count Vectorizer</li>\\n<li>TF-IDF transformation</li>\\n</ul>\\n</li>\\n</ul>\\n<p></p><h3>Modeling</h3><a href=\\\"#modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>Classifier:</strong> Linear Support Vector Classifier (LinearSVC)</li>\\n<li><strong>Validation:</strong> 10-fold cross-validation</li>\\n</ul>\\n<p></p><h3>Topic Modeling</h3><a href=\\\"#topic-modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>LDA (Latent Dirichlet Allocation)</strong></li>\\n<li><strong>NMF (Non-negative Matrix Factorization)</strong></li>\\n</ul>\\n<hr/>\\n<p></p><h2>üìà Results</h2><a href=\\\"#-results\\\"></a><p></p>\\n<p>‚úÖ <strong>Key findings:</strong></p>\\n<ul>\\n<li>Tweets span diverse topics from <strong>domestic politics</strong> to <strong>international relations</strong>.</li>\\n<li>Different political leanings across countries: e.g., Netherlands leans Center, Iceland leans Left.</li>\\n<li>Gender distribution varies: Sweden had a <strong>female-majority</strong>, others skewed male.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üõ†Ô∏è Requirements</h2><a href=\\\"#Ô∏è-requirements\\\"></a><p></p>\\n<ul>\\n<li>Python 3.8+</li>\\n<li>Key libraries:\\n<ul>\\n<li>pandas, numpy</li>\\n<li>scikit-learn</li>\\n<li>nltk, spaCy</li>\\n<li>gensim</li>\\n<li>matplotlib, seaborn</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Install via:</p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<hr/>\\n<p></p><h2>üñ•Ô∏è Usage</h2><a href=\\\"#Ô∏è-usage\\\"></a><p></p>\\n<p>1Ô∏è‚É£ <strong>Clone the repository:</strong></p>\\n<div><pre>git clone https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe.git\\n<span>cd</span> kaggle-project-classification-of-tweets-from-northern-europe</pre></div>\\n<p>2Ô∏è‚É£ <strong>Install dependencies:</strong></p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<p>3Ô∏è‚É£ <strong>Run analysis:</strong></p>\\n<ul>\\n<li>Open <code>kaggle_project.ipynb</code> for full preprocessing, modeling, and evaluation workflows.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üìÇ Repository Structure</h2><a href=\\\"#-repository-structure\\\"></a><p></p>\\n<div><pre><code>‚îú‚îÄ‚îÄ data/ # Dataset (not included)\\n‚îú‚îÄ‚îÄ kaggle_project.ipynb # Main Jupyter notebook\\n‚îú‚îÄ‚îÄ kaggle_project_report.pdf # Final project report\\n‚îú‚îÄ‚îÄ README.md\\n‚îú‚îÄ‚îÄ requirements.txt # Dependencies\\n‚îî‚îÄ‚îÄ LICENSE\\n</code></pre></div>\\n<hr/>\\n<p></p><h2>üë• Contributors</h2><a href=\\\"#-contributors\\\"></a><p></p>\\n<ul>\\n<li><strong>Yuesong Huang</strong> (<a href=\\\"mailto:yhu116@u.rochester.edu\\\">yhu116@u.rochester.edu</a>)</li>\\n<li><strong>Junhua Huang</strong> (<a href=\\\"mailto:jhuang77@u.rochester.edu\\\">jhuang77@u.rochester.edu</a>)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>üìÑ License</h2><a href=\\\"#-license\\\"></a><p></p>\\n<p>This project is licensed under the BSD 3-Clause License ‚Äì see the <a href=\\\"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe/blob/main/LICENSE\\\">LICENSE</a> file for details.</p>\\n<hr/>\\n</article></div></div>\", \"url\": \"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe\"}, {\"title\": \"Text Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle‚Äôs top NLP competitions.\\n\\nNamely, I‚Äôve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) ‚Äì $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) ‚Äì $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) ‚Äì $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) ‚Äì $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) ‚Äì $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let‚Äôs begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let‚Äôs see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet‚Äôs see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in ¬†R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\", \"url\": \"https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\"}, {\"title\": \"\", \"text\": \"Adar Kahana adarkahana@microsoft.com\\nMicrosoft R&D center\\nILDC\\nOren Elisha orelisha@microsoft.com\\nMicrosoft R&D center\\nILDC\\nMESSAGENET: MESSAGE CLASSIFICATION USING NATURAL LANGUAGE PROCESSING AND META-DATA\\nMessage classification ¬∑ Meta data injection ¬∑ Deep learning ¬∑ Natural language processing\\nIn this paper we propose a new Deep Learning (DL) approach for message classification. Our method is based on the state-of-the-art Natural Language Processing (NLP) building blocks, combined with a novel technique for infusing the meta-data input that is typically available in messages such as the sender information, timestamps, attached image, audio, affiliations, and more. As we demonstrate throughout the paper, going beyond the mere text by leveraging all available channels in the message, could yield an improved representation and higher classification accuracy. To achieve message representation, each type of input is processed in a dedicated block in the neural network architecture that is suitable for the data type. Such an implementation enables training all blocks together simultaneously, and forming cross channels features in the network. We show in the Experiments Section that in some cases, message's meta-data holds an additional information that cannot be extracted just from the text, and when using this information we achieve better performance. Furthermore, we demonstrate that our multi-modality block approach outperforms other approaches for injecting the meta data to the the text classifier.Many real world applications require message classification and regression, such as handling spam emails [1], ticket routing [2], article sentiment review [3] and more. Accurate message classification could improve critical scenarios such as in call centers (routing tickets based on topic) [2], alert systems (flagging highly important alert messages) [4], and categorizing incoming messages (automatically unclutter emails)[1,5]. The main distinction between text and message classification is the availability of additional attributes, such as the sender information, timestamps, attached image, audio, affiliations, and more. New message classification contests often appear in the prominent platforms (i.e., Kaggle [6]), showing how this topic is sought after. There are already many data-sets to explore in this field, but no clear winner algorithm that fits all scenarios with high accuracy, efficiency and simplicity (in terms of implementation and interpretation).A notable advancement in the field of NLP is the attention based transformers architecture[7]. This family of methods excels in finding local connections between words, and better understanding the meaning of a sentence. A leading example is the Bidirectional Encoder Representations from Transformers (BERT) [8] as well as its variations [9, 10, 11], winning certain benchmarks[12,13]. Several packages, such as Huggingface Transformers[14], make such models accessible and easy to use as well as provide pre-trained versions. In addition, one can use transfer learning [15] to further train BERT on their on data, creating a tailored model for the specific task at hand.BERT, and often other transformer based models, are designed to handle text. They operate on the words of a given text by encoding them into tokens, and by the connections between the tokens they learn the context of sentences. This approach is limited, since sometimes more information can be extracted and used, not necessarily textual. Throughout this paper we refer to this information as meta-data to distinguish it from the main stream of textual content (though one may recognize it as the core data, depending on the application). For example, a meta-data could be the time stamp of when the text was written, sent, published, etc. Another example is the writer of the text, when dealing with a small\\nlist of writers of a corpus. There have been some attempts to incorporate these into BERT models, for example by assigning artificial tokens for writers or for temporal segments (token per month for example) [16]. This approach is limited since not all meta-data entries are suitable for encoding by tokenization. In the example of temporal segments, more segments introduce more tokens, leading to large computational resources consumption, and less segments cause loss of information. Another approach is to concatenate the embeddings, created by the transformer module, with the outputs of an embedding module for the meta-data. In this approach, a transformer for the text is trained (using direct or transfer learning) on the text, and other separate modules (time series embedding, senders embeddings, etc.) are used to embed the meta-data. All the embeddings are\", \"url\": \"https://export.arxiv.org/pdf/2301.01808v1.pdf\"}, {\"title\": \"Search code, repositories, users, issues, pull requests...\", \"text\": \"GitHub - r-salas/bert-kaggle-stackoverflow: Predict if a question from StackOverflow will be closed by fine-tuning BERT.\\n[Skip to content](#start-of-content)\\n## Navigation Menu\\nToggle navigation\\n[](https://github.com/)\\n[Sign in](https://github.com/login?return_to=https://github.com/r-salas/bert-kaggle-stackoverflow)\\nAppearance settings\\nSearch or jump to...\\n# Search code, repositories, users, issues, pull requests...\\n</option></form>\\nSearch\\nClear\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\n# Provide feedback\\n</option></form>\\nWe read every piece of feedback, and take your input very seriously.\\nInclude my email address so I can be contacted\\nCancelSubmit feedback\\n# Saved searches\\n## Use saved searches to filter your results more quickly\\n</option></form>\\nName\\nQuery\\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\nCancelCreate saved search\\n[Sign in](https://github.com/login?return_to=https://github.com/r-salas/bert-kaggle-stackoverflow)\\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=r-salas/bert-kaggle-stackoverflow)\\nAppearance settings\\nResetting focus\\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\\n{{ message }}\\n[r-salas](https://github.com/r-salas)/**[bert-kaggle-stackoverflow](https://github.com/r-salas/bert-kaggle-stackoverflow)**Public\\n* [Notifications](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)You must be signed in to change notification settings\\n* [Fork0](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)\\n* [Star0](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)\\nPredict if a question from StackOverflow will be closed by fine-tuning BERT.\\n### License\\n[MIT license](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/LICENSE)\\n[0stars](https://github.com/r-salas/bert-kaggle-stackoverflow/stargazers)[0forks](https://github.com/r-salas/bert-kaggle-stackoverflow/forks)[Branches](https://github.com/r-salas/bert-kaggle-stackoverflow/branches)[Tags](https://github.com/r-salas/bert-kaggle-stackoverflow/tags)[Activity](https://github.com/r-salas/bert-kaggle-stackoverflow/activity)\\n[Star](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)\\n[Notifications](https://github.com/login?return_to=/r-salas/bert-kaggle-stackoverflow)You must be signed in to change notification settings\\n# r-salas/bert-kaggle-stackoverflow\\nmain\\n[Branches](https://github.com/r-salas/bert-kaggle-stackoverflow/branches)[Tags](https://github.com/r-salas/bert-kaggle-stackoverflow/tags)\\n[](https://github.com/r-salas/bert-kaggle-stackoverflow/branches)[](https://github.com/r-salas/bert-kaggle-stackoverflow/tags)\\nGo to file\\nCode\\nOpen more actions menu\\n## Folders and files\\n|Name|Name|\\nLast commit message\\n|\\nLast commit date\\n|\\n## Latest commit\\n## History\\n[57 Commits](https://github.com/r-salas/bert-kaggle-stackoverflow/commits/main/)\\n[](https://github.com/r-salas/bert-kaggle-stackoverflow/commits/main/)\\n|\\n[.gitignore](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/.gitignore)\\n|\\n[.gitignore](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/.gitignore)\\n|\\n|\\n|\\n[LICENSE](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/LICENSE)\\n|\\n[LICENSE](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/LICENSE)\\n|\\n|\\n|\\n[Pipfile](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile)\\n|\\n[Pipfile](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile)\\n|\\n|\\n|\\n[Pipfile.lock](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile.lock)\\n|\\n[Pipfile.lock](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/Pipfile.lock)\\n|\\n|\\n|\\n[README.md](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/README.md)\\n|\\n[README.md](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/README.md)\\n|\\n|\\n|\\n[dataset.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/dataset.py)\\n|\\n[dataset.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/dataset.py)\\n|\\n|\\n|\\n[eda.ipynb](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/eda.ipynb)\\n|\\n[eda.ipynb](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/eda.ipynb)\\n|\\n|\\n|\\n[model.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/model.py)\\n|\\n[model.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/model.py)\\n|\\n|\\n|\\n[train.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/train.py)\\n|\\n[train.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/train.py)\\n|\\n|\\n|\\n[utils.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/utils.py)\\n|\\n[utils.py](https://github.com/r-salas/bert-kaggle-stackoverflow/blob/main/utils.py)\\n|\\n|\\n|\\nView all files\\n|\\n## Repository files navigation\\n# bert-kaggle-stackoverflow\\n[](#bert-kaggle-stackoverflow)\\nThis project tries to predict if a question from StackOverflow will be closed. There are 5 classes:\\n* `open`\\n* `not a real question`\\n* `off topic`\\n* `not constructive`\\n* `too localized`\\nOne of the main challenges is that the dataset is hugely imbalanced with 98% of the data belonging to`open`.\\nUndersampling was performed and two approaches were used:\\n* **Binary classification**: to predict whether the question will be closed or not (`85% accuracy`)\\n* **Multiclass classification**: to classify the question into one of the five categories (`65% accuracy`)\\nTwo types of data were used:\\n* `title`&amp;`question`by finetuning BERT for text classification\\n* metadata such as`reputation`,`seconds since registration`,`number of answers`, etc\\n## Installation\\n[](#installation)\\n1. Install Python dependencies\\n```\\n$pipenv install\\n```\\n1. Download dataset**Predict Closed Questions on Stack Overflow**from[Kaggle](https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/data)\\n## Training\\n[](#training)\\n```\\n$pipenv run python train.py predict-closed-questions-on-stack-overflow/train.csv\\n```\\n## About\\nPredict if a question from StackOverflow will be closed by fine-tuning BERT.\\n### Topics\\n[text-classification](https://github.com/topics/text-classification)[pytorch](https://github.com/topics/pytorch)[kaggle](https://github.com/topics/kaggle)[bert](https://github.com/topics/bert)\\n### Resources\\n[Readme](#readme-ov-file)\\n### License\\n[MIT license](#MIT-1-ov-file)\\n### Uh oh!\\nThere was an error while loading.[Please reload this page]().\\n[Activity](https://github.com/r-salas/bert-kaggle-stackoverflow/activity)\\n### Stars\\n[**0**stars](https://github.com/r-salas/bert-kaggle-stackoverflow/stargazers)\\n### Watchers\\n[**1**watching](https://github.com/r-salas/bert-kaggle-stackoverflow/watchers)\\n### Forks\\n[**0**forks](https://github.com/r-salas/bert-kaggle-stackoverflow/forks)\\n[Report repository](https://github.com/contact/report-content?content_url=https://github.com/r-salas/bert-kaggle-stackoverflow&amp;report=r-salas+(user))\\n### Uh oh!\\nThere was an error while loading.[Please reload this page]().\\n## Languages\\n* [Jupyter Notebook95.4%](https://github.com/r-salas/bert-kaggle-stackoverflow/search?l=jupyter-notebook)\\n* [Python4.6%](https://github.com/r-salas/bert-kaggle-stackoverflow/search?l=python)\\nYou can‚Äôt perform that action at this time.\", \"url\": \"https://github.com/r-salas/bert-kaggle-stackoverflow\"}, {\"title\": \"GitHub - SergKhachikyan/NaturalLanguageProcessingwithDisasterTweets: Natural Language Processing with Disaster Tweets (Kaggle) This repository contains my full pipeline for the Kaggle competition NLP - Getting Started. The goal is to classify whether a tweet is about a real disaster or not (target = 1 or 0).\", \"text\": \"<div><div><article><p></p><h2>üß† Natural Language Processing with Disaster Tweets (Kaggle)</h2><a href=\\\"#-natural-language-processing-with-disaster-tweets-kaggle\\\"></a><p></p>\\n<p>This repository contains my full pipeline for the Kaggle competition <a href=\\\"https://www.kaggle.com/competitions/nlp-getting-started\\\">NLP - Getting Started</a>. The goal is to classify whether a tweet is about a real disaster or not (<code>target = 1</code> or <code>0</code>).</p>\\n<p></p><h3>üöÄ Installation</h3><a href=\\\"#-installation\\\"></a><p></p>\\n<ol>\\n<li>Clone the repository:</li>\\n</ol>\\n<div><pre><code>git clone https://github.com/SergKhachikyan/London_House_Price_Prediction_Advanced_Techniques.git\\n</code></pre></div>\\n<ol>\\n<li>Change directory:</li>\\n</ol>\\n<div><pre><code>cd London_House_Price_Prediction_Advanced_Techniques\\n</code></pre></div>\\n<ol>\\n<li>Create virtual environment:</li>\\n</ol>\\n<ol>\\n<li>Activate virtual environment:</li>\\n</ol>\\n<ol>\\n<li>Update the package manager:</li>\\n</ol>\\n<p>6.Install dependencies:</p>\\n<div><pre><code>pip install -r requirements.txt\\n</code></pre></div>\\n<p>7.Launch the notebook:</p>\\n<div><pre><code>jupyter notebook untitled.ipynb\\n</code></pre></div>\\n<p></p><h3>üìÇ Project Structure:</h3><a href=\\\"#-project-structure\\\"></a><p></p>\\n<ul>\\n<li><code>train.csv</code> / <code>test.csv</code> ‚Äì original dataset</li>\\n<li><code>submission.csv</code> ‚Äì prediction file for Kaggle submission</li>\\n<li><code>NLP_Disaster_Tweets.ipynb</code> ‚Äì main notebook with full preprocessing, training and evaluation</li>\\n<li><code>README.md</code> ‚Äì this file</li>\\n</ul>\\n<p></p><h3>‚úÖ Key Steps:</h3><a href=\\\"#-key-steps\\\"></a><p></p>\\n<ul>\\n<li>Missing values in <code>keyword</code> filled using weighted random sampling based on value_counts distribution (with <code>normalize=True</code>)</li>\\n<li>Texts cleaned with <code>clean_text()</code> function: lowercased, removed links, punctuation, digits, extra whitespaces</li>\\n<li><code>TfidfVectorizer</code> used with <code>max_features=10000</code>, <code>ngram_range=(1,2)</code>, <code>stop_words='english'</code>, <code>min_df=3</code></li>\\n<li>Trained multiple models: <code>RandomForestClassifier</code>, <code>GridSearchCV(RF)</code>, <code>XGBoostClassifier</code>, <code>LogisticRegression(class_weight='balanced')</code></li>\\n<li>Best performance: <code>LogisticRegression</code> with TF-IDF vectorizer gave <strong>Public Score: <code>0.79098</code></strong></li>\\n<li>Evaluation based on <code>classification_report()</code> with attention to F1-score for class <code>1</code> (disaster-related tweets)</li>\\n</ul>\\n<p></p><h3>üìä Model Comparison (Validation Set):</h3><a href=\\\"#-model-comparison-validation-set\\\"></a><p></p>\\n<p></p><h3>üî• Best Model:</h3><a href=\\\"#-best-model\\\"></a><p></p>\\n<p>LogisticRegression(\\nC=1.0,\\nclass_weight='balanced',\\nmax_iter=1000,\\nrandom_state=42,\\nn_jobs=-1\\n)</p>\\n<p></p><h3>üõ† Possible Next Steps:</h3><a href=\\\"#-possible-next-steps\\\"></a><p></p>\\n<ul>\\n<li>Include keyword as encoded feature</li>\\n<li>Try StackingClassifier with LogReg + XGB + RF</li>\\n<li>Switch to transformers like BERT/RoBERTa</li>\\n<li>Tune TF-IDF (min_df, max_df) or try CountVectorizer</li>\\n<li>Feature importance via coef_ in LogReg</li>\\n</ul>\\n<p></p><h3>üìà Submission Format:</h3><a href=\\\"#-submission-format\\\"></a><p></p>\\n<p>csv\\nid,target\\n0,1\\n1,0\\n2,1\\n...</p>\\n<p></p><h3>üí¨ Final Note:</h3><a href=\\\"#-final-note\\\"></a><p></p>\\n<p>This repo helped me go from 0.75x public score to 0.79098. The most effective combo was cleaned text + tf-idf + balanced logistic regression. Clean, simple, and powerful.</p>\\n</article></div></div>\", \"url\": \"https://github.com/SergKhachikyan/NaturalLanguageProcessingwithDisasterTweets\"}, {\"title\": \"20% Accuracy Bump in Text Classification with ME-ULMFiT\", \"text\": \"[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4545e993aa78&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\n# 20% Accuracy Bump in Text Classification with ME-ULMFiT\\n\\n[![Matthew Teschke](https://miro.medium.com/v2/resize:fill:88:88/0*nsQhCoB0HTmu_sw5.)](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)[![Novetta](https://miro.medium.com/v2/resize:fill:48:48/1*FLKm3lgYky637lUrGUFvrA.png)](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n[Matthew Teschke](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)\\n\\n¬∑\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb8201a221b3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&user=Matthew+Teschke&userId=cb8201a221b3&source=post_page-cb8201a221b3----4545e993aa78---------------------post_header-----------)\\n\\nPublished in\\n\\n[Novetta](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n¬∑\\n\\n5 min read\\n\\n¬∑\\n\\nMar 31, 2020\\n\\n10\\n\\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D4545e993aa78&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=-----4545e993aa78---------------------post_audio_button-----------)\\n\\nShare\\n\\n_By incorporating metadata into the text before training, we see as much as a 20% increase in accuracy_\\n\\n# Introduction\\n\\nIn a [previous post](https://www.novetta.com/2019/03/introducing_me_ulmfit/), we discussed an enhanced method of passing additional data to the ULMFiT¬π algorithm for unstructured text classification, which improved accuracy by up to 9%. In follow-on experiments, we have demonstrated that this approach ‚Äî in which we prepend metadata to the text to provide more information to the model ‚Äî increased accuracy by up to 20%, in relative terms. This improvement required just a few lines of Python, such that the performance boost comes at essentially no cost.\\n\\nThis blog post explains the findings of this more-detailed evaluation, where we have compared how the following affect accuracy:\\n\\n1. Each individual piece of metadata\\n2. Different combinations of metadata\\n3. Different ways of passing the metadata to the model\\n\\nFor the evaluation, we worked with six [Novetta Mission Analytics](https://www.novetta.com/products/novetta-mission-analytics/) datasets, described below. The objective is to assign metadata tags to quotes, using granular categories called Submessages.\\n\\nSome highlights from our results include:\\n\\n- Source-related metadata tags consistently performed the best, confirming our original hypothesis. Sources (e.g. UN Official) are the metadata most closely associated with the quotes we are tagging, so it makes sense that sources add the most information.\\n- Including media name (e.g. New York Times) either decreased accuracy or showed little improvement. We found this surprising as we had assumed that including metadata would only improve accuracy.\\n- Adding more metadata tags to the model did not always improve performance.\\n- For five of the six datasets, using metadata tags that distinguish between columns in our data performed better than the approach (currently suggested by the fast.ai library) in which each field was demarcated by xxfld.\\n\\n# Dataset Overview\\n\\nWe conducted evaluations on the six datasets listed in Table 1. For each dataset, we made the following splits time-based between training and validation, where later data is held out for validation. We used 80% of the data for training and 17% for validation on the language model. Using that 97% of the data, we then trained the classifier, reserving the last 3% for the final validation.\\n\\n![](https://miro.medium.com/v2/resize:fit:700/1*WtQcwQm1EIIoHz3Gy6EoaA.png)\\n\\nTable 1: Characteristics of Datasets Used in ME-ULMFiT Evaluation\\n\\n# Experiment 1: Individual Metadata\\n\\nOur first experiment evaluated how individual pieces of metadata impacted performance of the model. Working with six types of metadata (Author Name, Media Name, Media Nation, Source Name, Source Nation, Source Type), we created a model that prepended only that piece of information to the quote text. For example, a Source Name of ‚ÄúSteffen Seibert‚Äù prepended to a quote results in:\\n\\n```\\nsource_name source_name_Steffen_Seibert quote_text that ‚Äúwe have started down the path‚Äù towards the NATO goal.\\n```\\n\\nThe results of Experiment 1 are shown in the following heat map:\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*6-wTYPDfG1meArIiwnA1oA.png)\\n\\nFigure 1: Heat map showing relative change in accuracy of ME-ULMFiT with individual types of metadata compared to the baseline ULMFiT model with no metadata\\n\\nFrom the heat map, we made the following observations:\\n\\n- Source-related metadata tags generally performed the best\\n- The Media Name metadata tag performed the worst for four out of six datasets\\n- Datasets with fewer Submessage labels saw less improvement from the inclusion of metadata than those with more Submessage labels\\n\\n# Experiment 2: Combinations of Metadata\\n\\nIn the second experiment, we evaluated how different combinations of metadata impacted model accuracy. Our intuition suggested that more metadata would always lead to better performance, but as shown in Figure 2, this was not the case. We will be conducting further investigation into why.\\n\\nWe examined five different combinations of metadata as follows:\\n\\n- The top-2 performing metadata tags from Experiment 1\\n- The top-3 performing metadata tags from Experiment 1\\n- The top-4 performing metadata tags from Experiment 1\\n- All metadata tags\\n- Fast.ai built-in tags, which utilized all metadata but whose fields are separated by xxfld instead of column-specific tags. In this method, values within the data are not concatenated (e.g., The New York Times was not converted to xxfld\\\\_The\\\\_New\\\\_York\\\\_Times)\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*UQW6om7aQzCpUEp9pvyQpQ.png)\\n\\nFigure 2: Heat map showing relative change in accuracy of ME-ULMFiT with combinations of metadata compared to the baseline ULMFiT model with no metadata\\n\\nWhile we see that more often than not multiple tags improve performance over the best baseline, that is not always the case. For only one dataset did the model with all metadata perform the best and it generally underperformed the most accurate individual metadata model. We believe this has to do with the way batches are created within the model, so we will explore further to see why more data is not always improving accuracy.\\n\\n# Experiment 3: Methods of Passing Metadata\\n\\nLastly, for the models that included all metadata, we wanted to compare different methods of passing in metadata. Our [original post](https://www.novetta.com/2019/03/introducing_me_ulmfit/) describes our method of using separate tags for each metadata field and concatenating all valu...\", \"url\": \"https://medium.com/novetta/20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78\"}, {\"title\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n‚ÜêPrevious\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\", \"url\": \"https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\"}]}",
        "tool.duration_ms": 5111.698150634766
      }
    }
  ],
  "metadata": {
    "span_count": 12,
    "last_updated": "2026-01-09T23:50:40.080605"
  }
}