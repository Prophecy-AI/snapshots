{
  "query": "What are winning approaches for text classification with meta-data features on Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle text\u2011classification solutions treat meta\u2011data as a first\u2011class signal rather than an after\u2011thought.  In most competitions (e.g., Jigsaw Toxicity, Quora Insincere, Google\u202fQUEST) the best scores were achieved by **fusing a textual encoder (BERT, RoBERTa or TF\u2011IDF\u202f+\u202fLinear models) with a separate representation of the auxiliary fields** such as author ID, timestamp, length, or attached media\u202f([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  The most common pattern is **early\u2011fusion**: the meta\u2011features are encoded (one\u2011hot, target\u2011encoded, or passed through a small dense network) and concatenated to the final text embedding before the classifier head.  This can be implemented with a multi\u2011modal block that processes each data type in its own sub\u2011network and merges them with attention or simple concatenation, as demonstrated in the MESSAGENET architecture where sender info, timestamps, and images are each handled by a dedicated layer and jointly trained with the text encoder\u202f([arxiv.org](https://export.arxiv.org/pdf/2301.01808v1.pdf)).\n\nA complementary winning strategy is to **use meta\u2011data as a source of weak supervision**.  The META framework builds a \u201ctext\u2011rich network\u201d that links documents to their meta\u2011entities (author, venue, time) and extracts high\u2011confidence \u201cseed motifs\u201d that act as additional pseudo\u2011labels for the classifier\u202f([aclanthology.org](https://aclanthology.org/2020.emnlp-main.670.pdf)).  The open\u2011source implementation on GitHub shows how these motifs can be iteratively expanded and combined with a standard text model to boost performance\u202f([github.com](https://github.com/dheeraj7596/META)).  A simpler linear\u2011model view\u2014treating the problem as\u202f\\(y = \\beta_0 X_{\\text{meta}} + \\beta_1 X_{\\text{text}}\\)\u2014also works well when the meta\u2011features are engineered (e.g., word\u2011count statistics, publication hour) and appended to a TF\u2011IDF matrix before training an SVM or LightGBM\u202f([stats.stackexchange.com](https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model)).\n\nFinally, top Kaggle teams often **stack heterogeneous learners**: a deep\u2011learning model that consumes the fused embeddings, a gradient\u2011boosted tree that ingests the raw meta\u2011features, and a shallow model on TF\u2011IDF alone.  Ensembling these predictions (via weighted averaging or a meta\u2011learner) consistently pushes the leaderboard rank\u202f([bricken.co](https://bricken.co/nlp_disaster_tweets_2)).  Practical tricks such as loading large corpora with Dask or parquet, normalising numeric meta\u2011fields, and using target encoding for high\u2011cardinality categorical meta\u2011data further reduce memory pressure and improve generalisation\u202f([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  Together, these approaches form the backbone of the winning pipelines for text classification with meta\u2011data on Kaggle.",
      "url": ""
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "",
      "text": "Adar Kahana adarkahana@microsoft.com\nMicrosoft R&D center\nILDC\nOren Elisha orelisha@microsoft.com\nMicrosoft R&D center\nILDC\nMESSAGENET: MESSAGE CLASSIFICATION USING NATURAL LANGUAGE PROCESSING AND META-DATA\nMessage classification \u00b7 Meta data injection \u00b7 Deep learning \u00b7 Natural language processing\nIn this paper we propose a new Deep Learning (DL) approach for message classification. Our method is based on the state-of-the-art Natural Language Processing (NLP) building blocks, combined with a novel technique for infusing the meta-data input that is typically available in messages such as the sender information, timestamps, attached image, audio, affiliations, and more. As we demonstrate throughout the paper, going beyond the mere text by leveraging all available channels in the message, could yield an improved representation and higher classification accuracy. To achieve message representation, each type of input is processed in a dedicated block in the neural network architecture that is suitable for the data type. Such an implementation enables training all blocks together simultaneously, and forming cross channels features in the network. We show in the Experiments Section that in some cases, message's meta-data holds an additional information that cannot be extracted just from the text, and when using this information we achieve better performance. Furthermore, we demonstrate that our multi-modality block approach outperforms other approaches for injecting the meta data to the the text classifier.Many real world applications require message classification and regression, such as handling spam emails [1], ticket routing [2], article sentiment review [3] and more. Accurate message classification could improve critical scenarios such as in call centers (routing tickets based on topic) [2], alert systems (flagging highly important alert messages) [4], and categorizing incoming messages (automatically unclutter emails)[1,5]. The main distinction between text and message classification is the availability of additional attributes, such as the sender information, timestamps, attached image, audio, affiliations, and more. New message classification contests often appear in the prominent platforms (i.e., Kaggle [6]), showing how this topic is sought after. There are already many data-sets to explore in this field, but no clear winner algorithm that fits all scenarios with high accuracy, efficiency and simplicity (in terms of implementation and interpretation).A notable advancement in the field of NLP is the attention based transformers architecture[7]. This family of methods excels in finding local connections between words, and better understanding the meaning of a sentence. A leading example is the Bidirectional Encoder Representations from Transformers (BERT) [8] as well as its variations [9, 10, 11], winning certain benchmarks[12,13]. Several packages, such as Huggingface Transformers[14], make such models accessible and easy to use as well as provide pre-trained versions. In addition, one can use transfer learning [15] to further train BERT on their on data, creating a tailored model for the specific task at hand.BERT, and often other transformer based models, are designed to handle text. They operate on the words of a given text by encoding them into tokens, and by the connections between the tokens they learn the context of sentences. This approach is limited, since sometimes more information can be extracted and used, not necessarily textual. Throughout this paper we refer to this information as meta-data to distinguish it from the main stream of textual content (though one may recognize it as the core data, depending on the application). For example, a meta-data could be the time stamp of when the text was written, sent, published, etc. Another example is the writer of the text, when dealing with a small\nlist of writers of a corpus. There have been some attempts to incorporate these into BERT models, for example by assigning artificial tokens for writers or for temporal segments (token per month for example) [16]. This approach is limited since not all meta-data entries are suitable for encoding by tokenization. In the example of temporal segments, more segments introduce more tokens, leading to large computational resources consumption, and less segments cause loss of information. Another approach is to concatenate the embeddings, created by the transformer module, with the outputs of an embedding module for the meta-data. In this approach, a transformer for the text is trained (using direct or transfer learning) on the text, and other separate modules (time series embedding, senders embeddings, etc.) are used to embed the meta-data. All the embeddings are",
      "url": "https://export.arxiv.org/pdf/2301.01808v1.pdf"
    },
    {
      "title": "GitHub - dheeraj7596/META: Code for the paper \"META: Metadata-Empowered Weak Supervision for Text Classification\"",
      "text": "[Skip to content](https://github.com/dheeraj7596/META#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/dheeraj7596/META) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[dheeraj7596](https://github.com/dheeraj7596)/ **[META](https://github.com/dheeraj7596/META)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\n- [Star\\\n7](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\n\n\nCode for the paper \"META: Metadata-Empowered Weak Supervision for Text Classification\"\n\n[7\\\nstars](https://github.com/dheeraj7596/META/stargazers) [0\\\nforks](https://github.com/dheeraj7596/META/forks) [Branches](https://github.com/dheeraj7596/META/branches) [Tags](https://github.com/dheeraj7596/META/tags) [Activity](https://github.com/dheeraj7596/META/activity)\n\n[Star](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA)\n\n[Notifications](https://github.com/login?return_to=%2Fdheeraj7596%2FMETA) You must be signed in to change notification settings\n\n# dheeraj7596/META\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/dheeraj7596/META/branches) [Tags](https://github.com/dheeraj7596/META/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[24 Commits](https://github.com/dheeraj7596/META/commits/master/) |\n| [data](https://github.com/dheeraj7596/META/tree/master/data) | [data](https://github.com/dheeraj7596/META/tree/master/data) |  |  |\n| [docs](https://github.com/dheeraj7596/META/tree/master/docs) | [docs](https://github.com/dheeraj7596/META/tree/master/docs) |  |  |\n| [keras\\_han](https://github.com/dheeraj7596/META/tree/master/keras_han) | [keras\\_han](https://github.com/dheeraj7596/META/tree/master/keras_han) |  |  |\n| [util](https://github.com/dheeraj7596/META/tree/master/util) | [util](https://github.com/dheeraj7596/META/tree/master/util) |  |  |\n| [README.md](https://github.com/dheeraj7596/META/blob/master/README.md) | [README.md](https://github.com/dheeraj7596/META/blob/master/README.md) |  |  |\n| [preprocess.py](https://github.com/dheeraj7596/META/blob/master/preprocess.py) | [preprocess.py](https://github.com/dheeraj7596/META/blob/master/preprocess.py) |  |  |\n| [train.py](https://github.com/dheeraj7596/META/blob/master/train.py) | [train.py](https://github.com/dheeraj7596/META/blob/master/train.py) |  |  |\n| [train.sh](https://github.com/dheeraj7596/META/blob/master/train.sh) | [train.sh](https://github.com/dheeraj7596/META/blob/master/train.sh) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# META: Metadata-Empowered Weak Supervision for Text Classification\n\n- [Model](https://github.com/dheeraj7596/META#model)\n- [Training](https://github.com/dheeraj7596/META#training)\n  - [Required Inputs](https://github.com/dheeraj7596/META#required-inputs)\n  - [Commands](https://github.com/dheeraj7596/META#commands)\n  - [Requirements](https://github.com/dheeraj7596/META#requirements)\n- [Citation](https://github.com/dheeraj7596/META#citation)\n\n## Model\n\n[![META-Framework](https://github.com/dheeraj7596/META/raw/master/docs/META-overview.png)](https://github.com/dheeraj7596/META/blob/master/docs/META-overview.png)\n\n## Training\n\n### Required inputs\n\nEach Dataset should contain following files:\n\n- **DataFrame pickle file**\n  - Example: `data/books/df.pkl`\n    - This dataset can contain any number of columns but must contain two columns named `text`, `label`\n    - `text` contains text and `label` contains its corresponding label.\n    - Must be named as `df.pkl`\n- **Seed Words Json file**\n  - Example: `data/books/seedwords.json`\n    - This json file contains seed words list for each label.\n    - Must be named as `seedwords.json`\n- **Metadata config file**\n  - Example: `data/books/metadata_config.json`\n    - This json file contains metadata type as key and there must be `separator` in the values which indicates\n      the separator of that entry in the dataframe.\n    - If no separator is required, the value for `separator` should be `null`.\n    - For example, if authors information are provided as a comma-separated string, the `separator` should be `\",\"`.\n- **Motif Patterns file**\n  - Example: `data/books/motif_patterns.txt`\n    - This text file contains one motif pattern per line.\n    - A motif pattern is represented as comma-separated string. For example: `authors, authors` represents\n      co-authorship.\n\n### Commands\n\n#### META - Iterative Framework:\n\nThe `train.sh` requires five arguments in the following order:\n\n- `use_gpu`: indicates whether to use GPU. This should be set to 1 to use GPU and 0 to use CPU.\n- `GPU`: refers to GPU id.\n- `dataset_path`: refers to absolute path of dataset.\n- `tmp_path`: refers to a path to a temporary directory which is used for dumping intermediate files.\n- `print_flag`: indicates whether to print the expanded phrases and motif instances. This should be set to 1\nto print and 0 to not print.\n\nFor example, to train META on books dataset on GPU Id 4, please run:\n\n```\n$ ./train.sh 1 4 <DATA_PATH TO BOOKS> <TEMP_DIR PATH> 1\n```\n\n### Requirements\n\nThis project is based on `python==3.7`. The dependencies are as follow:\n\n```\nkeras==2.1.5\nscikit-learn==0.21.3\nscipy=1.3.1\ngensim==3.8.1\nnumpy==1.17.2\nfast-pagerank==0.0.4\ntensorflow==1.15\nnltk\nbleach==3.1.5\npandas\nbeautifulsoup4\n\n```\n\n## Citation\n\n```\n@inproceedings{mekala-etal-2020-meta,\n    title = \"{META}: Metadata-Empowered Weak Supervision for Text Classification\",\n    author = \"Mekala, Dheeraj  and\n      Zhang, Xinyang  and\n      Shang, Jingbo\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.670\",\n    doi = \"10.18653/v1/2020.emnlp-main.670\",\n    pages = \"8351--8361\",\n    abstract = \"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as {``}seed motifs{''}, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.\",\n}\n\n```\n\n## About\n\nCode for the paper \"META: Metadata-Empowered Weak Supervision for Text Classification\"\n\n### Topics\n\n[...",
      "url": "https://github.com/dheeraj7596/META"
    },
    {
      "title": "How to incorporate meta data into text classification model?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How to incorporate meta data into text classification model?](https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked7 years ago\n\nModified [7 years ago](https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model?lastactivity)\n\nViewed\n1k times\n\n4\n\n$\\\\begingroup$\n\nI looked here: [How it's better to include non-word features into text classification model?](https://stats.stackexchange.com/questions/253806/how-its-better-to-include-non-word-features-into-text-classification-model) but there aren't any useful answers.\n\nI have a possibly naive question: I'd like to incorporate meta data into a text classification model. However I'm not sure how to proceed.\n\nAssume that I have a dataset that is $N \\\\times 3$, where the columns are:\n\n1. text document - for example, an amazon review or newspaper article\n2. some meta\\_data - for example, number of words of length > 5, or time article was published\n3. category - either A, B or C\n\nThe goal is to use the text document and the meta\\_data to classify the example in the correct category.\n\nTypically one would perform text classification on the text document (tokenize, lemmatize, remove stopwords, etc...) and build a sparse matrix of word counts. A model (for example SVM is popular) would be trained on this sparse matrix and tested on some unseen data, whereby it would be classified A, B or C.\n\nBut what about the meta data? I'd like to incorporate that somehow but in this paradigm it's unclear to me where I can inject it. I feel like what I want is a model of the form:\n\n$y = \\\\beta\\_0X\\_0 + \\\\beta\\_1X\\_1$\n\nWhere $X\\_0$ is the meta data and $X\\_1$ is the result of the NLP part. But how would I set up such a model? Can I reduce the text classification portion into a single coefficient? Or am I conflating two distinct approaches of modeling text?\n\n- [machine-learning](https://stats.stackexchange.com/questions/tagged/machine-learning)\n- [text-mining](https://stats.stackexchange.com/questions/tagged/text-mining)\n- [natural-language](https://stats.stackexchange.com/questions/tagged/natural-language)\n\n[Share](https://stats.stackexchange.com/q/285224)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/285224/edit)\n\nFollow\n\nasked Jun 13, 2017 at 23:43\n\n[![ilanman's user avatar](https://www.gravatar.com/avatar/790b4c3cafa53954ceb10c33dc9b21e5?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/73527/ilanman)\n\n[ilanman](https://stats.stackexchange.com/users/73527/ilanman) ilanman\n\n4,78711 gold badge2929 silver badges4848 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n2\n\n$\\\\begingroup$\n\nJust use the metadata features as features for the SVM.\n\nTypically the features that you feed into the SVM would be the $n \\\\times k$ document-term matrix $\\\\mathbf{T}$. You also have the $n \\\\times j$ matrix of metadata features $\\\\mathbf{M}$ (not including the category). So you want to give your SVM algorithm the combined $n \\\\times (k + j)$ matrix\n$$ \\\\left( \\\\array{\\\\mathbf{T} & \\\\mathbf{M}} \\\\right)$$\n\n[Share](https://stats.stackexchange.com/a/285227)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/285227/edit)\n\nFollow\n\n[edited Jun 14, 2017 at 11:24](https://stats.stackexchange.com/posts/285227/revisions)\n\nanswered Jun 14, 2017 at 0:08\n\n[![Dan Hicks's user avatar](https://www.gravatar.com/avatar/c2948a88f3177149308a7a6f7cff75c1?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/68397/dan-hicks)\n\n[Dan Hicks](https://stats.stackexchange.com/users/68397/dan-hicks) Dan Hicks\n\n80277 silver badges2121 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$I see. So the elements of $\\\\mathbf{T}$ would be, say, word counts and the elements of $\\\\mathbf{M}$ would be some other features. But I the algorithm doesn't treat the columns differently right? For example, $k$ could be large whereas $j$ may be small, but much more important than any of the $k$ words individually. And therefore I suppose this is where feature selection comes in...?$\\\\endgroup$\n\n\u2013\u00a0[ilanman](https://stats.stackexchange.com/users/73527/ilanman)\n\nCommentedJun 14, 2017 at 16:29\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Right. Depending on the particularities of the fitting algorithm, you probably want to do some feature selection and might want to center and rescale all of the features. But there's nothing relevantly different about text features.$\\\\endgroup$\n\n\u2013\u00a0[Dan Hicks](https://stats.stackexchange.com/users/68397/dan-hicks)\n\nCommentedJun 14, 2017 at 18:27\n\n\n[Add a comment](https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f285224%2fhow-to-incorporate-meta-data-into-text-classification-model%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://stats.stackexchange.com/questions/tagged/machine-learning) - [text-mining](https://stats.stackexchange.com/questions/tagged/text-mining) - [natural-language](https://stats.stackexchange.com/questions/tagged/natural-language)   or [ask your own question](https://stats.stackexchange.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Linked\n\n[1](https://stats.stackexchange.com/q/253806) [What's the best way to include non-word features into text classification model?](https://stats.stackexchange.com/questions/253806/whats-the-best-way-to-include-non-word-features-into-text-classification-model?noredirect=1)\n\n#### Related\n\n[21](https://stats.stackexchange.com/q/14856) [Large scale text classification](https://stats.stackexchange.com/questions/14856/large-scale-text-classification)\n\n[8](https://stats.stackexchange.com/q/32876) [Support vector machine for text classification](https://stats.stackexchange.com/questions/32876/support-vector-machine-for-text-classification)\n\n[29](https://stats.stackexchange.com/q/153069) [Bag-of-Words for Text Classification: Why not just use word frequencies instead of TFIDF?](https://stats.stackexchange.com/questions/153069/bag-of-words-for-text-classification-why-not-just-use-word-frequencies-instead)\n\n[1](https://stats.stackexchange.com/q/203809) [Text classification algorithms for small sets](https://stats.stackexchange.com/questions/203809/text-classification-algorithms-for-small-sets)\n\n[3](https://stats.stackexchange.com/q/266803) [How to find words/phrases causing document classification](https://stats.stackexchange.com/questions/266803/how-to-find-words-phrases-causing-document-classification)\n\n[4](https://stats.stackexchange.com/q/269024) [Confused among Gaussian, Multinomial and Binomial Naive Baye...",
      "url": "https://stats.stackexchange.com/questions/285224/how-to-incorporate-meta-data-into-text-classification-model"
    },
    {
      "title": "Does BERT Need Clean Data? Part 2 - Classification.",
      "text": "![Does BERT Need Clean Data? Part 2 - Classification.](https://bricken.co/assets/images/disaster/volcano.jpeg)\n\n# Does BERT Need Clean Data? Part 2 - Classification.\n\nWednesday. November 17, 2021 \\- 18 mins\n\n[data science](https://Briiick.github.io/tags/#data-science) [nlp](https://Briiick.github.io/tags/#nlp) [2021](https://Briiick.github.io/tags/#2021)\n\n**Now for the fun stuff. Light cleaning, heavy cleaning, or no language models at all?**\n\n* * *\n\nBy the end of this article, you should be able to get a top 50 score (84% accuracy) on the [NLP Disaster Tweets Kaggle Competition](https://www.kaggle.com/c/nlp-getting-started/overview)!\n\n![](https://bricken.co/assets/images/disaster/score.png)\n\nI am 63 on the leaderboard here. However, the top ~20 submissions are taken up by cheaters who have simply submitted the answers to the competition.\n\nRemember, we are trying to classify whether a Tweet designates a disaster (such as a hurricane or forest fire) or doesn\u2019t. The difficulty of this task is a result of the contextual meaning of certain words being different (for example, describing shoes as \u201cfire\u201d).\n\nAs mentioned in [Part 1](https://bricken.co/nlp_disaster_tweets_1/), once completing standard text cleaning, we need to decide what machine learning models we want to use and how the input data should look. The goal for this article is to provide a comparison of meta-feature learning using a Deep Neural Network, less intense (lighter) text pre-processing before BERT, and more intense (heavier) text pre-processing before BERT.\n\nAs well as this, I will provide an overview of BERT, how it works, and why it is one of the leading language models right now.\n\nBefore getting started, we just need to read the pickles from part 1!\n\n```\n# download the pickles saved from part 1\ntrain_df = pd.read_pickle(\"../data/pickles/clean_train_data.pkl\")\ntest_df = pd.read_pickle(\"../data/pickles/clean_test_data.pkl\")\n\n```\n\nNow, let\u2019s start with the first way to predict disasters: using meta-features and a neural network.\n\n## Meta-Feature Learning\n\nHere, we use just meta-feature data to predict disaster Tweets. This is an alternative method that is being tested. My hypothesis is that it won\u2019t be as good as one of the BERT models, however, it is still interesting to see how it performs. Also, perhaps to improve on BERT in the future, this type of data could be included, so we should test how well our meta-features help distinguish disaster from non-disaster Tweets.\n\n### Normalisation\n\nIn order to incorporate our meta-features into our modelling, we need to normalise our columns. Normalisation is used to change the integer columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. We normalise by using `MinMaxScaler()`, which allows us to keep the data in the range of \\[0,1\\].\n\n```\n# normalise columns\nscaler = MinMaxScaler()\nfinal_train_df = scaler.fit_transform(final_train_df)\nfinal_test_df = scaler.fit_transform(final_test_df)\n\n```\n\n![](https://bricken.co/assets/images/disaster/array1.png)\n\nWhat our data looks like after MinMaxScaler.\n\n### Train-Test Splitting\n\nBefore running our Deep Neural Network, we need a way of separating the data so that we can evaluate the model on a test dataset that has not been seen in the train data before. We use sci-kit learn\u2019s `train_test_split()` function for this.\n\n```\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(final_train_df, train_labels, test_size=0.3, random_state=42)\n\n```\n\nAnd now that we have a train dataset, a labelled test dataset, and the unlabelled submission test dataset, we can generate our model, fit it on the training data, and test the outcome on our test data, and then run the final model on the submission data before uploading the submissions to Kaggle.\n\n### Generating Classification Probabilities Using A Deep Neural Network\n\nI generate a Deep Neural Network by using the Sequential function, allowing me to build the DNN layer-by-layer. The \u201cdeep\u201d here means that the NN is connected deeply, as with dense layers the layers receive input from all neurons of the prior layer. I add 5 layers with varying density, using 3 ReLu activation functions (Rectified Linear Activation - this activation function works well in neural networks) and the final sigmoid function that allows me to output the associated probability of an input row being a disaster or non-disaster.\n\n```\n# create DNN\nmodel = Sequential()\nmodel.add(Dense(90, input_dim=9, activation='relu'))\nmodel.add(Dense(120, activation='relu'))\nmodel.add(Dense(100))\nmodel.add(Dense(20, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n# adam optimizer\noptimizer = tf.keras.optimizers.Adam(lr=1e-4)\n\n# compile and summarise\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()\n\n# first fit is history1\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=35, validation_data=(X_test, y_test), verbose=1)\n\n```\n\nBefore moving on to the model evaluation, which has the same method across models, the other models\u2019 construction can be shown. Then, a comparison can be made across models in the model evaluation portion.\n\n## Light & Heavy Data Cleaning\n\nSome text cleaning might be necessary to optimise BERT performance, but the amount of removal of features of our Tweets is debatable. Because BERT is a model that uses the context of words provided their placement in a sentence in relation to all other words, generally the preservation of such information would be advised. However, in some cases BERT might outperform if data is pruned to a higher degree. In this section light and heavy text cleaning is done. We can then test the performance of either approach in the model evaluation section.\n\n### Regex Text Cleaning\n\nBelow is some text cleaning. The code is separated into heavy and light portions. In the heavy cleaning, all substitutions are made. In light cleaning, only the light are made (edit the function as you see necessary).\n\n```\n# function taken and modified\n# from https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76\n\nstopwords = set(STOPWORDS)\nstopwords.update([\"nan\"])\n\ndef text_clean(x):\n\n    ### Light\n    x = x.lower() # lowercase everything\n    x = x.encode('ascii', 'ignore').decode()  # remove unicode characters\n    x = re.sub(r'https*\\S+', ' ', x) # remove links\n    x = re.sub(r'http*\\S+', ' ', x)\n    # cleaning up text\n    x = re.sub(r'\\'\\w+', '', x)\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    x = re.sub(r'\\s{2,}', ' ', x)\n    x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n\n    ### Heavy\n    x = ' '.join([word for word in x.split(' ') if word not in stopwords])\n    x = re.sub(r'@\\S', '', x)\n    x = re.sub(r'#\\S+', ' ', x)\n    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n    # remove single letters and numbers surrounded by space\n    x = re.sub(r'\\s[a-z]\\s|\\s[0-9]\\s', ' ', x)\n\n    return x\n\n```\n\nWe apply this function in the following way:\n\n```\ntrain_df['cleaned_text'] = train_df.text.apply(text_clean)\ntest_df['cleaned_text'] = test_df.text.apply(text_clean)\n\n```\n\n### Lemmatisation (For Heavy Cleaning)\n\nJust for heavy cleaning, we add one more step: lemmatisation. This is where a library of words is used to remove the inflectional endings of words to return them to their base form, which is known as the lemma. For example, instead of \u201crunning\u201d or \u201cruns\u201d, we would just get \u201crun\u201d.\n\nThis can sometimes help a model interpret sentences better, so we test it here.\n\n```\ntrain_list = []\nfor word in train_text:\n    tokens = word_tokenize(word)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n    train_list.append(' '.join(lemmatized))\n\ntest_list = []\nfor word in test_text:\n    tokens = word_tokenize(word)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n    test_list.append(' '.join(lemmatized))\n\n```\n\n### Tokenisat...",
      "url": "https://bricken.co/nlp_disaster_tweets_2"
    },
    {
      "title": "",
      "text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8351\u20138361,\nNovember 16\u201320, 2020. \rc 2020 Association for Computational Linguistics\n8351\nMETA: Metadata-Empowered Weak Supervision for Text Classification\nDheeraj Mekala1 Xinyang Zhang2 Jingbo Shang1,3\n1 Department of Computer Science and Engineering, University of California San Diego, CA, USA\n2 Department of Computer Science, University of at Illinois Urbana-Champaign, IL, USA\n3 Hal\u0131c\u0131oglu Data Science Institute, University of California San Diego, CA, USA \u02d8\n1,3\n{dmekala, jshang}@ucsd.edu 2 xz43@illinois.edu\nAbstract\nRecent advances in weakly supervised learn\u0002ing enable training high-quality text classifiers\nby only providing a few user-provided seed\nwords. Existing methods mainly use text data\nalone to generate pseudo-labels despite the\nfact that metadata information (e.g., author\nand timestamp) is widely available across var\u0002ious domains. Strong label indicators exist in\nthe metadata and it has been long overlooked\nmainly due to the following challenges: (1)\nmetadata is multi-typed, requiring systematic\nmodeling of different types and their combi\u0002nations, (2) metadata is noisy, some metadata\nentities (e.g., authors, venues) are more com\u0002pelling label indicators than others. In this\npaper, we propose a novel framework, META,\nwhich goes beyond the existing paradigm and\nleverages metadata as an additional source of\nweak supervision. Specifically, we organize\nthe text data and metadata together into a\ntext-rich network and adopt network motifs to\ncapture appropriate combinations of metadata.\nBased on seed words, we rank and filter mo\u0002tif instances to distill highly label-indicative\nones as \u201cseed motifs\u201d, which provide addi\u0002tional weak supervision. Following a boot\u0002strapping manner, we train the classifier and\nexpand the seed words and seed motifs itera\u0002tively. Extensive experiments and case stud\u0002ies on real-world datasets demonstrate supe\u0002rior performance and significant advantages of\nleveraging metadata as weak supervision.\n1 Introduction\nWeakly supervised text classification has recently\ngained much attention from the researchers because\nit reduces the burden of annotating the data. So far,\nthe major source of weak supervision lies in text\ndata itself (Agichtein and Gravano, 2000; Kuipers\net al., 2006; Riloff et al., 2003; Tao et al., 2015;\nMeng et al., 2018; Mekala and Shang, 2020). These\nmethods typically require a few user-provided seed\nPaper Authors Year Category\nP1 G. Hinton, S. Osindero, YW. Teh 2006 ML\nP2 G. Hinton, O. Vinyals, J. Dean 2015 ML\nP3 J. Dean, S.Ghemawat 2008 Sys\n(a) Examples of research papers with metadata.\nO. Vinyals\nG. Hinton\n2016\nneural\nsystem\ndata\nlearning\nJ. Dean Doc\nAuthor 1 Author 2\nP2\nO. Vinyals G. Hinton\nP1 P2\n\u2026 \u2026\n\u2026\n(b) A text-rich network view of \nthe papers.\n(c) A motif pattern and \na motif instance.\nFigure 1: Text corpus, text-rich network, and motif.\nwords for each class as weak supervision. They ex\u0002pand seed words with generated pseudo labels and\nimprove their text classifier in an iterative fashion.\nMetadata information (e.g., author, published\nyear) in addition to textual information, is widely\navailable across various domains (e.g., news arti\u0002cles, social media posts, and scientific papers) and\nit could serve as a strong, complementary weak\nsupervision source. Take a look at the research\npapers in Figure 1(a) as an example. It shall be\nlearned in a data-driven manner that G. Hinton is a\nhighly-reputed machine learning researcher, thus\nhis presence is a strong indicator of a paper belong\u0002ing to the Machine Learning category.\nDistilling effective metadata for weak supervi\u0002sion faces several major challenges. Metadata is\noften multi-typed, each type and the type combi\u0002nations could have very different semantics and\nmay not be equally important. Moreover, even en\u0002tities within a single metadata type could be noisy.\nContinuing our example in Figure 1(a), we shall\nnotice that year is less helpful than an author to do\nclassification. Among the authors, J. Dean might\nbe an important figure but has research interests\n8352\nm1,1 .95 .01 .04\nm1,2 .32 .30 .38\nText Classifier\nSeed Words \u2460 Pseudo Label Generation \u2462 Motif Ranking & \u2026\nC1 Neural Network\nC2 Image Classification\nC3 Translation\nPaper\nAuthor 1 Author 2\nM1\nM2\nm2,1 .03 .89 .08\nm2,2 .31 .26 .43\nInstance C1 C2 C3\nKlein Manning\nP1 P2\n\u2461 Classifier Training / Prediction\nRecursion\nUser Given\nMotif Pattern\nMatched (Seed)\nMotif Instances\n\u2026\n\u2026\nC1 m1,1 m1,6 m2,3 m1,4 \u2026\nC2 m2,1 m2,4 m1,2 m1,4 \u2026\nC3 m1,3 m2,5 m2,2 m1,7 \u2026\n... Expansion\nExpand from high score to low\nNew seed set cutoff\nFigure 2: Our META framework. In each iteration, we generate pseudo labels for documents, train the text classifier,\nand rank all words and motif instances in a unified ranking framework. We then expand seed sets until an automatic\ncutoff is reached. The quality of the classifier and the seed sets are improved through iterations.\nspanning across different domains. However, if we\njoin the author with year, it carries more accurate\nsemantics, and we may discover J. Dean has more\ninterest in machine learning in recent years, thus\nbecoming highly label-indicative.\nBearing the challenges in mind, we propose\nMETA, a principled framework for metadata\u0002empowered weakly-supervised text classification.\nAs illustrated in Figure 1 and Figure 2, we first\norganize the text data and metadata together into\na text-rich network. The network structure gives\nus a holistic view of the corpus and enables us to\nrank and select useful metadata entities. We lever\u0002age motif patterns (Benson et al., 2016; Milo et al.,\n2002; Shang et al., 2020) to model typed metadata\nas well as their combinations. A motif pattern is\na subgraph pattern at the meta-level that captures\nhigher-order connections and the semantics repre\u0002sented by these connections. It serves as a useful\ntool to model typed edges, typed paths (a.k.a. meta\u0002paths) (Sun et al., 2011), and higher-order struc\u0002tures in the network. With little effort, users can\nspecify a few possibly useful motif patterns as in\u0002put to our model. We develop a unified, principled\nranking mechanism to select label-indicative motif\ninstances and words, forming expanded weak su\u0002pervision. Note that, such instance-level selection\nprocess also implicitly refines the motif patterns,\nensuring the robust performance of META even\nwhen irrelevant motif patterns exist in input. It is\nworth a mention that META is compatible with any\ntext classifiers.\nOur contributions are summarized as follows:\n\u2022 We explore to incorporate metadata information\nas an additional source of weak supervision for\ntext classification along with seed words.\n\u2022 We propose a novel framework META, which in\u0002troduces motif patterns to capture the high-order\ncombinations among different types of metadata\nand conducts a unified ranking and selection of\nlabel-indicative motif instances and words.\n\u2022 We conduct experiments on two real-world\ndatasets. The results and case studies demon\u0002strate the superiority of incorporating metadata\nas parts of weak supervision and verify the effec\u0002tiveness of META.\nReproducibility. Our code is made publicly avail\u0002able at GitHub1\n.\n2 Preliminaries\n2.1 Documents as Text-rich Network\nGiven a collection of n text documents D =\n{D1, D2, . . . , Dn}, and their corresponding meta\u0002data, we propose to organize them into a text-rich\nnetwork, as illustrated in Figure 1(b). A text-rich\nnetwork is a heterogeneous network with docu\u0002ments, words, different types of metadata as nodes,\nand their associations as edges. For example, our\ntext-rich network for research papers has papers,\nwords, authors, and publication years as nodes.\nEach paper is connected to its associated words\nand metadata nodes. Such a network provides a\nholistic and structured representation of the input.\n2.2 Seed Words and Motif Patterns\nUsers are asked to provide a few seed words S\n= {Sw\n1\n, S\nw\n2\n, . . . , S\nw\nl\n} for each of l classes (i.e.,\nC1, C2, . . . , ...",
      "url": "https://aclanthology.org/2020.emnlp-main.670.pdf"
    },
    {
      "title": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023",
      "text": "A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\n[\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\n18 min read\n\u00b7Apr 28, 2024\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\nListen\nShare\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\n**> Note:\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\n*> \u201cIt is estimated that 90% of the world\u2019s data was generated in the last two years alone\u201d\n*> by [> Amount of Data Created Daily (2023)\n](https://explodingtopics.com/blog/data-generated-per-day)\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\n## Methodology\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\nPress enter or click to view image in full size\n![]()\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\n## Key Discoveries of Text Data Competitions\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term \u201cquestion answering\u201d exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...",
      "url": "https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312"
    },
    {
      "title": "What my first Silver Medal taught me about Text Classification and Kaggle in general? - MLWhiz",
      "text": "[Natural Language Processing](https://mlwhiz.com/categories/natural-language-processing) [Deep Learning](https://mlwhiz.com/categories/deep-learning) [Awesome Guides](https://mlwhiz.com/categories/awesome-guides)\n\n# What my first Silver Medal taught me about Text Classification and Kaggle in general?\n\nBy Rahul Agarwal19 February 2019\n\n![What my first Silver Medal taught me about Text Classification and Kaggle in general?](https://mlwhiz.com/images/silver/CV_vs_LB.png)\n\nKaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on **Quora Insincere questions classification** in which I got a rank of **`182/4037`**. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.\n\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome\n[Natural Language Processing Specialization](https://coursera.pxf.io/9WjZo0)\n. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.\n\nSo first a little bit of summary about the competition for the uninitiated. In this competition, we had to develop models that identify and flag insincere questions. _**The challenge was not only a test for performance but also a test of efficient code writing skills.**_ As it was a kernel competition with limited outside data options, competitors were limited to use only the word embeddings provided by the competition organizers. That means we were not allowed to use State of the art models like BERT. We were also limited in the sense that all our models should run in a time of 2 hours. So say bye bye to stacking and monster ensembles though some solutions were able to do this by making their code ultra-efficient. More on this later.\n\n## Some Kaggle Learnings:\n\nThere were a couple of **learnings about kaggle as a whole** that I would like to share before jumping into my final solution:\n\n### 1\\. Always trust your CV\n\n![](https://mlwhiz.com/images/silver/CV_vs_LB.png)\n\nOne of the things that genuinely baffled a lot of people in this competition was that a good CV score did not necessarily translate well to a good LB score. The main reason for this was **small test dataset**(only 65k rows) in the first stage(around 15% of total test data).\n\nA common theme on discussion forums was focussing on which submissions we should select as the final submission:\n\n- The one having the best local CV? or\n- The one having the best LB?\n\nAnd while it seems simple to say to trust your CV, common sense goes for a toss when you see that your LB score is going down or remaining constant whenever your Local CV score increases.\n\nLuckily I didn\u2019t end up making the mistake of not trusting my CV score. Owing to a lot of excellent posts on Kaggle discussion board, _**I selected a kernel with Public LB score of 0.697 and a Local CV of 0.701, which was around >1200 rank on Public LB as of the final submission. It achieved a score of 0.702 and ranked 182 on the private LB.**_\n\nWhile this seems like a straightforward choice post-facto, it was a hard decision to make at a time when you have at your disposal some public kernels having Public LB score >= 0.70\n\n### 2\\. Use the code from public kernels but check for errors\n\n[This](https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch)\nPytorch kernel by Benjamin Minixhofer is awesome. It made the base of many of my submissions for this competition. But this kernel had a mistake. It didn\u2019t implement spatial dropout in the right way. You can find the correct implementation of spatial dropout in my post\n[here](https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/)\nor on my\n[kernel](https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-spatial-dropout)\n. Implementing spatial dropout in the right way gave a boost of around 0.004 to the local CV.\n\nNonetheless, I learned pytorch using this kernel, and I am grateful to him for the same.\n\n### 3\\. Don\u2019t trust everything that goes on the discussion forums\n\n![](https://mlwhiz.com/images/silver/read-what-the-smart-people-are-saying.png)\n\nI will talk about two things here:\n\n- **Seed tuning**: While in the middle of the competition, everyone was trying to get the best possible rank on the public LB. It is just human nature. A lot of discussions was around good seeds and bad seeds for neural network initialization. While it seems okay in the first look, the conversation went a stage further where **people started tuning seeds in the kernel as a hyper param**. Some discussions even went on to say that it was a valid strategy. And that is where a large amount of overfitting to public LB started happening. The same submission would score 0.704 from 0.699 just by changing the seed. For a reference, that meant you could go from anywhere near 400-500 rank to top 50 only by changing seed in a public kernel. And that spelled disaster. Some people did that. They went up the public LB. Went crashing out at the private stage.\n\n- **CV score disclosure on discussion forums**: We always try to gauge our performance against other people. In a lot of discussions, people provided their CV scores and corresponding Public LB scores. The scores were all over the place and not comparable due to Different CV schemes, No of folds in CV, Metric reported, Overfitting or just plain Wrong implementation of Cross-Validation. But they ended up influencing a lot of starters and newcomers.\n\n\n### 4\\. On that note, be active on Discussion forums and check public kernels regularly\n\nYou can learn a lot just by being part of discussion forums and following public kernels. This competition had a lot of excellent public kernels on embeddings by\n[SRK](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n, Models by\n[Shujian](https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding)\n, and Preprocessing by\n[Theo Viel](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2)\nwhich gave everyone a headstart. As the competition progressed, the discussions also evolved. There were discussions on speeding up the code, working approaches, F1 threshold finders, and other exciting topics which kept me occupied with new ideas and improvements.\n\nEven after the end, while reading up discussions on solutions overview, I learned a lot. And I would say it is very \\*\\* vital to check out the winning solutions.\\*\\*\n\n### 5\\. Share a lot\n\nSharing is everything on Kaggle. People have shared their codes as well as their ideas while competing as well as after the competition ended. It is only together that we can go forward. I like blogging, so I am sharing the knowledge via a series of blog posts on text classification. The\n[first post](https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/)\ntalked about the different **preprocessing techniques that work with Deep learning models** and **increasing embeddings coverage**. In the\n[second post](https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/)\n, I talked through some **basic conventional models** like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I will delve deeper into **Deep learning models and the various architectures** we could use to solve the text Classification problem. To make this post platform generic I will try to write code in both Keras and Pytorch. We will try to use various other models which we were not able to use in this competition like **ULMFit transfer learning** approaches in the fourth post in the series.\n\nIt might take me a little time to write the whole series. Till then you can take a look at my other posts too:\n[What Kagglers are using for...",
      "url": "https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings"
    }
  ]
}