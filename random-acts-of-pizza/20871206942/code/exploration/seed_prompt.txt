## Problem Type
Binary classification combining text features (request title, text) with meta-data features (user activity, votes, subreddit).

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: 2,878 samples, 32 features (16 numeric, 9 object, 6 float, 1 boolean), 75%/25% class imbalance, missing value patterns, text length distributions (title avg 72 chars, text avg 403 chars), mixed data types in `post_was_edited`
- Use findings from these notebooks when implementing features

## Models
For binary classification with text + meta-data:
- **Gradient boosting (XGBoost/LightGBM/CatBoost)** on engineered features from text + meta-data
- **Pretrained language models (BERT/RoBERTa)** fine-tuned on text with meta-features concatenated to final layer
- **Hybrid approaches**: Text embeddings (TF-IDF, BERT) + meta-features in ensemble
- Neural networks for large datasets (>100K samples) - not applicable here with only 2,878 samples

## Preprocessing
- **Text preprocessing**: Clean HTML entities, handle mixed types in `post_was_edited`, standardize boolean representations
- **Missing values**: 75% missing in `requester_user_flair` - consider dropping or special encoding
- **Text representation**: TF-IDF for traditional models, tokenization for BERT variants
- **Meta-data encoding**: One-hot for categorical, scaling for numeric
- **Class imbalance**: 75%/25% split - consider SMOTE, class weights, or stratified sampling

## Feature Engineering
- **Text features**: Length, word count, sentiment, presence of keywords ("please", "thank you", "hungry")
- **Meta-data interactions**: User activity ratios, vote ratios, temporal features
- **Subreddit encoding**: Target encoding or one-hot based on frequency
- **Combine text + meta**: Concatenate meta-features to text embeddings or use as separate model inputs

## Validation Strategy
- **Stratified K-fold (k=5)**: Essential for 75%/25% class imbalance
- **AUC-ROC optimization**: Target metric requires probability calibration
- **Early stopping**: Use validation AUC for model selection

## Ensembling
- **Stacking**: Combine predictions from text-based models (BERT) and meta-data models (XGBoost)
- **Weighted averaging**: Higher weight to best-performing model
- **Diverse models**: 3-5 models with different architectures (e.g., TF-IDF+LR, BERT, XGBoost on meta-features)

## Optimization
- **Hyperparameter tuning**: Bayesian optimization for gradient boosting, learning rate scheduling for BERT
- **Class weights**: Adjust for 75%/25% imbalance
- **Threshold tuning**: Optimize decision threshold for AUC-ROC