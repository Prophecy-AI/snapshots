{
  "query": "What are winning Kaggle solutions for small text classification datasets with meta-data features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning Kaggle solutions for **small\u2011scale text classification problems that also contain meta\u2011data (e.g., user, country, gender, timestamps, etc.)** follow a repeatable recipe that blends powerful language models with classic tabular\u2011learning tricks.  Most top\u2011ranked notebooks start by fine\u2011tuning a pretrained transformer such as\u202fRoBERTa, XLM\u2011R or a multilingual BERT on the raw text, then extract the pooled CLS embedding and concatenate it with engineered meta\u2011features (one\u2011hot or target\u2011encoded categorical columns, numeric statistics, or custom \u201ctext\u202f+\u202fmetadata\u201d strings).  The combined vector is fed to a gradient\u2011boosted tree model (LightGBM or CatBoost) or a shallow neural net, and the predictions of the transformer\u2011only and the tabular model are blended (often via a weighted average or a ridge\u2011regression stack).  To avoid over\u2011fitting on the tiny training set, participants use stratified K\u2011fold or group\u2011aware CV, heavy regularisation (drop\u2011out, early stopping, low\u2011learning\u2011rate fine\u2011tuning), and data\u2011augmentation tricks such as back\u2011translation or synonym replacement.  These patterns are repeatedly highlighted in the \u201cText Classification: All Tips and Tricks from 5 Kaggle Competitions\u201d post, which extracts the most effective ideas from the Jigsaw toxicity, Quora Insincere Questions, Google\u202fQUEST, and other high\u2011prize contests\u202f([neptune.ai](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  \n\nConcrete examples illustrate the approach.  In the 2025 **Playground Series\u202f\u2013\u202fSeason\u202f5, Episode\u202f12** (a small\u2011dataset challenge) the 1st\u2011place team built a ridge\u2011ensemble of TF\u2011IDF vectors and a LightGBM model that also ingested ID\u2011shift features, achieving the top ROC\u2011AUC score\u202f([farid.one](https://farid.one/kaggle-solutions)).  A later GitHub project that won a top\u20112\u202f% spot in the **Quora Question Pairs** competition combined a cleaned tweet text with categorical metadata (country, gender) into a single feature string (`text_clean_country_gender_user`) before feeding it to a transformer\u2011plus\u2011LightGBM pipeline, demonstrating how meta\u2011data can be merged directly with the text representation\u202f([github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe](https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe)).  Together, these solutions show that the winning formula for small text\u2011classification tasks with meta\u2011data is: **pretrained language model\u202f+\u202fmetadata feature engineering\u202f+\u202ftabular model ensembling\u202f+\u202frigorous cross\u2011validation**.",
      "url": ""
    },
    {
      "title": "The Most Comprehensive List of Kaggle Solutions and Ideas",
      "text": "Kaggle Solutions\n[\n**Kaggle**Solutions\n](https://farid.one/kaggle-solutions/)\n[![Fork me on GitHub](https://github.blog/wp-content/uploads/2008/12/forkme_right_red_aa0000.png)](https://github.com/faridrashidi/kaggle-solutions)\n## The Most Comprehensive List of Kaggle Solutions and Ideas\nThis repository contains a comprehensive collection of solutions and ideas shared by top performers from past Kaggle competitions. The list is continuously updated with new insights after each competition concludes. If you discover a solution not yet listed here, feel free to contribute by submitting a pull request. You can find a guide to the symbols used in this list[here](https://farid.one/kaggle-solutions/resources/symbols.html).\nIf you find this resource valuable, consider giving it a star or forking it to support and expand the community!\nExplore these insightful pages for:\n* [Top Kagglers Interviews and Lectures](https://farid.one/kaggle-solutions/resources/videos.html)\n* [Kernels of The Week](https://farid.one/kaggle-solutions/resources/kernels.html)\n</br>\n****Last Updated:**January 3, 2026\n||Title & Description|Details|Solutions|Pins|\n[![](https://farid.one/kaggle-solutions/assets/logos/91723.webp)](https://www.kaggle.com/c/playground-series-s5e12)|[**682. Playground Series - Season 5, Episode 12**](https://www.kaggle.com/c/playground-series-s5e12)\n</br>\nDiabetes Prediction Challenge\n|\nPrize: Swag\nTeam: 4,206\nKind: Playground\nMetric: Roc Auc Score\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-gold.png)1st place](https://www.kaggle.com/c/playground-series-s5e12/writeups/1st-place-solution-hill-climbing-ridge-ensembl)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-silver.png)2nd place](https://www.kaggle.com/c/playground-series-s5e12/writeups/2nd-place-solution-winning-based-on-id-shift-an)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/playground-series-s5e12/writeups/add-tabm-for-diversity)|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/114250.webp)](https://www.kaggle.com/c/nfl-big-data-bowl-2026-analytics)|[**681. NFL Big Data Bowl 2026 - Analytics**](https://www.kaggle.com/c/nfl-big-data-bowl-2026-analytics)\n</br>\nUnderstand player movement while the ball is in the air\n|\nPrize: $50,000\nTeam: 278\nKind: Featured\nMetric: -\nYear: 2025\n|\n* **Not Available!|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/59156.webp)](https://www.kaggle.com/c/MABe-mouse-behavior-detection)|[**680. MABe Challenge - Social Action Recognition in Mice**](https://www.kaggle.com/c/MABe-mouse-behavior-detection)\n</br>\nDetect unique behaviors from pose estimates of mice.\n|\nPrize: $50,000\nTeam: 1,412\nKind: Research\nMetric: MABe F Beta\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-silver.png)2nd place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/2nd-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)3rd place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/3rd-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/4th-place-solution-xgb-nn-ensemble)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)5th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/5th-place-gnn-egocentric-squeezeformer)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)7th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/7th-place-gold-cnn-transformer-with-invariant-fe)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)9th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/9th-place-gold)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)10th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/10th-place-solution-st-gcn-transformer)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)12th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/12th-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)15th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/15th-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)17th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/key-improvements-in-our-solution-a-technical-summ)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)19th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/19th-place-4-body-parts-invariant-features-with)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)26th place](https://www.kaggle.com/c/MABe-mouse-behavior-detection/writeups/27th-place-solution)|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/120570.webp)](https://www.kaggle.com/c/gemini-3)|[**679. Google DeepMind - Vibe Code with Gemini 3 Pro in Al Studio**](https://www.kaggle.com/c/gemini-3)\n</br>\nBuild with Gemini 3 and compete for $500,000 in credits\n|\nPrize: $500,000\nTeam: 4,102\nKind: Featured\nMetric: -\nYear: 2025\n|\n* **Not Available!|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/91722.webp)](https://www.kaggle.com/c/playground-series-s5e11)|[**678. Playground Series - Season 5, Episode 11**](https://www.kaggle.com/c/playground-series-s5e11)\n</br>\nPredicting Loan Payback\n|\nPrize: Swag\nTeam: 3,724\nKind: Playground\nMetric: Roc Auc Score\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-gold.png)1st place](https://www.kaggle.com/c/playground-series-s5e11/writeups/1st-place-a-lot-of-features-a-lot-of-models-an)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-silver.png)2nd place](https://www.kaggle.com/c/playground-series-s5e11/writeups/2nd-place-solution-7-models-but-1-was-also-enou)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/4th-place-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)5th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/5th-place-solution-xgb-lgbm-tabm5seeds-ag)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)6th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/6-solution-ensembling-was-the-key)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)8th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/rank8-approach-trust-the-cv-score)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)10th place](https://www.kaggle.com/c/playground-series-s5e11/writeups/a-10th-place-experiment)|**\n|\n[![](https://farid.one/kaggle-solutions/assets/logos/91496.webp)](https://www.kaggle.com/c/arc-prize-2025)|[**677. ARC Prize 2025**](https://www.kaggle.com/c/arc-prize-2025)\n</br>\nCreate an AI capable of novel reasoning\n|\nPrize: $1,000,000\nTeam: 1,554\nKind: Featured\nMetric: Abstraction and Reasoning Challenge\nYear: 2025\n|\n* [![](https://farid.one/kaggle-solutions/assets/images/description-gold.png)1st place](https://www.kaggle.com/c/arc-prize-2025/writeups/nvarc)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)3rd place](https://www.kaggle.com/c/arc-prize-2025/writeups/mindsai-and-tufa-labs-arc-prize-2025-solution)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)4th place](https://www.kaggle.com/c/arc-prize-2025/writeups/arc-prize-2025-competition-writeup-5th-place)\n* [![](https://farid.one/kaggle-solutions/assets/images/description-bronze.png)5th place](https://www.kaggle.com/c/arc-prize-2025/wri...",
      "url": "https://farid.one/kaggle-solutions"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "GitHub - AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe: Classifies 500K+ political tweets from Northern Europe using NLP and machine learning to analyze political discourse.",
      "text": "<div><div><article><p></p><h2>\ud83d\udc26 Kaggle Project: Classification of Tweets from Northern Europe</h2><a href=\"#-kaggle-project-classification-of-tweets-from-northern-europe\"></a><p></p>\n<p><a href=\"https://camo.githubusercontent.com/439fd5ffe2ac21b3daa03ac6474978163be089da5fbba2790769ae2275464e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\"></a></p>\n<p></p><h2>\ud83d\ude80 Overview</h2><a href=\"#-overview\"></a><p></p>\n<p>This project analyzes a dataset of <strong>509,031 tweets</strong> from politicians across <strong>seven Northern European countries</strong>, aiming to classify tweets by <strong>political spectrum (Left, Right, Center)</strong> and <strong>geography</strong>. Using <strong>Natural Language Processing (NLP)</strong> and machine learning, we extracted key insights into political discourse and developed a classifier with strong performance.</p>\n<hr/>\n<p></p><h2>\ud83d\udcca Dataset</h2><a href=\"#-dataset\"></a><p></p>\n<ul>\n<li><strong>Countries:</strong> Belgium, Denmark, Iceland, Ireland, Netherlands, Norway, Sweden</li>\n<li><strong>Time span:</strong> Dec 2008 \u2013 Jan 2023</li>\n<li><strong>Dataset size:</strong> 509,031 tweets\n<ul>\n<li><strong>Training set:</strong> 407,223 tweets</li>\n<li><strong>Test set:</strong> 101,808 tweets</li>\n</ul>\n</li>\n</ul>\n<p><strong>Attributes:</strong></p>\n<ul>\n<li>Tweet text</li>\n<li>Country</li>\n<li>Gender</li>\n<li>Political spectrum label (for supervised learning)</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udd2c Methods</h2><a href=\"#-methods\"></a><p></p>\n<p></p><h3>Data Cleaning &amp; Preprocessing</h3><a href=\"#data-cleaning--preprocessing\"></a><p></p>\n<ul>\n<li>Text cleaning: Lowercasing, punctuation removal, stopword removal, lemmatization.</li>\n<li>New feature creation: <code>text_clean_country_gender_user</code> combining multiple metadata.</li>\n</ul>\n<p></p><h3>Feature Engineering</h3><a href=\"#feature-engineering\"></a><p></p>\n<ul>\n<li><strong>Vectorization:</strong>\n<ul>\n<li>Count Vectorizer</li>\n<li>TF-IDF transformation</li>\n</ul>\n</li>\n</ul>\n<p></p><h3>Modeling</h3><a href=\"#modeling\"></a><p></p>\n<ul>\n<li><strong>Classifier:</strong> Linear Support Vector Classifier (LinearSVC)</li>\n<li><strong>Validation:</strong> 10-fold cross-validation</li>\n</ul>\n<p></p><h3>Topic Modeling</h3><a href=\"#topic-modeling\"></a><p></p>\n<ul>\n<li><strong>LDA (Latent Dirichlet Allocation)</strong></li>\n<li><strong>NMF (Non-negative Matrix Factorization)</strong></li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcc8 Results</h2><a href=\"#-results\"></a><p></p>\n<p>\u2705 <strong>Key findings:</strong></p>\n<ul>\n<li>Tweets span diverse topics from <strong>domestic politics</strong> to <strong>international relations</strong>.</li>\n<li>Different political leanings across countries: e.g., Netherlands leans Center, Iceland leans Left.</li>\n<li>Gender distribution varies: Sweden had a <strong>female-majority</strong>, others skewed male.</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udee0\ufe0f Requirements</h2><a href=\"#\ufe0f-requirements\"></a><p></p>\n<ul>\n<li>Python 3.8+</li>\n<li>Key libraries:\n<ul>\n<li>pandas, numpy</li>\n<li>scikit-learn</li>\n<li>nltk, spaCy</li>\n<li>gensim</li>\n<li>matplotlib, seaborn</li>\n</ul>\n</li>\n</ul>\n<p>Install via:</p>\n<div><pre>pip install -r requirements.txt</pre></div>\n<hr/>\n<p></p><h2>\ud83d\udda5\ufe0f Usage</h2><a href=\"#\ufe0f-usage\"></a><p></p>\n<p>1\ufe0f\u20e3 <strong>Clone the repository:</strong></p>\n<div><pre>git clone https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe.git\n<span>cd</span> kaggle-project-classification-of-tweets-from-northern-europe</pre></div>\n<p>2\ufe0f\u20e3 <strong>Install dependencies:</strong></p>\n<div><pre>pip install -r requirements.txt</pre></div>\n<p>3\ufe0f\u20e3 <strong>Run analysis:</strong></p>\n<ul>\n<li>Open <code>kaggle_project.ipynb</code> for full preprocessing, modeling, and evaluation workflows.</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcc2 Repository Structure</h2><a href=\"#-repository-structure\"></a><p></p>\n<div><pre><code>\u251c\u2500\u2500 data/ # Dataset (not included)\n\u251c\u2500\u2500 kaggle_project.ipynb # Main Jupyter notebook\n\u251c\u2500\u2500 kaggle_project_report.pdf # Final project report\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt # Dependencies\n\u2514\u2500\u2500 LICENSE\n</code></pre></div>\n<hr/>\n<p></p><h2>\ud83d\udc65 Contributors</h2><a href=\"#-contributors\"></a><p></p>\n<ul>\n<li><strong>Yuesong Huang</strong> (<a href=\"mailto:yhu116@u.rochester.edu\">yhu116@u.rochester.edu</a>)</li>\n<li><strong>Junhua Huang</strong> (<a href=\"mailto:jhuang77@u.rochester.edu\">jhuang77@u.rochester.edu</a>)</li>\n</ul>\n<hr/>\n<p></p><h2>\ud83d\udcc4 License</h2><a href=\"#-license\"></a><p></p>\n<p>This project is licensed under the BSD 3-Clause License \u2013 see the <a href=\"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe/blob/main/LICENSE\">LICENSE</a> file for details.</p>\n<hr/>\n</article></div></div>",
      "url": "https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe"
    },
    {
      "title": "Winning solutions of kaggle competitions",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=9a25cc6d5bc1be8a8370:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions/notebook?scriptVersionId=4534558"
    },
    {
      "title": "How to Win Kaggle Competitions",
      "text": "How to Win Kaggle Competitions | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n[\n](https://www.kaggle.com/zusmani)\n[Zeeshan-ul-hassan Usmani](https://www.kaggle.com/zusmani)\u00b7Posted8 years agoin[Getting Started](https://www.kaggle.com/discussions/getting-started)\narrow\\_drop\\_up534\nmore\\_vert\n# How to Win Kaggle Competitions\nKaggle is the perfect platform for a data scientist to hone their skills, build a great reputation and potentially get some quick cash. However, succeeding on Kaggle is no small task; it takes patience, hard work, and consistent practice. Keep in mind that this platform is home to some of the most brilliant minds in data sciences, so the competition is tough. To become a grandmaster, you need a high level of commitment and industry insights. This chapter will give you a brief guideline on how to succeed on Kaggle.\n**Step one**is to start by reading the competition guidelines thoroughly. Many Kagglers who are struggling to succeed on this platform do not have a thorough understanding of the competition, that is the overview, description, timeline, evaluation and eligibility criteria and the prize. Ignoring these little details will cost you big time in the long run. You need to know the deadline for your last submission. Small details such as the timeline of a particular competition are deal breakers. By studying the guidelines clearly, you will also uncover other commonly missed details such as the appropriate submission format and a guide on reproducing benchmarks. Do not start working on a Kaggle competition before you are clear about all the instructions. Take your time before jumping in.\n**The second and very crucial step**is to understand the performance measures. How the performance measure works is the yardstick your submission will be measured against, and you need to know it inside out. According to most experienced Kagglers, an optimised approach that is suitable to a particular measure makes it substantially easy to boost your score. For instance, Mean Square Error (MSE) and Mean Absolute Error (MAE) are closely related, not knowing the difference will penalize your end score.\n**Step three**is to understand the data in detail. You start with exploratory data analysis to find missing and null values and hidden patterns in the dataset. The more you know about the data, the better models you can build on top of it to improve your performance. Over-specialisation works in your favor as far as you do not over-fit. See what data weaknesses you can exploit for your own advantage, can you extract second fields from the given primary values, or can you typecast the given values to any other format to make it more machine learning friendly.\n**Step four**is to know what you want (objective) before worrying about how. Most novoices on Kaggle tend to worry excessively about which language to use (R or Python). It is wise, to begin with learning the data and ascertaining the patterns you intend to model. Knowing the domain and understanding data goes a long way when it comes to winning the competition.\n**Step five**and the often neglected step is to setup your own local validation environment. By doing that, you will be able to move at a faster pace. This will enable you to produce dependable results instead of solely relying on leader-board scores. You can skip this step if you are out of time or the dataset is too small and can easily be managed and executed on Kaggle dockers. By setting up your own environment, you can run the submission as many times as you like and you are not bound with five submissions a day restriction on Kaggle competitions. Once you feel confident enough about the results, you can submit it to live competition. It gives you an immense edge over your peers who do not have their local environments setup. By reducing the number of submissions you make, you are also substantially reducing the probability of over-fitting the leader-board, and it will save you for poor results at the evaluation stage.\n**Step six**is to read the forums. Forums and discussions are your friend. Take your time to consistently monitor the forum as you work on the competition, there is no way around it. Please subscribe to the forum and receive notifications related to the competition you are participating in. The forum will help you keep abreast with what the competition is up to. This has been made possible by the recent Kaggle trend of sharing code as the competition is going on. The host also shares their insights and directions about the competition on the forum more often. Even if you do not win, you can keep trying and learn from the post-competition summaries available at the forum to see where you went wrong or what your peers did to supersede your brilliance. This is a great way to learn from the best and improve consistently.\n**Step seven**is to research exhaustively. There is a good possibility that the competition you are participating is by people who have dedicated their lives to finding a viable solution. The people who host such competitions often have codes, benchmarks, official company blogs and extensive published papers or patents that come in handy. Even if you do not win in your first several attempts, you will learn, hone your skills and become a better data scientist.\n**Step eight**to stay with basics and apply it rigorously. While playing around with obscure methods is fun for data scientists, it is the basics that will get you far in a competition. The common algorithms you may ignore have great implementations. It is wise to do manual tuning or main parameters when experimenting with methods. Experienced Kagglers admit that one of the winning habits is to do the manual tuning.\n**Step nine**is the mother of all steps. It\u2019s time to ensemble models. It simply means combining all the models that you have developed independently. In most high profile competitions, different teams usually come together to combine their models to boost their scores. Since no competition on Kaggle has ever been won through a single model, it is wise to merge different independent models even when you are doing the solo ride.\n**Step ten**is the commitment to work on a single or selected few projects. If you commit and try to compete in every single competition, you will lose focus. It is better to focus on one or two and prove your mettle. The rank progression all the way to grand master will come naturally doing that. Remember the time and patience are two prime factors along with your data science expertise to move forward.\n**Step eleven**is the final step to pick the right approach. In the history of Kaggle, there are only two winning approaches that keep emerging from all the competitions. Feature engineering and Neural/Deep Learning Networks.\nFeature engineering is the best approach if you understand the data. The first step is taking the provided data and using it to accurately plot histograms to help you explore more. You will then typically spend a large amount of time generating features and then testing which ones correlate with the given target variables. For example, in a recent Kaggle competition titled Don\u2019t Get Kicked hosted by a chain of dealers known as Carvana. The participants were required to predict the cars that would go up for sale in a second hand (pre-owned) auction and the ones that will not be sold. Many participants put forward their algorithms and models. Ultimately, it turns out that the most feasible predictive feature was color. The participants grouped the cars into two categories: standard colors and unusual colors. It turns out that unusually colored car is more likely to be sold at a second-hand auction. Before Kaggle was able to arrive at this conclusion, there were numerous hypotheses, models, and kernel that did not perform the way expected.\nThe most popular winning al...",
      "url": "https://www.kaggle.com/discussions/getting-started/44997"
    },
    {
      "title": "What are the most effective text classification algorithms for NLP?",
      "text": "How to Choose the Best Text Classification Algorithm for NLP\nAgree & Join LinkedIn\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n``````````````\n![]()## Sign in to view more content\nCreate your free account or sign in to continue your search\nSign in\n## Welcome back\n````````````````````\nEmail or phone\nPassword\nShow\n[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_forgot_password)Sign in\nor\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=pulse-article_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy).\nNew to LinkedIn?[Join now](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&trk=pulse-article_contextual-sign-in-modal_sign-in-modal_join-link)\nor\nNew to LinkedIn?[Join now](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&trk=pulse-article_contextual-sign-in-modal_join-link)\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\nLinkedIn\nLinkedIn is better on the app\nDon\u2019t have the app? Get it in the Microsoft Store.\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&amp;mode=mini&amp;cid=guest_desktop_upsell)\n``\n````````````\n[Skip to main content](#base-template__workspace)\n````````\n1. [All](https://www.linkedin.com/pulse/topics/home/)\n2. [Engineering](https://www.linkedin.com/pulse/topics/engineering-s166/)\n3. [Machine Learning](https://www.linkedin.com/pulse/topics/engineering-s166/machine-learning-s3289/)\n# What are the most effective text classification algorithms for NLP?\nPowered by AI and the LinkedIn community\n### 1\n[Naive Bayes](#naive-bayes)\n### 2\n[Logistic Regression](#logistic-regression)\n### 3\n[Support Vector Machines](#support-vector-machines)\n### 4\n[Neural Networks](#neural-networks)\n### 5\n[Decision Trees and Random Forests](#decision-trees-and-random-forests)\n### 6\n[Here\u2019s what else to consider](#here\u2019s-what-else-to-consider)\nText classification is a common task in natural language processing (NLP), where you want to assign a label or category to a piece of text based on its content and context. For example, you might want to classify an email as spam or not, a product review as positive or negative, or a news article as political or sports. But how do you choose the best algorithm for your text classification problem? In this article, you will learn about some of the most effective text classification algorithms for NLP, and how to apply them to your data.\n``\nTop experts in this article\nSelected by the community from 47 contributions.[Learn more](https://www.linkedin.com/help/linkedin/answer/a1652832)\n* [![Member profile image]()\nTavishi Jaglan\nData Science Manager @Publicis Sapient | 4xGoogle Cloud Certified | Gen AI | LLM | RAG | Graph RAG | Mlops |DL | NLP |\u2026\n](https://in.linkedin.com/in/tavishi1402?trk=article-ssr-frontend-x-article)\nView contribution````\n![]()![]()18\n``````````````\n* [![Member profile image]()\nJyothsna Devi Tiruveedhula\nEngineer-1 @Verizon | DE Intern\u201925 @Verizon | DS Intern\u201924 @PayPal | CSE-AI ML Grad (VIT-AP Rank 4) | Data Science |\u2026\n](https://in.linkedin.com/in/jyothsna-devi-tiruveedhula-b70a33222?trk=article-ssr-frontend-x-article)\nView contribution````\n![]()![]()15\n``````````````\n* [![Member profile image]()\nRishabh Mishra\nData Analyst @ Highspring (On-site Google) | Ex. AI Engineer Intern @ THEFINANSOL | Aspiring Data Scientist |\u2026\n](https://in.linkedin.com/in/rishabhh-mishra?trk=article-ssr-frontend-x-article)\nView contribution````\n![]()![]()8\n``````````````\n![]()![]()![]()\nSee what others are saying\n``\n## [](#naive-bayes)1Naive Bayes\nNaive Bayes is a simple and fast algorithm that works well for many text classification problems. It is based on the assumption that the words in a text are independent of each other, and that the probability of a text belonging to a class is proportional to the product of the probabilities of each word in that class. Naive Bayes can handle large and sparse data sets, and can deal with multiple classes. However, it may not perform well when the words are not independent, or when there are strong correlations between features and classes. To use Naive Bayes for text classification, you need to first convert your text into a vector of word counts or frequencies, and then apply the Bayes theorem to calculate the class probabilities.\n``````\n[\nAdd your perspective\n](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&amp;trk=article-ssr-frontend-x-article)\n````````````````````````\nHelp others by sharing more (125 characters min.)Cancel\nAddSave\n* [![Contributor profile photo]()![Contributor profile photo]()\nJyothsna Devi Tiruveedhula\nEngineer-1 @Verizon | DE Intern\u201925 @Verizon | DS Intern\u201924 @PayPal | CSE-AI ML Grad (VIT-AP Rank 4) | Data Science | Machine Learning | Deep Learning | NLP\n](https://in.linkedin.com/in/jyothsna-devi-tiruveedhula-b70a33222?trk=article-ssr-frontend-x-article)\n* Copy link to contribution\n* [Report contribution](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/advice/1/what-most-effective-text-classification-algorithms&amp;trk=comment-semaphore-sign-in-redirect&amp;guestReportContentType=AUTO_GENERATED_SKILL_ARTICLE_CONTRIBUTION&amp;_f=guest-reporting)\nThanks for letting us know&#33; You&#39;ll no longer see this contribution\n``````\nNaive Bayes is indeed a solid choice for text classification due to its simplicity and efficiency, especially when dealing with large datasets. However, its assumption of word independence can be limiting in capturing more complex relationships within text data. In scenarios where semantic meaning and context play a crucial role, more advanced algorithms such as recurrent neural networks (RNNs) or transformer-based models like BERT and GPT-3 might outperform Naive Bayes. These deep learning models excel in capturing intricate patterns and contextual nuances in language, making them particularly effective for sophisticated NLP tasks.\n\u2026see more\n``\n[\nLike\n](https://www.linkedin.com/signup/cold-join?session_redirect=/advice/1/what-most-effective-text-classification-algorithms&amp;trk=article-ssr-frontend-x-article_contribution-social-activity_like-cta)\n![Like]()Like\n![Celebrate]()Celebrate\n![Support]()Support\n![Love]()Love\n![Insightful]()Insightful\n![Funny]()Funny\n````\n![]()![]()14\n``````````````\n* [![Contributor profile photo]()![Contributor profile photo]()\nShravan Kumar K.\nAI Leader | AI Speaker | IIT Madras - IIM Bangalore Alumnus | Associate Director at Novartis| Gen AI | 40 under 40 DS, AIM-25\n](https://in.linkedin.com/in/shravankoninti?trk=article-ssr-frontend-x-article)\n* Copy link to contribution\n* [Report contribution](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/advice/1/what-most-effective-text-classification-algorithms&...",
      "url": "https://www.linkedin.com/advice/1/what-most-effective-text-classification-algorithms"
    },
    {
      "title": "Winning solutions of kaggle competitions 2021",
      "text": "Winning solutions of kaggle competitions 2021\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a2d795818a3e7102:1:10007)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/mathurinache/winning-solutions-of-kaggle-competitions-2021"
    },
    {
      "title": "GitHub - YuriyGuts/kaggle-quora-question-pairs: My solution to Kaggle Quora Question Pairs competition (Top 2%, Private LB log loss 0.13497).",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[YuriyGuts](https://github.com/YuriyGuts)/ **[kaggle-quora-question-pairs](https://github.com/YuriyGuts/kaggle-quora-question-pairs)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FYuriyGuts%2Fkaggle-quora-question-pairs) You must be signed in to change notification settings\n- [Fork\\\n38](https://github.com/login?return_to=%2FYuriyGuts%2Fkaggle-quora-question-pairs)\n- [Star\\\n102](https://github.com/login?return_to=%2FYuriyGuts%2Fkaggle-quora-question-pairs)\n\n\nMy solution to Kaggle Quora Question Pairs competition (Top 2%, Private LB log loss 0.13497).\n\n[www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)\n\n### License\n\n[MIT license](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/LICENSE)\n\n[102\\\nstars](https://github.com/YuriyGuts/kaggle-quora-question-pairs/stargazers) [38\\\nforks](https://github.com/YuriyGuts/kaggle-quora-question-pairs/forks) [Branches](https://github.com/YuriyGuts/kaggle-quora-question-pairs/branches) [Tags](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tags) [Activity](https://github.com/YuriyGuts/kaggle-quora-question-pairs/activity)\n\n[Star](https://github.com/login?return_to=%2FYuriyGuts%2Fkaggle-quora-question-pairs)\n\n[Notifications](https://github.com/login?return_to=%2FYuriyGuts%2Fkaggle-quora-question-pairs) You must be signed in to change notification settings\n\n# YuriyGuts/kaggle-quora-question-pairs\n\nmaster\n\n[Branches](https://github.com/YuriyGuts/kaggle-quora-question-pairs/branches) [Tags](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[71 Commits](https://github.com/YuriyGuts/kaggle-quora-question-pairs/commits/master/) |\n| [assets](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/assets) | [assets](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/assets) |\n| [data](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/data) | [data](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/data) |\n| [notebooks](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/notebooks) | [notebooks](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/notebooks) |\n| [provisioning](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/provisioning) | [provisioning](https://github.com/YuriyGuts/kaggle-quora-question-pairs/tree/master/provisioning) |\n| [.gitignore](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/.gitignore) | [.gitignore](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/.gitignore) |\n| [LICENSE](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/LICENSE) | [LICENSE](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/LICENSE) |\n| [README.md](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/README.md) | [README.md](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/README.md) |\n| [Vagrantfile](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/Vagrantfile) | [Vagrantfile](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/Vagrantfile) |\n| [requirements.txt](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/requirements.txt) | [requirements.txt](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/requirements.txt) |\n| [run-all.sh](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/run-all.sh) | [run-all.sh](https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/run-all.sh) |\n| View all files |\n\n## Repository files navigation\n\n# kaggle-quora-question-pairs\n\nMy solution to [Kaggle Quora Question Pairs competition](https://www.kaggle.com/c/quora-question-pairs) (Top 2%, Private LB log loss 0.13497).\n\n## Overview\n\nThe solution uses a mixture of purely statistical features, classical NLP features, and deep learning.\nAlmost 200 handcrafted features are combined with out-of-fold predictions from 4 neural networks having different architectures.\n\nThe final model is a GBM (LightGBM), trained with early stopping and a very small learning rate, using stratified K-fold cross validation.\n\n## Reproducing the Solution\n\n### Hardware Requirements\n\nAlmost all code (with the exception of some 3rd-party scripts) can efficiently utilize multi-core machines.\nAt the same time, some of them might be memory-hungry.\nAll code has been tested on a machine with 64 GB RAM.\nFor all non-neural notebooks, a `c4.8xlarge` AWS instance should do excellent.\n\nFor neural networks, a GPU is highly recommended.\nOn a GTX 1080 Ti, it takes about 8-9 hours to complete all 4 \"neural\" notebooks.\n\nYou'll need about 30 GB of free disk space to store the pre-trained word embeddings and the extracted features.\n\n### Software Requirements\n\n1. Python >= 3.6.\n2. [LightGBM](https://github.com/Microsoft/LightGBM) (compiled from sources).\n3. [FastText](https://github.com/facebookresearch/fastText) (compiled from sources).\n4. Python packages from `requirements.txt`.\n5. (Recommended) NVIDIA CUDA and a GPU version of TensorFlow.\n\n### Environment Provisioning\n\nYou can spin up a fresh Ubuntu 16.04 AWS instance and use Ansible to make all the necessary software installation and configuration (except the GPU-related stuff).\n\n1. Make sure to open the ports 22 and 8888 on the target machine.\n2. Navigate to `provisioning` directory.\n3. Edit `config.yml`:\n   - `jupyter_plaintext_password`: the password to set for the Jupyter server on the target machine.\n   - `kaggle_username`, `kaggle_password`: your Kaggle credentials (required to download the competition datasets).\n     Otherwise, download them to the `data` folder manually.\n4. Edit `inventory.ini` and specify your instance DNS and the private key file (\\*.pem) to access it.\n5. Run:\n\n\n```\n$ ansible-galaxy install -r requirements.yml\n$ ansible-playbook playbook.yml -i inventory.ini\n\n```\n\n\n### Running the Code\n\n#### Automatic\n\nRun `run-all.sh` from the repository root. Check `notebooks/output` for execution progress and `data/submissions` for the final results.\n\n#### Manual\n\nStart a Jupyter server in the `notebooks` directory. If you used the Ansible playbook, the server will already be running on port 8888.\n\nRun the notebooks in the following order:\n\n1. **Preprocessing**.\n\n\n\n```\n1) preproc-tokenize-spellcheck.ipynb\n2) preproc-extract-unique-questions.ipynb\n3) preproc-embeddings-fasttext.ipynb\n4) preproc-nn-sequences-fasttext.ipynb\n\n```\n\n2. **Feature extraction**.\n\nRun all `feature-*.ipynb` notebooks in arbitrary order.\n\n_Note_: for faster execution, run all `feature-oofp-nn-*.ipynb` notebooks on a machine with a GPU and NVIDIA CUDA.\n\n3. **Prediction**.\n\nRun `classify-lightgbm-cv-pred.ipynb`.\nThe output file will be saved as `DATETIME-submission-draft-CVSCORE.csv`\n\n\n## About\n\nMy solution to Kaggle Quora Question Pairs competition (Top 2%, Private LB log loss 0.13497).\n\n[www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs)\n\n### Topics\n\n[nlp](https://github.com/topics/nlp) [competition](https://github.com/topics/competition) [data-science](https://github.com/topics/data-science) [machine-learning](https://github.com/topics/machine-learning) [kaggle](https://github.com/topics/kaggle)\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/github.com#MIT-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Act...",
      "url": "https://github.com/YuriyGuts/kaggle-quora-question-pairs"
    }
  ]
}