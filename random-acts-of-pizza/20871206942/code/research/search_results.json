{
  "query": "What are winning approaches for text classification on Reddit posts combining content with user metadata features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning systems for Reddit\u2011post classification typically fuse a strong neural text encoder with a set of engineered user\u2011metadata features, then let a downstream classifier learn from the combined representation.  Modern approaches start by feeding the post title\u202f+\u202fbody into a pretrained transformer (e.g., BERT, RoBERTa, or fastText\u2011based embeddings) to capture contextual semantics, while a parallel branch extracts numeric or categorical signals such as the author\u2019s karma, account age, posting frequency, subreddit history, and inferred traits (age, gender, profession) from the Reddit API\u202f([RedDust](https://aclanthology.org/2020.lrec-1.751.pdf))\u202f([RedDust](https://aclanthology.org/2020.lrec-1.751.pdf)).  These metadata vectors are often passed through a shallow feed\u2011forward or gradient\u2011boosted tree (e.g., XGBoost, Random Forest) to learn non\u2011linear interactions, and the two branches are merged by concatenation or attention\u2011based late\u2011fusion before a final soft\u2011max layer.  This \u201cbest\u2011of\u2011both\u2011worlds\u201d design has been shown to improve performance on health\u2011related Reddit tasks, where a BLSTM trained on psycholinguistic features is combined with a transformer to raise prediction accuracy\u202f([Zanwar\u202fet\u202fal.,\u202f2022](https://aclanthology.org/2022.smm4h-1.50)).\n\nA second successful pattern is to treat the problem as extreme multi\u2011label classification (thousands of subreddit classes) and augment the text\u2011only baseline with metadata\u2011driven priors.  Researchers have demonstrated that LSTMs alone struggle with such high\u2011cardinality settings, but adding engineered features (e.g., subreddit\u2011specific posting patterns, user\u2011level activity statistics) and using ensemble methods (e.g., stacking a transformer, a fastText model, and a Random Forest on metadata) yields state\u2011of\u2011the\u2011art results on the 1\u202fM\u2011post, 1\u202f000\u2011class Reddit dataset\u202f([Kwasny\u202fet\u202fal.,\u202f2020](https://www.evolution.ai/post/an-imagenet-like-text-classification-task-based-on-reddit-posts))\u202f([Kwasny\u202fet\u202fal.,\u202f2020](https://www.evolution.ai/post/an-imagenet-like-text-classification-task-based-on-reddit-posts)).  Open\u2011source pipelines such as the Valohai 4\u202fM\u2011post classifier illustrate this workflow: collect posts with PRAW, generate fastText embeddings for the text, compute user\u2011level statistics, and train an ensemble that merges both signals\u202f([Valohai\u202fBlog,\u202f2020](https://valohai.com/blog/machine-learning-pipeline-classifying-reddit-posts)).\n\nFinally, task\u2011specific studies (e.g., predicting Reddit user age groups) confirm that combining linguistic cues with explicit account attributes\u2014such as comment count, subreddit participation diversity, and temporal activity patterns\u2014produces the highest F1 scores, especially when feature selection is guided by cross\u2011validation and the final model is a hybrid of a linear classifier on text embeddings and a tree\u2011based model on metadata\u202f([PMCID\u202f8087286,\u202f2021](https://pmc.ncbi.nlm.nih.gov/articles/PMC8087286)).  In practice, the winning recipe therefore consists of: (1) a pretrained transformer or fastText encoder for raw post content, (2) a curated set of user\u2011metadata features (karma, account age, activity metrics, inferred traits), (3) a fusion strategy (concatenation, attention, or stacking), and (4) an ensemble or multi\u2011task classifier that jointly optimizes over both modalities.",
      "url": ""
    },
    {
      "title": "An ImageNet-like text classification task based on Reddit posts",
      "text": "## Book a demo\n\nWhat would you like to know?I'd like to receive news & updates from Evolution AI.\n\nFor full terms & conditions, please read our [**privacy policy**](https://www.evolution.ai/privacy-policy).\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n[Blog Home](https://www.evolution.ai/about/blog)\n\n# An ImageNet-like text classification task based on Reddit posts\n\nRafal Kwasny, Daniel Friar, Giuseppe Papallo\n\nSeptember 8, 2020\n\n###### SECTIONS\n\nToday we are announcing a new NLP dataset which we are [hosting on Kaggle](https://www.kaggle.com/mswarbrickjones/reddit-selfposts), based on self-posts from reddit.com. Our aim was to try and create a text corpus which had a large number of distinct classes, but still have many examples per class. We have created a dataset of roughly 1M text posts, with 1013 distinct classes (1000 examples per class). The classes are based on the assumed \u2018topic\u2019 of the text post, the topics being a manually curated taxonomy based on subreddits (see next section).\n\nIt is similar in size and label variety to datasets such as [ImageNet](http://www.image-net.org/challenges/LSVRC/) in computer vision, though our labels are not individually checked by humans. We felt that there was a lack of interesting, publicly available datasets that fit this profile, even though we have seen private real-world datasets that do (for example, classifying companies into [SIC codes](https://www.sec.gov/info/edgar/siccodes.htm) based on their websites).\n\nWe also think that this type of problem is an interesting counterpoint to text classification problems with lower class numbers such as sentiment analysis, which are well studied. We find that the state of the art techniques here such as LSTMs do not always translate seamlessly to the many-class domain. We hope that this dataset will be used to guide research in NLP, [extreme classification](http://manikvarma.org/downloads/XC/XMLRepository.html), and be of interest to the wider machine learning community.\n\n\u200d\n\n## The dataset\n\nReddit is a popular link aggregating website. Users can submit posts, and other user\u2019s vote them up or down, allowing the most highly rated posts to gain the most attention. Reddit is divided into various \u2018subreddits\u2019 based on the types of posts being submitted, for example [r/politics](https://www.reddit.com/r/politics) or [r/MachineLearning](https://www.reddit.com/r/MachineLearning/). Subreddits are generally created and moderated by the users themselves, rather than the admins of Reddit.\n\n\u200d\n\n_Frontpage of r/MachineLearning, with one self-post expanded_\n\n\u200d\n\nThere are two main types of post one can submit on reddit - simple url link posts, and self-posts, with a title and a body of markdown text written by the user. We found from ad-hoc analyses that the large majority of self-posts were talking about the topic that their subreddit implied, suggesting that this may be an interesting task from a machine learning perspective.\n\nWe downloaded all the self-posts in a two year period (2016/06/01 --- 2018/06/01), and did a number of cleaning steps to try and find posts that were sufficiently detailed. This left us with about 3,000 subreddits which had 1,000 posts or more.Classifying into subreddits is often not feasible on its own due to massive overlap between the topics of different subreddits. For example consider the three subreddits [r/buildapc](https://www.reddit.com/r/buildapc), [r/buildmeapc](https://www.evolution.ai/www.evolution.ai), [r/buildapcforme](https://www.reddit.com/r/buildapcforme), or the 26 popular subreddits dedicated to the video game League of Legends (each popular character has its own dedicated subreddit). For this reason, we decided to build a taxonomy of subreddits --- classifying each subreddit into categories, and subcategories, so that we could easily find major overlaps. This was a long and painful process, for the full gory details, see \\[1\\].\n\n\u200d\n\n_A small sample of the dataset_\n\n\u200d\n\n## Things we learned about reddit making this dataset\n\n\u200d\n\n### Popular genres of subreddits\n\nHere is a breakdown of top-level categories in our taxonomy in this datasetWe found a few popular categories of subreddit with many many subcategories, that we were not aware of before this project:\n\n- Autos - subreddits dedicated to cars\n- Health - subreddits dedicated to any common health problem you can think of\n- Roleplay - there are many subreddits dedicated to users playing out fantasies (often sexual) in text form.\n- r4r (\u2018redditor for redditor\u2019) - subreddits acting as a craiglist type forum, or for introducing redditors with similar sexual preferences to one another.\n\n\u200d\n\n### Some unusual subreddits\n\nHere are some of the more interesting subreddits that made it into our dataset:\n\n- [r/emojipasta](https://www.reddit.com/r/emojipasta) \\- reddit users write stories using eye-watering quantities of emojis\n- [r/wayfarerspub](https://www.reddit.com/r/wayfarerspub), a role-playing subreddit set in a quaint fantasy pub, where redditors write their personal rpg characters (e.g. from dungeons and dragons) into narratives.\n- [r/SexWorkers](https://www.reddit.com/r/SexWorkers/)\\- a place for sex workers to discuss their trade.\n- [r/swoleacceptance](https://www.reddit.com/r/preppers/) \\- a tongue-in-cheek subreddit where the muscular disciples of the \u2018Iron Temple\u2019 (the gym) discuss their religion.\n- [r/preppers](https://www.reddit.com/r/preppers/) \\- redditors preparing for the end of western civilization.\n\n\u200d\n\n\u200d\n\n## Visualizing the dataset\n\nWe can map the contents of all the subreddits in our dataset by looking at the word frequencies in their titles/text and using standard techniques to map these onto a 2d plot (t-SNE). This gives us the following plot (N.B. this is an interactive plot, mouseover points and use the tools on the right to help navigate).\n\nSubreddits with similar content (in terms of word frequencies) will tend to mapped closer together. Also we have colored using the top-level category of the subreddit.\n\n\u200d\n\n\u200d\n\n## Benchmarking\n\nSadly we can\u2019t give too much away about our best performance on this dataset --- it builds upon proprietary research. However we can give a couple of basic benchmarks based on bag-of-words models (models based on word frequencies). We give benchmarks for Naive-Bayes (using unigrams/bigrams, Tf-Idf, chi2 feature selection), and FastText (using Facebook\u2019s official implementation \\[2\\]).\n\nThe metrics we give are \"Precision-at-K\" (P@K), this means that we give the model K guesses at the subreddit for each self-post, and find the proportion of the time one of these guesses is correct (for K=1,3,5).\n\nInterestingly, we found that popular sequential models, such as LSTMs as well as a transfer learning framework : [Open AI\u2019s transformer model](https://blog.openai.com/language-unsupervised/), were not competitive with these baselines (in fact, we struggled to get the transformer to train). It would be interesting to know if this is due to lack of effort on our part, or indicates something more interesting about their limitations.\n\n\u200d\n\n\u200d\n\n## Issues with the dataset\n\nThe biggest issue with the data is noisy labels - while many subreddits have been omitted for being generally off topic, posts have not been curated individually. We did a cursory analysis to try and work out what proportion of posts were \u2018good\u2019 enough to be potentially classified at top 5 precision, we believe that number is about 96%.\n\nThe taxonomy was also created manually, and due to it size this introduces ample room for human error. If you spot any problems, please send an email to [us](mailto:hello@evolution.ai), or post a topic on the [Kaggle discussion page](https://www.kaggle.com/mswarbrickjones/reddit-selfposts/discussion).\n\n\u200d\n\n\u200d\n\n## Talk to us!\n\nAt Evolution AI, we specialise in natural language processing, offering an annotation platform, as well as consultancy services for information extraction, classification and text-matchi...",
      "url": "https://www.evolution.ai/post/an-imagenet-like-text-classification-task-based-on-reddit-posts"
    },
    {
      "title": "Predicting Reddit User Age Groups: Classification Model Development",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<section><h3>Background</h3>\n<p>Social media are important for monitoring perceptions of public health issues and for educating target audiences about health; however, limited information about the demographics of social media users makes it challenging to identify conversations among target audiences and limits how well social media can be used for public health surveillance and education outreach efforts. Certain social media platforms provide demographic information on followers of a user account, if given, but they are not always disclosed, and researchers have developed machine learning algorithms to predict social media users\u2019 demographic characteristics, mainly for Twitter. To date, there has been limited research on predicting the demographic characteristics of Reddit users.</p></section><section><h3>Objective</h3>\n<p>We aimed to develop a machine learning algorithm that predicts the age segment of Reddit users, as either adolescents or adults, based on publicly available data.</p></section><section><h3>Methods</h3>\n<p>This study was conducted between January and September 2020 using publicly available Reddit posts as input data. We manually labeled Reddit users\u2019 age by identifying and reviewing public posts in which Reddit users self-reported their age. We then collected sample posts, comments, and metadata for the labeled user accounts and created variables to capture linguistic patterns, posting behavior, and account details that would distinguish the adolescent age group (aged 13 to 20 years) from the adult age group (aged 21 to 54 years). We split the data into training (n=1660) and test sets (n=415) and performed 5-fold cross validation on the training set to select hyperparameters and perform feature selection. We ran multiple classification algorithms and tested the performance of the models (precision, recall, F1 score) in predicting the age segments of the users in the labeled data. To evaluate associations between each feature and the outcome, we calculated means and confidence intervals and compared the two age groups, with 2-sample t tests, for each transformed model feature.</p></section><section><h3>Results</h3>\n<p>The gradient boosted trees classifier performed the best, with an F1 score of 0.78. The test set precision and recall scores were 0.79 and 0.89, respectively, for the adolescent group (n=254) and 0.78 and 0.63, respectively, for the adult group (n=161). The most important feature in the model was the number of sentences per comment (permutation score: mean 0.100, SD 0.004). Members of the adolescent age group tended to have created accounts more recently, have higher proportions of submissions and comments in the r/teenagers subreddit, and post more in subreddits with higher subscriber counts than those in the adult group.</p></section><section><h3>Conclusions</h3>\n<p>We created a Reddit age prediction algorithm with competitive accuracy using publicly available data, suggesting machine learning methods can help public health agencies identify age-related target audiences on Reddit. Our results also suggest that there are characteristics of Reddit users\u2019 posting behavior, linguistic patterns, and account features that distinguish adolescents from adults.</p></section><section><p><strong>Keywords:</strong> Reddit, social media, age, machine learning, classification</p></section></section><section><h2>Introduction</h2>\n<p>Public health campaigns are a primary means for government agencies and nongovernmental organizations to raise awareness about important health issues affecting their communities. Running effective public health education campaigns requires organizations to tailor messages and provide outreach that resonates with their target audience. For example, the US Food and Drug Administration (FDA) Center for Tobacco Products has developed separate public education campaigns on the health consequences of tobacco products for at-risk teens [<a href=\"#ref1\">1</a>]; lesbian, gay, bisexual, and transgender young adults [<a href=\"#ref2\">2</a>]; multicultural youth [<a href=\"#ref3\">3</a>]; rural male teens [<a href=\"#ref4\">4</a>]; and adult smokers [<a href=\"#ref5\">5</a>]. With increased media consumption and interpersonal interactions occurring online, social media platforms have become important in both engaging target audiences in public education campaigns and in understanding behaviors and perceptions around emerging public health issues across these target audiences. However, it can be challenging to apply results from social media analyses because there is limited information about who these data represent, and thus it is difficult to glean information from populations of interest. In tobacco prevention and control, being able to segment social media posts by age-based audience segments, would help researchers and public health agencies identify emerging issues and changes in behaviors and attitudes to facilitate public health surveillance and educational outreach to these at-risk populations. This would allow researchers to naturalistically observe of their target audience and how they interact with the discussion of tobacco products. This study presents a model developed to classify users of the popular social media site <em>Reddit</em> into those more likely to be older or younger than 21 years of age. These age categories mirror the federal minimum age for the purchase of tobacco products in the United States, which was amended from 18 years to 21 years of age [<a href=\"#ref6\">6</a>].</p>\n<p>Reddit is a public network of online communities organized around people\u2019s interests. As of 2019, an estimated 11% of US adults use Reddit, with an estimated 22% of US adults aged 18 to 29 years using the platform [<a href=\"#ref7\">7</a>]. Unlike other social media platforms such as Twitter that restrict post length, Reddit was designed for long-form entries, which allows users to provide much greater levels of detail in their posts. In addition, Reddit, similar to traditional forums, allows users to respond to posted material in the form of comment chains. This combination of longer post length and an engaged user base encourages active and nuanced discussions across a myriad of topics relevant to the public health community. In recent years, researchers have used Reddit data to understand emerging public health issues, including cannabis [<a href=\"#ref8\">8</a>-<a href=\"#ref10\">10</a>], opioid [<a href=\"#ref11\">11</a>,<a href=\"#ref12\">12</a>], and alcohol [<a href=\"#ref13\">13</a>,<a href=\"#ref14\">14</a>] use.</p>\n<p>Recent studies have used Reddit to investigate underage electronic nicotine delivery systems use. Brett et al [<a href=\"#ref15\">15</a>] performed a content analysis on Reddit posts to identify perceptions among Redditors about underage use of Juul. Zhan et al [<a href=\"#ref16\">16</a>] coded conversations about youth using Juul on the now-banned UnderageJuul subreddit. For studies focused on underage electronic nicotine delivery systems use, the authors used text indicators in posts or the subreddit community\u2019s age as proxies for identifying conversations among youth. However, not everyone posting on subreddits such as UnderageJuul are likely youth, and it is likely that youth are having conversations about vaping in other Reddit communities outside of electronic nicotine delivery systems specific subreddits. For these reasons, a broader approach is needed to identify youth who are discussing electronic nicotine delivery systems across Reddit communities.</p>\n<p>One way to identify adolescent conversations about electronic nicotine delivery systems is to examine the characteristics of Reddit users\u2019 posting behavior, look for patterns that distinguish adolescents from adults, and develop an algorithm to predict the age of Reddit users. Predicting latent user demographics on social media is a popular area of research, wi...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8087286"
    },
    {
      "title": "The Best of Both Worlds: Combining Engineered Features with Transformers for Improved Mental Health Prediction from Reddit Posts",
      "text": "## [The Best of Both Worlds: Combining Engineered Features with Transformers for Improved Mental Health Prediction from Reddit Posts](https://aclanthology.org/2022.smm4h-1.50.pdf)\n\n[Sourabh Zanwar](https://aclanthology.org/people/sourabh-zanwar/),\n[Daniel Wiechmann](https://aclanthology.org/people/daniel-wiechmann/),\n[Yu Qiao](https://aclanthology.org/people/yu-qiao/),\n[Elma Kerz](https://aclanthology.org/people/elma-kerz/)\n\n##### Abstract\n\nIn recent years, there has been increasing interest in the application of natural language processing and machine learning techniques to the detection of mental health conditions (MHC) based on social media data. In this paper, we aim to improve the state-of-the-art (SoTA) detection of six MHC in Reddit posts in two ways: First, we built models leveraging Bidirectional Long Short-Term Memory (BLSTM) networks trained on in-text distributions of a comprehensive set of psycholinguistic features for more explainable MHC detection as compared to black-box solutions. Second, we combine these BLSTM models with Transformers to improve the prediction accuracy over SoTA models. In addition, we uncover nuanced patterns of linguistic markers characteristic of specific MHC.\n\nAnthology ID:2022.smm4h-1.50Volume:[Proceedings of the Seventh Workshop on Social Media Mining for Health Applications, Workshop & Shared Task](https://aclanthology.org/volumes/2022.smm4h-1/)Month:OctoberYear:2022Address:Gyeongju, Republic of KoreaEditors:[Graciela Gonzalez-Hernandez](https://aclanthology.org/people/graciela-gonzalez/),\n[Davy Weissenbacher](https://aclanthology.org/people/davy-weissenbacher/)Venue:[SMM4H](https://aclanthology.org/venues/smm4h/)SIG:Publisher:Association for Computational LinguisticsNote:Pages:197\u2013202Language:URL:[https://aclanthology.org/2022.smm4h-1.50/](https://aclanthology.org/2022.smm4h-1.50/)DOI:Bibkey:zanwar-etal-2022-bestCite (ACL):Sourabh Zanwar, Daniel Wiechmann, Yu Qiao, and Elma Kerz. 2022. [The Best of Both Worlds: Combining Engineered Features with Transformers for Improved Mental Health Prediction from Reddit Posts](https://aclanthology.org/2022.smm4h-1.50/). In _Proceedings of the Seventh Workshop on Social Media Mining for Health Applications, Workshop & Shared Task_, pages 197\u2013202, Gyeongju, Republic of Korea. Association for Computational Linguistics.Cite (Informal):[The Best of Both Worlds: Combining Engineered Features with Transformers for Improved Mental Health Prediction from Reddit Posts](https://aclanthology.org/2022.smm4h-1.50/) (Zanwar et al., SMM4H 2022)Copy Citation:BibTeXMarkdownMODS XMLEndnoteMore options\u2026PDF:[https://aclanthology.org/2022.smm4h-1.50.pdf](https://aclanthology.org/2022.smm4h-1.50.pdf)\n\n[PDF](https://aclanthology.org/2022.smm4h-1.50.pdf) [Cite](https://aclanthology.org/aclanthology.org) [Search](https://www.semanticscholar.org/search?q=The+Best+of+Both+Worlds%3A+Combining+Engineered+Features+with+Transformers+for+Improved+Mental+Health+Prediction+from+Reddit+Posts) [Fix data](https://aclanthology.org/aclanthology.org)",
      "url": "https://aclanthology.org/2022.smm4h-1.50"
    },
    {
      "title": "",
      "text": "Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 6118\u20136126\nMarseille, 11\u201316 May 2020\n\rc European Language Resources Association (ELRA), licensed under CC-BY-NC\n6118\nRedDust: a Large Reusable Dataset of Reddit User Traits\nAnna Tigunova, Andrew Yates, Paramita Mirza, Gerhard Weikum\nMax Planck Institute for Informatics\nSaarland Informatics Campus, Saarbrucken, Germany \u00a8\n{tigunova, ayates, paramita, weikum}@mpi-inf.mpg.de\nAbstract\nSocial media is a rich source of assertions about personal traits, such as I am a doctor or my hobby is playing tennis. Precisely identifying\nexplicit assertions is difficult, though, because of the users\u2019 highly varied vocabulary and language expressions. Identifying personal\ntraits from implicit assertions like I\u2019ve been at work treating patients all day is even more challenging. This paper presents RedDust, a\nlarge-scale annotated resource for user profiling for over 300k Reddit users across five attributes: profession, hobby, family status, age,\nand gender. We construct RedDust using a diverse set of high-precision patterns and demonstrate its use as a resource for developing\nlearning models to deal with implicit assertions. RedDust consists of users\u2019 personal traits, which are (attribute, value) pairs, along\nwith users\u2019 post ids, which may be used to retrieve the posts from a publicly available crawl or from the Reddit API. We discuss the\nconstruction of the resource and show interesting statistics and insights into the data. We also compare different classifiers, which can\nbe learned from RedDust. To the best of our knowledge, RedDust is the first annotated language resource about Reddit users at large\nscale. We envision further use cases of RedDust for providing background knowledge about user traits, to enhance personalized search\nand recommendation as well as conversational agents.\nKeywords: personal knowledge, user profiling, conversational text, online forums\n1 Introduction\nReddit is a popular social media platform for discussing a\nwide range of topics. It is an important source of infor\u0002mation for data analysis on social media as it provides rich\nstructure, abundance of data and covers a broad range of\ntopics. Reddit is used by approximately 330 million users1\nwith 2.8 million comments written each day2. Alexa.com\nranks it as the 21st most popular website worldwide.\nDespite its popular and rich data, few have considered Red\u0002dit as a source of data about users\u2019 personal traits like their\nprofessions and hobbies. Prior work has focused on Red\u0002dit as a source of demographic information, whereas we\nconsider rich attributes like profession and hobbies in addi\u0002tion to demographic ones (age, gender, family status). Such\ndata has many applications, including personalizing health\u0002care (Gyrard et al., 2018), recommendations, search, and\nconversational agents.\nWe address this gap by creating a labeled dataset of Reddit\nusers (including their posts and comments) that covers five\nuser attributes: profession, hobby, family status, age, and\ngender. We leveraged three high-precision approaches to\nidentify predicates and their object values in users\u2019 posts:\n(1) natural language patterns matching assertions like I am\na flight attendant, (2) bracket patterns matching structured\nassertions of users\u2019 ages and genders (I [35m] just broke\nup with my girlfriend), and (3) flair metadata specific to\nparticular subfora. We used human judgments to validate\nthe high-precision nature of these approaches before per\u0002forming an analysis of the resulting dataset. To the best\nof our knowledge, RedDust is the first large scale semantic\nresource about user traits.\nWe illustrate the dataset\u2019s utility in two different use cases:\nbuilding lexicons specific to attribute values and predicting\n1\nhttps://redditblog.com/2018/11/13/holiday-on-reddit/\n2\nhttps://www.digitaltrends.com/social-media/\nreddit-ads-promoted-posts/\nusers\u2019 attribute values expressed implicitly (e.g., I\u2019ve been\nfixing sinks all day) after removing explicit assertions. This\nwork makes the following contributions:\n\u2022 We create a dataset of Reddit users traits, which are\nmined from users\u2019 personal assertions with several\nhigh-precision techniques. This resource is available\nat https://pkb.mpi-inf.mpg.de/reddust\n\u2022 We perform a thorough analysis of the dataset, which\nsheds light on its structure and composition.\n\u2022 We demonstrate two use cases for the dataset by build\u0002ing attribute-value-specific lexicons and performing\nclassification of the labeled attributes with several\nstate-of-the-art models.\n2 Related work\nUser Profiling in Online Communication: The popular\u0002ity of social media and online forums brings about massive\namounts of user-generated content that is freely accessible.\nThis has opened many research opportunities on text anal\u0002ysis, in particular on automatically identifying latent de\u0002mographic features of online users for personalized down\u0002stream applications such as personalized search or recom\u0002mendation. Such latent demographic attributes include age\nand gender (Basile et al., 2017; Bayot and Gonc\u00b8alves,\n2018; Burger et al., 2011; Fabian et al., 2015; Flekova\net al., 2016a; Kim et al., 2017; Rao et al., 2010; Sap et\nal., 2014; Schwartz et al., 2013a; Vijayaraghavan et al.,\n2017), personality (Gjurkovic and \u00b4 Snajder, 2018; Schwartz \u02c7\net al., 2013a), regional origin (Fabian et al., 2015; Rao et\nal., 2010), political orientation and ethnicity (Pennacchiotti\nand Popescu, 2011; Preot\u00b8iuc-Pietro et al., 2017; Preot\u00b8iuc\u0002Pietro and Ungar, 2018; Rao et al., 2010; Vijayaraghavan et\nal., 2017), as well as occupational class mapped to income\n(Flekova et al., 2016b; Preot\u00b8iuc-Pietro et al., 2015).\n6119\nMost prior works on automatically identifying users\u2019 la\u0002tent attributes from online communication rely on classi\u0002fication over hand-crafted features such as word/character\nn-grams (Basile et al., 2017; Burger et al., 2011; Rao et al.,\n2010), Linguistic Inquiry and Word Count (LIWC) (Pen\u0002nebaker et al., 2001) categories (Gjurkovic and \u00b4 Snajder, \u02c7\n2018; Preot\u00b8iuc-Pietro et al., 2017; Preot\u00b8iuc-Pietro and Un\u0002gar, 2018), topic distributions (Flekova et al., 2016a; Pen\u0002nacchiotti and Popescu, 2011; Preot\u00b8iuc-Pietro et al., 2015)\nand sentiment/emotion labels of words derived from exist\u0002ing emotion lexicon (Gjurkovic and \u00b4 Snajder, 2018; Pen- \u02c7\nnacchiotti and Popescu, 2011; Preot\u00b8iuc-Pietro et al., 2017;\nPreot\u00b8iuc-Pietro and Ungar, 2018). The recently prominent\nneural network approaches have also been adopted to solve\nthe task (Bayot and Gonc\u00b8alves, 2018; Kim et al., 2017; Ti\u0002gunova et al., 2019; Vijayaraghavan et al., 2017). Among\nthose prior works, Preot\u00b8iuc-Pietro et al. (2015), Basile et\nal. (2017), Bayot and Gonc\u00b8alves (2018) and Tigunova et al.\n(2019) are the ones that infer users\u2019 latent attributes based\nsolely on user-generated text, without relying on features\nspecific to social media like hashtags or users\u2019 profile de\u0002scription.\nDataset for User Profiling: Automatic methods, particu\u0002larly supervised learning approaches, for identifying users\u2019\npersonal attributes require a collection of user-generated\ncontent labelled with personal attributes of interest. Most\nof existing works mentioned above focus on user-generated\ncontent from Twitter, with a few exceptions that explore\nFacebook (Sap et al., 2014; Schwartz et al., 2013a) or\nReddit (Fabian et al., 2015; Finlay, 2014; Gjurkovic and \u00b4\nSnajder, 2018) posts. \u02c7\nData collection was mostly done via: manual annotation\nafter a focused search with specific keywords or hash\u0002tags (Preot\u00b8iuc-Pietro et al., 2015; Rao et al., 2010), public\nprofile linked to Twitter profile description (Burger et al.,\n2011; Flekova et al., 2016a), self-reports as part of an on\u0002line survey (Finlay, 2014; Flekova et al., 2016a; Preot\u00b8iuc\u0002Pietro et al., 2017; Preot\u00b8iuc-Pietro and Ungar, 2018; Sap\net al., 2014; Schwartz et al., 2013b), or pattern-based ex\u0002traction approach (e.g., (I|i) (am|\u2019m|was) born in +\nnumber (1920-2013)) on user ...",
      "url": "https://aclanthology.org/2020.lrec-1.751.pdf"
    },
    {
      "title": "A machine learning pipeline for classifying 4M Reddit posts",
      "text": "A machine learning pipeline for classifying 4M Reddit posts\n* [Documentation](https://docs.valohai.com)\n* [Login](https://app.valohai.com)\n[![Valohai logo](https://valohai.com/assets/img/valohai-logo.svg)](https://valohai.com/)\n* Platform\n* Solutions\n* Resources\n* [Pricing](https://valohai.com/pricing/)\n* [Start for free](https://app.valohai.com/accounts/signup/trial/)\n* [Book a demo](https://valohai.com/book-a-demo/)\n[Blog](https://valohai.com/blog/)/A machine learning pipeline for classifying 4M Reddit posts\n![A machine learning pipeline for classifying 4M Reddit posts](https://valohai.com/blog/machine-learning-pipeline-classifying-reddit-posts/subreddit-finder.jpg)\n# A machine learning pipeline for classifying 4M Reddit posts\nby[Ari Bajo](https://valohai.com/blog/author/ari-bajo/)| onMarch 31, 2020\nFinding the right subreddit to submit your post can be tricky, especially for people new to Reddit. There are thousands of active subreddits with overlapping content. If it is no easy task for a human, I didn\u2019t expect it to be easier for a machine. Currently, redditors can ask for suitable subreddits in a special subreddit:[r/findareddit](https://reddit.com/r/subreddit).\n![findareddit](findareddit.png)\nA Reddit user asking for subreddit suggestions.\nIn this article, I share how to build an end-to-end machine learning pipeline and an actual data product that suggests subreddits for a post. You get access to the data, code, model, an API endpoint and a user interface to[try it yourself](https://valohai.com/subreddit-finder).\n## Collecting 4M Reddit posts from 4k subreddits\nI used the Python Reddit API Wrapper ([PRAW](https://github.com/praw-dev/praw)) to ask for the 5k most popular subreddits but only got back 4k of them. For each subreddit, I collected the newest 1k post\u2019s titles and texts up to the 17th of March. I exported the data to a CSV[available for download on S3.](https://valohai-fasttext-example.s3.eu-west-3.amazonaws.com/reddit/reddit_posts_4M.csv)\n![Data set of reddit posts](posts_df_sample.png)\nDataFrame sample from 4M Reddit posts.\nThis dataset is far from perfect in terms of data quality. Some subreddits are too general, others are very similar to each other and some posts don\u2019t have enough information in the text to know where they should be posted. A previous approach[selected a subset of 1k subreddits that are more coherent topic-wise](https://www.kaggle.com/mswarbrickjones/reddit-selfposts/). While it\u2019s true that such data selection will make the model achieve higher scores on the test split, it won\u2019t necessarily be more useful on real data as it will miss a lot of subreddits.\nIt\u2019s known that data cleaning can have a high impact on a model&#x27;s performance and data scientists spend up to 80% of time cleaning data. Instead of spending a lot of time early on data transformations, I prefer to directly build an end-to-end baseline as fast and as simple as possible. Once I have got the first results, I can run version controlled experiments to see the impact of each transformation. Proceeding otherwise, you may end up with a more complex baseline and ignoring the impact of each transformation.\nAs a Data Scientist, I often overestimate the impact of a data transformation. By letting the final user interact with the model early on, you can learn and iterate faster. Also, as you start serving ML models on production you\u2019ll experience other challenges like retraining and releasing new model versions to avoid a common problem called[concept drift](https://en.wikipedia.org/wiki/Concept_drift).\n## Training a text classifier with fastText\nIn a previous article, I built a generic[ML pipeline for text classification using fastText](https://valohai.com/blog/production-machine-learning-pipeline-text-classification-fasttext). I reuse that code to train it on the Reddit dataset using the*title*and*selftext*as features and the*subreddit\\_name*as the label. In the training dataset each post is assigned to a single label, but it\u2019s natural to instead think of the problem as multi-label. In multi-label text classification, each post is assigned to each subreddit by a probability.\nReleased by Facebook, fastText is a neural network with two layers. The first layer trains word vectors and the second layer trains a classifier. As noted in[the original paper](https://arxiv.org/abs/1607.01759), fastText works well with a high number of labels. The collected dataset contains 300M words, enough to train word vectors from scratch with Reddit data. On top of that, fastText creates vectors for subwords controlled by two parameters,*minn*and*maxn,*to set the minimum and maximum character spans to split a word into subwords. On Reddit, typos are common and specific terms may be out of vocabulary if not using subwords.\n### The machine learning pipeline\nThe machine learning pipeline consists of 5 executions that exchange data through[Valohai pipelines](https://docs.valohai.com/hc/en-us/articles/18704268438033-Introduction-to-pipelines). Each execution is a Python CLI and you can find the code of each one on[Github](https://github.com/arimbr/valohai-fasttext-example/blob/master/models/classification/commands.py)and more details about how to create a pipeline that runs on the cloud in[the previous article](https://valohai.com/blog/production-machine-learning-pipeline-text-classification-fasttext).\n![end-to-end-ml-pipeline](end-to-end-ml-pipeline.jpg)\nEnd-to-end ML pipeline generated from dependencies between data artifacts, executions and API endpoints.\nThe text features are concatenated, transformed to lowercase and punctuation is removed. Then the data is split into train (80%), validation (10%) and test (10%). I set the autotune command to run for 24 hours on a cloud machine with 16 cores to find the best parameters on the validation dataset. Finally, the model is retrained on all the data and the final metrics are reported on the test dataset. For each execution, Valohai takes care of launching and stopping a cloud machine with the proper environment, code, data and parameters.\n![Autotune node in the machine learning pipeline](autotune.png)## Exploring the predictions and metrics\nClassification tasks can be evaluated with classic metrics such as precision, recall and f1-score. The autotune execution logs the best parameters and reports a f1-score of 0.41 in the validation dataset. The autotune execution smartly chose 9 different sets of parameters to decide on a final model that trained for 100 epochs, word vectors of 92 dimensions, n-grams of up to 3 words and subwords from 2 to 5 characters. That results on a vocabulary size of 3M words (including subwords) and a model that trains on 7 hours and weighs 2 GB.\n![Autotune logs](autotune-logs.png)\nLogs from the autotune execution.\nIn[extreme multi-label text classification tasks,](http://manikvarma.org/downloads/XC/XMLRepository.html)it\u2019s common to also report metrics P@k (precision when taking the first k predictions) and R@k (recall when taking the first k predictions). Below, we can see the precision and recall on the test dataset for different k values. R@k goes from 0.4 when taking one prediction (k=1) to 0.71 when taking twenty predictions (k=20).\n![Test executions for subreddit finder](test_executions.png)\nComparing metrics for the test execution for different k values\n### **Classification report by label**\nNaturally, metrics vary between subreddits. It\u2019s interesting to explore the f1-score histogram by subreddits and the relationship between the prediction probability and the f1-score. For that, I created a[Google Colab Notebook](https://colab.research.google.com/drive/12HhnbaHycgHsPZAhLRyBdUUQoTZzXjX7)to make graphs based on test\\_predictions.csv, the output of the test execution with k=20.\n![subreddit-hist](subreddit-hist.png)![p_vs_f1](p_vs_f1.png)There is a positive correlation between the f1-score and P@1, the probability of the first prediction given by the model. Still, p@1 lacks behind the f1-score on th...",
      "url": "https://valohai.com/blog/machine-learning-pipeline-classifying-reddit-posts"
    },
    {
      "title": "[PDF] Classification of posts on Reddit",
      "text": "Classification of posts on Reddit\u2217\nPooja Naik\nGraduate Student\nCSE Dept\nUCSD, CA, USA\npanaik@ucsd.edu\nSachin A S\nGraduate Student\nCSE Dept\nUCSD, CA, USA\nsachinas@ucsd.edu\nVincent Kuri\nGraduate Student\nCSE Dept\nUCSD, CA, USA\nvkuri@ucsd.edu\nABSTRACT\nOnline communities such as Reddit.com have unwritten rules\nof conduct that are only governed by the community itself.\nThe idea of creating and placing content to gain the most\namount of attention with the least amount of effort is the\ngoal of any user. A number of factors play a role in de\u0002termining the \u2019likeness\u2019 of a post on Reddit.com. Multiple\nresubmissions of the same content in multiple subreddits can\nprovide insightful relationships into how popular a new post\nabout the same content is going to be in a subreddit. Our\nmain goal is to predict the number of upvotes received on a\npost so we can analyse the factors affecting the prediction\nto use them to our advantage. Our experiment also aims\nto classify posts into subreddits using only textual features\nso we could use this technique to recommend sub-reddits to\nusers.\nKeywords\nData Science, Reddit post analysis, Multi class classification,\nRegression, Principal Component Analysis, Ransdom For\u0002est, Decision Tree, Collaborative Filtering, Machine Learn\u0002ing, Artificial Intelligence\n1. INTRODUCTION\nThe task of predicting the popularity of a post is espe\u0002cially complex because it depends on a number of factors.\nTo further increase the difficulty, online communities in Red\u0002dit.com have the concept of sub reddit which is akin to a\nsmaller sub community within a larger community. Since\neach such subreddit is unique in its own way, the unwritten\nrules related to posting content can vary widely between\ndifferent sub reddits. Theoretically, if we have enough data\nabout each and every subreddit, and also about each and\nevery post, then we might be able to gain insight about the\n\u2217\n(Produces the permission block, and copyright informa\u0002tion). For use with SIG-ALTERNATE.CLS. Supported by\nACM.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nCSE255 \u201915 San Diego, California USA\nCopyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\npopularity of a new post accurately. But such a dataset is\nvery hard to obtain, and some subreddits are so niche that\nthey just do not have enough data.\nAs a user of an online community similar to reddit, the\naim of the posting user is to gain the most number of up\u0002votes from the community.The main goal of our paper is to\npredict the number of upvotes a post gets. Since upvotes\nis a measure of how \u2019liked\u2019 a post is, being able to predict\nupvotes can provide a key insight into the factors affecting\nthis prediction and provides scope to influence these factors\nto maximize upvotes. To this effect, we have evaluated pre\u0002diction for a random test set (by doing a random split on\nthe data).\nWe further went on to build a recommendation model\nto recommend user what time of the day would be best to\nsubmit his post inorder to receive maximum upvotes. The\napproach was similar to item based collaborative filtering\nand built on a similarity matrix of post vs hour of the day\nwith values involving scaled version of upvote count.\nAs an additional challenge, we tried to accurately classify\nposts into sub-reddits. This is particularly hard due to the\nskewness of the data, but we try to use only the text in the\ntitle despite the presence of other features with the aim to\nbe able to use this in sub-reddit suggestion to users before\nthey submit the post.\n2. THE DATASET\nThe dataset used for this experiment was the Reddit dataset\nfrom the Stanford Network Analysis Project [1]. The dataset\nis made up of reddit posts that had been resubmitted mul\u0002tiple times with the same content. To ensure that posts had\nthe same content, only posts with images were considered.\nThe dataset consists of 132,307 images, which is made up of\n16,736 unique images. Each image has been submitted an\naverage of about 7 times.\nThe number of upvotes range from 0 to 86,707, with an\naverage of about 1058 upvotes per post. 45 posts have 0\nupvotes as compared to only 10 posts that have more than\n60,000 upvotes. Fig.1 shows the distribution of upvotes in\nthe data over 50 buckets.\nThere are 63,337 unique users whose posts are recorded in\nthis dataset giving us the idea that these users post multiple\ntimes. A little more than 20,000 posts don\u2019t have users\nassociated with them. Although the highest number of posts\na user has in the data is 5608, on an average a user posts\nabout 1-2 times, so user specific data is not very useful to\nus.\nThe number of downvotes range anywhere from 0 to 86707,\nFigure 1: Histogram showing distribution of upvotes\nwith an average of 825 downvotes per post. Surprisingly the\nnumber of posts with 0 downvotes is around 1,830 posts\nwhich shows that poeople prefer to upvote posts before they\neven begin downvoting posts, so it is not surprising to find\nonly 14 posts with more than 50,000 downvotes.\nThe dataset also gives us the number of comments that\nwere posted to a particular reddit post. The number of\ncomments range anywhere between 0 to 8357 comments for\nthe most popular one. On average 39 comments are posted\nper post. The low number can probably be attributed to the\nmultiple steps involved in posting a comment as compared\nto downvoting or upvoting a post. Hence, it is of no surprise\nthat 45,102 posts have 0 comments, and only 492 posts have\nmore than 1,000 comments.\nThe number of unique sub-reddits are 867, and only 63 of\nthose have more than 20 submissions, leading to a massive\nskew. The 63 sub-reddits account for around 129K posts\nwhile the remaining 804 sub-reddits only account for 2K\nposts. For the classification problem, we ignore posts from\nthe 804 sub-reddits in the training data, considering them\nas misclassified in the test data. The data is so skewed that\nonly 6 sub-reddits account for 116,253 posts and the largest\ngroup of posts, about 55k - almost half of the 116k, belong\nto the sub-reddit \u2019funny\u2019.\n3. FEATURES\nThe features have to be carefully selected so that they\ncan provide us with the most insight about the new post.\nEach of the selected features used in our model are outlined\nbelow.\nTitle Length : The number of characters in the title and\nthe number of words in the title are used as features because\nshorter titles are easier to read as compared to longer titles.\nHour of the day : Users are simply more active in cer\u0002tain hours of the day and the tendency to upvote is a loose\nfunction of that. As per our analysis, there was a weak cor\u0002relation between time of the day and upvotes received so we\nadded this information in the form of a 23-bit vector.\nAutomatic Readability Index of the title : ARI is a\nreadability test that is used to gauge the understanding of\na text. The output of the ARI is a number which gives the\nUS grade level of education needed to comprehend the text.\nThe ARI can provide insight on how the community reacts\nto different titles.\nDownvotes : Downvotes indicates how many users have\nFigure 2: Scatter plot showing correlation of upvotes\nand downvotes\ndisliked a post. Downvotes, as we find out from our evalu\u0002ation, turns out to be one of our most important features.\nFig.2 demonstates a clear correlation between upvotes and\ndownvotes.\nNumber of comments : Number of comments is very\nindicative of the popularity of a post and has a positive\ncorrelation with the upvotes.\nCommunity : The community or sub-reddit the post is\nposted in has a large influence on the upvotes. Communities\nare places where like-minded people interact with posts of\ntheir interest. Good content posted in the right subreddit\ncan g...",
      "url": "http://jmcauley.ucsd.edu/cse255/projects/fa15/021.pdf"
    },
    {
      "title": "GitHub - crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning: This project uses natural language processing (NLP) techniques with ensembled machine learning models to predict from which subreddit a post originates.",
      "text": "[Skip to content](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[crlandsc](https://github.com/crlandsc)/ **[Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fcrlandsc%2FReddit-Classification-With-Natural-Language-Processing-And-Machine-Learning) You must be signed in to change notification settings\n- [Fork\\\n0](https://github.com/login?return_to=%2Fcrlandsc%2FReddit-Classification-With-Natural-Language-Processing-And-Machine-Learning)\n- [Star\\\n1](https://github.com/login?return_to=%2Fcrlandsc%2FReddit-Classification-With-Natural-Language-Processing-And-Machine-Learning)\n\n\nThis project uses natural language processing (NLP) techniques with ensembled machine learning models to predict from which subreddit a post originates.\n\n[1\\\nstar](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/stargazers) [0\\\nforks](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/forks) [Branches](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/branches) [Tags](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tags) [Activity](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/activity)\n\n[Star](https://github.com/login?return_to=%2Fcrlandsc%2FReddit-Classification-With-Natural-Language-Processing-And-Machine-Learning)\n\n[Notifications](https://github.com/login?return_to=%2Fcrlandsc%2FReddit-Classification-With-Natural-Language-Processing-And-Machine-Learning) You must be signed in to change notification settings\n\n# crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/branches) [Tags](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[2 Commits](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/commits/main/) |\n| [code](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/code) | [code](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/code) |  |  |\n| [data](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/data) | [data](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/data) |  |  |\n| [images](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/images) | [images](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/images) |  |  |\n| [models](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/models) | [models](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/models) |  |  |\n| [presentation](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/presentation) | [presentation](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/tree/main/presentation) |  |  |\n| [.gitignore](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/blob/main/.gitignore) | [.gitignore](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/blob/main/.gitignore) |  |  |\n| [README.md](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/blob/main/README.md) | [README.md](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/blob/main/README.md) |  |  |\n| View all files |\n\n## Repository files navigation\n\n[![](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/raw/main/images/CL%20Banner.png)](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning/blob/main/images/CL%20Banner.png)\n\n# Reddit Classification With Natural Language Processing And Machine Learning\n\n##### Contents:\n\n- [Problem Statement](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#problem-statement)\n- [Background](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#background)\n- [Datasets](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#datasets)\n- [Data Collection](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#data-collection)\n- [Data Cleaning](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#data-cleaning)\n- [Preprocessing](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#preprocessing)\n- [Exploratory Data Analysis (EDA)](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#exploratory-data-analysis)\n- [Modeling](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#modeling)\n- [Conclusions](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#conclusions)\n- [Recommendations](https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning#recommendations)\n\n## Problem Statement\n\nReddit has hired my team of data scientists to prototype a machine learning model that utilizes Natural Language Processing (NLP) to determine which subreddit a post originated from, only provided its content (i.e. no title, no comments, no metadata). This model will be a binary classification model for two specific subreddits using a bag-of-words approach.\nReddit has determined 2 requirements for our team to measure success:\n\n1. Accuracy of classification > 95%\n2. Both Precision and Recall must remain > 90%\n\n## Background\n\nIdentifying the subreddit origin of a random post on Reddit is a challenging task that requires an in-depth understanding of the language, style, and content of different subreddits. In a world where misinformation and disinformation are prevalent, being able to accurately classify posts to their original source can significantly impact the credibility and reliability of the information being shared.\n...",
      "url": "https://github.com/crlandsc/Reddit-Classification-With-Natural-Language-Processing-And-Machine-Learning"
    }
  ]
}