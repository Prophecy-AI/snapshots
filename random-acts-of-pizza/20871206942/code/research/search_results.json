{
  "query": "How do Kaggle winners handle class imbalance in binary classification with AUC-ROC optimization?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**How top Kaggle solutions tackle severe class imbalance when the competition is scored with AUC\u2011ROC**\u202f([Lieberman\u202fet\u202fal.,\u202f2024](https://arxiv.org/html/2402.05400v2);\u202f[Zhu\u202fet\u202fal.,\u202f2022](https://arxiv.org/abs/2203.14177);\u202f[Mueller,\u202f2020](https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html))  \n\n1. **Pick loss functions that directly optimize AUC\u2011ROC** \u2013 instead of the usual cross\u2011entropy, winners use AU\u2011optimizing objectives (e.g., ROC\u2011LCT, AUC\u2011loss, focal\u2011type losses) that are shown to improve ROC curves on imbalanced data\u202f([Lieberman\u202fet\u202fal.](https://arxiv.org/html/2402.05400v2);\u202f[Zhu\u202fet\u202fal.](https://arxiv.org/abs/2203.14177)).  \n\n2. **Train over a *family* of such losses (Loss\u2011Conditional Training)** \u2013 rather than fixing a single loss, they sample several related AU\u2011optimizing losses during training. This reduces sensitivity to hyper\u2011parameters and yields models that are more robust on the validation AUC\u202f([Lieberman\u202fet\u202fal.](https://arxiv.org/html/2402.05400v2)).  \n\n3. **Balance the training distribution** \u2013 apply resampling techniques (random oversampling of the minority class, random undersampling of the majority, or a combination) so that the effective class prior is closer to 0.5, which stabilizes gradient updates for AU\u2011optimizing losses\u202f([Mueller,\u202f2020](https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html)).  \n\n4. **Add class\u2011weight or cost\u2011sensitive terms** \u2013 when resampling is not enough, incorporate higher weights for minority\u2011class examples in the loss (many Kaggle kernels expose a `class_weight` argument in PyTorch/TF or `scale_pos_weight` in XGBoost) to force the model to pay more attention to rare positives\u202f([Mueller,\u202f2020](https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html)).  \n\n5. **Validate with stratified folds and early\u2011stop on AUC\u2011ROC** \u2013 use stratified K\u2011fold cross\u2011validation so each fold preserves the original imbalance, monitor the validation AUC\u2011ROC after each epoch, and stop training when it stops improving. This ensures the selected checkpoint truly maximizes the competition metric\u202f([Zhu\u202fet\u202fal.](https://arxiv.org/abs/2203.14177)).  \n\n6. **Optionally calibrate probabilities** \u2013 after training, apply temperature scaling or isotonic regression to align predicted scores with true probabilities; calibrated scores often yield a smoother ROC curve and a higher final AUC\u202f([Zhu\u202fet\u202fal.](https://arxiv.org/abs/2203.14177)).  \n\nFollowing these steps\u2014AU\u2011optimizing losses, loss\u2011conditional training, careful resampling/weighting, and AUC\u2011focused validation\u2014mirrors the strategies that Kaggle competition winners repeatedly use to turn extreme class imbalance into high\u2011scoring AUC\u2011ROC models.",
      "url": ""
    },
    {
      "title": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions",
      "text": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\n# Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\nKelsey Lieberman, Shuai Yuan, Swarna Kamlam Ravindran &amp; Carlo Tomasi\nDepartment of Computer Science\nDuke University\nDurham, NC 27708 USA\n###### Abstract\nAlthough binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices. Code is available at[https://github.com/klieberman/roc\\_lct](https://github.com/klieberman/roc_lct).\n## 1Introduction\nConsider a classifier which takes images of skin lesions and predicts whether the lesions are melanoma> Rotemberg et\u00a0al. (\n[> 2020\n](https://arxiv.org/html/2402.05400v2#bib.bib16)> )\n. Such a system could be especially valuable in underdeveloped countries where expert resources for diagnosis are scarce> Cassidy et\u00a0al. (\n[> 2022\n](https://arxiv.org/html/2402.05400v2#bib.bib3)> )\n. Classifying melanoma from images is a problem with class imbalance since benign lesions are far more common than melanomas. Furthermore, the accuracy on the melanoma (minority) class is much more important than the accuracy on the benign (majority) class because predicting a benign lesion as melanoma would result in the cost of a biopsy while predicting a melanoma lesion as benign could result in the melanoma spreading before the patient can receive appropriate treatment.\nIn this case, overall accuracy, even on a balanced test set, is clearly an inadequate metric, as it implies that the accuracies on both classes are equally important. Instead, Receiver Operating Characteristic (ROC) curves are better suited for such problems> Egan (\n[> 1975\n](https://arxiv.org/html/2402.05400v2#bib.bib7)> )\n. These curves plot the tradeoff between the true positive rate (TPR) on the y-axis and the false positive rate (FPR) on the x-axis over a range of classification thresholds. Unlike scalar metrics (e.g., overall accuracy on a balanced test set orF\u03b2subscript\ud835\udc39\ud835\udefdF\\_{\\\\beta}italic\\_F start\\_POSTSUBSCRIPT italic\\_\u03b2 end\\_POSTSUBSCRIPT), ROC curves show model performance over a wide range of classification thresholds. This allows practitioners to understand how the model\u2019s performance changes based on different classification thresholds and choose the best tradeoff for their needs. ROC curves can also be summarized by their Area Under the Curve (AUC)> Egan (\n[> 1975\n](https://arxiv.org/html/2402.05400v2#bib.bib7)> )\n. Furthermore, both ROC curves and AUC have several mathematical properties which make them preferred to alternative precision-recall curves> Flach &amp; Kull (\n[> 2015\n](https://arxiv.org/html/2402.05400v2#bib.bib9)> )\n111We provide definitions and visualizations of commonly-used metrics for binary problems with imbalanced data in Appendix[A](https://arxiv.org/html/2402.05400v2#A1).\nAlthough binary problems, like melanoma classification, are often cited as the motivation for class imbalance problems and ROC curves are thede factometric of choice for such problems, the class imbalance literature largely focuses on improving performance on longtailed multi-class datasets in terms of overall accuracy on a balanced test set. We instead focus on binary problems with severe imbalance and propose a method, which adapts existing techniques for handling class imbalance, to optimize for ROC curves in these binary scenarios.\n![Refer to caption](extracted/5641151/figures/Melanoma_auc_boxplots.png)Figure 1:Distribution of Area Under the ROC Curve (AUC) values obtained by training the same model on the SIIM-ISIC Melanoma classification dataset with 48 different combinations of hyperparameters on VS Loss (hyperparameter values are given in Section[6](https://arxiv.org/html/2402.05400v2#S6)). Results are shown at three different imbalance ratios.As the imbalance becomes more severe, model performance drops and the variance in performance drastically increases. LCT addresses both of these issues by training over a family of loss functions, instead of a single loss function with one combination of hyperparameter values.\nIn particular, we adapt Vector Scaling (VS) loss, which is a general loss function for imbalanced learning with strong theoretical backing> Kini et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2402.05400v2#bib.bib11)> )\n. VS loss is a modification of Cross-entropy loss that adjusts the logits via additive and multiplicative factors. There is theory supporting the use of both of these factors: multiplicative factors are essential for the terminal phase of training, but these have negative effects early during training, so additive factors are necessary speed up convergence. Although VS loss has shown strong performance in the multi-class setting, it does have hyperparameters which require tuning (e.g., the additive and multiplicative factors on the loss function).\nWe find that, in the binary case, the effect of these hyperparameters is small and reasonable at moderate imbalance ratios\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2(where\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2is defined as the ratio of majority to minority samples); however, at large imbalance ratios, small differences in these hyperparameters lead to very wide variance in the results (Figure[1](https://arxiv.org/html/2402.05400v2#S1.F1)). This figure shows that increasing the imbalance ratio not only decreases the AUCs (as expected), but also drastically increases the variance in AUCs obtained by training models with slightly different hyperparameter values.\nIn this work, we highlight the practical effect of the theoretically-motivated VS loss on the ROC metric, especially for data problems with high imbalance ratios. We propose a method that adapts VS loss to align the training objective more closely with ROC curve optimization. Our method trains a single model on a wide range of hyperparameter values using Loss Conditional Training> Dosovitskiy &amp; Djolonga (\n[> 2020\n](https://arxiv.org/html/2402.05400v2#bib.bib5)> )\n. We find that this method not only reduces the variance in model performance caused by hyperparameter choices, but also improves performance over the best hyperparameter choices since it optimizes for many tradeoffs on the ROC curve. We provide extensive results\u2013 both on CIFAR datasets and datasets of real applications derived from Kaggle competitions\u2013 at multiple imbalance ratios and across a wide range of hyperparameter choices.\nIn summary, our contributions are as follows.\n* \u2022We identify that higher levels of imbalance are not only associated with worse model performance, but also more variance.\n* \u2022We recognize that training over a range of hyperparameter values can actually benefit classification models that are otherwise prone to overfitting to a single loss function.\n* \u2022We propose using Loss Conditional Training (LCT) to improve the training regimen for classification models trained under imbalance.\n* \u2022We show that this method consistently improves performance at high imbalance ratios.\n## 2Related work\nMany solutions have been proposed...",
      "url": "https://arxiv.org/html/2402.05400v2"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2203.14177] Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2203.14177\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2203.14177**(cs)\n[Submitted on 27 Mar 2022 ([v1](https://arxiv.org/abs/2203.14177v1)), last revised 4 Jul 2022 (this version, v3)]\n# Title:Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices\nAuthors:[Dixian Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+D),[Xiaodong Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+X),[Tianbao Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+T)\nView a PDF of the paper titled Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices, by Dixian Zhu and 2 other authors\n[View PDF](https://arxiv.org/pdf/2203.14177)> > Abstract:\n> The area under the ROC curve (AUROC) has been vigorously applied for imbalanced classification and moreover combined with deep learning techniques. However, there is no existing work that provides sound information for peers to choose appropriate deep AUROC maximization techniques. In this work, we fill this gap from three aspects. (i) We benchmark a variety of loss functions with different algorithmic choices for deep AUROC optimization problem. We study the loss functions in two categories: pairwise loss and composite loss, which includes a total of 10 loss functions. Interestingly, we find composite loss, as an innovative loss function class, shows more competitive performance than pairwise loss from both training convergence and testing generalization perspectives. Nevertheless, data with more corrupted labels favors a pairwise symmetric loss. (ii) Moreover, we benchmark and highlight the essential algorithmic choices such as positive sampling rate, regularization, normalization/activation, and optimizers. Key findings include: higher positive sampling rate is likely to be beneficial for deep AUROC maximization; different datasets favors different weights of regularizations; appropriate normalization techniques, such as sigmoid and $\\ell_2$ score normalization, could improve model performance. (iii) For optimization aspect, we benchmark SGD-type, Momentum-type, and Adam-type optimizers for both pairwise and composite loss. Our findings show that although Adam-type method is more competitive from training perspective, but it does not outperform others from testing perspective. Comments:|32 pages|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2203.14177](https://arxiv.org/abs/2203.14177)[cs.LG]|\n|(or[arXiv:2203.14177v3](https://arxiv.org/abs/2203.14177v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2203.14177](https://doi.org/10.48550/arXiv.2203.14177)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Dixian Zhu [[view email](https://arxiv.org/show-email/be468da8/2203.14177)]\n**[[v1]](https://arxiv.org/abs/2203.14177v1)**Sun, 27 Mar 2022 00:47:00 UTC (558 KB)\n**[[v2]](https://arxiv.org/abs/2203.14177v2)**Tue, 29 Mar 2022 03:39:26 UTC (1,110 KB)\n**[v3]**Mon, 4 Jul 2022 18:49:54 UTC (1,114 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices, by Dixian Zhu and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2203.14177)\n* [TeX Source](https://arxiv.org/src/2203.14177)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2203.14177&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2203.14177&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-03](https://arxiv.org/list/cs.LG/2022-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2203.14177?context=cs)\n[cs.AI](https://arxiv.org/abs/2203.14177?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.14177)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.14177)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.14177)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2203.14177&amp;description=Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2203.14177&amp;title=Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individu...",
      "url": "https://arxiv.org/abs/2203.14177"
    },
    {
      "title": "Working with highly imbalanced data \u2014 Applied Machine Learning in Python",
      "text": "[**Applied Machine Learning in Python**](https://amueller.github.io/index.html)\n\n[.ipynb](https://amueller.github.io/_sources/05-advanced-topics/11-imbalanced-datasets.ipynb).pdf\n\nOn this page\n\n# Working with highly imbalanced data [\u00b6](https://amueller.github.io/amueller.github.io\\#working-with-highly-imbalanced-data)\n\n## Recap on imbalanced data [\u00b6](https://amueller.github.io/amueller.github.io\\#recap-on-imbalanced-data)\n\n## Two sources of imbalance [\u00b6](https://amueller.github.io/amueller.github.io\\#two-sources-of-imbalance)\n\n- Asymmetric cost\n\n- Asymmetric data\n\n\nIn general, there\u2019s are two ways in which a classification\ntask can be imbalanced. First one is asymmetric costs. Even\nif the probability of class 0 and class 1 are the same, they\nmight be different like in business costs, or health costs,\nor any other kind of cost or benefit associated with making\ndifferent kinds of mistakes. The second one is having\nasymmetrical data. Meaning that one class is much more\ncommon than the other class.\n\n## Why do we care? [\u00b6](https://amueller.github.io/amueller.github.io\\#why-do-we-care)\n\n- Why should cost be symmetric?\n\n- All data is imbalanced\n\n- Detect rare events\n\n\nOne of these two is true in basically all real world\napplications. Usually, both of them are true. There\u2019s no\nreason why a false positive and a false negative should have\nthe same business cost, they\u2019re usually very, very different\nthings, no matter whether you do ad-click prediction or\nwhether you do health, the two kinds of mistakes are usually\nquite different, and have quite different real world\nconsequences.\n\nAlso, data is always imbalanced, and often very drastically.\nIn particular if you do diagnosis, or if you do ad clicks-or\nmarketing\u2026.For ad-clicks, I think it\u2019s like below 0.01% of\nads I clicked on, depending on how good doing with your\ntargeting. So very often, we have very few positives. And so\nthis is really a topic that is basically all of\nclassification. So balance classification with balance\ncosts, is not really something that happens a lot in the\nreal world.\n\n## Changing Thresholds [\u00b6](https://amueller.github.io/amueller.github.io\\#changing-thresholds)\n\n.tiny-code\\[\n\n```\ndata = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\n\nlr = LogisticRegression().fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nclassification_report(y_test, y_pred)\n```\n\n```\nprecision   recall  f1-score  support\n0              0.91     0.92      0.92       53\n1              0.96     0.94      0.95       90\navg/total      0.94     0.94      0.94      143\n```\n\n```\ny_pred = lr.predict_proba(X_test)[:, 1] > .85\n\nclassification_report(y_test, y_pred)\n```\n\n```\nprecision   recall  f1-score  support\n0              0.84     1.00      0.91       53\n1              1.00     0.89      0.94       90\navg/total      0.94     0.93      0.93      143\n```\n\n\\]\n\nSo apart from evaluation we talked about one way we could\u2019ve\nchanged the outcome to take into account which was changing\nthe threshold of greater probability. So not only taking\ninto account the predicted class. Assume I have a logistic\nregression model and I can either use the predict method\nwhich basically makes the cut off at 0.5 probability of the\npositive class, then I can look at the classification\nreport, which will tell me precision and recall for both the\npositive and the negative class. But if I want to increase\nrecall for class 0 or increase precision for class 1, I can\nsay only predict things as class 1 where the estimated\nprobability of class 1 is 0.85. And then I will have only\nthe ones that I\u2019m very certain predicted as class 1.\n\nIf you have given actual a cost function of how much each\nmistake costs, you can optimize this threshold.\n\nFIXME new classification report!!\n\n## Roc Curve [\u00b6](https://amueller.github.io/amueller.github.io\\#roc-curve)\n\n.center\\[\n\\]\n\nWe also looked at ROC curves, which basically look at all\npossible thresholds as you can apply. Either for\nprobabilistic prediction, or for any sort of continuous\nuncertainty estimate.\n\n### Remedies for the model [\u00b6](https://amueller.github.io/amueller.github.io\\#remedies-for-the-model)\n\nToday, I really want to talk about more, how we can change\nthe model more than just changing the threshold. So how can\nwe change the building of the model so that it takes into\naccount the asymmetric costs, or asymmetric data.\n\n## Mammography Data [\u00b6](https://amueller.github.io/amueller.github.io\\#mammography-data)\n\n.smallest\\[\n.left-column\\[\n\n```\nfrom sklearn.datasets import fetch_openml\n## mammography https://www.openml.org/d/310\ndata = fetch_openml('mammography', as_frame=True)\nX, y = data.data, data.target\nX.shape\n```\n\n(11183, 6)\n\n```\ny.value_counts()\n```\n\n```\n-1    10923\n1       260\n```\n\n\\]\n.right-column\\[\n.center\\[\n\\]\n\\]\n.reset-column\\[\n\n```\n## make y boolean\n## this allows sklearn to determine the positive class more easily\nX_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n```\n\n\\]\n\\]\n\nI use this mammography data set, which is very imbalanced.\nThis is a data set that has many samples, only six features\nand it\u2019s very imbalanced.\n\nThe datasets are about mammography data, and whether there\nare calcium deposits in the breast. They are often mistaken\nfor cancer, which is why it\u2019s good to detect them. Since its\nrigidly low dimensional, we can do a scatter plot. And we\ncan see that these are much skewed distributions and there\u2019s\nreally a lot more of one class than the other.\n\n## Mammography Data [\u00b6](https://amueller.github.io/amueller.github.io\\#id1)\n\n.smaller\\[\n\n```\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\n\nscores = cross_validate(LogisticRegression(),\n                        X_train, y_train, cv=10,\n                        scoring=('roc_auc', 'average_precision'))\nscores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n```\n\n0.920, 0.630\n\n```\nfrom sklearn.ensemble import RandomForestClassifier\nscores = cross_validate(RandomForestClassifier(),\n                        X_train, y_train, cv=10,\n                        scoring=('roc_auc', 'average_precision'))\nscores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n```\n\n0.939, 0.722\n\\]\n\nSo as a baseline here is just evaluating logistic regression\nand the random forest on this. Actually, I ran it under ROC\ncurve and average precision. I\u2019ve used a cross-validate\nfunction model selection that allows you to specify multiple\nmetrics. So I only need to train the model once but I can\nlook at multiple metrics.\n\nI do a 10 fold cross-validation on the data set and I split\ninto training and test and so I can look at the scores here.\nThe scores are dictionary, they give me training and test\nscores for all the metrics I specified. And so you can look\nat the mean test drug score and the mean test average\nprecision score. This gives a high AUC and a quite low\naverage precision.\n\nHere is a second baseline with a random forest doing the\nsame evaluation with ROC AUC and average precision. We get\nslightly higher AUC and quite a bit higher average\nprecision.\n\n## Basic Approaches [\u00b6](https://amueller.github.io/amueller.github.io\\#basic-approaches)\n\n.left-column\\[\n.center\\[\n\\]\n\\]\n\n.right-column\\[\n\nChange the training procedure\n\\]\n\nNow we want to change these basic training methods to be\nbetter adapted to this imbalanced dataset. There are\ngenerally two approaches. One is changing the data. And the\nother is change the training procedure and how you built the\nmodel. The easier one is to change the data. We can either\nadd samples to the data, we can remove samples to the data,\nor we can do both. Resampling is not possible in\nscikit-learn because of some API issues.\n\n## Sckit-learn vs resampling [\u00b6](https://amueller.github.io/amueller.github.io\\#sckit-learn-vs-resampling)\n\nThe problem with pipelines, as they\u2019re in scikit-learn right\nnow is, if you create a pipeline, and you call fit,...",
      "url": "https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html"
    },
    {
      "title": "Precision-Recall AUC vs ROC AUC for class imbalance problems | Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/general/7517#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/discussions/general/7517)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/general/7517#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fdiscussions%2Fgeneral%2F7517)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fdiscussions%2Fgeneral%2F7517)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=f732451c36ab3245b68f:1:10718)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/discussions/general/7517"
    },
    {
      "title": "Imbalanced data & why you should NOT use ROC curve",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Flct14558%2Fimbalanced-data-why-you-should-not-use-roc-curve)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Flct14558%2Fimbalanced-data-why-you-should-not-use-roc-curve)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=5502c2919e74bf426769:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve"
    },
    {
      "title": "Incorrect ROC score for Kaggle competition?",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [Incorrect ROC score for Kaggle competition?](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked6 years, 6 months ago\n\nModified [6 years, 6 months ago](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition?lastactivity)\n\nViewed\n177 times\n\n1\n\nI was researching a Kaggle competition and used a Logistic Regression classifier to test the top 10 competitiors' approaches.\n\nLink to the competition: [https://www.kaggle.com/c/detecting-insults-in-social-commentary/leaderboard](https://www.kaggle.com/c/detecting-insults-in-social-commentary/leaderboard)\n\nI'm still fairly new to the classification problems so I just tested classifiers without too much modifications. In this case I used scikit-learn's logreg. I cleaned the test/train data and used it to generate a ROC curve.\n\nMy area under the curve was 0.89 which would have placed me in 1st place with a significant lead and this seems quite impossible to me considering my implementation's simplicity. Could someone tell me if my program is doing something incorrectly that gives such a score (Ex. somehow overfitting or bug in code)?\n\n```\nimport csv\nimport preprocessor as p\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\npath = \"C:\\\\Users\\\\Mike\\\\Desktop\"\n\ndef vectorize_dataset(subpath, stem, vectorizer):\n    comments = []\n    labels = []\n    stemmer = SnowballStemmer(\"english\")\n    with open(path + subpath + '.csv', 'r') as f:\n        data_csv = csv.reader(f)\n\n        for row in data_csv:\n            clean_txt = p.clean(row[2])\n            clean_txt = clean_txt.strip().replace('\"', '').replace('\\\\\\\\', '\\\\').replace('_', ' ')\n            clean_txt = bytes(clean_txt, 'utf-8').decode('unicode_escape', 'ignore')\n            if stem:\n                clean_txt = [stemmer.stem(word.lower()) for word in word_tokenize(clean_txt)]\n            clean_txt = [word for word in clean_txt if word.isalpha()]\n            clean_txt = \" \".join(clean_txt)\n\n            if clean_txt != \"\":\n                if row[0] == str(1) or row[0] == str(0):\n                    comments.append(clean_txt)\n                    labels.append(int(row[0]))\n    if subpath == \"\\\\train\":\n        return (vectorizer.fit_transform(comments), labels)\n    return (vectorizer.transform(comments), labels)\n\ndef print_auroc_for_classifier(vect_tuple, classifier):\n    y_true, y_score = [], []\n\n    for sample, label in zip(vect_tuple[0], vect_tuple[1]):\n        y_true.append(label)\n        y_score.append(classifier.predict_proba(sample)[0][1])\n\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    print(\"ROC AUC: %.2f\" % roc_auc)\n\n    plt.plot(fpr, tpr)\n\nif __name__ == '__main__':\n    plt.figure()\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n    vectorizer = TfidfVectorizer()\n    train_tuple = vectorize_dataset('\\\\train', True, vectorizer)\n    test_tuple = vectorize_dataset('\\\\test', True, vectorizer)\n\n    logreg = linear_model.LogisticRegression(C=7)\n    logreg.fit(train_tuple[0].toarray(), train_tuple[1])\n\n    print_auroc_for_classifier(test_tuple, logreg)\n\n```\n\nInstructions:\n\n1. From the Kaggle link download the train.csv and test\\_with\\_solutions.csv.\n[https://www.kaggle.com/c/detecting-insults-in-social-commentary/data](https://www.kaggle.com/c/detecting-insults-in-social-commentary/data)\n2. Rename test\\_with\\_solutions.csv to test.csv\n3. In code set `path` to be the path to the .csv files\n\nFor the `C` parameter I do not understand it too much and if it is the reason my score is this high, please let me know and I appreciate any advice in finding a good value for it. Thanks.\n\nThe approach:\n\n1. Read .csv file and clean the text (used preprocessor package and manually replaced certain characters)\n2. Used Snowball stemmer and check each word isalpha()\n3. Vectorize the test and train data using scikit-learn's TfidfVectorizer\n4. Train logreg with training data\n5. Calculate and plot ROC curve\n\nEdit:\n\nSo I played around with the C parameter and setting C to a high value such as 1e5 gives me a lower ROC curve area. Perhaps now the main question is, should I be optimizing C to give me the highest ROC curve area assuming my code is correct and C was the parameter I needed to tune?\n\nEdit2: I used GridSearchSV to test C in range of 0.1 to 10 and still got high results (going past 10 and below 0.1 didnt do anything).\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [classification](https://stackoverflow.com/questions/tagged/classification)\n- [logistic-regression](https://stackoverflow.com/questions/tagged/logistic-regression)\n- [kaggle](https://stackoverflow.com/questions/tagged/kaggle)\n\n[Share](https://stackoverflow.com/q/47744347)\n\n[Improve this question](https://stackoverflow.com/posts/47744347/edit)\n\nFollow\n\n[edited Dec 11, 2017 at 4:58](https://stackoverflow.com/posts/47744347/revisions)\n\nMike\n\nasked Dec 10, 2017 at 22:52\n\n[![Mike's user avatar](https://www.gravatar.com/avatar/2b8b5d8b7bd87b50fb5579d45bc8eee2?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/9074689/mike)\n\n[Mike](https://stackoverflow.com/users/9074689/mike) Mike\n\n7588 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n0\n\nYou are using different testing data than would have been available - use just the test.csv file to find the best model and value for C, then evaluate it only on the impermium\\_verification\\_set.csv. When the competition was running, looks like only test was available to find a model, then models were locked and leaderboard was based on the verification set. You are using the full set of both to select the best model.\n\nYou can always ask on the discussion boards on the Kaggle competition page if you want - I'm sure people there will help also. Also some of the top placers, including the winner, have posted their code on the discussion page for interest.\n\n[Share](https://stackoverflow.com/a/47750365)\n\n[Improve this answer](https://stackoverflow.com/posts/47750365/edit)\n\nFollow\n\nanswered Dec 11, 2017 at 10:02\n\n[![Ken Syme's user avatar](https://www.gravatar.com/avatar/8f39257133ad3bbe7221f2a5d808674b?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/2236150/ken-syme)\n\n[Ken Syme](https://stackoverflow.com/users/2236150/ken-syme) Ken Syme\n\n3,55222 gold badges1818 silver badges1919 bronze badges\n\n2\n\n- Tha...",
      "url": "https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition"
    },
    {
      "title": "roc_auc_score #",
      "text": "roc\\_auc\\_score &#8212;&#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# roc\\_auc\\_score[#](#roc-auc-score)\nsklearn.metrics.roc\\_auc\\_score(*y\\_true*,*y\\_score*,*\\**,*average='macro'*,*sample\\_weight=None*,*max\\_fpr=None*,*multi\\_class='raise'*,*labels=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/metrics/_ranking.py#L472)[#](#sklearn.metrics.roc_auc_score)\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\nNote: this implementation can be used with[binary](../../glossary.html#term-binary),[multiclass](../../glossary.html#term-multiclass)and[multilabel](../../glossary.html#term-multilabel)classification, but some restrictions apply (see Parameters).\nRead more in the[User Guide](../model_evaluation.html#roc-metrics).\nParameters:**y\\_true**array-like of shape (n\\_samples,) or (n\\_samples, n\\_classes)\nTrue labels or binary label indicators. The binary and multiclass cases\nexpect labels with shape (n\\_samples,) while the multilabel case expects\nbinary label indicators with shape (n\\_samples, n\\_classes).\n**y\\_score**array-like of shape (n\\_samples,) or (n\\_samples, n\\_classes)\nTarget scores.\n* In the[binary](../../glossary.html#term-binary)case, it corresponds to an array of shape`(n\\_samples,)`. Both probability estimates and non-thresholded\ndecision values can be provided. The probability estimates correspond\nto the**probability of the class with the greater label**,\ni.e.`estimator.classes\\_[1]`and thus`estimator.predict\\_proba(X,y)[:,1]`. The decision values\ncorresponds to the output of`estimator.decision\\_function(X,y)`.\nSee more information in the[User guide](../model_evaluation.html#roc-auc-binary);\n* In the[multiclass](../../glossary.html#term-multiclass)case, it corresponds to an array of shape`(n\\_samples,n\\_classes)`of probability estimates provided by the`predict\\_proba`method. The probability estimates**must**sum to 1 across the possible classes. In addition, the order of the\nclass scores must correspond to the order of`labels`,\nif provided, or else to the numerical or lexicographical order of\nthe labels in`y\\_true`. See more information in the[User guide](../model_evaluation.html#roc-auc-multiclass);\n* In the[multilabel](../../glossary.html#term-multilabel)case, it corresponds to an array of shape`(n\\_samples,n\\_classes)`. Probability estimates are provided by the`predict\\_proba`method and the non-thresholded decision values by\nthe`decision\\_function`method. The probability estimates correspond\nto the**probability of the class with the greater label for each\noutput**of the classifier. See more information in the[User guide](../model_evaluation.html#roc-auc-multilabel).\n**average**{\u2018micro\u2019, \u2018macro\u2019, \u2018samples\u2019, \u2018weighted\u2019} or None, default=\u2019macro\u2019\nIf`None`, the scores for each class are returned.\nOtherwise, this determines the type of averaging performed on the data.\nNote: multiclass ROC AUC currently only handles the \u2018macro\u2019 and\n\u2018weighted\u2019 averages. For multiclass targets,`average=None`is only\nimplemented for`multi\\_class='ovr'`and`average='micro'`is only\nimplemented for`multi\\_class='ovr'`.\n`'micro'`:\nCalculate metrics globally by considering each element of the label\nindicator matrix as a label.\n`'macro'`:\nCalculate metrics for each label, and find their unweighted\nmean. This does not take label imbalance into account.\n`'weighted'`:\nCalculate metrics for each label, and find their average, weighted\nby support (the number of true instances for each label).\n`'samples'`:\nCalculate metrics for each instance, and find their average.\nWill be ignored when`y\\_true`is binary.\n**sample\\_weight**array-like of shape (n\\_samples,), default=None\nSample weights.\n**max\\_fpr**float &gt; 0 and &lt;= 1, default=None\nIf not`None`, the standardized partial AUC[[2]](#r4bb7c4558997-2)over the range\n[0, max\\_fpr] is returned. For the multiclass case,`max\\_fpr`,\nshould be either equal to`None`or`1.0`as AUC ROC partial\ncomputation currently is not supported for multiclass.\n**multi\\_class**{\u2018raise\u2019, \u2018ovr\u2019, \u2018ovo\u2019}, default=\u2019raise\u2019\nOnly used for multiclass targets. Determines the type of configuration\nto use. The default value raises an error, so either`'ovr'`or`'ovo'`must be passed explicitly.\n`'ovr'`:\nStands for One-vs-rest. Computes the AUC of each class\nagainst the rest[[3]](#r4bb7c4558997-3)[[4]](#r4bb7c4558997-4). This\ntreats the multiclass case in the same way as the multilabel case.\nSensitive to class imbalance even when`average=='macro'`,\nbecause class imbalance affects the composition of each of the\n\u2018rest\u2019 groupings.\n`'ovo'`:\nStands for One-vs-one. Computes the average AUC of all\npossible pairwise combinations of classes[[5]](#r4bb7c4558997-5).\nInsensitive to class imbalance when`average=='macro'`.\n**labels**array-like of shape (n\\_classes,), default=None\nOnly used for multiclass targets. List of labels that index the\nclasses in`y\\_score`. If`None`, the numerical or lexicographical\norder of the labels in`y\\_true`is used.\nReturns:**auc**float\nArea Under the Curve score.\nSee also\n[`average\\_precision\\_score`](sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)\nArea under the precision-recall curve.\n[`roc\\_curve`](sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)\nCompute Receiver operating characteristic (ROC) curve.\n[`RocCurveDisplay.from\\_estimator`](sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_estimator)\nPlot Receiver Operating Characteristic (ROC) curve given an estimator and some data.\n[`RocCurveDisplay.from\\_predictions`](sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_predictions)\nPlot Receiver Operating Characteristic (ROC) curve given the true and predicted values.\nNotes\nThe Gini Coefficient is a summary measure of the ranking ability of binary\nclassifiers. It is expressed using the area under of the ROC as follows:\nG = 2 \\* AUC - 1\nWhere G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation\nwill ensure that random guessing will yield a score of 0 in expectation, and it is\nupper bounded by 1.\nReferences\n[1]\n[Wikipedia entry for the Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n[[2](#id1)]\n[Analyzing a portion of the ROC curve. McClish, 1989](https://www.ncbi.nlm.nih.gov/pubmed/2668680)\n[[3](#id2)]\nProvost, F., Domingos, P. (2000). Well-trained PETs: Improving\nprobability estimation trees (Section 6.2), CeDER Working Paper\n#IS-00-04, Stern School of Business, New York University.\n[[4](#id3)]\n[Fawcett, T. (2006). An introduction to ROC analysis. Pattern\nRecognition Letters, 27(8), 861-874.](https://www.sciencedirect.com/science/article/pii/S016786550500303X)\n[[5](#id4)]\n[Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\nUnder the ROC Curve for Multiple Class Classification Problems.\nMachine Learning, 45(2), 171-186.](http://link.springer.com/article/10.1023/A:1010920819831)\n[6]\n[Wikipedia entry for the Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)\nExamples\nBinary case:\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportload\\_breast\\_cancer&gt;&gt;&gt;fromsklearn.linear\\_modelimportLogisticRegression&gt;&gt;&gt;fromsklearn.metricsimportroc\\_auc\\_score&gt;&gt;&gt;X,y=load\\_breast\\_cancer(return\\_X\\_y=True)&gt;&gt;&gt;clf=LogisticRegression(solver=&quot;newton-cholesky&quot;,random\\_state=0).fit(X,y)&gt;&gt;&gt;roc\\_auc\\_score(y,clf.predict\\_proba(X)[:,1])0.99&gt;&gt;&gt;roc\\_auc\\_score(y,clf.decision\\_function(X))0.99\n```\nMulticlass case:\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportload\\...",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
    },
    {
      "title": "Useful properties of ROC curves, AUC scoring, and Gini Coefficients",
      "text": "<div><article>\n\t<div>\n\t<p><strong>Receiver Operating Characteristic (ROC) curves</strong> and AUC values are often used to score binary classification models in Kaggle and in papers. However, for a long time I found them fairly unintuitive and confusing. In this blog post, I will explain some basic properties of ROC curves that are useful to know for Kaggle competitions, and how you should interpret them.</p>\n<p><em>Above: Example of a ROC curve</em></p>\n<p>First, the definitions. A ROC curve plots the performance of a binary classifier under various threshold settings; this is measured by true positive rate and false positive rate. If your classifier predicts \u201ctrue\u201d more often, it will have more true positives (good) but also more false positives (bad). If your classifier is more conservative, predicting \u201ctrue\u201d less often, it will have fewer false positives but fewer true positives as well. The ROC curve is a graphical representation of this tradeoff.</p>\n<p>A perfect classifier has a 100% true positive rate and 0% false positive rate, so its ROC curve passes through the upper left corner of the square. A completely random classifier (ie: predicting \u201ctrue\u201d with probability <em>p</em> and \u201cfalse\u201d with probability <em>1-p </em>for all inputs) will by random chance correctly classify proportion <em>p</em> of the actual true values and incorrectly classify proportion <em>p</em> of the false values, so its true and false positive rates are both <em>p</em>. Therefore, a completely random classifier\u2019s ROC curve is a straight line through the diagonal of the plot.</p>\n<p>The <strong>AUC (Area Under Curve)</strong> is the area enclosed by the ROC curve. A perfect classifier has AUC = 1 and a completely random classifier has AUC = 0.5. Usually, your model will score somewhere in between. The range of possible AUC values is [0, 1]. However, if your AUC is below 0.5, that means you can invert all the outputs of your classifier and get a better score, so you did something wrong.</p>\n<p>The <strong>Gini Coefficient</strong> is 2*AUC \u2013 1, and its purpose is to normalize the AUC so that a random classifier scores 0, and a perfect classifier scores 1. The range of possible Gini coefficient scores is [-1, 1]. If you search for \u201cGini Coefficient\u201d on Google, you will find a closely related concept from economics that measures wealth inequality within a country.</p>\n<hr/>\n<p>Why do we care about AUC, why not just score by percentage accuracy?</p>\n<p><strong>AUC is good for classification problems with a class imbalance.</strong> Suppose the task is to detect dementia from speech, and 99% of people don\u2019t have dementia and only 1% do. Then you can submit a classifier that always outputs \u201cno dementia\u201d, and that would achieve 99% accuracy. It would seem like your 99% accurate classifier is pretty good, when in fact it is completely useless. Using AUC scoring, your classifier would score 0.5.</p>\n<p>In many classification problems, the cost of a false positive is different from the cost of a false negative. For example, it is worse to falsely imprison an innocent person than to let a guilty criminal get away, which is why our justice system assumes you\u2019re innocent until proven guilty, and not the other way around. In a classification system, we would use a <em>threshold</em> rule, where everything above a certain probability is treated as 1, and everything below is treated as 0. However, deciding on where to draw the line requires weighing the cost of a false positive versus a false negative \u2014 this depends on external factors and has nothing to do with the classification problem.</p>\n<p><strong>AUC scoring lets us evaluate models independently of the threshold.</strong> This is why AUC is so popular in Kaggle: it enables competitors to focus on developing a good classifier without worrying about choosing the threshold, and let the organizers choose the threshold later.</p>\n<p><em>(Note: This isn\u2019t quite true \u2014 a classifier can sometimes be better at certain thresholds and worse at other thresholds. Sometimes it\u2019s necessary to combine classifiers to get the best one for a particular threshold. Details in the paper linked at the end of this post.)</em></p>\n<hr/>\n<p>Next, here\u2019s a mix of useful properties to know when working with ROC curves and AUC scoring.</p>\n<p><strong>AUC is not directly comparable to accuracy, precision, recall, or F1-score.</strong> If your model is achieving 0.65 AUC, it\u2019s incorrect to interpret that as \u201c65% accurate\u201d. The reason is that AUC exists independently of a threshold and is immune to class imbalance, whereas accuracy / precision / recall / F1-score do require you picking a threshold, so you\u2019re measuring two different things.</p>\n<p><strong>Only relative order matters for AUC score.</strong> When computing ROC AUC, we predict a probability for each data point, sort the points by predicted probability, and evaluate how close is it from a perfect ordering of the points. Therefore, AUC is invariant under scaling, or any transformation that preserves relative order. For example, predicting [0.03, 0.99, 0.05, 0.06] is the same as predicting [0.15, 0.92, 0.89, 0.91] because the relative ordering for the 4 items is the same in both cases.</p>\n<p>A corollary of this is we can\u2019t treat outputs of an AUC-optimized model as the likelihood that it\u2019s true. Some models may be<em> poorly calibrated</em> (eg: its output is always between 0.3 and 0.32) but still achieve a good AUC score because its relative ordering is correct. This is something to look out for when blending together predictions of different models.</p>\n<p>That\u2019s my summary of the most important properties to know about ROC curves. There\u2019s more that I haven\u2019t talked about, like how to compute AUC score. If you\u2019d like to learn more, I\u2019d recommend reading <a href=\"https://www.sciencedirect.com/science/article/pii/S016786550500303X\">\u201cAn introduction to ROC analysis\u201d by Tom Fawcett</a>.</p>\n\t</div>\n</article></div>",
      "url": "https://luckytoilet.wordpress.com/2018/04/04/useful-properties-of-roc-curves-auc-scoring-and-gini-coefficients"
    }
  ]
}