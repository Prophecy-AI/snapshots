{
  "query": "What ensembling strategies work best for combining text models (BERT) with gradient boosting on meta-data features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Combining a pretrained text encoder such as BERT with gradient\u2011boosted decision trees (GBDT) on auxiliary meta\u2011data works best when the two modalities are fused **at the feature level** and then fed to a single boosting model, or when they are kept separate and merged **at the prediction level** through stacking or weighted blending.  The most effective pattern reported in recent literature is to first extract dense BERT embeddings (or the output of a fine\u2011tuned transformer) for each text record, concatenate these vectors with the numeric and categorical meta\u2011features, and train a GBDT (e.g., LightGBM, CatBoost, XGBoost) on the combined representation.  This \u201cLLM\u2011Boost\u201d approach lets the tree\u2011based learner exploit the semantic information captured by the language model while still leveraging its strength on heterogeneous tabular data, and it consistently outperforms either component alone across small\u2011 to medium\u2011size datasets\u202f([arxiv.org/pdf/2502.02672](https://arxiv.org/pdf/2502.02672)).  A closely related variant, **PFN\u2011Boost**, replaces the generic LLM with the TabPFN transformer and achieves the highest average performance in the authors\u2019 benchmark\u202f([arxiv.org/pdf/2502.02672](https://arxiv.org/pdf/2502.02672)).\n\nWhen a full\u2011stack ensemble is preferred, a **two\u2011stage stacking** pipeline is often used: a BERT\u2011based TextPredictor is trained on the raw text column, a GBDT (LightGBM, CatBoost, or Random Forest) is trained on the structured meta\u2011features, and a meta\u2011learner (typically another lightweight GBDT or linear model) combines the individual predictions.  AutoGluon\u2019s multimodal tabular API implements exactly this workflow, automatically handling raw text with a pretrained BERT backbone and classical tabular models, then ensembling them via weighted averaging or stacking under the hood\u202f([auto.gluon.ai/0.7.0](https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html)).  Earlier work on generalized stacking for gradient\u2011boosting machines also supports this strategy, showing that a meta\u2011model can capture complementary error patterns between text\u2011only and tabular\u2011only learners\u202f([arxiv.org/abs/2010.06026](https://arxiv.org/abs/2010.06026)).\n\nA complementary **head\u2011replacement** technique replaces the usual MLP classification head on a fine\u2011tuned transformer with a GBDT trained on the transformer\u2019s hidden features (the \u201cFreeGBDT\u201d method).  This yields consistent gains on downstream tasks such as natural\u2011language inference, demonstrating that a GBDT can serve as a powerful, low\u2011overhead classifier on top of BERT representations\u202f([arxiv.org/abs/2105.03791](https://arxiv.org/abs/2105.03791)).  In practice, the choice among these strategies depends on data size and engineering constraints: feature\u2011level fusion (LLM\u2011Boost/PFN\u2011Boost) is simplest and scales well, stacking offers maximal flexibility when separate models are already in production, and head\u2011replacement provides a lightweight way to boost performance without adding a separate tabular model.",
      "url": ""
    },
    {
      "title": "",
      "text": "Transformers Boost the Performance of Decision Trees on Tabular\nData across Sample Sizes\nMayuka Jayawardhana\u2217 1, Renbo Tu2, Samuel Dooley3, Valeriia Cherepanova4,\nAndrew Gordon Wilson5, Frank Hutter6, Colin White7, Tom Goldstein1, Micah Goldblum8\n1 University of Maryland, 2 University of Toronto, 3 Meta, 4 Amazon, 5 New York University,\n6 University of Freiburg, 7 Abacus.AI, 8 Columbia University\nAbstract\nLarge language models (LLMs) perform remarkably well on tabular datasets in zero- and\nfew-shot settings, since they can extract meaning from natural language column headers that\ndescribe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on\nnumerous tables for in-context learning, has demonstrated excellent performance for dataset sizes\nup to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically\ntrained from scratch on each dataset without benefiting from pretraining data and must learn\nthe relationships between columns from their entries alone since they lack natural language\nunderstanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is\nessential, yet they are not competitive with GBDTs on medium or large datasets, since their\ncontext lengths are limited. In this paper, we propose a simple and lightweight approach for\nfusing large language models and TabPFN with gradient-boosted decision trees, which allows\nscalable GBDTs to benefit from the natural language capabilities and pretraining of transformers.\nWe name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or\nsurpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at\nsufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on\na wide range of dataset sizes in between. We demonstrate state-of-the-art performance against\nnumerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best\naverage performance among all methods we test for all but very small dataset sizes. We release\nour code at https://github.com/MayukaJ/LLM-Boost.\n1 Introduction\nTabular data, or spreadsheets, constitute a large portion of real-world machine learning problems\n[4]. Tabular data comprise (a) columns, each containing a different feature or label; (b) rows, each\ncontaining an individual data sample; and (c) column headers describing the content of each column,\noften in the form of text.\nGradient-boosted decision trees (GBDTs), such as XGBoost [6], LightGBM [22], and CatBoost\n[27], have remained the de facto machine learning algorithms for analyzing tabular data over the\npast decade [24]. They are efficient to train even on a CPU, they achieve competitive performance\non a wide variety of datasets and sample sizes, and they can be deployed via user-friendly packages\n\u2217Correspondence to: mayukaj@umd.edu, micah.g@columbia.edu.\n1\narXiv:2502.02672v2 [cs.CL] 6 Feb 2025\naccessible to non-experts. However, gradient-boosted decision trees have two major drawbacks:\n(a) They only ingest the row features in a table and not the column headers, which may contain\nuseful text descriptions. For example, one may not need training data to anticipate that a hospital\npatient\u2019s weight is useful for predicting occurrences of heart disease. Instead of leveraging column\nheaders, from which a human might intuit relationships between columns, GBDTs have to learn\nthese relationships from scratch from the feature values themselves. (b) GBDTs are trained from\nscratch on each dataset, instead of benefiting from vast prior experience on other datasets, a staple\nof foundation models.\nIn contrast to gradient-boosted decision trees, large language models (LLMs) can parse and\nextract meaning from column headers, enabling them to achieve performance superior to GBDTs on\nvery small tabular datasets with interpretable headers [18]. LLMs can even make accurate zero-shot\npredictions solely by applying natural language understanding to column headers without in-context\ntraining samples at all [18]. Despite their ability to parse column headers, LLMs are severely limited\nby their limited context length and high fine-tuning costs.\nTabPFN [19] is a tabular transformer, pretrained on a vast number of synthetic tables, that can\nsimultaneously perform in-context learning on an entire trainset and make predictions for the entire\ntestset all in a single forward pass. Similarly to LLMs, TabPFN performance is very strong on small\ndatasets, but it suffers from context-length limitations and can only handle datasets with up to\n1000 samples. Therefore, LLMs and TabPFN cannot easily make use of large sample sizes, whereas\nGBDTs scale well to massive datasets.\nIn this paper, we combine the strengths of gradient-boosted decision trees and recent transformers\nto build models that simultaneously benefit from pretraining and textual column headers while scaling\nto much larger tabular datasets than LLMs and TabPFN could alone. Our method LLM-Boost,\nuses LLM predictions as a starting point for GBDTs, and then learns the residuals from the LLM\npredictions to the label. This technique allows us to not only use the column headers for a strong\nprior but also benefits from the inductive bias and scalability of decision tree algorithms. In our\nexperiments, LLM-Boost showcases state-of-the-art performance, outcompeting strong baselines\nincluding both single models and other ensemble approaches, across a large range of dataset sizes.\nLLM-Boost excels at small and medium sized datasets that are too large for LLMs yet not large\nenough that column headers are not beneficial. Motivated by the strong performance of TabPFN,\nwe apply the same boosting approach swapping out LLMs for TabPFN. Importantly, we find that\nour boosted TabPFN combination, PFN-Boost, achieves the top performance among all methods\nwe consider outside of the very small dataset regime where our boosted LLMs reign supreme. We\nsummarize our contributions as follows.\n\u2022 We propose LLM-Boost: a novel yet simple and easy-to-implement boosting mechanism that\ncombines LLMs, which ingest semantic column headers, with GBDTs that can scale to massive\ndatasets.\n\u2022 We further propose PFN-Boost, where we instead fuse TabPFN and GBDTs for performance\ngains over GBDTs alone across dataset sizes without using column headers.\n\u2022 We conduct thorough experiments across numerous datasets and sample sizes, comparing to\nstrong baselines. LLM-Boost and PFN-Boost demonstrate consistently strong performance.\n2\n2 Related Work\n2.1 GBDTs for Tabular Data\nGradient boosted decision tree algorithms such as XGBoost [6], Catboost [27] and LightGBM [22]\noffer state-of-the-art or near state-of-the-art performance on many tabular tasks [16]. Compared to\ndeep learning models with similar performance, GBDTs offer faster training and inference speeds\neven without GPUs, are easy to tune, and are more straightforward to interpret. However, when\ncompared to deep learning models, tree based models do not generalize as well to diverse unseen\ndata and are not as robust to uninformative features [16]. Recently, TabPFN [19], a transformer for\ntabular in-context learning has demonstrated superior performance on small datasets [24]. In our\nwork, we adopt GBDTs as a base model due to their ability to benefit from large volumes of data,\nand we augment them with TabPFN and LLMs using boosting.\n2.2 Boosting\nBoosting is an ensembling technique for combining multiple weak learners to form a single strong\nprediction model [13]. Boosting algorithms are sequential processes whereby new learners are progres\u0002sively added to predict the residual error of the current ensemble until the error becomes sufficiently\nsmall. Gradient boosting additionally provides a mechanism to update the new learners using an\narbitrary differentiable loss function via gradient descent [14]. Although there are implementation\ndifferences in the GBDT algorithms mentioned above, they share the fundamental process of makin...",
      "url": "https://arxiv.org/pdf/2502.02672"
    },
    {
      "title": "A Generalized Stacking for Implementing Ensembles of Gradient Boosting Machines",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2010.06026"
    },
    {
      "title": "Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning",
      "text": "<div><div>\n \n \n \n \n \n \n <p><a href=\"/pdf/2105.03791\">Download PDF</a></p><blockquote>\n Abstract: Transfer learning has become the dominant paradigm for many natural language\nprocessing tasks. In addition to models being pretrained on large datasets,\nthey can be further trained on intermediate (supervised) tasks that are similar\nto the target task. For small Natural Language Inference (NLI) datasets,\nlanguage modelling is typically followed by pretraining on a large (labelled)\nNLI dataset before fine-tuning with each NLI subtask. In this work, we explore\nGradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used\nMulti-Layer Perceptron (MLP) classification head. GBDTs have desirable\nproperties such as good performance on dense, numerical features and are\neffective where the ratio of the number of samples w.r.t the number of features\nis low. We then introduce FreeGBDT, a method of fitting a GBDT head on the\nfeatures computed during fine-tuning to increase performance without additional\ncomputation by the neural network. We demonstrate the effectiveness of our\nmethod on several NLI datasets using a strong baseline model (RoBERTa-large\nwith MNLI pretraining). The FreeGBDT shows a consistent improvement over the\nMLP classification head.\n </blockquote>\n \n \n </div><div>\n <h2>Submission history</h2><p> From: Benjamin Minixhofer [<a href=\"/show-email/434c7b96/2105.03791\">view email</a>]\n <br />\n <strong><a href=\"/abs/2105.03791v1\">[v1]</a></strong>\n Sat, 8 May 2021 22:31:51 UTC (382 KB)<br /><strong>[v2]</strong>\nTue, 8 Jun 2021 14:35:26 UTC (386 KB)<br /></p></div>||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:2105.03791\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Computation and Language\n\n arXiv:2105.03791 (cs)\n [Submitted on 8 May 2021 (v1), last revised 8 Jun 2021 (this version, v2)]\n\n Title: Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning\n\n Authors: Benjamin Minixhofer, Milan Gritta, Ignacio Iacobacci\n Download PDF\n Abstract: Transfer learning has become the dominant paradigm for many natural language processing tasks. In addition to models being pretrained on large datasets, they can be further trained on intermediate (supervised) tasks that are similar to the target task. For small Natural Language Inference (NLI) datasets, language modelling is typically followed by pretraining on a large (labelled) NLI dataset before fine-tuning with each NLI subtask. In this work, we explore Gradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used Multi-Layer Perceptron (MLP) classification head. GBDTs have desirable properties such as good performance on dense, numerical features and are effective where the ratio of the number of samples w.r.t the number of features is low. We then introduce FreeGBDT, a method of fitting a GBDT head on the features computed during fine-tuning to increase performance without additional computation by the neural network. We demonstrate the effectiveness of our method on several NLI datasets using a strong baseline model (RoBERTa-large with MNLI pretraining). The FreeGBDT shows a consistent improvement over the MLP classification head.\n Comments: Findings of ACL 2021 \n Subjects: Computation and Language (cs.CL) \n Cite as: arXiv:2105.03791 [cs.CL] \n (or arXiv:2105.03791v2 [cs.CL] for this version)\n https://doi.org/10.48550/arXiv.2105.03791 \n Focus to learn more \n arXiv-issued DOI via DataCite \n https://doi.org/10.18653/v1/2021.findings-acl.26\n Related DOI: Focus to learn more \n DOI(s) linking to related resources \n \n\n Submission history\n\n From: Benjamin Minixhofer [view email]\n [v1] Sat, 8 May 2021 22:31:51 UTC (382 KB)\n [v2] Tue, 8 Jun 2021 14:35:26 UTC (386 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n Current browse context:\n cs.CL\n < prev | next >\n new | recent | 2105\n Change to browse by:\n cs\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Milan Gritta\n Ignacio Iacobacci\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
      "url": "https://arxiv.org/abs/2105.03791"
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.7.0 documentation",
      "text": "<div><div>\n<h2>Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models<a href=\"#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models\">\u00b6</a></h2>\n<p><strong>Tip</strong>: If your data contains images, consider also checking out\n<a href=\"https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal.html#sec-tabularprediction-multimodal\"><span>Multimodal Data Tables: Tabular, Text, and Image</span></a> which handles images in\naddition to text and tabular features.</p>\n<p>Here we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, <strong>raw text data</strong> is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c<span>sec_textprediction_architecture</span>\u201d of\n<span>sec_textprediction_multimodal</span> (used by AutoGluon\u2019s\n<code><span>TextPredictor</span></code>).</p>\n<div><pre><span></span><span>%</span><span>matplotlib</span> <span>inline</span>\n<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>\n<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\n<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>\n<span>import</span> <span>pprint</span>\n<span>import</span> <span>random</span>\n<span>from</span> <span>autogluon.tabular</span> <span>import</span> <span>TabularPredictor</span>\n<span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n<span>random</span><span>.</span><span>seed</span><span>(</span><span>123</span><span>)</span>\n</pre></div>\n<div>\n<h2>Product Sentiment Analysis Dataset<a href=\"#product-sentiment-analysis-dataset\">\u00b6</a></h2>\n<p>We consider the product sentiment analysis dataset from a <a href=\"https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard\">MachineHack\nhackathon</a>.\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).</p>\n<div><pre><span></span>!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n</pre></div>\n<div><pre><span></span>--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.217.110.220, 52.216.137.228, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 673.33K --.-KB/s in 0.009s\n2023-02-22 23:29:24 (73.9 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.10.27, 52.217.225.57, 3.5.11.212, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.10.27|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 73.75K --.-KB/s in 0.001s\n2023-02-22 23:29:24 (57.6 MB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n--2023-02-22 23:29:24-- https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.217.130.97, 52.216.10.27, 52.217.225.57, ...\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.217.130.97|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\nproduct_sentiment_m 100%[===================&gt;] 304.88K --.-KB/s in 0.002s\n2023-02-22 23:29:24 (137 MB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n</pre></div>\n<div><pre><span></span><span>subsample_size</span> <span>=</span> <span>2000</span> <span># for quick demo, try setting to larger values</span>\n<span>feature_columns</span> <span>=</span> <span>[</span><span>'Product_Description'</span><span>,</span> <span>'Product_Type'</span><span>]</span>\n<span>label</span> <span>=</span> <span>'Sentiment'</span>\n<span>train_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/train.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span><span>.</span><span>sample</span><span>(</span><span>2000</span><span>,</span> <span>random_state</span><span>=</span><span>123</span><span>)</span>\n<span>dev_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/dev.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>test_df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>'product_sentiment_machine_hack/test.csv'</span><span>,</span> <span>index_col</span><span>=</span><span>0</span><span>)</span>\n<span>train_df</span> <span>=</span> <span>train_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>dev_df</span> <span>=</span> <span>dev_df</span><span>[</span><span>feature_columns</span> <span>+</span> <span>[</span><span>label</span><span>]]</span>\n<span>test_df</span> <span>=</span> <span>test_df</span><span>[</span><span>feature_columns</span><span>]</span>\n<span>print</span><span>(</span><span>'Number of training samples:'</span><span>,</span> <span>len</span><span>(</span><span>train_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of dev samples:'</span><span>,</span> <span>len</span><span>(</span><span>dev_df</span><span>))</span>\n<span>print</span><span>(</span><span>'Number of test samples:'</span><span>,</span> <span>len</span><span>(</span><span>test_df</span><span>))</span>\n</pre></div>\n<div><pre><span></span><span>Number</span> <span>of</span> <span>training</span> <span>samples</span><span>:</span> <span>2000</span>\n<span>Number</span> <span>of</span> <span>dev</span> <span>samples</span><span>:</span> <span>637</span>\n<span>Number</span> <span>of</span> <span>test</span> <span>samples</span><span>:</span> <span>2728</span>\n</pre></div>\n<p>There are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.</p>\n<div>\n<table>\n <thead>\n <tr>\n <th></th>\n <th>Product_Description</th>\n <th>Product_Type</th>\n <th>Sentiment</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>4532</th>\n <td>they took away the lego pit but ...",
      "url": "https://auto.gluon.ai/0.7.0/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2411.01645"
    },
    {
      "title": "",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/pdf/2410.19889"
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.5.2 documentation",
      "text": "# Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models)\n\n**Tip**: If your data contains images, consider also checking out\n[Multimodal Data Tables: Tabular, Text, and Image](https://auto.gluon.ai/tabular-multimodal.html#sec-tabularprediction-multimodal) which handles images in\naddition to text and tabular features.\n\nHere we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, **raw text data** is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201csec\\_textprediction\\_architecture\u201d of\nsec\\_textprediction\\_multimodal (used by AutoGluon\u2019s\n`TextPredictor`).\n\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nfrom autogluon.tabular import TabularPredictor\n\nnp.random.seed(123)\nrandom.seed(123)\n```\n\n## Product Sentiment Analysis Dataset [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#product-sentiment-analysis-dataset)\n\nWe consider the product sentiment analysis dataset from a [MachineHack\\\nhackathon](https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard).\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).\n\n```\n!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n\n```\n\n```\n--2022-07-28 21:12:16--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.251.108\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.251.108|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 673.33K  --.-KB/s    in 0.01s\n\n2022-07-28 21:12:16 (48.1 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n\n--2022-07-28 21:12:16--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.251.108\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.251.108|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\n\nproduct_sentiment_m 100%[===================>]  73.75K  --.-KB/s    in 0.001s\n\n2022-07-28 21:12:16 (53.1 MB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n\n--2022-07-28 21:12:16--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.251.108\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.251.108|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 304.88K  --.-KB/s    in 0.007s\n\n2022-07-28 21:12:16 (42.4 MB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n\n```\n\n```\nsubsample_size = 2000  # for quick demo, try setting to larger values\nfeature_columns = ['Product_Description', 'Product_Type']\nlabel = 'Sentiment'\n\ntrain_df = pd.read_csv('product_sentiment_machine_hack/train.csv', index_col=0).sample(2000, random_state=123)\ndev_df = pd.read_csv('product_sentiment_machine_hack/dev.csv', index_col=0)\ntest_df = pd.read_csv('product_sentiment_machine_hack/test.csv', index_col=0)\n\ntrain_df = train_df[feature_columns + [label]]\ndev_df = dev_df[feature_columns + [label]]\ntest_df = test_df[feature_columns]\nprint('Number of training samples:', len(train_df))\nprint('Number of dev samples:', len(dev_df))\nprint('Number of test samples:', len(test_df))\n```\n\n```\nNumber of training samples: 2000\nNumber of dev samples: 637\nNumber of test samples: 2728\n```\n\nThere are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.\n\n```\ntrain_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 4532 | they took away the lego pit but replaced it wi... | 0 | 1 |\n| 1831 | #Apple to Open Pop-Up Shop at #SXSW \\[REPORT\\]: ... | 9 | 2 |\n| 3536 | RT @mention False Alarm: Google Circles Not Co... | 5 | 1 |\n| 5157 | Will Google reveal a new social network called... | 9 | 2 |\n| 4643 | Niceness RT @mention Less than 2 hours until w... | 6 | 3 |\n\n```\ndev_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 3170 | Do it. RT @mention Come party w/ Google tonigh... | 3 | 3 |\n| 6301 | Line for iPads at #SXSW. Doesn't look too bad!... | 6 | 3 |\n| 5643 | First up: iPad Design Headaches (2 Tablets, Ca... | 6 | 2 |\n| 1953 | #SXSW: Mint Talks Mobile App Development Chall... | 9 | 2 |\n| 2658 | \u0089\u00db\u00cf@mention Apple store downtown Austin open t... | 9 | 2 |\n\n```\ntest_df.head()\n```\n\n| Product\\_Description | Product\\_Type |\n| --- | --- |\n| Text\\_ID |\n| --- |\n| 5786 | RT @mention Going to #SXSW? The new iPhone gui... | 7 |\n| 5363 | RT @mention 95% of iPhone and Droid apps have ... | 9 |\n| 6716 | RT @mention Thank you to @mention for letting ... | 9 |\n| 4339 | #Thanks @mention we're lovin' the @mention app... | 7 |\n| 66 | At #sxsw? @mention / @mention wanna buy you a ... | 9 |\n\n## AutoGluon Tabular with Multimodal Support [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#autogluon-tabular-with-multimodal-support)\n\nTo utilize the `TextPredictor` model inside of `TabularPredictor`,\nwe must specify the `hyperparameters = 'multimodal'` in AutoGluon\nTabular. Internally, this will train multiple tabular models as well as\nthe TextPredictor model, and then combine them via either a weighted\nensemble or stack ensemble, as explained in [AutoGluon Tabular\\\nPaper](https://arxiv.org/pdf/2003.06505.pdf). If you do not specify\n`hyperparameters = 'multimodal'`, then AutoGluon Tabular will simply\nfeaturize text fields using N-grams and train only tabular models (which\nmay work better if your text is mostly uncommon strings/vocabulary).\n\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\npredictor.fit(train_df, hyperparameters='multimodal')\n```\n\n```\nBeginning AutoGluon training ...\nAutoGluon will save models to \"ag_tabular_product_sentiment_multimodal/\"\nAutoGluon Version:  0.5.2b20220728\nPython Version:     3.8.10\nOperating System:   Linux\nTrain Data Rows:    2000\nTrain Data Columns: 2\nLabel Column: Sentiment\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int...",
      "url": "https://auto.gluon.ai/0.5.2/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    },
    {
      "title": "Combining BERT/Transformers and Classical Tabular Models \u2014 AutoGluon Documentation 0.1.1 documentation",
      "text": "# Multimodal Data Tables: Combining BERT/Transformers and Classical Tabular Models [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#multimodal-data-tables-combining-bert-transformers-and-classical-tabular-models)\n\nHere we introduce how to use AutoGluon Tabular to deal with multimodal\ntabular data that contains text, numeric, and categorical columns. In\nAutoGluon, **raw text data** is considered as a first-class citizen of\ndata tables. AutoGluon Tabular can help you train and combine a diverse\nset of models including classical tabular models like\nLightGBM/RF/CatBoost as well as our pretrained NLP model based\nmultimodal network that is introduced in Section\n\u201c [What\u2019s happening inside?](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-architecture)\u201d of\n[Text Prediction - Multimodal Table with Text](https://auto.gluon.ai/text_prediction/multimodal_text.html#sec-textprediction-multimodal) (used by AutoGluon\u2019s\n`TextPredictor`).\n\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nfrom autogluon.tabular import TabularPredictor\nimport mxnet as mx\n\nnp.random.seed(123)\nrandom.seed(123)\nmx.random.seed(123)\n```\n\n## Product Sentiment Analysis Dataset [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#product-sentiment-analysis-dataset)\n\nWe consider the product sentiment analysis dataset from a [MachineHack\\\nhackathon](https://www.machinehack.com/hackathons/product_sentiment_classification_weekend_hackathon_19/leaderboard).\nThe goal is to predict a user\u2019s sentiment towards a product given their\nreview (raw text) and a categorical feature indicating the product\u2019s\ntype (e.g., Tablet, Mobile, etc.). We have already split the original\ndataset to be 90% for training and 10% for development/testing (if\nsubmitting your models to the hackathon, we recommend training them on\n100% of the dataset).\n\n```\n!mkdir -p product_sentiment_machine_hack\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv -O product_sentiment_machine_hack/train.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv -O product_sentiment_machine_hack/dev.csv\n!wget https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv -O product_sentiment_machine_hack/test.csv\n\n```\n\n```\n--2021-03-10 04:16:17--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/train.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689486 (673K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/train.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 673.33K  2.05MB/s    in 0.3s\n\n2021-03-10 04:16:18 (2.05 MB/s) - \u2018product_sentiment_machine_hack/train.csv\u2019 saved [689486/689486]\n\n--2021-03-10 04:16:18--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/dev.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75517 (74K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/dev.csv\u2019\n\nproduct_sentiment_m 100%[===================>]  73.75K   490KB/s    in 0.2s\n\n2021-03-10 04:16:19 (490 KB/s) - \u2018product_sentiment_machine_hack/dev.csv\u2019 saved [75517/75517]\n\n--2021-03-10 04:16:19--  https://autogluon-text-data.s3.amazonaws.com/multimodal_text/machine_hack_product_sentiment/test.csv\nResolving autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)... 52.216.147.43\nConnecting to autogluon-text-data.s3.amazonaws.com (autogluon-text-data.s3.amazonaws.com)|52.216.147.43|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 312194 (305K) [text/csv]\nSaving to: \u2018product_sentiment_machine_hack/test.csv\u2019\n\nproduct_sentiment_m 100%[===================>] 304.88K   943KB/s    in 0.3s\n\n2021-03-10 04:16:20 (943 KB/s) - \u2018product_sentiment_machine_hack/test.csv\u2019 saved [312194/312194]\n\n```\n\n```\nsubsample_size = 2000  # for quick demo, try setting to larger values\nfeature_columns = ['Product_Description', 'Product_Type']\nlabel = 'Sentiment'\n\ntrain_df = pd.read_csv('product_sentiment_machine_hack/train.csv', index_col=0).sample(2000, random_state=123)\ndev_df = pd.read_csv('product_sentiment_machine_hack/dev.csv', index_col=0)\ntest_df = pd.read_csv('product_sentiment_machine_hack/test.csv', index_col=0)\n\ntrain_df = train_df[feature_columns + [label]]\ndev_df = dev_df[feature_columns + [label]]\ntest_df = test_df[feature_columns]\nprint('Number of training samples:', len(train_df))\nprint('Number of dev samples:', len(dev_df))\nprint('Number of test samples:', len(test_df))\n```\n\n```\nNumber of training samples: 2000\nNumber of dev samples: 637\nNumber of test samples: 2728\n```\n\nThere are two features in the dataset: the users\u2019 review of the product\nand the product\u2019s type, and four possible classes to predict.\n\n```\ntrain_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 4532 | they took away the lego pit but replaced it wi... | 0 | 1 |\n| 1831 | #Apple to Open Pop-Up Shop at #SXSW \\[REPORT\\]: ... | 9 | 2 |\n| 3536 | RT @mention False Alarm: Google Circles Not Co... | 5 | 1 |\n| 5157 | Will Google reveal a new social network called... | 9 | 2 |\n| 4643 | Niceness RT @mention Less than 2 hours until w... | 6 | 3 |\n\n```\ndev_df.head()\n```\n\n| Product\\_Description | Product\\_Type | Sentiment |\n| --- | --- | --- |\n| 3170 | Do it. RT @mention Come party w/ Google tonigh... | 3 | 3 |\n| 6301 | Line for iPads at #SXSW. Doesn't look too bad!... | 6 | 3 |\n| 5643 | First up: iPad Design Headaches (2 Tablets, Ca... | 6 | 2 |\n| 1953 | #SXSW: Mint Talks Mobile App Development Chall... | 9 | 2 |\n| 2658 | \u0089\u00db\u00cf@mention Apple store downtown Austin open t... | 9 | 2 |\n\n```\ntest_df.head()\n```\n\n| Product\\_Description | Product\\_Type |\n| --- | --- |\n| Text\\_ID |\n| --- |\n| 5786 | RT @mention Going to #SXSW? The new iPhone gui... | 7 |\n| 5363 | RT @mention 95% of iPhone and Droid apps have ... | 9 |\n| 6716 | RT @mention Thank you to @mention for letting ... | 9 |\n| 4339 | #Thanks @mention we're lovin' the @mention app... | 7 |\n| 66 | At #sxsw? @mention / @mention wanna buy you a ... | 9 |\n\n## AutoGluon Tabular with Multimodal Support [\u00b6](https://auto.gluon.ai/auto.gluon.ai\\#autogluon-tabular-with-multimodal-support)\n\nTo utilize the `TextPredictor` model inside of `TabularPredictor`,\nwe must specify the `hyperparameters = 'multimodal'` in AutoGluon\nTabular. Internally, this will train multiple tabular models as well as\nthe TextPredictor model, and then combine them via either a weighted\nensemble or stack ensemble, as explained in [AutoGluon Tabular\\\nPaper](https://arxiv.org/pdf/2003.06505.pdf). If you do not specify\n`hyperparameters = 'multimodal'`, then AutoGluon Tabular will simply\nfeaturize text fields using N-grams and train only tabular models (which\nmay work better if your text is mostly uncommon strings/vocabulary).\n\n```\nfrom autogluon.tabular import TabularPredictor\npredictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\npredictor.fit(train_df, hyperparameters='multimodal')\n```\n\n```\nBeginning AutoGluon training ...\nAutoGluon will save models to \"ag_tabular_product_sentiment_multimodal/\"\nAutoGluon Version:  0.1.1b20210310\nTrain Data Rows:    2000\nTrain Data Columns: 2\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n    4 unique label values:  [1, 2, 3, 0]\n    If 'multiclass' is not the correct ...",
      "url": "https://auto.gluon.ai/scoredebugweight/tutorials/tabular_prediction/tabular-multimodal-text-others.html"
    }
  ]
}