{
  "query": "How do Kaggle winners handle 75/25 class imbalance in binary classification for AUC-ROC optimization?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the methods described in recent research papers and Kaggle\u2011focused tutorials, top Kaggle competitors typically combine loss\u2011function tricks, balanced sampling, and careful validation to squeeze the most AUC\u2011ROC out of a 75\u202f%\u202f/\u202f25\u202f% imbalanced binary problem.\u202f([arxiv.org](https://arxiv.org/html/2402.05400v2), [arxiv.org](https://arxiv.org/abs/2203.14177), [amueller.github.io](https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html), [stats.stackexchange.com](https://stats.stackexchange.com/questions/260164/auc-and-class-imbalance-in-training-test-dataset), [neptune.ai](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions))\n\n1. **Pick an AUROC\u2011optimizing loss** \u2013 instead of the standard log\u2011loss, use a loss that directly targets the ROC curve (e.g., AUC\u2011maximizing loss or the \u201cLoss Conditional Training\u201d family). This makes the model less sensitive to the exact class ratio while improving the shape of the ROC curve.\u202f([arxiv.org](https://arxiv.org/html/2402.05400v2), [arxiv.org](https://arxiv.org/abs/2203.14177))\n\n2. **Weight or resample the minority class** \u2013 apply class\u2011weights (e.g., `scale_pos_weight = (N\u2011neg / N\u2011pos)`) or use balanced/stratified mini\u2011batches so the 25\u202f% positive class is seen as often as the 75\u202f% negative class during training.\u202f([amueller.github.io](https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html))\n\n3. **Use stratified K\u2011fold cross\u2011validation** \u2013 split the data with stratification to preserve the 75/25 ratio in every fold. This gives reliable AUROC estimates because AUC is insensitive to test\u2011set imbalance but does depend on the training distribution.\u202f([stats.stackexchange.com](https://stats.stackexchange.com/questions/260164/auc-and-class-imbalance-in-training-test-dataset))\n\n4. **Train multiple strong tabular models** (e.g., LightGBM, CatBoost, XGBoost) with the AUROC loss and class\u2011weighting, then average their predicted probabilities. Ensembling is a recurring trick among Kaggle winners for boosting AUC.\u202f([neptune.ai](https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions))\n\n5. **Calibrate and evaluate** \u2013 optionally apply probability calibration (Platt scaling or isotonic regression) on a held\u2011out validation split, then compute the final AUC\u2011ROC. Calibration helps ensure the predicted scores reflect true ranking performance and avoids over\u2011optimistic scores.\u202f([arxiv.org](https://arxiv.org/html/2402.05400v2))\n\nFollowing these five steps\u2014AUROC\u2011specific loss, class\u2011balanced training, stratified validation, model ensembling, and final calibration\u2014mirrors the workflow that Kaggle competition winners use to handle a 75\u202f%\u202f/\u202f25\u202f% class imbalance while maximizing AUC\u2011ROC.",
      "url": ""
    },
    {
      "title": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions",
      "text": "Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\n# Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions\nKelsey Lieberman, Shuai Yuan, Swarna Kamlam Ravindran &amp; Carlo Tomasi\nDepartment of Computer Science\nDuke University\nDurham, NC 27708 USA\n###### Abstract\nAlthough binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices. Code is available at[https://github.com/klieberman/roc\\_lct](https://github.com/klieberman/roc_lct).\n## 1Introduction\nConsider a classifier which takes images of skin lesions and predicts whether the lesions are melanoma> Rotemberg et\u00a0al. (\n[> 2020\n](https://arxiv.org/html/2402.05400v2#bib.bib16)> )\n. Such a system could be especially valuable in underdeveloped countries where expert resources for diagnosis are scarce> Cassidy et\u00a0al. (\n[> 2022\n](https://arxiv.org/html/2402.05400v2#bib.bib3)> )\n. Classifying melanoma from images is a problem with class imbalance since benign lesions are far more common than melanomas. Furthermore, the accuracy on the melanoma (minority) class is much more important than the accuracy on the benign (majority) class because predicting a benign lesion as melanoma would result in the cost of a biopsy while predicting a melanoma lesion as benign could result in the melanoma spreading before the patient can receive appropriate treatment.\nIn this case, overall accuracy, even on a balanced test set, is clearly an inadequate metric, as it implies that the accuracies on both classes are equally important. Instead, Receiver Operating Characteristic (ROC) curves are better suited for such problems> Egan (\n[> 1975\n](https://arxiv.org/html/2402.05400v2#bib.bib7)> )\n. These curves plot the tradeoff between the true positive rate (TPR) on the y-axis and the false positive rate (FPR) on the x-axis over a range of classification thresholds. Unlike scalar metrics (e.g., overall accuracy on a balanced test set orF\u03b2subscript\ud835\udc39\ud835\udefdF\\_{\\\\beta}italic\\_F start\\_POSTSUBSCRIPT italic\\_\u03b2 end\\_POSTSUBSCRIPT), ROC curves show model performance over a wide range of classification thresholds. This allows practitioners to understand how the model\u2019s performance changes based on different classification thresholds and choose the best tradeoff for their needs. ROC curves can also be summarized by their Area Under the Curve (AUC)> Egan (\n[> 1975\n](https://arxiv.org/html/2402.05400v2#bib.bib7)> )\n. Furthermore, both ROC curves and AUC have several mathematical properties which make them preferred to alternative precision-recall curves> Flach &amp; Kull (\n[> 2015\n](https://arxiv.org/html/2402.05400v2#bib.bib9)> )\n111We provide definitions and visualizations of commonly-used metrics for binary problems with imbalanced data in Appendix[A](https://arxiv.org/html/2402.05400v2#A1).\nAlthough binary problems, like melanoma classification, are often cited as the motivation for class imbalance problems and ROC curves are thede factometric of choice for such problems, the class imbalance literature largely focuses on improving performance on longtailed multi-class datasets in terms of overall accuracy on a balanced test set. We instead focus on binary problems with severe imbalance and propose a method, which adapts existing techniques for handling class imbalance, to optimize for ROC curves in these binary scenarios.\n![Refer to caption](extracted/5641151/figures/Melanoma_auc_boxplots.png)Figure 1:Distribution of Area Under the ROC Curve (AUC) values obtained by training the same model on the SIIM-ISIC Melanoma classification dataset with 48 different combinations of hyperparameters on VS Loss (hyperparameter values are given in Section[6](https://arxiv.org/html/2402.05400v2#S6)). Results are shown at three different imbalance ratios.As the imbalance becomes more severe, model performance drops and the variance in performance drastically increases. LCT addresses both of these issues by training over a family of loss functions, instead of a single loss function with one combination of hyperparameter values.\nIn particular, we adapt Vector Scaling (VS) loss, which is a general loss function for imbalanced learning with strong theoretical backing> Kini et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2402.05400v2#bib.bib11)> )\n. VS loss is a modification of Cross-entropy loss that adjusts the logits via additive and multiplicative factors. There is theory supporting the use of both of these factors: multiplicative factors are essential for the terminal phase of training, but these have negative effects early during training, so additive factors are necessary speed up convergence. Although VS loss has shown strong performance in the multi-class setting, it does have hyperparameters which require tuning (e.g., the additive and multiplicative factors on the loss function).\nWe find that, in the binary case, the effect of these hyperparameters is small and reasonable at moderate imbalance ratios\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2(where\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2is defined as the ratio of majority to minority samples); however, at large imbalance ratios, small differences in these hyperparameters lead to very wide variance in the results (Figure[1](https://arxiv.org/html/2402.05400v2#S1.F1)). This figure shows that increasing the imbalance ratio not only decreases the AUCs (as expected), but also drastically increases the variance in AUCs obtained by training models with slightly different hyperparameter values.\nIn this work, we highlight the practical effect of the theoretically-motivated VS loss on the ROC metric, especially for data problems with high imbalance ratios. We propose a method that adapts VS loss to align the training objective more closely with ROC curve optimization. Our method trains a single model on a wide range of hyperparameter values using Loss Conditional Training> Dosovitskiy &amp; Djolonga (\n[> 2020\n](https://arxiv.org/html/2402.05400v2#bib.bib5)> )\n. We find that this method not only reduces the variance in model performance caused by hyperparameter choices, but also improves performance over the best hyperparameter choices since it optimizes for many tradeoffs on the ROC curve. We provide extensive results\u2013 both on CIFAR datasets and datasets of real applications derived from Kaggle competitions\u2013 at multiple imbalance ratios and across a wide range of hyperparameter choices.\nIn summary, our contributions are as follows.\n* \u2022We identify that higher levels of imbalance are not only associated with worse model performance, but also more variance.\n* \u2022We recognize that training over a range of hyperparameter values can actually benefit classification models that are otherwise prone to overfitting to a single loss function.\n* \u2022We propose using Loss Conditional Training (LCT) to improve the training regimen for classification models trained under imbalance.\n* \u2022We show that this method consistently improves performance at high imbalance ratios.\n## 2Related work\nMany solutions have been proposed...",
      "url": "https://arxiv.org/html/2402.05400v2"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2203.14177] Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2203.14177\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2203.14177**(cs)\n[Submitted on 27 Mar 2022 ([v1](https://arxiv.org/abs/2203.14177v1)), last revised 4 Jul 2022 (this version, v3)]\n# Title:Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices\nAuthors:[Dixian Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+D),[Xiaodong Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+X),[Tianbao Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+T)\nView a PDF of the paper titled Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices, by Dixian Zhu and 2 other authors\n[View PDF](https://arxiv.org/pdf/2203.14177)> > Abstract:\n> The area under the ROC curve (AUROC) has been vigorously applied for imbalanced classification and moreover combined with deep learning techniques. However, there is no existing work that provides sound information for peers to choose appropriate deep AUROC maximization techniques. In this work, we fill this gap from three aspects. (i) We benchmark a variety of loss functions with different algorithmic choices for deep AUROC optimization problem. We study the loss functions in two categories: pairwise loss and composite loss, which includes a total of 10 loss functions. Interestingly, we find composite loss, as an innovative loss function class, shows more competitive performance than pairwise loss from both training convergence and testing generalization perspectives. Nevertheless, data with more corrupted labels favors a pairwise symmetric loss. (ii) Moreover, we benchmark and highlight the essential algorithmic choices such as positive sampling rate, regularization, normalization/activation, and optimizers. Key findings include: higher positive sampling rate is likely to be beneficial for deep AUROC maximization; different datasets favors different weights of regularizations; appropriate normalization techniques, such as sigmoid and $\\ell_2$ score normalization, could improve model performance. (iii) For optimization aspect, we benchmark SGD-type, Momentum-type, and Adam-type optimizers for both pairwise and composite loss. Our findings show that although Adam-type method is more competitive from training perspective, but it does not outperform others from testing perspective. Comments:|32 pages|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2203.14177](https://arxiv.org/abs/2203.14177)[cs.LG]|\n|(or[arXiv:2203.14177v3](https://arxiv.org/abs/2203.14177v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2203.14177](https://doi.org/10.48550/arXiv.2203.14177)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Dixian Zhu [[view email](https://arxiv.org/show-email/be468da8/2203.14177)]\n**[[v1]](https://arxiv.org/abs/2203.14177v1)**Sun, 27 Mar 2022 00:47:00 UTC (558 KB)\n**[[v2]](https://arxiv.org/abs/2203.14177v2)**Tue, 29 Mar 2022 03:39:26 UTC (1,110 KB)\n**[v3]**Mon, 4 Jul 2022 18:49:54 UTC (1,114 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices, by Dixian Zhu and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2203.14177)\n* [TeX Source](https://arxiv.org/src/2203.14177)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2203.14177&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2203.14177&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-03](https://arxiv.org/list/cs.LG/2022-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2203.14177?context=cs)\n[cs.AI](https://arxiv.org/abs/2203.14177?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.14177)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.14177)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.14177)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2203.14177&amp;description=Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2203.14177&amp;title=Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individu...",
      "url": "https://arxiv.org/abs/2203.14177"
    },
    {
      "title": "Working with highly imbalanced data \u2014 Applied Machine Learning in Python",
      "text": "[**Applied Machine Learning in Python**](https://amueller.github.io/index.html)\n\n[.ipynb](https://amueller.github.io/_sources/05-advanced-topics/11-imbalanced-datasets.ipynb).pdf\n\nOn this page\n\n# Working with highly imbalanced data [\u00b6](https://amueller.github.io/amueller.github.io\\#working-with-highly-imbalanced-data)\n\n## Recap on imbalanced data [\u00b6](https://amueller.github.io/amueller.github.io\\#recap-on-imbalanced-data)\n\n## Two sources of imbalance [\u00b6](https://amueller.github.io/amueller.github.io\\#two-sources-of-imbalance)\n\n- Asymmetric cost\n\n- Asymmetric data\n\n\nIn general, there\u2019s are two ways in which a classification\ntask can be imbalanced. First one is asymmetric costs. Even\nif the probability of class 0 and class 1 are the same, they\nmight be different like in business costs, or health costs,\nor any other kind of cost or benefit associated with making\ndifferent kinds of mistakes. The second one is having\nasymmetrical data. Meaning that one class is much more\ncommon than the other class.\n\n## Why do we care? [\u00b6](https://amueller.github.io/amueller.github.io\\#why-do-we-care)\n\n- Why should cost be symmetric?\n\n- All data is imbalanced\n\n- Detect rare events\n\n\nOne of these two is true in basically all real world\napplications. Usually, both of them are true. There\u2019s no\nreason why a false positive and a false negative should have\nthe same business cost, they\u2019re usually very, very different\nthings, no matter whether you do ad-click prediction or\nwhether you do health, the two kinds of mistakes are usually\nquite different, and have quite different real world\nconsequences.\n\nAlso, data is always imbalanced, and often very drastically.\nIn particular if you do diagnosis, or if you do ad clicks-or\nmarketing\u2026.For ad-clicks, I think it\u2019s like below 0.01% of\nads I clicked on, depending on how good doing with your\ntargeting. So very often, we have very few positives. And so\nthis is really a topic that is basically all of\nclassification. So balance classification with balance\ncosts, is not really something that happens a lot in the\nreal world.\n\n## Changing Thresholds [\u00b6](https://amueller.github.io/amueller.github.io\\#changing-thresholds)\n\n.tiny-code\\[\n\n```\ndata = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\n\nlr = LogisticRegression().fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nclassification_report(y_test, y_pred)\n```\n\n```\nprecision   recall  f1-score  support\n0              0.91     0.92      0.92       53\n1              0.96     0.94      0.95       90\navg/total      0.94     0.94      0.94      143\n```\n\n```\ny_pred = lr.predict_proba(X_test)[:, 1] > .85\n\nclassification_report(y_test, y_pred)\n```\n\n```\nprecision   recall  f1-score  support\n0              0.84     1.00      0.91       53\n1              1.00     0.89      0.94       90\navg/total      0.94     0.93      0.93      143\n```\n\n\\]\n\nSo apart from evaluation we talked about one way we could\u2019ve\nchanged the outcome to take into account which was changing\nthe threshold of greater probability. So not only taking\ninto account the predicted class. Assume I have a logistic\nregression model and I can either use the predict method\nwhich basically makes the cut off at 0.5 probability of the\npositive class, then I can look at the classification\nreport, which will tell me precision and recall for both the\npositive and the negative class. But if I want to increase\nrecall for class 0 or increase precision for class 1, I can\nsay only predict things as class 1 where the estimated\nprobability of class 1 is 0.85. And then I will have only\nthe ones that I\u2019m very certain predicted as class 1.\n\nIf you have given actual a cost function of how much each\nmistake costs, you can optimize this threshold.\n\nFIXME new classification report!!\n\n## Roc Curve [\u00b6](https://amueller.github.io/amueller.github.io\\#roc-curve)\n\n.center\\[\n\\]\n\nWe also looked at ROC curves, which basically look at all\npossible thresholds as you can apply. Either for\nprobabilistic prediction, or for any sort of continuous\nuncertainty estimate.\n\n### Remedies for the model [\u00b6](https://amueller.github.io/amueller.github.io\\#remedies-for-the-model)\n\nToday, I really want to talk about more, how we can change\nthe model more than just changing the threshold. So how can\nwe change the building of the model so that it takes into\naccount the asymmetric costs, or asymmetric data.\n\n## Mammography Data [\u00b6](https://amueller.github.io/amueller.github.io\\#mammography-data)\n\n.smallest\\[\n.left-column\\[\n\n```\nfrom sklearn.datasets import fetch_openml\n## mammography https://www.openml.org/d/310\ndata = fetch_openml('mammography', as_frame=True)\nX, y = data.data, data.target\nX.shape\n```\n\n(11183, 6)\n\n```\ny.value_counts()\n```\n\n```\n-1    10923\n1       260\n```\n\n\\]\n.right-column\\[\n.center\\[\n\\]\n\\]\n.reset-column\\[\n\n```\n## make y boolean\n## this allows sklearn to determine the positive class more easily\nX_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n```\n\n\\]\n\\]\n\nI use this mammography data set, which is very imbalanced.\nThis is a data set that has many samples, only six features\nand it\u2019s very imbalanced.\n\nThe datasets are about mammography data, and whether there\nare calcium deposits in the breast. They are often mistaken\nfor cancer, which is why it\u2019s good to detect them. Since its\nrigidly low dimensional, we can do a scatter plot. And we\ncan see that these are much skewed distributions and there\u2019s\nreally a lot more of one class than the other.\n\n## Mammography Data [\u00b6](https://amueller.github.io/amueller.github.io\\#id1)\n\n.smaller\\[\n\n```\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\n\nscores = cross_validate(LogisticRegression(),\n                        X_train, y_train, cv=10,\n                        scoring=('roc_auc', 'average_precision'))\nscores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n```\n\n0.920, 0.630\n\n```\nfrom sklearn.ensemble import RandomForestClassifier\nscores = cross_validate(RandomForestClassifier(),\n                        X_train, y_train, cv=10,\n                        scoring=('roc_auc', 'average_precision'))\nscores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n```\n\n0.939, 0.722\n\\]\n\nSo as a baseline here is just evaluating logistic regression\nand the random forest on this. Actually, I ran it under ROC\ncurve and average precision. I\u2019ve used a cross-validate\nfunction model selection that allows you to specify multiple\nmetrics. So I only need to train the model once but I can\nlook at multiple metrics.\n\nI do a 10 fold cross-validation on the data set and I split\ninto training and test and so I can look at the scores here.\nThe scores are dictionary, they give me training and test\nscores for all the metrics I specified. And so you can look\nat the mean test drug score and the mean test average\nprecision score. This gives a high AUC and a quite low\naverage precision.\n\nHere is a second baseline with a random forest doing the\nsame evaluation with ROC AUC and average precision. We get\nslightly higher AUC and quite a bit higher average\nprecision.\n\n## Basic Approaches [\u00b6](https://amueller.github.io/amueller.github.io\\#basic-approaches)\n\n.left-column\\[\n.center\\[\n\\]\n\\]\n\n.right-column\\[\n\nChange the training procedure\n\\]\n\nNow we want to change these basic training methods to be\nbetter adapted to this imbalanced dataset. There are\ngenerally two approaches. One is changing the data. And the\nother is change the training procedure and how you built the\nmodel. The easier one is to change the data. We can either\nadd samples to the data, we can remove samples to the data,\nor we can do both. Resampling is not possible in\nscikit-learn because of some API issues.\n\n## Sckit-learn vs resampling [\u00b6](https://amueller.github.io/amueller.github.io\\#sckit-learn-vs-resampling)\n\nThe problem with pipelines, as they\u2019re in scikit-learn right\nnow is, if you create a pipeline, and you call fit,...",
      "url": "https://amueller.github.io/aml/05-advanced-topics/11-imbalanced-datasets.html"
    },
    {
      "title": "AUC and class imbalance in training/test dataset",
      "text": "**Stack Internal**\n\nKnowledge at work\n\nBring the best of human thought and AI automation together at your work.\n\n[Explore Stack Internal](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stats-community&utm_campaign=side-bar&utm_content=explore-teams-compact-popover)\n\n# [AUC and class imbalance in training/test dataset](https://stats.stackexchange.com/questions/260164/auc-and-class-imbalance-in-training-test-dataset)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked8 years, 10 months ago\n\nModified [2 years, 6 months ago](https://stats.stackexchange.com/stats.stackexchange.com?lastactivity)\n\nViewed\n62k times\n\n29\n\n$\\\\begingroup$\n\nI just start to learn the Area under the ROC curve (AUC). I am told that AUC is not reflected by data imbalance. I think it means that AUC is insensitive to imbalance in test data, rather than imbalance in training data.\n\nIn other words, only changing the distribution of positive and negative classes in the **test** data, the AUC value may not change much. But if we change the distribution in the **training** data, the AUC value may largely change. The reason is that the classifier cannot be learned well. In this case, we have to use undersampling and oversampling. Am I right? I just want to make sure my understanding on AUC is correct.\n\n- [model-evaluation](https://stats.stackexchange.com/questions/tagged/model-evaluation)\n- [roc](https://stats.stackexchange.com/questions/tagged/roc)\n- [auc](https://stats.stackexchange.com/questions/tagged/auc)\n\n[Share](https://stats.stackexchange.com/q/260164)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/260164/edit)\n\nFollow\n\n[edited Jul 31, 2021 at 16:13](https://stats.stackexchange.com/posts/260164/revisions)\n\n[Pavel Fedotov](https://stats.stackexchange.com/users/301448/pavel-fedotov)\n\n11566 bronze badges\n\nasked Feb 6, 2017 at 1:19\n\n[Munichong](https://stats.stackexchange.com/users/35802/munichong)\n\n2,12533 gold badges2121 silver badges2929 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Yes, I also read that @Azim PR is less sensitive to imbalanced datasets here is the paper [journals.plos.org/plosone/article?id=10.1371/\u2026](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)$\\\\endgroup$\n\n\n\nRae Wallace\n\n\u2013\n[Rae Wallace](https://stats.stackexchange.com/users/289976/rae-wallace)\n\n\n\n2020-06-30 00:33:00 +00:00\n\nCommentedJun 30, 2020 at 0:33\n\n\n[Add a comment](https://stats.stackexchange.com/stats.stackexchange.com)\u00a0\\|\n\n## 4 Answers 4\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/260164/auc-and-class-imbalance-in-training-test-dataset?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n19\n\n$\\\\begingroup$\n\nIt depends how you mean the word sensitive. The ROC AUC is sensitive to class imbalance in the sense that when there is a minority class, you typically define this as the positive class and it will have a strong impact on the AUC value. This is very much desirable behaviour. Accuracy is for example not sensitive in that way. It can be very high even if the minority class is not well predicted at all.\n\nIn most experimental setups (bootstrap or cross validation for example) the class distribution of training and test sets should be similar. But this is a result of how you sample those sets, not of using or not using ROC. Basically you are right to say that the ROC makes abstraction of class imbalance in the test set by giving equal importance to sensitivity and specificity. When the training set doesn't contain enough examples to learn the class, this will still affect ROC though, as it should.\n\nWhat you do in terms of oversampling and parameter tuning is a separate issue. The ROC can only ever tell you how well a specific configuration works. You can then try multiple config's and select the best.\n\n[Share](https://stats.stackexchange.com/a/260237)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/260237/edit)\n\nFollow\n\nanswered Feb 6, 2017 at 13:31\n\n[David Ernst](https://stats.stackexchange.com/users/134978/david-ernst)\n\n3,2691212 silver badges1616 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- 3\n\n$\\\\begingroup$In binary classification, do we usually distinguish \"AUC for positive class\" and \"AUC for negative class\"? OR there is only one AUC? I think it depends on whether \"default\" AUC calculated by TPR and NPR can reflect the ability of identifying both positive and negative classes OR only positive class.$\\\\endgroup$\n\n\n\nMunichong\n\n\u2013\n[Munichong](https://stats.stackexchange.com/users/35802/munichong)\n\n\n\n2017-02-06 14:01:07 +00:00\n\nCommentedFeb 6, 2017 at 14:01\n\n- $\\\\begingroup$That auc would look at the positive class. Nothing prevents you from computing and with respect to both classes and averaging it. But most of the time, your application scenario makes it clear which one is the positive class.$\\\\endgroup$\n\n\n\nDavid Ernst\n\n\u2013\n[David Ernst](https://stats.stackexchange.com/users/134978/david-ernst)\n\n\n\n2017-02-06 14:12:48 +00:00\n\nCommentedFeb 6, 2017 at 14:12\n\n- $\\\\begingroup$Thanks. I tried some experiments. But I get confused on calculating AUC for class 0: y\\_true=\\[1,0\\], y\\_pred=\\[0.9, 0.8\\], I use the sklearn.metrics.auc function to compute AUC. The result is 1.0. I assume it only reflects how the classifier identifies class1. I then switch 1 and 0 in y\\_true: y\\_true=\\[0,1\\], y\\_pred=\\[0.9, 0.8\\], The result is 0.0. This is how the classifier identifies class0. Am I right?$\\\\endgroup$\n\n\n\nMunichong\n\n\u2013\n[Munichong](https://stats.stackexchange.com/users/35802/munichong)\n\n\n\n2017-02-06 15:57:05 +00:00\n\nCommentedFeb 6, 2017 at 15:57\n\n- $\\\\begingroup$Trying different inputs, I also find that the sum of the two results is always 1. Am I right?$\\\\endgroup$\n\n\n\nMunichong\n\n\u2013\n[Munichong](https://stats.stackexchange.com/users/35802/munichong)\n\n\n\n2017-02-06 16:02:35 +00:00\n\nCommentedFeb 6, 2017 at 16:02\n\n- $\\\\begingroup$Roc is based on multiple confusion matrices based on different cutoffs. Read more theory till you understand what this means. I'm not familiar with scikit learn and its syntax. No the sum of two aucs with regard to both classes doesn't need to be 1.$\\\\endgroup$\n\n\n\nDavid Ernst\n\n\u2013\n[David Ernst](https://stats.stackexchange.com/users/134978/david-ernst)\n\n\n\n2017-02-06 16:03:19 +00:00\n\nCommentedFeb 6, 2017 at 16:03\n\n\n[Add a comment](https://stats.stackexchange.com/stats.stackexchange.com)\u00a0\\|\n\n15\n\n$\\\\begingroup$\n\n(a 3-years late answer, but maybe still useful!)\n\n**ROC** is sensitive to the class-imbalance issue, meaning that it favors the class with larger population solely because of its higher population. In other words, it is biased toward the larger population when it comes to classification/prediction.\n\nThis is indeed problematic. Imagine in different trials when data go under rounds of sampling (e.g., in cross validation), populations of subclasses may vary in each iteration. In such a case, the trained models are no longer comparable using a sensitive metric (like accuracy or ROC). To remedy this, either the number of each subclass should be kept fixed, or an insensitive metric must be used. _True Skill Statistic_ (also known as _Youden J Index_) is a metric that is indeed insensitive to this issue. These metrics are very popular in the domains which deal with extreme-imbalanced data, such as weather forecasting, fraud detection, and of course in bioinformatics.\n\nAlso, people modified ROC and introduced Precision-Recall curve for this very reason. PR curve seems to be less sensitive to this issue.\n\nFor Youden J Index, see [Youden 1950](https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.1002/1097-0142(1950)3:1%3C32::AID-CNCR2820030106%3E3.0.CO;2-3), for True Skill Statistic see [Bloomfield et al. 2018](https://arxiv.org/pdf/1202.5995.pdf).\n\nFor a thorough example, read this [blog post](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/) on _Machine Learning Master_....",
      "url": "https://stats.stackexchange.com/questions/260164/auc-and-class-imbalance-in-training-test-dataset"
    },
    {
      "title": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions[\n**\ud83d\udce3 BIG NEWS:****Neptune is joining OpenAI!**\u2192 Read the message from our CEO \ud83d\udce3![](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)](https://neptune.ai/blog/we-are-joining-openai)\n[![logo](https://neptune.ai/wp-content/themes/neptune/img/logo-neptune.svg)](https://neptune.ai)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)\nWhat do you want to find?\nSearch\n![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Log in](https://app.neptune.ai/login)[Contact us](https://neptune.ai/contact-us)\n[![Home](https://neptune.ai/wp-content/themes/neptune/img/icon-breadcrumbs-home.svg)](https://neptune.ai/)&gt;[Blog](https://neptune.ai/blog)&gt;[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n![search](https://neptune.ai/wp-content/themes/neptune/img/icon-search.svg)![cancel](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\nSearch in Blog...\n![search](https://neptune.ai/wp-content/themes/neptune/img/image-ratio-holder.svg)\n[Neptune Blog](https://neptune.ai/blog)# Tabular Data Binary Classification: All Tips and Tricks from 5 Kaggle Competitions\n![Author image](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/11/Shahul-Es-new-scaled.jpeg?fit=2560%2C1920&ssl=1)\n[Shahul ES](https://neptune.ai/blog/author/shahules)\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-time.svg)3 min\n![](https://neptune.ai/wp-content/themes/neptune/img/icon-meta-date.svg)1st September, 2023\n[ML Model Development](https://neptune.ai/blog/category/machine-learning-model-development)[Tabular Data](https://neptune.ai/blog/category/tabular-data)\nIn this article, I will discuss some great tips and tricks to improve the performance of your structured data binary classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top tabular data competitions. Without much lag, let\u2019s begin.\nThese are the five competitions that I have gone through to create this article:\n* [**Home credit default risk**](https://www.kaggle.com/c/home-credit-default-risk/)\n* [**Santander Customer Transaction Prediction**](https://www.kaggle.com/c/santander-customer-transaction-prediction/notebooks)\n* [**VSB Power Line Fault Detection**](https://www.kaggle.com/c/vsb-power-line-fault-detection/overview/evaluation)\n* [**Microsoft Malware Prediction**](https://www.kaggle.com/c/microsoft-malware-prediction/overview)\n* [**IEEE-CIS Fraud Detection**](https://www.kaggle.com/c/ieee-fraud-detection/overview/evaluation/)## Dealing with larger datasets\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n* Faster[data loading with pandas.](https://www.kaggle.com/c/home-credit-default-risk/discussion/59575)\n* Data compression techniques to[reduce the size of data by 70%](https://www.kaggle.com/nickycan/compress-70-of-dataset).\n* Optimize the memory by r[educing the size of some attributes.](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n* Use open-source libraries such as[Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space.\n* Use[cudf](https://github.com/rapidsai/cudf).\n* Convert data to[parquet](https://arrow.apache.org/docs/python/parquet.html)format.\n* Converting data to[feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03)format.\n* Reducing memory usage for[optimizing RAM](https://www.kaggle.com/mjbahmani/reducing-memory-size-for-ieee).## Data exploration\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n* EDA for microsoft[malware detection.](https://www.kaggle.com/youhanlee/my-eda-i-want-to-see-all)\n* Time Series[EDA for malware detection.](https://www.kaggle.com/cdeotte/time-split-validation-malware-0-68)\n* Complete[EDA for home credit loan prediction](https://www.kaggle.com/codename007/home-credit-complete-eda-feature-importance).\n* Complete[EDA for Santader prediction.](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n* EDA for[VSB Power Line Fault Detection.](https://www.kaggle.com/go1dfish/basic-eda)## Data preparation\nAfter data exploration, the first thing to do is to use those insights to prepare the data. To tackle issues like class imbalance, encoding categorical data, etc. Let\u2019s see the methods used to do it.\n* Methods to t[ackle class imbalance](https://www.kaggle.com/shahules/tackling-class-imbalance).\n* Data augmentation by[Synthetic Minority Oversampling Technique](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).\n* Fast inplace[shuffle for augmentation](https://www.kaggle.com/jiweiliu/fast-inplace-shuffle-for-augmentation).\n* Finding[synthetic samples in the dataset.](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split)\n* [Signal denoising](https://www.kaggle.com/jackvial/dwt-signal-denoising)used in signal processing competitions.\n* Finding[patterns of missing data](https://www.kaggle.com/jpmiller/patterns-of-missing-data).\n* Methods to handle[missing data](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n* An overview of various[encoding techniques for categorical data.](https://www.kaggle.com/shahules/an-overview-of-encoding-techniques)\n* Building[model to predict missing values.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64598)\n* Random[shuffling of data](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works)to create new synthetic training set.## Feature engineering\nNext, you can check the most popular feature and feature engineering techniques used in these top kaggle competitions. The feature engineering part varies from problem to problem depending on the domain.\n* Target[encoding cross validation](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b/)for better encoding.\n* Entity embedding to[handle categories](https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories).\n* Encoding c[yclic features for deep learning.](https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning)\n* Manual[feature engineering methods](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering).\n* Automated feature engineering techniques[using featuretools](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics).\n* Top hard crafted features used in[microsoft malware detection](https://www.kaggle.com/sanderf/7th-place-solution-microsoft-malware-prediction).\n* Denoising NN for[feature extraction](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798).\n* Feature engineering[using RAPIDS framework.](https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/)\n* Things to remember while processing f[eatures using LGBM.](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575)\n* [Lag features and moving averages.](https://www.kaggle.com/c/home-credit-default-risk/discussion/64593)\n* [Principal component analysis](https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567)for...",
      "url": "https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions"
    },
    {
      "title": "Precision-Recall AUC vs ROC AUC for class imbalance problems | Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/general/7517#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/discussions/general/7517)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/discussions/general/7517#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fdiscussions%2Fgeneral%2F7517)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fdiscussions%2Fgeneral%2F7517)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=f732451c36ab3245b68f:1:10718)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/discussions/general/7517"
    },
    {
      "title": "Incorrect ROC score for Kaggle competition?",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [Incorrect ROC score for Kaggle competition?](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked6 years, 6 months ago\n\nModified [6 years, 6 months ago](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition?lastactivity)\n\nViewed\n177 times\n\n1\n\nI was researching a Kaggle competition and used a Logistic Regression classifier to test the top 10 competitiors' approaches.\n\nLink to the competition: [https://www.kaggle.com/c/detecting-insults-in-social-commentary/leaderboard](https://www.kaggle.com/c/detecting-insults-in-social-commentary/leaderboard)\n\nI'm still fairly new to the classification problems so I just tested classifiers without too much modifications. In this case I used scikit-learn's logreg. I cleaned the test/train data and used it to generate a ROC curve.\n\nMy area under the curve was 0.89 which would have placed me in 1st place with a significant lead and this seems quite impossible to me considering my implementation's simplicity. Could someone tell me if my program is doing something incorrectly that gives such a score (Ex. somehow overfitting or bug in code)?\n\n```\nimport csv\nimport preprocessor as p\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\npath = \"C:\\\\Users\\\\Mike\\\\Desktop\"\n\ndef vectorize_dataset(subpath, stem, vectorizer):\n    comments = []\n    labels = []\n    stemmer = SnowballStemmer(\"english\")\n    with open(path + subpath + '.csv', 'r') as f:\n        data_csv = csv.reader(f)\n\n        for row in data_csv:\n            clean_txt = p.clean(row[2])\n            clean_txt = clean_txt.strip().replace('\"', '').replace('\\\\\\\\', '\\\\').replace('_', ' ')\n            clean_txt = bytes(clean_txt, 'utf-8').decode('unicode_escape', 'ignore')\n            if stem:\n                clean_txt = [stemmer.stem(word.lower()) for word in word_tokenize(clean_txt)]\n            clean_txt = [word for word in clean_txt if word.isalpha()]\n            clean_txt = \" \".join(clean_txt)\n\n            if clean_txt != \"\":\n                if row[0] == str(1) or row[0] == str(0):\n                    comments.append(clean_txt)\n                    labels.append(int(row[0]))\n    if subpath == \"\\\\train\":\n        return (vectorizer.fit_transform(comments), labels)\n    return (vectorizer.transform(comments), labels)\n\ndef print_auroc_for_classifier(vect_tuple, classifier):\n    y_true, y_score = [], []\n\n    for sample, label in zip(vect_tuple[0], vect_tuple[1]):\n        y_true.append(label)\n        y_score.append(classifier.predict_proba(sample)[0][1])\n\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    print(\"ROC AUC: %.2f\" % roc_auc)\n\n    plt.plot(fpr, tpr)\n\nif __name__ == '__main__':\n    plt.figure()\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n    vectorizer = TfidfVectorizer()\n    train_tuple = vectorize_dataset('\\\\train', True, vectorizer)\n    test_tuple = vectorize_dataset('\\\\test', True, vectorizer)\n\n    logreg = linear_model.LogisticRegression(C=7)\n    logreg.fit(train_tuple[0].toarray(), train_tuple[1])\n\n    print_auroc_for_classifier(test_tuple, logreg)\n\n```\n\nInstructions:\n\n1. From the Kaggle link download the train.csv and test\\_with\\_solutions.csv.\n[https://www.kaggle.com/c/detecting-insults-in-social-commentary/data](https://www.kaggle.com/c/detecting-insults-in-social-commentary/data)\n2. Rename test\\_with\\_solutions.csv to test.csv\n3. In code set `path` to be the path to the .csv files\n\nFor the `C` parameter I do not understand it too much and if it is the reason my score is this high, please let me know and I appreciate any advice in finding a good value for it. Thanks.\n\nThe approach:\n\n1. Read .csv file and clean the text (used preprocessor package and manually replaced certain characters)\n2. Used Snowball stemmer and check each word isalpha()\n3. Vectorize the test and train data using scikit-learn's TfidfVectorizer\n4. Train logreg with training data\n5. Calculate and plot ROC curve\n\nEdit:\n\nSo I played around with the C parameter and setting C to a high value such as 1e5 gives me a lower ROC curve area. Perhaps now the main question is, should I be optimizing C to give me the highest ROC curve area assuming my code is correct and C was the parameter I needed to tune?\n\nEdit2: I used GridSearchSV to test C in range of 0.1 to 10 and still got high results (going past 10 and below 0.1 didnt do anything).\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n- [classification](https://stackoverflow.com/questions/tagged/classification)\n- [logistic-regression](https://stackoverflow.com/questions/tagged/logistic-regression)\n- [kaggle](https://stackoverflow.com/questions/tagged/kaggle)\n\n[Share](https://stackoverflow.com/q/47744347)\n\n[Improve this question](https://stackoverflow.com/posts/47744347/edit)\n\nFollow\n\n[edited Dec 11, 2017 at 4:58](https://stackoverflow.com/posts/47744347/revisions)\n\nMike\n\nasked Dec 10, 2017 at 22:52\n\n[![Mike's user avatar](https://www.gravatar.com/avatar/2b8b5d8b7bd87b50fb5579d45bc8eee2?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stackoverflow.com/users/9074689/mike)\n\n[Mike](https://stackoverflow.com/users/9074689/mike) Mike\n\n7588 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n0\n\nYou are using different testing data than would have been available - use just the test.csv file to find the best model and value for C, then evaluate it only on the impermium\\_verification\\_set.csv. When the competition was running, looks like only test was available to find a model, then models were locked and leaderboard was based on the verification set. You are using the full set of both to select the best model.\n\nYou can always ask on the discussion boards on the Kaggle competition page if you want - I'm sure people there will help also. Also some of the top placers, including the winner, have posted their code on the discussion page for interest.\n\n[Share](https://stackoverflow.com/a/47750365)\n\n[Improve this answer](https://stackoverflow.com/posts/47750365/edit)\n\nFollow\n\nanswered Dec 11, 2017 at 10:02\n\n[![Ken Syme's user avatar](https://www.gravatar.com/avatar/8f39257133ad3bbe7221f2a5d808674b?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/2236150/ken-syme)\n\n[Ken Syme](https://stackoverflow.com/users/2236150/ken-syme) Ken Syme\n\n3,55222 gold badges1818 silver badges1919 bronze badges\n\n2\n\n- Tha...",
      "url": "https://stackoverflow.com/questions/47744347/incorrect-roc-score-for-kaggle-competition"
    },
    {
      "title": "Imbalanced data & why you should NOT use ROC curve",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Flct14558%2Fimbalanced-data-why-you-should-not-use-roc-curve)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Flct14558%2Fimbalanced-data-why-you-should-not-use-roc-curve)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5073 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=5502c2919e74bf426769:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/lct14558/imbalanced-data-why-you-should-not-use-roc-curve"
    }
  ]
}