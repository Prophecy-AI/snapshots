{
  "spans": [
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "02a6402458aeedd6",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767939673992943499,
      "end_time": 1767939673993311687,
      "duration": 0.000368188,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x738bd1a8a590>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (7.0KB)\\n[FILE] sampleSubmission.csv (13.0KB)\\n[FILE] test.json (2.0MB)\\n[FILE] test.zip (433.1KB)\\n[FILE] train.json (8.4MB)\\n[FILE] train.zip (1.3MB)\", \"is_error\": false}",
        "tool.duration_ms": 0.4742145538330078
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "6654cbd5d12edd1d",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767939682474727115,
      "end_time": 1767939682475095498,
      "duration": 0.000368383,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x738bd1a8a490>\", \"input\": {\"file_path\": \"/home/data/description.md\"}}",
        "tool.output": "{\"content\": \"# Overview\\n\\n## Description\\n\\n[Get started on this competition through Kaggle Scripts](https://www.kaggle.com/c/random-acts-of-pizza/forums/t/14286/kaggle-scripts-enabled-on-random-acts-of-pizza)\\n\\nIn machine learning, it is often said there are [no free lunches](http://en.wikipedia.org/wiki/No_free_lunch_theorem). *How wrong we were.*\\n\\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/) together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\\n\\n\\\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\\\" says one hopeful poster. What about making an algorithm?\\n\\n![Pizzas](https://storage.googleapis.com/kaggle-media/competitions/kaggle/3949/media/pizzas.png)\\n\\nKaggle is hosting this competition for the machine learning community to use for fun and practice. This [data](http://cs.stanford.edu/~althoff/raop-dataset/) was collected and graciously shared by [Althoff](http://www.timalthoff.com/) et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their [accompanying paper](http://cs.stanford.edu/~althoff/raop-dataset/altruistic_requests_icwsm.pdf) and ask that you cite the following reference in any publications that result from your work:\\n\\nTim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. *How to Ask for a Favor: A Case Study on the Success of Altruistic Requests*, Proceedings of ICWSM, 2014.\\n\\n~*Pizza icons designed by Matthew Dera from the [Noun Project](http://thenounproject.com/term/pizza/30194/)*~\\n\\n### Evaluation\\n\\nSubmissions are evaluated on [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability that a request will get pizza and the observed outcomes.\\n\\n### Submission File\\n\\nFor each request in the test set, you should predict a real-valued probability that it resulted in a pizza. The file should contain a header and have the following format:\\n\\n```\\nrequest_id,requester_received_pizza\\nt3_i8iy4,0\\nt3_1mfqi0,0\\nt3_lclka,0\\n...\\n...\\n```\\n\\n### Citation\\n\\nWill Cukierski. (2014). Random Acts of Pizza. Kaggle. https://kaggle.com/competitions/random-acts-of-pizza\\n\\n\\n# Data\\n\\n## Dataset Description\\n\\n[See, fork, and run a random forest benchmark model through Kaggle Scripts](https://www.kaggle.com/users/993/ben-hamner/random-acts-of-pizza/simple-julia-benchmark)\\n\\nThis dataset includes 5671 requests collected from the Reddit community [Random Acts of Pizza](http://www.reddit.com/r/Random_Acts_Of_Pizza/) between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\\n\\nEach JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\\n\\n## Data fields\\n\\n\\\"giver_username_if_known\\\": Reddit username of giver if known, i.e. the person satisfying the request (\\\"N/A\\\" otherwise).\\n\\n\\\"number_of_downvotes_of_request_at_retrieval\\\": Number of downvotes at the time the request was collected.\\n\\n\\\"number_of_upvotes_of_request_at_retrieval\\\": Number of upvotes at the time the request was collected.\\n\\n\\\"post_was_edited\\\": Boolean indicating whether this post was edited (from Reddit).\\n\\n\\\"request_id\\\": Identifier of the post on Reddit, e.g. \\\"t3_w5491\\\".\\n\\n\\\"request_number_of_comments_at_retrieval\\\": Number of comments for the request at time of retrieval.\\n\\n\\\"request_text\\\": Full text of the request.\\n\\n\\\"request_text_edit_aware\\\": Edit aware version of \\\"request_text\\\". We use a set of rules to strip edited comments indicating the success of the request such as \\\"EDIT: Thanks /u/foo, the pizza was delicous\\\".\\n\\n\\\"request_title\\\": Title of the request.\\n\\n\\\"requester_account_age_in_days_at_request\\\": Account age of requester in days at time of request.\\n\\n\\\"requester_account_age_in_days_at_retrieval\\\": Account age of requester in days at time of retrieval.\\n\\n\\\"requester_days_since_first_post_on_raop_at_request\\\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\\n\\n\\\"requester_days_since_first_post_on_raop_at_retrieval\\\": Number of days between requesters first post on RAOP and time of retrieval.\\n\\n\\\"requester_number_of_comments_at_request\\\": Total number of comments on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_comments_at_retrieval\\\": Total number of comments on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_comments_in_raop_at_request\\\": Total number of comments in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_comments_in_raop_at_retrieval\\\": Total number of comments in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_at_request\\\": Total number of posts on Reddit by requester at time of request.\\n\\n\\\"requester_number_of_posts_at_retrieval\\\": Total number of posts on Reddit by requester at time of retrieval.\\n\\n\\\"requester_number_of_posts_on_raop_at_request\\\": Total number of posts in RAOP by requester at time of request.\\n\\n\\\"requester_number_of_posts_on_raop_at_retrieval\\\": Total number of posts in RAOP by requester at time of retrieval.\\n\\n\\\"requester_number_of_subreddits_at_request\\\": The number of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_received_pizza\\\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\\n\\n\\\"requester_subreddits_at_request\\\": The list of subreddits in which the author had already posted in at the time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_request\\\": Difference of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_minus_downvotes_at_retrieval\\\": Difference of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_upvotes_plus_downvotes_at_request\\\": Sum of total upvotes and total downvotes of requester at time of request.\\n\\n\\\"requester_upvotes_plus_downvotes_at_retrieval\\\": Sum of total upvotes and total downvotes of requester at time of retrieval.\\n\\n\\\"requester_user_flair\\\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \\\"shroom\\\" (received pizza, but not given, N=1306), or \\\"PIF\\\" (pizza given after having received, N=83).\\n\\n\\\"requester_username\\\": Reddit username of requester.\\n\\n\\\"unix_timestamp_of_request\\\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\\n\\n\\\"unix_timestamp_of_request_utc\\\": Unit timestamp of request in UTC.\", \"is_error\": false}",
        "tool.duration_ms": 0.3952980041503906
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "095c5b20ae91d9ba",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767939693931716164,
      "end_time": 1767939695074433533,
      "duration": 1.142717369,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x738bd1a8a7d0>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\n\\n# Read the JSON file\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 1142.7264213562012
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "bfdf6866913d20a8",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767939705111198114,
      "end_time": 1767939705670197351,
      "duration": 0.558999237,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x738bf6c14150>\", \"input\": {\"start_cell\": 0, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\nimport json\\n\\n# Load the training data\\ntrain_path = \\\"/home/data/train.json\\\"\\n\\n# Read the JSON file\\nwith open(train_path, 'r') as f:\\n    train_data = json.load(f)\\n\\nprint(f\\\"Number of training samples: {len(train_data)}\\\")\\nprint(f\\\"Type of data: {type(train_data)}\\\")\\nprint(f\\\"First sample keys: {list(train_data[0].keys())}\\\")\\n```\\nOutput:\\nNumber of training samples: 2878\\nType of data: <class 'list'>\\nFirst sample keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\\n\", \"is_error\": false}",
        "tool.duration_ms": 559.065580368042
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "39fea410e09cf672",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767939724870230613,
      "end_time": 1767939724872244971,
      "duration": 0.002014358,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x738bd1df65d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\n\\n# Check target distribution\\nprint(\\\"Target distribution:\\\")\\nprint(df['requester_received_pizza'].value_counts())\\nprint(f\\\"\\\\nPercentage with pizza: {df['requester_received_pizza'].mean():.3f}\\\")\\n\\n# Check missing values\\nprint(f\\\"\\\\nMissing values:\\\")\\nprint(df.isnull().sum().head(10))\\n\\n# Check data types\\nprint(f\\\"\\\\nData types:\\\")\\nprint(df.dtypes.head(15))\"}}",
        "tool.output": "{\"content\": \"Added code cell at 1\", \"is_error\": false}",
        "tool.duration_ms": 2.1209716796875
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "b4c26e7ebf3d1641",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767939734042703654,
      "end_time": 1767939734149294435,
      "duration": 0.106590781,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x738bf6c14150>\", \"input\": {\"start_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Convert to DataFrame for easier analysis\\ndf = pd.DataFrame(train_data)\\n\\n# Check target distribution\\nprint(\\\"Target distribution:\\\")\\nprint(df['requester_received_pizza'].value_counts())\\nprint(f\\\"\\\\nPercentage with pizza: {df['requester_received_pizza'].mean():.3f}\\\")\\n\\n# Check missing values\\nprint(f\\\"\\\\nMissing values:\\\")\\nprint(df.isnull().sum().head(10))\\n\\n# Check data types\\nprint(f\\\"\\\\nData types:\\\")\\nprint(df.dtypes.head(15))\\n```\\nOutput:\\nTarget distribution:\\nrequester_received_pizza\\nFalse    2163\\nTrue      715\\nName: count, dtype: int64\\n\\nPercentage with pizza: 0.248\\n\\nMissing values:\\ngiver_username_if_known                        0\\nnumber_of_downvotes_of_request_at_retrieval    0\\nnumber_of_upvotes_of_request_at_retrieval      0\\npost_was_edited                                0\\nrequest_id                                     0\\nrequest_number_of_comments_at_retrieval        0\\nrequest_text                                   0\\nrequest_text_edit_aware                        0\\nrequest_title                                  0\\nrequester_account_age_in_days_at_request       0\\ndtype: int64\\n\\nData types:\\ngiver_username_if_known                                  object\\nnumber_of_downvotes_of_request_at_retrieval               int64\\nnumber_of_upvotes_of_request_at_retrieval                 int64\\npost_was_edited                                          object\\nrequest_id                                               object\\nrequest_number_of_comments_at_retrieval                   int64\\nrequest_text                                             object\\nrequest_text_edit_aware                                  object\\nrequest_title                                            object\\nrequester_account_age_in_days_at_request                float64\\nrequester_account_age_in_days_at_retrieval              float64\\nrequester_days_since_first_post_on_raop_at_request      float64\\nrequester_days_since_first_post_on_raop_at_retrieval    float64\\nrequester_number_of_comments_at_request                   int64\\nrequester_number_of_comments_at_retrieval                 int64\\ndtype: object\\n\", \"is_error\": false}",
        "tool.duration_ms": 106.66775703430176
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "d6fc3748f51a954d",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767939755469924107,
      "end_time": 1767939755471993885,
      "duration": 0.002069778,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x738bd1df65d0>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Explore text features\\nprint(\\\"Sample request title:\\\")\\nprint(df['request_title'].iloc[0][:200])\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\n\\nprint(\\\"Sample request text:\\\")\\nprint(df['request_text'].iloc[0][:300])\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\n\\n# Text length statistics\\nprint(\\\"Text length statistics:\\\")\\ndf['title_length'] = df['request_title'].str.len()\\ndf['text_length'] = df['request_text'].str.len()\\n\\nprint(f\\\"Title length - Mean: {df['title_length'].mean():.1f}, Max: {df['title_length'].max()}\\\")\\nprint(f\\\"Text length - Mean: {df['text_length'].mean():.1f}, Max: {df['text_length'].max()}\\\")\\n\\n# Check some key numerical features\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\nprint(\\\"Key numerical features correlation with target:\\\")\\nnumerical_features = ['number_of_upvotes_of_request_at_retrieval', \\n                      'request_number_of_comments_at_retrieval',\\n                      'requester_account_age_in_days_at_request',\\n                      'requester_number_of_comments_at_request']\\n\\nfor feature in numerical_features:\\n    corr = df[feature].corr(df['requester_received_pizza'])\\n    print(f\\\"{feature}: {corr:.3f}\\\")\"}}",
        "tool.output": "{\"content\": \"Added code cell at 2\", \"is_error\": false}",
        "tool.duration_ms": 2.1409988403320312
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "ef45c662a4218141",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767939765500734637,
      "end_time": 1767939765528417474,
      "duration": 0.027682837,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x738bf6c14150>\", \"input\": {\"start_cell\": 2, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 2-2\\n==================================================\\n\\nCell 2:\\n```\\n# Explore text features\\nprint(\\\"Sample request title:\\\")\\nprint(df['request_title'].iloc[0][:200])\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\n\\nprint(\\\"Sample request text:\\\")\\nprint(df['request_text'].iloc[0][:300])\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*50)\\n\\n# Text length statistics\\nprint(\\\"Text length statistics:\\\")\\ndf['title_length'] = df['request_title'].str.len()\\ndf['text_length'] = df['request_text'].str.len()\\n\\nprint(f\\\"Title length - Mean: {df['title_length'].mean():.1f}, Max: {df['title_length'].max()}\\\")\\nprint(f\\\"Text length - Mean: {df['te\\n```\\nOutput:\\nSample request title:\\n[REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\\n\\n==================================================\\nSample request text:\\nI will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\\n\\n==================================================\\nText length statistics:\\nTitle length - Mean: 71.6, Max: 272\\nText length - Mean: 402.5, Max: 4460\\n\\n==================================================\\nKey numerical features correlation with target:\\nnumber_of_upvotes_of_request_at_retrieval: 0.091\\nrequest_number_of_comments_at_retrieval: 0.291\\nrequester_account_age_in_days_at_request: 0.043\\nrequester_number_of_comments_at_request: 0.037\\n\", \"is_error\": false}",
        "tool.duration_ms": 27.768850326538086
      }
    },
    {
      "trace_id": "d0c9ba8b5b1e8c2ccb9a71cf0d1c9b24",
      "span_id": "54d31968a6dd5a0b",
      "parent_span_id": "4df94b9b0f13c589",
      "name": "WebSearchTool",
      "kind": "INTERNAL",
      "start_time": 1767939778909670514,
      "end_time": 1767939786706437273,
      "duration": 7.796766759,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WebSearchTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.research.WebSearchTool object at 0x738bf6c14210>\", \"input\": {\"query\": \"What are winning approaches for text classification with metadata features on Kaggle competitions?\"}}",
        "tool.output": "{\"content\": \"SEARCH RESULTS (9 found)\\n==================================================\\n\\n--- Result 1 ---\\nTitle: ANSWER\\n\\nContent:\\nWinning Kaggle solutions for text classification that also have metadata usually treat the non‑text fields as first‑class inputs rather than an after‑thought.  A common recipe is to **encode the text with a pre‑trained transformer** (e.g., BERT, RoBERTa) and **encode each metadata column separately** – categorical variables with embedding layers, numeric variables with scaling or bucketisation – then concatenate all vectors before the final classification head.  This “dual‑tower” design lets the model learn interactions between language and auxiliary signals while keeping each modality in a suitable representation space ([FreeAgent blog](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  \\n\\nA simpler but surprisingly effective trick is to **inject metadata directly into the transformer’s token stream**.  By joining the raw fields with a special separator token (e.g., `[SEP]`) the model sees the amount, category, timestamp, etc., as part of the same sequence, which often yields a quick performance bump and works well when the metadata can be expressed as short text ([FreeAgent blog](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  More sophisticated pipelines build **dedicated neural blocks for each metadata type** (image, audio, timestamps, sender IDs) and train them jointly with the text encoder, allowing cross‑modal attention and richer feature fusion; the authors of the MESSAGENET paper report consistent gains over plain‑text baselines ([arXiv 2301.01808](https://export.arxiv.org/pdf/2301.01808v1.pdf)).  \\n\\nBeyond architecture, top teams also exploit **metadata‑driven weak supervision**.  The META framework treats metadata (author, venue, time) as a source of noisy label cues, constructs a text‑rich graph, extracts high‑confidence “seed motifs”, and bootstraps a classifier that iteratively expands both seed words and motifs.  This approach has shown superior accuracy on real‑world datasets where metadata carries strong label signals ([EMNLP 2020 META](https://aclanthology.org/2020.emnlp-main.670.pdf)).  In practice, winning solutions combine these ideas with classic Kaggle tricks—large‑scale pre‑training, careful cross‑validation, and ensembling transformer‑based models with gradient‑boosted trees that consume the raw metadata features ([Neptune tips](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  Together, these strategies turn metadata from a peripheral attribute into a powerful driver of classification performance.\\n\\n--- Result 2 ---\\nTitle: Text Classification: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\\n\\nContent:\\nIn this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle’s top NLP competitions.\\n\\nNamely, I’ve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) – $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) – $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) – $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) – $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) – $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let’s begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let’s see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet’s see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in  R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\\n\\n--- Result 3 ---\\nTitle: Combining text with numerical and categorical features for classification\\nURL: https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification\\n\\nContent:\\n[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\\n\\n# Classification with transformer models\\n\\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\\n\\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\\n\\nFor example a transaction with the following set of features:\\n\\n|     |     |     |\\n| --- | --- | --- |\\n| **Description** | **Amount** | **Industry** |\\n| Plastic pipes | £-7.99 | Plumbing |\\n\\ncould be represented by the string:  `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We’ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\\n\\nThis approach however doesn’t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\\n\\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\\n\\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\\n\\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\\n\\n# Public wine reviews dataset\\n\\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores – wines between 80 and 86 (incl) were flagged as “neutral”, between 87 and 93 as “good” and greater than 94 as “excellent”.\\n\\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\\n\\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\\n\\n```\\nimport pandas as pd\\n\\nwine_df = pd.read_csv(\\\"data/wine_data.csv\\\")\\nbins = [0, 87, 94, np.inf]\\nnames = [\\\"neutral\\\", \\\"good\\\", \\\"excellent\\\"]\\n\\nwine_df[\\\"rating\\\"] = pd.cut(wine_df[\\\"points\\\"], bins, labels=names)\\n\\n```\\n\\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\\n\\n```\\nNUMERICAL_FEATURE = \\\"price\\\"\\nCATEGORICAL_FEATURE = \\\"variety\\\"\\nTEXT_FEATURE = \\\"description\\\"\\nTARGET = \\\"rating\\\"\\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\\n\\nwine_df = wine_df[FEATURES + [TARGET]]\\n\\n```\\n\\nWe then split our wine dataframe into an 80:20 training:evaluation split.\\n\\n```\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\\n\\n```\\n\\n## LightGBM classifier approach\\n\\n### Preprocessing\\n\\n#### Preprocessing numerical and categorical features\\n\\nIn a first step we will preprocess the numerical and categorical features using SKLearn’s ColumnTransformer.  For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\\n\\nWe preprocessed the categorical features by filling missing values with “other” and OneHot encoded them.\\n\\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\\n\\n```\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    OneHotEncoder,\\n    StandardScaler,\\n)\\n\\ndef preprocess_number():\\n    return make_pipeline(\\n        SimpleImputer(strategy=\\\"median\\\"),\\n        StandardScaler(),\\n    )\\n\\ndef preprocess_categories():\\n    return make_pipeline(\\n       SimpleImputer(strategy=\\\"constant\\\", fill_value=\\\"other\\\", missing_values=np.nan),\\n       OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n    )\\n\\ndef create_preprocessor():\\n\\n    transformers = [\\n        (\\\"num_preprocessor\\\", preprocess_number(), [NUMERICAL_FEATURE]),\\n        (\\\"cat_preprocessor\\\", preprocess_categories(), [CATEGORICAL_FEATURE]),\\n    ]\\n\\n    return ColumnTransformer(transformers=transformers, remainder=\\\"drop\\\")\\n\\ncolumn_transformer = create_preprocessor()\\ncolumn_transformer.set_output(transform=\\\"pandas\\\")\\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\\n)\\n\\n```\\n\\n#### Extracting text vector representation with a transformer model\\n\\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\\n\\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\\\[CLS\\\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\\\[CLS\\\\] token in the final layer as the vector representation of our wine descriptions.\\n\\nIn order to extract the \\\\[CLS\\\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\\n\\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\\\_ids, attention\\\\_mask and token\\\\_type\\\\_ids. Only the input\\\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\\n\\nThe code for the tokenization is shown below:\\n\\n```\\nfrom datasets import Dataset\\nfrom transformers import AutoTokenizer\\n\\nMODEL_NAME = \\\"distilbert-base-uncased\\\"\\n\\ndef tokenized_pytorch_tensors(\\n        df: pd.DataFrame,\\n        column_list: list\\n    ) -> Dataset:\\n\\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n    transformers_dataset = Dataset.from_pandas(df)\\n\\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\\n        return tokenizer(\\n            model_inputs_batch[TEXT_FEATURE],\\n            padding=True,\\n            max_length=120,\\n            truncation=True,\\n        )\\n\\n    tokenized_dataset = transformers_dataset.map(\\n        tokenize,\\n        batched=True,\\n        batch_size=128\\n    )\\n\\n    tokenized_dataset.set_format(\\n        \\\"torch\\\",\\n        columns=column_list\\n    )\\n\\n    columns_t...\\n\\n--- Result 4 ---\\nURL: https://export.arxiv.org/pdf/2301.01808v1.pdf\\n\\nContent:\\nAdar Kahana adarkahana@microsoft.com\\nMicrosoft R&D center\\nILDC\\nOren Elisha orelisha@microsoft.com\\nMicrosoft R&D center\\nILDC\\nMESSAGENET: MESSAGE CLASSIFICATION USING NATURAL LANGUAGE PROCESSING AND META-DATA\\nMessage classification · Meta data injection · Deep learning · Natural language processing\\nIn this paper we propose a new Deep Learning (DL) approach for message classification. Our method is based on the state-of-the-art Natural Language Processing (NLP) building blocks, combined with a novel technique for infusing the meta-data input that is typically available in messages such as the sender information, timestamps, attached image, audio, affiliations, and more. As we demonstrate throughout the paper, going beyond the mere text by leveraging all available channels in the message, could yield an improved representation and higher classification accuracy. To achieve message representation, each type of input is processed in a dedicated block in the neural network architecture that is suitable for the data type. Such an implementation enables training all blocks together simultaneously, and forming cross channels features in the network. We show in the Experiments Section that in some cases, message's meta-data holds an additional information that cannot be extracted just from the text, and when using this information we achieve better performance. Furthermore, we demonstrate that our multi-modality block approach outperforms other approaches for injecting the meta data to the the text classifier.Many real world applications require message classification and regression, such as handling spam emails [1], ticket routing [2], article sentiment review [3] and more. Accurate message classification could improve critical scenarios such as in call centers (routing tickets based on topic) [2], alert systems (flagging highly important alert messages) [4], and categorizing incoming messages (automatically unclutter emails)[1,5]. The main distinction between text and message classification is the availability of additional attributes, such as the sender information, timestamps, attached image, audio, affiliations, and more. New message classification contests often appear in the prominent platforms (i.e., Kaggle [6]), showing how this topic is sought after. There are already many data-sets to explore in this field, but no clear winner algorithm that fits all scenarios with high accuracy, efficiency and simplicity (in terms of implementation and interpretation).A notable advancement in the field of NLP is the attention based transformers architecture[7]. This family of methods excels in finding local connections between words, and better understanding the meaning of a sentence. A leading example is the Bidirectional Encoder Representations from Transformers (BERT) [8] as well as its variations [9, 10, 11], winning certain benchmarks[12,13]. Several packages, such as Huggingface Transformers[14], make such models accessible and easy to use as well as provide pre-trained versions. In addition, one can use transfer learning [15] to further train BERT on their on data, creating a tailored model for the specific task at hand.BERT, and often other transformer based models, are designed to handle text. They operate on the words of a given text by encoding them into tokens, and by the connections between the tokens they learn the context of sentences. This approach is limited, since sometimes more information can be extracted and used, not necessarily textual. Throughout this paper we refer to this information as meta-data to distinguish it from the main stream of textual content (though one may recognize it as the core data, depending on the application). For example, a meta-data could be the time stamp of when the text was written, sent, published, etc. Another example is the writer of the text, when dealing with a small\\nlist of writers of a corpus. There have been some attempts to incorporate these into BERT models, for example by assigning artificial tokens for writers or for temporal segments (token per month for example) [16]. This approach is limited since not all meta-data entries are suitable for encoding by tokenization. In the example of temporal segments, more segments introduce more tokens, leading to large computational resources consumption, and less segments cause loss of information. Another approach is to concatenate the embeddings, created by the transformer module, with the outputs of an embedding module for the meta-data. In this approach, a transformer for the text is trained (using direct or transfer learning) on the text, and other separate modules (time series embedding, senders embeddings, etc.) are used to embed the meta-data. All the embeddings are\\n\\n--- Result 5 ---\\nURL: https://aclanthology.org/2020.emnlp-main.670.pdf\\n\\nContent:\\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8351–8361,\\nNovember 16–20, 2020. \\rc 2020 Association for Computational Linguistics\\n8351\\nMETA: Metadata-Empowered Weak Supervision for Text Classification\\nDheeraj Mekala1 Xinyang Zhang2 Jingbo Shang1,3\\n1 Department of Computer Science and Engineering, University of California San Diego, CA, USA\\n2 Department of Computer Science, University of at Illinois Urbana-Champaign, IL, USA\\n3 Halıcıoglu Data Science Institute, University of California San Diego, CA, USA ˘\\n1,3\\n{dmekala, jshang}@ucsd.edu 2 xz43@illinois.edu\\nAbstract\\nRecent advances in weakly supervised learn\\u0002ing enable training high-quality text classifiers\\nby only providing a few user-provided seed\\nwords. Existing methods mainly use text data\\nalone to generate pseudo-labels despite the\\nfact that metadata information (e.g., author\\nand timestamp) is widely available across var\\u0002ious domains. Strong label indicators exist in\\nthe metadata and it has been long overlooked\\nmainly due to the following challenges: (1)\\nmetadata is multi-typed, requiring systematic\\nmodeling of different types and their combi\\u0002nations, (2) metadata is noisy, some metadata\\nentities (e.g., authors, venues) are more com\\u0002pelling label indicators than others. In this\\npaper, we propose a novel framework, META,\\nwhich goes beyond the existing paradigm and\\nleverages metadata as an additional source of\\nweak supervision. Specifically, we organize\\nthe text data and metadata together into a\\ntext-rich network and adopt network motifs to\\ncapture appropriate combinations of metadata.\\nBased on seed words, we rank and filter mo\\u0002tif instances to distill highly label-indicative\\nones as “seed motifs”, which provide addi\\u0002tional weak supervision. Following a boot\\u0002strapping manner, we train the classifier and\\nexpand the seed words and seed motifs itera\\u0002tively. Extensive experiments and case stud\\u0002ies on real-world datasets demonstrate supe\\u0002rior performance and significant advantages of\\nleveraging metadata as weak supervision.\\n1 Introduction\\nWeakly supervised text classification has recently\\ngained much attention from the researchers because\\nit reduces the burden of annotating the data. So far,\\nthe major source of weak supervision lies in text\\ndata itself (Agichtein and Gravano, 2000; Kuipers\\net al., 2006; Riloff et al., 2003; Tao et al., 2015;\\nMeng et al., 2018; Mekala and Shang, 2020). These\\nmethods typically require a few user-provided seed\\nPaper Authors Year Category\\nP1 G. Hinton, S. Osindero, YW. Teh 2006 ML\\nP2 G. Hinton, O. Vinyals, J. Dean 2015 ML\\nP3 J. Dean, S.Ghemawat 2008 Sys\\n(a) Examples of research papers with metadata.\\nO. Vinyals\\nG. Hinton\\n2016\\nneural\\nsystem\\ndata\\nlearning\\nJ. Dean Doc\\nAuthor 1 Author 2\\nP2\\nO. Vinyals G. Hinton\\nP1 P2\\n… …\\n…\\n(b) A text-rich network view of \\nthe papers.\\n(c) A motif pattern and \\na motif instance.\\nFigure 1: Text corpus, text-rich network, and motif.\\nwords for each class as weak supervision. They ex\\u0002pand seed words with generated pseudo labels and\\nimprove their text classifier in an iterative fashion.\\nMetadata information (e.g., author, published\\nyear) in addition to textual information, is widely\\navailable across various domains (e.g., news arti\\u0002cles, social media posts, and scientific papers) and\\nit could serve as a strong, complementary weak\\nsupervision source. Take a look at the research\\npapers in Figure 1(a) as an example. It shall be\\nlearned in a data-driven manner that G. Hinton is a\\nhighly-reputed machine learning researcher, thus\\nhis presence is a strong indicator of a paper belong\\u0002ing to the Machine Learning category.\\nDistilling effective metadata for weak supervi\\u0002sion faces several major challenges. Metadata is\\noften multi-typed, each type and the type combi\\u0002nations could have very different semantics and\\nmay not be equally important. Moreover, even en\\u0002tities within a single metadata type could be noisy.\\nContinuing our example in Figure 1(a), we shall\\nnotice that year is less helpful than an author to do\\nclassification. Among the authors, J. Dean might\\nbe an important figure but has research interests\\n8352\\nm1,1 .95 .01 .04\\nm1,2 .32 .30 .38\\nText Classifier\\nSeed Words ① Pseudo Label Generation ③ Motif Ranking & …\\nC1 Neural Network\\nC2 Image Classification\\nC3 Translation\\nPaper\\nAuthor 1 Author 2\\nM1\\nM2\\nm2,1 .03 .89 .08\\nm2,2 .31 .26 .43\\nInstance C1 C2 C3\\nKlein Manning\\nP1 P2\\n② Classifier Training / Prediction\\nRecursion\\nUser Given\\nMotif Pattern\\nMatched (Seed)\\nMotif Instances\\n…\\n…\\nC1 m1,1 m1,6 m2,3 m1,4 …\\nC2 m2,1 m2,4 m1,2 m1,4 …\\nC3 m1,3 m2,5 m2,2 m1,7 …\\n... Expansion\\nExpand from high score to low\\nNew seed set cutoff\\nFigure 2: Our META framework. In each iteration, we generate pseudo labels for documents, train the text classifier,\\nand rank all words and motif instances in a unified ranking framework. We then expand seed sets until an automatic\\ncutoff is reached. The quality of the classifier and the seed sets are improved through iterations.\\nspanning across different domains. However, if we\\njoin the author with year, it carries more accurate\\nsemantics, and we may discover J. Dean has more\\ninterest in machine learning in recent years, thus\\nbecoming highly label-indicative.\\nBearing the challenges in mind, we propose\\nMETA, a principled framework for metadata\\u0002empowered weakly-supervised text classification.\\nAs illustrated in Figure 1 and Figure 2, we first\\norganize the text data and metadata together into\\na text-rich network. The network structure gives\\nus a holistic view of the corpus and enables us to\\nrank and select useful metadata entities. We lever\\u0002age motif patterns (Benson et al., 2016; Milo et al.,\\n2002; Shang et al., 2020) to model typed metadata\\nas well as their combinations. A motif pattern is\\na subgraph pattern at the meta-level that captures\\nhigher-order connections and the semantics repre\\u0002sented by these connections. It serves as a useful\\ntool to model typed edges, typed paths (a.k.a. meta\\u0002paths) (Sun et al., 2011), and higher-order struc\\u0002tures in the network. With little effort, users can\\nspecify a few possibly useful motif patterns as in\\u0002put to our model. We develop a unified, principled\\nranking mechanism to select label-indicative motif\\ninstances and words, forming expanded weak su\\u0002pervision. Note that, such instance-level selection\\nprocess also implicitly refines the motif patterns,\\nensuring the robust performance of META even\\nwhen irrelevant motif patterns exist in input. It is\\nworth a mention that META is compatible with any\\ntext classifiers.\\nOur contributions are summarized as follows:\\n• We explore to incorporate metadata information\\nas an additional source of weak supervision for\\ntext classification along with seed words.\\n• We propose a novel framework META, which in\\u0002troduces motif patterns to capture the high-order\\ncombinations among different types of metadata\\nand conducts a unified ranking and selection of\\nlabel-indicative motif instances and words.\\n• We conduct experiments on two real-world\\ndatasets. The results and case studies demon\\u0002strate the superiority of incorporating metadata\\nas parts of weak supervision and verify the effec\\u0002tiveness of META.\\nReproducibility. Our code is made publicly avail\\u0002able at GitHub1\\n.\\n2 Preliminaries\\n2.1 Documents as Text-rich Network\\nGiven a collection of n text documents D =\\n{D1, D2, . . . , Dn}, and their corresponding meta\\u0002data, we propose to organize them into a text-rich\\nnetwork, as illustrated in Figure 1(b). A text-rich\\nnetwork is a heterogeneous network with docu\\u0002ments, words, different types of metadata as nodes,\\nand their associations as edges. For example, our\\ntext-rich network for research papers has papers,\\nwords, authors, and publication years as nodes.\\nEach paper is connected to its associated words\\nand metadata nodes. Such a network provides a\\nholistic and structured representation of the input.\\n2.2 Seed Words and Motif Patterns\\nUsers are asked to provide a few seed words S\\n= {Sw\\n1\\n, S\\nw\\n2\\n, . . . , S\\nw\\nl\\n} for each of l classes (i.e.,\\nC1, C2, . . . , ...\\n\\n--- Result 6 ---\\nTitle: 20% Accuracy Bump in Text Classification with ME-ULMFiT\\nURL: https://medium.com/novetta/20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78\\n\\nContent:\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4545e993aa78&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\n# 20% Accuracy Bump in Text Classification with ME-ULMFiT\\n\\n[![Matthew Teschke](https://miro.medium.com/v2/resize:fill:88:88/0*nsQhCoB0HTmu_sw5.)](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)[![Novetta](https://miro.medium.com/v2/resize:fill:48:48/1*FLKm3lgYky637lUrGUFvrA.png)](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n[Matthew Teschke](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)\\n\\n·\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb8201a221b3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&user=Matthew+Teschke&userId=cb8201a221b3&source=post_page-cb8201a221b3----4545e993aa78---------------------post_header-----------)\\n\\nPublished in\\n\\n[Novetta](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n·\\n\\n5 min read\\n\\n·\\n\\nMar 31, 2020\\n\\n10\\n\\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D4545e993aa78&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=-----4545e993aa78---------------------post_audio_button-----------)\\n\\nShare\\n\\n_By incorporating metadata into the text before training, we see as much as a 20% increase in accuracy_\\n\\n# Introduction\\n\\nIn a [previous post](https://www.novetta.com/2019/03/introducing_me_ulmfit/), we discussed an enhanced method of passing additional data to the ULMFiT¹ algorithm for unstructured text classification, which improved accuracy by up to 9%. In follow-on experiments, we have demonstrated that this approach — in which we prepend metadata to the text to provide more information to the model — increased accuracy by up to 20%, in relative terms. This improvement required just a few lines of Python, such that the performance boost comes at essentially no cost.\\n\\nThis blog post explains the findings of this more-detailed evaluation, where we have compared how the following affect accuracy:\\n\\n1. Each individual piece of metadata\\n2. Different combinations of metadata\\n3. Different ways of passing the metadata to the model\\n\\nFor the evaluation, we worked with six [Novetta Mission Analytics](https://www.novetta.com/products/novetta-mission-analytics/) datasets, described below. The objective is to assign metadata tags to quotes, using granular categories called Submessages.\\n\\nSome highlights from our results include:\\n\\n- Source-related metadata tags consistently performed the best, confirming our original hypothesis. Sources (e.g. UN Official) are the metadata most closely associated with the quotes we are tagging, so it makes sense that sources add the most information.\\n- Including media name (e.g. New York Times) either decreased accuracy or showed little improvement. We found this surprising as we had assumed that including metadata would only improve accuracy.\\n- Adding more metadata tags to the model did not always improve performance.\\n- For five of the six datasets, using metadata tags that distinguish between columns in our data performed better than the approach (currently suggested by the fast.ai library) in which each field was demarcated by xxfld.\\n\\n# Dataset Overview\\n\\nWe conducted evaluations on the six datasets listed in Table 1. For each dataset, we made the following splits time-based between training and validation, where later data is held out for validation. We used 80% of the data for training and 17% for validation on the language model. Using that 97% of the data, we then trained the classifier, reserving the last 3% for the final validation.\\n\\n![](https://miro.medium.com/v2/resize:fit:700/1*WtQcwQm1EIIoHz3Gy6EoaA.png)\\n\\nTable 1: Characteristics of Datasets Used in ME-ULMFiT Evaluation\\n\\n# Experiment 1: Individual Metadata\\n\\nOur first experiment evaluated how individual pieces of metadata impacted performance of the model. Working with six types of metadata (Author Name, Media Name, Media Nation, Source Name, Source Nation, Source Type), we created a model that prepended only that piece of information to the quote text. For example, a Source Name of “Steffen Seibert” prepended to a quote results in:\\n\\n```\\nsource_name source_name_Steffen_Seibert quote_text that “we have started down the path” towards the NATO goal.\\n```\\n\\nThe results of Experiment 1 are shown in the following heat map:\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*6-wTYPDfG1meArIiwnA1oA.png)\\n\\nFigure 1: Heat map showing relative change in accuracy of ME-ULMFiT with individual types of metadata compared to the baseline ULMFiT model with no metadata\\n\\nFrom the heat map, we made the following observations:\\n\\n- Source-related metadata tags generally performed the best\\n- The Media Name metadata tag performed the worst for four out of six datasets\\n- Datasets with fewer Submessage labels saw less improvement from the inclusion of metadata than those with more Submessage labels\\n\\n# Experiment 2: Combinations of Metadata\\n\\nIn the second experiment, we evaluated how different combinations of metadata impacted model accuracy. Our intuition suggested that more metadata would always lead to better performance, but as shown in Figure 2, this was not the case. We will be conducting further investigation into why.\\n\\nWe examined five different combinations of metadata as follows:\\n\\n- The top-2 performing metadata tags from Experiment 1\\n- The top-3 performing metadata tags from Experiment 1\\n- The top-4 performing metadata tags from Experiment 1\\n- All metadata tags\\n- Fast.ai built-in tags, which utilized all metadata but whose fields are separated by xxfld instead of column-specific tags. In this method, values within the data are not concatenated (e.g., The New York Times was not converted to xxfld\\\\_The\\\\_New\\\\_York\\\\_Times)\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*UQW6om7aQzCpUEp9pvyQpQ.png)\\n\\nFigure 2: Heat map showing relative change in accuracy of ME-ULMFiT with combinations of metadata compared to the baseline ULMFiT model with no metadata\\n\\nWhile we see that more often than not multiple tags improve performance over the best baseline, that is not always the case. For only one dataset did the model with all metadata perform the best and it generally underperformed the most accurate individual metadata model. We believe this has to do with the way batches are created within the model, so we will explore further to see why more data is not always improving accuracy.\\n\\n# Experiment 3: Methods of Passing Metadata\\n\\nLastly, for the models that included all metadata, we wanted to compare different methods of passing in metadata. Our [original post](https://www.novetta.com/2019/03/introducing_me_ulmfit/) describes our method of using separate tags for each metadata field and concatenating all valu...\\n\\n--- Result 7 ---\\nTitle: A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\nURL: https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312\\n\\nContent:\\nA Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\n[\\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n18 min read\\n·Apr 28, 2024\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\\nListen\\nShare\\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\\n**> Note:\\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\\n*> “It is estimated that 90% of the world’s data was generated in the last two years alone”\\n*> by [> Amount of Data Created Daily (2023)\\n](https://explodingtopics.com/blog/data-generated-per-day)\\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\\n## Methodology\\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\\nPress enter or click to view image in full size\\n![]()\\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\\n## Key Discoveries of Text Data Competitions\\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term “question answering” exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...\\n\\n--- Result 8 ---\\nTitle: How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nURL: https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\\n\\nContent:\\nHow To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n←Previous\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\\n\\n--- Result 9 ---\\nTitle: GitHub - AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe: Classifies 500K+ political tweets from Northern Europe using NLP and machine learning to analyze political discourse.\\nURL: https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe\\n\\nContent:\\n<div><div><article><p></p><h2>🐦 Kaggle Project: Classification of Tweets from Northern Europe</h2><a href=\\\"#-kaggle-project-classification-of-tweets-from-northern-europe\\\"></a><p></p>\\n<p><a href=\\\"https://camo.githubusercontent.com/439fd5ffe2ac21b3daa03ac6474978163be089da5fbba2790769ae2275464e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\\\"></a></p>\\n<p></p><h2>🚀 Overview</h2><a href=\\\"#-overview\\\"></a><p></p>\\n<p>This project analyzes a dataset of <strong>509,031 tweets</strong> from politicians across <strong>seven Northern European countries</strong>, aiming to classify tweets by <strong>political spectrum (Left, Right, Center)</strong> and <strong>geography</strong>. Using <strong>Natural Language Processing (NLP)</strong> and machine learning, we extracted key insights into political discourse and developed a classifier with strong performance.</p>\\n<hr/>\\n<p></p><h2>📊 Dataset</h2><a href=\\\"#-dataset\\\"></a><p></p>\\n<ul>\\n<li><strong>Countries:</strong> Belgium, Denmark, Iceland, Ireland, Netherlands, Norway, Sweden</li>\\n<li><strong>Time span:</strong> Dec 2008 – Jan 2023</li>\\n<li><strong>Dataset size:</strong> 509,031 tweets\\n<ul>\\n<li><strong>Training set:</strong> 407,223 tweets</li>\\n<li><strong>Test set:</strong> 101,808 tweets</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Attributes:</strong></p>\\n<ul>\\n<li>Tweet text</li>\\n<li>Country</li>\\n<li>Gender</li>\\n<li>Political spectrum label (for supervised learning)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>🔬 Methods</h2><a href=\\\"#-methods\\\"></a><p></p>\\n<p></p><h3>Data Cleaning &amp; Preprocessing</h3><a href=\\\"#data-cleaning--preprocessing\\\"></a><p></p>\\n<ul>\\n<li>Text cleaning: Lowercasing, punctuation removal, stopword removal, lemmatization.</li>\\n<li>New feature creation: <code>text_clean_country_gender_user</code> combining multiple metadata.</li>\\n</ul>\\n<p></p><h3>Feature Engineering</h3><a href=\\\"#feature-engineering\\\"></a><p></p>\\n<ul>\\n<li><strong>Vectorization:</strong>\\n<ul>\\n<li>Count Vectorizer</li>\\n<li>TF-IDF transformation</li>\\n</ul>\\n</li>\\n</ul>\\n<p></p><h3>Modeling</h3><a href=\\\"#modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>Classifier:</strong> Linear Support Vector Classifier (LinearSVC)</li>\\n<li><strong>Validation:</strong> 10-fold cross-validation</li>\\n</ul>\\n<p></p><h3>Topic Modeling</h3><a href=\\\"#topic-modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>LDA (Latent Dirichlet Allocation)</strong></li>\\n<li><strong>NMF (Non-negative Matrix Factorization)</strong></li>\\n</ul>\\n<hr/>\\n<p></p><h2>📈 Results</h2><a href=\\\"#-results\\\"></a><p></p>\\n<p>✅ <strong>Key findings:</strong></p>\\n<ul>\\n<li>Tweets span diverse topics from <strong>domestic politics</strong> to <strong>international relations</strong>.</li>\\n<li>Different political leanings across countries: e.g., Netherlands leans Center, Iceland leans Left.</li>\\n<li>Gender distribution varies: Sweden had a <strong>female-majority</strong>, others skewed male.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>🛠️ Requirements</h2><a href=\\\"#️-requirements\\\"></a><p></p>\\n<ul>\\n<li>Python 3.8+</li>\\n<li>Key libraries:\\n<ul>\\n<li>pandas, numpy</li>\\n<li>scikit-learn</li>\\n<li>nltk, spaCy</li>\\n<li>gensim</li>\\n<li>matplotlib, seaborn</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Install via:</p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<hr/>\\n<p></p><h2>🖥️ Usage</h2><a href=\\\"#️-usage\\\"></a><p></p>\\n<p>1️⃣ <strong>Clone the repository:</strong></p>\\n<div><pre>git clone https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe.git\\n<span>cd</span> kaggle-project-classification-of-tweets-from-northern-europe</pre></div>\\n<p>2️⃣ <strong>Install dependencies:</strong></p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<p>3️⃣ <strong>Run analysis:</strong></p>\\n<ul>\\n<li>Open <code>kaggle_project.ipynb</code> for full preprocessing, modeling, and evaluation workflows.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>📂 Repository Structure</h2><a href=\\\"#-repository-structure\\\"></a><p></p>\\n<div><pre><code>├── data/ # Dataset (not included)\\n├── kaggle_project.ipynb # Main Jupyter notebook\\n├── kaggle_project_report.pdf # Final project report\\n├── README.md\\n├── requirements.txt # Dependencies\\n└── LICENSE\\n</code></pre></div>\\n<hr/>\\n<p></p><h2>👥 Contributors</h2><a href=\\\"#-contributors\\\"></a><p></p>\\n<ul>\\n<li><strong>Yuesong Huang</strong> (<a href=\\\"mailto:yhu116@u.rochester.edu\\\">yhu116@u.rochester.edu</a>)</li>\\n<li><strong>Junhua Huang</strong> (<a href=\\\"mailto:jhuang77@u.rochester.edu\\\">jhuang77@u.rochester.edu</a>)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>📄 License</h2><a href=\\\"#-license\\\"></a><p></p>\\n<p>This project is licensed under the BSD 3-Clause License – see the <a href=\\\"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe/blob/main/LICENSE\\\">LICENSE</a> file for details.</p>\\n<hr/>\\n</article></div></div>\\n\", \"is_error\": false, \"results\": [{\"title\": \"ANSWER\", \"text\": \"Winning Kaggle solutions for text classification that also have metadata usually treat the non‑text fields as first‑class inputs rather than an after‑thought.  A common recipe is to **encode the text with a pre‑trained transformer** (e.g., BERT, RoBERTa) and **encode each metadata column separately** – categorical variables with embedding layers, numeric variables with scaling or bucketisation – then concatenate all vectors before the final classification head.  This “dual‑tower” design lets the model learn interactions between language and auxiliary signals while keeping each modality in a suitable representation space ([FreeAgent blog](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  \\n\\nA simpler but surprisingly effective trick is to **inject metadata directly into the transformer’s token stream**.  By joining the raw fields with a special separator token (e.g., `[SEP]`) the model sees the amount, category, timestamp, etc., as part of the same sequence, which often yields a quick performance bump and works well when the metadata can be expressed as short text ([FreeAgent blog](https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification)).  More sophisticated pipelines build **dedicated neural blocks for each metadata type** (image, audio, timestamps, sender IDs) and train them jointly with the text encoder, allowing cross‑modal attention and richer feature fusion; the authors of the MESSAGENET paper report consistent gains over plain‑text baselines ([arXiv 2301.01808](https://export.arxiv.org/pdf/2301.01808v1.pdf)).  \\n\\nBeyond architecture, top teams also exploit **metadata‑driven weak supervision**.  The META framework treats metadata (author, venue, time) as a source of noisy label cues, constructs a text‑rich graph, extracts high‑confidence “seed motifs”, and bootstraps a classifier that iteratively expands both seed words and motifs.  This approach has shown superior accuracy on real‑world datasets where metadata carries strong label signals ([EMNLP 2020 META](https://aclanthology.org/2020.emnlp-main.670.pdf)).  In practice, winning solutions combine these ideas with classic Kaggle tricks—large‑scale pre‑training, careful cross‑validation, and ensembling transformer‑based models with gradient‑boosted trees that consume the raw metadata features ([Neptune tips](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions)).  Together, these strategies turn metadata from a peripheral attribute into a powerful driver of classification performance.\", \"url\": \"\"}, {\"title\": \"Text Classification: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle’s top NLP competitions.\\n\\nNamely, I’ve gone through:\\n\\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) – $65,000\\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) – $35,000\\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) – $25,000\\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) – $25,000\\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) – $50,000\\n\\nand found a ton of great ideas.\\n\\nWithout much lag, let’s begin.\\n\\n## Dealing with larger datasets\\n\\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\\n\\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\\n- Use [cudf](https://github.com/rapidsai/cudf)\\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\\n\\n## Small datasets and external data\\n\\nBut, what can one do if the dataset is small? Let’s see some techniques to tackle this situation.\\n\\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\\n\\nLet’s see some of the external datasets.\\n\\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\\n- [Stackexchange data](https://archive.org/download/stackexchange)\\n- Prepare a dictionary of commonly misspelled words and corrected words.\\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\\n\\n## Data exploration and gaining insights\\n\\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\\n\\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in  R for Quora data\\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\\n\\n## Data cleaning\\n\\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\\n\\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\\n\\n## Text representations\\n\\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\\n\\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\\n\\nContextual embeddings models\\n\\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\\n- [GPT](https://github.com/openai/finetune-transformer-lm)\\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\\n- [XLNET](https://github.com/zihangdai/xlnet/)\\n\\n## Modeling\\n\\n### Model architecture\\n\\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\\n\\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\\n- [Bi...\", \"url\": \"https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions\"}, {\"title\": \"Combining text with numerical and categorical features for classification\", \"text\": \"[Skip to content](https://engineering.freeagent.com/engineering.freeagent.com#content)\\n\\n# Classification with transformer models\\n\\nA common approach for classification tasks with text data is to fine-tune a pre-trained transformer model on domain-specific data. At FreeAgent we apply this approach to [automatically categorise bank transactions](https://www.freeagent.com/blog/how-freeagent-uses-ai/), using raw inputs that are a mixture of text, numerical and categorical data types.\\n\\nThe current approach is to concatenate the input features for each transaction into a single string before passing to the model.\\n\\nFor example a transaction with the following set of features:\\n\\n|     |     |     |\\n| --- | --- | --- |\\n| **Description** | **Amount** | **Industry** |\\n| Plastic pipes | £-7.99 | Plumbing |\\n\\ncould be represented by the string:  `Plastic pipes [SEP] -7.99 [SEP] Plumbing` with `[SEP]` being a special token indicating the boundaries between sentences, which is used to separate the features. This is a simple way to combine features that have yielded positive performance. We’ve even noticed that the model exhibits unexpectedly good interpretations of the amount feature.\\n\\nThis approach however doesn’t feel like the most natural way to represent the information contained in a numerical feature like the transaction amount. An alternative approach is to use the transformer model to extract a vector representation of the text features, concatenate this with features extracted from non-text data types and pass the combination to a classifier.\\n\\nIn this post we compare the performance of these two approaches using a dataset of wine reviews to classify wines into 3 classes (neutral, good and excellent). Diagrams 1 and 2 below give an overview of the process for getting from input features to output categories in the two approaches.\\n\\nDiagram 1: Concatenating all inputs into a single string to fine-tune a base transformer model.\\n\\nDiagram 2: Preprocessing numerical and categorical features independently from the text, extracting the description vector representation and train a lightGBM classifier.\\n\\n# Public wine reviews dataset\\n\\nWe use the [Kaggle wine reviews dataset](https://www.kaggle.com/datasets/zynicide/wine-reviews) for this experiment, which contains a variety of descriptive features for just under 130,000 wines, including a wine score. The wine score is a number between 1-100, with 100 being the best, although this dataset only includes reviews for wines that score over 80. For the purpose of this blog post, we created the wine rating from the scores – wines between 80 and 86 (incl) were flagged as “neutral”, between 87 and 93 as “good” and greater than 94 as “excellent”.\\n\\nTo keep things simple we will use just the wine description, price and variety to attempt to classify the wines into these 3 ratings.\\n\\nWe loaded the data and created a target variable `rating` to split the wine scores into the 3 target categories (neutral, good, excellent).\\n\\n```\\nimport pandas as pd\\n\\nwine_df = pd.read_csv(\\\"data/wine_data.csv\\\")\\nbins = [0, 87, 94, np.inf]\\nnames = [\\\"neutral\\\", \\\"good\\\", \\\"excellent\\\"]\\n\\nwine_df[\\\"rating\\\"] = pd.cut(wine_df[\\\"points\\\"], bins, labels=names)\\n\\n```\\n\\nWe can then keep a DataFrame of the features and target. For the purpose of putting together this example we have limited the model input features to the wine variety (the type of grapes used to make the wine), the price (cost for a bottle of wine) and the wine description.\\n\\n```\\nNUMERICAL_FEATURE = \\\"price\\\"\\nCATEGORICAL_FEATURE = \\\"variety\\\"\\nTEXT_FEATURE = \\\"description\\\"\\nTARGET = \\\"rating\\\"\\nFEATURES = [TEXT_FEATURE, NUMERICAL_FEATURE, CATEGORICAL_FEATURE]\\n\\nwine_df = wine_df[FEATURES + [TARGET]]\\n\\n```\\n\\nWe then split our wine dataframe into an 80:20 training:evaluation split.\\n\\n```\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_df, test_df = train_test_split(wine_df, test_size=0.2)\\n\\n```\\n\\n## LightGBM classifier approach\\n\\n### Preprocessing\\n\\n#### Preprocessing numerical and categorical features\\n\\nIn a first step we will preprocess the numerical and categorical features using SKLearn’s ColumnTransformer.  For the numerical features (price) we decided to fill missing values using the median wine price and scale them using a StandardScaler.\\n\\nWe preprocessed the categorical features by filling missing values with “other” and OneHot encoded them.\\n\\nWe saved the output of the ColumnTransformer as a pandas DataFrame to concatenate it later with the vector representation of the text.\\n\\n```\\nimport numpy as np\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import (\\n    OneHotEncoder,\\n    StandardScaler,\\n)\\n\\ndef preprocess_number():\\n    return make_pipeline(\\n        SimpleImputer(strategy=\\\"median\\\"),\\n        StandardScaler(),\\n    )\\n\\ndef preprocess_categories():\\n    return make_pipeline(\\n       SimpleImputer(strategy=\\\"constant\\\", fill_value=\\\"other\\\", missing_values=np.nan),\\n       OneHotEncoder(handle_unknown=\\\"ignore\\\", sparse_output=False),\\n    )\\n\\ndef create_preprocessor():\\n\\n    transformers = [\\n        (\\\"num_preprocessor\\\", preprocess_number(), [NUMERICAL_FEATURE]),\\n        (\\\"cat_preprocessor\\\", preprocess_categories(), [CATEGORICAL_FEATURE]),\\n    ]\\n\\n    return ColumnTransformer(transformers=transformers, remainder=\\\"drop\\\")\\n\\ncolumn_transformer = create_preprocessor()\\ncolumn_transformer.set_output(transform=\\\"pandas\\\")\\npreprocessed_num_cat_features_df = column_transformer.fit_transform(\\n    train_df[[NUMERICAL_FEATURE, CATEGORICAL_FEATURE]]\\n)\\n\\n```\\n\\n#### Extracting text vector representation with a transformer model\\n\\nWe then moved on to the preprocessing of the text features. We decided to use `distilbert-base-uncased` as the base transformer model to extract the vector representation of the wine description.\\n\\nBERT-type models use stacks of [transformer encoder layers](https://arxiv.org/pdf/1706.03762.pdf). Each of these blocks processes the tokenized text through a multi-headed self-attention step and a feed-forward neural network, before passing outputs to the next layer. The hidden state is the output of a given layer. The \\\\[CLS\\\\] token (short for classification) is a special token that represents an entire text sequence. We choose the hidden state of the \\\\[CLS\\\\] token in the final layer as the vector representation of our wine descriptions.\\n\\nIn order to extract the \\\\[CLS\\\\] representations, we first transform the text inputs into a Dataset of tokenized PyTorch tensors.\\n\\nFor this step we tokenized batches of 128 wine descriptions padded to a fixed length of 120 suitable for our wine descriptions of ~40 words. The max\\\\_length is one of the parameters which should be adjusted depending on the length of the text feature to prevent truncating longer inputs. The padding is necessary to ensure we will process fixed-shape inputs with the transformer model. The tokenizer returns a dictionary of input\\\\_ids, attention\\\\_mask and token\\\\_type\\\\_ids. Only the input\\\\_ids (the indices of each token in the wine description) and the attention mask (binary tensor indicating the position of the padded indice) are required inputs to the model.\\n\\nThe code for the tokenization is shown below:\\n\\n```\\nfrom datasets import Dataset\\nfrom transformers import AutoTokenizer\\n\\nMODEL_NAME = \\\"distilbert-base-uncased\\\"\\n\\ndef tokenized_pytorch_tensors(\\n        df: pd.DataFrame,\\n        column_list: list\\n    ) -> Dataset:\\n\\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n    transformers_dataset = Dataset.from_pandas(df)\\n\\n    def tokenize(model_inputs_batch: Dataset) -> Dataset:\\n        return tokenizer(\\n            model_inputs_batch[TEXT_FEATURE],\\n            padding=True,\\n            max_length=120,\\n            truncation=True,\\n        )\\n\\n    tokenized_dataset = transformers_dataset.map(\\n        tokenize,\\n        batched=True,\\n        batch_size=128\\n    )\\n\\n    tokenized_dataset.set_format(\\n        \\\"torch\\\",\\n        columns=column_list\\n    )\\n\\n    columns_t...\", \"url\": \"https://engineering.freeagent.com/2024/05/17/combining-text-with-numerical-and-categorical-features-for-classification\"}, {\"title\": \"\", \"text\": \"Adar Kahana adarkahana@microsoft.com\\nMicrosoft R&D center\\nILDC\\nOren Elisha orelisha@microsoft.com\\nMicrosoft R&D center\\nILDC\\nMESSAGENET: MESSAGE CLASSIFICATION USING NATURAL LANGUAGE PROCESSING AND META-DATA\\nMessage classification · Meta data injection · Deep learning · Natural language processing\\nIn this paper we propose a new Deep Learning (DL) approach for message classification. Our method is based on the state-of-the-art Natural Language Processing (NLP) building blocks, combined with a novel technique for infusing the meta-data input that is typically available in messages such as the sender information, timestamps, attached image, audio, affiliations, and more. As we demonstrate throughout the paper, going beyond the mere text by leveraging all available channels in the message, could yield an improved representation and higher classification accuracy. To achieve message representation, each type of input is processed in a dedicated block in the neural network architecture that is suitable for the data type. Such an implementation enables training all blocks together simultaneously, and forming cross channels features in the network. We show in the Experiments Section that in some cases, message's meta-data holds an additional information that cannot be extracted just from the text, and when using this information we achieve better performance. Furthermore, we demonstrate that our multi-modality block approach outperforms other approaches for injecting the meta data to the the text classifier.Many real world applications require message classification and regression, such as handling spam emails [1], ticket routing [2], article sentiment review [3] and more. Accurate message classification could improve critical scenarios such as in call centers (routing tickets based on topic) [2], alert systems (flagging highly important alert messages) [4], and categorizing incoming messages (automatically unclutter emails)[1,5]. The main distinction between text and message classification is the availability of additional attributes, such as the sender information, timestamps, attached image, audio, affiliations, and more. New message classification contests often appear in the prominent platforms (i.e., Kaggle [6]), showing how this topic is sought after. There are already many data-sets to explore in this field, but no clear winner algorithm that fits all scenarios with high accuracy, efficiency and simplicity (in terms of implementation and interpretation).A notable advancement in the field of NLP is the attention based transformers architecture[7]. This family of methods excels in finding local connections between words, and better understanding the meaning of a sentence. A leading example is the Bidirectional Encoder Representations from Transformers (BERT) [8] as well as its variations [9, 10, 11], winning certain benchmarks[12,13]. Several packages, such as Huggingface Transformers[14], make such models accessible and easy to use as well as provide pre-trained versions. In addition, one can use transfer learning [15] to further train BERT on their on data, creating a tailored model for the specific task at hand.BERT, and often other transformer based models, are designed to handle text. They operate on the words of a given text by encoding them into tokens, and by the connections between the tokens they learn the context of sentences. This approach is limited, since sometimes more information can be extracted and used, not necessarily textual. Throughout this paper we refer to this information as meta-data to distinguish it from the main stream of textual content (though one may recognize it as the core data, depending on the application). For example, a meta-data could be the time stamp of when the text was written, sent, published, etc. Another example is the writer of the text, when dealing with a small\\nlist of writers of a corpus. There have been some attempts to incorporate these into BERT models, for example by assigning artificial tokens for writers or for temporal segments (token per month for example) [16]. This approach is limited since not all meta-data entries are suitable for encoding by tokenization. In the example of temporal segments, more segments introduce more tokens, leading to large computational resources consumption, and less segments cause loss of information. Another approach is to concatenate the embeddings, created by the transformer module, with the outputs of an embedding module for the meta-data. In this approach, a transformer for the text is trained (using direct or transfer learning) on the text, and other separate modules (time series embedding, senders embeddings, etc.) are used to embed the meta-data. All the embeddings are\", \"url\": \"https://export.arxiv.org/pdf/2301.01808v1.pdf\"}, {\"title\": \"\", \"text\": \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8351–8361,\\nNovember 16–20, 2020. \\rc 2020 Association for Computational Linguistics\\n8351\\nMETA: Metadata-Empowered Weak Supervision for Text Classification\\nDheeraj Mekala1 Xinyang Zhang2 Jingbo Shang1,3\\n1 Department of Computer Science and Engineering, University of California San Diego, CA, USA\\n2 Department of Computer Science, University of at Illinois Urbana-Champaign, IL, USA\\n3 Halıcıoglu Data Science Institute, University of California San Diego, CA, USA ˘\\n1,3\\n{dmekala, jshang}@ucsd.edu 2 xz43@illinois.edu\\nAbstract\\nRecent advances in weakly supervised learn\\u0002ing enable training high-quality text classifiers\\nby only providing a few user-provided seed\\nwords. Existing methods mainly use text data\\nalone to generate pseudo-labels despite the\\nfact that metadata information (e.g., author\\nand timestamp) is widely available across var\\u0002ious domains. Strong label indicators exist in\\nthe metadata and it has been long overlooked\\nmainly due to the following challenges: (1)\\nmetadata is multi-typed, requiring systematic\\nmodeling of different types and their combi\\u0002nations, (2) metadata is noisy, some metadata\\nentities (e.g., authors, venues) are more com\\u0002pelling label indicators than others. In this\\npaper, we propose a novel framework, META,\\nwhich goes beyond the existing paradigm and\\nleverages metadata as an additional source of\\nweak supervision. Specifically, we organize\\nthe text data and metadata together into a\\ntext-rich network and adopt network motifs to\\ncapture appropriate combinations of metadata.\\nBased on seed words, we rank and filter mo\\u0002tif instances to distill highly label-indicative\\nones as “seed motifs”, which provide addi\\u0002tional weak supervision. Following a boot\\u0002strapping manner, we train the classifier and\\nexpand the seed words and seed motifs itera\\u0002tively. Extensive experiments and case stud\\u0002ies on real-world datasets demonstrate supe\\u0002rior performance and significant advantages of\\nleveraging metadata as weak supervision.\\n1 Introduction\\nWeakly supervised text classification has recently\\ngained much attention from the researchers because\\nit reduces the burden of annotating the data. So far,\\nthe major source of weak supervision lies in text\\ndata itself (Agichtein and Gravano, 2000; Kuipers\\net al., 2006; Riloff et al., 2003; Tao et al., 2015;\\nMeng et al., 2018; Mekala and Shang, 2020). These\\nmethods typically require a few user-provided seed\\nPaper Authors Year Category\\nP1 G. Hinton, S. Osindero, YW. Teh 2006 ML\\nP2 G. Hinton, O. Vinyals, J. Dean 2015 ML\\nP3 J. Dean, S.Ghemawat 2008 Sys\\n(a) Examples of research papers with metadata.\\nO. Vinyals\\nG. Hinton\\n2016\\nneural\\nsystem\\ndata\\nlearning\\nJ. Dean Doc\\nAuthor 1 Author 2\\nP2\\nO. Vinyals G. Hinton\\nP1 P2\\n… …\\n…\\n(b) A text-rich network view of \\nthe papers.\\n(c) A motif pattern and \\na motif instance.\\nFigure 1: Text corpus, text-rich network, and motif.\\nwords for each class as weak supervision. They ex\\u0002pand seed words with generated pseudo labels and\\nimprove their text classifier in an iterative fashion.\\nMetadata information (e.g., author, published\\nyear) in addition to textual information, is widely\\navailable across various domains (e.g., news arti\\u0002cles, social media posts, and scientific papers) and\\nit could serve as a strong, complementary weak\\nsupervision source. Take a look at the research\\npapers in Figure 1(a) as an example. It shall be\\nlearned in a data-driven manner that G. Hinton is a\\nhighly-reputed machine learning researcher, thus\\nhis presence is a strong indicator of a paper belong\\u0002ing to the Machine Learning category.\\nDistilling effective metadata for weak supervi\\u0002sion faces several major challenges. Metadata is\\noften multi-typed, each type and the type combi\\u0002nations could have very different semantics and\\nmay not be equally important. Moreover, even en\\u0002tities within a single metadata type could be noisy.\\nContinuing our example in Figure 1(a), we shall\\nnotice that year is less helpful than an author to do\\nclassification. Among the authors, J. Dean might\\nbe an important figure but has research interests\\n8352\\nm1,1 .95 .01 .04\\nm1,2 .32 .30 .38\\nText Classifier\\nSeed Words ① Pseudo Label Generation ③ Motif Ranking & …\\nC1 Neural Network\\nC2 Image Classification\\nC3 Translation\\nPaper\\nAuthor 1 Author 2\\nM1\\nM2\\nm2,1 .03 .89 .08\\nm2,2 .31 .26 .43\\nInstance C1 C2 C3\\nKlein Manning\\nP1 P2\\n② Classifier Training / Prediction\\nRecursion\\nUser Given\\nMotif Pattern\\nMatched (Seed)\\nMotif Instances\\n…\\n…\\nC1 m1,1 m1,6 m2,3 m1,4 …\\nC2 m2,1 m2,4 m1,2 m1,4 …\\nC3 m1,3 m2,5 m2,2 m1,7 …\\n... Expansion\\nExpand from high score to low\\nNew seed set cutoff\\nFigure 2: Our META framework. In each iteration, we generate pseudo labels for documents, train the text classifier,\\nand rank all words and motif instances in a unified ranking framework. We then expand seed sets until an automatic\\ncutoff is reached. The quality of the classifier and the seed sets are improved through iterations.\\nspanning across different domains. However, if we\\njoin the author with year, it carries more accurate\\nsemantics, and we may discover J. Dean has more\\ninterest in machine learning in recent years, thus\\nbecoming highly label-indicative.\\nBearing the challenges in mind, we propose\\nMETA, a principled framework for metadata\\u0002empowered weakly-supervised text classification.\\nAs illustrated in Figure 1 and Figure 2, we first\\norganize the text data and metadata together into\\na text-rich network. The network structure gives\\nus a holistic view of the corpus and enables us to\\nrank and select useful metadata entities. We lever\\u0002age motif patterns (Benson et al., 2016; Milo et al.,\\n2002; Shang et al., 2020) to model typed metadata\\nas well as their combinations. A motif pattern is\\na subgraph pattern at the meta-level that captures\\nhigher-order connections and the semantics repre\\u0002sented by these connections. It serves as a useful\\ntool to model typed edges, typed paths (a.k.a. meta\\u0002paths) (Sun et al., 2011), and higher-order struc\\u0002tures in the network. With little effort, users can\\nspecify a few possibly useful motif patterns as in\\u0002put to our model. We develop a unified, principled\\nranking mechanism to select label-indicative motif\\ninstances and words, forming expanded weak su\\u0002pervision. Note that, such instance-level selection\\nprocess also implicitly refines the motif patterns,\\nensuring the robust performance of META even\\nwhen irrelevant motif patterns exist in input. It is\\nworth a mention that META is compatible with any\\ntext classifiers.\\nOur contributions are summarized as follows:\\n• We explore to incorporate metadata information\\nas an additional source of weak supervision for\\ntext classification along with seed words.\\n• We propose a novel framework META, which in\\u0002troduces motif patterns to capture the high-order\\ncombinations among different types of metadata\\nand conducts a unified ranking and selection of\\nlabel-indicative motif instances and words.\\n• We conduct experiments on two real-world\\ndatasets. The results and case studies demon\\u0002strate the superiority of incorporating metadata\\nas parts of weak supervision and verify the effec\\u0002tiveness of META.\\nReproducibility. Our code is made publicly avail\\u0002able at GitHub1\\n.\\n2 Preliminaries\\n2.1 Documents as Text-rich Network\\nGiven a collection of n text documents D =\\n{D1, D2, . . . , Dn}, and their corresponding meta\\u0002data, we propose to organize them into a text-rich\\nnetwork, as illustrated in Figure 1(b). A text-rich\\nnetwork is a heterogeneous network with docu\\u0002ments, words, different types of metadata as nodes,\\nand their associations as edges. For example, our\\ntext-rich network for research papers has papers,\\nwords, authors, and publication years as nodes.\\nEach paper is connected to its associated words\\nand metadata nodes. Such a network provides a\\nholistic and structured representation of the input.\\n2.2 Seed Words and Motif Patterns\\nUsers are asked to provide a few seed words S\\n= {Sw\\n1\\n, S\\nw\\n2\\n, . . . , S\\nw\\nl\\n} for each of l classes (i.e.,\\nC1, C2, . . . , ...\", \"url\": \"https://aclanthology.org/2020.emnlp-main.670.pdf\"}, {\"title\": \"20% Accuracy Bump in Text Classification with ME-ULMFiT\", \"text\": \"[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4545e993aa78&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\n\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n\\n# 20% Accuracy Bump in Text Classification with ME-ULMFiT\\n\\n[![Matthew Teschke](https://miro.medium.com/v2/resize:fill:88:88/0*nsQhCoB0HTmu_sw5.)](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)[![Novetta](https://miro.medium.com/v2/resize:fill:48:48/1*FLKm3lgYky637lUrGUFvrA.png)](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n[Matthew Teschke](https://medium.com/@mteschke?source=post_page-----4545e993aa78--------------------------------)\\n\\n·\\n\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb8201a221b3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&user=Matthew+Teschke&userId=cb8201a221b3&source=post_page-cb8201a221b3----4545e993aa78---------------------post_header-----------)\\n\\nPublished in\\n\\n[Novetta](https://medium.com/novetta?source=post_page-----4545e993aa78--------------------------------)\\n\\n·\\n\\n5 min read\\n\\n·\\n\\nMar 31, 2020\\n\\n10\\n\\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D4545e993aa78&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnovetta%2F20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78&source=-----4545e993aa78---------------------post_audio_button-----------)\\n\\nShare\\n\\n_By incorporating metadata into the text before training, we see as much as a 20% increase in accuracy_\\n\\n# Introduction\\n\\nIn a [previous post](https://www.novetta.com/2019/03/introducing_me_ulmfit/), we discussed an enhanced method of passing additional data to the ULMFiT¹ algorithm for unstructured text classification, which improved accuracy by up to 9%. In follow-on experiments, we have demonstrated that this approach — in which we prepend metadata to the text to provide more information to the model — increased accuracy by up to 20%, in relative terms. This improvement required just a few lines of Python, such that the performance boost comes at essentially no cost.\\n\\nThis blog post explains the findings of this more-detailed evaluation, where we have compared how the following affect accuracy:\\n\\n1. Each individual piece of metadata\\n2. Different combinations of metadata\\n3. Different ways of passing the metadata to the model\\n\\nFor the evaluation, we worked with six [Novetta Mission Analytics](https://www.novetta.com/products/novetta-mission-analytics/) datasets, described below. The objective is to assign metadata tags to quotes, using granular categories called Submessages.\\n\\nSome highlights from our results include:\\n\\n- Source-related metadata tags consistently performed the best, confirming our original hypothesis. Sources (e.g. UN Official) are the metadata most closely associated with the quotes we are tagging, so it makes sense that sources add the most information.\\n- Including media name (e.g. New York Times) either decreased accuracy or showed little improvement. We found this surprising as we had assumed that including metadata would only improve accuracy.\\n- Adding more metadata tags to the model did not always improve performance.\\n- For five of the six datasets, using metadata tags that distinguish between columns in our data performed better than the approach (currently suggested by the fast.ai library) in which each field was demarcated by xxfld.\\n\\n# Dataset Overview\\n\\nWe conducted evaluations on the six datasets listed in Table 1. For each dataset, we made the following splits time-based between training and validation, where later data is held out for validation. We used 80% of the data for training and 17% for validation on the language model. Using that 97% of the data, we then trained the classifier, reserving the last 3% for the final validation.\\n\\n![](https://miro.medium.com/v2/resize:fit:700/1*WtQcwQm1EIIoHz3Gy6EoaA.png)\\n\\nTable 1: Characteristics of Datasets Used in ME-ULMFiT Evaluation\\n\\n# Experiment 1: Individual Metadata\\n\\nOur first experiment evaluated how individual pieces of metadata impacted performance of the model. Working with six types of metadata (Author Name, Media Name, Media Nation, Source Name, Source Nation, Source Type), we created a model that prepended only that piece of information to the quote text. For example, a Source Name of “Steffen Seibert” prepended to a quote results in:\\n\\n```\\nsource_name source_name_Steffen_Seibert quote_text that “we have started down the path” towards the NATO goal.\\n```\\n\\nThe results of Experiment 1 are shown in the following heat map:\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*6-wTYPDfG1meArIiwnA1oA.png)\\n\\nFigure 1: Heat map showing relative change in accuracy of ME-ULMFiT with individual types of metadata compared to the baseline ULMFiT model with no metadata\\n\\nFrom the heat map, we made the following observations:\\n\\n- Source-related metadata tags generally performed the best\\n- The Media Name metadata tag performed the worst for four out of six datasets\\n- Datasets with fewer Submessage labels saw less improvement from the inclusion of metadata than those with more Submessage labels\\n\\n# Experiment 2: Combinations of Metadata\\n\\nIn the second experiment, we evaluated how different combinations of metadata impacted model accuracy. Our intuition suggested that more metadata would always lead to better performance, but as shown in Figure 2, this was not the case. We will be conducting further investigation into why.\\n\\nWe examined five different combinations of metadata as follows:\\n\\n- The top-2 performing metadata tags from Experiment 1\\n- The top-3 performing metadata tags from Experiment 1\\n- The top-4 performing metadata tags from Experiment 1\\n- All metadata tags\\n- Fast.ai built-in tags, which utilized all metadata but whose fields are separated by xxfld instead of column-specific tags. In this method, values within the data are not concatenated (e.g., The New York Times was not converted to xxfld\\\\_The\\\\_New\\\\_York\\\\_Times)\\n\\n![](https://miro.medium.com/v2/resize:fit:432/1*UQW6om7aQzCpUEp9pvyQpQ.png)\\n\\nFigure 2: Heat map showing relative change in accuracy of ME-ULMFiT with combinations of metadata compared to the baseline ULMFiT model with no metadata\\n\\nWhile we see that more often than not multiple tags improve performance over the best baseline, that is not always the case. For only one dataset did the model with all metadata perform the best and it generally underperformed the most accurate individual metadata model. We believe this has to do with the way batches are created within the model, so we will explore further to see why more data is not always improving accuracy.\\n\\n# Experiment 3: Methods of Passing Metadata\\n\\nLastly, for the models that included all metadata, we wanted to compare different methods of passing in metadata. Our [original post](https://www.novetta.com/2019/03/introducing_me_ulmfit/) describes our method of using separate tags for each metadata field and concatenating all valu...\", \"url\": \"https://medium.com/novetta/20-accuracy-bump-in-text-classification-with-me-ulmfit-4545e993aa78\"}, {\"title\": \"A Journey Through Kaggle Text Data Competitions From 2021 to 2023\", \"text\": \"A Journey Through Kaggle Text Data Competitions From 2021 to 2023 | by Salomon Marquez | Medium\\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n[\\nWrite\\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n[\\nSearch\\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\nSign up\\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\\n# A Journey Through Kaggle Text Data Competitions From 2021 to 2023\\n[\\n![Salomon Marquez](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*hbSxb57OAqPdRqks)\\n](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n[Salomon Marquez](https://medium.com/@sblaizer?source=post_page---byline--631a79f96312---------------------------------------)\\n18 min read\\n·Apr 28, 2024\\n[\\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;user=Salomon+Marquez&amp;userId=1a60dea97a2c&amp;source=---header_actions--631a79f96312---------------------clap_footer------------------)\\n--\\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/631a79f96312&amp;operation=register&amp;redirect=https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312&amp;source=---header_actions--631a79f96312---------------------bookmark_footer------------------)\\nListen\\nShare\\n*Authors:*[*Liliana Badillo*](https://www.kaggle.com/sophieb)*and*[*Salomon Marquez*](https://www.kaggle.com/sblaizer)\\n**> Note:\\n**> I created this Medium post to provide more visibility to our original report. Please refer to our [> A Journey Throuhgh Text Data Competitions\\n](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions)> notebook for further details.\\nN**atural Language Processing (NLP)**, the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine-learning techniques.\\n*> “It is estimated that 90% of the world’s data was generated in the last two years alone”\\n*> by [> Amount of Data Created Daily (2023)\\n](https://explodingtopics.com/blog/data-generated-per-day)\\n[**Kaggle**](https://www.kaggle.com/)is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.\\nAccessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups.**Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.**\\nIn this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on**four pivotal topics**that have emerged as the foundation for successful solutions in recent years:**model architectures**,**pseudo labeling**,**adversarial weight perturbation**, and**mask language modeling**. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.\\n## Methodology\\nTo gain a better understanding of the text data landscape, we carried out the following tasks:\\n* **Analyze 27 write-ups from nine Kaggle competitions specifically related to text data**. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see**Figure 1**. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.\\n* **Perform a general but not exhaustive analysis of the**[**arXiv dataset**](https://www.kaggle.com/datasets/Cornell-University/arxiv). We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.\\nPress enter or click to view image in full size\\n![]()\\n**Figure 1.**Kaggle text data competitions held from 2021 to 2023\\n## Key Discoveries of Text Data Competitions\\nThe result of the analysis of the 27 write-ups and their most notable comments can be seen in[Table I of Appendix A](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-A)and[Table II of Appendix B](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions#Appendix-B), respectively.**Table I**provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand,**Table II**presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to[EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets)notebook.\\nAs**Figure 2**shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, while the remaining targeted text matching and question-and-answer tasks. Interestingly,**Figure 3**shows that with the frequency of the term “question answering” exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems ...\", \"url\": \"https://medium.com/@sblaizer/a-journey-through-kaggle-text-data-competitions-from-2021-to-2023-631a79f96312\"}, {\"title\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\", \"text\": \"How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\\nDiscover Anything**\\n[\\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\\nHackernoon](https://hackernoon.com/)\\nSignup[Write](https://app.hackernoon.com/new)\\n******\\n**139reads\\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\\nby\\n**neptune.ai Jakub Czakon**\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\nAugust 23rd, 2020\\nTLDR**\\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\\nbyneptune.ai Jakub Czakon@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\nSubscribe\\n****\\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\\n[\\n←Previous\\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\\n### About Author\\n[](https://hackernoon.com/u/neptuneAI_jakub)\\n[**neptune.ai Jakub Czakon**@neptuneAI\\\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\\n#### Comments\\n#### TOPICS\\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\\n#### THIS ARTICLE WAS FEATURED IN\\n[\\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\\n**\\nArweave\\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\\nViewBlock\\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\\n](https://www.joyk.com/dig/detail/1707491664448324)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/48760739)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\\n](https://unni.io/domains/hackernoon.com/page/50536904)\\n#### Related Stories\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\\nNov 11, 2023\\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\\nAug 15, 2023\\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\\n[Bala Priya C](https://hackernoon.com/u/balapriya)\\nFeb 03, 2022\\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\\nJan 28, 2023\\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\\nNov 09, 2020\\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\\nMar 23, 2020\\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\\n[#METRICS](https://hackernoon.com/tagged/metrics)\\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...\", \"url\": \"https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh\"}, {\"title\": \"GitHub - AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe: Classifies 500K+ political tweets from Northern Europe using NLP and machine learning to analyze political discourse.\", \"text\": \"<div><div><article><p></p><h2>🐦 Kaggle Project: Classification of Tweets from Northern Europe</h2><a href=\\\"#-kaggle-project-classification-of-tweets-from-northern-europe\\\"></a><p></p>\\n<p><a href=\\\"https://camo.githubusercontent.com/439fd5ffe2ac21b3daa03ac6474978163be089da5fbba2790769ae2275464e35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\\\"></a></p>\\n<p></p><h2>🚀 Overview</h2><a href=\\\"#-overview\\\"></a><p></p>\\n<p>This project analyzes a dataset of <strong>509,031 tweets</strong> from politicians across <strong>seven Northern European countries</strong>, aiming to classify tweets by <strong>political spectrum (Left, Right, Center)</strong> and <strong>geography</strong>. Using <strong>Natural Language Processing (NLP)</strong> and machine learning, we extracted key insights into political discourse and developed a classifier with strong performance.</p>\\n<hr/>\\n<p></p><h2>📊 Dataset</h2><a href=\\\"#-dataset\\\"></a><p></p>\\n<ul>\\n<li><strong>Countries:</strong> Belgium, Denmark, Iceland, Ireland, Netherlands, Norway, Sweden</li>\\n<li><strong>Time span:</strong> Dec 2008 – Jan 2023</li>\\n<li><strong>Dataset size:</strong> 509,031 tweets\\n<ul>\\n<li><strong>Training set:</strong> 407,223 tweets</li>\\n<li><strong>Test set:</strong> 101,808 tweets</li>\\n</ul>\\n</li>\\n</ul>\\n<p><strong>Attributes:</strong></p>\\n<ul>\\n<li>Tweet text</li>\\n<li>Country</li>\\n<li>Gender</li>\\n<li>Political spectrum label (for supervised learning)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>🔬 Methods</h2><a href=\\\"#-methods\\\"></a><p></p>\\n<p></p><h3>Data Cleaning &amp; Preprocessing</h3><a href=\\\"#data-cleaning--preprocessing\\\"></a><p></p>\\n<ul>\\n<li>Text cleaning: Lowercasing, punctuation removal, stopword removal, lemmatization.</li>\\n<li>New feature creation: <code>text_clean_country_gender_user</code> combining multiple metadata.</li>\\n</ul>\\n<p></p><h3>Feature Engineering</h3><a href=\\\"#feature-engineering\\\"></a><p></p>\\n<ul>\\n<li><strong>Vectorization:</strong>\\n<ul>\\n<li>Count Vectorizer</li>\\n<li>TF-IDF transformation</li>\\n</ul>\\n</li>\\n</ul>\\n<p></p><h3>Modeling</h3><a href=\\\"#modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>Classifier:</strong> Linear Support Vector Classifier (LinearSVC)</li>\\n<li><strong>Validation:</strong> 10-fold cross-validation</li>\\n</ul>\\n<p></p><h3>Topic Modeling</h3><a href=\\\"#topic-modeling\\\"></a><p></p>\\n<ul>\\n<li><strong>LDA (Latent Dirichlet Allocation)</strong></li>\\n<li><strong>NMF (Non-negative Matrix Factorization)</strong></li>\\n</ul>\\n<hr/>\\n<p></p><h2>📈 Results</h2><a href=\\\"#-results\\\"></a><p></p>\\n<p>✅ <strong>Key findings:</strong></p>\\n<ul>\\n<li>Tweets span diverse topics from <strong>domestic politics</strong> to <strong>international relations</strong>.</li>\\n<li>Different political leanings across countries: e.g., Netherlands leans Center, Iceland leans Left.</li>\\n<li>Gender distribution varies: Sweden had a <strong>female-majority</strong>, others skewed male.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>🛠️ Requirements</h2><a href=\\\"#️-requirements\\\"></a><p></p>\\n<ul>\\n<li>Python 3.8+</li>\\n<li>Key libraries:\\n<ul>\\n<li>pandas, numpy</li>\\n<li>scikit-learn</li>\\n<li>nltk, spaCy</li>\\n<li>gensim</li>\\n<li>matplotlib, seaborn</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Install via:</p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<hr/>\\n<p></p><h2>🖥️ Usage</h2><a href=\\\"#️-usage\\\"></a><p></p>\\n<p>1️⃣ <strong>Clone the repository:</strong></p>\\n<div><pre>git clone https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe.git\\n<span>cd</span> kaggle-project-classification-of-tweets-from-northern-europe</pre></div>\\n<p>2️⃣ <strong>Install dependencies:</strong></p>\\n<div><pre>pip install -r requirements.txt</pre></div>\\n<p>3️⃣ <strong>Run analysis:</strong></p>\\n<ul>\\n<li>Open <code>kaggle_project.ipynb</code> for full preprocessing, modeling, and evaluation workflows.</li>\\n</ul>\\n<hr/>\\n<p></p><h2>📂 Repository Structure</h2><a href=\\\"#-repository-structure\\\"></a><p></p>\\n<div><pre><code>├── data/ # Dataset (not included)\\n├── kaggle_project.ipynb # Main Jupyter notebook\\n├── kaggle_project_report.pdf # Final project report\\n├── README.md\\n├── requirements.txt # Dependencies\\n└── LICENSE\\n</code></pre></div>\\n<hr/>\\n<p></p><h2>👥 Contributors</h2><a href=\\\"#-contributors\\\"></a><p></p>\\n<ul>\\n<li><strong>Yuesong Huang</strong> (<a href=\\\"mailto:yhu116@u.rochester.edu\\\">yhu116@u.rochester.edu</a>)</li>\\n<li><strong>Junhua Huang</strong> (<a href=\\\"mailto:jhuang77@u.rochester.edu\\\">jhuang77@u.rochester.edu</a>)</li>\\n</ul>\\n<hr/>\\n<p></p><h2>📄 License</h2><a href=\\\"#-license\\\"></a><p></p>\\n<p>This project is licensed under the BSD 3-Clause License – see the <a href=\\\"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe/blob/main/LICENSE\\\">LICENSE</a> file for details.</p>\\n<hr/>\\n</article></div></div>\", \"url\": \"https://github.com/AnakinHuang/kaggle-project-classification-of-tweets-from-northern-europe\"}]}",
        "tool.duration_ms": 7795.800685882568
      }
    }
  ],
  "metadata": {
    "span_count": 9,
    "last_updated": "2026-01-09T06:23:10.659298"
  }
}