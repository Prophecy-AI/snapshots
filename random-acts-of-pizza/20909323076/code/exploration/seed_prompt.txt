## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: class distribution (75% negative, 25% positive), text length statistics, metadata feature distributions
- Moderate class imbalance (3:1 ratio) - see eda.ipynb for details
- Text features: request_title, request_text (avg 71 chars and 402 chars respectively)
- Metadata features: account age, karma scores, activity metrics, subreddit participation
- User flair is highly predictive: None (75%, low success), shroom (23.5%, high success), PIF (1.3%, 100% success)

## Models
For text classification with metadata features, winning Kaggle solutions typically use:

**Primary Models:**
- Gradient boosting (XGBoost/LightGBM) on engineered features - strong baseline for tabular data
- Logistic regression with TF-IDF features - effective for text classification
- Neural networks (BERT/RoBERTa) for text understanding, optionally combined with metadata
- Random Forest on combined feature set

**Ensembling Strategy:**
- Blend 3-5 diverse models: tree-based + linear + neural
- Weighted averaging based on validation performance
- Stacking with logistic regression as meta-learner often improves results

## Feature Engineering
**Text Features (from original Stanford paper & winning solutions):**
- TF-IDF vectors (unigrams, bigrams) on request_title and request_text
- Text length features: character count, word count, sentence count
- Readability scores: Flesch-Kincaid, automated readability index
- Politeness indicators: gratitude expressions ("thank", "appreciate"), apologies ("sorry")
- Reciprocity promises: "will", "promise", "in return", "exchange"
- Narrative markers: first-person pronouns ("I", "we", "my"), story-telling verbs
- Sentiment analysis scores (positive/negative sentiment)
- Topic modeling (LDA/NMF) to extract latent topics - use 10-20 topics
- SVD dimensionality reduction on TF-IDF matrices (50-100 components)

**Metadata Features:**
- All numeric activity metrics (account age, karma, comment/post counts)
- Ratios between request/retrieval timepoints to capture activity changes
- Subreddit diversity metrics (number of unique subreddits)
- User flair encoding: One-hot or target encoding (None/shroom/PIF)
- Karma ratios: upvotes_minus_downvotes / upvotes_plus_downvotes
- Activity density: comments per day, posts per day
- Interaction features: text_length * account_age, reciprocity * karma

**High-Cardinality Features:**
- requester_username: Use target encoding with smoothing, or count encoding
- requester_subreddits_at_request: Count encoding or hash encoding
- giver_username_if_known: Binary indicator (known vs N/A)

## Preprocessing
**Text Preprocessing:**
- Lowercasing, punctuation removal
- Stopword removal (optional, test both ways)
- Lemmatization or stemming
- Handle edited posts using request_text_edit_aware field
- Remove Reddit-specific markup and URLs

**Handling Class Imbalance:**
- Stratified K-fold validation (k=5) to preserve class distribution
- Consider oversampling minority class (SMOTE or random oversampling)
- Class weights in loss function: inverse frequency weighting
- Focal loss for neural networks
- Adjust decision threshold based on validation set

## Validation Strategy
- Stratified K-fold cross-validation is essential given class imbalance
- Use AUC-ROC for evaluation metric (competition standard)
- Early stopping based on validation AUC (patience=5-10)
- Monitor both per-fold and mean validation scores
- Use GroupKFold on requester_username to prevent leakage if same users appear multiple times

## Special Considerations
- Reddit-specific features: user flair indicates pizza history (very strong predictor)
- Temporal features: account age, activity timelines, request timing (hour of day, day of week)
- Text sincerity indicators: politeness, gratitude expressions, personal stories
- Community engagement: comments on request, upvote/downvote ratios
- Request effort signals: length, readability, narrative coherence
- Social proof: previous successful requests (indicated by flair)
- Altruism indicators: PIF flair users always successful (100% in training data)