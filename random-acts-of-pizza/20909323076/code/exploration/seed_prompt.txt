## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: class distribution (75% negative, 25% positive), text length statistics, metadata feature distributions
- Moderate class imbalance (3:1 ratio) - see eda.ipynb for details
- Text features: request_title, request_text (avg 71 chars and 402 chars respectively)
- Metadata features: account age, karma scores, activity metrics, subreddit participation

## Models
For text classification with metadata features, winning Kaggle solutions typically use:

**Primary Models:**
- Gradient boosting (XGBoost/LightGBM) on engineered features - strong baseline for tabular data
- Logistic regression with TF-IDF features - effective for text classification
- Neural networks (BERT/RoBERTa) for text understanding, optionally combined with metadata

**Ensembling Strategy:**
- Blend 3-5 diverse models: tree-based + linear + neural
- Weighted averaging based on validation performance
- Stacking with logistic regression as meta-learner often improves results

## Feature Engineering
**Text Features:**
- TF-IDF vectors (unigrams, bigrams) on request_title and request_text
- Text length features (character count, word count, sentence count)
- Sentiment analysis scores
- LIWC (Linguistic Inquiry and Word Count) features if available
- Topic modeling (LDA/NMF) to extract latent topics
- SVD dimensionality reduction on TF-IDF matrices

**Metadata Features:**
- All numeric activity metrics (account age, karma, comment/post counts)
- Ratios between request/retrieval timepoints to capture activity changes
- Subreddit diversity metrics
- User flair encoding (None/shroom/PIF)
- Interaction features between text length and metadata

## Preprocessing
**Text Preprocessing:**
- Lowercasing, punctuation removal
- Stopword removal (optional, test both ways)
- Lemmatization or stemming
- Handle edited posts using request_text_edit_aware field

**Handling Class Imbalance:**
- Stratified K-fold validation (k=5) to preserve class distribution
- Consider oversampling minority class (SMOTE or random oversampling)
- Class weights in loss function
- Focal loss for neural networks

## Validation Strategy
- Stratified K-fold cross-validation is essential given class imbalance
- Use AUC-ROC for evaluation metric (competition standard)
- Early stopping based on validation AUC
- Monitor both per-fold and mean validation scores

## Special Considerations
- Reddit-specific features: user flair indicates pizza history
- Temporal features: account age, activity timelines
- Text sincerity indicators: politeness, gratitude expressions
- Community engagement: comments on request, upvote/downvote ratios