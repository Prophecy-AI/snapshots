{
  "query": "What are winning approaches for text classification with meta features on Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Winning solutions for text\u2011classification contests on Kaggle often treat the problem as a hybrid of classic NLP and tabular learning: the raw text is turned into high\u2011dimensional sparse vectors (e.g., TF\u2011IDF or hashed n\u2011grams) and then concatenated with engineered \u201cmeta\u201d features such as text length, number of punctuation marks, presence of URLs, or domain\u2011specific identifiers. A common recipe is to clean the posts, compute a TF\u2011IDF matrix, and append a handful of these meta columns before feeding the data to a linear model trained in a one\u2011vs\u2011rest fashion with stochastic gradient descent (SGD), which scales well to millions of features and can be tuned with simple regularisation tricks\u202f([Kaggle discussion](https://www.kaggle.com/competitions/facebook-recruiting-iii-keyword-extraction/writeups/barisumog-share-your-approach)).  \n\nIn addition to linear models, many top teams use an online\u2011learning baseline such as logistic regression with the \u201chash trick\u201d and an adaptive learning rate; this approach is fast, memory\u2011efficient, and easily accommodates extra meta columns alongside hashed text features\u202f([Kaggle discussion \u2013 Tradeshift competition](https://www.kaggle.com/competitions/tradeshift-text-classification/discussion/10537)).  Across competitions, successful pipelines also incorporate systematic preprocessing (lower\u2011casing, stop\u2011word removal, stemming), careful feature selection for the meta data, and extensive hyper\u2011parameter searches (grid or Bayesian) to optimise regularisation strength and learning\u2011rate schedules\u202f([DataScience StackExchange](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain)).  When the dataset is large enough, stacking several diverse models (e.g., linear + gradient\u2011boosted trees) on top of the combined text\u2011plus\u2011meta feature set can provide the final performance boost that separates winning submissions from the rest.",
      "url": ""
    },
    {
      "title": "Share your approach? | Kaggle",
      "text": "Share your approach? | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n###### We can't find that page.\nYou can search Kaggle above or[visit our homepage](https://www.kaggle.com/).",
      "url": "https://www.kaggle.com/competitions/facebook-recruiting-iii-keyword-extraction/writeups/barisumog-share-your-approach"
    },
    {
      "title": "What methods do top Kagglers employ for score gain?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [What methods do top Kagglers employ for score gain?](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked1 year, 8 months ago\n\nModified [1 year, 8 months ago](https://datascience.stackexchange.com/datascience.stackexchange.com?lastactivity)\n\nViewed\n424 times\n\n2\n\n$\\\\begingroup$\n\nI\u2019m currently taking a course on ML and part of my final grade is my position on a Kaggle competition (private one) regarding a classification task. The majority of groups tend to have a similar public score but there are some that spike through the rest. I am left wondering what techniques data scientists employ to increase their scores.\n\nI am aware that a thoughtful pre-processing of the data is of chief importance as well as techniques such as grid search which help us find the best hyperparameters. This question is more about some out-of-the-box techniques that you employ when your aim is to increase your final score, for example, in the setting of a Kaggle competition.\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [kaggle](https://datascience.stackexchange.com/questions/tagged/kaggle)\n\n[Share](https://datascience.stackexchange.com/q/124709)\n\n[Improve this question](https://datascience.stackexchange.com/posts/124709/edit)\n\nFollow\n\n[edited Nov 22, 2023 at 23:36](https://datascience.stackexchange.com/posts/124709/revisions)\n\n[Multivac](https://datascience.stackexchange.com/users/92050/multivac)\n\n3,25922 gold badges1010 silver badges2626 bronze badges\n\nasked Nov 22, 2023 at 19:32\n\n[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela) Frederico Portela\n\n4122 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n4\n\n$\\\\begingroup$\n\nTop Kagglers often employ a combination of traditional machine learning techniques and creative, out-of-the-box strategies to gain an edge in competitions. Here are some techniques:\n\n**1\\. Ensemble Methods**\n\n- _Stacking/Blending:_ Combining predictions from multiple models. This involves training several models and then using another model to predict the target variable based on the predictions of the initial models.\n\n- _Bagging and Boosting:_ Techniques like Random Forest (bagging) and Gradient Boosting (boosting) can significantly improve performance.\n\n\n**2\\. Feature Engineering**\n\n- _Creating new features:_ Deriving meaningful features from existing ones can provide valuable information to the model.\n\n- _Dimensionality Reduction:_ Techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) can be useful.\n\n\n**3\\. Advanced Preprocessing**\n\n- _Handling missing data:_ Creative ways to impute missing values.\n\n- _Outlier Detection and Treatment:_ Identifying and addressing outliers can improve model robustness.\n\n\n**4\\. Model Hyperparameter Tuning**\n\n- _Bayesian Optimization:_ Efficiently searching hyperparameter space.\n\n- _Optuna, Hyperopt:_ Libraries for hyperparameter optimization.\n\n\n**5\\. Neural Architecture Search (NAS)**\n\n- _Automated Model Design:_ Techniques to automatically search for the best neural network architecture.\n\n**6\\. Transfer Learning**\n\n- _Using pre-trained models:_ Leveraging models trained on large datasets and fine-tuning them for the specific task at hand.\n\n**7\\. Data Augmentation**\n\n- _Increasing Training Data:_ Applying transformations (rotations, flips, etc.) to artificially increase the size of the training dataset.\n\n**8\\. Domain-Specific Knowledge**\n\n- _Understanding the problem domain:_ Incorporating domain-specific knowledge can lead to better feature engineering and model performance.\n\n**9\\. Advanced Modeling Techniques**\n\n- _Neural Networks Architectures:_ Exploring different architectures, such as attention mechanisms, transformers, etc.\n- _XGBoost, LightGBM, CatBoost:_ Gradient boosting libraries that are often used for tabular data.\n\n**10\\. Time Series Techniques**\n\n- _LSTM, GRU:_ For sequential data.\n\n- _Feature lagging and rolling statistics:_ Utilizing information from past time points.\n\n\n**11\\. Post-Processing**\n\n- _Calibration:_ Adjusting predicted probabilities to improve the model's reliability.\n- _Threshold tuning:_ Adjusting the classification threshold based on the specific needs of the task.\n\n**12\\. Collaboration and Knowledge Sharing**\n\n- _Participating in discussions:_ Sharing insights and learning from others on Kaggle forums.\n- _Team Collaboration:_ Forming teams to combine diverse skills and perspectives.\n\n[Share](https://datascience.stackexchange.com/a/124714)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/124714/edit)\n\nFollow\n\nanswered Nov 22, 2023 at 23:43\n\n[lvvittor](https://datascience.stackexchange.com/users/156336/lvvittor) lvvittor\n\n10111 bronze badge\n\n$\\\\endgroup$\n\n1\n\n- 1\n\n$\\\\begingroup$Thank you very much. There are indeed here some techniques i was not aware of.$\\\\endgroup$\n\n\u2013\u00a0[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela)\n\nCommentedNov 23, 2023 at 16:55\n\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n4\n\n$\\\\begingroup$\n\nThere are many methods that can be employed to increase your score on a Kaggle competition. Here are a few examples:\n\n**Advanced classification techniques:** Using advanced classification techniques such as weighted average ensemble, stacked generalization ensemble, and power average ensemble can help improve your model's performance.\n\n**Data augmentation:** Techniques such as random rotation, hue adjustments, saturation adjustments, contrast adjustments, brightness adjustments, cropping, and more can help improve your model's accuracy when dealing with image data.\n\n**Text augmentation:** When dealing with text data, techniques such as exchanging words with synonyms, noising in RNN, and translating to other languages and back can help augment your training data and improve your model's performance.\n\n**External datasets:** Using external datasets that contain variables that influence the predicate variable can help increase the performance of your model pre-processing.\n\nI have even seen that in some cases, especially at the beginning of the site, teams took advantage of finding data leakage. For example, identifying customer's id that may potentially be linked to temporal patterns and thus model's prediction\n\nThis [discussion](https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111308) may give you a fine summary of most of the techniques I mentioned above.\n\nI hope it helps.\n\n[Share](https://datascience.stackexchange.com/a/124715)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/124715/edit)\n\nFollow\n\nanswered Nov 22, 2023 at 23:44\n\n[Multivac](https://datascience.stackexchange.com/users/92050/multivac) Multivac\n\n3,25922 gold badges1010 silver badges2626 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- $\\\\begingroup$Thank you for this. I'll take a look at the discussion.$\\\\endgroup$\n\n\u2013\u00a0[Frederico Portela](https://datascience.stackexchange.com/users/155367/frederico-portela)\n\nCommentedNov 23, 2023 at 16:55\n\n\n[Add a comment](https://datascience.stackexchange.com/datascience.stackexchange.com)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/logi...",
      "url": "https://datascience.stackexchange.com/questions/124709/what-methods-do-top-kagglers-employ-for-score-gain"
    },
    {
      "title": "Tradeshift Text Classification - Kaggle",
      "text": "- _code_\n\nNew Notebook\n\n- _table\\_chart_\n\nNew Dataset\n\n- _tenancy_\n\nNew Model\n\n- _emoji\\_events_\n\nNew Competition\n\n- _corporate\\_fare_\n\nNew Organization\n\n\n## No Active Events\n\nCreate notebooks and keep track of their status here.\n\n[_add_ New Notebook](https://www.kaggle.com/code/new)\n\n- _auto\\_awesome\\_motion_\n\n\n\n\n\n\n\n\n\n0 Active Events\n\n\n\n\n\n\n_expand\\_more_\n\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/tradeshift-text-classification/discussion/10537#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\n_search_\n\n- [_explore_\\\n\\\nHome](https://www.kaggle.com/)\n\n- [_emoji\\_events_\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [_table\\_chart_\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [_tenancy_\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [_code_\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [_comment_\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [_school_\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n- [_expand\\_more_\\\n\\\nMore](https://www.kaggle.com/competitions/tradeshift-text-classification/discussion/10537)\n\n\n_auto\\_awesome\\_motion_\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/competitions/tradeshift-text-classification/discussion/10537#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\n_search_\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcompetitions%2Ftradeshift-text-classification%2Fdiscussion%2F10537)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcompetitions%2Ftradeshift-text-classification%2Fdiscussion%2F10537)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more.](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 5462 failed.\n(error: https://www.kaggle.com/static/assets/5462.f5129f48620a03126642.css)\n    at t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=5502c2919e74bf426769:1:8254)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/competitions/tradeshift-text-classification/discussion/10537"
    },
    {
      "title": "Text Classification Techniques - Explained - Kaggle",
      "text": "Text Classification Techniques - Explained | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![Something went wrong and this page crashed!](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)###### Something went wrong and this page crashed!\nIf the issue persists, it's likely a problem on our side.\n```\nLoading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nkeyboard\\_arrow\\_upcontent\\_copy\nError: Loading CSS chunk 1019 failed.\\\\n(https://www.kaggle.com/static/assets/KernelViewer.21f94db0016b9074.css)\nat t.onerror.t.onload (https://www.kaggle.com/static/assets/runtime.js?v=a6aef772a946d7d4:1:10006)\n```\nRefresh",
      "url": "https://www.kaggle.com/code/eraikako/text-classification-techniques-explained"
    },
    {
      "title": "Text Classification: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "In this article, I will discuss some great tips and tricks to [improve the performance](https://neptune.ai/blog/improving-machine-learning-deep-learning-models) of your text classification model. These tricks are obtained from solutions of some of Kaggle\u2019s top NLP competitions.\n\nNamely, I\u2019ve gone through:\n\n- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) \u2013 $65,000\n- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/) \u2013 $35,000\n- [Quora Insincere Questions Classification](http://kaggle.com/c/quora-insincere-questions-classification) \u2013 $25,000\n- [Google QUEST Q&A Labeling](https://www.kaggle.com/c/google-quest-challenge) \u2013 $25,000\n- [TensorFlow 2.0 Question Answering](https://www.kaggle.com/c/tensorflow2-question-answering) \u2013 $50,000\n\nand found a ton of great ideas.\n\nWithout much lag, let\u2019s begin.\n\n## Dealing with larger datasets\n\nOne issue you might face in any machine learning competition is the size of your data set. If the size of your data is large, that is 3GB + for Kaggle kernels and more basic laptops you could find it difficult to load and process with limited resources. Here is the link to some of the articles and kernels that I have found useful in such situations.\n\n- Optimize the memory by [reducing the size of some attributes](https://www.kaggle.com/shrutimechlearn/large-data-loading-trick-with-ms-malware-data)\n- Use open-source libraries such as [Dask to read and manipulate the data](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask), it performs parallel computing and saves up memory space\n- Use [cudf](https://github.com/rapidsai/cudf)\n- Convert data to [parquet](https://arrow.apache.org/docs/python/parquet.html) format\n- Convert data to [feather](https://medium.com/@snehotosh.banerjee/feather-a-fast-on-disk-format-for-r-and-python-data-frames-de33d0516b03) format\n\n## Small datasets and external data\n\nBut, what can one do if the dataset is small? Let\u2019s see some techniques to tackle this situation.\n\nOne way to increase the performance of any machine learning model is to use some external data frame that contains some variables that influence the predicate variable.\n\nLet\u2019s see some of the external datasets.\n\n- Use of [squad](https://rajpurkar.github.io/SQuAD-explorer/) data for Question Answering tasks\n- Other [datasets](http://nlpprogress.com/english/question_answering.html) for QA tasks\n- Wikitext long term dependency language modeling [dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n- [Stackexchange data](https://archive.org/download/stackexchange)\n- Prepare a dictionary of commonly misspelled words and corrected words.\n- Use of [helper datasets](https://www.kaggle.com/kyakovlev/jigsaw-general-helper-public) for cleaning\n- [Pseudo labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969/) is the process of adding confidently predicted test data to your training data\n- Use different data [sampling methods](https://www.kaggle.com/shahules/tackling-class-imbalance)\n- Text augmentation by [Exchanging words with synonym](https://arxiv.org/pdf/1502.01710.pdf) [s](https://arxiv.org/pdf/1502.01710.pdf)\n- Text augmentation by [noising in RNN](https://arxiv.org/pdf/1703.02573.pdf)\n- Text augmentation by [translation to other languages and back](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038)\n\n## Data exploration and gaining insights\n\nData exploration always helps to better understand the data and gain insights from it. Before starting to develop machine learning models, top competitors always read/do a lot of exploratory data analysis for the data. This helps in feature engineering and cleaning of the data.\n\n- Twitter data [exploration methods](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)\n- Simple [EDA for tweets](https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw)\n- [EDA](https://www.kaggle.com/tunguz/just-some-simple-eda) for Quora data\n- [EDA](https://www.kaggle.com/kailex/r-eda-for-q-gru) in \u00a0R for Quora data\n- Complete [EDA](https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe) with stack exchange data\n- My previous article on [EDA for natural language processing](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools)\n\n## Data cleaning\n\nData cleaning is one of the important and integral parts of any NLP problem. Text data always needs some preprocessing and cleaning before we can represent it in a suitable form.\n\n- Use this [notebook](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) to clean social media data\n- [Data cleaning](https://www.kaggle.com/kyakovlev/preprocessing-bert-public) for BERT\n- Use [textblob](https://textblob.readthedocs.io/en/dev/quickstart.html) to correct misspellings\n- [Cleaning](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing) for pre-trained embeddings\n- [Language detection and translation](https://www.pythonprogramming.in/language-detection-and-translation-using-textblob.html) for multilingual tasks\n- Preprocessing for Glove [part 1](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda) and [part 2](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)\n- [Increasing word coverage](https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage) to get more from pre-trained word embeddings\n\n## Text representations\n\nBefore we feed our text data to the Neural network or ML model, the text input needs to be represented in a suitable format. These representations determine the performance of the model to a large extent.\n\n- Pretrained [Glove](https://nlp.stanford.edu/projects/glove/) vectors\n- Pretrained [fasttext](https://fasttext.cc/docs/en/english-vectors.html) vectors\n- Pretrained [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) vectors\n- My previous article on these [3 embeddings](https://neptune.ai/blog/document-classification-small-datasets)\n- Combining [pre-trained vectors](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778/). This can help in better representation of text and decreasing OOV words\n- [Paragram](https://cogcomp.seas.upenn.edu/page/resource_view/106) embeddings\n- [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1)\n- Use USE to generate [sentence-level features](https://www.kaggle.com/abhishek/distilbert-use-features-oof)\n- 3 methods to [combine embeddings](https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778)\n\nContextual embeddings models\n\n- [BERT](https://github.com/google-research/bert) Bidirectional Encoder Representations from Transformers\n- [GPT](https://github.com/openai/finetune-transformer-lm)\n- [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta) a Robustly Optimized BERT\n- [Albert](https://github.com/google-research/ALBERT) a Lite BERT for Self-supervised Learning of Language Representations\n- [Distilbert](https://github.com/huggingface/transformers/tree/master/examples/distillation) a lighter version of BERT\n- [XLNET](https://github.com/zihangdai/xlnet/)\n\n## Modeling\n\n### Model architecture\n\nChoosing the right architecture is important to develop a proper machine learning model, sequence to sequence models like LSTMs, GRUs perform well in NLP problems and is always worth trying. Stacking 2 layers of LSTM/GRU networks is a common approach.\n\n- [Stacking Bidirectional CuDNNLSTM](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644/)\n- [Stacking LSTM networks](https://www.kaggle.com/sakami/google-quest-single-lstm/)\n- [LSTM and 5 fold Attention](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold/)\n- [Bi...",
      "url": "https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions"
    },
    {
      "title": "Fine-tuning BERT for Text classification - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=235808c4728122ddc819:1:11382)\n```",
      "url": "https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification"
    },
    {
      "title": "How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions",
      "text": "How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions | HackerNoon\nDiscover Anything**\n[\n![Hackernoon logo](https://hackernoon.imgix.net/hn-icon.png?auto=format&amp;fit=max&amp;w=128)\nHackernoon](https://hackernoon.com/)\nSignup[Write](https://app.hackernoon.com/new)\n******\n**139reads\n# How To Run Text Categorization: All Tips and Tricks from 5 Kaggle Competitions\nby\n**neptune.ai Jakub Czakon**\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\nAugust 23rd, 2020\nTLDR**\n![neptune.ai Jakub Czakon](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\n****\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\n[\n![](https://cdn.hackernoon.com/images%2FL4OCiGu7n6cUBUyJKe9Rs0MX1d73-8x03wsi.jpeg?alt=media&amp;token=dc8a460d-ab44-4c8e-a72e-559d894a3fbc)\nbyneptune.ai Jakub Czakon@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\nSubscribe\n****\n**********[**](<mailto:?subject=I'd like to share a link with you &amp;body=>)\n[\n\u2190Previous\nHyperparameter Tuning on Any Python Script in 3 EasySteps [A How-To Guide]\n](https://hackernoon.com/hyperparameter-tuning-on-any-python-script-in-3-easy-steps-a-how-to-guide-5u4g329e)\n### About Author\n[](https://hackernoon.com/u/neptuneAI_jakub)\n[**neptune.ai Jakub Czakon**@neptuneAI\\_jakub](https://hackernoon.com/u/neptuneAI_jakub)\nSenior data scientist building experiment tracking tools for ML projects at https://neptune.ai\n[Read my stories](https://hackernoon.com/u/neptuneAI_jakub)[Learn More](https://hackernoon.com/about/neptuneAI_jakub)\n#### Comments\n#### TOPICS\n[**machine-learning](https://hackernoon.com/c/machine-learning)[#machine-learning](https://hackernoon.com/tagged/machine-learning)[#binary-classification](https://hackernoon.com/tagged/binary-classification)[#data-science](https://hackernoon.com/tagged/data-science)[#kaggle](https://hackernoon.com/tagged/kaggle)[#tips-and-tricks](https://hackernoon.com/tagged/tips-and-tricks)[#ml](https://hackernoon.com/tagged/ml)[#kaggle-competition](https://hackernoon.com/tagged/kaggle-competition)[#tutorial](https://hackernoon.com/tagged/tutorial)\n#### THIS ARTICLE WAS FEATURED IN\n[\n](https://sia.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh)[\n**\nArweave\n](https://www.arweave.net/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[\nViewBlock\n](https://viewblock.io/arweave/tx/ithJ8kJytzNBq5ZLT_ZTN24PAqZ047UthexwscEqhQw)[Terminal](https://terminal.hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[Lite](https://hackernoon.com/lite/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh?ref=hackernoon)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.joyk.com/dig/detail/1707491664448324&amp;size=16)Joyk\n](https://www.joyk.com/dig/detail/1707491664448324)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://www.learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\n](https://www.learnrepo.com/242-stories-to-learn-about-ml)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/48760739&amp;size=16)Unni\n](https://unni.io/domains/hackernoon.com/page/48760739)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial&amp;size=16)Hashnode\n](https://learnrepo.hashnode.dev/833-stories-to-learn-about-tutorial)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://learnrepo.com/242-stories-to-learn-about-ml&amp;size=16)Learnrepo\n](https://learnrepo.com/242-stories-to-learn-about-ml)[\n![](https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;url=https://unni.io/domains/hackernoon.com/page/50536904&amp;size=16)Unni\n](https://unni.io/domains/hackernoon.com/page/50536904)\n#### Related Stories\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\nMar 23, 2020\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[#METRICS](https://hackernoon.com/tagged/metrics)\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[Aleksei Toshchakov](https://hackernoon.com/u/toshchakov)\nNov 11, 2023\n[](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\n[#BINARY-CLASSIFICATION](https://hackernoon.com/tagged/binary-classification)\n## [Binary Classification: Understanding Activation and Loss Functions with a PyTorch Example](https://hackernoon.com/binary-classification-understanding-activation-and-loss-functions-with-a-pytorch-example)\n[Dmitrii Matveichev](https://hackernoon.com/u/owlgrey)\nAug 15, 2023\n[](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [Confusion Matrix in Machine Learning: Everything You Need to Know](https://hackernoon.com/confusion-matrix-in-machine-learning-everything-you-need-to-know)\n[Bala Priya C](https://hackernoon.com/u/balapriya)\nFeb 03, 2022\n[](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\n[#ARTIFICIAL-INTELLIGENCE](https://hackernoon.com/tagged/artificial-intelligence)\n## [Model Calibration in Machine Learning: An Important but Inconspicuous Concept](https://hackernoon.com/model-calibration-in-machine-learning-an-important-but-inconspicuous-concept)\n[Sanjay Kumar](https://hackernoon.com/u/sanjaykn170396)\nJan 28, 2023\n[](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [Practical Tips For Binary Classification Excellence](https://hackernoon.com/practical-tips-for-binary-classification-excellence-ape3w02)\n[neptune.ai Patrycja Jenkner](https://hackernoon.com/u/neptuneAI_patrycja)\nNov 09, 2020\n[](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[#MACHINE-LEARNING](https://hackernoon.com/tagged/machine-learning)\n## [A Complete(ish) Guide to Python Tools You Can Use To Analyse Text Data](https://hackernoon.com/a-completeish-guide-to-python-tools-you-can-use-to-analyse-text-data-13g53wgr)\n[neptune.ai Jakub Czakon](https://hackernoon.com/u/neptuneAI_jakub)\nMar 23, 2020\n[](https://hackernoon.com/beyond-recall-and-precision-rethinking-metrics-in-fraud-prevention)\n[#METRICS](https://hackernoon.com/tagged/metrics)\n## [Beyond Recall and Precision: Rethinking Metrics in Fraud Prevention](https://hackernoon.com/beyond-re...",
      "url": "https://hackernoon.com/how-to-run-text-categorization-all-tips-and-tricks-from-5-kaggle-competitions-7e1h3uyh"
    },
    {
      "title": "What my first Silver Medal taught me about Text Classification and Kaggle in general? - MLWhiz",
      "text": "[Natural Language Processing](https://mlwhiz.com/categories/natural-language-processing) [Deep Learning](https://mlwhiz.com/categories/deep-learning) [Awesome Guides](https://mlwhiz.com/categories/awesome-guides)\n\n# What my first Silver Medal taught me about Text Classification and Kaggle in general?\n\nBy Rahul Agarwal19 February 2019\n\n![What my first Silver Medal taught me about Text Classification and Kaggle in general?](https://mlwhiz.com/images/silver/CV_vs_LB.png)\n\nKaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on **Quora Insincere questions classification** in which I got a rank of **`182/4037`**. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.\n\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome\n[Natural Language Processing Specialization](https://coursera.pxf.io/9WjZo0)\n. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.\n\nSo first a little bit of summary about the competition for the uninitiated. In this competition, we had to develop models that identify and flag insincere questions. _**The challenge was not only a test for performance but also a test of efficient code writing skills.**_ As it was a kernel competition with limited outside data options, competitors were limited to use only the word embeddings provided by the competition organizers. That means we were not allowed to use State of the art models like BERT. We were also limited in the sense that all our models should run in a time of 2 hours. So say bye bye to stacking and monster ensembles though some solutions were able to do this by making their code ultra-efficient. More on this later.\n\n## Some Kaggle Learnings:\n\nThere were a couple of **learnings about kaggle as a whole** that I would like to share before jumping into my final solution:\n\n### 1\\. Always trust your CV\n\n![](https://mlwhiz.com/images/silver/CV_vs_LB.png)\n\nOne of the things that genuinely baffled a lot of people in this competition was that a good CV score did not necessarily translate well to a good LB score. The main reason for this was **small test dataset**(only 65k rows) in the first stage(around 15% of total test data).\n\nA common theme on discussion forums was focussing on which submissions we should select as the final submission:\n\n- The one having the best local CV? or\n- The one having the best LB?\n\nAnd while it seems simple to say to trust your CV, common sense goes for a toss when you see that your LB score is going down or remaining constant whenever your Local CV score increases.\n\nLuckily I didn\u2019t end up making the mistake of not trusting my CV score. Owing to a lot of excellent posts on Kaggle discussion board, _**I selected a kernel with Public LB score of 0.697 and a Local CV of 0.701, which was around >1200 rank on Public LB as of the final submission. It achieved a score of 0.702 and ranked 182 on the private LB.**_\n\nWhile this seems like a straightforward choice post-facto, it was a hard decision to make at a time when you have at your disposal some public kernels having Public LB score >= 0.70\n\n### 2\\. Use the code from public kernels but check for errors\n\n[This](https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch)\nPytorch kernel by Benjamin Minixhofer is awesome. It made the base of many of my submissions for this competition. But this kernel had a mistake. It didn\u2019t implement spatial dropout in the right way. You can find the correct implementation of spatial dropout in my post\n[here](https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/)\nor on my\n[kernel](https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-spatial-dropout)\n. Implementing spatial dropout in the right way gave a boost of around 0.004 to the local CV.\n\nNonetheless, I learned pytorch using this kernel, and I am grateful to him for the same.\n\n### 3\\. Don\u2019t trust everything that goes on the discussion forums\n\n![](https://mlwhiz.com/images/silver/read-what-the-smart-people-are-saying.png)\n\nI will talk about two things here:\n\n- **Seed tuning**: While in the middle of the competition, everyone was trying to get the best possible rank on the public LB. It is just human nature. A lot of discussions was around good seeds and bad seeds for neural network initialization. While it seems okay in the first look, the conversation went a stage further where **people started tuning seeds in the kernel as a hyper param**. Some discussions even went on to say that it was a valid strategy. And that is where a large amount of overfitting to public LB started happening. The same submission would score 0.704 from 0.699 just by changing the seed. For a reference, that meant you could go from anywhere near 400-500 rank to top 50 only by changing seed in a public kernel. And that spelled disaster. Some people did that. They went up the public LB. Went crashing out at the private stage.\n\n- **CV score disclosure on discussion forums**: We always try to gauge our performance against other people. In a lot of discussions, people provided their CV scores and corresponding Public LB scores. The scores were all over the place and not comparable due to Different CV schemes, No of folds in CV, Metric reported, Overfitting or just plain Wrong implementation of Cross-Validation. But they ended up influencing a lot of starters and newcomers.\n\n\n### 4\\. On that note, be active on Discussion forums and check public kernels regularly\n\nYou can learn a lot just by being part of discussion forums and following public kernels. This competition had a lot of excellent public kernels on embeddings by\n[SRK](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n, Models by\n[Shujian](https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding)\n, and Preprocessing by\n[Theo Viel](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2)\nwhich gave everyone a headstart. As the competition progressed, the discussions also evolved. There were discussions on speeding up the code, working approaches, F1 threshold finders, and other exciting topics which kept me occupied with new ideas and improvements.\n\nEven after the end, while reading up discussions on solutions overview, I learned a lot. And I would say it is very \\*\\* vital to check out the winning solutions.\\*\\*\n\n### 5\\. Share a lot\n\nSharing is everything on Kaggle. People have shared their codes as well as their ideas while competing as well as after the competition ended. It is only together that we can go forward. I like blogging, so I am sharing the knowledge via a series of blog posts on text classification. The\n[first post](https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/)\ntalked about the different **preprocessing techniques that work with Deep learning models** and **increasing embeddings coverage**. In the\n[second post](https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/)\n, I talked through some **basic conventional models** like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I will delve deeper into **Deep learning models and the various architectures** we could use to solve the text Classification problem. To make this post platform generic I will try to write code in both Keras and Pytorch. We will try to use various other models which we were not able to use in this competition like **ULMFit transfer learning** approaches in the fourth post in the series.\n\nIt might take me a little time to write the whole series. Till then you can take a look at my other posts too:\n[What Kagglers are using for...",
      "url": "https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings"
    }
  ]
}