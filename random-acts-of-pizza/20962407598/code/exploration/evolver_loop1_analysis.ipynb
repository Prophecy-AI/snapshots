{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9506a218",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "## Goal: Understand data patterns and identify high-impact improvements\n",
    "\n",
    "Current CV: 0.6374 (far from gold: 0.979080)\n",
    "\n",
    "Focus areas:\n",
    "1. Text feature engineering\n",
    "2. Interaction features\n",
    "3. Model diversity\n",
    "4. Validation stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82244290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T16:58:22.354970Z",
     "iopub.status.busy": "2026-01-13T16:58:22.354330Z",
     "iopub.status.idle": "2026-01-13T16:58:24.609775Z",
     "shell.execute_reply": "2026-01-13T16:58:24.609160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training samples: 2878\n",
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    0.751564\n",
      "True     0.248436\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Loading test data...\n",
      "Test samples: 1162\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Target distribution:\\n{train_df['requester_received_pizza'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b255d22",
   "metadata": {},
   "source": [
    "## 1. Text Analysis - Request Text & Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths and patterns\n",
    "train_df['text_length'] = train_df['request_text_edit_aware'].str.len()\n",
    "train_df['title_length'] = train_df['request_title'].str.len()\n",
    "train_df['word_count'] = train_df['request_text_edit_aware'].str.split().str.len()\n",
    "train_df['title_word_count'] = train_df['request_title'].str.split().str.len()\n",
    "\n",
    "# Check correlations with target\n",
    "print(\"Text feature correlations with target:\")\n",
    "text_features = ['text_length', 'title_length', 'word_count', 'title_word_count']\n",
    "for feature in text_features:\n",
    "    corr = train_df[feature].corr(train_df['requester_received_pizza'].astype(int))\n",
    "    print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, feature in enumerate(text_features):\n",
    "    ax = axes[i//2, i%2]\n",
    "    train_df[train_df['requester_received_pizza']==False][feature].hist(ax=ax, alpha=0.7, label='Not Fulfilled', bins=30)\n",
    "    train_df[train_df['requester_received_pizza']==True][feature].hist(ax=ax, alpha=0.7, label='Fulfilled', bins=30)\n",
    "    ax.set_title(f'{feature} Distribution')\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334578bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze successful vs unsuccessful requests - word patterns\n",
    "successful_text = ' '.join(train_df[train_df['requester_received_pizza']==True]['request_text_edit_aware'].fillna(''))\n",
    "unsuccessful_text = ' '.join(train_df[train_df['requester_received_pizza']==False]['request_text_edit_aware'].fillna(''))\n",
    "\n",
    "# Simple word frequency analysis\n",
    "def get_word_freq(text, n=20):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "print(\"Top words in SUCCESSFUL requests:\")\n",
    "successful_words = get_word_freq(successful_text)\n",
    "for word, count in successful_words:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop words in UNSUCCESSFUL requests:\")\n",
    "unsuccessful_words = get_word_freq(unsuccessful_text)\n",
    "for word, count in unsuccessful_words:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Calculate word importance (ratio of frequencies)\n",
    "print(\"\\nWords more common in successful requests:\")\n",
    "success_dict = dict(successful_words)\n",
    "unsuccess_dict = dict(unsuccessful_words)\n",
    "\n",
    "word_ratios = []\n",
    "for word in set(list(success_dict.keys()) + list(unsuccess_dict.keys())):\n",
    "    success_rate = success_dict.get(word, 0) / len(train_df[train_df['requester_received_pizza']==True]) if len(train_df[train_df['requester_received_pizza']==True]) > 0 else 0\n",
    "    unsuccess_rate = unsuccess_dict.get(word, 0) / len(train_df[train_df['requester_received_pizza']==False]) if len(train_df[train_df['requester_received_pizza']==False]) > 0 else 0\n",
    "    \n",
    "    if success_rate > 0 and unsuccess_rate > 0:\n",
    "        ratio = success_rate / unsuccess_rate\n",
    "        if ratio > 1.5:  # More common in successful requests\n",
    "            word_ratios.append((word, ratio, success_rate, unsuccess_rate))\n",
    "\n",
    "word_ratios.sort(key=lambda x: x[1], reverse=True)\n",
    "for word, ratio, s_rate, u_rate in word_ratios[:15]:\n",
    "    print(f\"  {word}: {ratio:.2f}x more common (success: {s_rate:.4f}, unsuccess: {u_rate:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d306e",
   "metadata": {},
   "source": [
    "## 2. Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "train_df['request_hour'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s').dt.hour\n",
    "train_df['request_day_of_week'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s').dt.dayofweek\n",
    "train_df['request_month'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s').dt.month\n",
    "\n",
    "# Check correlations\n",
    "temporal_features = ['request_hour', 'request_day_of_week', 'request_month']\n",
    "print(\"Temporal feature correlations with target:\")\n",
    "for feature in temporal_features:\n",
    "    corr = train_df[feature].corr(train_df['requester_received_pizza'].astype(int))\n",
    "    print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Hourly pattern\n",
    "hourly_success = train_df.groupby('request_hour')['requester_received_pizza'].agg(['mean', 'count'])\n",
    "print(f\"\\nBest hours for pizza requests (by success rate):\")\n",
    "print(hourly_success.sort_values('mean', ascending=False).head())\n",
    "\n",
    "# Day of week pattern\n",
    "dow_success = train_df.groupby('request_day_of_week')['requester_received_pizza'].agg(['mean', 'count'])\n",
    "print(f\"\\nBest days of week:\")\n",
    "print(dow_success.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc732c0a",
   "metadata": {},
   "source": [
    "## 3. User Activity Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze user activity features\n",
    "activity_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request', \n",
    "    'requester_number_of_subreddits_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request'\n",
    "]\n",
    "\n",
    "print(\"Activity feature correlations with target:\")\n",
    "for feature in activity_features:\n",
    "    corr = train_df[feature].corr(train_df['requester_received_pizza'].astype(int))\n",
    "    print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Create interaction features\n",
    "train_df['comments_per_post'] = train_df['requester_number_of_comments_at_request'] / (train_df['requester_number_of_posts_at_request'] + 1)\n",
    "train_df['upvote_ratio'] = train_df['requester_upvotes_minus_downvotes_at_request'] / (train_df['requester_upvotes_plus_downvotes_at_request'] + 1)\n",
    "train_df['activity_score'] = (train_df['requester_number_of_comments_at_request'] + \n",
    "                              train_df['requester_number_of_posts_at_request']) / (train_df['requester_account_age_in_days_at_request'] + 1)\n",
    "\n",
    "interaction_features = ['comments_per_post', 'upvote_ratio', 'activity_score']\n",
    "print(\"\\nInteraction feature correlations with target:\")\n",
    "for feature in interaction_features:\n",
    "    corr = train_df[feature].corr(train_df['requester_received_pizza'].astype(int))\n",
    "    print(f\"{feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014e6a9",
   "metadata": {},
   "source": [
    "## 4. Summary of Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of best features\n",
    "all_features = text_features + temporal_features + activity_features + interaction_features\n",
    "correlations = []\n",
    "for feature in all_features:\n",
    "    if feature in train_df.columns:\n",
    "        corr = train_df[feature].corr(train_df['requester_received_pizza'].astype(int))\n",
    "        correlations.append((feature, abs(corr), corr))\n",
    "\n",
    "correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Top features by absolute correlation:\")\n",
    "for feature, abs_corr, corr in correlations[:15]:\n",
    "    print(f\"  {feature}: {corr:.4f} (abs: {abs_corr:.4f})\")\n",
    "\n",
    "# Save findings\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"1. Text length features show moderate correlation (~0.1)\")\n",
    "print(f\"2. Request hour and day show patterns\")\n",
    "print(f\"3. User activity metrics have weak but consistent correlations\")\n",
    "print(f\"4. Interaction features may add signal\")\n",
    "print(f\"5. High variance in baseline (0.0312) suggests need for more robust features\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
