{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e86b289",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis\n",
    "\n",
    "## Goal: Diagnose failures and identify breakthrough strategies\n",
    "\n",
    "Current Status:\n",
    "- Best CV: 0.6374 (baseline)\n",
    "- Latest: 0.6129 (enhanced features) - WORSE\n",
    "- Evaluator verdict: UNRELIABLE due to data leakage\n",
    "- Gap to gold: 0.3417\n",
    "\n",
    "Focus areas:\n",
    "1. Understand why enhanced features failed\n",
    "2. Validate data leakage impact\n",
    "3. Research BERT/transformer approaches\n",
    "4. Design ensemble strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26059722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('/home/data/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "with open('/home/data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(f\"Target distribution: {train_df['requester_received_pizza'].value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "# Check for data leakage issue\n",
    "print(\"\\n=== DATA LEAKAGE ANALYSIS ===\")\n",
    "combined_text = pd.concat([\n",
    "    train_df['request_title'].fillna('') + ' ' + train_df['request_text_edit_aware'].fillna(''),\n",
    "    test_df['request_title'].fillna('') + ' ' + test_df['request_text_edit_aware'].fillna('')\n",
    "])\n",
    "print(f\"Combined text samples: {len(combined_text)}\")\n",
    "print(f\"Train text samples: {len(train_df)}\")\n",
    "print(f\"Test text samples: {len(test_df)}\")\n",
    "print(f\"Leakage issue: TF-IDF fit on {len(combined_text)} samples instead of {len(train_df)} training samples\")\n",
    "print(f\"This means test data influenced training - INVALID CV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature dimensionality issue\n",
    "print(\"=== FEATURE DIMENSIONALITY ANALYSIS ===\")\n",
    "\n",
    "# Simulate the enhanced features setup\n",
    "# Text features\n",
    "train_df['text_length'] = train_df['request_text_edit_aware'].str.len()\n",
    "train_df['word_count'] = train_df['request_text_edit_aware'].str.split().str.len()\n",
    "train_df['title_length'] = train_df['request_title'].str.len()\n",
    "train_df['title_word_count'] = train_df['request_title'].str.split().str.len()\n",
    "train_df['avg_word_length'] = train_df['text_length'] / (train_df['word_count'] + 1)\n",
    "\n",
    "# Persuasion features\n",
    "persuasion_words = ['please', 'help', 'appreciate', 'grateful', 'thank', 'thanks', 'kind', 'generous', 'need']\n",
    "for word in persuasion_words:\n",
    "    train_df[f'has_{word}'] = train_df['request_text_edit_aware'].str.lower().str.contains(word).astype(int)\n",
    "\n",
    "# Punctuation\n",
    "train_df['exclamation_count'] = train_df['request_text_edit_aware'].str.count('!')\n",
    "train_df['question_count'] = train_df['request_text_edit_aware'].str.count('\\?')\n",
    "\n",
    "# Interaction features\n",
    "train_df['user_credibility'] = (train_df['requester_upvotes_plus_downvotes_at_request']) / (train_df['requester_account_age_in_days_at_request'] + 1)\n",
    "train_df['comments_per_post'] = train_df['requester_number_of_comments_at_request'] / (train_df['requester_number_of_posts_at_request'] + 1)\n",
    "train_df['upvote_ratio'] = train_df['requester_upvotes_minus_downvotes_at_request'] / (train_df['requester_upvotes_plus_downvotes_at_request'] + 1)\n",
    "train_df['subreddit_diversity'] = train_df['requester_number_of_subreddits_at_request'] / (train_df['requester_account_age_in_days_at_request'] + 1)\n",
    "train_df['request_quality'] = train_df['word_count'] * train_df['upvote_ratio']\n",
    "\n",
    "# Temporal\n",
    "train_df['request_hour'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s').dt.hour\n",
    "train_df['request_day_of_week'] = pd.to_datetime(train_df['unix_timestamp_of_request'], unit='s').dt.dayofweek\n",
    "train_df['is_hour_15'] = (train_df['request_hour'] == 15).astype(int)\n",
    "\n",
    "# Count features\n",
    "base_features = [\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_subreddits_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'requester_account_age_in_days_at_request'\n",
    "]\n",
    "\n",
    "text_features = ['text_length', 'word_count', 'title_length', 'title_word_count', 'avg_word_length']\n",
    "persuasion_features = [f'has_{word}' for word in persuasion_words]\n",
    "punctuation_features = ['exclamation_count', 'question_count']\n",
    "interaction_features = [\n",
    "    'user_credibility', 'comments_per_post', 'upvote_ratio', 'subreddit_diversity', \n",
    "    'request_quality', 'request_hour', 'request_day_of_week', 'is_hour_15'\n",
    "]\n",
    "\n",
    "all_num_features = base_features + text_features + persuasion_features + punctuation_features + interaction_features\n",
    "available_features = [f for f in all_num_features if f in train_df.columns]\n",
    "\n",
    "print(f\"Numerical features: {len(available_features)}\")\n",
    "print(f\"TF-IDF features (enhanced): 10,000\")\n",
    "print(f\"Total features: {len(available_features) + 10000}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Feature-to-sample ratio: {(len(available_features) + 10000) / len(train_df):.2f}:1\")\n",
    "print(f\"\\nThis is SEVERELY OVERPARAMETRIZED!\")\n",
    "print(f\"Recommended ratio: < 0.1:1 for generalization\")\n",
    "\n",
    "# Check correlations among engineered features\n",
    "print(f\"\\n=== FEATURE CORRELATION ANALYSIS ===\")\n",
    "corr_matrix = train_df[available_features].corr().abs()\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.9:\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "print(f\"Highly correlated feature pairs (>0.9): {len(high_corr)}\")\n",
    "if high_corr:\n",
    "    print(\"Top correlated pairs:\")\n",
    "    for f1, f2, corr in high_corr[:5]:\n",
    "        print(f\"  {f1} <-> {f2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test proper TF-IDF fitting (no leakage)\n",
    "print(\"=== PROPER TF-IDF VALIDATION ===\")\n",
    "\n",
    "# Combine title and text\n",
    "combined_text_train = train_df['request_title'].fillna('') + ' ' + train_df['request_text_edit_aware'].fillna('')\n",
    "combined_text_test = test_df['request_title'].fillna('') + ' ' + test_df['request_text_edit_aware'].fillna('')\n",
    "\n",
    "# Test different TF-IDF configurations\n",
    "tfidf_configs = [\n",
    "    {\"name\": \"Baseline (100 features)\", \"max_features\": 100, \"ngram_range\": (1, 1)},\n",
    "    {\"name\": \"Enhanced-Leakage (10k features)\", \"max_features\": 10000, \"ngram_range\": (1, 3)},\n",
    "    {\"name\": \"Proper-No-Leakage (2k features)\", \"max_features\": 2000, \"ngram_range\": (1, 2)},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in tfidf_configs:\n",
    "    print(f\"\\nTesting: {config['name']}\")\n",
    "    \n",
    "    # Create TF-IDF\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=config['max_features'],\n",
    "        ngram_range=config['ngram_range'],\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    if \"Leakage\" in config['name']:\n",
    "        # Fit on combined (WRONG - what exp_001 did)\n",
    "        all_text = pd.concat([combined_text_train, combined_text_test])\n",
    "        tfidf.fit(all_text)\n",
    "        print(f\"  Fitting on {len(all_text)} samples (TRAIN+TEST) - LEAKAGE!\")\n",
    "    else:\n",
    "        # Fit on train only (CORRECT)\n",
    "        tfidf.fit(combined_text_train)\n",
    "        print(f\"  Fitting on {len(combined_text_train)} samples (TRAIN ONLY) - VALID\")\n",
    "    \n",
    "    # Transform\n",
    "    tfidf_train = tfidf.transform(combined_text_train)\n",
    "    \n",
    "    # Quick validation with simple model\n",
    "    y = train_df['requester_received_pizza'].astype(int).values\n",
    "    \n",
    "    # Simple LightGBM with few iterations\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(tfidf_train, y)):\n",
    "        X_tr, X_val = tfidf_train[train_idx], tfidf_train[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.1,\n",
    "            'verbose': -1,\n",
    "            'scale_pos_weight': 3.0\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "        valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=100,\n",
    "            valid_sets=[valid_data],\n",
    "            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        fold_score = roc_auc_score(y_val, val_pred)\n",
    "        fold_scores.append(fold_score)\n",
    "    \n",
    "    cv_score = np.mean(fold_scores)\n",
    "    cv_std = np.std(fold_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'config': config['name'],\n",
    "        'score': cv_score,\n",
    "        'std': cv_std,\n",
    "        'n_features': tfidf_train.shape[1]\n",
    "    })\n",
    "    \n",
    "    print(f\"  CV Score: {cv_score:.4f} ± {cv_std:.4f}\")\n",
    "    print(f\"  Features: {tfidf_train.shape[1]}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "best_config = results_df.loc[results_df['score'].idxmax()]\n",
    "print(f\"\\nBest configuration: {best_config['config']}\")\n",
    "print(f\"Score: {best_config['score']:.4f} ± {best_config['std']:.4f}\")\n",
    "print(f\"Features: {best_config['n_features']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12529da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research BERT approach feasibility\n",
    "print(\"=== BERT FEASIBILITY ANALYSIS ===\")\n",
    "\n",
    "# Check if GPU is available (BERT needs it for reasonable speed)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"✗ No GPU available - BERT will be very slow on CPU\")\n",
    "\n",
    "# Estimate BERT processing time\n",
    "n_samples = len(train_df) + len(test_df)\n",
    "print(f\"\\nTotal samples to process: {n_samples}\")\n",
    "print(f\"Average text length: {train_df['request_text_edit_aware'].str.len().mean():.0f} chars\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Estimated processing time on GPU: ~5-10 minutes\")\n",
    "else:\n",
    "    print(f\"Estimated processing time on CPU: ~1-2 hours (NOT RECOMMENDED)\")\n",
    "\n",
    "# Check available pretrained models\n",
    "print(f\"\\nRecommended models:\")\n",
    "print(f\"1. distilbert-base-uncased (fastest, 768 dims)\")\n",
    "print(f\"2. bert-base-uncased (standard, 768 dims)\")\n",
    "print(f\"3. all-MiniLM-L6-v2 (very fast, 384 dims)\")\n",
    "\n",
    "# Memory requirements\n",
    "embedding_dim = 768\n",
    "memory_per_sample = embedding_dim * 4 / 1e6  # MB\n",
    "print(f\"\\nMemory requirements:\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Memory per sample: {memory_per_sample:.2f} MB\")\n",
    "print(f\"  Total memory for all samples: {memory_per_sample * n_samples:.0f} MB\")\n",
    "print(f\"  This is manageable even on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what went wrong with enhanced features\n",
    "print(\"=== FAILURE ANALYSIS ===\")\n",
    "\n",
    "print(\"Why did enhanced features (0.6129) perform WORSE than baseline (0.6374)?\")\n",
    "print()\n",
    "print(\"1. DATA LEAKAGE (Primary Issue)\")\n",
    "print(\"   - TF-IDF fit on train+test data\")\n",
    "print(\"   - Test distribution influenced training\")\n",
    "print(\"   - CV scores are INVALID\")\n",
    "print()\n",
    "print(\"2. OVERPARAMETERIZATION (Secondary Issue)\")\n",
    "print(f\"   - 10,000 TF-IDF features + {len(available_features)} numerical features\")\n",
    "print(f\"   - {(len(available_features) + 10000) / len(train_df):.2f} features per sample\")\n",
    "print(\"   - Severe overfitting risk\")\n",
    "print()\n",
    "print(\"3. FEATURE QUALITY (Tertiary Issue)\")\n",
    "print(\"   - Many engineered features may be noisy\")\n",
    "print(\"   - No feature selection performed\")\n",
    "print(\"   - High correlation between some features\")\n",
    "print()\n",
    "print(\"4. MODEL INSTABILITY\")\n",
    "print(\"   - Early stopping varied 9-140 rounds across folds\")\n",
    "print(\"   - High learning rate (0.05) may cause instability\")\n",
    "print()\n",
    "print(\"=== WHAT TO DO NEXT ===\")\n",
    "print()\n",
    "print(\"1. FIX LEAKAGE: Fit TF-IDF only on training data\")\n",
    "print(\"2. REDUCE DIMENSIONALITY: Use 1,000-2,000 TF-IDF features max\")\n",
    "print(\"3. TRY BERT: Use pretrained embeddings (768 dims vs 10,000)\")\n",
    "print(\"4. ENSEMBLE: Combine multiple models for stability\")\n",
    "print(\"5. FEATURE SELECTION: Keep only important features\")\n",
    "print()\n",
    "print(\"Priority order: 3 > 1 > 4 > 2 > 5\")\n",
    "print(\"BERT is most likely to give breakthrough improvement\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
