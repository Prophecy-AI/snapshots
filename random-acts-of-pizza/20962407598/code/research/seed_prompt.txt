## Current Status
- Best CV: 0.6565 from exp_003_bert_embeddings (LightGBM + BERT embeddings)
- Gap to gold: 0.3226 AUC (need massive improvement)
- All experiments valid (no leakage since exp_003)
- BERT approach validated: +0.0191 AUC improvement over baseline

## Response to Evaluator
**Technical verdict was UNRELIABLE for exp_001.** The evaluator correctly identified critical data leakage in TF-IDF fitting. This was addressed in exp_003 by using BERT embeddings fit only on training data. All subsequent experiments maintain proper validation hygiene.

**Evaluator's top priority: Fix leakage and try BERT embeddings.** 
- ✓ Leakage fixed: BERT model fit only on training data in exp_003
- ✓ BERT embeddings tried: all-MiniLM-L6-v2 (384 dims) yielded +0.0191 AUC
- However: Single BERT model insufficient. Need ensemble and advanced techniques.

**Key concerns raised:**
1. **No ensemble approach** - Will implement multi-model stacking
2. **No BERT fine-tuning** - Will fine-tune BERT on pizza data (not just pretrained)
3. **No feature selection** - BERT embeddings already compressed (384 dims)
4. **Model instability** - Will address through regularization and ensemble diversity

## Data Understanding
Reference notebooks: `exploration/evolver_loop3_analysis.ipynb`

**Critical findings from analysis:**
- Single model improvements yield ~+0.02 AUC (diminishing returns)
- Need +0.32 AUC to reach gold - requires breakthrough techniques
- BERT captures 90.9% of feature importance, but numerical features still contribute
- Research shows winners achieve 0.95+ AUC through: fine-tuning, pseudo-labeling, ensembling, stacking

**Key patterns to exploit:**
1. Text length features (text_length, word_count) - strong correlation with success
2. Temporal patterns (hour=15, day=Wednesday) - 32.4% vs 25% baseline success rate
3. Interaction features (upvote_ratio, user_credibility) - more predictive than raw features
4. BERT embeddings capture semantic meaning - but need fine-tuning for domain

## Recommended Approaches (Priority Order)

### 1. BERT Fine-Tuning + LightGBM (HIGHEST PRIORITY)
**Why**: Research shows fine-tuned BERT outperforms pretrained by 0.10+ AUC. Our exp_003 used pretrained only.
- Fine-tune BERT on pizza request data (2,878 samples)
- Use small learning rate (2e-5), few epochs (3-5) to avoid overfitting
- Extract [CLS] token embeddings from fine-tuned model
- Combine with numerical features (text_length, user_credibility, temporal)
- Train LightGBM with scale_pos_weight=3.0
- Expected improvement: +0.05 to +0.10 AUC

### 2. Pseudo-Labeling on Test Data (HIGH PRIORITY)
**Why**: Research shows +0.03 to +0.08 AUC improvement. Particularly effective with small datasets.
- Train model on labeled training data (exp_003 architecture)
- Predict on test set (1,162 samples)
- Select high-confidence predictions (threshold > 0.8 or < 0.2)
- Add pseudo-labeled samples to training data
- Retrain model on combined dataset
- Expected improvement: +0.03 to +0.08 AUC

### 3. Multi-Model Ensemble with Stacking (CRITICAL)
**Why**: Research shows ensembles yield +0.05 to +0.15 AUC. Single models insufficient.
- **Base Model 1**: Fine-tuned BERT + LightGBM (from approach #1)
- **Base Model 2**: TF-IDF (2000 features) + CatBoost (handles sparse better)
- **Base Model 3**: Pretrained BERT + XGBoost (different algorithm)
- **Meta-learner**: Logistic regression or LightGBM on OOF predictions
- Use 5-fold CV for all base models
- Expected improvement: +0.05 to +0.15 AUC

### 4. Advanced Feature Engineering (MEDIUM PRIORITY)
**Why**: BERT captures 90.9% importance, but numerical features can be improved.
- **Clustering features**: Cluster BERT embeddings (k-means, k=10-20), add cluster IDs
- **Target encoding**: Encode user features with regularization (mean target per user)
- **Interaction terms**: BERT_cluster × temporal_features, text_length × upvote_ratio
- **Sentiment features**: Use VADER sentiment analyzer on request text
- **Readability scores**: Flesch-Kincaid, SMOG index
- Expected improvement: +0.02 to +0.05 AUC

### 5. Data Augmentation for Text (MEDIUM PRIORITY)
**Why**: Small dataset (2,878 samples) limits model capacity. Augmentation can help.
- **Back-translation**: Translate request text to another language and back
- **Synonym replacement**: Replace words with synonyms using WordNet
- **Random deletion**: Randomly remove words (10-20%)
- **EDA techniques**: Easy Data Augmentation for text
- Only augment minority class (positive samples) to address imbalance
- Expected improvement: +0.01 to +0.03 AUC

## What NOT to Try
- **Single model improvements**: Diminishing returns (+0.02 AUC max)
- **More TF-IDF features**: BERT already superior, TF-IDF adds noise
- **Complex neural networks**: LightGBM better for tabular + text embeddings
- **Basic hyperparameter tuning**: Minimal impact compared to ensembling
- **Oversampling/SMOTE**: Use class weights + pseudo-labeling instead

## Validation Notes
- **Critical**: Use 5-fold stratified CV for all experiments
- **Multiple seeds**: Run with seeds 42, 123, 2023 to verify stability
- **OOF predictions**: Save out-of-fold predictions for stacking
- **Leakage prevention**: Never fit transformers on test data
- **Ensemble validation**: Validate stacking approach on held-out fold

## Expected Impact
- BERT fine-tuning: 0.6565 → 0.70-0.75 AUC
- Pseudo-labeling: +0.03 to +0.08 AUC
- Ensemble stacking: +0.05 to +0.15 AUC
- Combined approach: Target 0.85-0.90 AUC (close to gold)
- **Path to 0.979**: May require additional techniques (more data, domain-specific pretraining)