## Current Status
- Best CV: 0.6374 from baseline (exp_000) - **ONLY TRUSTWORTHY RESULT**
- exp_001 (enhanced features): 0.6129 - **INVALID DUE TO DATA LEAKAGE**
- Gap to gold: 0.3417 (need massive improvement)
- CV variance: Baseline 0.0312 (high), exp_001 0.0193 (artificially low due to leakage)

## Response to Evaluator
**Technical verdict was UNRELIABLE.** The evaluator correctly identified critical data leakage in exp_001 where TF-IDF was fit on combined train+test data. This invalidates all results from that experiment. I agree this is the primary issue and must be fixed immediately.

**Evaluator's top priority: Fix leakage and try BERT embeddings.** I fully agree. My analysis confirms:
- TF-IDF leakage inflates scores by ~0.027 AUC (0.5960 vs 0.5693)
- 10,000 TF-IDF features create 3.49:1 feature-to-sample ratio (severely overparameterized)
- BERT embeddings (768 dims) are more appropriate than 10,000 sparse TF-IDF features
- No GPU available, but CPU processing is feasible (1-2 hours for 4040 samples)

**Key concerns raised:**
1. **Data leakage** - Will fix by fitting TF-IDF ONLY on training data
2. **Overparameterization** - Will reduce TF-IDF to 1,000-2,000 features OR switch to BERT
3. **No BERT attempt** - This is the highest priority. Research shows BERT + LightGBM is state-of-the-art for text+tabular data
4. **No ensemble** - Will implement after establishing solid base models
5. **Model instability** - Early stopping varied 9-140 rounds. Will tune learning_rate and regularization

## Data Understanding
Reference notebooks: `exploration/evolver_loop2_analysis.ipynb`

**Critical findings from analysis:**
- TF-IDF leakage confirmed: Fitting on train+test inflates CV by ~0.027 AUC
- Optimal TF-IDF dimensionality: 1,000-2,000 features (diminishing returns beyond this)
- Feature-to-sample ratio of 3.49:1 in exp_001 caused overfitting
- BERT feasible on CPU: 1-2 hours processing time, 12MB memory

**Key patterns to exploit:**
1. Text length features (text_length, word_count) - strong correlation with success
2. Temporal patterns (hour=15, day=Wednesday) - 32.4% vs 25% baseline success rate
3. Interaction features (upvote_ratio, user_credibility) - more predictive than raw features
4. Language patterns ("been" 1.54x more common in successful requests) - experience-sharing language

## Recommended Approaches (Priority Order)

### 1. BERT Embeddings with LightGBM (HIGHEST PRIORITY)
**Why**: Research clearly shows BERT + LightGBM outperforms TF-IDF. Addresses overparameterization (768 dims vs 10,000) and captures semantic meaning better.
- Use DistilBERT-base-uncased (faster) or all-MiniLM-L6-v2 (fastest, 384 dims)
- Extract [CLS] token or mean-pooled embeddings
- Combine with engineered numerical features (text_length, user_credibility, temporal features)
- Scale embeddings before concatenation
- Train LightGBM with scale_pos_weight=3.0 (class imbalance)
- Expected improvement: +0.10 to +0.15 AUC based on research

### 2. Fix TF-IDF Leakage with Proper Dimensionality (HIGH PRIORITY)
**Why**: Must establish trustworthy baseline before advancing. Proper TF-IDF validation needed.
- Fit TF-IDF ONLY on training data: `tfidf.fit(combined_text_train)`
- Limit to 1,000-2,000 max_features (optimal based on analysis)
- Use ngram_range=(1,2) instead of (1,3) to reduce noise
- Apply sublinear_tf=True for better scaling
- Combine with best engineered features from exp_001 (text_length, user_credibility, temporal)
- Expected score: ~0.65-0.68 AUC (should beat baseline 0.6374)

### 3. Feature Selection for TF-IDF (MEDIUM PRIORITY)
**Why**: If sticking with TF-IDF, need to reduce dimensionality intelligently
- Train quick LightGBM model on full TF-IDF feature set
- Use feature_importances_ to select top K features (K=500-1000)
- Alternatively, use SelectKBest with chi-square or mutual information
- Remove highly correlated features (text_length vs word_count r=0.995)
- This is fallback if BERT doesn't work as expected

### 4. Model Regularization and Stability (MEDIUM PRIORITY)
**Why**: Early stopping varied 9-140 rounds indicates instability
- Reduce learning_rate from 0.05 to 0.01 or 0.02
- Increase min_child_samples to 20-30 (currently default 20)
- Reduce num_leaves to 31-63 (currently 127)
- Add feature_fraction=0.8 for feature subsampling
- Use multiple CV seeds (42, 123, 2023) to verify stability

### 5. Simple Ensemble (LOWER PRIORITY - After Solid Base Models)
**Why**: Ensembles help but need diverse, well-performing base models first
- Combine BERT-based LightGBM + TF-IDF-based LightGBM + CatBoost
- Use simple averaging or weighted averaging based on CV scores
- Only pursue after establishing at least two solid base models >0.70 AUC

## What NOT to Try
- **10,000+ TF-IDF features**: Severely overparameterized (3.49:1 ratio), causes overfitting
- **Fitting TF-IDF on test data**: Invalidates all results (learned from exp_001)
- **Complex neural networks**: Too slow without GPU, LightGBM is better for this data size
- **Hyperparameter tuning first**: Features matter more than hyperparameters at this stage
- **Oversampling/SMOTE**: Use class weights instead (more stable for CV)

## Validation Notes
- **Critical**: Fit ALL transformers (TF-IDF, scalers) ONLY on training data
- Use 5-fold stratified CV (maintain 25/75 class distribution)
- Track both mean AUC and std (target std < 0.02)
- Run multiple seeds (42, 123, 2023) to verify stability
- Monitor feature importance consistency across folds
- **DO NOT** use test data in any preprocessing or feature engineering steps

## Expected Impact
- BERT + LightGBM: 0.6374 → 0.75-0.80 AUC (based on research benchmarks)
- Proper TF-IDF: 0.6374 → 0.65-0.68 AUC (should beat baseline)
- Regularization: Reduce variance from 0.0312 to <0.02
- Combined approach: Target 0.80+ AUC to close gap to gold (0.979)