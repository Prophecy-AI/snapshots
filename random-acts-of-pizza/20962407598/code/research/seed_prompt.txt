## Current Status
- Best CV: 0.6374 from baseline (exp_000)
- Gap to gold: 0.3417 (need massive improvement)
- CV variance: 0.0312 (high - indicates instability)
- Class imbalance: 75% negative, 25% positive

## Data Understanding
Reference notebooks: `exploration/eda.ipynb` and `exploration/evolver_loop1_analysis.ipynb`

Key patterns discovered:
1. **Text length matters**: text_length (0.1199) and word_count (0.1177) correlate positively with success
2. **Temporal patterns**: Hour 15 (3pm) has 32.4% success rate vs ~25% average; Wednesday best day
3. **Interaction features**: upvote_ratio (0.0953) stronger than raw upvotes
4. **Language patterns**: "been" is 1.54x more common in successful requests (experience-sharing language)
5. **High variance**: 0.0312 CV std suggests overfitting or weak features

## Recommended Approaches (Priority Order)

### 1. Enhanced Text Feature Engineering (CRITICAL)
**Why**: Baseline only used basic TF-IDF. Text is primary signal.
- **TF-IDF improvements**: 
  - Add n-grams (1-3) to capture phrases
  - Increase max_features (try 5000, 10000)
  - Use sublinear_tf=True for better scaling
- **Sentiment analysis**: Use VADER or TextBlob for sentiment scores
- **Readability metrics**: Flesch-Kincaid, word diversity (unique/total words)
- **Persuasion language**: Count persuasive words ("please", "help", "appreciate", "grateful")
- **Named entities**: Count mentions of locations, people, organizations
- **Punctuation patterns**: Count exclamation marks, question marks

### 2. Advanced Interaction Features (HIGH PRIORITY)
**Why**: Individual features weak, combinations may be stronger
- **User credibility**: (upvotes_plus_downvotes) / (account_age + 1)
- **Engagement ratio**: comments_at_request / posts_at_request
- **Subreddit diversity**: number_of_subreddits / account_age
- **Request quality**: word_count * upvote_ratio
- **Temporal interactions**: hour_of_day * day_of_week
- **Text-activity interactions**: word_count * requester_number_of_comments

### 3. Better Model Architecture (HIGH PRIORITY)
**Why**: LightGBM good but can be improved; class imbalance not addressed
- **Class weighting**: Use scale_pos_weight = 3.0 (negative/positive ratio)
- **Try CatBoost**: Handles categorical features natively, good with text features
- **Ensemble approach**: Combine LightGBM + CatBoost + XGBoost
- **Stacking**: Use linear model as meta-learner on top of base models
- **Regularization**: Increase min_child_samples, reduce num_leaves to combat variance

### 4. Feature Selection & Validation (MEDIUM PRIORITY)
**Why**: High variance suggests need for robust feature selection
- **Feature importance**: Use baseline feature importance to prune weak features
- **Correlation analysis**: Remove highly correlated features
- **Multiple CV seeds**: Run CV with seeds 42, 123, 2023 to verify stability
- **Stratified CV**: Ensure each fold maintains 25/75 class distribution

### 5. External Data & Embeddings (MEDIUM PRIORITY)
**Why**: Research shows BERT embeddings + tabular data works well
- **BERT embeddings**: Use pretrained BERT to encode request_text, add as features
- **GloVe embeddings**: Average word vectors for text
- **Sentiment lexicons**: Use AFINN, VADER lexicons for sentiment scores

## What NOT to Try
- **Basic hyperparameter tuning**: Not enough improvement potential vs feature engineering
- **Single model focus**: Need ensemble diversity given high variance
- **Complex neural networks**: Too slow, LightGBM/CatBoost better for tabular + text
- **Oversampling**: Use class weights instead (more stable for CV)

## Validation Notes
- Use 5-fold stratified CV (maintain 25/75 distribution)
- Track both mean AUC and std (target std < 0.02)
- Run multiple seeds (42, 123, 2023) to verify stability
- Monitor feature importance consistency across folds

## Expected Impact
- Text improvements: +0.05 to +0.10 AUC
- Interaction features: +0.03 to +0.05 AUC  
- Class weighting: +0.02 to +0.03 AUC
- Ensemble: +0.02 to +0.04 AUC
- **Total potential**: 0.6374 â†’ 0.75-0.80 (still need more breakthroughs)