{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc42b63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T21:54:23.713569Z",
     "iopub.status.busy": "2026-01-10T21:54:23.712348Z",
     "iopub.status.idle": "2026-01-10T21:54:24.285295Z",
     "shell.execute_reply": "2026-01-10T21:54:24.284606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2878\n",
      "Type of data: <class 'list'>\n",
      "First sample keys: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "## Data Understanding\n",
    "**Reference notebooks for data characteristics:**\n",
    "- `exploration/eda.ipynb` - Contains full EDA: 2,878 training samples, 24.8% positive rate (moderate class imbalance), text features (title: avg 72 chars, text: avg 403 chars), 32 total features including user activity metrics\n",
    "- Key finding: request_number_of_comments_at_retrieval has highest correlation (0.29) with target\n",
    "- Use findings from these notebooks when implementing features\n",
    "\n",
    "## Models for Text + Tabular Data\n",
    "For problems combining text and structured meta-data:\n",
    "\n",
    "**Primary Approaches:**\n",
    "1. **Gradient Boosting on Text Embeddings**: Extract embeddings from pretrained transformers (BERT/RoBERTa) and feed to LightGBM/XGBoost/CatBoost. This consistently outperforms single-model approaches by leveraging both semantic text understanding and tabular learning strengths.\n",
    "\n",
    "2. **Multimodal Transformers**: Convert tabular features to text strings and prepend to documents, then fine-tune end-to-end. Effective when text is primary signal but meta-data provides important context.\n",
    "\n",
    "3. **Two-Stage Ensembles**: Generate LLM embeddings → Train multiple tabular models → Stack with meta-learner. This approach won multiple Kaggle competitions by capturing diverse patterns.\n",
    "\n",
    "**Model Selection Guidelines:**\n",
    "- Start with LightGBM on TF-IDF + tabular features (strong baseline)\n",
    "- Add transformer embeddings for 0.02-0.05 AUC boost\n",
    "- Neural networks useful when >5K samples and text-heavy signal\n",
    "- For Reddit data: SocBERT (trained on 929M Reddit posts) outperforms generic BERT\n",
    "\n",
    "## Preprocessing for Reddit/Text Data\n",
    "\n",
    "**Text Features:**\n",
    "- **TF-IDF**: Use n-grams (1-3) on both title and text separately. Include char n-grams for robustness to typos.\n",
    "- **Text Length Features**: Word count, char count, avg word length - surprisingly predictive\n",
    "- **Sentiment/LIWC**: Extract gratitude, politeness markers (\"please\", \"thank\", \"appreciate\")\n",
    "- **Topic Modeling**: LDA with 10-20 topics captures request themes (financial hardship, celebration, etc.)\n",
    "- **Edit-Aware Text**: Use request_text_edit_aware to remove success-indicating edits\n",
    "- **Reddit-Specific Cleaning**: Handle markdown, user mentions (/u/username), subreddit links (r/subname), URLs, emojis\n",
    "\n",
    "**Tabular Features:**\n",
    "- **User Activity Ratios**: comments/posts ratios, RAOP-specific vs general activity\n",
    "- **Temporal Features**: Hour of day, day of week from timestamps (people may be more generous on weekends)\n",
    "- **Vote Ratios**: upvote/downvote ratios at request time vs retrieval time\n",
    "- **Flair Encoding**: One-hot or target encode (shroom=received, PIF=given after receiving)\n",
    "- **Subreddit Diversity**: Number of unique subreddits as social breadth indicator\n",
    "\n",
    "**Handling Data Quality Issues:**\n",
    "- post_was_edited contains mixed types (booleans + timestamps) - clean to boolean\n",
    "- Zero-length text fields need special handling\n",
    "\n",
    "## Handling Class Imbalance (24.8% positive)\n",
    "\n",
    "**Validation Strategy:**\n",
    "- **Stratified K-Fold (k=5)**: Essential for stable CV scores given class imbalance\n",
    "- **Stratified train/val split**: Use stratify parameter to maintain class distribution\n",
    "\n",
    "**Training Techniques:**\n",
    "1. **Class Weights**: Use scale_pos_weight (XGBoost) or class_weight (sklearn) = (negative_samples/positive_samples) ≈ 3.0\n",
    "2. **Focal Loss**: Alternative to class weighting, focuses on hard examples\n",
    "3. **Oversampling**: SMOTE or random oversample minority class in training folds only\n",
    "4. **Threshold Tuning**: Optimize decision threshold on validation set rather than using 0.5\n",
    "\n",
    "**Avoid**: Oversampling before CV splits - leads to data leakage\n",
    "\n",
    "## Feature Engineering Specific to This Problem\n",
    "\n",
    "**High-Impact Features:**\n",
    "1. **Request Urgency Signals**: Words like \"desperate\", \"starving\", \"broke\", \"need\" - but balance with authenticity\n",
    "2. **Gratitude Expressions**: \"thank you\", \"appreciate\", \"grateful\" - politeness matters\n",
    "3. **Narrative Length**: Longer, detailed stories often more successful (but not too long)\n",
    "4. **Community Engagement**: RAOP-specific comment count more predictive than general Reddit activity\n",
    "5. **Reciprocity Indicators**: Mention of past giving, promises to \"pay it forward\"\n",
    "\n",
    "**From Winning Solutions:**\n",
    "- Combine request_title + request_text for TF-IDF (captures full context)\n",
    "- Use request_text_edit_aware to remove post-success edits that leak target\n",
    "- Extract LIWC-style psycholinguistic features (positive emotion, social words)\n",
    "- Create interaction features: account_age × activity_level, text_length × politeness\n",
    "- **Character-level features**: Character n-grams (3-5) capture typos and writing style\n",
    "- **Domain-adapted embeddings**: SocBERT or Tweet2Vec for character-level patterns\n",
    "\n",
    "## Ensembling Strategies\n",
    "\n",
    "**For ROC-AUC Optimization:**\n",
    "1. **Weighted Average**: Weight models by validation AUC (e.g., 0.4×LGBM + 0.3×XGB + 0.3×CatBoost)\n",
    "2. **Rank Averaging**: More robust to outliers than probability averaging\n",
    "3. **Stacking**: Use logistic regression or LightGBM as meta-learner on out-of-fold predictions\n",
    "4. **Diversity is Key**: Combine tree models (different seeds, feature subsets) with neural networks\n",
    "\n",
    "**Best Practices:**\n",
    "- Ensure base models are diverse (different algorithms, different features)\n",
    "- Use 5-fold CV for stacking to avoid overfitting\n",
    "- Calibrate probabilities before ensembling (especially for tree models)\n",
    "\n",
    "## Validation and Optimization\n",
    "\n",
    "**Cross-Validation:**\n",
    "- **Primary**: Stratified 5-fold CV for all model development\n",
    "- **Final Score**: Use full training set, evaluate on public LB sparingly (risk of overfitting)\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "- **Optuna/Random Search**: 50-100 trials sufficient for gradient boosting\n",
    "- **Key Parameters**: Learning rate (0.01-0.1), max_depth (3-7), num_leaves (31-127), subsample (0.7-1.0)\n",
    "- **Early Stopping**: Use validation AUC with patience=50\n",
    "\n",
    "**Feature Selection:**\n",
    "- Use feature importance from LightGBM to prune low-impact features\n",
    "- Recursive Feature Elimination (RFE) can improve generalization\n",
    "\n",
    "## Implementation Priority\n",
    "\n",
    "**Phase 1 (Baseline):**\n",
    "1. TF-IDF on combined title+text (1-3 n-grams)\n",
    "2. Basic tabular features (scaling only)\n",
    "3. LightGBM with class weighting\n",
    "4. Stratified 5-fold CV\n",
    "\n",
    "**Phase 2 (Improvement):**\n",
    "1. Add transformer embeddings (BERT-base)\n",
    "2. Engineer Reddit-specific features (gratitude, urgency)\n",
    "3. Try CatBoost for categorical handling\n",
    "4. Simple ensemble (average 2-3 models)\n",
    "\n",
    "**Phase 3 (Competitive):**\n",
    "1. Full multimodal approach (text + tabular)\n",
    "2. Advanced feature engineering (topic modeling, LIWC)\n",
    "3. Stacked ensemble with 5+ diverse models\n",
    "4. Threshold optimization on validation set\n",
    "\n",
    "## Key Insights from Data Exploration\n",
    "- Moderate class imbalance requires careful handling but not extreme measures\n",
    "- Text length and community engagement metrics are predictive\n",
    "- User flair is sparse but informative (shroom=received before)\n",
    "- Edit-aware text prevents leakage from post-success edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4047f922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T21:55:02.373231Z",
     "iopub.status.busy": "2026-01-10T21:55:02.372759Z",
     "iopub.status.idle": "2026-01-10T21:55:02.413987Z",
     "shell.execute_reply": "2026-01-10T21:55:02.413178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "requester_received_pizza\n",
      "False    2163\n",
      "True      715\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance: 0.248 (positive rate)\n",
      "\n",
      "Missing values per column:\n",
      "giver_username_if_known                        0\n",
      "number_of_downvotes_of_request_at_retrieval    0\n",
      "number_of_upvotes_of_request_at_retrieval      0\n",
      "post_was_edited                                0\n",
      "request_id                                     0\n",
      "request_number_of_comments_at_retrieval        0\n",
      "request_text                                   0\n",
      "request_text_edit_aware                        0\n",
      "request_title                                  0\n",
      "requester_account_age_in_days_at_request       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# Check target distribution\n",
    "print(\"Target distribution:\")\n",
    "print(df['requester_received_pizza'].value_counts())\n",
    "print(f\"\\nClass balance: {df['requester_received_pizza'].mean():.3f} (positive rate)\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum().head(10))  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4964ab5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T21:56:35.063977Z",
     "iopub.status.busy": "2026-01-10T21:56:35.063119Z",
     "iopub.status.idle": "2026-01-10T21:56:35.090109Z",
     "shell.execute_reply": "2026-01-10T21:56:35.089332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text feature analysis:\n",
      "\n",
      "Request title length (characters):\n",
      "count    2878.000000\n",
      "mean       71.572967\n",
      "std        36.233487\n",
      "min         7.000000\n",
      "25%        46.000000\n",
      "50%        64.000000\n",
      "75%        90.000000\n",
      "max       272.000000\n",
      "Name: request_title, dtype: float64\n",
      "\n",
      "Request text length (characters):\n",
      "count    2878.000000\n",
      "mean      402.521543\n",
      "std       362.393727\n",
      "min         0.000000\n",
      "25%       182.000000\n",
      "50%       308.000000\n",
      "75%       503.750000\n",
      "max      4460.000000\n",
      "Name: request_text, dtype: float64\n",
      "\n",
      "Request text edit aware length (characters):\n",
      "count    2878.000000\n",
      "mean      394.567755\n",
      "std       351.922518\n",
      "min         0.000000\n",
      "25%       180.000000\n",
      "50%       302.000000\n",
      "75%       498.000000\n",
      "max      4460.000000\n",
      "Name: request_text_edit_aware, dtype: float64\n",
      "\n",
      "==================================================\n",
      "EXAMPLE 1 (successful):\n",
      "Title: [REQUEST] Not much food until tomorrow.\n",
      "Text (first 200 chars): I will go ahead and say that I got a pizza meal from here before as to not seem like I'm scamming anyone. I have been promised 2 well-paying jobs and one minimum wage + .40 to fill in the gaps of unem...\n",
      "\n",
      "==================================================\n",
      "EXAMPLE 2 (unsuccessful):\n",
      "Title: [REQUEST] Oceanside, Ca. USA-  US Marine getting ready to deploy.\n",
      "Text (first 200 chars): I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time ...\n"
     ]
    }
   ],
   "source": [
    "# Examine text features\n",
    "print(\"Text feature analysis:\")\n",
    "print(f\"\\nRequest title length (characters):\")\n",
    "print(df['request_title'].str.len().describe())\n",
    "\n",
    "print(f\"\\nRequest text length (characters):\")\n",
    "print(df['request_text'].str.len().describe())\n",
    "\n",
    "print(f\"\\nRequest text edit aware length (characters):\")\n",
    "print(df['request_text_edit_aware'].str.len().describe())\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE 1 (successful):\")\n",
    "successful = df[df['requester_received_pizza'] == True].iloc[0]\n",
    "print(f\"Title: {successful['request_title']}\")\n",
    "print(f\"Text (first 200 chars): {successful['request_text'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE 2 (unsuccessful):\")\n",
    "unsuccessful = df[df['requester_received_pizza'] == False].iloc[0]\n",
    "print(f\"Title: {unsuccessful['request_title']}\")\n",
    "print(f\"Text (first 200 chars): {unsuccessful['request_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4b2c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T21:57:39.727848Z",
     "iopub.status.busy": "2026-01-10T21:57:39.726799Z",
     "iopub.status.idle": "2026-01-10T21:57:39.768684Z",
     "shell.execute_reply": "2026-01-10T21:57:39.767883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical feature statistics:\n",
      "\n",
      "requester_account_age_in_days_at_request:\n",
      "count    2878.000000\n",
      "mean      250.682364\n",
      "std       301.838771\n",
      "min         0.000000\n",
      "25%         3.038877\n",
      "50%       155.156377\n",
      "75%       383.640090\n",
      "max      2809.750787\n",
      "Name: requester_account_age_in_days_at_request, dtype: float64\n",
      "\n",
      "requester_number_of_comments_at_request:\n",
      "count    2878.000000\n",
      "mean      112.311327\n",
      "std       192.017515\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%        22.000000\n",
      "75%       132.000000\n",
      "max       981.000000\n",
      "Name: requester_number_of_comments_at_request, dtype: float64\n",
      "\n",
      "requester_number_of_posts_at_request:\n",
      "count    2878.000000\n",
      "mean       21.614663\n",
      "std        51.580719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         4.000000\n",
      "75%        21.000000\n",
      "max       867.000000\n",
      "Name: requester_number_of_posts_at_request, dtype: float64\n",
      "\n",
      "requester_upvotes_minus_downvotes_at_request:\n",
      "count      2878.000000\n",
      "mean       1184.582349\n",
      "std        4198.255486\n",
      "min         -67.000000\n",
      "25%           3.000000\n",
      "50%         171.000000\n",
      "75%        1124.500000\n",
      "max      155010.000000\n",
      "Name: requester_upvotes_minus_downvotes_at_request, dtype: float64\n",
      "\n",
      "requester_upvotes_plus_downvotes_at_request:\n",
      "count    2.878000e+03\n",
      "mean     3.988570e+03\n",
      "std      3.012747e+04\n",
      "min      0.000000e+00\n",
      "25%      8.000000e+00\n",
      "50%      3.355000e+02\n",
      "75%      2.251500e+03\n",
      "max      1.286864e+06\n",
      "Name: requester_upvotes_plus_downvotes_at_request, dtype: float64\n",
      "\n",
      "requester_number_of_subreddits_at_request:\n",
      "count    2878.000000\n",
      "mean       17.857192\n",
      "std        21.784934\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%        11.000000\n",
      "75%        26.000000\n",
      "max       186.000000\n",
      "Name: requester_number_of_subreddits_at_request, dtype: float64\n",
      "\n",
      "request_number_of_comments_at_retrieval:\n",
      "count    2878.000000\n",
      "mean        2.858582\n",
      "std         4.783656\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         4.000000\n",
      "max        61.000000\n",
      "Name: request_number_of_comments_at_retrieval, dtype: float64\n",
      "\n",
      "number_of_upvotes_of_request_at_retrieval:\n",
      "count    2878.000000\n",
      "mean        6.090688\n",
      "std        10.501259\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         7.000000\n",
      "max       345.000000\n",
      "Name: number_of_upvotes_of_request_at_retrieval, dtype: float64\n",
      "\n",
      "number_of_downvotes_of_request_at_retrieval:\n",
      "count    2878.000000\n",
      "mean        2.428075\n",
      "std         3.035568\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         3.000000\n",
      "max        47.000000\n",
      "Name: number_of_downvotes_of_request_at_retrieval, dtype: float64\n",
      "\n",
      "============================================================\n",
      "Categorical features:\n",
      "\n",
      "User flair distribution:\n",
      "requester_user_flair\n",
      "shroom    677\n",
      "PIF        38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Post edited distribution:\n",
      "post_was_edited\n",
      "False           2423\n",
      "True             241\n",
      "1375324604.0       1\n",
      "1366314331.0       1\n",
      "1367280954.0       1\n",
      "                ... \n",
      "1379372126.0       1\n",
      "1378425306.0       1\n",
      "1374109637.0       1\n",
      "1358627245.0       1\n",
      "1372729287.0       1\n",
      "Name: count, Length: 216, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Correlation with target:\n",
      "requester_received_pizza                        1.000000\n",
      "request_number_of_comments_at_retrieval         0.290709\n",
      "number_of_upvotes_of_request_at_retrieval       0.090767\n",
      "requester_number_of_subreddits_at_request       0.047001\n",
      "requester_account_age_in_days_at_request        0.043374\n",
      "requester_upvotes_minus_downvotes_at_request    0.043084\n",
      "requester_number_of_posts_at_request            0.037975\n",
      "requester_number_of_comments_at_request         0.036705\n",
      "requester_upvotes_plus_downvotes_at_request     0.033247\n",
      "number_of_downvotes_of_request_at_retrieval     0.020646\n",
      "Name: requester_received_pizza, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Examine numerical features\n",
    "numerical_cols = [\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'requester_number_of_subreddits_at_request',\n",
    "    'request_number_of_comments_at_retrieval',\n",
    "    'number_of_upvotes_of_request_at_retrieval',\n",
    "    'number_of_downvotes_of_request_at_retrieval'\n",
    "]\n",
    "\n",
    "print(\"Numerical feature statistics:\")\n",
    "for col in numerical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].describe())\n",
    "\n",
    "# Check categorical features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Categorical features:\")\n",
    "print(f\"\\nUser flair distribution:\")\n",
    "print(df['requester_user_flair'].value_counts())\n",
    "\n",
    "print(f\"\\nPost edited distribution:\")\n",
    "print(df['post_was_edited'].value_counts())\n",
    "\n",
    "# Correlation with target for numerical features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Correlation with target:\")\n",
    "correlations = df[numerical_cols + ['requester_received_pizza']].corr()['requester_received_pizza'].sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
