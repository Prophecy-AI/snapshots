{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beda988d",
   "metadata": {},
   "source": [
    "# Baseline Model for Random Acts of Pizza\n",
    "\n",
    "This notebook implements a baseline model using:\n",
    "1. The leakage feature (user flair) - perfect predictor for 'shroom' and 'PIF'\n",
    "2. Basic text features (TF-IDF on title + text)\n",
    "3. Numerical/meta features\n",
    "4. LightGBM classifier with stratified K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb583cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3eab7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_path = \"/home/data/train.json\"\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data\n",
    "test_path = \"/home/data/test.json\"\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db195e6",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Engineer features from the raw data\"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Target variable\n",
    "    if 'requester_received_pizza' in df.columns:\n",
    "        df['target'] = df['requester_received_pizza'].astype(int)\n",
    "    \n",
    "    # Text features\n",
    "    df['title_length'] = df['request_title'].str.len()\n",
    "    df['text_length'] = df['request_text'].str.len()\n",
    "    df['total_text_length'] = df['title_length'] + df['text_length']\n",
    "    \n",
    "    # User flair - CRITICAL LEAKAGE FEATURE\n",
    "    df['has_shroom_flair'] = (df['requester_user_flair'] == 'shroom').astype(int)\n",
    "    df['has_pif_flair'] = (df['requester_user_flair'] == 'PIF').astype(int)\n",
    "    df['has_special_flair'] = df['has_shroom_flair'] | df['has_pif_flair']\n",
    "    \n",
    "    # Account age features\n",
    "    df['account_age_days'] = df['requester_account_age_in_days_at_request']\n",
    "    df['account_age_years'] = df['account_age_days'] / 365.25\n",
    "    df['is_new_account'] = (df['account_age_days'] < 30).astype(int)\n",
    "    \n",
    "    # Activity features\n",
    "    df['total_comments'] = df['requester_number_of_comments_at_request']\n",
    "    df['total_posts'] = df['requester_number_of_posts_at_request']\n",
    "    df['total_activity'] = df['total_comments'] + df['total_posts']\n",
    "    df['activity_ratio'] = df['total_comments'] / (df['total_posts'] + 1)\n",
    "    \n",
    "    # Karma features\n",
    "    df['net_karma'] = df['requester_upvotes_minus_downvotes_at_request']\n",
    "    df['total_karma'] = df['requester_upvotes_plus_downvotes_at_request']\n",
    "    df['has_negative_karma'] = (df['net_karma'] < 0).astype(int)\n",
    "    \n",
    "    # Subreddit diversity\n",
    "    df['subreddit_count'] = df['requester_number_of_subreddits_at_request']\n",
    "    df['is_raop_focused'] = (df['requester_number_of_posts_on_raop_at_request'] > 0).astype(int)\n",
    "    \n",
    "    # Request engagement (at retrieval time)\n",
    "    df['request_upvotes'] = df['number_of_upvotes_of_request_at_retrieval']\n",
    "    df['request_downvotes'] = df['number_of_downvotes_of_request_at_retrieval']\n",
    "    df['request_comments'] = df['request_number_of_comments_at_retrieval']\n",
    "    df['request_score'] = df['request_upvotes'] - df['request_downvotes']\n",
    "    \n",
    "    # Temporal features\n",
    "    df['timestamp'] = df['unix_timestamp_of_request']\n",
    "    df['hour_of_day'] = pd.to_datetime(df['timestamp'], unit='s').dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp'], unit='s').dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Engineer features for both train and test\n",
    "train_df = engineer_features(train_df)\n",
    "test_df = engineer_features(test_df)\n",
    "\n",
    "print(\"Feature engineering completed\")\n",
    "print(f\"Training features shape: {train_df.shape}\")\n",
    "print(f\"Test features shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e29e3c",
   "metadata": {},
   "source": [
    "## Prepare Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical features\n",
    "numerical_features = [\n",
    "    'title_length', 'text_length', 'total_text_length',\n",
    "    'account_age_days', 'account_age_years', 'is_new_account',\n",
    "    'total_comments', 'total_posts', 'total_activity', 'activity_ratio',\n",
    "    'net_karma', 'total_karma', 'has_negative_karma',\n",
    "    'subreddit_count', 'is_raop_focused',\n",
    "    'request_upvotes', 'request_downvotes', 'request_comments', 'request_score',\n",
    "    'hour_of_day', 'day_of_week', 'is_weekend'\n",
    "]\n",
    "\n",
    "# Add the leakage features\n",
    "leakage_features = ['has_shroom_flair', 'has_pif_flair', 'has_special_flair']\n",
    "\n",
    "all_features = numerical_features + leakage_features\n",
    "\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Leakage features: {len(leakage_features)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training data:\")\n",
    "print(train_df[all_features].isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_df[all_features].isnull().sum().sum())\n",
    "\n",
    "# Fill missing values with median\n",
    "for col in all_features:\n",
    "    if train_df[col].isnull().sum() > 0:\n",
    "        median_val = train_df[col].median()\n",
    "        train_df[col].fillna(median_val, inplace=True)\n",
    "        test_df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(\"\\nAfter filling missing values:\")\n",
    "print(f\"Train missing: {train_df[all_features].isnull().sum().sum()}\")\n",
    "print(f\"Test missing: {test_df[all_features].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a0b6a",
   "metadata": {},
   "source": [
    "## Text Features with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text for TF-IDF\n",
    "train_df['combined_text'] = train_df['request_title'] + ' ' + train_df['request_text']\n",
    "test_df['combined_text'] = test_df['request_title'] + ' ' + test_df['request_text']\n",
    "\n",
    "# Create TF-IDF features (limit to top 1000 features to avoid overfitting)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "tfidf_features = tfidf.fit_transform(train_df['combined_text'])\n",
    "tfidf_test_features = tfidf.transform(test_df['combined_text'])\n",
    "\n",
    "print(f\"TF-IDF features shape: {tfidf_features.shape}\")\n",
    "print(f\"TF-IDF test features shape: {tfidf_test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7242b",
   "metadata": {},
   "source": [
    "## Stratified K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bafb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X_numerical = train_df[all_features].values\n",
    "X_text = tfidf_features\n",
    "y = train_df['target'].values\n",
    "\n",
    "# Combine numerical and text features\n",
    "from scipy.sparse import hstack\n",
    "X_combined = hstack([X_text, X_numerical])\n",
    "\n",
    "print(f\"Combined training features shape: {X_combined.shape}\")\n",
    "\n",
    "# Stratified K-Fold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "cv_scores = []\n",
    "predictions = np.zeros(len(test_df))\n",
    "feature_importance_list = []\n",
    "\n",
    "print(f\"Starting {n_splits}-fold stratified cross-validation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34247623",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 1\n",
    "for train_idx, val_idx in skf.split(X_combined, y):\n",
    "    print(f\"\\nFold {fold}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_combined[train_idx], X_combined[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    val_auc = roc_auc_score(y_val, val_pred)\n",
    "    cv_scores.append(val_auc)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred = model.predict_proba(hstack([tfidf_test_features, test_df[all_features].values]))[:, 1]\n",
    "    predictions += test_pred / n_splits\n",
    "    \n",
    "    # Store feature importance\n",
    "    feature_importance_list.append(model.feature_importances_)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\n=== CROSS-VALIDATION RESULTS ===\")\n",
    "print(f\"Mean AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\n",
    "print(f\"Individual fold scores: {[f'{score:.4f}' for score in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df4585",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04824781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average feature importance\n",
    "avg_importance = np.mean(feature_importance_list, axis=0)\n",
    "\n",
    "# Create feature names\n",
    "tfidf_feature_names = [f'tfidf_{i}' for i in range(tfidf_features.shape[1])]\n",
    "feature_names = tfidf_feature_names + all_features\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': avg_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== TOP 20 MOST IMPORTANT FEATURES ===\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Check importance of leakage features\n",
    "leakage_importance = importance_df[importance_df['feature'].isin(leakage_features)]\n",
    "print(f\"\\n=== LEAKAGE FEATURE IMPORTANCE ===\")\n",
    "print(leakage_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9810d",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'request_id': test_df['request_id'],\n",
    "    'requester_received_pizza': predictions\n",
    "})\n",
    "\n",
    "print(\"=== SUBMISSION PREVIEW ===\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(submission['requester_received_pizza'].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = \"/home/submission/submission_001_baseline.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
