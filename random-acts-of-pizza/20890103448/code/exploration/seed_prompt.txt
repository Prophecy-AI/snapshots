## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution (24.8% positive, 75.2% negative), correlations
- **Critical finding**: User flair ('shroom' and 'PIF') has 100% success rate - this is a leakage feature that perfectly predicts positive class
- Text features: request_title (avg 72 chars), request_text (avg 403 chars)
- Numerical features: user activity metrics, vote counts, timestamps

## Models
For text classification with tabular meta-features, winning Kaggle solutions typically use:

**Primary Approaches:**
1. **Multimodal ensembles**: Combine transformer-based text models (BERT/RoBERTa) with gradient boosting on tabular features
   - Two-stage pipeline: Encode text → concatenate with engineered features → feed to LightGBM/CatBoost
   - End-to-end: Prepend tabular data to text and use single transformer
   
2. **Social media specialized models**: 
   - BERTweet or SocBERT (pre-trained on Twitter/Reddit data) outperform generic BERT
   - Character-level tokenization handles misspellings and emojis better than word-level

3. **Gradient boosting for tabular features**:
   - LightGBM, CatBoost, XGBoost work well for numerical/meta features
   - Target encoding for high-cardinality categorical features

**Model Selection Guidelines:**
- For text-heavy problems: Transformers + tabular ensemble
- For balanced performance: Use 3-5 diverse models (different architectures)
- AutoGluon TabularPredictor can automate multimodal training

## Preprocessing
**Text preprocessing:**
- Reddit-specific tokenization (preserve mentions, hashtags, emojis)
- Character-level representations handle OOV words better than word-level
- Edit-aware text cleaning (remove success indicators like "EDIT: Thanks")
- TF-IDF or fastText as baseline features

**Tabular preprocessing:**
- Feature scaling important for neural networks
- One-hot encoding for low-cardinality categoricals
- Target encoding for high-cardinality features (be careful with leakage)
- Handle timestamp features (extract day-of-week, hour, etc.)

**Class imbalance handling (24.8%/75.2% split):**
- Class weights in loss function (inverse frequency)
- Random oversampling of minority class
- Stratified K-fold validation (k=5) to preserve class ratios
- Monitor AUC/LogLoss instead of accuracy

## Feature Engineering
**Text features:**
- Length features (title_length, text_length)
- Sentiment analysis scores
- LIWC-style psycholinguistic features
- N-gram TF-IDF (unigrams, bigrams)
- Named entity recognition features

**Meta features:**
- User activity ratios (comments/posts per day)
- Vote ratios (upvotes/downvotes)
- Subreddit diversity metrics
- Temporal features from timestamps
- Interaction features between text length and user activity

**Key insight from EDA:**
User flair is a perfect predictor - incorporate this as a high-priority feature but be aware it might be removed in test set

## Validation Strategy
- Stratified K-Fold (k=5) most common for moderate imbalance
- Time-based splits if temporal patterns exist
- Use AUC-ROC as primary metric (competition evaluation)
- Early stopping on validation AUC to prevent overfitting

## Ensembling
**Best practices:**
- Weighted average based on validation performance
- Stacking with logistic regression or LightGBM meta-learner
- Blend transformer predictions with gradient boosting predictions
- Diverse architectures: BERT + RoBERTa + DeBERTa + LightGBM + CatBoost

**Ensemble size:**
- 3-5 models optimal for most competitions
- More models = diminishing returns after 5-7
- Focus on diversity rather than quantity

## Hyperparameter Optimization
- Bayesian optimization (Optuna) for gradient boosting models
- Learning rate scheduling for transformers
- Batch size: 16-32 for transformers (memory constraints)
- Epochs: 3-5 for pretrained transformers (early stopping)
- Learning rates: 2e-5 to 5e-5 for transformer fine-tuning