## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution (24.8% positive rate), correlations
- **CRITICAL FINDING**: User flair feature shows perfect separation - 'shroom' and 'PIF' flairs have 100% success rate, 'None' has 0% success. This is likely data leakage as flair is assigned after receiving pizza.

## Problem Type
Binary classification combining text (Reddit request title and text) with tabular metadata features (user activity, account age, votes, etc.). Evaluation metric: ROC AUC.

## Text Preprocessing
For Reddit/social media text data:
- Combine request_title and request_text into single document for modeling
- Convert to lowercase
- Remove URLs, Reddit-specific artifacts (stickied posts, cross-posts)
- Tokenize using appropriate tokenizer (NLTK, spaCy, or transformer tokenizer)
- Remove stopwords
- Apply lemmatization or stemming
- For transformer models: use model's built-in tokenizer, no need for manual preprocessing

## Feature Engineering
### Text Features:
- TF-IDF vectors (unigrams, bigrams, trigrams)
- Transformer embeddings (BERT, RoBERTa) - use pooled embeddings or CLS token
- Text length features (character count, word count)
- Sentiment analysis scores
- Presence of specific keywords ("please", "thank you", "desperate", etc.)

### Tabular Features:
- Scale numeric features (StandardScaler, MinMaxScaler)
- Handle missing values in requester_user_flair (impute as "None" or create separate category)
- Create interaction features between user activity metrics
- Bin account age into categories (new user, established user, veteran)
- Extract temporal features from timestamps (hour of day, day of week)

## Modeling Approaches
### Primary Models:
1. **Gradient Boosting on Tabular Features**: LightGBM, XGBoost, or CatBoost
   - Handle class imbalance with scale_pos_weight or class_weight
   - Excellent for mixed numeric/categorical data
   - Fast training and good performance

2. **Transformers on Text**: BERT, RoBERTa, DeBERTa
   - Fine-tune on combined title + text
   - Use pooled embeddings for downstream classification
   - Best for capturing nuanced language patterns

3. **Neural Networks (Dual Input)**:
   - Branch 1: Text input → Embedding → LSTM/Transformer → Dense
   - Branch 2: Tabular input → Dense layers
   - Merge branches and add final classification layer

### Handling Class Imbalance (24.8% positive):
- Use stratified k-fold validation (k=5)
- Apply class weights: scale_pos_weight = (negative_samples/positive_samples) ≈ 3.0
- Consider focal loss for neural networks
- Optional: SMOTE oversampling on training folds
- Monitor AUC-ROC rather than accuracy

## Ensembling Strategy
**Stacking Approach (Most Effective):**
1. Train diverse base models:
   - LightGBM on tabular features only
   - Transformer on text features only  
   - CatBoost on all features (handles text as categorical)
   - Neural network on combined features

2. Generate out-of-fold predictions from each model

3. Train meta-learner (typically LightGBM or logistic regression) on stacked predictions
   - Optionally include original features as additional meta-features

4. Final prediction: weighted average of base models and meta-learner

**Alternative: Weighted Averaging**
- Assign weights based on validation performance
- Use higher weights for more diverse models
- Simple but often less effective than stacking

## Validation Strategy
- **Stratified K-Fold**: Essential for maintaining class distribution across folds
- **Group K-Fold**: If multiple requests from same user, group by requester_username to prevent leakage
- **Time-based Split**: If temporal patterns exist, use timestamp for split
- **Early Stopping**: Use validation AUC for early stopping in all models

## Hyperparameter Optimization
- **Tree Models**: Learning rate, max_depth, num_leaves, min_child_samples
- **Neural Networks**: Learning rate, batch size, dropout, number of layers
- **Transformers**: Learning rate, batch size, number of epochs, warmup steps
- Use Bayesian optimization (Optuna) or random search
- Optimize for validation AUC

## Post-Processing
- **Threshold Optimization**: Tune decision threshold on validation set to maximize F1 or custom metric
- **Calibration**: Apply Platt scaling or isotonic regression to improve probability estimates
- **Ensemble Selection**: Use greedy forward selection to choose best subset of models

## Key Insights from EDA
- User flair is perfectly predictive but likely data leakage - handle carefully
- Text features likely contain strong predictive signals (personal stories, politeness, etc.)
- User activity metrics show wide variation - normalization important
- Account age ranges from 0 to 2809 days - new users may have different patterns

## Implementation Notes
- Start with simple models (Logistic Regression + TF-IDF) as baseline
- Build separate text and tabular pipelines before combining
- Use cross-validation for all hyperparameter tuning
- Save OOF predictions for ensembling
- Monitor for overfitting on text features (use dropout, regularization)