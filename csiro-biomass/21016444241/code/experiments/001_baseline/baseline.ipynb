{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4729a446",
   "metadata": {},
   "source": [
    "# Baseline: DINOv2 Embeddings + LightGBM\n",
    "\n",
    "This baseline extracts DINOv2 embeddings from images and combines with tabular features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78721d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115df20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = '/home/data'\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')\n",
    "print(f'\\nTrain columns: {train_df.columns.tolist()}')\n",
    "print(f'\\nTarget names: {train_df[\"target_name\"].unique()}')\n",
    "\n",
    "# Get unique images\n",
    "train_images = train_df['image_path'].unique()\n",
    "test_images = test_df['image_path'].unique()\n",
    "print(f'\\nUnique train images: {len(train_images)}')\n",
    "print(f'Unique test images: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot train data to have one row per image with all targets\n",
    "train_pivot = train_df.pivot_table(\n",
    "    index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "    columns='target_name',\n",
    "    values='target'\n",
    ").reset_index()\n",
    "\n",
    "print(f'Pivoted train shape: {train_pivot.shape}')\n",
    "print(train_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d02da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2 model\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "model_name = 'facebook/dinov2-base'\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "print(f'Model loaded: {model_name}')\n",
    "print(f'Hidden size: {model.config.hidden_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d84c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for all images\n",
    "def extract_embeddings(image_paths, data_dir, batch_size=16):\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                img = Image.open(f'{data_dir}/{path}').convert('RGB')\n",
    "                images.append(img)\n",
    "            \n",
    "            inputs = processor(images=images, return_tensors='pt').to('cuda')\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Use CLS token embedding\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Extract train embeddings\n",
    "print('Extracting train embeddings...')\n",
    "train_embeddings = extract_embeddings(train_pivot['image_path'].values, DATA_DIR)\n",
    "print(f'Train embeddings shape: {train_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test embeddings\n",
    "print('Extracting test embeddings...')\n",
    "test_images_unique = test_df['image_path'].unique()\n",
    "test_embeddings = extract_embeddings(test_images_unique, DATA_DIR)\n",
    "print(f'Test embeddings shape: {test_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d51127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature dataframe with embeddings\n",
    "emb_cols = [f'emb_{i}' for i in range(train_embeddings.shape[1])]\n",
    "train_emb_df = pd.DataFrame(train_embeddings, columns=emb_cols)\n",
    "train_emb_df['image_path'] = train_pivot['image_path'].values\n",
    "\n",
    "test_emb_df = pd.DataFrame(test_embeddings, columns=emb_cols)\n",
    "test_emb_df['image_path'] = test_images_unique\n",
    "\n",
    "print(f'Train embeddings df shape: {train_emb_df.shape}')\n",
    "print(f'Test embeddings df shape: {test_emb_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical features\n",
    "le_state = LabelEncoder()\n",
    "le_species = LabelEncoder()\n",
    "\n",
    "# Fit on all data\n",
    "all_states = pd.concat([train_pivot['State'], pd.Series(['Unknown'])])\n",
    "all_species = pd.concat([train_pivot['Species'], pd.Series(['Unknown'])])\n",
    "\n",
    "le_state.fit(all_states)\n",
    "le_species.fit(all_species)\n",
    "\n",
    "train_pivot['State_enc'] = le_state.transform(train_pivot['State'])\n",
    "train_pivot['Species_enc'] = le_species.transform(train_pivot['Species'])\n",
    "\n",
    "print(f'States: {le_state.classes_}')\n",
    "print(f'Species count: {len(le_species.classes_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc176729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge embeddings with tabular features\n",
    "train_full = train_pivot.merge(train_emb_df, on='image_path')\n",
    "print(f'Train full shape: {train_full.shape}')\n",
    "\n",
    "# Define target columns and weights\n",
    "target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "target_weights = {'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1, 'GDM_g': 0.2, 'Dry_Total_g': 0.5}\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'State_enc', 'Species_enc'] + emb_cols\n",
    "print(f'Number of features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weighted R2 metric\n",
    "def weighted_r2(y_true_dict, y_pred_dict, weights):\n",
    "    \"\"\"Calculate globally weighted R2 across all targets.\"\"\"\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for target in y_true_dict.keys():\n",
    "        all_y_true.extend(y_true_dict[target])\n",
    "        all_y_pred.extend(y_pred_dict[target])\n",
    "        all_weights.extend([weights[target]] * len(y_true_dict[target]))\n",
    "    \n",
    "    all_y_true = np.array(all_y_true)\n",
    "    all_y_pred = np.array(all_y_pred)\n",
    "    all_weights = np.array(all_weights)\n",
    "    \n",
    "    # Weighted mean\n",
    "    y_mean = np.sum(all_weights * all_y_true) / np.sum(all_weights)\n",
    "    \n",
    "    # SS_res and SS_tot\n",
    "    ss_res = np.sum(all_weights * (all_y_true - all_y_pred) ** 2)\n",
    "    ss_tot = np.sum(all_weights * (all_y_true - y_mean) ** 2)\n",
    "    \n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross Validation with LightGBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Store OOF predictions\n",
    "oof_preds = {target: np.zeros(len(train_full)) for target in target_cols}\n",
    "fold_scores = []\n",
    "\n",
    "X = train_full[feature_cols].values\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f'\\n=== Fold {fold + 1} ===')\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    \n",
    "    fold_y_true = {}\n",
    "    fold_y_pred = {}\n",
    "    \n",
    "    for target in target_cols:\n",
    "        y = train_full[target].values\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # LightGBM parameters\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model_lgb = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        preds = model_lgb.predict(X_val)\n",
    "        preds = np.clip(preds, 0, None)  # Biomass can't be negative\n",
    "        \n",
    "        oof_preds[target][val_idx] = preds\n",
    "        fold_y_true[target] = y_val\n",
    "        fold_y_pred[target] = preds\n",
    "    \n",
    "    # Calculate fold weighted R2\n",
    "    fold_r2 = weighted_r2(fold_y_true, fold_y_pred, target_weights)\n",
    "    fold_scores.append(fold_r2)\n",
    "    print(f'Fold {fold + 1} Weighted R2: {fold_r2:.4f}')\n",
    "\n",
    "print(f'\\n=== Overall CV Results ===')\n",
    "print(f'Mean Weighted R2: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee662ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall OOF weighted R2\n",
    "oof_y_true = {target: train_full[target].values for target in target_cols}\n",
    "overall_r2 = weighted_r2(oof_y_true, oof_preds, target_weights)\n",
    "print(f'Overall OOF Weighted R2: {overall_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full data and make predictions\n",
    "final_models = {}\n",
    "X_full = train_full[feature_cols].values\n",
    "\n",
    "for target in target_cols:\n",
    "    print(f'Training final model for {target}...')\n",
    "    y_full = train_full[target].values\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_full, label=y_full)\n",
    "    \n",
    "    model_lgb = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500\n",
    "    )\n",
    "    \n",
    "    final_models[target] = model_lgb\n",
    "\n",
    "print('All final models trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "# For test, we don't have tabular features, so we'll use defaults\n",
    "test_features = test_emb_df.copy()\n",
    "\n",
    "# Add default tabular features (use training means/modes)\n",
    "test_features['Pre_GSHH_NDVI'] = train_pivot['Pre_GSHH_NDVI'].mean()\n",
    "test_features['Height_Ave_cm'] = train_pivot['Height_Ave_cm'].mean()\n",
    "test_features['State_enc'] = train_pivot['State_enc'].mode()[0]\n",
    "test_features['Species_enc'] = train_pivot['Species_enc'].mode()[0]\n",
    "\n",
    "X_test = test_features[feature_cols].values\n",
    "print(f'Test features shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaad3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for test set\n",
    "test_preds = {}\n",
    "for target in target_cols:\n",
    "    preds = final_models[target].predict(X_test)\n",
    "    preds = np.clip(preds, 0, None)  # Biomass can't be negative\n",
    "    test_preds[target] = preds\n",
    "    print(f'{target}: mean={preds.mean():.2f}, min={preds.min():.2f}, max={preds.max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_rows = []\n",
    "\n",
    "for i, img_path in enumerate(test_images_unique):\n",
    "    img_id = img_path.split('/')[-1].replace('.jpg', '')\n",
    "    \n",
    "    for target in target_cols:\n",
    "        sample_id = f'{img_id}__{target}'\n",
    "        pred_value = test_preds[target][i]\n",
    "        submission_rows.append({'sample_id': sample_id, 'target': pred_value})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "print(f'Submission shape: {submission_df.shape}')\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5da940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "print('Submission saved to /home/submission/submission.csv')\n",
    "\n",
    "# Verify format matches sample\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "print(f'\\nSample submission columns: {sample_sub.columns.tolist()}')\n",
    "print(f'Our submission columns: {submission_df.columns.tolist()}')\n",
    "print(f'\\nSample submission shape: {sample_sub.shape}')\n",
    "print(f'Our submission shape: {submission_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('='*50)\n",
    "print('BASELINE RESULTS SUMMARY')\n",
    "print('='*50)\n",
    "print(f'Model: DINOv2-base embeddings + LightGBM')\n",
    "print(f'Features: {len(feature_cols)} (768 DINOv2 + 4 tabular)')\n",
    "print(f'CV Folds: {N_FOLDS}')\n",
    "print(f'Mean CV Weighted R2: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})')\n",
    "print(f'Overall OOF Weighted R2: {overall_r2:.4f}')\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
