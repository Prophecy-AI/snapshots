{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4729a446",
   "metadata": {},
   "source": [
    "# Baseline: DINOv2 Embeddings + LightGBM\n",
    "\n",
    "This baseline extracts DINOv2 embeddings from images and combines with tabular features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78721d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T01:52:06.786559Z",
     "iopub.status.busy": "2026-01-15T01:52:06.786101Z",
     "iopub.status.idle": "2026-01-15T01:52:08.190796Z",
     "shell.execute_reply": "2026-01-15T01:52:08.190320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115df20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T01:52:08.192308Z",
     "iopub.status.busy": "2026-01-15T01:52:08.192108Z",
     "iopub.status.idle": "2026-01-15T01:52:08.202889Z",
     "shell.execute_reply": "2026-01-15T01:52:08.202469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1785, 9)\n",
      "Test shape: (5, 3)\n",
      "\n",
      "Train columns: ['sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'target_name', 'target']\n",
      "\n",
      "Target names: ['Dry_Clover_g' 'Dry_Dead_g' 'Dry_Green_g' 'Dry_Total_g' 'GDM_g']\n",
      "\n",
      "Unique train images: 357\n",
      "Unique test images: 1\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = '/home/data'\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')\n",
    "print(f'\\nTrain columns: {train_df.columns.tolist()}')\n",
    "print(f'\\nTarget names: {train_df[\"target_name\"].unique()}')\n",
    "\n",
    "# Get unique images\n",
    "train_images = train_df['image_path'].unique()\n",
    "test_images = test_df['image_path'].unique()\n",
    "print(f'\\nUnique train images: {len(train_images)}')\n",
    "print(f'Unique test images: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651ee00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T01:52:26.132777Z",
     "iopub.status.busy": "2026-01-15T01:52:26.132201Z",
     "iopub.status.idle": "2026-01-15T01:52:26.146694Z",
     "shell.execute_reply": "2026-01-15T01:52:26.146249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted train shape: (357, 11)\n",
      "target_name              image_path Sampling_Date State            Species  \\\n",
      "0            train/ID1011485656.jpg      2015/9/4   Tas    Ryegrass_Clover   \n",
      "1            train/ID1012260530.jpg      2015/4/1   NSW            Lucerne   \n",
      "2            train/ID1025234388.jpg      2015/9/1    WA  SubcloverDalkeith   \n",
      "3            train/ID1028611175.jpg     2015/5/18   Tas           Ryegrass   \n",
      "4            train/ID1035947949.jpg     2015/9/11   Tas           Ryegrass   \n",
      "\n",
      "target_name  Pre_GSHH_NDVI  Height_Ave_cm  Dry_Clover_g  Dry_Dead_g  \\\n",
      "0                     0.62         4.6667        0.0000     31.9984   \n",
      "1                     0.55        16.0000        0.0000      0.0000   \n",
      "2                     0.38         1.0000        6.0500      0.0000   \n",
      "3                     0.66         5.0000        0.0000     30.9703   \n",
      "4                     0.54         3.5000        0.4343     23.2239   \n",
      "\n",
      "target_name  Dry_Green_g  Dry_Total_g    GDM_g  \n",
      "0                16.2751      48.2735  16.2750  \n",
      "1                 7.6000       7.6000   7.6000  \n",
      "2                 0.0000       6.0500   6.0500  \n",
      "3                24.2376      55.2079  24.2376  \n",
      "4                10.5261      34.1844  10.9605  \n"
     ]
    }
   ],
   "source": [
    "# Pivot train data to have one row per image with all targets\n",
    "train_pivot = train_df.pivot_table(\n",
    "    index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "    columns='target_name',\n",
    "    values='target'\n",
    ").reset_index()\n",
    "\n",
    "print(f'Pivoted train shape: {train_pivot.shape}')\n",
    "print(train_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d02da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T01:52:26.147966Z",
     "iopub.status.busy": "2026-01-15T01:52:26.147850Z",
     "iopub.status.idle": "2026-01-15T01:52:30.031245Z",
     "shell.execute_reply": "2026-01-15T01:52:30.030766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 01:52:27.152606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-15 01:52:27.168411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-15 01:52:27.172879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625ce836e7064cc4bd2667fb217059d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae95259575ee4c0eb5015ad8f6381f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c769e7ea179a4b2db1aaaf827f94f3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: facebook/dinov2-base\n",
      "Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv2 model\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "model_name = 'facebook/dinov2-base'\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "print(f'Model loaded: {model_name}')\n",
    "print(f'Hidden size: {model.config.hidden_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d84c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T01:52:30.032641Z",
     "iopub.status.busy": "2026-01-15T01:52:30.032359Z",
     "iopub.status.idle": "2026-01-15T01:52:51.541011Z",
     "shell.execute_reply": "2026-01-15T01:52:51.540552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/23 [00:01<00:25,  1.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▊         | 2/23 [00:02<00:22,  1.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 3/23 [00:03<00:20,  1.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/23 [00:04<00:18,  1.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 5/23 [00:05<00:17,  1.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 6/23 [00:05<00:16,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 7/23 [00:06<00:15,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▍      | 8/23 [00:07<00:14,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 9/23 [00:08<00:13,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 10/23 [00:09<00:12,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 11/23 [00:10<00:11,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 12/23 [00:11<00:10,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 13/23 [00:12<00:09,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 14/23 [00:13<00:08,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 15/23 [00:14<00:07,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████▉   | 16/23 [00:15<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 17/23 [00:16<00:05,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 18/23 [00:17<00:04,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 19/23 [00:18<00:03,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 20/23 [00:19<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████▏| 21/23 [00:20<00:01,  1.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 22/23 [00:21<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 23/23 [00:21<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 23/23 [00:21<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (357, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings for all images\n",
    "def extract_embeddings(image_paths, data_dir, batch_size=16):\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                img = Image.open(f'{data_dir}/{path}').convert('RGB')\n",
    "                images.append(img)\n",
    "            \n",
    "            inputs = processor(images=images, return_tensors='pt').to('cuda')\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Use CLS token embedding\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Extract train embeddings\n",
    "print('Extracting train embeddings...')\n",
    "train_embeddings = extract_embeddings(train_pivot['image_path'].values, DATA_DIR)\n",
    "print(f'Train embeddings shape: {train_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test embeddings\n",
    "print('Extracting test embeddings...')\n",
    "test_images_unique = test_df['image_path'].unique()\n",
    "test_embeddings = extract_embeddings(test_images_unique, DATA_DIR)\n",
    "print(f'Test embeddings shape: {test_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d51127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature dataframe with embeddings\n",
    "emb_cols = [f'emb_{i}' for i in range(train_embeddings.shape[1])]\n",
    "train_emb_df = pd.DataFrame(train_embeddings, columns=emb_cols)\n",
    "train_emb_df['image_path'] = train_pivot['image_path'].values\n",
    "\n",
    "test_emb_df = pd.DataFrame(test_embeddings, columns=emb_cols)\n",
    "test_emb_df['image_path'] = test_images_unique\n",
    "\n",
    "print(f'Train embeddings df shape: {train_emb_df.shape}')\n",
    "print(f'Test embeddings df shape: {test_emb_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical features\n",
    "le_state = LabelEncoder()\n",
    "le_species = LabelEncoder()\n",
    "\n",
    "# Fit on all data\n",
    "all_states = pd.concat([train_pivot['State'], pd.Series(['Unknown'])])\n",
    "all_species = pd.concat([train_pivot['Species'], pd.Series(['Unknown'])])\n",
    "\n",
    "le_state.fit(all_states)\n",
    "le_species.fit(all_species)\n",
    "\n",
    "train_pivot['State_enc'] = le_state.transform(train_pivot['State'])\n",
    "train_pivot['Species_enc'] = le_species.transform(train_pivot['Species'])\n",
    "\n",
    "print(f'States: {le_state.classes_}')\n",
    "print(f'Species count: {len(le_species.classes_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc176729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge embeddings with tabular features\n",
    "train_full = train_pivot.merge(train_emb_df, on='image_path')\n",
    "print(f'Train full shape: {train_full.shape}')\n",
    "\n",
    "# Define target columns and weights\n",
    "target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "target_weights = {'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1, 'GDM_g': 0.2, 'Dry_Total_g': 0.5}\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'State_enc', 'Species_enc'] + emb_cols\n",
    "print(f'Number of features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weighted R2 metric\n",
    "def weighted_r2(y_true_dict, y_pred_dict, weights):\n",
    "    \"\"\"Calculate globally weighted R2 across all targets.\"\"\"\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for target in y_true_dict.keys():\n",
    "        all_y_true.extend(y_true_dict[target])\n",
    "        all_y_pred.extend(y_pred_dict[target])\n",
    "        all_weights.extend([weights[target]] * len(y_true_dict[target]))\n",
    "    \n",
    "    all_y_true = np.array(all_y_true)\n",
    "    all_y_pred = np.array(all_y_pred)\n",
    "    all_weights = np.array(all_weights)\n",
    "    \n",
    "    # Weighted mean\n",
    "    y_mean = np.sum(all_weights * all_y_true) / np.sum(all_weights)\n",
    "    \n",
    "    # SS_res and SS_tot\n",
    "    ss_res = np.sum(all_weights * (all_y_true - all_y_pred) ** 2)\n",
    "    ss_tot = np.sum(all_weights * (all_y_true - y_mean) ** 2)\n",
    "    \n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross Validation with LightGBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Store OOF predictions\n",
    "oof_preds = {target: np.zeros(len(train_full)) for target in target_cols}\n",
    "fold_scores = []\n",
    "\n",
    "X = train_full[feature_cols].values\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f'\\n=== Fold {fold + 1} ===')\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    \n",
    "    fold_y_true = {}\n",
    "    fold_y_pred = {}\n",
    "    \n",
    "    for target in target_cols:\n",
    "        y = train_full[target].values\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # LightGBM parameters\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model_lgb = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        preds = model_lgb.predict(X_val)\n",
    "        preds = np.clip(preds, 0, None)  # Biomass can't be negative\n",
    "        \n",
    "        oof_preds[target][val_idx] = preds\n",
    "        fold_y_true[target] = y_val\n",
    "        fold_y_pred[target] = preds\n",
    "    \n",
    "    # Calculate fold weighted R2\n",
    "    fold_r2 = weighted_r2(fold_y_true, fold_y_pred, target_weights)\n",
    "    fold_scores.append(fold_r2)\n",
    "    print(f'Fold {fold + 1} Weighted R2: {fold_r2:.4f}')\n",
    "\n",
    "print(f'\\n=== Overall CV Results ===')\n",
    "print(f'Mean Weighted R2: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee662ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall OOF weighted R2\n",
    "oof_y_true = {target: train_full[target].values for target in target_cols}\n",
    "overall_r2 = weighted_r2(oof_y_true, oof_preds, target_weights)\n",
    "print(f'Overall OOF Weighted R2: {overall_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full data and make predictions\n",
    "final_models = {}\n",
    "X_full = train_full[feature_cols].values\n",
    "\n",
    "for target in target_cols:\n",
    "    print(f'Training final model for {target}...')\n",
    "    y_full = train_full[target].values\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_full, label=y_full)\n",
    "    \n",
    "    model_lgb = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500\n",
    "    )\n",
    "    \n",
    "    final_models[target] = model_lgb\n",
    "\n",
    "print('All final models trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "# For test, we don't have tabular features, so we'll use defaults\n",
    "test_features = test_emb_df.copy()\n",
    "\n",
    "# Add default tabular features (use training means/modes)\n",
    "test_features['Pre_GSHH_NDVI'] = train_pivot['Pre_GSHH_NDVI'].mean()\n",
    "test_features['Height_Ave_cm'] = train_pivot['Height_Ave_cm'].mean()\n",
    "test_features['State_enc'] = train_pivot['State_enc'].mode()[0]\n",
    "test_features['Species_enc'] = train_pivot['Species_enc'].mode()[0]\n",
    "\n",
    "X_test = test_features[feature_cols].values\n",
    "print(f'Test features shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaad3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for test set\n",
    "test_preds = {}\n",
    "for target in target_cols:\n",
    "    preds = final_models[target].predict(X_test)\n",
    "    preds = np.clip(preds, 0, None)  # Biomass can't be negative\n",
    "    test_preds[target] = preds\n",
    "    print(f'{target}: mean={preds.mean():.2f}, min={preds.min():.2f}, max={preds.max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_rows = []\n",
    "\n",
    "for i, img_path in enumerate(test_images_unique):\n",
    "    img_id = img_path.split('/')[-1].replace('.jpg', '')\n",
    "    \n",
    "    for target in target_cols:\n",
    "        sample_id = f'{img_id}__{target}'\n",
    "        pred_value = test_preds[target][i]\n",
    "        submission_rows.append({'sample_id': sample_id, 'target': pred_value})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "print(f'Submission shape: {submission_df.shape}')\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5da940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "print('Submission saved to /home/submission/submission.csv')\n",
    "\n",
    "# Verify format matches sample\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "print(f'\\nSample submission columns: {sample_sub.columns.tolist()}')\n",
    "print(f'Our submission columns: {submission_df.columns.tolist()}')\n",
    "print(f'\\nSample submission shape: {sample_sub.shape}')\n",
    "print(f'Our submission shape: {submission_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('='*50)\n",
    "print('BASELINE RESULTS SUMMARY')\n",
    "print('='*50)\n",
    "print(f'Model: DINOv2-base embeddings + LightGBM')\n",
    "print(f'Features: {len(feature_cols)} (768 DINOv2 + 4 tabular)')\n",
    "print(f'CV Folds: {N_FOLDS}')\n",
    "print(f'Mean CV Weighted R2: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})')\n",
    "print(f'Overall OOF Weighted R2: {overall_r2:.4f}')\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
