{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a76a5c",
   "metadata": {},
   "source": [
    "# Experiment 003: DINOv2-giant + Image Patching + 4-Model Ensemble\n",
    "\n",
    "Following evolver strategy Priority 1:\n",
    "1. Image patching (520px patches with overlap) to preserve spatial detail\n",
    "2. DINOv2-giant (1536 dims) for more expressive features\n",
    "3. 4-model ensemble (LightGBM, CatBoost, XGBoost, HistGradientBoosting)\n",
    "4. Post-processing for biomass constraints\n",
    "\n",
    "Target: CV â‰¥ 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22ced09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T02:14:18.478271Z",
     "iopub.status.busy": "2026-01-15T02:14:18.477664Z",
     "iopub.status.idle": "2026-01-15T02:14:19.845342Z",
     "shell.execute_reply": "2026-01-15T02:14:19.844891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261872aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T02:14:19.846821Z",
     "iopub.status.busy": "2026-01-15T02:14:19.846636Z",
     "iopub.status.idle": "2026-01-15T02:14:19.868979Z",
     "shell.execute_reply": "2026-01-15T02:14:19.868586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted train shape: (357, 11)\n",
      "Test shape: (5, 3)\n",
      "Sample image size: (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = '/home/data'\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# Pivot train data\n",
    "train_pivot = train_df.pivot_table(\n",
    "    index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "    columns='target_name',\n",
    "    values='target'\n",
    ").reset_index()\n",
    "\n",
    "print(f'Pivoted train shape: {train_pivot.shape}')\n",
    "print(f'Test shape: {test_df.shape}')\n",
    "\n",
    "# Check image dimensions\n",
    "sample_img = Image.open(f'{DATA_DIR}/{train_pivot[\"image_path\"].iloc[0]}')\n",
    "print(f'Sample image size: {sample_img.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7061426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T02:14:19.869969Z",
     "iopub.status.busy": "2026-01-15T02:14:19.869853Z",
     "iopub.status.idle": "2026-01-15T02:14:27.500331Z",
     "shell.execute_reply": "2026-01-15T02:14:27.499856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 02:14:20.865117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-15 02:14:20.880868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-15 02:14:20.885324: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a1688e4fc64de6a9ceea8ae01cd75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a5ff8984a9422da0d86ad7c7550c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def0f9d8d9664a319c134791c7b400fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: facebook/dinov2-giant\n",
      "Hidden size: 1536\n",
      "Image size expected: {'shortest_edge': 256}\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv2-giant model (1536 dims)\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "model_name = 'facebook/dinov2-giant'\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "print(f'Model loaded: {model_name}')\n",
    "print(f'Hidden size: {model.config.hidden_size}')\n",
    "print(f'Image size expected: {processor.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59909780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T02:14:43.462483Z",
     "iopub.status.busy": "2026-01-15T02:14:43.461727Z",
     "iopub.status.idle": "2026-01-15T02:14:43.526349Z",
     "shell.execute_reply": "2026-01-15T02:14:43.525903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches per image: 8\n",
      "Patch size: (518, 518)\n"
     ]
    }
   ],
   "source": [
    "# Image patching function - split image into 520px patches with overlap\n",
    "def split_image_into_patches(image, patch_size=518, overlap=16):\n",
    "    \"\"\"Split image into overlapping patches.\n",
    "    \n",
    "    For 2000x1000 image with 518px patches and 16px overlap:\n",
    "    - stride = 518 - 16 = 502\n",
    "    - horizontal: ceil((2000-518)/502) + 1 = 4 patches\n",
    "    - vertical: ceil((1000-518)/502) + 1 = 2 patches\n",
    "    - Total: 8 patches per image\n",
    "    \"\"\"\n",
    "    img_array = np.array(image)\n",
    "    h, w, c = img_array.shape\n",
    "    stride = patch_size - overlap\n",
    "    \n",
    "    patches = []\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            # Extract patch\n",
    "            y_end = min(y + patch_size, h)\n",
    "            x_end = min(x + patch_size, w)\n",
    "            patch = img_array[y:y_end, x:x_end]\n",
    "            \n",
    "            # Pad if needed\n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                pad_h = patch_size - patch.shape[0]\n",
    "                pad_w = patch_size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            \n",
    "            patches.append(Image.fromarray(patch))\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Test patching\n",
    "test_patches = split_image_into_patches(sample_img)\n",
    "print(f'Number of patches per image: {len(test_patches)}')\n",
    "print(f'Patch size: {test_patches[0].size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patch-based embeddings with DINOv2-giant\n",
    "def extract_patch_embeddings(image_paths, data_dir, batch_size=4):\n",
    "    \"\"\"Extract embeddings by averaging patch embeddings.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_path in tqdm(image_paths):\n",
    "            # Load image and split into patches\n",
    "            img = Image.open(f'{data_dir}/{img_path}').convert('RGB')\n",
    "            patches = split_image_into_patches(img)\n",
    "            \n",
    "            # Process patches in batches\n",
    "            patch_embeddings = []\n",
    "            for i in range(0, len(patches), batch_size):\n",
    "                batch_patches = patches[i:i+batch_size]\n",
    "                inputs = processor(images=batch_patches, return_tensors='pt').to('cuda')\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                # Use mean of patch tokens (excluding CLS)\n",
    "                patch_tokens = outputs.last_hidden_state[:, 1:, :]  # (batch, num_patches, hidden)\n",
    "                patch_mean = patch_tokens.mean(dim=1)  # (batch, hidden)\n",
    "                patch_embeddings.append(patch_mean.cpu().numpy())\n",
    "            \n",
    "            # Average all patch embeddings for this image\n",
    "            patch_embeddings = np.vstack(patch_embeddings)\n",
    "            image_embedding = patch_embeddings.mean(axis=0)\n",
    "            all_embeddings.append(image_embedding)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Extract train embeddings\n",
    "print('Extracting train embeddings with DINOv2-giant + patching...')\n",
    "train_embeddings = extract_patch_embeddings(train_pivot['image_path'].values, DATA_DIR)\n",
    "print(f'Train embeddings shape: {train_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test embeddings\n",
    "print('Extracting test embeddings...')\n",
    "test_images_unique = test_df['image_path'].unique()\n",
    "test_embeddings = extract_patch_embeddings(test_images_unique, DATA_DIR)\n",
    "print(f'Test embeddings shape: {test_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bcc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import gc\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print('GPU memory cleared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f141e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature dataframe\n",
    "emb_cols = [f'emb_{i}' for i in range(train_embeddings.shape[1])]\n",
    "train_emb_df = pd.DataFrame(train_embeddings, columns=emb_cols)\n",
    "train_emb_df['image_path'] = train_pivot['image_path'].values\n",
    "\n",
    "test_emb_df = pd.DataFrame(test_embeddings, columns=emb_cols)\n",
    "test_emb_df['image_path'] = test_images_unique\n",
    "\n",
    "print(f'Train embeddings df shape: {train_emb_df.shape}')\n",
    "print(f'Test embeddings df shape: {test_emb_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tabular features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_state = LabelEncoder()\n",
    "le_species = LabelEncoder()\n",
    "\n",
    "all_states = pd.concat([train_pivot['State'], pd.Series(['Unknown'])])\n",
    "all_species = pd.concat([train_pivot['Species'], pd.Series(['Unknown'])])\n",
    "\n",
    "le_state.fit(all_states)\n",
    "le_species.fit(all_species)\n",
    "\n",
    "train_pivot['State_enc'] = le_state.transform(train_pivot['State'])\n",
    "train_pivot['Species_enc'] = le_species.transform(train_pivot['Species'])\n",
    "\n",
    "# Merge embeddings with tabular features\n",
    "train_full = train_pivot.merge(train_emb_df, on='image_path')\n",
    "print(f'Train full shape: {train_full.shape}')\n",
    "\n",
    "# Define target columns and weights\n",
    "target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "target_weights = {'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1, 'GDM_g': 0.2, 'Dry_Total_g': 0.5}\n",
    "\n",
    "# Define feature columns (1536 DINOv2-giant + 4 tabular)\n",
    "feature_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'State_enc', 'Species_enc'] + emb_cols\n",
    "print(f'Number of features: {len(feature_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weighted R2 metric\n",
    "def weighted_r2(y_true_dict, y_pred_dict, weights):\n",
    "    all_y_true, all_y_pred, all_weights = [], [], []\n",
    "    \n",
    "    for target in y_true_dict.keys():\n",
    "        all_y_true.extend(y_true_dict[target])\n",
    "        all_y_pred.extend(y_pred_dict[target])\n",
    "        all_weights.extend([weights[target]] * len(y_true_dict[target]))\n",
    "    \n",
    "    all_y_true = np.array(all_y_true)\n",
    "    all_y_pred = np.array(all_y_pred)\n",
    "    all_weights = np.array(all_weights)\n",
    "    \n",
    "    y_mean = np.sum(all_weights * all_y_true) / np.sum(all_weights)\n",
    "    ss_res = np.sum(all_weights * (all_y_true - all_y_pred) ** 2)\n",
    "    ss_tot = np.sum(all_weights * (all_y_true - y_mean) ** 2)\n",
    "    \n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "# Post-processing function\n",
    "def post_process_biomass(preds_dict):\n",
    "    ordered_cols = ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']\n",
    "    Y = np.vstack([preds_dict[col] for col in ordered_cols])\n",
    "    \n",
    "    C = np.array([[1, 1, 0, -1, 0], [0, 0, 1, 1, -1]])\n",
    "    C_T = C.T\n",
    "    inv_CCt = np.linalg.inv(C @ C_T)\n",
    "    P = np.eye(5) - C_T @ inv_CCt @ C\n",
    "    \n",
    "    Y_reconciled = (P @ Y).clip(min=0)\n",
    "    \n",
    "    return {col: Y_reconciled[i] for i, col in enumerate(ordered_cols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-Model Ensemble with 5-Fold CV\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Store OOF predictions\n",
    "oof_preds = {target: np.zeros(len(train_full)) for target in target_cols}\n",
    "oof_preds_pp = {target: np.zeros(len(train_full)) for target in target_cols}\n",
    "fold_scores = []\n",
    "fold_scores_pp = []\n",
    "\n",
    "X = train_full[feature_cols].values\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f'\\n=== Fold {fold + 1} ===')\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    \n",
    "    fold_y_true = {}\n",
    "    fold_y_pred = {}\n",
    "    \n",
    "    for target in target_cols:\n",
    "        y = train_full[target].values\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # 4-Model Ensemble\n",
    "        preds_list = []\n",
    "        \n",
    "        # 1. LightGBM\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, num_leaves=31,\n",
    "            feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5,\n",
    "            verbose=-1, random_state=42\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                      callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "        preds_list.append(lgb_model.predict(X_val))\n",
    "        \n",
    "        # 2. CatBoost\n",
    "        cb_model = CatBoostRegressor(\n",
    "            iterations=500, learning_rate=0.05, depth=6,\n",
    "            verbose=0, random_state=42\n",
    "        )\n",
    "        cb_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "        preds_list.append(cb_model.predict(X_val))\n",
    "        \n",
    "        # 3. XGBoost\n",
    "        xgb_model = XGBRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            verbosity=0, random_state=42\n",
    "        )\n",
    "        xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                      early_stopping_rounds=50, verbose=False)\n",
    "        preds_list.append(xgb_model.predict(X_val))\n",
    "        \n",
    "        # 4. HistGradientBoosting\n",
    "        hgb_model = HistGradientBoostingRegressor(\n",
    "            max_iter=500, learning_rate=0.05, max_depth=6,\n",
    "            random_state=42\n",
    "        )\n",
    "        hgb_model.fit(X_train, y_train)\n",
    "        preds_list.append(hgb_model.predict(X_val))\n",
    "        \n",
    "        # Average predictions\n",
    "        ensemble_preds = np.mean(preds_list, axis=0)\n",
    "        ensemble_preds = np.clip(ensemble_preds, 0, None)\n",
    "        \n",
    "        oof_preds[target][val_idx] = ensemble_preds\n",
    "        fold_y_true[target] = y_val\n",
    "        fold_y_pred[target] = ensemble_preds\n",
    "    \n",
    "    # Calculate fold R2\n",
    "    fold_r2 = weighted_r2(fold_y_true, fold_y_pred, target_weights)\n",
    "    fold_scores.append(fold_r2)\n",
    "    \n",
    "    # Apply post-processing\n",
    "    fold_y_pred_pp = post_process_biomass(fold_y_pred)\n",
    "    fold_r2_pp = weighted_r2(fold_y_true, fold_y_pred_pp, target_weights)\n",
    "    fold_scores_pp.append(fold_r2_pp)\n",
    "    \n",
    "    for target in target_cols:\n",
    "        oof_preds_pp[target][val_idx] = fold_y_pred_pp[target]\n",
    "    \n",
    "    print(f'Fold {fold + 1} Weighted R2: {fold_r2:.4f} -> {fold_r2_pp:.4f} (post-processed)')\n",
    "\n",
    "print(f'\\n=== Overall CV Results ===')\n",
    "print(f'Mean Weighted R2 (raw): {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})')\n",
    "print(f'Mean Weighted R2 (post-processed): {np.mean(fold_scores_pp):.4f} (+/- {np.std(fold_scores_pp):.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25482a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall OOF weighted R2\n",
    "oof_y_true = {target: train_full[target].values for target in target_cols}\n",
    "overall_r2 = weighted_r2(oof_y_true, oof_preds, target_weights)\n",
    "overall_r2_pp = weighted_r2(oof_y_true, oof_preds_pp, target_weights)\n",
    "\n",
    "print(f'Overall OOF Weighted R2 (raw): {overall_r2:.4f}')\n",
    "print(f'Overall OOF Weighted R2 (post-processed): {overall_r2_pp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final ensemble models on full data\n",
    "print('Training final ensemble models on full data...')\n",
    "final_models = {target: [] for target in target_cols}\n",
    "X_full = train_full[feature_cols].values\n",
    "\n",
    "for target in target_cols:\n",
    "    print(f'Training models for {target}...')\n",
    "    y_full = train_full[target].values\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, num_leaves=31,\n",
    "        feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5,\n",
    "        verbose=-1, random_state=42\n",
    "    )\n",
    "    lgb_model.fit(X_full, y_full)\n",
    "    final_models[target].append(lgb_model)\n",
    "    \n",
    "    # CatBoost\n",
    "    cb_model = CatBoostRegressor(\n",
    "        iterations=500, learning_rate=0.05, depth=6,\n",
    "        verbose=0, random_state=42\n",
    "    )\n",
    "    cb_model.fit(X_full, y_full)\n",
    "    final_models[target].append(cb_model)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        verbosity=0, random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_full, y_full)\n",
    "    final_models[target].append(xgb_model)\n",
    "    \n",
    "    # HistGradientBoosting\n",
    "    hgb_model = HistGradientBoostingRegressor(\n",
    "        max_iter=500, learning_rate=0.05, max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "    hgb_model.fit(X_full, y_full)\n",
    "    final_models[target].append(hgb_model)\n",
    "\n",
    "print('All final models trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "test_features = test_emb_df.copy()\n",
    "test_features['Pre_GSHH_NDVI'] = train_pivot['Pre_GSHH_NDVI'].mean()\n",
    "test_features['Height_Ave_cm'] = train_pivot['Height_Ave_cm'].mean()\n",
    "test_features['State_enc'] = train_pivot['State_enc'].mode()[0]\n",
    "test_features['Species_enc'] = train_pivot['Species_enc'].mode()[0]\n",
    "\n",
    "X_test = test_features[feature_cols].values\n",
    "print(f'Test features shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb4ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ensemble predictions for test set\n",
    "test_preds = {}\n",
    "for target in target_cols:\n",
    "    preds_list = [model.predict(X_test) for model in final_models[target]]\n",
    "    ensemble_preds = np.mean(preds_list, axis=0)\n",
    "    ensemble_preds = np.clip(ensemble_preds, 0, None)\n",
    "    test_preds[target] = ensemble_preds\n",
    "    print(f'{target}: mean={ensemble_preds.mean():.2f}')\n",
    "\n",
    "# Apply post-processing\n",
    "test_preds_pp = post_process_biomass(test_preds)\n",
    "print('\\nAfter post-processing:')\n",
    "for target in target_cols:\n",
    "    print(f'{target}: mean={test_preds_pp[target].mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28999a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_rows = []\n",
    "\n",
    "for i, img_path in enumerate(test_images_unique):\n",
    "    img_id = img_path.split('/')[-1].replace('.jpg', '')\n",
    "    \n",
    "    for target in target_cols:\n",
    "        sample_id = f'{img_id}__{target}'\n",
    "        pred_value = test_preds_pp[target][i]\n",
    "        submission_rows.append({'sample_id': sample_id, 'target': pred_value})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "print(f'Submission shape: {submission_df.shape}')\n",
    "print(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "print('Submission saved to /home/submission/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f288b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('='*70)\n",
    "print('EXPERIMENT 003 RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "print(f'Model: DINOv2-giant + Image Patching + 4-Model Ensemble + Post-processing')\n",
    "print(f'Features: {len(feature_cols)} (1536 DINOv2-giant patch + 4 tabular)')\n",
    "print(f'Ensemble: LightGBM, CatBoost, XGBoost, HistGradientBoosting')\n",
    "print(f'CV Folds: {N_FOLDS}')\n",
    "print(f'\\nRaw predictions:')\n",
    "print(f'  Mean CV Weighted R2: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})')\n",
    "print(f'  Overall OOF Weighted R2: {overall_r2:.4f}')\n",
    "print(f'\\nPost-processed predictions:')\n",
    "print(f'  Mean CV Weighted R2: {np.mean(fold_scores_pp):.4f} (+/- {np.std(fold_scores_pp):.4f})')\n",
    "print(f'  Overall OOF Weighted R2: {overall_r2_pp:.4f}')\n",
    "print(f'\\nComparison:')\n",
    "print(f'  Baseline (exp_000): 0.7584')\n",
    "print(f'  DINOv2-large patch (exp_001): 0.7715')\n",
    "print(f'  This experiment: {overall_r2_pp:.4f} ({overall_r2_pp - 0.7715:+.4f} vs exp_001)')\n",
    "print(f'\\nTarget: 0.79, Gap: {0.79 - overall_r2_pp:.4f}')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
