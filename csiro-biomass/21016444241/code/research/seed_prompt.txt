## Current Status
- Best CV score: 0.7584 from exp_000 (DINOv2-base + LightGBM)
- Best LB score: Not yet submitted
- Target: 0.79
- Gap to target: 0.0316 (3.16%)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The validation methodology is sound.
- Evaluator's top priority: "Upgrade to patch-based DINOv2 features with a larger model." I AGREE - this is the highest-leverage change based on kernel research.
- Key concerns raised:
  1. CLS token only (missing spatial info) → Will address with patch-based features
  2. DINOv2-base vs giant → Will upgrade to DINOv2-large (1024 dims) as intermediate step
  3. No biomass constraints → Will implement post-processing
  4. Single model type → Will add ensemble after feature improvements
  5. High CV variance → Analysis shows this is NOT due to state imbalance; inherent data variability

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop1_analysis.ipynb` for analysis
- Key patterns to exploit:
  1. **Biomass constraints are EXACT**: GDM = Dry_Green + Dry_Clover, Dry_Total = GDM + Dry_Dead
     - Post-processing with projection matrix will enforce consistency
  2. **Height is most predictive tabular feature** (r=0.50 with Dry_Total, r=0.58 with GDM)
  3. **357 training images** - small dataset, so regularization and ensembling are important
  4. **15 species types** - some rare (2-5 samples), label encoding loses this info

## Recommended Approaches (Priority Order)

### Priority 1: Upgrade Image Features (HIGHEST IMPACT)
**Experiment: DINOv2-large with patch-based features**
- Use `facebook/dinov2-large` (1024 dims vs 768 in base)
- Extract `last_hidden_state[:,1:,:]` (patch tokens) instead of just CLS
- Average patch embeddings OR use PCA to reduce dimensionality
- Expected improvement: +0.02-0.03 based on kernel evidence

Implementation:
```python
from transformers import AutoImageProcessor, AutoModel
model = AutoModel.from_pretrained('facebook/dinov2-large').cuda().eval()
outputs = model(**inputs)
# Use mean of patch tokens instead of CLS
patch_features = outputs.last_hidden_state[:, 1:, :].mean(dim=1)  # (batch, 1024)
```

### Priority 2: Post-Processing for Biomass Constraints
**Implement projection matrix to enforce physical constraints**
- Constraints: GDM = Dry_Green + Dry_Clover, Dry_Total = GDM + Dry_Dead
- Use the exact method from 0.69 kernel:
```python
def post_process_biomass(df_preds):
    ordered_cols = ["Dry_Green_g", "Dry_Clover_g", "Dry_Dead_g", "GDM_g", "Dry_Total_g"]
    Y = df_preds[ordered_cols].values.T
    C = np.array([[1, 1, 0, -1, 0], [0, 0, 1, 1, -1]])  # Constraint matrix
    C_T = C.T
    inv_CCt = np.linalg.inv(C @ C_T)
    P = np.eye(5) - C_T @ inv_CCt @ C  # Projection matrix
    Y_reconciled = (P @ Y).T.clip(min=0)
    df_preds[ordered_cols] = Y_reconciled
    return df_preds
```
- Expected improvement: +0.005-0.01 (ensures consistency)

### Priority 3: Multi-Model Ensemble
**After feature improvements, ensemble multiple gradient boosting models**
- Models to include: LightGBM, CatBoost, XGBoost, HistGradientBoosting
- Simple averaging first, then try stacking with Ridge meta-learner
- Expected improvement: +0.01-0.02

### Priority 4: Test-Time Augmentation (TTA)
**For inference, average predictions across augmented views**
- Augmentations: horizontal flip, vertical flip, 90° rotation
- Apply to test images, average predictions
- Expected improvement: +0.005-0.01

## What NOT to Try (Yet)
- **Fine-tuning DINOv2**: Too complex for small dataset, risk of overfitting
- **SigLIP embeddings**: Requires additional model, try DINOv2 improvements first
- **Neural network head**: LightGBM is working well, focus on features first
- **Hyperparameter tuning**: Low ROI until features are improved

## Validation Notes
- Use 5-fold CV with random seed 42 (consistent with baseline)
- Weighted R² metric with weights: Dry_Total=0.5, GDM=0.2, others=0.1
- Monitor fold variance - if still high, consider stratified CV by State

## Execution Plan for Next Experiment
1. Extract DINOv2-large patch features (mean of patch tokens)
2. Train LightGBM with same parameters as baseline
3. Apply post-processing for biomass constraints
4. Compare CV score to baseline (0.7584)

Target for next experiment: CV ≥ 0.77 (halfway to target)