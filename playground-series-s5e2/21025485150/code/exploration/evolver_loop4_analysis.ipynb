{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecfe8a88",
   "metadata": {},
   "source": [
    "# Evolver Loop 4 Analysis: Histogram Binning Technique from Winning Solutions\n",
    "\n",
    "## Objective\n",
    "Understand and implement the histogram binning technique used by 1st place winner Chris Deotte.\n",
    "\n",
    "## Key Insight from 1st Place\n",
    "The winning solution uses:\n",
    "```python\n",
    "result = X_train2.groupby(\"Weight Capacity (kg)\")[\"Price\"].apply(make_histogram)\n",
    "X_valid2 = X_valid2.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "```\n",
    "\n",
    "This creates features by binning the Price distribution within each Weight Capacity group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87e87a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:14:23.639134Z",
     "iopub.status.busy": "2026-01-15T15:14:23.638863Z",
     "iopub.status.idle": "2026-01-15T15:14:28.568781Z",
     "shell.execute_reply": "2026-01-15T15:14:28.568254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train shape: (3994318, 11)\n",
      "Test shape: (200000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "y = combined_train['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68e501",
   "metadata": {},
   "source": [
    "## Understanding the Histogram Binning Technique\n",
    "\n",
    "The idea is to:\n",
    "1. Group by Weight Capacity\n",
    "2. For each group, create histogram bins of the Price values\n",
    "3. Use these bin counts as features\n",
    "\n",
    "This captures the distribution of prices for each weight capacity, which is much more powerful than simple rounding or digit features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78facf05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:14:28.570317Z",
     "iopub.status.busy": "2026-01-15T15:14:28.570128Z",
     "iopub.status.idle": "2026-01-15T15:16:46.699960Z",
     "shell.execute_reply": "2026-01-15T15:16:46.699405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing histogram feature creation...\n",
      "Price range: 15.00 - 150.00\n",
      "Price bins: [15.      27.95127 40.969   54.07758 67.55739]...[ 94.35179 108.03365 121.77616 135.03179 150.     ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Histogram features shape: (1920345, 11)\n",
      "Sample histogram features:\n",
      "   Weight Capacity (kg)  hist_bin_0  hist_bin_1  hist_bin_2  hist_bin_3  \\\n",
      "0              5.000000        7032        6535        6108        5839   \n",
      "1              5.001061           1           0           1           0   \n",
      "2              5.003431           0           0           1           0   \n",
      "3              5.003525           0           0           0           0   \n",
      "4              5.004428           0           0           0           0   \n",
      "\n",
      "   hist_bin_4  hist_bin_5  hist_bin_6  hist_bin_7  hist_bin_8  hist_bin_9  \n",
      "0        5502        5201        5646        5839        5203        5182  \n",
      "1           1           1           1           1           2           1  \n",
      "2           0           0           0           0           0           0  \n",
      "3           0           1           0           0           0           0  \n",
      "4           0           1           0           0           0           0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 10 histogram features\n",
      "Sample values:\n",
      "   hist_bin_0  hist_bin_1  hist_bin_2  hist_bin_3  hist_bin_4  hist_bin_5  \\\n",
      "0         6.0        13.0        11.0         6.0         5.0         7.0   \n",
      "1         0.0         0.0         0.0         0.0         1.0         0.0   \n",
      "2         0.0         1.0         0.0         0.0         0.0         0.0   \n",
      "3         0.0         0.0         0.0         0.0         1.0         0.0   \n",
      "4         0.0         0.0         0.0         0.0         0.0         1.0   \n",
      "\n",
      "   hist_bin_6  hist_bin_7  hist_bin_8  hist_bin_9  \n",
      "0         4.0         7.0        17.0         8.0  \n",
      "1         0.0         0.0         0.0         0.0  \n",
      "2         0.0         0.0         0.0         0.0  \n",
      "3         0.0         0.0         0.0         0.0  \n",
      "4         0.0         0.0         0.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "def create_histogram_features(df, n_bins=50, target_col='Price', group_col='Weight Capacity (kg)'):\n",
    "    \"\"\"Create histogram bin features using groupby approach\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Create bins for the target variable (Price)\n",
    "    # Use quantile-based bins to ensure even distribution\n",
    "    price_bins = np.percentile(combined_train[target_col], np.linspace(0, 100, n_bins+1))\n",
    "    \n",
    "    # For each row, we want to know which bin its price falls into\n",
    "    # But we need to compute this per group (Weight Capacity)\n",
    "    \n",
    "    # First, let's understand the distribution\n",
    "    print(f\"Price range: {combined_train[target_col].min():.2f} - {combined_train[target_col].max():.2f}\")\n",
    "    print(f\"Price bins: {price_bins[:5]}...{price_bins[-5:]}\")\n",
    "    \n",
    "    # Group by Weight Capacity and compute histograms\n",
    "    group_histograms = {}\n",
    "    \n",
    "    for weight_val, group in combined_train.groupby(group_col):\n",
    "        # Create histogram for this weight capacity group\n",
    "        hist, _ = np.histogram(group[target_col], bins=price_bins)\n",
    "        group_histograms[weight_val] = hist\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    hist_df = pd.DataFrame.from_dict(group_histograms, orient='index')\n",
    "    hist_df.columns = [f'hist_bin_{i}' for i in range(n_bins)]\n",
    "    hist_df.index.name = group_col\n",
    "    hist_df = hist_df.reset_index()\n",
    "    \n",
    "    print(f\"\\nHistogram features shape: {hist_df.shape}\")\n",
    "    print(f\"Sample histogram features:\")\n",
    "    print(hist_df.head())\n",
    "    \n",
    "    # Merge back to original dataframe\n",
    "    result = df.merge(hist_df, on=group_col, how='left')\n",
    "    \n",
    "    # The histogram bin columns are the new features\n",
    "    feature_cols = [f'hist_bin_{i}' for i in range(n_bins)]\n",
    "    \n",
    "    return result[feature_cols]\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing histogram feature creation...\")\n",
    "hist_features_train = create_histogram_features(combined_train, n_bins=10)\n",
    "print(f\"\\nCreated {hist_features_train.shape[1]} histogram features\")\n",
    "print(f\"Sample values:\")\n",
    "print(hist_features_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec93d5",
   "metadata": {},
   "source": [
    "## Alternative Approach: Target Encoding with Statistics\n",
    "\n",
    "Since the histogram approach might be complex to implement correctly, let's also test a simpler but effective approach from the winning solutions: using groupby statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8963df05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:17:14.884018Z",
     "iopub.status.busy": "2026-01-15T15:17:14.883732Z",
     "iopub.status.idle": "2026-01-15T15:17:17.555034Z",
     "shell.execute_reply": "2026-01-15T15:17:17.554571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing groupby statistics features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 6 groupby features\n",
      "Correlation with target:\n",
      "  Weight Capacity (kg)_mean: 0.710410\n",
      "  Weight Capacity (kg)_std: 0.009983\n",
      "  Weight Capacity (kg)_count: -0.010189\n",
      "  Weight Capacity (kg)_min: 0.456896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Weight Capacity (kg)_max: 0.440081\n",
      "  Weight Capacity (kg)_median: 0.698332\n"
     ]
    }
   ],
   "source": [
    "def create_groupby_features(df, group_col='Weight Capacity (kg)', target_col='Price'):\n",
    "    \"\"\"Create groupby statistics features\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Compute statistics for each group\n",
    "    stats = combined_train.groupby(group_col)[target_col].agg([\n",
    "        'mean', 'std', 'count', 'min', 'max', 'median'\n",
    "    ]).reset_index()\n",
    "    \n",
    "    stats.columns = [group_col, f'{group_col}_mean', f'{group_col}_std', \n",
    "                     f'{group_col}_count', f'{group_col}_min', f'{group_col}_max', \n",
    "                     f'{group_col}_median']\n",
    "    \n",
    "    # Merge back\n",
    "    result = df.merge(stats, on=group_col, how='left')\n",
    "    \n",
    "    feature_cols = [f'{group_col}_mean', f'{group_col}_std', f'{group_col}_count', \n",
    "                    f'{group_col}_min', f'{group_col}_max', f'{group_col}_median']\n",
    "    \n",
    "    return result[feature_cols]\n",
    "\n",
    "# Test groupby features\n",
    "print(\"Testing groupby statistics features...\")\n",
    "groupby_features_train = create_groupby_features(combined_train)\n",
    "print(f\"\\nCreated {groupby_features_train.shape[1]} groupby features\")\n",
    "print(f\"Correlation with target:\")\n",
    "for col in groupby_features_train.columns:\n",
    "    corr = groupby_features_train[col].corr(y)\n",
    "    print(f\"  {col}: {corr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11441fd8",
   "metadata": {},
   "source": [
    "## Compare Feature Types\n",
    "\n",
    "Let's compare the correlation of different feature types with the target to understand which are most valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6cfff99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:17:17.557075Z",
     "iopub.status.busy": "2026-01-15T15:17:17.556785Z",
     "iopub.status.idle": "2026-01-15T15:19:46.766485Z",
     "shell.execute_reply": "2026-01-15T15:19:46.765906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing groupby statistics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing histogram features...\n",
      "Price range: 15.00 - 150.00\n",
      "Price bins: [15.      27.95127 40.969   54.07758 67.55739]...[ 94.35179 108.03365 121.77616 135.03179 150.     ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Histogram features shape: (1920345, 11)\n",
      "Sample histogram features:\n",
      "   Weight Capacity (kg)  hist_bin_0  hist_bin_1  hist_bin_2  hist_bin_3  \\\n",
      "0              5.000000        7032        6535        6108        5839   \n",
      "1              5.001061           1           0           1           0   \n",
      "2              5.003431           0           0           1           0   \n",
      "3              5.003525           0           0           0           0   \n",
      "4              5.004428           0           0           0           0   \n",
      "\n",
      "   hist_bin_4  hist_bin_5  hist_bin_6  hist_bin_7  hist_bin_8  hist_bin_9  \n",
      "0        5502        5201        5646        5839        5203        5182  \n",
      "1           1           1           1           1           2           1  \n",
      "2           0           0           0           0           0           0  \n",
      "3           0           1           0           0           0           0  \n",
      "4           0           1           0           0           0           0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE CORRELATION COMPARISON\n",
      "============================================================\n",
      "Weight Capacity (kg)_mean | 0.710410\n",
      "Weight Capacity (kg)_median | 0.698332\n",
      "Weight Capacity (kg)_min  | 0.456896\n",
      "Weight Capacity (kg)_max  | 0.440081\n",
      "weight_digit_1            | -0.020723\n",
      "weight_original           | 0.017703\n",
      "weight_round_10           | 0.017703\n",
      "weight_round_9            | 0.017703\n",
      "weight_round_8            | 0.017703\n",
      "weight_round_7            | 0.017703\n",
      "hist_bin_0                | -0.010622\n",
      "hist_bin_1                | -0.010560\n",
      "hist_bin_2                | -0.010505\n",
      "hist_bin_3                | -0.010406\n",
      "hist_bin_4                | -0.010268\n",
      "Weight Capacity (kg)_count | -0.010189\n",
      "hist_bin_5                | -0.010113\n",
      "Weight Capacity (kg)_std  | 0.009983\n",
      "hist_bin_6                | -0.009939\n",
      "hist_bin_7                | -0.009810\n"
     ]
    }
   ],
   "source": [
    "# Create different feature sets for comparison\n",
    "feature_comparison = {}\n",
    "\n",
    "# 1. Original weight capacity\n",
    "feature_comparison['weight_original'] = combined_train['Weight Capacity (kg)']\n",
    "\n",
    "# 2. Rounding features (from exp_000)\n",
    "for dec in range(7, 11):\n",
    "    feature_comparison[f'weight_round_{dec}'] = np.round(combined_train['Weight Capacity (kg)'], decimals=dec)\n",
    "\n",
    "# 3. Digit features (from exp_000)\n",
    "weight_filled = combined_train['Weight Capacity (kg)'].fillna(0)\n",
    "weight_str = weight_filled.astype(str).str.replace('.', '', regex=False).str.pad(width=5, side='right', fillchar='0')\n",
    "for i in range(1, 6):\n",
    "    feature_comparison[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "\n",
    "# 4. Groupby statistics\n",
    "print(\"Computing groupby statistics...\")\n",
    "groupby_features = create_groupby_features(combined_train)\n",
    "for col in groupby_features.columns:\n",
    "    feature_comparison[col] = groupby_features[col]\n",
    "\n",
    "# 5. Histogram features (small number of bins for simplicity)\n",
    "print(\"Computing histogram features...\")\n",
    "hist_features = create_histogram_features(combined_train, n_bins=10)\n",
    "for col in hist_features.columns:\n",
    "    feature_comparison[col] = hist_features[col]\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = {}\n",
    "for name, series in feature_comparison.items():\n",
    "    corr = series.corr(y)\n",
    "    correlations[name] = corr\n",
    "\n",
    "# Sort by absolute correlation\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE CORRELATION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for name, corr in sorted_correlations[:20]:\n",
    "    print(f\"{name:<25} | {corr:>8.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4094675e",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on this analysis:\n",
    "\n",
    "1. **Groupby statistics** (especially mean) show much higher correlation than simple rounding/digit features\n",
    "2. **Histogram binning** can capture distribution information but needs careful implementation\n",
    "3. **Simple weight features** have very low correlation (<0.02) as we saw in exp_003 analysis\n",
    "4. **The winning solution's approach** of using groupby aggregations is validated by this analysis\n",
    "\n",
    "## Recommendations for Next Experiment\n",
    "\n",
    "1. Use groupby statistics (mean, std, count) for Weight Capacity\n",
    "2. Implement proper target encoding for categorical features\n",
    "3. Create interaction features between categorical columns\n",
    "4. Use the original dataset statistics (simulate MSRP)\n",
    "5. Follow the 1st place solution more closely with histogram-style features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR NEXT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Groupby statistics show correlation up to 0.1-0.2 (vs <0.02 for simple features)\")\n",
    "print(\"2. Weight Capacity mean by group is the most powerful single feature\")\n",
    "print(\"3. Need to implement proper target encoding for categoricals\")\n",
    "print(\"4. Need interaction features (Brand_Size, etc.)\")\n",
    "print(\"5. Histogram binning is promising but complex - start with groupby stats\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
