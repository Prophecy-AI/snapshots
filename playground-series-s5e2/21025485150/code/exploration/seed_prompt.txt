## Competition Overview
This is a tabular regression problem predicting backpack prices. The evaluation metric is RMSE (lower is better). Target score to beat: 38.616280.

**Reference notebooks for data characteristics:**
- Create EDA notebooks to understand feature distributions, missing values, and target characteristics
- Analyze Weight Capacity (kg) distribution - this is the most important feature per winning solutions

## Core Winning Strategies

### 1. Feature Engineering (Most Critical)
Based on 1st, 2nd, and 3rd place solutions, extensive feature engineering is essential:

**Weight Capacity (kg) Engineering:**
- This is the most powerful column - create multiple derived features:
  - Round to different decimal places (7-10 decimal places)
  - Extract digits: For k in range(1,10): ((value * 10**k) % 10).fillna(-1)
  - Create histogram bins: Group by rounded/binned values and count target distribution in bins
  - Create quantile features: Calculate quantiles [5,10,40,45,55,60,90,95] within groups

**Groupby Aggregations:**
- Use format: groupby(COL1)[COL2].agg(STAT)
- Key statistics: mean, std, count, nunique, min, max, skew, median
- COL1 can be any categorical or engineered feature
- COL2 can be target (Price) or other features
- Use nested folds when COL2 is target to prevent leakage

**Categorical Feature Combinations:**
- Combine all pairs of categorical columns (28 combinations for 8 cats)
- Combine categorical columns with Weight Capacity (kg)
- Use label encoding first, then combine: new_col = cat1 * multiplier + cat2
- Also try 1-2-3-4-5-6-7 gram combinations for sequential features

**Missing Value Indicators:**
- Create binary indicators for each column's missing values
- Create combined NaN column: sum(is_na * 2**i) across multiple columns
- Count total missing values per row

**Target Encoding:**
- Use smoothed target encoding for high-cardinality categoricals
- Implement with proper cross-validation to prevent leakage
- Try aggregators: mean, median, count, nunique

**External Data:**
- Use original Student Bag Prediction dataset that this synthetic data was based on
- Group original data by key features (especially Weight Capacity) and aggregate target
- Merge these statistics as additional features

### 2. Model Architecture

**Primary Models (Level 1):**
- Gradient boosting: XGBoost, LightGBM, CatBoost
- Each model should use different feature subsets for diversity
- Train 20+ variants of each model type with different features
- Use GPU acceleration (RAPIDS cuML) for faster iteration

**Secondary Models (Level 2 - Stacking):**
- Neural network stacker: MLP that takes L1 predictions as input
- Train on out-of-fold predictions from L1 models
- Use TPU/GPU for training

**Tertiary Models (Level 3 - Final Blend):**
- Ridge regression or Bayesian Ridge for final ensemble
- Input: L2 predictions + L1 predictions + any additional models
- This stabilizes predictions and reduces RMSE

### 3. Validation Strategy
- Use 20-fold KFold (shuffle=True, random_state=42) for consistency
- Stratified splits if possible based on target distribution
- Time-based splits NOT needed (data is not temporal)

### 4. Ensembling Strategy
- **Diversity is key**: Use different feature sets, different model types, different hyperparameters
- **Number of models**: 25-100 models in final ensemble (2nd place used 100)
- **Stacking approach**: L1 (tree models) → L2 (NN stacker) → L3 (Ridge)
- **Alternative**: Simple averaging of diverse models can also work

### 5. Implementation Optimizations

**GPU Acceleration:**
- Use RAPIDS cuDF/cuML for feature engineering (10-100x speedup)
- Essential for running thousands of feature engineering experiments
- Enables training 300+ XGBoost models in a month

**TPU Usage:**
- Use Google Colab/Kaggle TPUs for neural network training
- Especially effective for L2 stacker models
- Provides 200GB+ RAM for feature creation

**Feature Store:**
- Pre-compute features and store in parquet files
- Create separate datasets for different feature groups
- Use Polars for fast feature retrieval

### 6. Hyperparameters
- Focus on feature engineering over hyperparameter tuning
- Standard hyperparameters work well (learning_rate=0.1, max_depth=6-8)
- For XGBoost: tree_method='gpu_hist' for GPU training
- For LightGBM: device='gpu' for GPU training

### 7. Post-Processing
- Try rounding predictions to nearest integer (may improve LB but hurt CV)
- Clip predictions to reasonable range based on target distribution
- Weighted blending of different submissions

### 8. What Didn't Work
- Sample weights
- Appending original data to training data directly
- Variance/std-dev aggregators in target encoding
- Copying targets across repeated rows between train/test
- Using models other than Ridge/Bayesian Ridge for final blending

## Key Takeaways
1. **Feature engineering is 80% of the solution** - Run thousands of experiments
2. **GPU acceleration enables rapid iteration** - Use RAPIDS for feature engineering
3. **Ensembling with stacking beats single models** - But single model can still win with perfect features
4. **Weight Capacity (kg) is gold** - Extract every possible feature from it
5. **External data helps** - Original dataset provides valuable signals
6. **Diversity over quantity** - 25 diverse models > 100 similar models

## Expected Resources
- **GPU**: 16GB+ VRAM recommended (A100 80GB ideal, T4 16GB workable)
- **RAM**: 128GB+ for feature engineering
- **Time**: 20-30 hours for full pipeline with proper GPU acceleration
- **Storage**: 10GB+ for feature store and intermediate results