{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671ac676",
   "metadata": {},
   "source": [
    "# Loop 7 Analysis: Diagnosing Submission Duplicate IDs\n",
    "\n",
    "**Goal**: Identify why exp_006 submission has 40 duplicate IDs and fix the issue\n",
    "\n",
    "**Problem**: Submission file has 200,041 rows instead of 200,000 - 40 duplicate IDs found\n",
    "\n",
    "**Hypothesis**: Index misalignment or concatenation bug in prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading data to diagnose duplicate issue...\")\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Test ID range: {test['id'].min()} - {test['id'].max()}\")\n",
    "print(f\"Test ID uniqueness: {test['id'].nunique() == len(test)}\")\n",
    "\n",
    "# Load submission\n",
    "sub = pd.read_csv('/home/code/submission_candidates/candidate_005.csv')\n",
    "print(f\"\\nSubmission shape: {sub.shape}\")\n",
    "print(f\"Submission ID range: {sub['id'].min()} - {sub['id'].max()}\")\n",
    "print(f\"Submission ID uniqueness: {sub['id'].nunique() == len(sub)}\")\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = sub[sub['id'].duplicated(keep=False)].copy()\n",
    "print(f\"\\nTotal duplicate rows: {len(duplicates)}\")\n",
    "print(f\"Unique duplicated IDs: {sub['id'].duplicated().sum()}\")\n",
    "\n",
    "# Show sample duplicates\n",
    "print(\"\\nSample duplicate rows:\")\n",
    "print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb45b5b",
   "metadata": {},
   "source": [
    "## Analyze Duplicate Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the pattern of duplicates\n",
    "print(\"=== DUPLICATE PATTERN ANALYSIS ===\")\n",
    "\n",
    "# Group by ID to see the duplication pattern\n",
    "dup_groups = duplicates.groupby('id').agg({\n",
    "    'Price': ['count', 'first', 'last', 'nunique']\n",
    "}).round(6)\n",
    "dup_groups.columns = ['count', 'first_price', 'last_price', 'unique_prices']\n",
    "\n",
    "print(f\"Duplicate ID groups: {len(dup_groups)}\")\n",
    "print(f\"IDs with 2 rows: {(dup_groups['count'] == 2).sum()}\")\n",
    "print(f\"IDs with >2 rows: {(dup_groups['count'] > 2).sum()}\")\n",
    "print(f\"IDs with different prices: {(dup_groups['unique_prices'] > 1).sum()}\")\n",
    "\n",
    "print(\"\\nSample duplicate groups:\")\n",
    "print(dup_groups.head(10))\n",
    "\n",
    "# Check if duplicates are consecutive\n",
    "print(\"\\n=== CHECKING IF DUPLICATES ARE CONSECUTIVE ===\")\n",
    "dup_ids = sub[sub['id'].duplicated()]['id'].unique()\n",
    "for dup_id in dup_ids[:5]:\n",
    "    idx = sub[sub['id'] == dup_id].index\n",
    "    print(f\"ID {dup_id}: indices {list(idx)}, consecutive: {idx[1] - idx[0] == 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8a2d9",
   "metadata": {},
   "source": [
    "## Investigate Experiment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f18ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the experiment folder to understand submission generation\n",
    "import os\n",
    "import json\n",
    "\n",
    "exp_folder = \"/home/code/experiments/006_original_dataset_combo\"\n",
    "print(f\"Checking experiment folder: {exp_folder}\")\n",
    "\n",
    "# List files in experiment folder\n",
    "if os.path.exists(exp_folder):\n",
    "    files = os.listdir(exp_folder)\n",
    "    print(f\"Files in experiment folder: {files}\")\n",
    "    \n",
    "    # Look for submission generation code\n",
    "    py_files = [f for f in files if f.endswith('.py')]\n",
    "    print(f\"Python files: {py_files}\")\n",
    "    \n",
    "    # Check if there's a notebook\n",
    "    nb_files = [f for f in files if f.endswith('.ipynb')]\n",
    "    print(f\"Notebook files: {nb_files}\")\n",
    "else:\n",
    "    print(\"Experiment folder not found!\")\n",
    "\n",
    "# Check session state for clues\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "exp_006 = next((e for e in session_state['experiments'] if e['id'] == 'exp_005'), None)\n",
    "if exp_006:\n",
    "    print(f\"\\nExperiment notes: {exp_006.get('notes', 'No notes')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088627ca",
   "metadata": {},
   "source": [
    "## Root Cause Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ROOT CAUSE HYPOTHESES ===\")\n",
    "\n",
    "print(\"\\n1. INDEX MISALIGNMENT:\")\n",
    "print(\"   - Test data reordered during feature engineering\")\n",
    "print(\"   - Predictions generated with different index order\")\n",
    "print(\"   - When merging back, some indices duplicated\")\n",
    "\n",
    "print(\"\\n2. CONCATENATION BUG:\")\n",
    "print(\"   - Multiple prediction arrays concatenated incorrectly\")\n",
    "print(\"   - Could happen if using np.concatenate or pd.concat with wrong axis\")\n",
    "\n",
    "print(\"\\n3. CROSS-VALIDATION LEAKAGE:\")\n",
    "print(\"   - CV predictions not properly aligned with test IDs\")\n",
    "print(\"   - Some folds might have overlapping indices\")\n",
    "\n",
    "print(\"\\n4. FEATURE ENGINEERING SIDE EFFECT:\")\n",
    "print(\"   - Groupby operations or merges might create duplicate rows\")\n",
    "print(\"   - If test data has duplicate Weight Capacity values, groupby could expand rows\")\n",
    "\n",
    "# Check if test data has any duplicate weight capacity values that could cause issues\n",
    "test_dup_weights = test['Weight Capacity (kg)'].duplicated().sum()\n",
    "print(f\"\\nTest data duplicate Weight Capacity values: {test_dup_weights}\")\n",
    "\n",
    "# Check distribution of weight capacity\n",
    "weight_counts = test['Weight Capacity (kg)'].value_counts()\n",
    "print(f\"Max duplicates for any weight capacity: {weight_counts.max()}\")\n",
    "print(f\"Number of weight capacities with >1 occurrence: {(weight_counts > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df047d",
   "metadata": {},
   "source": [
    "## Solution Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SOLUTION PLAN ===\")\n",
    "\n",
    "print(\"\\n1. CREATE CLEAN SUBMISSION:\")\n",
    "print(\"   - Load test.csv to get correct ID order\")\n",
    "print(\"   - Generate predictions using trained model from exp_006\")\n",
    "print(\"   - Ensure predictions align 1:1 with test IDs\")\n",
    "print(\"   - Save with exactly 200,000 rows\")\n",
    "\n",
    "print(\"\\n2. VERIFY ALIGNMENT:\")\n",
    "print(\"   - Check that prediction length matches test length\")\n",
    "print(\"   - Verify no duplicate IDs in output\")\n",
    "print(\"   - Validate ID range matches test data\")\n",
    "\n",
    "print(\"\\n3. RESUBMIT:\")\n",
    "print(\"   - Use Submit() function with corrected candidate\")\n",
    "print(\"   - Monitor for successful upload\")\n",
    "\n",
    "# Create a clean submission template\n",
    "clean_sub = test[['id']].copy()\n",
    "clean_sub['Price'] = 0.0  # Placeholder\n",
    "\n",
    "print(f\"\\nClean submission template shape: {clean_sub.shape}\")\n",
    "print(f\"Clean submission ID uniqueness: {clean_sub['id'].nunique() == len(clean_sub)}\")\n",
    "print(f\"ID range: {clean_sub['id'].min()} - {clean_sub['id'].max()}\")\n",
    "\n",
    "# Save clean template\n",
    "clean_sub.to_csv('/home/code/submission_candidates/candidate_005_clean.csv', index=False)\n",
    "print(\"\\nâœ“ Clean template saved to: /home/code/submission_candidates/candidate_005_clean.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
