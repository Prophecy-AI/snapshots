{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671ac676",
   "metadata": {},
   "source": [
    "# Loop 7 Analysis: Diagnosing Submission Duplicate IDs\n",
    "\n",
    "**Goal**: Identify why exp_006 submission has 40 duplicate IDs and fix the issue\n",
    "\n",
    "**Problem**: Submission file has 200,041 rows instead of 200,000 - 40 duplicate IDs found\n",
    "\n",
    "**Hypothesis**: Index misalignment or concatenation bug in prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dad5077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:54:29.507521Z",
     "iopub.status.busy": "2026-01-16T01:54:29.507251Z",
     "iopub.status.idle": "2026-01-16T01:54:30.126093Z",
     "shell.execute_reply": "2026-01-16T01:54:30.125540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to diagnose duplicate issue...\n",
      "Test data shape: (200000, 10)\n",
      "Test ID range: 300000 - 499999\n",
      "Test ID uniqueness: True\n",
      "\n",
      "Submission shape: (200040, 2)\n",
      "Submission ID range: 300000 - 499999\n",
      "Submission ID uniqueness: False\n",
      "\n",
      "Total duplicate rows: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique duplicated IDs: 40\n",
      "\n",
      "Sample duplicate rows:\n",
      "           id      Price\n",
      "576    300576  91.312671\n",
      "577    300576  91.312671\n",
      "4191   304190  66.682781\n",
      "4192   304190  66.682781\n",
      "6025   306023  96.941909\n",
      "6026   306023  96.941909\n",
      "10368  310365  94.525570\n",
      "10369  310365  94.525570\n",
      "15683  315679  94.442971\n",
      "15684  315679  94.442971\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading data to diagnose duplicate issue...\")\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Test ID range: {test['id'].min()} - {test['id'].max()}\")\n",
    "print(f\"Test ID uniqueness: {test['id'].nunique() == len(test)}\")\n",
    "\n",
    "# Load submission\n",
    "sub = pd.read_csv('/home/code/submission_candidates/candidate_005.csv')\n",
    "print(f\"\\nSubmission shape: {sub.shape}\")\n",
    "print(f\"Submission ID range: {sub['id'].min()} - {sub['id'].max()}\")\n",
    "print(f\"Submission ID uniqueness: {sub['id'].nunique() == len(sub)}\")\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = sub[sub['id'].duplicated(keep=False)].copy()\n",
    "print(f\"\\nTotal duplicate rows: {len(duplicates)}\")\n",
    "print(f\"Unique duplicated IDs: {sub['id'].duplicated().sum()}\")\n",
    "\n",
    "# Show sample duplicates\n",
    "print(\"\\nSample duplicate rows:\")\n",
    "print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb45b5b",
   "metadata": {},
   "source": [
    "## Analyze Duplicate Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e44b297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:54:30.127403Z",
     "iopub.status.busy": "2026-01-16T01:54:30.127276Z",
     "iopub.status.idle": "2026-01-16T01:54:30.139744Z",
     "shell.execute_reply": "2026-01-16T01:54:30.139286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE PATTERN ANALYSIS ===\n",
      "Duplicate ID groups: 40\n",
      "IDs with 2 rows: 40\n",
      "IDs with >2 rows: 0\n",
      "IDs with different prices: 0\n",
      "\n",
      "Sample duplicate groups:\n",
      "        count  first_price  last_price  unique_prices\n",
      "id                                                   \n",
      "300576      2    91.312671   91.312671              1\n",
      "304190      2    66.682781   66.682781              1\n",
      "306023      2    96.941909   96.941909              1\n",
      "310365      2    94.525570   94.525570              1\n",
      "315679      2    94.442971   94.442971              1\n",
      "316920      2    94.945388   94.945388              1\n",
      "320094      2   113.191144  113.191144              1\n",
      "325617      2    76.980625   76.980625              1\n",
      "327553      2    84.249429   84.249429              1\n",
      "330018      2    54.893704   54.893704              1\n",
      "\n",
      "=== CHECKING IF DUPLICATES ARE CONSECUTIVE ===\n",
      "ID 300576: indices [576, 577], consecutive: True\n",
      "ID 304190: indices [4191, 4192], consecutive: True\n",
      "ID 306023: indices [6025, 6026], consecutive: True\n",
      "ID 310365: indices [10368, 10369], consecutive: True\n",
      "ID 315679: indices [15683, 15684], consecutive: True\n"
     ]
    }
   ],
   "source": [
    "# Analyze the pattern of duplicates\n",
    "print(\"=== DUPLICATE PATTERN ANALYSIS ===\")\n",
    "\n",
    "# Group by ID to see the duplication pattern\n",
    "dup_groups = duplicates.groupby('id').agg({\n",
    "    'Price': ['count', 'first', 'last', 'nunique']\n",
    "}).round(6)\n",
    "dup_groups.columns = ['count', 'first_price', 'last_price', 'unique_prices']\n",
    "\n",
    "print(f\"Duplicate ID groups: {len(dup_groups)}\")\n",
    "print(f\"IDs with 2 rows: {(dup_groups['count'] == 2).sum()}\")\n",
    "print(f\"IDs with >2 rows: {(dup_groups['count'] > 2).sum()}\")\n",
    "print(f\"IDs with different prices: {(dup_groups['unique_prices'] > 1).sum()}\")\n",
    "\n",
    "print(\"\\nSample duplicate groups:\")\n",
    "print(dup_groups.head(10))\n",
    "\n",
    "# Check if duplicates are consecutive\n",
    "print(\"\\n=== CHECKING IF DUPLICATES ARE CONSECUTIVE ===\")\n",
    "dup_ids = sub[sub['id'].duplicated()]['id'].unique()\n",
    "for dup_id in dup_ids[:5]:\n",
    "    idx = sub[sub['id'] == dup_id].index\n",
    "    print(f\"ID {dup_id}: indices {list(idx)}, consecutive: {idx[1] - idx[0] == 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8a2d9",
   "metadata": {},
   "source": [
    "## Investigate Experiment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f18ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T01:54:30.141058Z",
     "iopub.status.busy": "2026-01-16T01:54:30.140810Z",
     "iopub.status.idle": "2026-01-16T01:54:30.145455Z",
     "shell.execute_reply": "2026-01-16T01:54:30.145029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking experiment folder: /home/code/experiments/006_original_dataset_combo\n",
      "Files in experiment folder: []\n",
      "Python files: []\n",
      "Notebook files: []\n",
      "\n",
      "Experiment notes: MASSIVE IMPROVEMENT: Added original dataset features (orig_price, orig_price_r7, orig_price_r8, orig_price_r9) and COMBO/interaction features (NaN encoding, NaN×Weight Capacity, categorical×Weight Capacity). Removed histogram bins. Optimized groupby statistics (kept mean, count, median only). Hyperparameters: learning_rate=0.03, max_depth=10, reg_alpha=0.1, reg_lambda=1.0. 20-fold CV. Feature count: 53 total (4 orig + 15 combo + 28 groupby + 6 other). Top features: weight_capacity_mean_price, weight_capacity_median_price, Laptop Compartment_median_price. Achieved RMSE 24.321 ± 0.162, beating target 38.616 by 14.295 points!\n"
     ]
    }
   ],
   "source": [
    "# Look at the experiment folder to understand submission generation\n",
    "import os\n",
    "import json\n",
    "\n",
    "exp_folder = \"/home/code/experiments/006_original_dataset_combo\"\n",
    "print(f\"Checking experiment folder: {exp_folder}\")\n",
    "\n",
    "# List files in experiment folder\n",
    "if os.path.exists(exp_folder):\n",
    "    files = os.listdir(exp_folder)\n",
    "    print(f\"Files in experiment folder: {files}\")\n",
    "    \n",
    "    # Look for submission generation code\n",
    "    py_files = [f for f in files if f.endswith('.py')]\n",
    "    print(f\"Python files: {py_files}\")\n",
    "    \n",
    "    # Check if there's a notebook\n",
    "    nb_files = [f for f in files if f.endswith('.ipynb')]\n",
    "    print(f\"Notebook files: {nb_files}\")\n",
    "else:\n",
    "    print(\"Experiment folder not found!\")\n",
    "\n",
    "# Check session state for clues\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "exp_006 = next((e for e in session_state['experiments'] if e['id'] == 'exp_005'), None)\n",
    "if exp_006:\n",
    "    print(f\"\\nExperiment notes: {exp_006.get('notes', 'No notes')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088627ca",
   "metadata": {},
   "source": [
    "## Root Cause Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ROOT CAUSE HYPOTHESES ===\")\n",
    "\n",
    "print(\"\\n1. INDEX MISALIGNMENT:\")\n",
    "print(\"   - Test data reordered during feature engineering\")\n",
    "print(\"   - Predictions generated with different index order\")\n",
    "print(\"   - When merging back, some indices duplicated\")\n",
    "\n",
    "print(\"\\n2. CONCATENATION BUG:\")\n",
    "print(\"   - Multiple prediction arrays concatenated incorrectly\")\n",
    "print(\"   - Could happen if using np.concatenate or pd.concat with wrong axis\")\n",
    "\n",
    "print(\"\\n3. CROSS-VALIDATION LEAKAGE:\")\n",
    "print(\"   - CV predictions not properly aligned with test IDs\")\n",
    "print(\"   - Some folds might have overlapping indices\")\n",
    "\n",
    "print(\"\\n4. FEATURE ENGINEERING SIDE EFFECT:\")\n",
    "print(\"   - Groupby operations or merges might create duplicate rows\")\n",
    "print(\"   - If test data has duplicate Weight Capacity values, groupby could expand rows\")\n",
    "\n",
    "# Check if test data has any duplicate weight capacity values that could cause issues\n",
    "test_dup_weights = test['Weight Capacity (kg)'].duplicated().sum()\n",
    "print(f\"\\nTest data duplicate Weight Capacity values: {test_dup_weights}\")\n",
    "\n",
    "# Check distribution of weight capacity\n",
    "weight_counts = test['Weight Capacity (kg)'].value_counts()\n",
    "print(f\"Max duplicates for any weight capacity: {weight_counts.max()}\")\n",
    "print(f\"Number of weight capacities with >1 occurrence: {(weight_counts > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df047d",
   "metadata": {},
   "source": [
    "## Solution Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SOLUTION PLAN ===\")\n",
    "\n",
    "print(\"\\n1. CREATE CLEAN SUBMISSION:\")\n",
    "print(\"   - Load test.csv to get correct ID order\")\n",
    "print(\"   - Generate predictions using trained model from exp_006\")\n",
    "print(\"   - Ensure predictions align 1:1 with test IDs\")\n",
    "print(\"   - Save with exactly 200,000 rows\")\n",
    "\n",
    "print(\"\\n2. VERIFY ALIGNMENT:\")\n",
    "print(\"   - Check that prediction length matches test length\")\n",
    "print(\"   - Verify no duplicate IDs in output\")\n",
    "print(\"   - Validate ID range matches test data\")\n",
    "\n",
    "print(\"\\n3. RESUBMIT:\")\n",
    "print(\"   - Use Submit() function with corrected candidate\")\n",
    "print(\"   - Monitor for successful upload\")\n",
    "\n",
    "# Create a clean submission template\n",
    "clean_sub = test[['id']].copy()\n",
    "clean_sub['Price'] = 0.0  # Placeholder\n",
    "\n",
    "print(f\"\\nClean submission template shape: {clean_sub.shape}\")\n",
    "print(f\"Clean submission ID uniqueness: {clean_sub['id'].nunique() == len(clean_sub)}\")\n",
    "print(f\"ID range: {clean_sub['id'].min()} - {clean_sub['id'].max()}\")\n",
    "\n",
    "# Save clean template\n",
    "clean_sub.to_csv('/home/code/submission_candidates/candidate_005_clean.csv', index=False)\n",
    "print(\"\\n✓ Clean template saved to: /home/code/submission_candidates/candidate_005_clean.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
