{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25263dc5",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "Analyze experiment results and identify patterns to exploit for the next iteration.\n",
    "\n",
    "Focus areas:\n",
    "1. Feature importance analysis from baseline model\n",
    "2. Data patterns in Weight Capacity\n",
    "3. Categorical feature distributions\n",
    "4. Opportunities for target encoding and interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train1 = pd.read_csv('/home/data/train.csv')\n",
    "train2 = pd.read_csv('/home/data/training_extra.csv')\n",
    "train = pd.concat([train1, train2], ignore_index=True)\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing (same as baseline)\n",
    "cat_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Handle missing values\n",
    "for col in cat_features:\n",
    "    train[col] = train[col].fillna('Missing')\n",
    "    test[col] = test[col].fillna('Missing')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train[col], test[col]], ignore_index=True)\n",
    "    le.fit(combined.astype(str))\n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "# Handle numerical missing values\n",
    "train['Compartments'] = train['Compartments'].fillna(train['Compartments'].median())\n",
    "test['Compartments'] = test['Compartments'].fillna(train['Compartments'].median())\n",
    "train['Weight Capacity (kg)'] = train['Weight Capacity (kg)'].fillna(train['Weight Capacity (kg)'].median())\n",
    "test['Weight Capacity (kg)'] = test['Weight Capacity (kg)'].fillna(train['Weight Capacity (kg)'].median())\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d34998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Weight Capacity distribution - the most important feature\n",
    "print(\"Weight Capacity analysis:\")\n",
    "print(f\"Range: {train['Weight Capacity (kg)'].min():.4f} to {train['Weight Capacity (kg)'].max():.4f}\")\n",
    "print(f\"Mean: {train['Weight Capacity (kg)'].mean():.4f}\")\n",
    "print(f\"Std: {train['Weight Capacity (kg)'].std():.4f}\")\n",
    "\n",
    "# Look at relationship with target\n",
    "correlation = train['Weight Capacity (kg)'].corr(train['Price'])\n",
    "print(f\"Correlation with Price: {correlation:.4f}\")\n",
    "\n",
    "# Check unique values\n",
    "unique_weights = train['Weight Capacity (kg)'].nunique()\n",
    "print(f\"Unique Weight Capacity values: {unique_weights:,}\")\n",
    "\n",
    "# Look at distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train['Weight Capacity (kg)'], bins=50, alpha=0.7)\n",
    "plt.title('Weight Capacity Distribution')\n",
    "plt.xlabel('Weight Capacity (kg)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(train['Weight Capacity (kg)'], train['Price'], alpha=0.1, s=1)\n",
    "plt.title('Weight Capacity vs Price')\n",
    "plt.xlabel('Weight Capacity (kg)')\n",
    "plt.ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a656ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features - look for patterns\n",
    "print(\"Categorical feature analysis:\")\n",
    "for col in cat_features:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {train[col].nunique()}\")\n",
    "    print(f\"  Most common: {train[col].value_counts().head(3).to_dict()}\")\n",
    "    \n",
    "    # Check relationship with target\n",
    "    target_by_cat = train.groupby(col)['Price'].agg(['mean', 'std', 'count'])\n",
    "    print(f\"  Price range: {target_by_cat['mean'].min():.2f} to {target_by_cat['mean'].max():.2f}\")\n",
    "    print(f\"  Price std range: {target_by_cat['std'].min():.2f} to {target_by_cat['std'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at categorical combinations that might be important\n",
    "# Create some simple interaction features and see their effect\n",
    "\n",
    "# Brand × Material\n",
    "brand_material = train['Brand'].astype(str) + '_' + train['Material'].astype(str)\n",
    "train['brand_material_combo'] = brand_material\n",
    "print(f\"Brand×Material combinations: {train['brand_material_combo'].nunique():,}\")\n",
    "\n",
    "# Size × Style  \n",
    "size_style = train['Size'].astype(str) + '_' + train['Style'].astype(str)\n",
    "train['size_style_combo'] = size_style\n",
    "print(f\"Size×Style combinations: {train['size_style_combo'].nunique():,}\")\n",
    "\n",
    "# Check target variance in these combinations\n",
    "bm_stats = train.groupby('brand_material_combo')['Price'].agg(['mean', 'std', 'count'])\n",
    "ss_stats = train.groupby('size_style_combo')['Price'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(f\"\\nBrand×Material - Price std range: {bm_stats['std'].min():.2f} to {bm_stats['std'].max():.2f}\")\n",
    "print(f\"Size×Style - Price std range: {ss_stats['std'].min():.2f} to {ss_stats['std'].max():.2f}\")\n",
    "\n",
    "# Look at combinations with high variance (most predictive potential)\n",
    "print(\"\\nTop 5 Brand×Material combos by price variance:\")\n",
    "print(bm_stats.nlargest(5, 'std')[['mean', 'std', 'count']])\n",
    "\n",
    "print(\"\\nTop 5 Size×Style combos by price variance:\")\n",
    "print(ss_stats.nlargest(5, 'std')[['mean', 'std', 'count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59610d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the potential for target encoding\n",
    "# Look at how stable target means are across different folds\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data for faster analysis\n",
    "sample_train = train.sample(frac=0.1, random_state=42)  # 10% sample\n",
    "\n",
    "# Test target encoding stability for a few key features\n",
    "test_features = ['Brand', 'Material', 'Size', 'Style']\n",
    "\n",
    "print(\"Target encoding stability analysis:\")\n",
    "for feature in test_features:\n",
    "    # Split sample into two halves\n",
    "    half1, half2 = train_test_split(train, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Compute target means on each half\n",
    "    means1 = half1.groupby(feature)['Price'].mean()\n",
    "    means2 = half2.groupby(feature)['Price'].mean()\n",
    "    \n",
    "    # Join and compute correlation\n",
    "    common_cats = means1.index.intersection(means2.index)\n",
    "    if len(common_cats) > 10:\n",
    "        correlation = means1[common_cats].corr(means2[common_cats])\n",
    "        print(f\"{feature}: Correlation between halves = {correlation:.4f}\")\n",
    "        print(f\"  Categories with >100 samples: {(half1[feature].value_counts() > 100).sum()}\")\n",
    "    else:\n",
    "        print(f\"{feature}: Too few common categories for stable encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379daf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if original Student Bag dataset is available and what it contains\n",
    "# The competition mentions we can use the original dataset\n",
    "\n",
    "import os\n",
    "original_dataset_path = '/home/data/student_bag_original.csv'\n",
    "\n",
    "if os.path.exists(original_dataset_path):\n",
    "    print(\"Original Student Bag dataset found!\")\n",
    "    original = pd.read_csv(original_dataset_path)\n",
    "    print(f\"Original dataset shape: {original.shape}\")\n",
    "    print(f\"Columns: {list(original.columns)}\")\n",
    "    \n",
    "    # Check overlap with current data\n",
    "    print(f\"\\nOriginal Weight Capacity range: {original['Weight Capacity (kg)'].min():.4f} to {original['Weight Capacity (kg)'].max():.4f}\")\n",
    "    print(f\"Current Weight Capacity range: {train['Weight Capacity (kg)'].min():.4f} to {train['Weight Capacity (kg)'].max():.4f}\")\n",
    "    \n",
    "    # Look at target distribution differences\n",
    "    print(f\"\\nOriginal Price mean: {original['Price'].mean():.4f}\")\n",
    "    print(f\"Current Price mean: {train['Price'].mean():.4f}\")\n",
    "else:\n",
    "    print(\"Original Student Bag dataset not found at expected location\")\n",
    "    print(\"Need to download from Kaggle or create from available data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings for next experiment\n",
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS FOR NEXT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. WEIGHT CAPACITY IS CRITICAL:\")\n",
    "print(f\"   - Correlation with Price: {correlation:.4f}\")\n",
    "print(f\"   - {unique_weights:,} unique values (high cardinality)\")\n",
    "print(f\"   - Complex relationship with target (non-linear)\")\n",
    "\n",
    "print(\"\\n2. CATEGORICAL FEATURES HAVE SIGNAL:\")\n",
    "for col in cat_features:\n",
    "    target_range = train.groupby(col)['Price'].mean().agg(['min', 'max'])\n",
    "    print(f\"   - {col}: Price range {target_range['min']:.2f} to {target_range['max']:.2f}\")\n",
    "\n",
    "print(\"\\n3. INTERACTIONS SHOW PROMISE:\")\n",
    "print(f\"   - Brand×Material: {train['brand_material_combo'].nunique():,} combos\")\n",
    "print(f\"   - Size×Style: {train['size_style_combo'].nunique():,} combos\")\n",
    "print(f\"   - High variance in target within combos = predictive signal\")\n",
    "\n",
    "print(\"\\n4. TARGET ENCODING SHOULD BE STABLE:\")\n",
    "print(\"   - Large dataset (4M rows) supports stable target statistics\")\n",
    "print(\"   - Many categories have sufficient samples\")\n",
    "\n",
    "print(\"\\n5. ORIGINAL DATASET:\")\n",
    "print(\"   - Competition explicitly mentions it can help\")\n",
    "print(\"   - Need to investigate availability or create proxy\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
