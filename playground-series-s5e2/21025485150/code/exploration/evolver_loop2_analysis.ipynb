{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ce3458",
   "metadata": {},
   "source": [
    "# Evolver Loop 2: Deep Dive into Feature Engineering Opportunities\n",
    "\n",
    "Based on evaluator feedback and initial analysis, this notebook explores:\n",
    "1. Advanced Weight Capacity feature engineering (histogram binning, quantile bins, interactions)\n",
    "2. Target encoding strategies for categorical features\n",
    "3. Interaction feature creation and validation\n",
    "4. Count encoding and other proven techniques from winning solutions\n",
    "\n",
    "Goal: Identify specific feature engineering approaches to close the 0.165 RMSE gap to target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da717e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "train_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "# Combine train and extra as done in baseline\n",
    "combined_train = pd.concat([train, train_extra], ignore_index=True)\n",
    "print(f\"Combined training data: {combined_train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Weight Capacity patterns in detail\n",
    "print(\"=\"*60)\n",
    "print(\"WEIGHT CAPACITY DEEP DIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weight_col = 'Weight Capacity (kg)'\n",
    "print(f\"\\nBasic stats:\")\n",
    "print(combined_train[weight_col].describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = combined_train[weight_col].isna().sum()\n",
    "print(f\"\\nMissing values: {missing_count:,} ({missing_count/len(combined_train)*100:.2f}%)\")\n",
    "\n",
    "# Check distribution (excluding missing)\n",
    "weight_non_missing = combined_train[weight_col].dropna()\n",
    "print(f\"\\nDistribution (non-missing):\")\n",
    "print(f\"  Zeros: {(weight_non_missing == 0).sum():,} ({(weight_non_missing == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Negative: {(weight_non_missing < 0).sum():,} ({(weight_non_missing < 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Positive: {(weight_non_missing > 0).sum():,} ({(weight_non_missing > 0).mean()*100:.2f}%)\")\n",
    "\n",
    "# Look at relationship with target (using rows with non-missing weight)\n",
    "valid_rows = combined_train[weight_col].notna()\n",
    "print(f\"\\nCorrelation with Price (valid rows): {combined_train.loc[valid_rows, weight_col].corr(combined_train.loc[valid_rows, 'Price']):.4f}\")\n",
    "\n",
    "# Check if there are patterns in the decimal places\n",
    "combined_train['weight_decimal'] = (combined_train[weight_col] % 1).round(10)\n",
    "decimal_stats = combined_train.groupby('weight_decimal')['Price'].agg(['mean', 'std', 'count']).sort_values('count', ascending=False)\n",
    "print(f\"\\nTop 10 most common decimal values:\")\n",
    "print(decimal_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different binning strategies for Weight Capacity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING WEIGHT CAPACITY BINNING STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use only non-missing values for binning analysis\n",
    "weight_for_binning = combined_train[combined_train[weight_col].notna()].copy()\n",
    "\n",
    "# Remove zeros for binning analysis (they're a special case)\n",
    "weight_nonzero = weight_for_binning[weight_for_binning[weight_col] != 0].copy()\n",
    "\n",
    "print(f\"Using {len(weight_nonzero):,} non-zero, non-missing rows for binning analysis\")\n",
    "\n",
    "binning_results = []\n",
    "\n",
    "# Test different numbers of bins\n",
    "for n_bins in [5, 8, 10, 15, 20, 30, 50]:\n",
    "    # Equal-width binning\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    weight_nonzero[f'weight_bin_uniform_{n_bins}'] = discretizer.fit_transform(weight_nonzero[[weight_col]]).flatten()\n",
    "    \n",
    "    # Calculate target variance within bins\n",
    "    bin_stats = weight_nonzero.groupby(f'weight_bin_uniform_{n_bins}')['Price'].agg(['mean', 'std', 'count'])\n",
    "    binning_results.append({\n",
    "        'strategy': 'uniform',\n",
    "        'n_bins': n_bins,\n",
    "        'avg_bin_std': bin_stats['std'].mean(),\n",
    "        'min_bin_count': bin_stats['count'].min(),\n",
    "        'price_range': bin_stats['mean'].max() - bin_stats['mean'].min()\n",
    "    })\n",
    "    \n",
    "    # Quantile binning\n",
    "    discretizer_q = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    weight_nonzero[f'weight_bin_quantile_{n_bins}'] = discretizer_q.fit_transform(weight_nonzero[[weight_col]]).flatten()\n",
    "    \n",
    "    bin_stats_q = weight_nonzero.groupby(f'weight_bin_quantile_{n_bins}')['Price'].agg(['mean', 'std', 'count'])\n",
    "    binning_results.append({\n",
    "        'strategy': 'quantile',\n",
    "        'n_bins': n_bins,\n",
    "        'avg_bin_std': bin_stats_q['std'].mean(),\n",
    "        'min_bin_count': bin_stats_q['count'].min(),\n",
    "        'price_range': bin_stats_q['mean'].max() - bin_stats_q['mean'].min()\n",
    "    })\n",
    "\n",
    "binning_df = pd.DataFrame(binning_results)\n",
    "print(\"\\nBinning strategy comparison:\")\n",
    "print(binning_df.sort_values('price_range', ascending=False).head(10))\n",
    "\n",
    "# Best strategies based on price range (signal) and min bin count (stability)\n",
    "best_uniform = binning_df[binning_df['strategy'] == 'uniform'].sort_values('price_range', ascending=False).iloc[0]\n",
    "best_quantile = binning_df[binning_df['strategy'] == 'quantile'].sort_values('price_range', ascending=False).iloc[0]\n",
    "\n",
    "print(f\"\\nBest uniform binning: {best_uniform['n_bins']} bins (price range: {best_uniform['price_range']:.2f})\")\n",
    "print(f\"Best quantile binning: {best_quantile['n_bins']} bins (price range: {best_quantile['price_range']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features for target encoding potential\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATEGORICAL FEATURE ANALYSIS FOR TARGET ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cat_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', \n",
    "                'Waterproof', 'Style', 'Color']\n",
    "\n",
    "target_encoding_analysis = []\n",
    "\n",
    "for feature in cat_features:\n",
    "    # Basic stats\n",
    "    n_unique = combined_train[feature].nunique()\n",
    "    value_counts = combined_train[feature].value_counts()\n",
    "    \n",
    "    # Calculate target statistics\n",
    "    target_stats = combined_train.groupby(feature)['Price'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    # Check stability (correlation between two halves)\n",
    "    half1, half2 = combined_train.sample(frac=0.5, random_state=42), combined_train.sample(frac=0.5, random_state=43)\n",
    "    means1 = half1.groupby(feature)['Price'].mean()\n",
    "    means2 = half2.groupby(feature)['Price'].mean()\n",
    "    common_cats = means1.index.intersection(means2.index)\n",
    "    \n",
    "    if len(common_cats) > 5:\n",
    "        stability_corr = means1[common_cats].corr(means2[common_cats])\n",
    "    else:\n",
    "        stability_corr = np.nan\n",
    "    \n",
    "    # Categories with sufficient samples (>100)\n",
    "    sufficient_cats = (target_stats['count'] >= 100).sum()\n",
    "    \n",
    "    target_encoding_analysis.append({\n",
    "        'feature': feature,\n",
    "        'n_unique': n_unique,\n",
    "        'sufficient_cats': sufficient_cats,\n",
    "        'stability_corr': stability_corr,\n",
    "        'price_range': target_stats['mean'].max() - target_stats['mean'].min(),\n",
    "        'max_count': value_counts.iloc[0],\n",
    "        'min_count': value_counts.iloc[-1]\n",
    "    })\n",
    "\n",
    "te_df = pd.DataFrame(target_encoding_analysis)\n",
    "print(\"\\nTarget encoding potential:\")\n",
    "print(te_df.sort_values('price_range', ascending=False))\n",
    "\n",
    "# Features with highest encoding potential\n",
    "best_for_te = te_df.nlargest(3, 'price_range')['feature'].tolist()\n",
    "print(f\"\\nTop 3 features for target encoding: {best_for_te}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test interaction features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTION FEATURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create interaction combinations\n",
    "interactions = [\n",
    "    ('Brand', 'Material'),\n",
    "    ('Size', 'Style'),\n",
    "    ('Brand', 'Size'),\n",
    "    ('Material', 'Style'),\n",
    "    ('Brand', 'Color'),\n",
    "    ('Size', 'Color')\n",
    "]\n",
    "\n",
    "interaction_results = []\n",
    "\n",
    "for feat1, feat2 in interactions:\n",
    "    # Create interaction\n",
    "    interaction_name = f\"{feat1}_{feat2}\"\n",
    "    combined_train[interaction_name] = combined_train[feat1].astype(str) + '_' + combined_train[feat2].astype(str)\n",
    "    \n",
    "    # Stats\n",
    "    n_unique = combined_train[interaction_name].nunique()\n",
    "    target_stats = combined_train.groupby(interaction_name)['Price'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    # Filter for combinations with sufficient data\n",
    "    sufficient_combos = target_stats[target_stats['count'] >= 50]\n",
    "    \n",
    "    if len(sufficient_combos) > 5:\n",
    "        price_range = sufficient_combos['mean'].max() - sufficient_combos['mean'].min()\n",
    "        avg_std = sufficient_combos['std'].mean()\n",
    "        \n",
    "        interaction_results.append({\n",
    "            'interaction': interaction_name,\n",
    "            'n_unique': n_unique,\n",
    "            'sufficient_combos': len(sufficient_combos),\n",
    "            'price_range': price_range,\n",
    "            'avg_std': avg_std,\n",
    "            'signal_score': price_range / avg_std if avg_std > 0 else 0\n",
    "        })\n",
    "\n",
    "interaction_df = pd.DataFrame(interaction_results)\n",
    "print(\"\\nInteraction feature analysis:\")\n",
    "print(interaction_df.sort_values('signal_score', ascending=False))\n",
    "\n",
    "# Best interactions\n",
    "best_interactions = interaction_df.nlargest(3, 'signal_score')['interaction'].tolist()\n",
    "print(f\"\\nTop 3 interactions: {best_interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05eecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test count encoding potential\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COUNT ENCODING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count encoding can capture popularity/rarity which might correlate with price\n",
    "for feature in cat_features:\n",
    "    # Calculate counts\n",
    "    counts = combined_train[feature].value_counts()\n",
    "    combined_train[f'{feature}_count'] = combined_train[feature].map(counts)\n",
    "    \n",
    "    # Correlation with target\n",
    "    corr_with_target = combined_train[f'{feature}_count'].corr(combined_train['Price'])\n",
    "    \n",
    "    # Unique count values\n",
    "    n_unique_counts = combined_train[f'{feature}_count'].nunique()\n",
    "    \n",
    "    print(f\"{feature:20s}: corr={corr_with_target:6.4f}, unique_counts={n_unique_counts:4d}\")\n",
    "\n",
    "# Count encoding looks promising for features with reasonable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e079eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings for next experiment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR EXPERIMENT DESIGN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. WEIGHT CAPACITY FEATURE ENGINEERING:\")\n",
    "print(f\"   - Best uniform binning: {best_uniform['n_bins']} bins\")\n",
    "print(f\"   - Best quantile binning: {best_quantile['n_bins']} bins\")\n",
    "print(f\"   - Decimal patterns exist and should be captured\")\n",
    "print(f\"   - Zero and negative values need special handling\")\n",
    "\n",
    "print(\"\\n2. TARGET ENCODING PRIORITY:\")\n",
    "for i, feat in enumerate(best_for_te, 1):\n",
    "    price_range = te_df[te_df['feature'] == feat]['price_range'].iloc[0]\n",
    "    print(f\"   {i}. {feat}: price range {price_range:.2f}\")\n",
    "\n",
    "print(\"\\n3. INTERACTION FEATURES:\")\n",
    "for i, interaction in enumerate(best_interactions, 1):\n",
    "    signal_score = interaction_df[interaction_df['interaction'] == interaction]['signal_score'].iloc[0]\n",
    "    print(f\"   {i}. {interaction}: signal score {signal_score:.2f}\")\n",
    "\n",
    "print(\"\\n4. COUNT ENCODING:\")\n",
    "print(\"   - Should be applied to all categorical features\")\n",
    "print(\"   - Captures popularity/rarity patterns\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Implement k-fold target encoding with smoothing\")\n",
    "print(\"   - Create interaction features with proper cross-validation\")\n",
    "print(\"   - Add histogram and quantile bins for Weight Capacity\")\n",
    "print(\"   - Include count encoding for all categoricals\")\n",
    "print(\"   - Tune hyperparameters (lower learning rate, more trees)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
