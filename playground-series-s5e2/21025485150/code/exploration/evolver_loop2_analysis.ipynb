{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ce3458",
   "metadata": {},
   "source": [
    "# Evolver Loop 2: Deep Dive into Feature Engineering Opportunities\n",
    "\n",
    "Based on evaluator feedback and initial analysis, this notebook explores:\n",
    "1. Advanced Weight Capacity feature engineering (histogram binning, quantile bins, interactions)\n",
    "2. Target encoding strategies for categorical features\n",
    "3. Interaction feature creation and validation\n",
    "4. Count encoding and other proven techniques from winning solutions\n",
    "\n",
    "Goal: Identify specific feature engineering approaches to close the 0.165 RMSE gap to target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da717e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:58:57.152047Z",
     "iopub.status.busy": "2026-01-15T10:58:57.151807Z",
     "iopub.status.idle": "2026-01-15T10:59:01.312397Z",
     "shell.execute_reply": "2026-01-15T10:59:01.311883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training data: (3994318, 11)\n",
      "Test data: (200000, 10)\n",
      "\n",
      "Target statistics:\n",
      "count    3.994318e+06\n",
      "mean     8.136217e+01\n",
      "std      3.893868e+01\n",
      "min      1.500000e+01\n",
      "25%      4.747002e+01\n",
      "50%      8.098495e+01\n",
      "75%      1.148550e+02\n",
      "max      1.500000e+02\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "train_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "# Combine train and extra as done in baseline\n",
    "combined_train = pd.concat([train, train_extra], ignore_index=True)\n",
    "print(f\"Combined training data: {combined_train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c47118f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:59:01.315342Z",
     "iopub.status.busy": "2026-01-15T10:59:01.315123Z",
     "iopub.status.idle": "2026-01-15T10:59:02.621918Z",
     "shell.execute_reply": "2026-01-15T10:59:02.621484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WEIGHT CAPACITY DEEP DIVE\n",
      "============================================================\n",
      "\n",
      "Basic stats:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.992510e+06\n",
      "mean     1.801042e+01\n",
      "std      6.973969e+00\n",
      "min      5.000000e+00\n",
      "25%      1.206896e+01\n",
      "50%      1.805436e+01\n",
      "75%      2.398751e+01\n",
      "max      3.000000e+01\n",
      "Name: Weight Capacity (kg), dtype: float64\n",
      "\n",
      "Missing values: 1,808 (0.05%)\n",
      "\n",
      "Distribution (non-missing):\n",
      "  Zeros: 0 (0.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Negative: 0 (0.00%)\n",
      "  Positive: 3,992,510 (100.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation with Price (valid rows): 0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most common decimal values:\n",
      "                     mean        std  count\n",
      "weight_decimal                             \n",
      "0.000000        77.798619  39.576711  60677\n",
      "0.898250        84.355394  40.679164   1677\n",
      "0.908437        78.151331  38.040513   1562\n",
      "0.898382        77.630059  38.767438   1433\n",
      "0.824082        83.088466  37.932090   1270\n",
      "0.817374        85.946458  37.501260   1165\n",
      "0.837673        78.936614  39.913323   1124\n",
      "0.829310        82.682448  40.529524   1057\n",
      "0.879323        84.342518  38.987022   1012\n",
      "0.814070        80.631850  41.312545    995\n"
     ]
    }
   ],
   "source": [
    "# Analyze Weight Capacity patterns in detail\n",
    "print(\"=\"*60)\n",
    "print(\"WEIGHT CAPACITY DEEP DIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weight_col = 'Weight Capacity (kg)'\n",
    "print(f\"\\nBasic stats:\")\n",
    "print(combined_train[weight_col].describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = combined_train[weight_col].isna().sum()\n",
    "print(f\"\\nMissing values: {missing_count:,} ({missing_count/len(combined_train)*100:.2f}%)\")\n",
    "\n",
    "# Check distribution (excluding missing)\n",
    "weight_non_missing = combined_train[weight_col].dropna()\n",
    "print(f\"\\nDistribution (non-missing):\")\n",
    "print(f\"  Zeros: {(weight_non_missing == 0).sum():,} ({(weight_non_missing == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Negative: {(weight_non_missing < 0).sum():,} ({(weight_non_missing < 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Positive: {(weight_non_missing > 0).sum():,} ({(weight_non_missing > 0).mean()*100:.2f}%)\")\n",
    "\n",
    "# Look at relationship with target (using rows with non-missing weight)\n",
    "valid_rows = combined_train[weight_col].notna()\n",
    "print(f\"\\nCorrelation with Price (valid rows): {combined_train.loc[valid_rows, weight_col].corr(combined_train.loc[valid_rows, 'Price']):.4f}\")\n",
    "\n",
    "# Check if there are patterns in the decimal places\n",
    "combined_train['weight_decimal'] = (combined_train[weight_col] % 1).round(10)\n",
    "decimal_stats = combined_train.groupby('weight_decimal')['Price'].agg(['mean', 'std', 'count']).sort_values('count', ascending=False)\n",
    "print(f\"\\nTop 10 most common decimal values:\")\n",
    "print(decimal_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5a2e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:59:02.623060Z",
     "iopub.status.busy": "2026-01-15T10:59:02.622894Z",
     "iopub.status.idle": "2026-01-15T10:59:09.309764Z",
     "shell.execute_reply": "2026-01-15T10:59:09.309215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING WEIGHT CAPACITY BINNING STRATEGIES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3,992,510 non-zero, non-missing rows for binning analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binning strategy comparison:\n",
      "    strategy  n_bins  avg_bin_std  min_bin_count  price_range\n",
      "12   uniform      50    38.907152          14607     8.848311\n",
      "13  quantile      50    38.913814          76503     5.998738\n",
      "10   uniform      30    38.903957          80724     5.704248\n",
      "8    uniform      20    38.907561         115443     5.241227\n",
      "11  quantile      30    38.915683         129316     5.232910\n",
      "9   quantile      20    38.919904         195303     4.266600\n",
      "6    uniform      15    38.910155         161557     4.259689\n",
      "7   quantile      15    38.920063         263684     3.746829\n",
      "4    uniform      10    38.910452         291492     3.464889\n",
      "2    uniform       8    38.913851         395057     3.187601\n",
      "\n",
      "Best uniform binning: 50 bins (price range: 8.85)\n",
      "Best quantile binning: 50 bins (price range: 6.00)\n"
     ]
    }
   ],
   "source": [
    "# Test different binning strategies for Weight Capacity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING WEIGHT CAPACITY BINNING STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use only non-missing values for binning analysis\n",
    "weight_for_binning = combined_train[combined_train[weight_col].notna()].copy()\n",
    "\n",
    "# Remove zeros for binning analysis (they're a special case)\n",
    "weight_nonzero = weight_for_binning[weight_for_binning[weight_col] != 0].copy()\n",
    "\n",
    "print(f\"Using {len(weight_nonzero):,} non-zero, non-missing rows for binning analysis\")\n",
    "\n",
    "binning_results = []\n",
    "\n",
    "# Test different numbers of bins\n",
    "for n_bins in [5, 8, 10, 15, 20, 30, 50]:\n",
    "    # Equal-width binning\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    weight_nonzero[f'weight_bin_uniform_{n_bins}'] = discretizer.fit_transform(weight_nonzero[[weight_col]]).flatten()\n",
    "    \n",
    "    # Calculate target variance within bins\n",
    "    bin_stats = weight_nonzero.groupby(f'weight_bin_uniform_{n_bins}')['Price'].agg(['mean', 'std', 'count'])\n",
    "    binning_results.append({\n",
    "        'strategy': 'uniform',\n",
    "        'n_bins': n_bins,\n",
    "        'avg_bin_std': bin_stats['std'].mean(),\n",
    "        'min_bin_count': bin_stats['count'].min(),\n",
    "        'price_range': bin_stats['mean'].max() - bin_stats['mean'].min()\n",
    "    })\n",
    "    \n",
    "    # Quantile binning\n",
    "    discretizer_q = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    weight_nonzero[f'weight_bin_quantile_{n_bins}'] = discretizer_q.fit_transform(weight_nonzero[[weight_col]]).flatten()\n",
    "    \n",
    "    bin_stats_q = weight_nonzero.groupby(f'weight_bin_quantile_{n_bins}')['Price'].agg(['mean', 'std', 'count'])\n",
    "    binning_results.append({\n",
    "        'strategy': 'quantile',\n",
    "        'n_bins': n_bins,\n",
    "        'avg_bin_std': bin_stats_q['std'].mean(),\n",
    "        'min_bin_count': bin_stats_q['count'].min(),\n",
    "        'price_range': bin_stats_q['mean'].max() - bin_stats_q['mean'].min()\n",
    "    })\n",
    "\n",
    "binning_df = pd.DataFrame(binning_results)\n",
    "print(\"\\nBinning strategy comparison:\")\n",
    "print(binning_df.sort_values('price_range', ascending=False).head(10))\n",
    "\n",
    "# Best strategies based on price range (signal) and min bin count (stability)\n",
    "best_uniform = binning_df[binning_df['strategy'] == 'uniform'].sort_values('price_range', ascending=False).iloc[0]\n",
    "best_quantile = binning_df[binning_df['strategy'] == 'quantile'].sort_values('price_range', ascending=False).iloc[0]\n",
    "\n",
    "print(f\"\\nBest uniform binning: {best_uniform['n_bins']} bins (price range: {best_uniform['price_range']:.2f})\")\n",
    "print(f\"Best quantile binning: {best_quantile['n_bins']} bins (price range: {best_quantile['price_range']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8c15f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:59:09.311160Z",
     "iopub.status.busy": "2026-01-15T10:59:09.310976Z",
     "iopub.status.idle": "2026-01-15T10:59:20.755997Z",
     "shell.execute_reply": "2026-01-15T10:59:20.755440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CATEGORICAL FEATURE ANALYSIS FOR TARGET ENCODING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target encoding potential:\n",
      "              feature  n_unique  sufficient_cats  stability_corr  price_range  \\\n",
      "6               Color         6                6         0.99631     1.926271   \n",
      "1            Material         4                4             NaN     1.540674   \n",
      "0               Brand         5                5             NaN     1.537524   \n",
      "2                Size         3                3             NaN     0.410370   \n",
      "5               Style         3                3             NaN     0.314476   \n",
      "3  Laptop Compartment         2                2             NaN     0.069704   \n",
      "4          Waterproof         2                2             NaN     0.027099   \n",
      "\n",
      "   max_count  min_count  \n",
      "6     688257     617024  \n",
      "1    1060882     903632  \n",
      "0     801035     749340  \n",
      "2    1354487    1239751  \n",
      "5    1329677    1262519  \n",
      "3    1972937    1922848  \n",
      "4    1969205    1930789  \n",
      "\n",
      "Top 3 features for target encoding: ['Color', 'Material', 'Brand']\n"
     ]
    }
   ],
   "source": [
    "# Analyze categorical features for target encoding potential\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATEGORICAL FEATURE ANALYSIS FOR TARGET ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cat_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', \n",
    "                'Waterproof', 'Style', 'Color']\n",
    "\n",
    "target_encoding_analysis = []\n",
    "\n",
    "for feature in cat_features:\n",
    "    # Basic stats\n",
    "    n_unique = combined_train[feature].nunique()\n",
    "    value_counts = combined_train[feature].value_counts()\n",
    "    \n",
    "    # Calculate target statistics\n",
    "    target_stats = combined_train.groupby(feature)['Price'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    # Check stability (correlation between two halves)\n",
    "    half1, half2 = combined_train.sample(frac=0.5, random_state=42), combined_train.sample(frac=0.5, random_state=43)\n",
    "    means1 = half1.groupby(feature)['Price'].mean()\n",
    "    means2 = half2.groupby(feature)['Price'].mean()\n",
    "    common_cats = means1.index.intersection(means2.index)\n",
    "    \n",
    "    if len(common_cats) > 5:\n",
    "        stability_corr = means1[common_cats].corr(means2[common_cats])\n",
    "    else:\n",
    "        stability_corr = np.nan\n",
    "    \n",
    "    # Categories with sufficient samples (>100)\n",
    "    sufficient_cats = (target_stats['count'] >= 100).sum()\n",
    "    \n",
    "    target_encoding_analysis.append({\n",
    "        'feature': feature,\n",
    "        'n_unique': n_unique,\n",
    "        'sufficient_cats': sufficient_cats,\n",
    "        'stability_corr': stability_corr,\n",
    "        'price_range': target_stats['mean'].max() - target_stats['mean'].min(),\n",
    "        'max_count': value_counts.iloc[0],\n",
    "        'min_count': value_counts.iloc[-1]\n",
    "    })\n",
    "\n",
    "te_df = pd.DataFrame(target_encoding_analysis)\n",
    "print(\"\\nTarget encoding potential:\")\n",
    "print(te_df.sort_values('price_range', ascending=False))\n",
    "\n",
    "# Features with highest encoding potential\n",
    "best_for_te = te_df.nlargest(3, 'price_range')['feature'].tolist()\n",
    "print(f\"\\nTop 3 features for target encoding: {best_for_te}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc86ac7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:59:20.757788Z",
     "iopub.status.busy": "2026-01-15T10:59:20.757601Z",
     "iopub.status.idle": "2026-01-15T10:59:26.619662Z",
     "shell.execute_reply": "2026-01-15T10:59:26.619238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTERACTION FEATURE ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interaction feature analysis:\n",
      "      interaction  n_unique  sufficient_combos  price_range    avg_std  \\\n",
      "2      Brand_Size        24                 24    19.350779  39.453170   \n",
      "5      Size_Color        28                 28     6.574969  39.234953   \n",
      "1      Size_Style        16                 16     4.610007  39.762607   \n",
      "3  Material_Style        20                 20     4.551122  39.348497   \n",
      "4     Brand_Color        42                 42     3.402197  39.054657   \n",
      "0  Brand_Material        30                 30     3.172758  39.130819   \n",
      "\n",
      "   signal_score  \n",
      "2      0.490475  \n",
      "5      0.167579  \n",
      "1      0.115938  \n",
      "3      0.115662  \n",
      "4      0.087114  \n",
      "0      0.081081  \n",
      "\n",
      "Top 3 interactions: ['Brand_Size', 'Size_Color', 'Size_Style']\n"
     ]
    }
   ],
   "source": [
    "# Test interaction features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERACTION FEATURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create interaction combinations\n",
    "interactions = [\n",
    "    ('Brand', 'Material'),\n",
    "    ('Size', 'Style'),\n",
    "    ('Brand', 'Size'),\n",
    "    ('Material', 'Style'),\n",
    "    ('Brand', 'Color'),\n",
    "    ('Size', 'Color')\n",
    "]\n",
    "\n",
    "interaction_results = []\n",
    "\n",
    "for feat1, feat2 in interactions:\n",
    "    # Create interaction\n",
    "    interaction_name = f\"{feat1}_{feat2}\"\n",
    "    combined_train[interaction_name] = combined_train[feat1].astype(str) + '_' + combined_train[feat2].astype(str)\n",
    "    \n",
    "    # Stats\n",
    "    n_unique = combined_train[interaction_name].nunique()\n",
    "    target_stats = combined_train.groupby(interaction_name)['Price'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    # Filter for combinations with sufficient data\n",
    "    sufficient_combos = target_stats[target_stats['count'] >= 50]\n",
    "    \n",
    "    if len(sufficient_combos) > 5:\n",
    "        price_range = sufficient_combos['mean'].max() - sufficient_combos['mean'].min()\n",
    "        avg_std = sufficient_combos['std'].mean()\n",
    "        \n",
    "        interaction_results.append({\n",
    "            'interaction': interaction_name,\n",
    "            'n_unique': n_unique,\n",
    "            'sufficient_combos': len(sufficient_combos),\n",
    "            'price_range': price_range,\n",
    "            'avg_std': avg_std,\n",
    "            'signal_score': price_range / avg_std if avg_std > 0 else 0\n",
    "        })\n",
    "\n",
    "interaction_df = pd.DataFrame(interaction_results)\n",
    "print(\"\\nInteraction feature analysis:\")\n",
    "print(interaction_df.sort_values('signal_score', ascending=False))\n",
    "\n",
    "# Best interactions\n",
    "best_interactions = interaction_df.nlargest(3, 'signal_score')['interaction'].tolist()\n",
    "print(f\"\\nTop 3 interactions: {best_interactions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05eecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test count encoding potential\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COUNT ENCODING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count encoding can capture popularity/rarity which might correlate with price\n",
    "for feature in cat_features:\n",
    "    # Calculate counts\n",
    "    counts = combined_train[feature].value_counts()\n",
    "    combined_train[f'{feature}_count'] = combined_train[feature].map(counts)\n",
    "    \n",
    "    # Correlation with target\n",
    "    corr_with_target = combined_train[f'{feature}_count'].corr(combined_train['Price'])\n",
    "    \n",
    "    # Unique count values\n",
    "    n_unique_counts = combined_train[f'{feature}_count'].nunique()\n",
    "    \n",
    "    print(f\"{feature:20s}: corr={corr_with_target:6.4f}, unique_counts={n_unique_counts:4d}\")\n",
    "\n",
    "# Count encoding looks promising for features with reasonable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e079eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings for next experiment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR EXPERIMENT DESIGN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. WEIGHT CAPACITY FEATURE ENGINEERING:\")\n",
    "print(f\"   - Best uniform binning: {best_uniform['n_bins']} bins\")\n",
    "print(f\"   - Best quantile binning: {best_quantile['n_bins']} bins\")\n",
    "print(f\"   - Decimal patterns exist and should be captured\")\n",
    "print(f\"   - Zero and negative values need special handling\")\n",
    "\n",
    "print(\"\\n2. TARGET ENCODING PRIORITY:\")\n",
    "for i, feat in enumerate(best_for_te, 1):\n",
    "    price_range = te_df[te_df['feature'] == feat]['price_range'].iloc[0]\n",
    "    print(f\"   {i}. {feat}: price range {price_range:.2f}\")\n",
    "\n",
    "print(\"\\n3. INTERACTION FEATURES:\")\n",
    "for i, interaction in enumerate(best_interactions, 1):\n",
    "    signal_score = interaction_df[interaction_df['interaction'] == interaction]['signal_score'].iloc[0]\n",
    "    print(f\"   {i}. {interaction}: signal score {signal_score:.2f}\")\n",
    "\n",
    "print(\"\\n4. COUNT ENCODING:\")\n",
    "print(\"   - Should be applied to all categorical features\")\n",
    "print(\"   - Captures popularity/rarity patterns\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Implement k-fold target encoding with smoothing\")\n",
    "print(\"   - Create interaction features with proper cross-validation\")\n",
    "print(\"   - Add histogram and quantile bins for Weight Capacity\")\n",
    "print(\"   - Include count encoding for all categoricals\")\n",
    "print(\"   - Tune hyperparameters (lower learning rate, more trees)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
