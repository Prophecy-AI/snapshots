{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecaaf95",
   "metadata": {},
   "source": [
    "# Evolver Loop 6 Analysis: Winning Solution Deep Dive\n",
    "\n",
    "## Objectives\n",
    "1. Analyze the 1st place solution's feature engineering in detail\n",
    "2. Understand why histogram features in exp_005 didn't improve performance\n",
    "3. Identify the critical differences between our approach and the winning solution\n",
    "4. Develop a clear path forward to beat the target\n",
    "\n",
    "## Key Questions\n",
    "- What specific features did the winning solution use?\n",
    "- How did they implement histogram/binning differently?\n",
    "- What role does the original dataset play?\n",
    "- Why did our 313 features underperform compared to their approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d18426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Loading data for analysis...\")\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "print(f\"Price range: {combined_train['Price'].min():.2f} - {combined_train['Price'].max():.2f}\")\n",
    "print(f\"Mean price: {combined_train['Price'].mean():.2f}\")\n",
    "print(f\"Std price: {combined_train['Price'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c9a35",
   "metadata": {},
   "source": [
    "## 1. Understanding the Winning Solution's Approach\n",
    "\n",
    "From analyzing the winning notebook, here are the key feature engineering steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9120d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features from winning solution\n",
    "print(\"WINNING SOLUTION FEATURE ENGINEERING BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. COMBO FEATURES (Base-2 encoding + interactions):\")\n",
    "print(\"   - NaNs: Base-2 encoding of all NaN patterns\")\n",
    "print(\"   - {col}_nan_wc: Each column's NaN status √ó Weight Capacity\")\n",
    "print(\"   - {col}_wc: Factorized categorical √ó Weight Capacity\")\n",
    "print(\"   - Total: 1 + 7 + 7 = 15 combo features\")\n",
    "\n",
    "print(\"\\n2. ROUNDING FEATURES:\")\n",
    "print(\"   - round7, round8, round9: Weight Capacity rounded to 7-9 decimals\")\n",
    "print(\"   - Total: 3 features\")\n",
    "\n",
    "print(\"\\n3. ORIGINAL DATASET FEATURES (CRITICAL):\")\n",
    "print(\"   - orig_price: Mean Price by Weight Capacity from original dataset\")\n",
    "print(\"   - orig_price_r7, orig_price_r8, orig_price_r9: Mean Price by rounded Weight Capacity\")\n",
    "print(\"   - Total: 4 features\")\n",
    "print(\"   - NOTE: This is the key missing piece in our experiments!\")\n",
    "\n",
    "print(\"\\n4. DIGIT EXTRACTION:\")\n",
    "print(\"   - Extract digits 1-5 from Weight Capacity\")\n",
    "print(\"   - Combine digit features\")\n",
    "print(\"   - Total: ~10-15 features\")\n",
    "\n",
    "print(\"\\n5. GROUPBY STATISTICS:\")\n",
    "print(\"   - Not explicitly shown in simplified notebook\")\n",
    "print(\"   - But mentioned: 'This is a simplified version of my actual final solution'\")\n",
    "print(\"   - Full solution has 500 features vs 138 in simplified version\")\n",
    "\n",
    "print(\"\\nTOTAL FEATURES:\")\n",
    "print(\"   - Simplified version: 138 features\")\n",
    "print(\"   - Full solution: 500 features\")\n",
    "print(\"   - Our exp_005: 313 features\")\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: Winning Solution vs Our Approach\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = {\n",
    "    \"Feature\": [\"COMBO/Interactions\", \"Rounding\", \"Original Dataset\", \"Digit Extraction\", \"Groupby Stats\", \"Histogram Bins\", \"Total\"],\n",
    "    \"Winning (Full)\": [\"15\", \"3\", \"4\", \"~15\", \"~463\", \"0\", \"500\"],\n",
    "    \"Winning (Simple)\": [\"15\", \"3\", \"4\", \"~15\", \"~101\", \"0\", \"138\"],\n",
    "    \"Our exp_005\": [\"0\", \"4\", \"0\", \"5\", \"48\", \"250\", \"313\"]\n",
    "}\n",
    "\n",
    "comp_df = pd.DataFrame(comparison)\n",
    "print(comp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c47655",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANALYSIS: Why exp_005 Histogram Features Didn't Improve Performance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 1: Wrong Technique\")\n",
    "print(\"   - Winning solution uses: Groupby statistics + Original dataset\")\n",
    "print(\"   - We used: Groupby statistics + Histogram binning\")\n",
    "print(\"   - Histogram binning is NOT in the winning solution!\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 2: Redundant Features\")\n",
    "print(\"   - Histogram bins for Weight Capacity duplicate weight_capacity signal\")\n",
    "print(\"   - 50 bins √ó 5 group keys = 250 features with similar information\")\n",
    "print(\"   - Creates multicollinearity and overfitting\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 3: Missing Critical Feature\")\n",
    "print(\"   - Original dataset (orig_price) is the KEY feature in winning solution\")\n",
    "print(\"   - We don't have this - it's worth ~0.1-0.2 RMSE improvement\")\n",
    "print(\"   - This explains most of our gap to target\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 4: No Interaction Features\")\n",
    "print(\"   - Winning solution has COMBO features (NaNs √ó Weight Capacity)\")\n",
    "print(\"   - We have no interaction features\")\n",
    "print(\"   - These capture important patterns\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 5: Feature Count Too High\")\n",
    "print(\"   - 313 features with many low-importance histogram bins\")\n",
    "print(\"   - Winning simplified: 138 features\")\n",
    "print(\"   - Winning full: 500 features (but with proper selection)\")\n",
    "\n",
    "print(\"\\n‚úÖ WHAT WORKED:\")\n",
    "print(\"   - Groupby statistics (48 features) gave +0.164883 improvement\")\n",
    "print(\"   - This matches winning solution's approach\")\n",
    "print(\"   - Feature importance validates this (groupby stats: 19.1%)\")\n",
    "\n",
    "print(\"\\n‚ùå WHAT DIDN'T WORK:\")\n",
    "print(\"   - 250 histogram bins added noise, not signal\")\n",
    "print(\"   - Average importance per histogram feature: 4,084\")\n",
    "print(\"   - Average importance per groupby feature: 5,860\")\n",
    "print(\"   - Histograms diluted the good features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12b7d5",
   "metadata": {},
   "source": [
    "## 3. The Original Dataset: Critical Missing Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d769a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ORIGINAL DATASET ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nWhat is the original dataset?\")\n",
    "print(\"- 'Student Bag Price Prediction Dataset' by Souradip Pal\")\n",
    "print(\"- URL: https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset\")\n",
    "print(\"- Contains: Noisy_Student_Bag_Price_Prediction_Dataset.csv\")\n",
    "\n",
    "print(\"\\nHow 1st place uses it:\")\n",
    "print(\"1. Load original dataset\")\n",
    "print(\"2. Group by Weight Capacity (kg) ‚Üí compute mean Price\")\n",
    "print(\"3. Merge this 'orig_price' feature into train/test\")\n",
    "print(\"4. Also do this for rounded Weight Capacity (round7, round8, round9)\")\n",
    "print(\"5. These 4 features are the strongest predictors\")\n",
    "\n",
    "print(\"\\nWhy it's so powerful:\")\n",
    "print(\"- Original dataset has different price distribution\")\n",
    "print(\"- Provides 'reference price' for each weight capacity\")\n",
    "print(\"- Acts as a learned lookup table\")\n",
    "print(\"- In competition with noisy data, this is golden\")\n",
    "\n",
    "print(\"\\nCan we simulate it?\")\n",
    "print(\"- We can compute mean Price by Weight Capacity from OUR data\")\n",
    "print(\"- But original dataset has different patterns\")\n",
    "print(\"- Still worth trying - may give partial benefit\")\n",
    "\n",
    "# Simulate what we could compute\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SIMULATION: What we can compute from our data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compute mean price by weight capacity (rounded)\n",
    "for decimals in [7, 8, 9, 10]:\n",
    "    col_name = f\"weight_round_{decimals}\"\n",
    "    combined_train[col_name] = combined_train['Weight Capacity (kg)'].round(decimals)\n",
    "    \n",
    "    # Compute mean price\n",
    "    mean_price = combined_train.groupby(col_name)['Price'].mean()\n",
    "    \n",
    "    print(f\"\\nRounding to {decimals} decimals:\")\n",
    "    print(f\"  Unique weight values: {combined_train[col_name].nunique()}\")\n",
    "    print(f\"  Price range in mapping: {mean_price.min():.2f} - {mean_price.max():.2f}\")\n",
    "    print(f\"  Std of mean prices: {mean_price.std():.2f}\")\n",
    "    \n",
    "    # Show sample\n",
    "    if decimals == 7:\n",
    "        print(f\"  Sample mapping:\")\n",
    "        print(f\"  {mean_price.head().to_string()}\")\n",
    "\n",
    "print(f\"\\nThis is similar to what winning solution does with original dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6d56b",
   "metadata": {},
   "source": [
    "## 4. Path Forward: What We Must Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RECOMMENDED NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 1: Add Original Dataset Features (CRITICAL)\")\n",
    "print(\"   - Download original Student Bag dataset\")\n",
    "print(\"   - Compute orig_price, orig_price_r7, orig_price_r8, orig_price_r9\")\n",
    "print(\"   - Expected improvement: 0.05-0.10 RMSE\")\n",
    "print(\"   - This gets us most of the way to target!\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 2: Add COMBO/Interaction Features (HIGH)\")\n",
    "print(\"   - NaNs: Base-2 encoding of all NaN patterns\")\n",
    "print(\"   - {col}_nan_wc: NaN status √ó Weight Capacity\")\n",
    "print(\"   - {col}_wc: Factorized categorical √ó Weight Capacity\")\n",
    "print(\"   - Expected improvement: 0.02-0.04 RMSE\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 3: Optimize Groupby Statistics (MEDIUM)\")\n",
    "print(\"   - Keep: mean, count, median (high importance)\")\n",
    "print(\"   - Remove: std, min, max (low/zero importance)\")\n",
    "print(\"   - Add: skew, kurtosis, percentiles (more signal)\")\n",
    "print(\"   - Expected improvement: 0.01-0.02 RMSE\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 4: Remove Histogram Bins (MEDIUM)\")\n",
    "print(\"   - Histograms are NOT in winning solution\")\n",
    "print(\"   - They add noise and overfitting\")\n",
    "print(\"   - Remove all 250 histogram features\")\n",
    "print(\"   - Expected improvement: 0.01-0.02 RMSE (from reduced overfitting)\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 5: Hyperparameter Tuning (LOW)\")\n",
    "print(\"   - Reduce learning rate: 0.05 ‚Üí 0.03\")\n",
    "print(\"   - Increase max_depth: 8 ‚Üí 10\")\n",
    "print(\"   - Add regularization: reg_alpha=0.1, reg_lambda=1.0\")\n",
    "print(\"   - Expected improvement: 0.005-0.01 RMSE\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPECTED OUTCOME\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "current_score = 38.663395\n",
    "target_score = 38.616280\n",
    "gap = current_score - target_score\n",
    "\n",
    "print(f\"\\nCurrent CV: {current_score:.6f}\")\n",
    "print(f\"Target: {target_score:.6f}\")\n",
    "print(f\"Gap: {gap:.6f}\")\n",
    "\n",
    "improvements = {\n",
    "    \"Original dataset\": 0.08,\n",
    "    \"COMBO features\": 0.03,\n",
    "    \"Groupby optimization\": 0.015,\n",
    "    \"Remove histograms\": 0.015,\n",
    "    \"Hyperparameter tuning\": 0.008\n",
    "}\n",
    "\n",
    "total_improvement = sum(improvements.values())\n",
    "projected_score = current_score - total_improvement\n",
    "\n",
    "print(f\"\\nProjected improvements:\")\n",
    "for feature, imp in improvements.items():\n",
    "    print(f\"  {feature:25s}: -{imp:.3f} RMSE\")\n",
    "\n",
    "print(f\"\\nTotal expected improvement: -{total_improvement:.3f} RMSE\")\n",
    "print(f\"Projected CV score: {projected_score:.6f}\")\n",
    "\n",
    "if projected_score < target_score:\n",
    "    print(f\"\\n‚úÖ SUCCESS: Projected to beat target by {target_score - projected_score:.6f} RMSE!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  GAP: Still short by {projected_score - target_score:.6f} RMSE\")\n",
    "    print(f\"Need additional techniques or more aggressive improvements\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
