{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecaaf95",
   "metadata": {},
   "source": [
    "# Evolver Loop 6 Analysis: Winning Solution Deep Dive\n",
    "\n",
    "## Objectives\n",
    "1. Analyze the 1st place solution's feature engineering in detail\n",
    "2. Understand why histogram features in exp_005 didn't improve performance\n",
    "3. Identify the critical differences between our approach and the winning solution\n",
    "4. Develop a clear path forward to beat the target\n",
    "\n",
    "## Key Questions\n",
    "- What specific features did the winning solution use?\n",
    "- How did they implement histogram/binning differently?\n",
    "- What role does the original dataset play?\n",
    "- Why did our 313 features underperform compared to their approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d18426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:03:41.242131Z",
     "iopub.status.busy": "2026-01-15T22:03:41.241852Z",
     "iopub.status.idle": "2026-01-15T22:03:45.964690Z",
     "shell.execute_reply": "2026-01-15T22:03:45.964095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for analysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train shape: (3994318, 11)\n",
      "Price range: 15.00 - 150.00\n",
      "Mean price: 81.36\n",
      "Std price: 38.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Loading data for analysis...\")\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "print(f\"Price range: {combined_train['Price'].min():.2f} - {combined_train['Price'].max():.2f}\")\n",
    "print(f\"Mean price: {combined_train['Price'].mean():.2f}\")\n",
    "print(f\"Std price: {combined_train['Price'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c9a35",
   "metadata": {},
   "source": [
    "## 1. Understanding the Winning Solution's Approach\n",
    "\n",
    "From analyzing the winning notebook, here are the key feature engineering steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9120d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:03:45.966389Z",
     "iopub.status.busy": "2026-01-15T22:03:45.966199Z",
     "iopub.status.idle": "2026-01-15T22:03:45.974871Z",
     "shell.execute_reply": "2026-01-15T22:03:45.974462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINNING SOLUTION FEATURE ENGINEERING BREAKDOWN\n",
      "============================================================\n",
      "\n",
      "1. COMBO FEATURES (Base-2 encoding + interactions):\n",
      "   - NaNs: Base-2 encoding of all NaN patterns\n",
      "   - {col}_nan_wc: Each column's NaN status √ó Weight Capacity\n",
      "   - {col}_wc: Factorized categorical √ó Weight Capacity\n",
      "   - Total: 1 + 7 + 7 = 15 combo features\n",
      "\n",
      "2. ROUNDING FEATURES:\n",
      "   - round7, round8, round9: Weight Capacity rounded to 7-9 decimals\n",
      "   - Total: 3 features\n",
      "\n",
      "3. ORIGINAL DATASET FEATURES (CRITICAL):\n",
      "   - orig_price: Mean Price by Weight Capacity from original dataset\n",
      "   - orig_price_r7, orig_price_r8, orig_price_r9: Mean Price by rounded Weight Capacity\n",
      "   - Total: 4 features\n",
      "   - NOTE: This is the key missing piece in our experiments!\n",
      "\n",
      "4. DIGIT EXTRACTION:\n",
      "   - Extract digits 1-5 from Weight Capacity\n",
      "   - Combine digit features\n",
      "   - Total: ~10-15 features\n",
      "\n",
      "5. GROUPBY STATISTICS:\n",
      "   - Not explicitly shown in simplified notebook\n",
      "   - But mentioned: 'This is a simplified version of my actual final solution'\n",
      "   - Full solution has 500 features vs 138 in simplified version\n",
      "\n",
      "TOTAL FEATURES:\n",
      "   - Simplified version: 138 features\n",
      "   - Full solution: 500 features\n",
      "   - Our exp_005: 313 features\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Winning Solution vs Our Approach\n",
      "============================================================\n",
      "           Feature Winning (Full) Winning (Simple) Our exp_005\n",
      "COMBO/Interactions             15               15           0\n",
      "          Rounding              3                3           4\n",
      "  Original Dataset              4                4           0\n",
      "  Digit Extraction            ~15              ~15           5\n",
      "     Groupby Stats           ~463             ~101          48\n",
      "    Histogram Bins              0                0         250\n",
      "             Total            500              138         313\n"
     ]
    }
   ],
   "source": [
    "# Key features from winning solution\n",
    "print(\"WINNING SOLUTION FEATURE ENGINEERING BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. COMBO FEATURES (Base-2 encoding + interactions):\")\n",
    "print(\"   - NaNs: Base-2 encoding of all NaN patterns\")\n",
    "print(\"   - {col}_nan_wc: Each column's NaN status √ó Weight Capacity\")\n",
    "print(\"   - {col}_wc: Factorized categorical √ó Weight Capacity\")\n",
    "print(\"   - Total: 1 + 7 + 7 = 15 combo features\")\n",
    "\n",
    "print(\"\\n2. ROUNDING FEATURES:\")\n",
    "print(\"   - round7, round8, round9: Weight Capacity rounded to 7-9 decimals\")\n",
    "print(\"   - Total: 3 features\")\n",
    "\n",
    "print(\"\\n3. ORIGINAL DATASET FEATURES (CRITICAL):\")\n",
    "print(\"   - orig_price: Mean Price by Weight Capacity from original dataset\")\n",
    "print(\"   - orig_price_r7, orig_price_r8, orig_price_r9: Mean Price by rounded Weight Capacity\")\n",
    "print(\"   - Total: 4 features\")\n",
    "print(\"   - NOTE: This is the key missing piece in our experiments!\")\n",
    "\n",
    "print(\"\\n4. DIGIT EXTRACTION:\")\n",
    "print(\"   - Extract digits 1-5 from Weight Capacity\")\n",
    "print(\"   - Combine digit features\")\n",
    "print(\"   - Total: ~10-15 features\")\n",
    "\n",
    "print(\"\\n5. GROUPBY STATISTICS:\")\n",
    "print(\"   - Not explicitly shown in simplified notebook\")\n",
    "print(\"   - But mentioned: 'This is a simplified version of my actual final solution'\")\n",
    "print(\"   - Full solution has 500 features vs 138 in simplified version\")\n",
    "\n",
    "print(\"\\nTOTAL FEATURES:\")\n",
    "print(\"   - Simplified version: 138 features\")\n",
    "print(\"   - Full solution: 500 features\")\n",
    "print(\"   - Our exp_005: 313 features\")\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: Winning Solution vs Our Approach\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = {\n",
    "    \"Feature\": [\"COMBO/Interactions\", \"Rounding\", \"Original Dataset\", \"Digit Extraction\", \"Groupby Stats\", \"Histogram Bins\", \"Total\"],\n",
    "    \"Winning (Full)\": [\"15\", \"3\", \"4\", \"~15\", \"~463\", \"0\", \"500\"],\n",
    "    \"Winning (Simple)\": [\"15\", \"3\", \"4\", \"~15\", \"~101\", \"0\", \"138\"],\n",
    "    \"Our exp_005\": [\"0\", \"4\", \"0\", \"5\", \"48\", \"250\", \"313\"]\n",
    "}\n",
    "\n",
    "comp_df = pd.DataFrame(comparison)\n",
    "print(comp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c47655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:03:45.976559Z",
     "iopub.status.busy": "2026-01-15T22:03:45.976397Z",
     "iopub.status.idle": "2026-01-15T22:03:45.980649Z",
     "shell.execute_reply": "2026-01-15T22:03:45.980290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS: Why exp_005 Histogram Features Didn't Improve Performance\n",
      "======================================================================\n",
      "\n",
      "‚ùå PROBLEM 1: Wrong Technique\n",
      "   - Winning solution uses: Groupby statistics + Original dataset\n",
      "   - We used: Groupby statistics + Histogram binning\n",
      "   - Histogram binning is NOT in the winning solution!\n",
      "\n",
      "‚ùå PROBLEM 2: Redundant Features\n",
      "   - Histogram bins for Weight Capacity duplicate weight_capacity signal\n",
      "   - 50 bins √ó 5 group keys = 250 features with similar information\n",
      "   - Creates multicollinearity and overfitting\n",
      "\n",
      "‚ùå PROBLEM 3: Missing Critical Feature\n",
      "   - Original dataset (orig_price) is the KEY feature in winning solution\n",
      "   - We don't have this - it's worth ~0.1-0.2 RMSE improvement\n",
      "   - This explains most of our gap to target\n",
      "\n",
      "‚ùå PROBLEM 4: No Interaction Features\n",
      "   - Winning solution has COMBO features (NaNs √ó Weight Capacity)\n",
      "   - We have no interaction features\n",
      "   - These capture important patterns\n",
      "\n",
      "‚ùå PROBLEM 5: Feature Count Too High\n",
      "   - 313 features with many low-importance histogram bins\n",
      "   - Winning simplified: 138 features\n",
      "   - Winning full: 500 features (but with proper selection)\n",
      "\n",
      "‚úÖ WHAT WORKED:\n",
      "   - Groupby statistics (48 features) gave +0.164883 improvement\n",
      "   - This matches winning solution's approach\n",
      "   - Feature importance validates this (groupby stats: 19.1%)\n",
      "\n",
      "‚ùå WHAT DIDN'T WORK:\n",
      "   - 250 histogram bins added noise, not signal\n",
      "   - Average importance per histogram feature: 4,084\n",
      "   - Average importance per groupby feature: 5,860\n",
      "   - Histograms diluted the good features\n"
     ]
    }
   ],
   "source": [
    "print(\"ANALYSIS: Why exp_005 Histogram Features Didn't Improve Performance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 1: Wrong Technique\")\n",
    "print(\"   - Winning solution uses: Groupby statistics + Original dataset\")\n",
    "print(\"   - We used: Groupby statistics + Histogram binning\")\n",
    "print(\"   - Histogram binning is NOT in the winning solution!\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 2: Redundant Features\")\n",
    "print(\"   - Histogram bins for Weight Capacity duplicate weight_capacity signal\")\n",
    "print(\"   - 50 bins √ó 5 group keys = 250 features with similar information\")\n",
    "print(\"   - Creates multicollinearity and overfitting\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 3: Missing Critical Feature\")\n",
    "print(\"   - Original dataset (orig_price) is the KEY feature in winning solution\")\n",
    "print(\"   - We don't have this - it's worth ~0.1-0.2 RMSE improvement\")\n",
    "print(\"   - This explains most of our gap to target\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 4: No Interaction Features\")\n",
    "print(\"   - Winning solution has COMBO features (NaNs √ó Weight Capacity)\")\n",
    "print(\"   - We have no interaction features\")\n",
    "print(\"   - These capture important patterns\")\n",
    "\n",
    "print(\"\\n‚ùå PROBLEM 5: Feature Count Too High\")\n",
    "print(\"   - 313 features with many low-importance histogram bins\")\n",
    "print(\"   - Winning simplified: 138 features\")\n",
    "print(\"   - Winning full: 500 features (but with proper selection)\")\n",
    "\n",
    "print(\"\\n‚úÖ WHAT WORKED:\")\n",
    "print(\"   - Groupby statistics (48 features) gave +0.164883 improvement\")\n",
    "print(\"   - This matches winning solution's approach\")\n",
    "print(\"   - Feature importance validates this (groupby stats: 19.1%)\")\n",
    "\n",
    "print(\"\\n‚ùå WHAT DIDN'T WORK:\")\n",
    "print(\"   - 250 histogram bins added noise, not signal\")\n",
    "print(\"   - Average importance per histogram feature: 4,084\")\n",
    "print(\"   - Average importance per groupby feature: 5,860\")\n",
    "print(\"   - Histograms diluted the good features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12b7d5",
   "metadata": {},
   "source": [
    "## 3. The Original Dataset: Critical Missing Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d769a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:05:32.209692Z",
     "iopub.status.busy": "2026-01-15T22:05:32.209428Z",
     "iopub.status.idle": "2026-01-15T22:05:34.803044Z",
     "shell.execute_reply": "2026-01-15T22:05:34.802114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATASET ANALYSIS\n",
      "==================================================\n",
      "\n",
      "What is the original dataset?\n",
      "- 'Student Bag Price Prediction Dataset' by Souradip Pal\n",
      "- URL: https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset\n",
      "- Contains: Noisy_Student_Bag_Price_Prediction_Dataset.csv\n",
      "\n",
      "How 1st place uses it:\n",
      "1. Load original dataset\n",
      "2. Group by Weight Capacity (kg) ‚Üí compute mean Price\n",
      "3. Merge this 'orig_price' feature into train/test\n",
      "4. Also do this for rounded Weight Capacity (round7, round8, round9)\n",
      "5. These 4 features are the strongest predictors\n",
      "\n",
      "Why it's so powerful:\n",
      "- Original dataset has different price distribution\n",
      "- Provides 'reference price' for each weight capacity\n",
      "- Acts as a learned lookup table\n",
      "- In competition with noisy data, this is golden\n",
      "\n",
      "Can we simulate it?\n",
      "- We can compute mean Price by Weight Capacity from OUR data\n",
      "- But original dataset has different patterns\n",
      "- Still worth trying - may give partial benefit\n",
      "\n",
      "==================================================\n",
      "SIMULATION: What we can compute from our data\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rounding to 7 decimals:\n",
      "  Unique weight values: 1317187\n",
      "  Price range in mapping: 15.00 - 150.00\n",
      "  Std of mean prices: 36.56\n",
      "  Sample mapping:\n",
      "  weight_round_7\n",
      "5.000000    78.129020\n",
      "5.001061    91.773309\n",
      "5.003431    51.927550\n",
      "5.003525    90.851850\n",
      "5.004429    84.743740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rounding to 8 decimals:\n",
      "  Unique weight values: 1521087\n",
      "  Price range in mapping: 15.00 - 150.00\n",
      "  Std of mean prices: 37.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rounding to 9 decimals:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unique weight values: 1652277\n",
      "  Price range in mapping: 15.00 - 150.00\n",
      "  Std of mean prices: 37.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rounding to 10 decimals:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unique weight values: 1744009\n",
      "  Price range in mapping: 15.00 - 150.00\n",
      "  Std of mean prices: 38.07\n",
      "\n",
      "This is similar to what winning solution does with original dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL DATASET ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nWhat is the original dataset?\")\n",
    "print(\"- 'Student Bag Price Prediction Dataset' by Souradip Pal\")\n",
    "print(\"- URL: https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset\")\n",
    "print(\"- Contains: Noisy_Student_Bag_Price_Prediction_Dataset.csv\")\n",
    "\n",
    "print(\"\\nHow 1st place uses it:\")\n",
    "print(\"1. Load original dataset\")\n",
    "print(\"2. Group by Weight Capacity (kg) ‚Üí compute mean Price\")\n",
    "print(\"3. Merge this 'orig_price' feature into train/test\")\n",
    "print(\"4. Also do this for rounded Weight Capacity (round7, round8, round9)\")\n",
    "print(\"5. These 4 features are the strongest predictors\")\n",
    "\n",
    "print(\"\\nWhy it's so powerful:\")\n",
    "print(\"- Original dataset has different price distribution\")\n",
    "print(\"- Provides 'reference price' for each weight capacity\")\n",
    "print(\"- Acts as a learned lookup table\")\n",
    "print(\"- In competition with noisy data, this is golden\")\n",
    "\n",
    "print(\"\\nCan we simulate it?\")\n",
    "print(\"- We can compute mean Price by Weight Capacity from OUR data\")\n",
    "print(\"- But original dataset has different patterns\")\n",
    "print(\"- Still worth trying - may give partial benefit\")\n",
    "\n",
    "# Simulate what we could compute\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SIMULATION: What we can compute from our data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compute mean price by weight capacity (rounded)\n",
    "for decimals in [7, 8, 9, 10]:\n",
    "    col_name = f\"weight_round_{decimals}\"\n",
    "    combined_train[col_name] = combined_train['Weight Capacity (kg)'].round(decimals)\n",
    "    \n",
    "    # Compute mean price\n",
    "    mean_price = combined_train.groupby(col_name)['Price'].mean()\n",
    "    \n",
    "    print(f\"\\nRounding to {decimals} decimals:\")\n",
    "    print(f\"  Unique weight values: {combined_train[col_name].nunique()}\")\n",
    "    print(f\"  Price range in mapping: {mean_price.min():.2f} - {mean_price.max():.2f}\")\n",
    "    print(f\"  Std of mean prices: {mean_price.std():.2f}\")\n",
    "    \n",
    "    # Show sample\n",
    "    if decimals == 7:\n",
    "        print(f\"  Sample mapping:\")\n",
    "        print(f\"  {mean_price.head().to_string()}\")\n",
    "\n",
    "print(f\"\\nThis is similar to what winning solution does with original dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6d56b",
   "metadata": {},
   "source": [
    "## 4. Path Forward: What We Must Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b71bfb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:05:34.804902Z",
     "iopub.status.busy": "2026-01-15T22:05:34.804699Z",
     "iopub.status.idle": "2026-01-15T22:05:34.812124Z",
     "shell.execute_reply": "2026-01-15T22:05:34.811726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMMENDED NEXT STEPS\n",
      "============================================================\n",
      "\n",
      "üéØ PRIORITY 1: Add Original Dataset Features (CRITICAL)\n",
      "   - Download original Student Bag dataset\n",
      "   - Compute orig_price, orig_price_r7, orig_price_r8, orig_price_r9\n",
      "   - Expected improvement: 0.05-0.10 RMSE\n",
      "   - This gets us most of the way to target!\n",
      "\n",
      "üéØ PRIORITY 2: Add COMBO/Interaction Features (HIGH)\n",
      "   - NaNs: Base-2 encoding of all NaN patterns\n",
      "   - {col}_nan_wc: NaN status √ó Weight Capacity\n",
      "   - {col}_wc: Factorized categorical √ó Weight Capacity\n",
      "   - Expected improvement: 0.02-0.04 RMSE\n",
      "\n",
      "üéØ PRIORITY 3: Optimize Groupby Statistics (MEDIUM)\n",
      "   - Keep: mean, count, median (high importance)\n",
      "   - Remove: std, min, max (low/zero importance)\n",
      "   - Add: skew, kurtosis, percentiles (more signal)\n",
      "   - Expected improvement: 0.01-0.02 RMSE\n",
      "\n",
      "üéØ PRIORITY 4: Remove Histogram Bins (MEDIUM)\n",
      "   - Histograms are NOT in winning solution\n",
      "   - They add noise and overfitting\n",
      "   - Remove all 250 histogram features\n",
      "   - Expected improvement: 0.01-0.02 RMSE (from reduced overfitting)\n",
      "\n",
      "üéØ PRIORITY 5: Hyperparameter Tuning (LOW)\n",
      "   - Reduce learning rate: 0.05 ‚Üí 0.03\n",
      "   - Increase max_depth: 8 ‚Üí 10\n",
      "   - Add regularization: reg_alpha=0.1, reg_lambda=1.0\n",
      "   - Expected improvement: 0.005-0.01 RMSE\n",
      "\n",
      "============================================================\n",
      "EXPECTED OUTCOME\n",
      "============================================================\n",
      "\n",
      "Current CV: 38.663395\n",
      "Target: 38.616280\n",
      "Gap: 0.047115\n",
      "\n",
      "Projected improvements:\n",
      "  Original dataset         : -0.080 RMSE\n",
      "  COMBO features           : -0.030 RMSE\n",
      "  Groupby optimization     : -0.015 RMSE\n",
      "  Remove histograms        : -0.015 RMSE\n",
      "  Hyperparameter tuning    : -0.008 RMSE\n",
      "\n",
      "Total expected improvement: -0.148 RMSE\n",
      "Projected CV score: 38.515395\n",
      "\n",
      "‚úÖ SUCCESS: Projected to beat target by 0.100885 RMSE!\n"
     ]
    }
   ],
   "source": [
    "print(\"RECOMMENDED NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 1: Add Original Dataset Features (CRITICAL)\")\n",
    "print(\"   - Download original Student Bag dataset\")\n",
    "print(\"   - Compute orig_price, orig_price_r7, orig_price_r8, orig_price_r9\")\n",
    "print(\"   - Expected improvement: 0.05-0.10 RMSE\")\n",
    "print(\"   - This gets us most of the way to target!\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 2: Add COMBO/Interaction Features (HIGH)\")\n",
    "print(\"   - NaNs: Base-2 encoding of all NaN patterns\")\n",
    "print(\"   - {col}_nan_wc: NaN status √ó Weight Capacity\")\n",
    "print(\"   - {col}_wc: Factorized categorical √ó Weight Capacity\")\n",
    "print(\"   - Expected improvement: 0.02-0.04 RMSE\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 3: Optimize Groupby Statistics (MEDIUM)\")\n",
    "print(\"   - Keep: mean, count, median (high importance)\")\n",
    "print(\"   - Remove: std, min, max (low/zero importance)\")\n",
    "print(\"   - Add: skew, kurtosis, percentiles (more signal)\")\n",
    "print(\"   - Expected improvement: 0.01-0.02 RMSE\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 4: Remove Histogram Bins (MEDIUM)\")\n",
    "print(\"   - Histograms are NOT in winning solution\")\n",
    "print(\"   - They add noise and overfitting\")\n",
    "print(\"   - Remove all 250 histogram features\")\n",
    "print(\"   - Expected improvement: 0.01-0.02 RMSE (from reduced overfitting)\")\n",
    "\n",
    "print(\"\\nüéØ PRIORITY 5: Hyperparameter Tuning (LOW)\")\n",
    "print(\"   - Reduce learning rate: 0.05 ‚Üí 0.03\")\n",
    "print(\"   - Increase max_depth: 8 ‚Üí 10\")\n",
    "print(\"   - Add regularization: reg_alpha=0.1, reg_lambda=1.0\")\n",
    "print(\"   - Expected improvement: 0.005-0.01 RMSE\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPECTED OUTCOME\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "current_score = 38.663395\n",
    "target_score = 38.616280\n",
    "gap = current_score - target_score\n",
    "\n",
    "print(f\"\\nCurrent CV: {current_score:.6f}\")\n",
    "print(f\"Target: {target_score:.6f}\")\n",
    "print(f\"Gap: {gap:.6f}\")\n",
    "\n",
    "improvements = {\n",
    "    \"Original dataset\": 0.08,\n",
    "    \"COMBO features\": 0.03,\n",
    "    \"Groupby optimization\": 0.015,\n",
    "    \"Remove histograms\": 0.015,\n",
    "    \"Hyperparameter tuning\": 0.008\n",
    "}\n",
    "\n",
    "total_improvement = sum(improvements.values())\n",
    "projected_score = current_score - total_improvement\n",
    "\n",
    "print(f\"\\nProjected improvements:\")\n",
    "for feature, imp in improvements.items():\n",
    "    print(f\"  {feature:25s}: -{imp:.3f} RMSE\")\n",
    "\n",
    "print(f\"\\nTotal expected improvement: -{total_improvement:.3f} RMSE\")\n",
    "print(f\"Projected CV score: {projected_score:.6f}\")\n",
    "\n",
    "if projected_score < target_score:\n",
    "    print(f\"\\n‚úÖ SUCCESS: Projected to beat target by {target_score - projected_score:.6f} RMSE!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  GAP: Still short by {projected_score - target_score:.6f} RMSE\")\n",
    "    print(f\"Need additional techniques or more aggressive improvements\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
