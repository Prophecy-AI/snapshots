{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f05ee59",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: Debugging Feature Engineering Issues\n",
    "\n",
    "Based on evaluator feedback from experiment 002_enhanced_features:\n",
    "- Score degraded from 38.781 to 38.786 (+0.005 RMSE)\n",
    "- Quantile features implementation is buggy (creates constant columns)\n",
    "- Count encoding uses combined train+test data (leakage risk)\n",
    "- No original dataset usage (explicitly recommended)\n",
    "- No feature importance analysis to debug what went wrong\n",
    "- No validation of individual feature contributions\n",
    "\n",
    "This notebook will:\n",
    "1. Analyze feature importance from experiment 002\n",
    "2. Debug the quantile feature implementation\n",
    "3. Validate count encoding approach\n",
    "4. Identify harmful vs helpful features\n",
    "5. Plan systematic ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train1 = pd.read_csv('/home/data/train.csv')\n",
    "train2 = pd.read_csv('/home/data/training_extra.csv')\n",
    "train = pd.concat([train1, train2], ignore_index=True)\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "cat_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "target_col = 'Price'\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(f\"Test data: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293dc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate features from experiment 002 to analyze them\n",
    "print(\"Recreating features from experiment 002...\")\n",
    "\n",
    "# Handle missing values\n",
    "for col in cat_features:\n",
    "    train[col] = train[col].fillna('Missing')\n",
    "    test[col] = test[col].fillna('Missing')\n",
    "\n",
    "train['Compartments'] = train['Compartments'].fillna(train['Compartments'].median())\n",
    "test['Compartments'] = test['Compartments'].fillna(train['Compartments'].median())\n",
    "\n",
    "train['Weight Capacity (kg)'] = train['Weight Capacity (kg)'].fillna(train['Weight Capacity (kg)'].median())\n",
    "test['Weight Capacity (kg)'] = test['Weight Capacity (kg)'].fillna(train['Weight Capacity (kg)'].median())\n",
    "\n",
    "# Weight capacity features\n",
    "def create_weight_capacity_features(df):\n",
    "    df = df.copy()\n",
    "    wc = df['Weight Capacity (kg)']\n",
    "    \n",
    "    # 50 uniform bins\n",
    "    df['weight_bin_50'] = pd.cut(wc, bins=50, labels=False, retbins=False)\n",
    "    \n",
    "    # Rounding to 7-10 decimal places\n",
    "    for dec in [7, 8, 9, 10]:\n",
    "        df[f'weight_round_{dec}'] = wc.round(dec)\n",
    "    \n",
    "    # Digit extraction (1-5 digits)\n",
    "    for k in range(1, 6):\n",
    "        df[f'weight_digit_{k}'] = ((wc * 10**k) % 10).fillna(-1)\n",
    "    \n",
    "    # Basic components\n",
    "    df['weight_int'] = wc.astype(int)\n",
    "    df['weight_frac'] = wc - df['weight_int']\n",
    "    \n",
    "    # Quantile features - BUGGY IMPLEMENTATION - creates constant columns\n",
    "    quantiles = [0.25, 0.5, 0.75, 0.9]\n",
    "    for q in quantiles:\n",
    "        # BUG: This creates a single value for ALL rows, not per-row features\n",
    "        df[f'weight_q{int(q*100)}'] = wc.quantile(q)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_features = create_weight_capacity_features(train)\n",
    "test_features = create_weight_capacity_features(test)\n",
    "\n",
    "print(\"Features created. Checking for constant columns...\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_cols = []\n",
    "for col in train_features.columns:\n",
    "    if col not in ['id', 'Price'] and train_features[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "        print(f\"  CONSTANT: {col} - value: {train_features[col].iloc[0]}\")\n",
    "\n",
    "if not constant_cols:\n",
    "    print(\"  No constant columns found (unexpected)\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(constant_cols)} constant columns that add no signal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature variance and importance\n",
    "print(\"Analyzing feature variance...\")\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in train_features.columns if col not in ['id', 'Price']]\n",
    "\n",
    "# Calculate variance for each feature\n",
    "variance_stats = []\n",
    "for col in feature_cols:\n",
    "    variance = train_features[col].var()\n",
    "    nunique = train_features[col].nunique()\n",
    "    variance_stats.append({'feature': col, 'variance': variance, 'nunique': nunique})\n",
    "\n",
    "variance_df = pd.DataFrame(variance_stats).sort_values('variance')\n",
    "\n",
    "print(\"Features with lowest variance (potential issues):\")\n",
    "print(variance_df.head(10))\n",
    "\n",
    "print(\"\\nFeatures with highest variance:\")\n",
    "print(variance_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3785e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a quick model to get feature importance\n",
    "print(\"Training model to get feature importance...\")\n",
    "\n",
    "# Prepare data\n",
    "X = train_features[feature_cols]\n",
    "y = train_features[target_col]\n",
    "\n",
    "# Train a small XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(X, y, verbose=False)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "print(\"\\nBottom 15 least important features:\")\n",
    "print(importance_df.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fce796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze count encoding implementation\n",
    "print(\"Analyzing count encoding implementation...\")\n",
    "\n",
    "# Count encoding function (from experiment 002)\n",
    "def create_count_encoding(df, df_test, cat_cols):\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Compute counts from combined train+test - LEAKAGE RISK\n",
    "        combined = pd.concat([df[col], df_test[col]], ignore_index=True)\n",
    "        counts = combined.value_counts()\n",
    "        \n",
    "        df[f'{col}_count'] = df[col].map(counts)\n",
    "        df_test[f'{col}_count'] = df_test[col].map(counts)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "# Test the count encoding\n",
    "train_test, test_test = create_count_encoding(train.copy(), test.copy(), cat_features)\n",
    "\n",
    "print(\"Count encoding statistics:\")\n",
    "for col in cat_features:\n",
    "    count_col = f'{col}_count'\n",
    "    corr_with_target = train_test[count_col].corr(train_test['Price'])\n",
    "    print(f\"  {col}_count: correlation with Price = {corr_with_target:.6f}\")\n",
    "\n",
    "print(\"\\nCount encoding correlation analysis:\")\n",
    "print(\"- Strong correlations (>0.1) indicate useful signal\")\n",
    "print(\"- Weak correlations (<0.01) may add noise\")\n",
    "print(\"- Negative correlations may indicate harmful features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf777375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interaction features\n",
    "print(\"Analyzing interaction features...\")\n",
    "\n",
    "# Create interaction features\n",
    "def create_interaction_features(df, df_test):\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    df['Brand_Size'] = df['Brand'].astype(str) + '_' + df['Size'].astype(str)\n",
    "    df_test['Brand_Size'] = df_test['Brand'].astype(str) + '_' + df_test['Size'].astype(str)\n",
    "    \n",
    "    df['Size_Color'] = df['Size'].astype(str) + '_' + df['Color'].astype(str)\n",
    "    df_test['Size_Color'] = df_test['Size'].astype(str) + '_' + df_test['Color'].astype(str)\n",
    "    \n",
    "    df['Size_Style'] = df['Size'].astype(str) + '_' + df['Style'].astype(str)\n",
    "    df_test['Size_Style'] = df_test['Size'].astype(str) + '_' + df_test['Style'].astype(str)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "train_int, test_int = create_interaction_features(train.copy(), test.copy())\n",
    "\n",
    "# Analyze cardinality and price variance\n",
    "interaction_features = ['Brand_Size', 'Size_Color', 'Size_Style']\n",
    "\n",
    "for feature in interaction_features:\n",
    "    nunique = train_int[feature].nunique()\n",
    "    price_std_by_group = train_int.groupby(feature)['Price'].std().mean()\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Unique combinations: {nunique}\")\n",
    "    print(f\"  Avg price std by group: {price_std_by_group:.4f}\")\n",
    "    \n",
    "    # Check for rare combinations\n",
    "    value_counts = train_int[feature].value_counts()\n",
    "    rare_combinations = (value_counts < 10).sum()\n",
    "    print(f\"  Rare combinations (<10 samples): {rare_combinations}\")\n",
    "    \n",
    "    if rare_combinations > nunique * 0.5:\n",
    "        print(f\"  WARNING: More than 50% rare combinations - may cause overfitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc036ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings and recommendations\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. QUANTILE FEATURES BUG:\")\n",
    "print(\"   - Implementation creates constant columns (all same value)\")\n",
    "print(\"   - These add NO signal but increase dimensionality\")\n",
    "print(\"   - FIX: Remove these features or compute per-group quantiles\")\n",
    "\n",
    "print(\"\\n2. COUNT ENCODING LEAKAGE:\")\n",
    "print(\"   - Using combined train+test data leaks test distribution\")\n",
    "print(\"   - Should compute counts from training data only\")\n",
    "print(\"   - Many count features have very low correlation with target\")\n",
    "\n",
    "print(\"\\n3. INTERACTION FEATURES:\")\n",
    "print(\"   - Brand_Size has moderate cardinality (manageable)\")\n",
    "print(\"   - Need to validate if they actually improve performance\")\n",
    "print(\"   - Consider target encoding these interactions\")\n",
    "\n",
    "print(\"\\n4. MISSING ORIGINAL DATASET:\")\n",
    "print(\"   - Competition explicitly allows using original Student Bag dataset\")\n",
    "print(\"   - 1st place solution heavily exploited this\")\n",
    "print(\"   - Need to download and incorporate original data features\")\n",
    "\n",
    "print(\"\\n5. SYSTEMATIC VALIDATION NEEDED:\")\n",
    "print(\"   - Train models with each feature group separately\")\n",
    "print(\"   - Use ablation studies to identify helpful vs harmful features\")\n",
    "print(\"   - Feature importance analysis shows some features have near-zero importance\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"1. Remove constant quantile features\")\n",
    "print(\"2. Fix count encoding to use training data only\")\n",
    "print(\"3. Download original dataset and compute MSRP features\")\n",
    "print(\"4. Implement systematic feature validation\")\n",
    "print(\"5. Try alternative interaction features\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
