{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02ede76",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis: Understanding Feature Importance Issues in exp_003\n",
    "\n",
    "## Problem Statement\n",
    "exp_003 achieved CV RMSE of 38.8257 (worse than baseline 38.7811) and feature importance showed all weight features at zero importance. This analysis investigates why.\n",
    "\n",
    "## Hypotheses\n",
    "1. Rounding features are too similar to original (high correlation)\n",
    "2. Digit features have low correlation with target\n",
    "3. Missing the key histogram binning approach from winning solutions\n",
    "4. Need target encoding to capture categorical signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "y = combined_train['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18690fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weight capacity features from exp_003\n",
    "weight = combined_train['Weight Capacity (kg)'].copy()\n",
    "\n",
    "# Create features like in exp_003\n",
    "features = pd.DataFrame(index=combined_train.index)\n",
    "features['weight_original'] = weight\n",
    "\n",
    "for dec in range(7, 11):\n",
    "    features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "\n",
    "weight_filled = weight.fillna(0)\n",
    "weight_str = weight_filled.astype(str).str.replace('.', '', regex=False)\n",
    "weight_str = weight_str.str.pad(width=5, side='right', fillchar='0')\n",
    "\n",
    "for i in range(1, 6):\n",
    "    features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "\n",
    "features['weight_int'] = weight.fillna(0).astype(int)\n",
    "features['weight_frac'] = weight.fillna(0) - weight.fillna(0).astype(int)\n",
    "\n",
    "print(\"Feature correlations with target:\")\n",
    "for col in features.columns:\n",
    "    corr = features[col].corr(y)\n",
    "    print(f\"{col}: {corr:.6f}\")\n",
    "\n",
    "print(\"\\nFeature variances:\")\n",
    "for col in features.columns:\n",
    "    var = features[col].var()\n",
    "    print(f\"{col}: {var:.6f}\")\n",
    "\n",
    "# Check correlation between rounding features and original\n",
    "print(\"\\nCorrelation with weight_original:\")\n",
    "for col in features.columns:\n",
    "    if col != 'weight_original':\n",
    "        corr = features[col].corr(features['weight_original'])\n",
    "        print(f\"{col}: {corr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "print(\"Categorical feature analysis:\")\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        # Label encode\n",
    "        le = LabelEncoder()\n",
    "        combined_data = pd.concat([combined_train[col], test[col]], ignore_index=True)\n",
    "        le.fit(combined_data.astype(str).fillna('missing'))\n",
    "        \n",
    "        encoded = le.transform(combined_train[col].astype(str).fillna('missing'))\n",
    "        corr = np.corrcoef(encoded, y)[0, 1]\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Cardinality: {len(le.classes_)}\")\n",
    "        print(f\"  Correlation: {corr:.6f}\")\n",
    "        print(f\"  Mean price by category:\")\n",
    "        \n",
    "        # Show mean price by category\n",
    "        category_stats = combined_train.groupby(col)['Price'].agg(['count', 'mean']).round(2)\n",
    "        print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test histogram binning approach from winning solutions\n",
    "# Instead of simple rounding, create histogram features\n",
    "\n",
    "weight_nonan = weight.dropna()\n",
    "y_nonan = y[weight_nonan.index]\n",
    "\n",
    "# Create 50 uniform bins\n",
    "bins = np.linspace(weight_nonan.min(), weight_nonan.max(), 51)\n",
    "weight_binned = pd.cut(weight_nonan, bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "# Calculate statistics per bin\n",
    "bin_stats = pd.DataFrame({\n",
    "    'weight_bin': weight_binned,\n",
    "    'price': y_nonan\n",
    "}).groupby('weight_bin')['price'].agg(['count', 'mean', 'std']).reset_index()\n",
    "\n",
    "print(\"Bin statistics (first 10):\")\n",
    "print(bin_stats.head(10))\n",
    "\n",
    "print(f\"\\nPrice range across bins: {bin_stats['mean'].max() - bin_stats['mean'].min():.2f}\")\n",
    "print(f\"Correlation between bin and price: {weight_binned.corr(y_nonan):.6f}\")\n",
    "\n",
    "# Create histogram feature\n",
    "hist_feature = pd.Series(index=weight.index, dtype=float)\n",
    "for bin_idx in bin_stats['weight_bin']:\n",
    "    mask = (weight_binned == bin_idx)\n",
    "    hist_feature.loc[mask.index[mask]] = bin_stats.loc[bin_stats['weight_bin'] == bin_idx, 'mean'].values[0]\n",
    "\n",
    "# Fill NaN values\n",
    "hist_feature = hist_feature.fillna(y.mean())\n",
    "\n",
    "print(f\"\\nHistogram feature correlation with target: {hist_feature.corr(y):.6f}\")\n",
    "print(f\"Histogram feature variance: {hist_feature.var():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test target encoding approach\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def target_encode(train_df, test_df, col, target_col='Price', n_folds=5):\n",
    "    \"\"\"Target encode with nested CV to prevent leakage\"\"\"\n",
    "    \n",
    "    # Initialize encoded columns\n",
    "    train_encoded = pd.Series(index=train_df.index, dtype=float)\n",
    "    test_encoded = pd.Series(index=test_df.index, dtype=float)\n",
    "    \n",
    "    # For test data, use full training data to compute statistics\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    # Compute statistics for each category in full training data\n",
    "    stats = train_df.groupby(col)[target_col].agg(['mean', 'count']).reset_index()\n",
    "    \n",
    "    # Apply smoothing\n",
    "    min_samples = 100\n",
    "    smoothing = 100\n",
    "    \n",
    "    stats['smooth_mean'] = (stats['count'] * stats['mean'] + smoothing * global_mean) / (stats['count'] + smoothing)\n",
    "    \n",
    "    # Map to test data\n",
    "    category_map = dict(zip(stats[col], stats['smooth_mean']))\n",
    "    test_encoded = test_df[col].map(category_map).fillna(global_mean)\n",
    "    \n",
    "    # For train data, use nested CV\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_df):\n",
    "        X_tr, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n",
    "        \n",
    "        # Compute statistics on training fold\n",
    "        fold_stats = X_tr.groupby(col)[target_col].agg(['mean', 'count']).reset_index()\n",
    "        fold_stats['smooth_mean'] = (fold_stats['count'] * fold_stats['mean'] + smoothing * global_mean) / (fold_stats['count'] + smoothing)\n",
    "        \n",
    "        fold_map = dict(zip(fold_stats[col], fold_stats['smooth_mean']))\n",
    "        train_encoded.loc[val_idx] = X_val[col].map(fold_map).fillna(global_mean)\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Test target encoding on Color (moderate cardinality)\n",
    "train_subset = combined_train.sample(frac=0.1, random_state=42)  # Sample for speed\n",
    "test_subset = test.sample(frac=0.1, random_state=42)\n",
    "\n",
    "print(\"Testing target encoding on Color feature:\")\n",
    "train_encoded, test_encoded = target_encode(train_subset, test_subset, 'Color')\n",
    "\n",
    "print(f\"Encoded feature correlation with target: {train_encoded.corr(train_subset['Price']):.6f}\")\n",
    "print(f\"Encoded feature variance: {train_encoded.var():.6f}\")\n",
    "\n",
    "# Compare with label encoding\n",
    "le = LabelEncoder()\n",
    "combined_data = pd.concat([train_subset['Color'], test_subset['Color']], ignore_index=True)\n",
    "le.fit(combined_data.astype(str).fillna('missing'))\n",
    "label_encoded = le.transform(train_subset['Color'].astype(str).fillna('missing'))\n",
    "\n",
    "print(f\"Label encoded correlation: {np.corrcoef(label_encoded, train_subset['Price'])[0, 1]:.6f}\")\n",
    "print(f\"Target encoding improvement: {train_encoded.corr(train_subset['Price']) - np.corrcoef(label_encoded, train_subset['Price'])[0, 1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8065fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Weight Capacity Features (exp_003):\")\n",
    "print(\"   - Rounding features: Highly correlated with original (0.999+)\")\n",
    "print(\"   - Digit features: Low correlation with target (0.0007-0.0207)\")\n",
    "print(\"   - All features: Low correlation with target (<0.02)\")\n",
    "print(\"   - CONCLUSION: These features have minimal predictive power\")\n",
    "\n",
    "print(\"\\n2. Categorical Features:\")\n",
    "print(\"   - All have low correlation with target (<0.009)\")\n",
    "print(\"   - Small differences in mean price between categories\")\n",
    "print(\"   - CONCLUSION: Raw categoricals have weak signal\")\n",
    "\n",
    "print(\"\\n3. Histogram Binning:\")\n",
    "print(\"   - Creates 8.85 price range across 50 bins\")\n",
    "print(\"   - Correlation: 0.0177 (similar to raw weight)\")\n",
    "print(\"   - CONCLUSION: Better than rounding, but still limited\")\n",
    "\n",
    "print(\"\\n4. Target Encoding:\")\n",
    "print(\"   - Can improve correlation over label encoding\")\n",
    "print(\"   - Captures target statistics per category\")\n",
    "print(\"   - CONCLUSION: Essential for categorical features\")\n",
    "\n",
    "print(\"\\n5. Key Insights:\")\n",
    "print(\"   - Simple weight features are insufficient\")\n",
    "print(\"   - Need original dataset features (MSRP)\")\n",
    "print(\"   - Need proper target encoding\")\n",
    "print(\"   - Need interaction features\")\n",
    "print(\"   - Need to follow winning solution patterns exactly\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
