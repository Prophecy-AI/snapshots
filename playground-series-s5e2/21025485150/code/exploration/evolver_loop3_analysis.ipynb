{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f05ee59",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: Debugging Feature Engineering Issues\n",
    "\n",
    "Based on evaluator feedback from experiment 002_enhanced_features:\n",
    "- Score degraded from 38.781 to 38.786 (+0.005 RMSE)\n",
    "- Quantile features implementation is buggy (creates constant columns)\n",
    "- Count encoding uses combined train+test data (leakage risk)\n",
    "- No original dataset usage (explicitly recommended)\n",
    "- No feature importance analysis to debug what went wrong\n",
    "- No validation of individual feature contributions\n",
    "\n",
    "This notebook will:\n",
    "1. Analyze feature importance from experiment 002\n",
    "2. Debug the quantile feature implementation\n",
    "3. Validate count encoding approach\n",
    "4. Identify harmful vs helpful features\n",
    "5. Plan systematic ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cc5df3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:25:13.997021Z",
     "iopub.status.busy": "2026-01-15T12:25:13.996836Z",
     "iopub.status.idle": "2026-01-15T12:25:17.668569Z",
     "shell.execute_reply": "2026-01-15T12:25:17.667975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (3994318, 11)\n",
      "\n",
      "Analyzing quantile features bug from experiment 002...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buggy quantile features would be:\n",
      "  weight_q25 = 12.068964 (same for ALL rows)\n",
      "  weight_q50 = 18.054360 (same for ALL rows)\n",
      "  weight_q75 = 23.987505 (same for ALL rows)\n",
      "  weight_q90 = 27.563675 (same for ALL rows)\n",
      "\n",
      "These are 4 CONSTANT columns with ZERO variance!\n",
      "They add NO predictive signal but increase dimensionality.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train1 = pd.read_csv('/home/data/train.csv')\n",
    "train2 = pd.read_csv('/home/data/training_extra.csv')\n",
    "train = pd.concat([train1, train2], ignore_index=True)\n",
    "\n",
    "cat_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "target_col = 'Price'\n",
    "\n",
    "print(f\"Training data: {train.shape}\")\n",
    "print(\"\\nAnalyzing quantile features bug from experiment 002...\")\n",
    "\n",
    "# The bug: quantile features create constant columns\n",
    "wc = train['Weight Capacity (kg)']\n",
    "\n",
    "# This is what experiment 002 did (BUGGY):\n",
    "quantile_25 = wc.quantile(0.25)\n",
    "quantile_50 = wc.quantile(0.5)\n",
    "quantile_75 = wc.quantile(0.75)\n",
    "quantile_90 = wc.quantile(0.9)\n",
    "\n",
    "print(f\"Buggy quantile features would be:\")\n",
    "print(f\"  weight_q25 = {quantile_25:.6f} (same for ALL rows)\")\n",
    "print(f\"  weight_q50 = {quantile_50:.6f} (same for ALL rows)\")\n",
    "print(f\"  weight_q75 = {quantile_75:.6f} (same for ALL rows)\")\n",
    "print(f\"  weight_q90 = {quantile_90:.6f} (same for ALL rows)\")\n",
    "print(f\"\\nThese are 4 CONSTANT columns with ZERO variance!\")\n",
    "print(f\"They add NO predictive signal but increase dimensionality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "293dc2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:25:17.669848Z",
     "iopub.status.busy": "2026-01-15T12:25:17.669679Z",
     "iopub.status.idle": "2026-01-15T12:25:17.675269Z",
     "shell.execute_reply": "2026-01-15T12:25:17.674888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing quantile features bug...\n",
      "Sample Weight Capacity values (first 5): [11.61172281 27.07853658 16.64375995 12.93722031 17.74933847]\n",
      "\n",
      "Buggy implementation:\n",
      "  wc.quantile(0.25) = 12.222638085591132\n",
      "  This creates a constant column where EVERY row = 12.222638085591132\n",
      "\n",
      "Correct implementation would be:\n",
      "  Group by some feature (e.g., weight_bin), THEN compute quantile\n",
      "  Result: different quantile values for different groups\n",
      "\n",
      "Quantile features created: ['weight_q25', 'weight_q50', 'weight_q75', 'weight_q90']\n",
      "All of these are constant columns with zero variance!\n",
      "They add 4 features with NO predictive signal.\n"
     ]
    }
   ],
   "source": [
    "# Analyze the quantile features bug from experiment 002\n",
    "print(\"Analyzing quantile features bug...\")\n",
    "\n",
    "# The bug: quantile features create constant columns\n",
    "# Let's demonstrate the issue with a simple example\n",
    "\n",
    "wc_sample = train['Weight Capacity (kg)'].head(1000)\n",
    "print(f\"Sample Weight Capacity values (first 5): {wc_sample.head().values}\")\n",
    "\n",
    "# This is what experiment 002 did (BUGGY):\n",
    "quantile_25_buggy = wc_sample.quantile(0.25)\n",
    "print(f\"\\nBuggy implementation:\")\n",
    "print(f\"  wc.quantile(0.25) = {quantile_25_buggy}\")\n",
    "print(f\"  This creates a constant column where EVERY row = {quantile_25_buggy}\")\n",
    "\n",
    "# This is what it should do (if we want per-group quantiles):\n",
    "print(f\"\\nCorrect implementation would be:\")\n",
    "print(f\"  Group by some feature (e.g., weight_bin), THEN compute quantile\")\n",
    "print(f\"  Result: different quantile values for different groups\")\n",
    "\n",
    "# Check how many constant columns this creates\n",
    "quantiles = [0.25, 0.5, 0.75, 0.9]\n",
    "print(f\"\\nQuantile features created: {[f'weight_q{int(q*100)}' for q in quantiles]}\")\n",
    "print(f\"All of these are constant columns with zero variance!\")\n",
    "print(f\"They add 4 features with NO predictive signal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature variance and importance\n",
    "print(\"Analyzing feature variance...\")\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in train_features.columns if col not in ['id', 'Price']]\n",
    "\n",
    "# Calculate variance for each feature\n",
    "variance_stats = []\n",
    "for col in feature_cols:\n",
    "    variance = train_features[col].var()\n",
    "    nunique = train_features[col].nunique()\n",
    "    variance_stats.append({'feature': col, 'variance': variance, 'nunique': nunique})\n",
    "\n",
    "variance_df = pd.DataFrame(variance_stats).sort_values('variance')\n",
    "\n",
    "print(\"Features with lowest variance (potential issues):\")\n",
    "print(variance_df.head(10))\n",
    "\n",
    "print(\"\\nFeatures with highest variance:\")\n",
    "print(variance_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3785e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a quick model to get feature importance\n",
    "print(\"Training model to get feature importance...\")\n",
    "\n",
    "# Prepare data\n",
    "X = train_features[feature_cols]\n",
    "y = train_features[target_col]\n",
    "\n",
    "# Train a small XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 500,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "model.fit(X, y, verbose=False)\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "print(\"\\nBottom 15 least important features:\")\n",
    "print(importance_df.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fce796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze count encoding implementation\n",
    "print(\"Analyzing count encoding implementation...\")\n",
    "\n",
    "# Count encoding function (from experiment 002)\n",
    "def create_count_encoding(df, df_test, cat_cols):\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Compute counts from combined train+test - LEAKAGE RISK\n",
    "        combined = pd.concat([df[col], df_test[col]], ignore_index=True)\n",
    "        counts = combined.value_counts()\n",
    "        \n",
    "        df[f'{col}_count'] = df[col].map(counts)\n",
    "        df_test[f'{col}_count'] = df_test[col].map(counts)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "# Test the count encoding\n",
    "train_test, test_test = create_count_encoding(train.copy(), test.copy(), cat_features)\n",
    "\n",
    "print(\"Count encoding statistics:\")\n",
    "for col in cat_features:\n",
    "    count_col = f'{col}_count'\n",
    "    corr_with_target = train_test[count_col].corr(train_test['Price'])\n",
    "    print(f\"  {col}_count: correlation with Price = {corr_with_target:.6f}\")\n",
    "\n",
    "print(\"\\nCount encoding correlation analysis:\")\n",
    "print(\"- Strong correlations (>0.1) indicate useful signal\")\n",
    "print(\"- Weak correlations (<0.01) may add noise\")\n",
    "print(\"- Negative correlations may indicate harmful features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf777375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interaction features\n",
    "print(\"Analyzing interaction features...\")\n",
    "\n",
    "# Create interaction features\n",
    "def create_interaction_features(df, df_test):\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    df['Brand_Size'] = df['Brand'].astype(str) + '_' + df['Size'].astype(str)\n",
    "    df_test['Brand_Size'] = df_test['Brand'].astype(str) + '_' + df_test['Size'].astype(str)\n",
    "    \n",
    "    df['Size_Color'] = df['Size'].astype(str) + '_' + df['Color'].astype(str)\n",
    "    df_test['Size_Color'] = df_test['Size'].astype(str) + '_' + df_test['Color'].astype(str)\n",
    "    \n",
    "    df['Size_Style'] = df['Size'].astype(str) + '_' + df['Style'].astype(str)\n",
    "    df_test['Size_Style'] = df_test['Size'].astype(str) + '_' + df_test['Style'].astype(str)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "train_int, test_int = create_interaction_features(train.copy(), test.copy())\n",
    "\n",
    "# Analyze cardinality and price variance\n",
    "interaction_features = ['Brand_Size', 'Size_Color', 'Size_Style']\n",
    "\n",
    "for feature in interaction_features:\n",
    "    nunique = train_int[feature].nunique()\n",
    "    price_std_by_group = train_int.groupby(feature)['Price'].std().mean()\n",
    "    \n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Unique combinations: {nunique}\")\n",
    "    print(f\"  Avg price std by group: {price_std_by_group:.4f}\")\n",
    "    \n",
    "    # Check for rare combinations\n",
    "    value_counts = train_int[feature].value_counts()\n",
    "    rare_combinations = (value_counts < 10).sum()\n",
    "    print(f\"  Rare combinations (<10 samples): {rare_combinations}\")\n",
    "    \n",
    "    if rare_combinations > nunique * 0.5:\n",
    "        print(f\"  WARNING: More than 50% rare combinations - may cause overfitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc036ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings and recommendations\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. QUANTILE FEATURES BUG:\")\n",
    "print(\"   - Implementation creates constant columns (all same value)\")\n",
    "print(\"   - These add NO signal but increase dimensionality\")\n",
    "print(\"   - FIX: Remove these features or compute per-group quantiles\")\n",
    "\n",
    "print(\"\\n2. COUNT ENCODING LEAKAGE:\")\n",
    "print(\"   - Using combined train+test data leaks test distribution\")\n",
    "print(\"   - Should compute counts from training data only\")\n",
    "print(\"   - Many count features have very low correlation with target\")\n",
    "\n",
    "print(\"\\n3. INTERACTION FEATURES:\")\n",
    "print(\"   - Brand_Size has moderate cardinality (manageable)\")\n",
    "print(\"   - Need to validate if they actually improve performance\")\n",
    "print(\"   - Consider target encoding these interactions\")\n",
    "\n",
    "print(\"\\n4. MISSING ORIGINAL DATASET:\")\n",
    "print(\"   - Competition explicitly allows using original Student Bag dataset\")\n",
    "print(\"   - 1st place solution heavily exploited this\")\n",
    "print(\"   - Need to download and incorporate original data features\")\n",
    "\n",
    "print(\"\\n5. SYSTEMATIC VALIDATION NEEDED:\")\n",
    "print(\"   - Train models with each feature group separately\")\n",
    "print(\"   - Use ablation studies to identify helpful vs harmful features\")\n",
    "print(\"   - Feature importance analysis shows some features have near-zero importance\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"1. Remove constant quantile features\")\n",
    "print(\"2. Fix count encoding to use training data only\")\n",
    "print(\"3. Download original dataset and compute MSRP features\")\n",
    "print(\"4. Implement systematic feature validation\")\n",
    "print(\"5. Try alternative interaction features\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
