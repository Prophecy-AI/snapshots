{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5014a0e6",
   "metadata": {},
   "source": [
    "# Evolver Loop 5 Analysis: Feature Selection & Original Dataset\n",
    "\n",
    "## Objectives\n",
    "1. Analyze exp_005 feature importance to identify valuable vs noisy features\n",
    "2. Understand why 313 features didn't improve over 67 features in exp_004\n",
    "3. Research winning solutions for feature selection strategies\n",
    "4. Prepare for incorporating original Student Bag dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537a5c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:16:49.210410Z",
     "iopub.status.busy": "2026-01-15T20:16:49.210188Z",
     "iopub.status.idle": "2026-01-15T20:16:53.875271Z",
     "shell.execute_reply": "2026-01-15T20:16:53.874751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exp_005 data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train shape: (3994318, 11)\n",
      "Target price range: 15.00 - 150.00\n",
      "Target price mean: 81.36\n",
      "Target price std: 38.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load exp_005 results\n",
    "print(\"Loading exp_005 data...\")\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "print(f\"Target price range: {combined_train['Price'].min():.2f} - {combined_train['Price'].max():.2f}\")\n",
    "print(f\"Target price mean: {combined_train['Price'].mean():.2f}\")\n",
    "print(f\"Target price std: {combined_train['Price'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c670073",
   "metadata": {},
   "source": [
    "## 1. Feature Importance Analysis from exp_005\n",
    "\n",
    "Load and analyze the feature importance data to understand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac7ead8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:16:53.876593Z",
     "iopub.status.busy": "2026-01-15T20:16:53.876479Z",
     "iopub.status.idle": "2026-01-15T20:16:53.881004Z",
     "shell.execute_reply": "2026-01-15T20:16:53.880636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature importance by category:\n",
      "Histogram bins: 1,020,884\n",
      "Groupby statistics: 281,304\n",
      "Baseline features: 172,796\n",
      "Grand total: 1,474,984\n",
      "\n",
      "Percentage of total importance:\n",
      "Histogram bins: 69.2%\n",
      "Groupby statistics: 19.1%\n",
      "Baseline features: 11.7%\n"
     ]
    }
   ],
   "source": [
    "# Simulate feature importance from exp_005 (based on executor output)\n",
    "# This is reconstructed from the experiment results\n",
    "\n",
    "feature_importance_data = {\n",
    "    # Baseline features (top ones)\n",
    "    'weight_capacity': 58096,\n",
    "    'weight_dec_2': 35493,\n",
    "    'weight_dec_1': 25000,  # estimated\n",
    "    'weight_round_7': 20000,  # estimated\n",
    "    'brand_encoded': 15000,  # estimated\n",
    "    'material_encoded': 12000,  # estimated\n",
    "    'size_encoded': 10000,  # estimated\n",
    "    \n",
    "    # Groupby statistics (top ones)\n",
    "    'weight_capacity_kg_mean_price': 35025,\n",
    "    'brand_mean_price': 28000,  # estimated\n",
    "    'material_mean_price': 22000,  # estimated\n",
    "    'size_mean_price': 18000,  # estimated\n",
    "    'weight_capacity_kg_count_price': 15000,  # estimated\n",
    "    'color_mean_price': 12000,  # estimated\n",
    "    \n",
    "    # Histogram bins (top ones - weight_capacity)\n",
    "    'weight_capacity_kg_hist_bin_0': 8000,  # estimated\n",
    "    'weight_capacity_kg_hist_bin_1': 7500,  # estimated\n",
    "    'weight_capacity_kg_hist_bin_2': 7200,  # estimated\n",
    "    # ... many more histogram features\n",
    "}\n",
    "\n",
    "# Add the actual reported totals\n",
    "total_histogram_importance = 1020884\n",
    "total_groupby_importance = 281304\n",
    "total_baseline_importance = 172796\n",
    "\n",
    "print(f\"Total feature importance by category:\")\n",
    "print(f\"Histogram bins: {total_histogram_importance:,}\")\n",
    "print(f\"Groupby statistics: {total_groupby_importance:,}\")\n",
    "print(f\"Baseline features: {total_baseline_importance:,}\")\n",
    "print(f\"Grand total: {total_histogram_importance + total_groupby_importance + total_baseline_importance:,}\")\n",
    "\n",
    "# Calculate percentages\n",
    "grand_total = total_histogram_importance + total_groupby_importance + total_baseline_importance\n",
    "print(f\"\\nPercentage of total importance:\")\n",
    "print(f\"Histogram bins: {total_histogram_importance/grand_total*100:.1f}%\")\n",
    "print(f\"Groupby statistics: {total_groupby_importance/grand_total*100:.1f}%\")\n",
    "print(f\"Baseline features: {total_baseline_importance/grand_total*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbd5a9",
   "metadata": {},
   "source": [
    "## 2. Feature Count vs Performance Analysis\n",
    "\n",
    "Why did 313 features perform worse than 67 features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1feb8180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:16:53.882250Z",
     "iopub.status.busy": "2026-01-15T20:16:53.881944Z",
     "iopub.status.idle": "2026-01-15T20:16:53.886000Z",
     "shell.execute_reply": "2026-01-15T20:16:53.885653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count vs CV performance:\n",
      "==================================================\n",
      "exp_003 (baseline)  :  19 features → 38.825723 RMSE\n",
      "exp_004 (groupby)   :  67 features → 38.660840 RMSE\n",
      "exp_005 (histogram) : 313 features → 38.663395 RMSE\n",
      "\n",
      "Improvement from exp_003 to exp_004: 0.164883 RMSE\n",
      "Change from exp_004 to exp_005: 0.002555 RMSE (worse by 0.002555)\n",
      "\n",
      "exp_005 feature breakdown:\n",
      "Baseline features: 15\n",
      "Groupby statistics: 48\n",
      "Histogram bins: 250\n",
      "Total: 313\n",
      "\n",
      "Added from exp_004 to exp_005: 246 features (all histogram bins)\n",
      "Performance change: +0.002555 RMSE (worse)\n",
      "Conclusion: Adding 250 histogram features hurt performance slightly\n"
     ]
    }
   ],
   "source": [
    "# Analyze the feature count progression\n",
    "experiments = {\n",
    "    'exp_003 (baseline)': {'features': 19, 'cv_score': 38.825723},\n",
    "    'exp_004 (groupby)': {'features': 67, 'cv_score': 38.660840},\n",
    "    'exp_005 (histogram)': {'features': 313, 'cv_score': 38.663395},\n",
    "}\n",
    "\n",
    "print(\"Feature count vs CV performance:\")\n",
    "print(\"=\" * 50)\n",
    "for name, data in experiments.items():\n",
    "    print(f\"{name:20s}: {data['features']:3d} features → {data['cv_score']:.6f} RMSE\")\n",
    "\n",
    "print(f\"\\nImprovement from exp_003 to exp_004: {38.825723 - 38.660840:.6f} RMSE\")\n",
    "print(f\"Change from exp_004 to exp_005: {38.663395 - 38.660840:.6f} RMSE (worse by 0.002555)\")\n",
    "\n",
    "# Calculate features per category in exp_005\n",
    "baseline_features = 15\n",
    "groupby_features = 48\n",
    "histogram_features = 250\n",
    "\n",
    "print(f\"\\nexp_005 feature breakdown:\")\n",
    "print(f\"Baseline features: {baseline_features}\")\n",
    "print(f\"Groupby statistics: {groupby_features}\")\n",
    "print(f\"Histogram bins: {histogram_features}\")\n",
    "print(f\"Total: {baseline_features + groupby_features + histogram_features}\")\n",
    "\n",
    "# Features added from exp_004 to exp_005\n",
    "print(f\"\\nAdded from exp_004 to exp_005: {313 - 67} features (all histogram bins)\")\n",
    "print(f\"Performance change: +0.002555 RMSE (worse)\")\n",
    "print(f\"Conclusion: Adding 250 histogram features hurt performance slightly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5fbac",
   "metadata": {},
   "source": [
    "## 3. Histogram Bin Analysis\n",
    "\n",
    "Analyze the histogram binning approach - 50 bins may be too granular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90539427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze histogram bin distribution\n",
    "# Let's look at the price distribution to understand if 50 bins is appropriate\n",
    "\n",
    "price_data = combined_train['Price'].values\n",
    "print(f\"Price distribution analysis:\")\n",
    "print(f\"Min: {price_data.min():.2f}\")\n",
    "print(f\"Max: {price_data.max():.2f}\")\n",
    "print(f\"Range: {price_data.max() - price_data.min():.2f}\")\n",
    "print(f\"Mean: {price_data.mean():.2f}\")\n",
    "print(f\"Median: {np.median(price_data):.2f}\")\n",
    "print(f\"Std: {price_data.std():.2f}\")\n",
    "\n",
    "# Calculate what 50 bins means\n",
    "bin_width = (price_data.max() - price_data.min()) / 50\n",
    "print(f\"\\nWith 50 uniform bins:\")\n",
    "print(f\"Each bin width: {bin_width:.2f}\")\n",
    "print(f\"Average samples per bin: {len(price_data) / 50:.0f}\")\n",
    "\n",
    "# Check distribution across groups\n",
    "print(f\"\\nGroup sizes for histogram binning:\")\n",
    "for col in ['Weight Capacity (kg)', 'Brand', 'Material', 'Size', 'Color']:\n",
    "    n_groups = combined_train[col].nunique()\n",
    "    avg_per_group = len(combined_train) / n_groups\n",
    "    print(f\"{col:20s}: {n_groups:4d} groups, avg {avg_per_group:6.0f} samples/group\")\n",
    "    \n",
    "    # For histogram bins, each group gets 50 features\n",
    "    # So total histogram features = n_groups × 50 (but we only use top groups)\n",
    "    if col in ['Weight Capacity (kg)', 'Brand', 'Material', 'Size', 'Color']:\n",
    "        print(f\"{'':20s}  → {n_groups} groups × 50 bins = {n_groups * 50} potential features\")\n",
    "\n",
    "print(f\"\\nActual histogram features used: 250 (5 group keys × 50 bins)\")\n",
    "print(f\"This means we're using histograms for: Weight Capacity, Brand, Material, Size, Color\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b3219",
   "metadata": {},
   "source": [
    "## 4. Feature Selection Strategy\n",
    "\n",
    "Based on the analysis, we need aggressive feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propose feature selection strategy\n",
    "print(\"PROPOSED FEATURE SELECTION STRATEGY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. REMOVE zero-importance features (14 features):\")\n",
    "zero_importance_features = [\n",
    "    'brand_min_price', 'brand_max_price',\n",
    "    'material_min_price', 'material_max_price', \n",
    "    'size_min_price', 'size_max_price',\n",
    "    'laptop_compartment_min_price', 'laptop_compartment_max_price',\n",
    "    'waterproof_min_price', 'waterproof_max_price',\n",
    "    'style_min_price', 'style_max_price',\n",
    "    'color_min_price', 'color_max_price'\n",
    "]\n",
    "for feat in zero_importance_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\n2. REDUCE histogram bins from 50 to 20-30:\")\n",
    "print(f\"   - Current: 250 histogram features\")\n",
    "print(f\"   - Proposed: 100-150 histogram features (5 keys × 20-30 bins)\")\n",
    "print(f\"   - Reason: 50 bins too granular, many bins capture noise\")\n",
    "\n",
    "print(f\"\\n3. FOCUS on Weight Capacity histograms (highest importance):\")\n",
    "print(f\"   - weight_capacity_kg has 58,096 importance (top feature)\")\n",
    "print(f\"   - Keep all 50 bins for Weight Capacity only\")\n",
    "print(f\"   - Reduce to 20-30 bins for Brand, Material, Size, Color\")\n",
    "\n",
    "print(f\"\\n4. KEEP top groupby statistics:\")\n",
    "print(f\"   - mean_price features (strong signal)\")\n",
    "print(f\"   - count_price features (useful for frequency)\")\n",
    "print(f\"   - median_price features (robust statistic)\")\n",
    "print(f\"   - REMOVE std, min, max (lower importance, many are zero)\")\n",
    "\n",
    "print(f\"\\n5. TARGET feature count: ~100-150 features\")\n",
    "print(f\"   - Current: 313 features (overkill)\")\n",
    "print(f\"   - Proposed: ~100-150 features (focused, less noise)\")\n",
    "print(f\"   - Expected: Better or equal performance with less overfitting\")\n",
    "\n",
    "# Calculate expected feature count\n",
    "baseline_keep = 15  # all baseline\n",
    "groupby_keep = 24  # mean, count, median for 8 keys (3 × 8 = 24)\n",
    "weight_capacity_hist = 50  # keep all 50 for weight capacity\n",
    "other_hist = 80  # 4 keys × 20 bins each\n",
    "\n",
    "print(f\"\\n6. ESTIMATED FEATURE COUNT AFTER SELECTION:\")\n",
    "print(f\"   Baseline features: {baseline_keep}\")\n",
    "print(f\"   Groupby statistics: {groupby_keep}\")\n",
    "print(f\"   Weight Capacity histogram: {weight_capacity_hist}\")\n",
    "print(f\"   Other histograms: {other_hist}\")\n",
    "print(f\"   TOTAL: {baseline_keep + groupby_keep + weight_capacity_hist + other_hist} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38cb97",
   "metadata": {},
   "source": [
    "## 5. Original Student Bag Dataset\n",
    "\n",
    "Research the original dataset that 1st place used heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfa696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research the original Student Bag dataset\n",
    "print(\"ORIGINAL STUDENT BAG DATASET RESEARCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nFrom competition description and 1st place solution:\")\n",
    "print(\"- Original dataset: 'Student Bag Price Prediction Dataset'\")\n",
    "print(\"- URL: https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset\")\n",
    "print(\"- 1st place (Chris Deotte) heavily exploited this dataset\")\n",
    "print(\"- Key insight: Compute mean Price by Weight Capacity as 'MSRP' feature\")\n",
    "\n",
    "print(\"\\nWhat the original dataset likely contains:\")\n",
    "print(\"- More samples of backpacks with prices\")\n",
    "print(\"- Additional weight capacity values\")\n",
    "print(\"- MSRP (Manufacturer's Suggested Retail Price) or similar reference prices\")\n",
    "print(\"- More granular categorization\")\n",
    "\n",
    "print(\"\\nHow 1st place used it:\")\n",
    "print(\"1. Download original dataset\")\n",
    "print(\"2. Compute mean Price by Weight Capacity (rounded to different decimals)\")\n",
    "print(\"3. Create 'MSRP' feature - reference price for each weight capacity\")\n",
    "print(\"4. Use this as a strong baseline predictor\")\n",
    "print(\"5. Combine with other features for final model\")\n",
    "\n",
    "print(\"\\nWhat we can do without the original dataset:\")\n",
    "print(\"- Compute mean Price by Weight Capacity from OUR training data\")\n",
    "print(\"- Use different rounding levels (7-10 decimals) to capture variations\")\n",
    "print(\"- This is already partially done in our baseline features\")\n",
    "print(\"- But original dataset has MORE samples → better statistics\")\n",
    "\n",
    "print(\"\\nACTION ITEM: Download original dataset and compute:\")\n",
    "print(\"- mean_price_by_weight_capacity (multiple rounding levels)\")\n",
    "print(\"- std_price_by_weight_capacity (price variance)\")\n",
    "print(\"- count_by_weight_capacity (sample size)\")\n",
    "print(\"- Merge these as additional features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aae26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute what we can from our current data\n",
    "print(\"COMPUTING WEIGHT CAPACITY STATISTICS FROM TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by Weight Capacity and compute statistics\n",
    "weight_stats = combined_train.groupby('Weight Capacity (kg)')['Price'].agg([\n",
    "    ('mean_price', 'mean'),\n",
    "    ('std_price', 'std'),\n",
    "    ('count_price', 'count'),\n",
    "    ('min_price', 'min'),\n",
    "    ('max_price', 'max'),\n",
    "    ('median_price', 'median')\n",
    "]).reset_index()\n",
    "\n",
    "print(f\"Unique Weight Capacity values: {len(weight_stats)}\")\n",
    "print(f\"Weight Capacity range: {weight_stats['Weight Capacity (kg)'].min():.10f} - {weight_stats['Weight Capacity (kg)'].max():.10f}\")\n",
    "\n",
    "# Show top weight capacities by count\n",
    "print(f\"\\nTop 10 Weight Capacities by sample count:\")\n",
    "top_weights = weight_stats.nlargest(10, 'count_price')\n",
    "for idx, row in top_weights.iterrows():\n",
    "    print(f\"  {row['Weight Capacity (kg)']:.10f}: {row['count_price']:6.0f} samples, mean price = {row['mean_price']:6.2f}\")\n",
    "\n",
    "# Show price variation\n",
    "print(f\"\\nPrice variation by Weight Capacity:\")\n",
    "print(f\"Mean price range: {weight_stats['mean_price'].min():.2f} - {weight_stats['mean_price'].max():.2f}\")\n",
    "print(f\"Std dev range: {weight_stats['std_price'].min():.2f} - {weight_stats['std_price'].max():.2f}\")\n",
    "\n",
    "# Correlation between weight capacity and mean price\n",
    "corr = np.corrcoef(weight_stats['Weight Capacity (kg)'], weight_stats['mean_price'])[0,1]\n",
    "print(f\"Correlation (weight capacity vs mean price): {corr:.4f}\")\n",
    "\n",
    "print(f\"\\nThis confirms Weight Capacity is a strong predictor!\")\n",
    "print(f\"The original dataset would have MORE samples per weight capacity,\")\n",
    "print(f\"giving us more reliable statistics (especially for rare weights).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze potential reasons why histogram bins didn't improve performance\n",
    "# despite high feature importance\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HYPOTHESIS 1: Overfitting to histogram bins despite high importance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# The histogram bins might be capturing noise rather than signal\n",
    "# High feature importance doesn't always mean better generalization\n",
    "print(\"Key insight: Feature importance measures contribution to training fit,\")\n",
    "print(\"not necessarily contribution to generalization/validation performance.\")\n",
    "print()\n",
    "\n",
    "# Check if we're overfitting\n",
    "print(\"Overfitting indicators:\")\n",
    "print(f\"- Feature count increased from 67 → 313 (+246 features)\")\n",
    "print(f\"- CV score got worse by 0.002555 despite more features\")\n",
    "print(f\"- Histogram bins: 69.2% of feature importance but no score improvement\")\n",
    "print()\n",
    "\n",
    "# Calculate feature importance per feature\n",
    "baseline_imp_per_feat = 172796 / 15  # ~11,520 per feature\n",
    "groupby_imp_per_feat = 281304 / 48   # ~5,860 per feature  \n",
    "histogram_imp_per_feat = 1020884 / 250  # ~4,083 per feature\n",
    "\n",
    "print(\"Average importance per feature by category:\")\n",
    "print(f\"Baseline features:     {baseline_imp_per_feat:,.0f} per feature\")\n",
    "print(f\"Groupby statistics:    {groupby_imp_per_feat:,.0f} per feature\")\n",
    "print(f\"Histogram bins:        {histogram_imp_per_feat:,.0f} per feature\")\n",
    "print()\n",
    "print(\"Conclusion: Histogram bins have LOWER average importance per feature\")\n",
    "print(\"despite high total importance. Many weak features = overfitting risk.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
