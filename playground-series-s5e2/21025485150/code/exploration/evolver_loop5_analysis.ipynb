{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4370925",
   "metadata": {},
   "source": [
    "# Evolver Loop 5 Analysis: Debugging Feature Importance and Adding Histogram Binning\n",
    "\n",
    "## Objectives\n",
    "1. Debug the zero feature importance issue in exp_004\n",
    "2. Implement histogram binning (1st place technique)\n",
    "3. Fix potential feature name issues\n",
    "4. Prepare for next experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the features from exp_004 to debug\n",
    "print(\"Loading exp_004 features...\")\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "\n",
    "# Recreate features from exp_004 (simplified version)\n",
    "def create_debug_features():\n",
    "    \"\"\"Recreate exp_004 features to debug\"\"\"\n",
    "    \n",
    "    # Baseline features\n",
    "    features = pd.DataFrame(index=combined_train.index)\n",
    "    weight = combined_train['Weight Capacity (kg)'].copy()\n",
    "    \n",
    "    features['weight_original'] = weight\n",
    "    for dec in range(7, 11):\n",
    "        features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "    \n",
    "    # Simple groupby feature (just mean to test)\n",
    "    weight_capacity_mean = combined_train.groupby('Weight Capacity (kg)')['Price'].mean()\n",
    "    features['weight_capacity_mean_price'] = weight.map(weight_capacity_mean)\n",
    "    \n",
    "    return features\n",
    "\n",
    "X_debug = create_debug_features()\n",
    "print(f\"Debug features shape: {X_debug.shape}\")\n",
    "print(f\"Feature names: {X_debug.columns.tolist()}\")\n",
    "print(f\"\\nSample values:\")\n",
    "print(X_debug.head())\n",
    "\n",
    "# Check for NaNs and constants\n",
    "print(f\"\\nNaN counts:\")\n",
    "print(X_debug.isnull().sum())\n",
    "\n",
    "print(f\"\\nValue ranges:\")\n",
    "for col in X_debug.columns:\n",
    "    print(f\"{col}: {X_debug[col].min():.6f} - {X_debug[col].max():.6f} (std: {X_debug[col].std():.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fba9d",
   "metadata": {},
   "source": [
    "## Test Feature Importance Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test XGBoost with debug features\n",
    "y = combined_train['Price'].values\n",
    "\n",
    "# Simple train/validation split\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = list(kf.split(X_debug))[0]\n",
    "\n",
    "X_train, X_val = X_debug.iloc[train_idx], X_debug.iloc[val_idx]\n",
    "y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "# Train XGBoost\n",
    "model = xgb.XGBRegressor(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Try different importance extraction methods\n",
    "print(\"Method 1: model.get_booster().get_score()\")\n",
    "try:\n",
    "    importance1 = model.get_booster().get_score(importance_type='gain')\n",
    "    print(f\"Importance dict: {importance1}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nMethod 2: model.get_booster().get_score() with fmap\")\n",
    "try:\n",
    "    # Create feature map\n",
    "    feature_names = X_debug.columns.tolist()\n",
    "    with open('/tmp/feature_map.txt', 'w') as f:\n",
    "        for i, name in enumerate(feature_names):\n",
    "            # Clean feature names - remove special characters\n",
    "            clean_name = name.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
    "            f.write(f'{i}\\t{clean_name}\\t{0}\\tq\\n')\n",
    "    \n",
    "    importance2 = model.get_booster().get_score(fmap='/tmp/feature_map.txt', importance_type='gain')\n",
    "    print(f\"Importance dict: {importance2}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nMethod 3: model.feature_importances_\")\n",
    "try:\n",
    "    importance3 = model.feature_importances_\n",
    "    for name, imp in zip(X_debug.columns, importance3):\n",
    "        print(f\"{name}: {imp:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Test predictions to verify model is working\n",
    "preds = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "print(f\"\\nValidation RMSE: {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed35232",
   "metadata": {},
   "source": [
    "## Test with Cleaned Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6aef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with cleaned feature names (no special characters)\n",
    "def create_clean_features():\n",
    "    \"\"\"Create features with cleaned names\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=combined_train.index)\n",
    "    weight = combined_train['Weight Capacity (kg)'].copy()\n",
    "    \n",
    "    # Clean names - no special characters\n",
    "    features['weight_original'] = weight\n",
    "    for dec in range(7, 11):\n",
    "        features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "    \n",
    "    # Groupby with cleaned name\n",
    "    weight_capacity_mean = combined_train.groupby('Weight Capacity (kg)')['Price'].mean()\n",
    "    features['weight_capacity_mean_price'] = weight.map(weight_capacity_mean)\n",
    "    \n",
    "    return features\n",
    "\n",
    "X_clean = create_clean_features()\n",
    "\n",
    "# Rename columns to be XGBoost-friendly\n",
    "clean_names = {}\n",
    "for col in X_clean.columns:\n",
    "    clean_name = col.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
    "    clean_names[col] = clean_name\n",
    "\n",
    "X_clean_renamed = X_clean.rename(columns=clean_names)\n",
    "print(f\"Cleaned feature names: {X_clean_renamed.columns.tolist()}\")\n",
    "\n",
    "# Train with cleaned names\n",
    "X_train_clean = X_clean_renamed.iloc[train_idx]\n",
    "model_clean = xgb.XGBRegressor(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_clean.fit(X_train_clean, y_train)\n",
    "\n",
    "print(\"\\nFeature importance with cleaned names:\")\n",
    "importance_clean = model_clean.get_booster().get_score(importance_type='gain')\n",
    "for name, imp in importance_clean.items():\n",
    "    print(f\"{name}: {imp:.6f}\")\n",
    "\n",
    "# Check if any features have zero importance\n",
    "zero_importance = [name for name, imp in importance_clean.items() if imp == 0]\n",
    "if zero_importance:\n",
    "    print(f\"\\nFeatures with zero importance: {zero_importance}\")\n",
    "else:\n",
    "    print(\"\\nAll features have non-zero importance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2352a",
   "metadata": {},
   "source": [
    "## Implement Histogram Binning\n",
    "\n",
    "Based on 1st place solution by Chris Deotte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47853ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram_features(train_df, test_df, target_col='Price', group_col='Weight Capacity (kg)', n_bins=50):\n",
    "    \"\"\"\n",
    "    Create histogram bin features following 1st place solution\n",
    "    Pattern: groupby(COL1)[COL2].apply(make_histogram)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create bins based on training data\n",
    "    price_bins = np.percentile(train_df[target_col], np.linspace(0, 100, n_bins+1))\n",
    "    print(f\"Creating {n_bins} histogram bins for {group_col}\")\n",
    "    print(f\"Price bin edges: {price_bins[:5]}...{price_bins[-5:]}\")\n",
    "    \n",
    "    def make_histogram(series):\n",
    "        \"\"\"Create histogram counts for a series\"\"\"\n",
    "        counts, _ = np.histogram(series, bins=price_bins)\n",
    "        # Return as Series with bin names\n",
    "        return pd.Series(counts, index=[f'hist_bin_{i}' for i in range(n_bins)])\n",
    "    \n",
    "    # Compute histograms for each group\n",
    "    print(f\"Computing histograms for each {group_col} group...\")\n",
    "    group_histograms = train_df.groupby(group_col)[target_col].apply(make_histogram)\n",
    "    \n",
    "    # group_histograms is a Series with MultiIndex (group_col, bin_name)\n",
    "    # Convert to DataFrame\n",
    "    group_histograms_df = group_histograms.unstack(level=-1)\n",
    "    print(f\"Histogram features shape: {group_histograms_df.shape}\")\n",
    "    \n",
    "    # Merge to train data\n",
    "    train_with_hist = train_df[[group_col]].merge(\n",
    "        group_histograms_df, \n",
    "        left_on=group_col, \n",
    "        right_index=True, \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # For test data, use same histograms computed from train\n",
    "    test_with_hist = test_df[[group_col]].merge(\n",
    "        group_histograms_df,\n",
    "        left_on=group_col,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaN for groups not seen in training\n",
    "    hist_features_train = train_with_hist.drop(columns=[group_col])\n",
    "    hist_features_test = test_with_hist.drop(columns=[group_col])\n",
    "    \n",
    "    hist_features_train = hist_features_train.fillna(0)\n",
    "    hist_features_test = hist_features_test.fillna(0)\n",
    "    \n",
    "    return hist_features_train, hist_features_test\n",
    "\n",
    "# Test histogram features\n",
    "print(\"Testing histogram feature creation...\")\n",
    "hist_train, hist_test = create_histogram_features(\n",
    "    combined_train, test, \n",
    "    target_col='Price', \n",
    "    group_col='Weight Capacity (kg)', \n",
    "    n_bins=20  # Use 20 bins for faster testing\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain histogram features shape: {hist_train.shape}\")\n",
    "print(f\"Test histogram features shape: {hist_test.shape}\")\n",
    "print(f\"\\nSample histogram features:\")\n",
    "print(hist_train.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32cc0df",
   "metadata": {},
   "source": [
    "## Test Combined Features (Baseline + Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine baseline and histogram features\n",
    "X_combined = pd.concat([X_clean_renamed, hist_train], axis=1)\n",
    "print(f\"Combined features shape: {X_combined.shape}\")\n",
    "\n",
    "# Train with combined features\n",
    "X_train_combined = X_combined.iloc[train_idx]\n",
    "model_combined = xgb.XGBRegressor(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "# Check importance\n",
    "print(\"\\nTop 10 features by importance:\")\n",
    "importance_combined = model_combined.get_booster().get_score(importance_type='gain')\n",
    "# Sort by importance\n",
    "sorted_importance = sorted(importance_combined.items(), key=lambda x: x[1], reverse=True)\n",
    "for name, imp in sorted_importance[:10]:\n",
    "    print(f\"{name}: {imp:.6f}\")\n",
    "\n",
    "# Check if histogram features have importance\n",
    "hist_feature_names = [col for col in X_combined.columns if col.startswith('hist_bin_')]\n",
    "hist_importance = [(name, importance_combined.get(name, 0)) for name in hist_feature_names if importance_combined.get(name, 0) > 0]\n",
    "print(f\"\\nHistogram features with non-zero importance: {len(hist_importance)}\")\n",
    "if hist_importance:\n",
    "    print(\"Top histogram features:\")\n",
    "    for name, imp in sorted(hist_importance, key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  {name}: {imp:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377c7f9",
   "metadata": {},
   "source": [
    "## Key Findings for Next Experiment\n",
    "\n",
    "Based on this debugging analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS FOR EXPERIMENT 005\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Feature names with special characters (spaces, parentheses) cause issues\")\n",
    "print(\"   - Clean names: weight_capacity_mean_price (works)\")\n",
    "print(\"   - Original names: 'Weight Capacity (kg)_mean_price' (may fail)\")\n",
    "print(\"\\n2. Histogram binning is implementable and creates valuable features\")\n",
    "print(\"   - 20 bins create 20 features per group key\")\n",
    "print(\"   - Some histogram bins show non-zero importance\")\n",
    "print(\"\\n3. Combined approach (baseline + histogram) is promising\")\n",
    "print(\"   - Feature importance extraction works with cleaned names\")\n",
    "print(\"   - Can identify which features are actually valuable\")\n",
    "print(\"\\n4. Next experiment should:\")\n",
    "print(\"   - Use cleaned feature names throughout\")\n",
    "print(\"   - Implement histogram binning (50 bins as per 1st place)\")\n",
    "print(\"   - Apply to multiple group keys (Weight Capacity, Brand, etc.)\")\n",
    "print(\"   - Monitor feature importance to validate features are being used\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
