{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"sourceType":"competition"},{"sourceId":9198133,"sourceType":"datasetVersion","datasetId":5560970},{"sourceId":223071113,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":6047.948412,"end_time":"2025-02-27T03:18:15.230472","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-27T01:37:27.28206","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First Place - Single Model - LB 38.81\nI'm excited to share my `First Place - Gold Medal - Single Model` solution in Kaggle's playground backpack prediction competition. We begin with my starter notebook [here][1] and then we add more engineered features using [RAPIDS cuDF-Pandas][2]. The notebook you are reading has 138 features. This is a simplfied version of my actual final solution which has 500 features! None-the-less, the notebook you are reading achieves first place accuracy!\n\nA simple way to boost the performance of the notebook you are reading is to train 20 folds inside 20 folds instead of 7 inside 7 (or 10 in 10, or 15 in 15). And we can decrease XGBoost learning rate from 0.01 to 0.005 (and change early stopping to 1000). This will of course increase runtime but it will also improve model accuracy too!\n\nAnother way to boost this notebook's accuracy is to include the additional 362 features from my actual final solution. The text below is the introduction from my original starter notebook:\n\n[1]: https://www.kaggle.com/code/cdeotte/feature-engineering-with-rapids-lb-38-847\n[2]: https://rapids.ai/cudf-pandas/","metadata":{"papermill":{"duration":0.008457,"end_time":"2025-02-27T01:37:32.074015","exception":false,"start_time":"2025-02-27T01:37:32.065558","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Feature Engineering with Fast cuDF-Pandas!\nOne of the most powerful feature engineering techniques is `groupby(COL1)[COL2].agg(STAT)`. This is where we group by `COL1` column and aggregate (i.e. compute) a statistic `STAT` over another column `COL2`. This is the underlying method to compute `target encoding` and `count encoding`. By computing raw statistics and inputting them into our model, our model can do more than only receiving `TE` or `CE`. This notebook illustrates creating 50 engineered features, but we can create hundreds more and improve CV score and LB score!\n\nWhen our dataset has millions of rows like Kaggle's Backpack competition, then `groupby` operations take time to compute. The fastest way to compute a `groupby` aggregation is to use GPU with [RAPIDS cuDF-Pandas][1] library.\n\nThere are two ways to use [RAPIDS cuDF][2]. We can write cuDF code which looks just like Pandas code and starts with `import cudf`. Or we can write normal Pandas code with `import pandas` but before that we add the cell magic command `%load_ext cudf.pandas`. By adding this magic command all calls to Pandas afterward will use [RAPIDS cuDF][2] behind the scenes taking advantage of the massive speed boost of GPU!\n\nAlternatively, we can use [cuDF-Polars][3]. To use [cuDF-Polars][3], we write Polars code with lazy frame. Then the final call includes `.collect(engine=\"gpu\")` which will run all previous Polars code behind the scenes with [RAPIDS cuDF][2].\n\n[1]: https://rapids.ai/cudf-pandas/\n[2]: https://docs.rapids.ai/install/\n[3]: https://rapids.ai/polars-gpu-engine/","metadata":{"papermill":{"duration":0.0058,"end_time":"2025-02-27T01:37:32.086585","exception":false,"start_time":"2025-02-27T01:37:32.080785","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# RAPIDS v25.02\n[RAPIDS v25.02][1] was just released Feb 15, 2025! Instructions on installing RAPIDS is [here][1]. On Kaggle, the easiest way to pip install new libraries is to do it once in a `Utility Script` notebook. Then whenever we attach the `Utility Script` notebook to another Kaggle notebook, the second Kaggle notebook immediately gets the benefit of the pip installed libraries. We created a [RAPIDS 25.02][1] `Utility Script` [here][2], and we attach it to the notebook you are reading. Therefore the notebook you are reading can import RAPIDS v25.02 without needing to pip install!\n\n[1]: https://docs.rapids.ai/install/\n[2]: https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02","metadata":{"papermill":{"duration":0.005745,"end_time":"2025-02-27T01:37:32.099893","exception":false,"start_time":"2025-02-27T01:37:32.094148","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# GPU Acceleration\nWe activate [cuDF-Pandas][1] with the magic command `%load_ext cudf.pandas` below. Afterward, all calls to Pandas will use fast GPU [RAPIDS cuDF][2] behind the scenes! Since we attached `Utility Script` notebook [here][3] to the notebook you are reading, we will be using the new [RAPIDS v25.02][2]!\n\n[1]: https://rapids.ai/cudf-pandas/\n[2]: https://docs.rapids.ai/install/\n[3]: https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02","metadata":{"papermill":{"duration":0.005728,"end_time":"2025-02-27T01:37:32.111665","exception":false,"start_time":"2025-02-27T01:37:32.105937","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%load_ext cudf.pandas\n\nimport numpy as np, pandas as pd, gc\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 500)\n\nVER=1","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:09.066785Z","iopub.execute_input":"2025-02-27T05:21:09.067126Z","iopub.status.idle":"2025-02-27T05:21:18.179861Z","shell.execute_reply.started":"2025-02-27T05:21:09.067099Z","shell.execute_reply":"2025-02-27T05:21:18.179095Z"},"papermill":{"duration":37.823872,"end_time":"2025-02-27T01:38:09.941462","exception":false,"start_time":"2025-02-27T01:37:32.11759","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data\nWe load train, train extra, and test data. The combined train data has 4 million rows! This means we do not need to fear overfitting train. We can make hundreds/thousands of new features and every time our CV improves our LB will improve too!","metadata":{"papermill":{"duration":0.005763,"end_time":"2025-02-27T01:38:09.953867","exception":false,"start_time":"2025-02-27T01:38:09.948104","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e2/train.csv\")\nprint(\"Train shape\", train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:18.180818Z","iopub.execute_input":"2025-02-27T05:21:18.181362Z","iopub.status.idle":"2025-02-27T05:21:18.466414Z","shell.execute_reply.started":"2025-02-27T05:21:18.181337Z","shell.execute_reply":"2025-02-27T05:21:18.465469Z"},"papermill":{"duration":0.850698,"end_time":"2025-02-27T01:38:10.810724","exception":false,"start_time":"2025-02-27T01:38:09.960026","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2 = pd.read_csv(\"/kaggle/input/playground-series-s5e2/training_extra.csv\")\nprint(\"Extra Train shape\", train2.shape )\ntrain2.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:18.468348Z","iopub.execute_input":"2025-02-27T05:21:18.468589Z","iopub.status.idle":"2025-02-27T05:21:18.828356Z","shell.execute_reply.started":"2025-02-27T05:21:18.468568Z","shell.execute_reply":"2025-02-27T05:21:18.827471Z"},"papermill":{"duration":1.259661,"end_time":"2025-02-27T01:38:12.077263","exception":false,"start_time":"2025-02-27T01:38:10.817602","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.concat([train,train2],axis=0,ignore_index=True)\nprint(\"Combined Train shape\", train.shape)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:18.829577Z","iopub.execute_input":"2025-02-27T05:21:18.829837Z","iopub.status.idle":"2025-02-27T05:21:18.857784Z","shell.execute_reply.started":"2025-02-27T05:21:18.829815Z","shell.execute_reply":"2025-02-27T05:21:18.857039Z"},"papermill":{"duration":0.118584,"end_time":"2025-02-27T01:38:12.206821","exception":false,"start_time":"2025-02-27T01:38:12.088237","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/playground-series-s5e2/test.csv\")\nprint(\"Test shape\", test.shape )\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:18.85864Z","iopub.execute_input":"2025-02-27T05:21:18.858881Z","iopub.status.idle":"2025-02-27T05:21:18.920296Z","shell.execute_reply.started":"2025-02-27T05:21:18.858859Z","shell.execute_reply":"2025-02-27T05:21:18.919589Z"},"papermill":{"duration":0.198488,"end_time":"2025-02-27T01:38:12.41211","exception":false,"start_time":"2025-02-27T01:38:12.213622","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer Columns\n\nFirst we will engineer 8 new columns by combining each categorical column with Weight Capacity as was done in my starter notebook. Next we engineer 63 more columns listed below:\n* One column to indicate all NANs using a base-2 encoding\n* NANs per feature combined with Weight Capacity\n* Bin Weight Capacity by rounding it in different ways\n* Merge original dataset price\n* Extract digits from Weight Capacity\n* Combine digit features\n* Combine original categorical columns","metadata":{}},{"cell_type":"code","source":"CATS = list(train.columns[1:-2])\nprint(f\"There are {len(CATS)} categorical columns:\")\nprint( CATS )\nprint(f\"There are 1 numerical column:\")\nprint( [\"Weight Capacity (kg)\"] )","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:18.921039Z","iopub.execute_input":"2025-02-27T05:21:18.921277Z","iopub.status.idle":"2025-02-27T05:21:18.931734Z","shell.execute_reply.started":"2025-02-27T05:21:18.921257Z","shell.execute_reply":"2025-02-27T05:21:18.93058Z"},"papermill":{"duration":0.02097,"end_time":"2025-02-27T01:38:12.45397","exception":false,"start_time":"2025-02-27T01:38:12.433","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"COMBO = [\"NaNs\"]\ntrain[\"NaNs\"] = np.float32(0)\ntest[\"NaNs\"] = np.float32(0)\n\nfor i,c in enumerate(CATS):\n\n    # NEW FEATURE - ENCODE ALL NAN AS ONE BASE-2 FEATURE\n    train[\"NaNs\"] += train[c].isna()*2**i\n    test[\"NaNs\"] += test[c].isna()*2**i\n\n    # NEW FEATURE - COMBINE EACH COLUMN'S NAN WITH WEIGHT CAPACITY\n    n = f\"{c}_nan_wc\"\n    train[n] = train[c].isna()*100 + train[\"Weight Capacity (kg)\"]\n    test[n] = test[c].isna()*100 + test[\"Weight Capacity (kg)\"]\n    COMBO.append(n)\n    \n    combine = pd.concat([train[c],test[c]],axis=0)\n    combine,_ = pd.factorize(combine)\n    train[c] = combine[:len(train)].astype(\"float32\")\n    test[c] = combine[len(train):].astype(\"float32\")\n    n = f\"{c}_wc\"\n    train[n] = train[c]*100 + train[\"Weight Capacity (kg)\"]\n    test[n] = test[c]*100 + test[\"Weight Capacity (kg)\"]\n    COMBO.append(n)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:18.932844Z","iopub.execute_input":"2025-02-27T05:21:18.933178Z","iopub.status.idle":"2025-02-27T05:21:19.970233Z","shell.execute_reply.started":"2025-02-27T05:21:18.933132Z","shell.execute_reply":"2025-02-27T05:21:19.96923Z"},"papermill":{"duration":3.178897,"end_time":"2025-02-27T01:38:15.639301","exception":false,"start_time":"2025-02-27T01:38:12.460404","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW FEATURE - BIN WEIGHT CAPACITY USING ROUNDING\nfor k in range(7,10):\n    n = f\"round{k}\"\n    train[n] = train[\"Weight Capacity (kg)\"].round(k)\n    test[n] = test[\"Weight Capacity (kg)\"].round(k)\n    COMBO.append(n)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:19.971266Z","iopub.execute_input":"2025-02-27T05:21:19.971559Z","iopub.status.idle":"2025-02-27T05:21:19.987481Z","shell.execute_reply.started":"2025-02-27T05:21:19.97153Z","shell.execute_reply":"2025-02-27T05:21:19.986735Z"},"papermill":{"duration":0.074026,"end_time":"2025-02-27T01:38:15.7212","exception":false,"start_time":"2025-02-27T01:38:15.647174","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW FEATURE - ORIGINAL DATASET PRICE\nNEW_COLS = []\norig = pd.read_csv(\"/kaggle/input/student-bag-price-prediction-dataset/Noisy_Student_Bag_Price_Prediction_Dataset.csv\")\ntmp = orig.groupby(\"Weight Capacity (kg)\").Price.mean()\ntmp.name = \"orig_price\"\ntrain = train.merge(tmp, on=\"Weight Capacity (kg)\", how=\"left\")\ntest = test.merge(tmp, on=\"Weight Capacity (kg)\", how=\"left\")\nNEW_COLS.append(\"orig_price\")","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:19.99053Z","iopub.execute_input":"2025-02-27T05:21:19.990822Z","iopub.status.idle":"2025-02-27T05:21:20.200009Z","shell.execute_reply.started":"2025-02-27T05:21:19.990802Z","shell.execute_reply":"2025-02-27T05:21:20.199355Z"},"papermill":{"duration":0.524302,"end_time":"2025-02-27T01:38:16.252588","exception":false,"start_time":"2025-02-27T01:38:15.728286","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW FEATURE - ORIGINAL DATASET PRICE FROM ROUNDED WEIGHT CAPACITY \nfor k in range(7,10):\n    n = f\"round{k}\"\n    orig[n] = orig[\"Weight Capacity (kg)\"].round(k)\n    tmp = orig.groupby(n).Price.mean()\n    tmp.name = f\"orig_price_r{k}\"\n    train = train.merge(tmp, on=n, how=\"left\")\n    test = test.merge(tmp, on=n, how=\"left\")\n    NEW_COLS.append(f\"orig_price_r{k}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:20.201112Z","iopub.execute_input":"2025-02-27T05:21:20.201336Z","iopub.status.idle":"2025-02-27T05:21:20.414103Z","shell.execute_reply.started":"2025-02-27T05:21:20.201318Z","shell.execute_reply":"2025-02-27T05:21:20.413175Z"},"papermill":{"duration":0.311771,"end_time":"2025-02-27T01:38:16.571633","exception":false,"start_time":"2025-02-27T01:38:16.259862","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW FEATURE - DIGIT EXTRACTION FROM WEIGHT CAPACITY\nfor k in range(1,10):\n    train[f'digit{k}'] = ((train['Weight Capacity (kg)'] * 10**k) % 10).fillna(-1).astype(\"int8\")\n    test[f'digit{k}'] = ((test['Weight Capacity (kg)'] * 10**k) % 10).fillna(-1).astype(\"int8\")\nDIGITS = [f\"digit{k}\" for k in range(1,10)]","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:20.41502Z","iopub.execute_input":"2025-02-27T05:21:20.415329Z","iopub.status.idle":"2025-02-27T05:21:20.571557Z","shell.execute_reply.started":"2025-02-27T05:21:20.4153Z","shell.execute_reply":"2025-02-27T05:21:20.57086Z"},"papermill":{"duration":0.351268,"end_time":"2025-02-27T01:38:16.93051","exception":false,"start_time":"2025-02-27T01:38:16.579242","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW FEATURE - COMBINATIONS OF DIGITS \nfor i in range(4):\n    for j in range(i+1,5):\n        n = f\"digit_{i+1}_{j+1}\"\n        train[n] = ((train[f'digit{i+1}']+1)*11 + train[f'digit{j+1}']+1).astype(\"int8\")\n        test[n] = ((test[f'digit{i+1}']+1)*11 + test[f'digit{j+1}']+1).astype(\"int8\")\n        COMBO.append(n)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:20.572384Z","iopub.execute_input":"2025-02-27T05:21:20.572695Z","iopub.status.idle":"2025-02-27T05:21:20.644334Z","shell.execute_reply.started":"2025-02-27T05:21:20.572664Z","shell.execute_reply":"2025-02-27T05:21:20.643666Z"},"papermill":{"duration":0.070447,"end_time":"2025-02-27T01:38:17.008149","exception":false,"start_time":"2025-02-27T01:38:16.937702","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NEW FEATURE - COMBINATIONS OF CATS\nPAIRS = []\nfor i,c1 in enumerate(CATS[:-1]):\n    for j,c2 in enumerate(CATS[i+1:]):\n        n = f\"{c1}_{c2}\"\n        m1 = train[c1].max()+1\n        m2 = train[c2].max()+1\n        train[n] = ((train[c1]+1 + (train[c2]+1)/(m2+1))*(m2+1)).astype(\"int8\")\n        test[n] = ((test[c1]+1 + (test[c2]+1)/(m2+1))*(m2+1)).astype(\"int8\")\n        COMBO.append(n)\n        PAIRS.append(n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:21:20.644994Z","iopub.execute_input":"2025-02-27T05:21:20.645272Z","iopub.status.idle":"2025-02-27T05:21:21.017576Z","shell.execute_reply.started":"2025-02-27T05:21:20.645234Z","shell.execute_reply":"2025-02-27T05:21:21.016947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"New Train shape:\", train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:21.018325Z","iopub.execute_input":"2025-02-27T05:21:21.018542Z","iopub.status.idle":"2025-02-27T05:21:21.121867Z","shell.execute_reply.started":"2025-02-27T05:21:21.018523Z","shell.execute_reply":"2025-02-27T05:21:21.120905Z"},"papermill":{"duration":0.145323,"end_time":"2025-02-27T01:38:17.160769","exception":false,"start_time":"2025-02-27T01:38:17.015446","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = CATS + [\"Weight Capacity (kg)\"] + COMBO + DIGITS + NEW_COLS\nprint(f\"We now have {len(FEATURES)} columns:\")\nprint( FEATURES )","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:21.122814Z","iopub.execute_input":"2025-02-27T05:21:21.123096Z","iopub.status.idle":"2025-02-27T05:21:21.127608Z","shell.execute_reply.started":"2025-02-27T05:21:21.123077Z","shell.execute_reply":"2025-02-27T05:21:21.126947Z"},"papermill":{"duration":0.015795,"end_time":"2025-02-27T01:38:17.184442","exception":false,"start_time":"2025-02-27T01:38:17.168647","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost with Feature Engineer GroupBy\nWe train XGBoost with nested folds. We use the inner nested fold to create new features that aggregate the target `price`. We must do this to prevent target leakage. And we use the outer fold to create new features that do not aggregate the target `price`. In each k fold loop, we engineer new features using the advanced feature engineering technique `groupby(COL1)[COL2].agg(STAT)`. Since we are using [RAPIDS cuDF-Pandas][1], these groupby computations will run fast on GPU! And we will train our model quickly on GPU using XGBoost!\n\n**NEW FEATURES** Compared with our starter notebook, we add two new types of aggregations. Namely we groupby and compute quantiles. And we groupby and compute histogram bin counts! We also create two new division features. We divide aggregated count by nunique. And we divide aggregated std by count. (We also removed some features from my starter notebook which saves memory).\n\n**UPDATE** We reduce all engineered features to `float32` to reduce memory usage. (But we keep original column Weight Capacity (and it's combinations) as `float64` so as not to lose the original digits). \n\n[1]: https://rapids.ai/cudf-pandas/","metadata":{"papermill":{"duration":0.007909,"end_time":"2025-02-27T01:38:17.200232","exception":false,"start_time":"2025-02-27T01:38:17.192323","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nprint(f\"XGBoost version\",xgb.__version__)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:21.128438Z","iopub.execute_input":"2025-02-27T05:21:21.128714Z","iopub.status.idle":"2025-02-27T05:21:23.222017Z","shell.execute_reply.started":"2025-02-27T05:21:21.128687Z","shell.execute_reply":"2025-02-27T05:21:23.221256Z"},"papermill":{"duration":6.874857,"end_time":"2025-02-27T01:38:24.08303","exception":false,"start_time":"2025-02-27T01:38:17.208173","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STATISTICS TO AGGEGATE FOR OUR FEATURE GROUPS\nSTATS = [\"mean\",\"std\",\"count\",\"nunique\",\"median\",\"min\",\"max\",\"skew\"]\nSTATS2 = [\"mean\"]","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:23.222867Z","iopub.execute_input":"2025-02-27T05:21:23.22332Z","iopub.status.idle":"2025-02-27T05:21:23.227255Z","shell.execute_reply.started":"2025-02-27T05:21:23.223295Z","shell.execute_reply":"2025-02-27T05:21:23.226335Z"},"papermill":{"duration":0.014772,"end_time":"2025-02-27T01:38:24.105978","exception":false,"start_time":"2025-02-27T01:38:24.091206","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# QUANTILES AND HISTOGRAM BINS TO AGGREGATE\nBINS=10\nQUANTILES = [5,10,40,45,55,60,90,95]\ndef make_histogram(prices, bins=BINS, range_min=15, range_max=150):\n    hist, _ = np.histogram(prices, bins=bins, range=(range_min, range_max))\n    return hist","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:23.228074Z","iopub.execute_input":"2025-02-27T05:21:23.228385Z","iopub.status.idle":"2025-02-27T05:21:23.242016Z","shell.execute_reply.started":"2025-02-27T05:21:23.228363Z","shell.execute_reply":"2025-02-27T05:21:23.24131Z"},"papermill":{"duration":0.014334,"end_time":"2025-02-27T01:38:24.127947","exception":false,"start_time":"2025-02-27T01:38:24.113613","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nFOLDS = 7\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\noof = np.zeros((len(train)))\npred = np.zeros((len(test)))\n\n# OUTER K FOLD\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    print(f\"### OUTER Fold {i+1} ###\")\n\n    X_train = train.loc[train_index,FEATURES+['Price']].reset_index(drop=True).copy()\n    y_train = train.loc[train_index,'Price']\n\n    X_valid = train.loc[test_index,FEATURES].reset_index(drop=True).copy()\n    y_valid = train.loc[test_index,'Price']\n\n    X_test = test[FEATURES].reset_index(drop=True).copy()\n\n    # INNER K FOLD (TO PREVENT LEAKAGE WHEN USING PRICE)\n    kf2 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)   \n    for j, (train_index2, test_index2) in enumerate(kf2.split(X_train)):\n        print(f\" ## INNER Fold {j+1} (outer fold {i+1}) ##\")\n\n        X_train2 = X_train.loc[train_index2,FEATURES+['Price']].copy()\n        X_valid2 = X_train.loc[test_index2,FEATURES].copy()\n\n        ### FEATURE SET 1 (uses price) ###\n        col = \"Weight Capacity (kg)\"\n        tmp = X_train2.groupby(col).Price.agg(STATS)\n        tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n        X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n        for c in tmp.columns:\n            X_train.loc[test_index2,c] = X_valid2[c].values.astype(\"float32\")\n\n        ### FEATURE SET 2 (uses price) ###\n        for col in COMBO:\n            tmp = X_train2.groupby(col).Price.agg(STATS2)\n            tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n            X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n            for c in tmp.columns:\n                X_train.loc[test_index2,c] = X_valid2[c].values.astype(\"float32\")\n\n        # AGGREGATE QUANTILES (uses price)\n        for k in QUANTILES:\n            result = X_train2.groupby('Weight Capacity (kg)').agg({'Price': lambda x: x.quantile(k/100)})\n            result.columns = [f\"quantile_{k}\"]\n            X_valid2 = X_valid2.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n            X_train.loc[test_index2,f\"quantile_{k}\"] = X_valid2[f\"quantile_{k}\"].values.astype(\"float32\")\n\n        # AGGREGATE HISTOGRAMS (uses price)\n        tmp = X_train2.loc[~X_train2.orig_price.isna()].groupby(\"Weight Capacity (kg)\")[[\"Price\"]].agg(\"count\")\n        tmp.columns = ['ct']\n        X_train3 = X_train2.merge(tmp.loc[tmp['ct']>1],on=\"Weight Capacity (kg)\",how=\"left\")\n        X_train3 = X_train3.loc[~X_train3['ct'].isna()]\n        result = X_train3.groupby(\"Weight Capacity (kg)\")[\"Price\"].apply(make_histogram)\n        result = result.to_frame()['Price'].apply(pd.Series)\n        result.columns = [f\"histogram_{x}\" for x in range(BINS)]\n        X_valid2 = X_valid2.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n        for c in [f\"histogram_{x}\" for x in range(BINS)]:\n            X_train.loc[test_index2,c] = X_valid2[c].values.astype(\"float32\")\n            \n        del result, X_train3, tmp\n        del X_train2, X_valid2\n        gc.collect()\n\n    ### FEATURE SET 1 (uses price) ###\n    col = \"Weight Capacity (kg)\"\n    tmp = X_train.groupby(col).Price.agg(STATS)\n    tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n    tmp = tmp.astype(\"float32\")\n    X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n    X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    ### FEATURE SET 2 (uses price) ###\n    for col in COMBO:\n        tmp = X_train.groupby(col).Price.agg(STATS2)\n        tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n        tmp = tmp.astype(\"float32\")\n        X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n        X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    # AGGREGATE QUANTILES (uses price)\n    for k in QUANTILES:\n        result = X_train.groupby('Weight Capacity (kg)').agg({'Price': lambda x: x.quantile(k/100)})\n        result.columns = [f\"quantile_{k}\"]\n        result = result.astype(\"float32\")\n        X_valid = X_valid.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n        X_test = X_test.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n\n    # AGGREGATE HISTOGRAMS (uses price)\n    tmp = X_train.loc[~X_train.orig_price.isna()].groupby(\"Weight Capacity (kg)\")[[\"Price\"]].agg(\"count\")\n    tmp.columns = ['ct']\n    X_train3 = X_train.merge(tmp.loc[tmp['ct']>1],on=\"Weight Capacity (kg)\",how=\"left\")\n    X_train3 = X_train3.loc[~X_train3['ct'].isna()]\n    result = X_train3.groupby(\"Weight Capacity (kg)\")[\"Price\"].apply(make_histogram)\n    result = result.to_frame()['Price'].apply(pd.Series)\n    result.columns = [f\"histogram_{x}\" for x in range(BINS)]\n    result = result.astype(\"float32\")\n    X_valid = X_valid.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n    X_test = X_test.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n    del result, X_train3, tmp\n\n    # COUNT PER NUNIQUE\n    X_train['TE1_wc_count_per_nunique'] = X_train['TE1_wc_count']/X_train['TE1_wc_nunique']\n    X_valid['TE1_wc_count_per_nunique'] = X_valid['TE1_wc_count']/X_valid['TE1_wc_nunique']\n    X_test['TE1_wc_count_per_nunique'] = X_test['TE1_wc_count']/X_test['TE1_wc_nunique']\n    \n    # STD PER COUNT\n    X_train['TE1_wc_std_per_count'] = X_train['TE1_wc_std']/X_train['TE1_wc_count']\n    X_valid['TE1_wc_std_per_count'] = X_valid['TE1_wc_std']/X_valid['TE1_wc_count']\n    X_test['TE1_wc_std_per_count'] = X_test['TE1_wc_std']/X_test['TE1_wc_count']\n\n    # CONVERT TO CATS SO XGBOOST RECOGNIZES THEM\n    X_train[CATS+DIGITS] = X_train[CATS+DIGITS].astype(\"category\")\n    X_valid[CATS+DIGITS] = X_valid[CATS+DIGITS].astype(\"category\")\n    X_test[CATS+DIGITS] = X_test[CATS+DIGITS].astype(\"category\")\n\n    # DROP PRICE THAT WAS USED FOR TARGET ENCODING\n    X_train = X_train.drop(['Price'],axis=1)\n\n    # DROP NON-TE CAT PAIRS\n    X_train = X_train.drop(PAIRS,axis=1)\n    X_valid = X_valid.drop(PAIRS,axis=1)\n    X_test = X_test.drop(PAIRS,axis=1)\n\n    # BUILD MODEL\n    model = XGBRegressor(\n        device=\"cuda\",\n        max_depth=6,  \n        colsample_bynode=0.3, \n        subsample=0.8,  \n        n_estimators=50_000,  \n        learning_rate=0.01,  \n        enable_categorical=True,\n        min_child_weight=10,\n        early_stopping_rounds=500,\n    )\n    \n    # TRAIN MODEL\n    COLS = X_train.columns\n    model.fit(\n        X_train[COLS], y_train,\n        eval_set=[(X_valid[COLS], y_valid)],  \n        verbose=500,\n    )\n\n    # PREDICT OOF AND TEST\n    oof[test_index] = model.predict(X_valid[COLS])\n    pred += model.predict(X_test[COLS])\n\n    # CLEAR MEMORY\n    del X_train, X_valid, X_test\n    del y_train, y_valid\n    if i != FOLDS-1: del model\n    gc.collect()\n\npred /= FOLDS","metadata":{"execution":{"iopub.status.busy":"2025-02-27T05:21:23.242828Z","iopub.execute_input":"2025-02-27T05:21:23.243086Z"},"papermill":{"duration":5978.624362,"end_time":"2025-02-27T03:18:02.759964","exception":false,"start_time":"2025-02-27T01:38:24.135602","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overall CV Score\nBelow we display overall cv score and save oof predictions to disk so we can use them later to assist finding ensemble weights with our other models.","metadata":{"papermill":{"duration":0.017467,"end_time":"2025-02-27T03:18:02.8012","exception":false,"start_time":"2025-02-27T03:18:02.783733","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# COMPUTE OVERALL CV SCORE\ntrue = train.Price.values\ns = np.sqrt(np.mean( (oof-true)**2.0 ) )\nprint(f\"=> Overall CV Score = {s}\")","metadata":{"papermill":{"duration":1.662836,"end_time":"2025-02-27T03:18:04.481755","exception":false,"start_time":"2025-02-27T03:18:02.818919","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SAVE OOF TO DISK FOR ENSEMBLES\nnp.save(f\"oof_v{VER}\",oof)\nprint(\"Saved oof to disk\")","metadata":{"papermill":{"duration":0.051362,"end_time":"2025-02-27T03:18:04.551865","exception":false,"start_time":"2025-02-27T03:18:04.500503","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Names\nBelow we list all our engineered features. We are using 138 features in total!","metadata":{"papermill":{"duration":0.018055,"end_time":"2025-02-27T03:18:04.58848","exception":false,"start_time":"2025-02-27T03:18:04.570425","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"\\nIn total, we used {len(COLS)} features, Wow!\\n\")\nprint( list(COLS) )","metadata":{"papermill":{"duration":0.03039,"end_time":"2025-02-27T03:18:04.637229","exception":false,"start_time":"2025-02-27T03:18:04.606839","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGB Feature Importance\nHere is top 100 XGBoost feature importance sorted by `gain`.","metadata":{"papermill":{"duration":0.017859,"end_time":"2025-02-27T03:18:04.673617","exception":false,"start_time":"2025-02-27T03:18:04.655758","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import xgboost as xgb\nfig, ax = plt.subplots(figsize=(10, 20))\nxgb.plot_importance(model, max_num_features=100, importance_type='gain',ax=ax)\nplt.title(\"Top 100 Feature Importances (XGBoost)\")\nplt.show()","metadata":{"papermill":{"duration":4.902029,"end_time":"2025-02-27T03:18:09.593631","exception":true,"start_time":"2025-02-27T03:18:04.691602","status":"failed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make Submission CSV\nWe save our test predictions to submission.csv and plot our predictions. ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/playground-series-s5e2/sample_submission.csv\")\nsub.Price = pred\nsub.to_csv(f\"submission_v{VER}.csv\",index=False)\nsub.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.hist(sub.Price,bins=100)\nplt.title(\"Test Predictions\")\nplt.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}