{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":223071113,"sourceType":"kernelVersion"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1995.935386,"end_time":"2025-02-18T03:02:00.966351","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-18T02:28:45.030965","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"8c086c32","cell_type":"markdown","source":"# Feature Engineering with Fast cuDF-Pandas!\nOne of the most powerful feature engineering techniques is `groupby(COL1)[COL2].agg(STAT)`. This is where we group by `COL1` column and aggregate (i.e. compute) a statistic `STAT` over another column `COL2`. This is the underlying method to compute `target encoding` and `count encoding`. By computing raw statistics and inputting them into our model, our model can do more than only receiving `TE` or `CE`. This notebook illustrates creating 50 engineered features, but we can create hundreds more and improve CV score and LB score!\n\nWhen our dataset has millions of rows like Kaggle's Backpack competition, then `groupby` operations take time to compute. The fastest way to compute a `groupby` aggregation is to use GPU with [RAPIDS cuDF-Pandas][1] library.\n\nThere are two ways to use [RAPIDS cuDF][2]. We can write cuDF code which looks just like Pandas code and starts with `import cudf`. Or we can write normal Pandas code with `import pandas` but before that we add the cell magic command `%load_ext cudf.pandas`. By adding this magic command all calls to Pandas afterward will use [RAPIDS cuDF][2] behind the scenes taking advantage of the massive speed boost of GPU!\n\nAlternatively, we can use [cuDF-Polars][3]. To use [cuDF-Polars][3], we write Polars code with lazy frame. Then the final call includes `.collect(engine=\"gpu\")` which will run all previous Polars code behind the scenes with [RAPIDS cuDF][2].\n\n[1]: https://rapids.ai/cudf-pandas/\n[2]: https://docs.rapids.ai/install/\n[3]: https://rapids.ai/polars-gpu-engine/","metadata":{"papermill":{"duration":0.004363,"end_time":"2025-02-18T02:28:48.234506","exception":false,"start_time":"2025-02-18T02:28:48.230143","status":"completed"},"tags":[]}},{"id":"6f2033b4","cell_type":"markdown","source":"# RAPIDS v25.02\n[RAPIDS v25.02][1] was just released Feb 15, 2025! Instructions on installing RAPIDS is [here][1]. On Kaggle, the easiest way to pip install new libraries is to do it once in a `Utility Script` notebook. Then whenever we attach the `Utility Script` notebook to another Kaggle notebook, the second Kaggle notebook immediately gets the benefit of the pip installed libraries. We created a [RAPIDS 25.02][1] `Utility Script` [here][2], and we attach it to the notebook you are reading. Therefore the notebook you are reading can import RAPIDS v25.02 without needing to pip install!\n\n[1]: https://docs.rapids.ai/install/\n[2]: https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02","metadata":{"papermill":{"duration":0.003485,"end_time":"2025-02-18T02:28:48.241835","exception":false,"start_time":"2025-02-18T02:28:48.23835","status":"completed"},"tags":[]}},{"id":"8ddcf4cc","cell_type":"markdown","source":"# GPU Acceleration\nWe activate [cuDF-Pandas][1] with the magic command `%load_ext cudf.pandas` below. Afterward, all calls to Pandas will use fast GPU [RAPIDS cuDF][2] behind the scenes! Since we attached `Utility Script` notebook [here][3] to the notebook you are reading, we will be using the new [RAPIDS v25.02][2]!\n\n[1]: https://rapids.ai/cudf-pandas/\n[2]: https://docs.rapids.ai/install/\n[3]: https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02","metadata":{"papermill":{"duration":0.003342,"end_time":"2025-02-18T02:28:48.248764","exception":false,"start_time":"2025-02-18T02:28:48.245422","status":"completed"},"tags":[]}},{"id":"c7caa687","cell_type":"code","source":"%load_ext cudf.pandas\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 500)\n\nVER=1","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:28:48.257723Z","iopub.status.busy":"2025-02-18T02:28:48.257418Z","iopub.status.idle":"2025-02-18T02:29:09.467296Z","shell.execute_reply":"2025-02-18T02:29:09.466504Z"},"papermill":{"duration":21.215657,"end_time":"2025-02-18T02:29:09.468801","exception":false,"start_time":"2025-02-18T02:28:48.253144","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"0568c7d5","cell_type":"markdown","source":"# Load Data\nWe load train, train extra, and test data. The combined train data has 4 million rows! This means we do not need to fear overfitting train. We can make hundreds/thousands of new features and every time our CV improves our LB will improve too!","metadata":{"papermill":{"duration":0.003715,"end_time":"2025-02-18T02:29:09.476774","exception":false,"start_time":"2025-02-18T02:29:09.473059","status":"completed"},"tags":[]}},{"id":"386628fc","cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e2/train.csv\")\nprint(\"Train shape\", train.shape )\ntrain.head()","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:09.485796Z","iopub.status.busy":"2025-02-18T02:29:09.485179Z","iopub.status.idle":"2025-02-18T02:29:10.451726Z","shell.execute_reply":"2025-02-18T02:29:10.450883Z"},"papermill":{"duration":0.972395,"end_time":"2025-02-18T02:29:10.453077","exception":false,"start_time":"2025-02-18T02:29:09.480682","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"638d59c3","cell_type":"code","source":"train2 = pd.read_csv(\"/kaggle/input/playground-series-s5e2/training_extra.csv\")\nprint(\"Extra Train shape\", train2.shape )\ntrain2.head()","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:10.462275Z","iopub.status.busy":"2025-02-18T02:29:10.462056Z","iopub.status.idle":"2025-02-18T02:29:11.860485Z","shell.execute_reply":"2025-02-18T02:29:11.859626Z"},"papermill":{"duration":1.404572,"end_time":"2025-02-18T02:29:11.861991","exception":false,"start_time":"2025-02-18T02:29:10.457419","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"4b1c2dce","cell_type":"code","source":"train = pd.concat([train,train2],axis=0,ignore_index=True)\nprint(\"Combined Train shape\", train.shape)","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:11.871454Z","iopub.status.busy":"2025-02-18T02:29:11.871229Z","iopub.status.idle":"2025-02-18T02:29:11.945776Z","shell.execute_reply":"2025-02-18T02:29:11.944555Z"},"papermill":{"duration":0.081414,"end_time":"2025-02-18T02:29:11.94801","exception":false,"start_time":"2025-02-18T02:29:11.866596","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"9cc4f83f","cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/playground-series-s5e2/test.csv\")\nprint(\"Test shape\", test.shape )\ntest.head()","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:11.961326Z","iopub.status.busy":"2025-02-18T02:29:11.961096Z","iopub.status.idle":"2025-02-18T02:29:12.109537Z","shell.execute_reply":"2025-02-18T02:29:12.108607Z"},"papermill":{"duration":0.156787,"end_time":"2025-02-18T02:29:12.111039","exception":false,"start_time":"2025-02-18T02:29:11.954252","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"a915f494","cell_type":"markdown","source":"# Feature Engineer Columns\nWe will engineer 8 new columns by combining existing columns.","metadata":{"papermill":{"duration":0.004315,"end_time":"2025-02-18T02:29:12.120568","exception":false,"start_time":"2025-02-18T02:29:12.116253","status":"completed"},"tags":[]}},{"id":"d71497d7","cell_type":"code","source":"CATS = list(train.columns[1:-2])\nprint(f\"There are {len(CATS)} categorical columns:\")\nprint( CATS )\nprint(f\"There are 1 numerical column:\")\nprint( [\"Weight Capacity (kg)\"] )","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:12.130186Z","iopub.status.busy":"2025-02-18T02:29:12.129929Z","iopub.status.idle":"2025-02-18T02:29:12.141894Z","shell.execute_reply":"2025-02-18T02:29:12.14108Z"},"papermill":{"duration":0.018185,"end_time":"2025-02-18T02:29:12.143122","exception":false,"start_time":"2025-02-18T02:29:12.124937","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"5434571e","cell_type":"code","source":"COMBO = []\nfor i,c in enumerate(CATS):\n    #print(f\"{c}, \",end=\"\")\n    combine = pd.concat([train[c],test[c]],axis=0)\n    combine,_ = pd.factorize(combine)\n    train[c] = combine[:len(train)]\n    test[c] = combine[len(train):]\n    n = f\"{c}_wc\"\n    train[n] = train[c]*100 + train[\"Weight Capacity (kg)\"]\n    test[n] = test[c]*100 + test[\"Weight Capacity (kg)\"]\n    COMBO.append(n)\nprint()\nprint(f\"We engineer {len(COMBO)} new columns!\")\nprint( COMBO )","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:12.152929Z","iopub.status.busy":"2025-02-18T02:29:12.152681Z","iopub.status.idle":"2025-02-18T02:29:13.003398Z","shell.execute_reply":"2025-02-18T02:29:13.002212Z"},"papermill":{"duration":0.857441,"end_time":"2025-02-18T02:29:13.005106","exception":false,"start_time":"2025-02-18T02:29:12.147665","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"e3c1c4d3","cell_type":"code","source":"FEATURES = CATS + [\"Weight Capacity (kg)\"] + COMBO\nprint(f\"We now have {len(FEATURES)} columns:\")\nprint( FEATURES )","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:13.024197Z","iopub.status.busy":"2025-02-18T02:29:13.023932Z","iopub.status.idle":"2025-02-18T02:29:13.028648Z","shell.execute_reply":"2025-02-18T02:29:13.027803Z"},"papermill":{"duration":0.015669,"end_time":"2025-02-18T02:29:13.030197","exception":false,"start_time":"2025-02-18T02:29:13.014528","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"11d6b892","cell_type":"markdown","source":"# XGBoost with Feature Engineer GroupBy\nWe train XGBoost with nested folds. We use the inner nested fold to create new features that aggregate the target `price`. And we use the outer fold to create new features that do not aggregate the target `price`. In each k fold loop, we engineer new features using the advanced feature engineering technique `groupby(COL1)[COL2].agg(STAT)`. Since we are using [RAPIDS cuDF-Pandas][1], these groupby computations will run fast on GPU! And we will train our model quickly on GPU using XGBoost!\n\n[1]: https://rapids.ai/cudf-pandas/","metadata":{"papermill":{"duration":0.006054,"end_time":"2025-02-18T02:29:13.044966","exception":false,"start_time":"2025-02-18T02:29:13.038912","status":"completed"},"tags":[]}},{"id":"b66346a4","cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nprint(f\"XGBoost version\",xgb.__version__)","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:13.055092Z","iopub.status.busy":"2025-02-18T02:29:13.054783Z","iopub.status.idle":"2025-02-18T02:29:16.648416Z","shell.execute_reply":"2025-02-18T02:29:16.647526Z"},"papermill":{"duration":3.600119,"end_time":"2025-02-18T02:29:16.649658","exception":false,"start_time":"2025-02-18T02:29:13.049539","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"6f72c574","cell_type":"code","source":"# STATISTICS TO AGGEGATE FOR OUR FEATURE GROUPS\nSTATS = [\"mean\",\"std\",\"count\",\"nunique\",\"median\",\"min\",\"max\",\"skew\"]\nSTATS2 = [\"mean\",\"std\"]","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:16.661069Z","iopub.status.busy":"2025-02-18T02:29:16.66063Z","iopub.status.idle":"2025-02-18T02:29:16.664189Z","shell.execute_reply":"2025-02-18T02:29:16.663549Z"},"papermill":{"duration":0.010236,"end_time":"2025-02-18T02:29:16.665371","exception":false,"start_time":"2025-02-18T02:29:16.655135","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"a10fae48","cell_type":"code","source":"%%time\n\nFOLDS = 7\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\noof = np.zeros((len(train)))\npred = np.zeros((len(test)))\n\n# OUTER K FOLD\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    print(f\"### OUTER Fold {i+1} ###\")\n\n    X_train = train.loc[train_index,FEATURES+['Price']].reset_index(drop=True).copy()\n    y_train = train.loc[train_index,'Price']\n\n    X_valid = train.loc[test_index,FEATURES].reset_index(drop=True).copy()\n    y_valid = train.loc[test_index,'Price']\n\n    X_test = test[FEATURES].reset_index(drop=True).copy()\n\n    # INNER K FOLD (TO PREVENT LEAKAGE WHEN USING PRICE)\n    kf2 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)   \n    for j, (train_index2, test_index2) in enumerate(kf2.split(X_train)):\n        print(f\" ## INNER Fold {j+1} (outer fold {i+1}) ##\")\n\n        X_train2 = X_train.loc[train_index2,FEATURES+['Price']].copy()\n        X_valid2 = X_train.loc[test_index2,FEATURES].copy()\n\n        ### FEATURE SET 1 (uses price) ###\n        col = \"Weight Capacity (kg)\"\n        tmp = X_train2.groupby(col).Price.agg(STATS)\n        tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n        X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n        for c in tmp.columns:\n            X_train.loc[test_index2,c] = X_valid2[c].values\n\n        ### FEATURE SET 2 (uses price) ###\n        for col in COMBO:\n            tmp = X_train2.groupby(col).Price.agg(STATS2)\n            tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n            X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n            for c in tmp.columns:\n                X_train.loc[test_index2,c] = X_valid2[c].values\n\n    ### FEATURE SET 1 (uses price) ###\n    col = \"Weight Capacity (kg)\"\n    tmp = X_train.groupby(col).Price.agg(STATS)\n    tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n    X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n    X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    ### FEATURE SET 2 (uses price) ###\n    for col in COMBO:\n        tmp = X_train.groupby(col).Price.agg(STATS2)\n        tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n        X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n        X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    ### FEATURE SET 3 (does not use price) ###\n    for col in CATS:\n        col2 = \"Weight Capacity (kg)\"\n        tmp = X_train.groupby(col)[col2].agg(STATS2)\n        tmp.columns = [f\"FE3_{col}_wc_{s}\" for s in STATS2]\n        X_train = X_train.merge(tmp, on=col, how=\"left\")\n        X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n        X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    # CONVERT TO CATS SO XGBOOST RECOGNIZES THEM\n    X_train[CATS] = X_train[CATS].astype(\"category\")\n    X_valid[CATS] = X_valid[CATS].astype(\"category\")\n    X_test[CATS] = X_test[CATS].astype(\"category\")\n\n    # DROP PRICE THAT WAS USED FOR TARGET ENCODING\n    X_train = X_train.drop(['Price'],axis=1)\n\n    # BUILD MODEL\n    model = XGBRegressor(\n        device=\"cuda\",\n        max_depth=6,  \n        colsample_bytree=0.5, \n        subsample=0.8,  \n        n_estimators=10_000,  \n        learning_rate=0.02,  \n        enable_categorical=True,\n        min_child_weight=10,\n        early_stopping_rounds=100,\n    )\n    \n    # TRAIN MODEL\n    COLS = X_train.columns\n    model.fit(\n        X_train[COLS], y_train,\n        eval_set=[(X_valid[COLS], y_valid)],  \n        verbose=300,\n    )\n\n    # PREDICT OOF AND TEST\n    oof[test_index] = model.predict(X_valid[COLS])\n    pred += model.predict(X_test[COLS])\n\npred /= FOLDS","metadata":{"execution":{"iopub.execute_input":"2025-02-18T02:29:16.675947Z","iopub.status.busy":"2025-02-18T02:29:16.675692Z","iopub.status.idle":"2025-02-18T03:01:51.815393Z","shell.execute_reply":"2025-02-18T03:01:51.814351Z"},"papermill":{"duration":1955.146651,"end_time":"2025-02-18T03:01:51.816981","exception":false,"start_time":"2025-02-18T02:29:16.67033","status":"completed"},"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"id":"2bc6b07d","cell_type":"markdown","source":"# Overall CV Score\nBelow we display overall cv score and save oof predictions to disk so we can use them later to assist finding ensemble weights with our other models.","metadata":{"papermill":{"duration":0.010703,"end_time":"2025-02-18T03:01:51.839031","exception":false,"start_time":"2025-02-18T03:01:51.828328","status":"completed"},"tags":[]}},{"id":"bcf7b370","cell_type":"code","source":"# COMPUTE OVERALL CV SCORE\ntrue = train.Price.values\ns = np.sqrt(np.mean( (oof-true)**2.0 ) )\nprint(f\"=> Overall CV Score = {s}\")","metadata":{"execution":{"iopub.execute_input":"2025-02-18T03:01:51.86137Z","iopub.status.busy":"2025-02-18T03:01:51.861102Z","iopub.status.idle":"2025-02-18T03:01:52.629443Z","shell.execute_reply":"2025-02-18T03:01:52.628291Z"},"papermill":{"duration":0.78111,"end_time":"2025-02-18T03:01:52.630915","exception":false,"start_time":"2025-02-18T03:01:51.849805","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"e772634a","cell_type":"code","source":"# SAVE OOF TO DISK FOR ENSEMBLES\nnp.save(f\"oof_v{VER}\",oof)\nprint(\"Saved oof to disk\")","metadata":{"execution":{"iopub.execute_input":"2025-02-18T03:01:52.653604Z","iopub.status.busy":"2025-02-18T03:01:52.653367Z","iopub.status.idle":"2025-02-18T03:01:52.679648Z","shell.execute_reply":"2025-02-18T03:01:52.678848Z"},"papermill":{"duration":0.039041,"end_time":"2025-02-18T03:01:52.681063","exception":false,"start_time":"2025-02-18T03:01:52.642022","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"43385e92","cell_type":"markdown","source":"# Feature Names\nBelow we list all our engineered features. We are using 57 features in total!","metadata":{"papermill":{"duration":0.010308,"end_time":"2025-02-18T03:01:52.702663","exception":false,"start_time":"2025-02-18T03:01:52.692355","status":"completed"},"tags":[]}},{"id":"496e7297","cell_type":"code","source":"print(f\"\\nIn total, we used {len(COLS)} features, Wow!\\n\")\nprint( list(COLS) )","metadata":{"execution":{"iopub.execute_input":"2025-02-18T03:01:52.767418Z","iopub.status.busy":"2025-02-18T03:01:52.767055Z","iopub.status.idle":"2025-02-18T03:01:52.775316Z","shell.execute_reply":"2025-02-18T03:01:52.774513Z"},"papermill":{"duration":0.063025,"end_time":"2025-02-18T03:01:52.77659","exception":false,"start_time":"2025-02-18T03:01:52.713565","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"5bcf8e27","cell_type":"markdown","source":"# XGB Feature Importance\nHere is XGBoost feature importance sorted by `gain`.","metadata":{"papermill":{"duration":0.009924,"end_time":"2025-02-18T03:01:52.797167","exception":false,"start_time":"2025-02-18T03:01:52.787243","status":"completed"},"tags":[]}},{"id":"24a111b9","cell_type":"code","source":"import xgboost as xgb\nfig, ax = plt.subplots(figsize=(10, 20))\nxgb.plot_importance(model, max_num_features=100, importance_type='gain',ax=ax)\nplt.title(\"Top 100 Feature Importances (XGBoost)\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-02-18T03:01:52.819445Z","iopub.status.busy":"2025-02-18T03:01:52.819175Z","iopub.status.idle":"2025-02-18T03:01:53.513791Z","shell.execute_reply":"2025-02-18T03:01:53.512821Z"},"papermill":{"duration":0.709464,"end_time":"2025-02-18T03:01:53.517477","exception":false,"start_time":"2025-02-18T03:01:52.808013","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"0bdc9147","cell_type":"markdown","source":"# Make Submission CSV\nWe save our test predictions to submission.csv and plot our predictions. ","metadata":{"papermill":{"duration":0.014783,"end_time":"2025-02-18T03:01:53.547748","exception":false,"start_time":"2025-02-18T03:01:53.532965","status":"completed"},"tags":[]}},{"id":"d7e4d268","cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/playground-series-s5e2/sample_submission.csv\")\nsub.Price = pred\nsub.to_csv(f\"submission_v{VER}.csv\",index=False)\nsub.head()","metadata":{"execution":{"iopub.execute_input":"2025-02-18T03:01:53.577767Z","iopub.status.busy":"2025-02-18T03:01:53.577477Z","iopub.status.idle":"2025-02-18T03:01:53.656564Z","shell.execute_reply":"2025-02-18T03:01:53.65559Z"},"papermill":{"duration":0.096407,"end_time":"2025-02-18T03:01:53.658057","exception":false,"start_time":"2025-02-18T03:01:53.56165","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"11282c73","cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.hist(sub.Price,bins=100)\nplt.title(\"Test Predictions\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-02-18T03:01:53.689658Z","iopub.status.busy":"2025-02-18T03:01:53.689425Z","iopub.status.idle":"2025-02-18T03:01:57.391551Z","shell.execute_reply":"2025-02-18T03:01:57.390718Z"},"papermill":{"duration":3.719347,"end_time":"2025-02-18T03:01:57.393096","exception":false,"start_time":"2025-02-18T03:01:53.673749","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}