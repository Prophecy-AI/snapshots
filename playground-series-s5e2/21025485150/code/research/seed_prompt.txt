## Current Status
- Best CV score: 38.781061 from exp_000 (baseline)
- Last experiment: 38.825723 from exp_002 (ablation study, worse than baseline)
- Target: 38.616280
- Gap: 0.165 RMSE to close
- CV-LB gap: Unknown (no LB submissions yet)

## Response to Evaluator
- Technical verdict from evolver_loop3: Feature importance issues identified
- Evaluator's top priority: Fix bugs and test features incrementally
- **Critical finding**: Groupby statistics show 0.71 correlation vs <0.02 for simple features
- **Root cause identified**: exp_003 used wrong feature engineering approach

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop4_analysis.ipynb` for breakthrough analysis
- **Key breakthrough**: Groupby aggregations (Price grouped by Weight Capacity) have 0.71 correlation
- **Previous mistake**: Simple weight transformations (rounding, digits) have <0.02 correlation
- **Winning solution pattern**: `groupby(COL1)[COL2].agg(STAT)` where COL2 is the target

## Recommended Approaches

### 1. Groupby Statistics (HIGHEST PRIORITY - FIXES ROOT CAUSE)
**Why**: Analysis shows 0.71 correlation vs <0.02 for current features
**Implementation**:
- Groupby Weight Capacity: compute Price mean, std, count, min, max, median
- Groupby each categorical: compute Price mean, std, count
- Use nested CV to prevent leakage
- Merge back to train/test based on the group key

### 2. Histogram Binning (from 1st place solution)
**Why**: Chris Deotte's winning technique - bins Price distribution within each Weight Capacity group
**Implementation**:
- Create 50 quantile-based bins for Price
- For each Weight Capacity value, count how many training prices fall in each bin
- Use these counts as features (50 features per group)
- This captures the full distribution, not just statistics

### 3. Target Encoding with Nested CV (High Priority)
**Why**: Properly encode categoricals using target statistics
**Implementation**:
- Features: Color, Material, Brand, Size, Laptop Compartment, Waterproof, Style
- Method: 5-fold nested CV to compute target statistics
- Statistics: mean, std, count for each category
- Smoothing: Apply regularization (smoothing=10) for rare categories
- Leakage prevention: Compute on training folds only, apply to validation

### 4. Categorical Interactions (High Priority)
**Why**: 1st and 2nd place solutions both use interaction features
**Implementation**:
- Create interaction strings: Brand_Size, Size_Color, Size_Style, Brand_Material
- Apply target encoding to interactions
- Use label encoding for simple interactions
- Limit to 2-3 interactions to avoid explosion

### 5. Original Dataset Features (Critical)
**Why**: 1st place solution explicitly uses original dataset as "MSRP"
**Implementation**:
- Compute mean Price by Weight Capacity from training data (simulates MSRP)
- Compute mean Price by categorical combinations
- Use these as additional features
- This gives the model knowledge of "manufacturer suggested retail price"

### 6. NaN Encoding (Medium Priority)
**Why**: 1st place solution uses NaN patterns as features
**Implementation**:
- Base-2 encoding: Create single feature representing all NaN patterns
- NaN Ã— Weight Capacity: Combine each column's NaN status with Weight Capacity
- Captures missing value patterns that may correlate with price

## What NOT to Try
- Simple weight transformations: rounding, digits (proven to have <0.02 correlation)
- Quantile features: Created constant columns in exp_001
- Count encoding without target encoding: Low correlation, need target stats instead
- Complex ensembles: Focus on single strong model first
- Neural networks: Winning solutions are XGBoost-based

## Validation Notes
- CV scheme: 20-fold CV (consistent with previous experiments)
- **Critical**: Use nested CV for all target encoding and groupby statistics
- Feature importance: Monitor to confirm groupby features have high importance
- Ablation threshold: Keep features that improve CV by >0.001 RMSE
- Early stopping: Use to prevent overfitting with more powerful features
- **Expected improvement**: Should see significant CV improvement from current 38.78 to ~38.6x range