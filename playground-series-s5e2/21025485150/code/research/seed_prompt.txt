## Current Status
- Best CV score: 38.781061 from exp_000 (baseline)
- Last experiment: 38.786412 from exp_001 (worse by +0.005)
- Target: 38.616280
- Gap: 0.165 RMSE to close
- CV-LB gap: Unknown (no LB submissions yet)

## Response to Evaluator
- Technical verdict: Need to debug feature engineering issues in exp_001
- Evaluator's top priority: Investigate why enhanced features degraded performance
- Key concerns addressed:
  - ✅ Quantile features bug identified: Creates constant columns with zero variance
  - ✅ Count encoding leakage confirmed: Uses combined train+test data
  - ✅ Feature importance analyzed: Most engineered features have near-zero importance
  - ✅ Original dataset: Need to download and use (1st place heavily exploited this)
  - ✅ Systematic ablation needed: Test features incrementally

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop3_analysis.ipynb` for detailed analysis
- Key findings:
  - Quantile features (weight_q25, q50, q75, q90) are constant columns - REMOVE these
  - Count encoding uses leaky implementation but correlation impact is minimal
  - Feature importance: Weight Capacity (42.6%), weight_digit_1 (28.9%), weight_digit_2 (28.5%)
  - Most count features have correlation <0.005 with target - likely low value
  - Original Student Bag dataset not used - this is critical missing signal

## Recommended Approaches

### 1. Fix Bugs from exp_001 (Highest Priority)
- Remove quantile features completely - they add only noise
- Fix count encoding: Compute from training data only, not combined train+test
- Validate each feature group individually before combining
- Start with baseline features that work: Weight Capacity + digit features

### 2. Use Original Student Bag Dataset (Critical Priority)
- Download original dataset from Kaggle competition URL
- Compute mean Price by Weight Capacity (rounded to different decimals) - MSRP feature
- Compute target statistics by categorical combinations
- Merge these features into training data
- This was the key insight in 1st place solution by Chris Deotte

### 3. Proper Target Encoding with Nested CV (High Priority)
- Features: Color, Material, Brand (top 3 from earlier analysis)
- Method: 5-fold nested CV to compute target statistics
- Statistics: mean, std, count for each category
- Smoothing: Apply regularization for rare categories
- Leakage prevention: Compute on training folds only
- DO NOT use target encoding on high-cardinality interactions yet

### 4. Systematic Feature Ablation Study (High Priority)
- Start with baseline: Weight Capacity + digit features only
- Add one feature group at a time, measure CV impact:
  - Group 1: Weight Capacity bins (50 uniform bins)
  - Group 2: Rounding features (7-10 decimal places)
  - Group 3: Count encoding (fixed version, training data only)
  - Group 4: Interaction features (Brand_Size, Size_Color, Size_Style)
  - Group 5: Original dataset features (MSRP)
  - Group 6: Target encoded features
- Keep only features that improve CV by >0.001
- Remove features that hurt performance or add no value

### 5. Hyperparameter Refinement (Medium Priority)
- Learning rate: 0.05 (from 0.1) - winning solutions use 0.01-0.05
- Max depth: 6-8 (tune based on feature count)
- n_estimators: 1000-2000 with early_stopping_rounds=100
- Subsample: 0.8, colsample_bytree: 0.8 (prevent overfitting)
- Tune AFTER identifying optimal feature set

## What NOT to Try
- Quantile features: Proven to be constant columns - waste of compute
- Complex interactions before fixing basics: Brand_Size interactions need validation
- High-cardinality target encoding: Risk of overfitting, test after basics work
- Neural networks: Winning solutions are XGBoost-based, focus there
- Excessive feature count: Quality over quantity - 20 good features > 100 noisy ones

## Validation Notes
- CV scheme: 20-fold CV (consistent with previous experiments)
- Feature importance: Monitor at each iteration to identify valuable features
- Ablation threshold: Keep features that improve CV by >0.001 RMSE
- Early stopping: Use to prevent overfitting on each feature set
- LB calibration: Submit after significant improvement (>0.002) to verify CV-LB correlation