{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60905c7",
   "metadata": {},
   "source": [
    "# Experiment 003: Systematic Ablation Study\n",
    "\n",
    "## Goal\n",
    "Fix bugs from exp_001 and test features incrementally to identify what works vs. what hurts.\n",
    "\n",
    "## Strategy\n",
    "1. Start with proven baseline: Weight Capacity + digit features only\n",
    "2. Add one feature group at a time\n",
    "3. Measure CV impact for each group\n",
    "4. Keep only features that improve CV by >0.001\n",
    "\n",
    "## Bug Fixes\n",
    "- Remove quantile features (proven constant columns)\n",
    "- Fix count encoding: compute from training data only\n",
    "- Validate each feature before combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e503e392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:32:38.971291Z",
     "iopub.status.busy": "2026-01-15T13:32:38.970952Z",
     "iopub.status.idle": "2026-01-15T13:32:41.014768Z",
     "shell.execute_reply": "2026-01-15T13:32:41.014294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "GPU Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5fa16",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0728b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:32:41.016040Z",
     "iopub.status.busy": "2026-01-15T13:32:41.015841Z",
     "iopub.status.idle": "2026-01-15T13:32:44.760716Z",
     "shell.execute_reply": "2026-01-15T13:32:44.760198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (300000, 11)\n",
      "Test shape: (200000, 10)\n",
      "Training extra shape: (3694318, 11)\n",
      "Combined train shape: (3994318, 11)\n",
      "\n",
      "Target stats:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.994318e+06\n",
      "mean     8.136217e+01\n",
      "std      3.893868e+01\n",
      "min      1.500000e+01\n",
      "25%      4.747002e+01\n",
      "50%      8.098495e+01\n",
      "75%      1.148550e+02\n",
      "max      1.500000e+02\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Training extra shape: {training_extra.shape}\")\n",
    "\n",
    "# Combine train and training_extra for more data\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nTarget stats:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d455d3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:32:44.761885Z",
     "iopub.status.busy": "2026-01-15T13:32:44.761771Z",
     "iopub.status.idle": "2026-01-15T13:32:48.695419Z",
     "shell.execute_reply": "2026-01-15T13:32:48.691226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (300000, 11)\n",
      "Test shape: (200000, 10)\n",
      "Training extra shape: (3694318, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train shape: (3994318, 11)\n",
      "\n",
      "Target stats:\n",
      "count    3.994318e+06\n",
      "mean     8.136217e+01\n",
      "std      3.893868e+01\n",
      "min      1.500000e+01\n",
      "25%      4.747002e+01\n",
      "50%      8.098495e+01\n",
      "75%      1.148550e+02\n",
      "max      1.500000e+02\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Training extra shape: {training_extra.shape}\")\n",
    "\n",
    "# Combine train and training_extra for more data\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nTarget stats:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353f2084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:32:48.703968Z",
     "iopub.status.busy": "2026-01-15T13:32:48.703741Z",
     "iopub.status.idle": "2026-01-15T13:32:56.970226Z",
     "shell.execute_reply": "2026-01-15T13:32:56.969637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline features shape: (3994318, 12)\n",
      "Baseline test features shape: (200000, 12)\n",
      "\n",
      "Baseline feature names:\n",
      "['weight_original', 'weight_round_7', 'weight_round_8', 'weight_round_9', 'weight_round_10', 'weight_digit_1', 'weight_digit_2', 'weight_digit_3', 'weight_digit_4', 'weight_digit_5', 'weight_int', 'weight_frac']\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_features(df):\n",
    "    \"\"\"Create proven baseline features only\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Weight Capacity features (proven to work)\n",
    "    if 'Weight Capacity (kg)' in df.columns:\n",
    "        weight = df['Weight Capacity (kg)'].copy()\n",
    "        \n",
    "        # Original value\n",
    "        features['weight_original'] = weight\n",
    "        \n",
    "        # Rounding to different decimals (7-10 as per winning solutions)\n",
    "        for dec in range(7, 11):\n",
    "            features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "        \n",
    "        # Digit extraction (1-5 digits) - handle NaN properly\n",
    "        # Replace NaN with 0 for digit extraction\n",
    "        weight_filled = weight.fillna(0)\n",
    "        weight_str = weight_filled.astype(str).str.replace('.', '', regex=False)\n",
    "        \n",
    "        # Remove decimal point and get first 5 digits\n",
    "        # Pad with zeros if needed\n",
    "        weight_str = weight_str.str.pad(width=5, side='right', fillchar='0')\n",
    "        \n",
    "        for i in range(1, 6):\n",
    "            features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "        \n",
    "        # Integer and fractional parts - handle NaN\n",
    "        features['weight_int'] = weight.fillna(0).astype(int)\n",
    "        features['weight_frac'] = weight.fillna(0) - weight.fillna(0).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create baseline features\n",
    "try:\n",
    "    X_baseline = create_baseline_features(combined_train)\n",
    "    X_test_baseline = create_baseline_features(test)\n",
    "    print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "    print(f\"Baseline test features shape: {X_test_baseline.shape}\")\n",
    "    print(f\"\\nBaseline feature names:\")\n",
    "    print(X_baseline.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error in create_baseline_features: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7a1ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf5b51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:23:58.180958Z",
     "iopub.status.busy": "2026-01-15T13:23:58.180647Z",
     "iopub.status.idle": "2026-01-15T13:23:58.186007Z",
     "shell.execute_reply": "2026-01-15T13:23:58.185582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in create_baseline_features: name 'create_baseline_features' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_480323/3565338585.py\", line 3, in <module>\n",
      "    X_baseline = create_baseline_features(combined_train)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'create_baseline_features' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Create baseline features with error handling\n",
    "try:\n",
    "    X_baseline = create_baseline_features(combined_train)\n",
    "    X_test_baseline = create_baseline_features(test)\n",
    "    print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "    print(f\"Baseline test features shape: {X_test_baseline.shape}\")\n",
    "    print(f\"\\nBaseline feature names:\")\n",
    "    print(X_baseline.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error in create_baseline_features: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_features(df):\n",
    "    \"\"\"Create proven baseline features only\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Weight Capacity features (proven to work)\n",
    "    if 'Weight Capacity (kg)' in df.columns:\n",
    "        weight = df['Weight Capacity (kg)'].copy()\n",
    "        \n",
    "        # Original value\n",
    "        features['weight_original'] = weight\n",
    "        \n",
    "        # Rounding to different decimals (7-10 as per winning solutions)\n",
    "        for dec in range(7, 11):\n",
    "            features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "        \n",
    "        # Digit extraction (1-5 digits)\n",
    "        weight_str = weight.astype(str).str.replace('.', '')\n",
    "        for i in range(1, 6):\n",
    "            features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "        \n",
    "        # Integer and fractional parts\n",
    "        features['weight_int'] = weight.astype(int)\n",
    "        features['weight_frac'] = weight - weight.astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create baseline features\n",
    "X_baseline = create_baseline_features(combined_train)\n",
    "X_test_baseline = create_baseline_features(test)\n",
    "\n",
    "print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "print(f\"Baseline test features shape: {X_test_baseline.shape}\")\n",
    "print(f\"\\nBaseline feature names:\")\n",
    "print(X_baseline.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug categorical columns for any issues\n",
    "print(\"Checking categorical columns for issues:\")\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        # Check for NaN\n",
    "        nan_count = combined_train[col].isna().sum()\n",
    "        # Check unique values\n",
    "        unique_vals = combined_train[col].nunique()\n",
    "        print(f\"{col}: {unique_vals} unique, {nan_count} NaN\")\n",
    "        \n",
    "        # Check if any values might cause issues\n",
    "        if nan_count > 0:\n",
    "            print(f\"  Sample NaN rows in {col}:\")\n",
    "            print(f\"  {combined_train[col].isna().sum()}\")\n",
    "        \n",
    "        # Check for problematic string patterns\n",
    "        col_str = combined_train[col].astype(str)\n",
    "        if col_str.str.contains('n', na=False).sum() > 0:\n",
    "            print(f\"  WARNING: {col} contains 'n' characters\")\n",
    "            # Show some examples\n",
    "            mask = col_str.str.contains('n', na=False)\n",
    "            print(f\"  Examples: {col_str[mask].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2cb98",
   "metadata": {},
   "source": [
    "## Label Encode Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992a667b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:35:07.011413Z",
     "iopub.status.busy": "2026-01-15T13:35:07.011106Z",
     "iopub.status.idle": "2026-01-15T13:35:13.484359Z",
     "shell.execute_reply": "2026-01-15T13:35:13.483774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 7 label encoded features\n",
      "Total baseline features: 19\n"
     ]
    }
   ],
   "source": [
    "# Label encode categorical features\n",
    "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Fit label encoders on combined train + test data\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined data to handle all categories\n",
    "        combined_data = pd.concat([combined_train[col], test[col]], ignore_index=True)\n",
    "        le.fit(combined_data.astype(str).fillna('missing'))\n",
    "        \n",
    "        # Transform train and test\n",
    "        X_baseline[f'{col}_encoded'] = le.transform(combined_train[col].astype(str).fillna('missing'))\n",
    "        X_test_baseline[f'{col}_encoded'] = le.transform(test[col].astype(str).fillna('missing'))\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Added {len(categorical_cols)} label encoded features\")\n",
    "print(f\"Total baseline features: {X_baseline.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd6bf2",
   "metadata": {},
   "source": [
    "## Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec22bc8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:35:13.485760Z",
     "iopub.status.busy": "2026-01-15T13:35:13.485563Z",
     "iopub.status.idle": "2026-01-15T13:35:13.956900Z",
     "shell.execute_reply": "2026-01-15T13:35:13.956460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (3994318, 19)\n",
      "Target shape: (3994318,)\n",
      "CV folds: 20\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for CV\n",
    "y = combined_train['Price'].values\n",
    "X = X_baseline.copy()\n",
    "\n",
    "# 20-fold CV (consistent with previous experiments)\n",
    "n_folds = 20\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"CV folds: {n_folds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37fdab",
   "metadata": {},
   "source": [
    "## Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (from winning solutions)\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "\n",
    "print(\"Starting 20-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(dval)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    fold_scores.append(rmse)\n",
    "    \n",
    "    print(f\"Fold {fold+1:2d}/{n_folds} - RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_score = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "print(f\"\\nOverall CV RMSE: {cv_score:.6f}\")\n",
    "print(f\"Mean fold RMSE: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
    "print(f\"Fold RMSE range: {np.min(fold_scores):.6f} - {np.max(fold_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11015045",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from last fold\n",
    "importance = model.get_score(importance_type='gain')\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': [importance.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 features by importance:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Check for zero-variance features\n",
    "zero_var_features = X.columns[X.nunique() <= 1]\n",
    "if len(zero_var_features) > 0:\n",
    "    print(f\"\\n⚠️  Zero variance features: {zero_var_features.tolist()}\")\n",
    "else:\n",
    "    print(\"\\n✓ No zero variance features detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea63f0",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48edc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "\n",
    "final_model = xgb.train(\n",
    "    params,\n",
    "    dtrain_full,\n",
    "    num_boost_round=2000,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "dtest = xgb.DMatrix(X_test_baseline)\n",
    "test_predictions = final_model.predict(dtest)\n",
    "\n",
    "# Clip predictions to training range\n",
    "min_price = y.min()\n",
    "max_price = y.max()\n",
    "test_predictions = np.clip(test_predictions, min_price, max_price)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test predictions range: {test_predictions.min():.2f} - {test_predictions.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e481e7a",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "# Create submission\n",
    "submission = sample_sub.copy()\n",
    "submission['Price'] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission Price stats:\")\n",
    "print(submission['Price'].describe())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
