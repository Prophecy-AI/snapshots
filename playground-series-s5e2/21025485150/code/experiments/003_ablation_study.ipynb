{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60905c7",
   "metadata": {},
   "source": [
    "# Experiment 003: Systematic Ablation Study\n",
    "\n",
    "## Goal\n",
    "Fix bugs from exp_001 and test features incrementally to identify what works vs. what hurts.\n",
    "\n",
    "## Strategy\n",
    "1. Start with proven baseline: Weight Capacity + digit features only\n",
    "2. Add one feature group at a time\n",
    "3. Measure CV impact for each group\n",
    "4. Keep only features that improve CV by >0.001\n",
    "\n",
    "## Bug Fixes\n",
    "- Remove quantile features (proven constant columns)\n",
    "- Fix count encoding: compute from training data only\n",
    "- Validate each feature before combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e503e392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:02:12.937914Z",
     "iopub.status.busy": "2026-01-15T13:02:12.937668Z",
     "iopub.status.idle": "2026-01-15T13:02:12.941573Z",
     "shell.execute_reply": "2026-01-15T13:02:12.941174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "GPU Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5fa16",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0728b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:02:12.942726Z",
     "iopub.status.busy": "2026-01-15T13:02:12.942457Z",
     "iopub.status.idle": "2026-01-15T13:02:16.670776Z",
     "shell.execute_reply": "2026-01-15T13:02:16.670173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weight values:\n",
      "0     11.611723\n",
      "1     27.078537\n",
      "2     16.643760\n",
      "3     12.937220\n",
      "4     17.749338\n",
      "5      7.241812\n",
      "6      6.828123\n",
      "7     21.488864\n",
      "8     10.207780\n",
      "9     15.895100\n",
      "10    27.806951\n",
      "11    13.697554\n",
      "12    15.872050\n",
      "13    26.079409\n",
      "14    14.744147\n",
      "15     7.934018\n",
      "16    18.988774\n",
      "17    15.533611\n",
      "18    26.116081\n",
      "19    26.262637\n",
      "Name: Weight Capacity (kg), dtype: float64\n",
      "\n",
      "NaN count: 1808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample string patterns:\n",
      "  '11.611722805222309'\n",
      "  '27.07853658053123'\n",
      "  '16.643759949103497'\n",
      "  '12.937220306632067'\n",
      "  '17.749338465908988'\n",
      "  '7.241812431393921'\n",
      "  '6.82812289959413'\n",
      "  '21.48886449943962'\n",
      "  '10.207780204196547'\n",
      "  '15.895100441539512'\n",
      "  '27.80695093142625'\n",
      "  '13.697553716032546'\n",
      "  '15.872049592139104'\n",
      "  '26.07940932802842'\n",
      "  '14.744146515842305'\n",
      "  '7.934018057756724'\n",
      "  '18.98877353521776'\n",
      "  '15.533610728685636'\n",
      "  '26.11608101874347'\n",
      "  '26.262636968992748'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows containing 'n': 1808\n"
     ]
    }
   ],
   "source": [
    "# Debug the weight column to see what's causing the error\n",
    "weight_sample = combined_train['Weight Capacity (kg)'].head(20)\n",
    "print(\"Sample weight values:\")\n",
    "print(weight_sample)\n",
    "print(f\"\\nNaN count: {combined_train['Weight Capacity (kg)'].isna().sum()}\")\n",
    "\n",
    "# Check unique values that might cause issues\n",
    "weight_str = combined_train['Weight Capacity (kg)'].astype(str)\n",
    "unique_patterns = weight_str.unique()[:20]\n",
    "print(f\"\\nSample string patterns:\")\n",
    "for pattern in unique_patterns:\n",
    "    print(f\"  '{pattern}'\")\n",
    "\n",
    "# Check for 'n' specifically\n",
    "has_n = weight_str.str.contains('n', na=False).sum()\n",
    "print(f\"\\nRows containing 'n': {has_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d455d3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T13:02:16.672196Z",
     "iopub.status.busy": "2026-01-15T13:02:16.671993Z",
     "iopub.status.idle": "2026-01-15T13:02:20.493118Z",
     "shell.execute_reply": "2026-01-15T13:02:20.492521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (300000, 11)\n",
      "Test shape: (200000, 10)\n",
      "Training extra shape: (3694318, 11)\n",
      "Combined train shape: (3994318, 11)\n",
      "\n",
      "Target stats:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.994318e+06\n",
      "mean     8.136217e+01\n",
      "std      3.893868e+01\n",
      "min      1.500000e+01\n",
      "25%      4.747002e+01\n",
      "50%      8.098495e+01\n",
      "75%      1.148550e+02\n",
      "max      1.500000e+02\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Training extra shape: {training_extra.shape}\")\n",
    "\n",
    "# Combine train and training_extra for more data\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nTarget stats:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7a1ac",
   "metadata": {},
   "source": [
    "def create_baseline_features(df):\n",
    "    \"\"\"Create proven baseline features only\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Weight Capacity features (proven to work)\n",
    "    if 'Weight Capacity (kg)' in df.columns:\n",
    "        weight = df['Weight Capacity (kg)'].copy()\n",
    "        \n",
    "        # Original value\n",
    "        features['weight_original'] = weight\n",
    "        \n",
    "        # Rounding to different decimals (7-10 as per winning solutions)\n",
    "        for dec in range(7, 11):\n",
    "            features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "        \n",
    "        # Digit extraction (1-5 digits) - handle NaN properly\n",
    "        # Replace NaN with 0 for digit extraction\n",
    "        weight_filled = weight.fillna(0)\n",
    "        weight_str = weight_filled.astype(str).str.replace('.', '', regex=False)\n",
    "        \n",
    "        # Remove decimal point and get first 5 digits\n",
    "        # Pad with zeros if needed\n",
    "        weight_str = weight_str.str.pad(width=5, side='right', fillchar='0')\n",
    "        \n",
    "        for i in range(1, 6):\n",
    "            features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "        \n",
    "        # Integer and fractional parts\n",
    "        features['weight_int'] = weight_filled.astype(int)\n",
    "        features['weight_frac'] = weight_filled - weight_filled.astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create baseline features\n",
    "X_baseline = create_baseline_features(combined_train)\n",
    "X_test_baseline = create_baseline_features(test)\n",
    "\n",
    "print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "print(f\"Baseline test features shape: {X_test_baseline.shape}\")\n",
    "print(f\"\\nBaseline feature names:\")\n",
    "print(X_baseline.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the digit extraction step by step\n",
    "weight = combined_train['Weight Capacity (kg)'].copy()\n",
    "print(f\"Original weight sample (first 10):\")\n",
    "print(weight.head(10))\n",
    "print(f\"\\nNaN count: {weight.isna().sum()}\")\n",
    "\n",
    "# Step 1: Fill NaN with 0\n",
    "weight_filled = weight.fillna(0)\n",
    "print(f\"\\nAfter fillna(0) sample:\")\n",
    "print(weight_filled.head(10))\n",
    "\n",
    "# Step 2: Convert to string\n",
    "weight_str = weight_filled.astype(str)\n",
    "print(f\"\\nAfter astype(str) sample:\")\n",
    "print(weight_str.head(10))\n",
    "\n",
    "# Step 3: Remove decimal point\n",
    "weight_str_no_dec = weight_str.str.replace('.', '', regex=False)\n",
    "print(f\"\\nAfter removing decimal point sample:\")\n",
    "print(weight_str_no_dec.head(10))\n",
    "\n",
    "# Step 4: Check for any non-digit characters\n",
    "non_digit_mask = weight_str_no_dec.str.contains('[^0-9]', regex=True, na=False)\n",
    "print(f\"\\nRows with non-digit characters: {non_digit_mask.sum()}\")\n",
    "if non_digit_mask.sum() > 0:\n",
    "    print(\"Sample non-digit rows:\")\n",
    "    print(weight_str_no_dec[non_digit_mask].head())\n",
    "\n",
    "# Step 5: Pad with zeros\n",
    "weight_str_padded = weight_str_no_dec.str.pad(width=5, side='right', fillchar='0')\n",
    "print(f\"\\nAfter padding sample:\")\n",
    "print(weight_str_padded.head(10))\n",
    "\n",
    "# Step 6: Extract first digit\n",
    "try:\n",
    "    first_digit = weight_str_padded.str[0].astype(float)\n",
    "    print(f\"\\nFirst digit extraction successful!\")\n",
    "    print(first_digit.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in first digit extraction: {e}\")\n",
    "    # Find problematic rows\n",
    "    for i, val in enumerate(weight_str_padded):\n",
    "        try:\n",
    "            float(val[0])\n",
    "        except:\n",
    "            print(f\"Problem at index {i}: '{val}'\")\n",
    "            if i > 10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_features(df):\n",
    "    \"\"\"Create proven baseline features only\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Weight Capacity features (proven to work)\n",
    "    if 'Weight Capacity (kg)' in df.columns:\n",
    "        weight = df['Weight Capacity (kg)'].copy()\n",
    "        \n",
    "        # Original value\n",
    "        features['weight_original'] = weight\n",
    "        \n",
    "        # Rounding to different decimals (7-10 as per winning solutions)\n",
    "        for dec in range(7, 11):\n",
    "            features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "        \n",
    "        # Digit extraction (1-5 digits)\n",
    "        weight_str = weight.astype(str).str.replace('.', '')\n",
    "        for i in range(1, 6):\n",
    "            features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "        \n",
    "        # Integer and fractional parts\n",
    "        features['weight_int'] = weight.astype(int)\n",
    "        features['weight_frac'] = weight - weight.astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create baseline features\n",
    "X_baseline = create_baseline_features(combined_train)\n",
    "X_test_baseline = create_baseline_features(test)\n",
    "\n",
    "print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "print(f\"Baseline test features shape: {X_test_baseline.shape}\")\n",
    "print(f\"\\nBaseline feature names:\")\n",
    "print(X_baseline.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug categorical columns for any issues\n",
    "print(\"Checking categorical columns for issues:\")\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        # Check for NaN\n",
    "        nan_count = combined_train[col].isna().sum()\n",
    "        # Check unique values\n",
    "        unique_vals = combined_train[col].nunique()\n",
    "        print(f\"{col}: {unique_vals} unique, {nan_count} NaN\")\n",
    "        \n",
    "        # Check if any values might cause issues\n",
    "        if nan_count > 0:\n",
    "            print(f\"  Sample NaN rows in {col}:\")\n",
    "            print(f\"  {combined_train[col].isna().sum()}\")\n",
    "        \n",
    "        # Check for problematic string patterns\n",
    "        col_str = combined_train[col].astype(str)\n",
    "        if col_str.str.contains('n', na=False).sum() > 0:\n",
    "            print(f\"  WARNING: {col} contains 'n' characters\")\n",
    "            # Show some examples\n",
    "            mask = col_str.str.contains('n', na=False)\n",
    "            print(f\"  Examples: {col_str[mask].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2cb98",
   "metadata": {},
   "source": [
    "## Label Encode Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Fit label encoders on combined train + test data\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined data to handle all categories\n",
    "        combined_data = pd.concat([combined_train[col], test[col]], ignore_index=True)\n",
    "        le.fit(combined_data.astype(str).fillna('missing'))\n",
    "        \n",
    "        # Transform train and test\n",
    "        X_baseline[f'{col}_encoded'] = le.transform(combined_train[col].astype(str).fillna('missing'))\n",
    "        X_test_baseline[f'{col}_encoded'] = le.transform(test[col].astype(str).fillna('missing'))\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Added {len(categorical_cols)} label encoded features\")\n",
    "print(f\"Total baseline features: {X_baseline.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd6bf2",
   "metadata": {},
   "source": [
    "## Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CV\n",
    "y = combined_train['Price'].values\n",
    "X = X_baseline.copy()\n",
    "\n",
    "# 20-fold CV (consistent with previous experiments)\n",
    "n_folds = 20\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"CV folds: {n_folds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37fdab",
   "metadata": {},
   "source": [
    "## Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (from winning solutions)\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "\n",
    "print(\"Starting 20-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(dval)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    fold_scores.append(rmse)\n",
    "    \n",
    "    print(f\"Fold {fold+1:2d}/{n_folds} - RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_score = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "print(f\"\\nOverall CV RMSE: {cv_score:.6f}\")\n",
    "print(f\"Mean fold RMSE: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
    "print(f\"Fold RMSE range: {np.min(fold_scores):.6f} - {np.max(fold_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11015045",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from last fold\n",
    "importance = model.get_score(importance_type='gain')\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': [importance.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 features by importance:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Check for zero-variance features\n",
    "zero_var_features = X.columns[X.nunique() <= 1]\n",
    "if len(zero_var_features) > 0:\n",
    "    print(f\"\\n⚠️  Zero variance features: {zero_var_features.tolist()}\")\n",
    "else:\n",
    "    print(\"\\n✓ No zero variance features detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea63f0",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48edc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "\n",
    "final_model = xgb.train(\n",
    "    params,\n",
    "    dtrain_full,\n",
    "    num_boost_round=2000,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "dtest = xgb.DMatrix(X_test_baseline)\n",
    "test_predictions = final_model.predict(dtest)\n",
    "\n",
    "# Clip predictions to training range\n",
    "min_price = y.min()\n",
    "max_price = y.max()\n",
    "test_predictions = np.clip(test_predictions, min_price, max_price)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test predictions range: {test_predictions.min():.2f} - {test_predictions.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e481e7a",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "# Create submission\n",
    "submission = sample_sub.copy()\n",
    "submission['Price'] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission Price stats:\")\n",
    "print(submission['Price'].describe())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
