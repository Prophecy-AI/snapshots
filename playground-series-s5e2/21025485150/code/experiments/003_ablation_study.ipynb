{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60905c7",
   "metadata": {},
   "source": [
    "# Experiment 003: Systematic Ablation Study\n",
    "\n",
    "## Goal\n",
    "Fix bugs from exp_001 and test features incrementally to identify what works vs. what hurts.\n",
    "\n",
    "## Strategy\n",
    "1. Start with proven baseline: Weight Capacity + digit features only\n",
    "2. Add one feature group at a time\n",
    "3. Measure CV impact for each group\n",
    "4. Keep only features that improve CV by >0.001\n",
    "\n",
    "## Bug Fixes\n",
    "- Remove quantile features (proven constant columns)\n",
    "- Fix count encoding: compute from training data only\n",
    "- Validate each feature before combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5fa16",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d455d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Training extra shape: {training_extra.shape}\")\n",
    "\n",
    "# Combine train and training_extra for more data\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nTarget stats:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7a1ac",
   "metadata": {},
   "source": [
    "## Feature Engineering - Baseline (Proven Features Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_features(df):\n",
    "    \"\"\"Create proven baseline features only\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Weight Capacity features (proven to work)\n",
    "    if 'Weight Capacity (kg)' in df.columns:\n",
    "        weight = df['Weight Capacity (kg)'].copy()\n",
    "        \n",
    "        # Original value\n",
    "        features['weight_original'] = weight\n",
    "        \n",
    "        # Rounding to different decimals (7-10 as per winning solutions)\n",
    "        for dec in range(7, 11):\n",
    "            features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "        \n",
    "        # Digit extraction (1-5 digits)\n",
    "        weight_str = weight.astype(str).str.replace('.', '')\n",
    "        for i in range(1, 6):\n",
    "            features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "        \n",
    "        # Integer and fractional parts\n",
    "        features['weight_int'] = weight.astype(int)\n",
    "        features['weight_frac'] = weight - weight.astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create baseline features\n",
    "X_baseline = create_baseline_features(combined_train)\n",
    "X_test_baseline = create_baseline_features(test)\n",
    "\n",
    "print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "print(f\"Baseline test features shape: {X_test_baseline.shape}\")\n",
    "print(f\"\\nBaseline feature names:\")\n",
    "print(X_baseline.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2cb98",
   "metadata": {},
   "source": [
    "## Label Encode Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Fit label encoders on combined train + test data\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined data to handle all categories\n",
    "        combined_data = pd.concat([combined_train[col], test[col]], ignore_index=True)\n",
    "        le.fit(combined_data.astype(str).fillna('missing'))\n",
    "        \n",
    "        # Transform train and test\n",
    "        X_baseline[f'{col}_encoded'] = le.transform(combined_train[col].astype(str).fillna('missing'))\n",
    "        X_test_baseline[f'{col}_encoded'] = le.transform(test[col].astype(str).fillna('missing'))\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Added {len(categorical_cols)} label encoded features\")\n",
    "print(f\"Total baseline features: {X_baseline.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd6bf2",
   "metadata": {},
   "source": [
    "## Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CV\n",
    "y = combined_train['Price'].values\n",
    "X = X_baseline.copy()\n",
    "\n",
    "# 20-fold CV (consistent with previous experiments)\n",
    "n_folds = 20\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"CV folds: {n_folds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37fdab",
   "metadata": {},
   "source": [
    "## Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (from winning solutions)\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "\n",
    "print(\"Starting 20-fold CV...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(dval)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    fold_scores.append(rmse)\n",
    "    \n",
    "    print(f\"Fold {fold+1:2d}/{n_folds} - RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_score = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "print(f\"\\nOverall CV RMSE: {cv_score:.6f}\")\n",
    "print(f\"Mean fold RMSE: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
    "print(f\"Fold RMSE range: {np.min(fold_scores):.6f} - {np.max(fold_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11015045",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from last fold\n",
    "importance = model.get_score(importance_type='gain')\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': [importance.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 features by importance:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Check for zero-variance features\n",
    "zero_var_features = X.columns[X.nunique() <= 1]\n",
    "if len(zero_var_features) > 0:\n",
    "    print(f\"\\n⚠️  Zero variance features: {zero_var_features.tolist()}\")\n",
    "else:\n",
    "    print(\"\\n✓ No zero variance features detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea63f0",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48edc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "\n",
    "final_model = xgb.train(\n",
    "    params,\n",
    "    dtrain_full,\n",
    "    num_boost_round=2000,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "dtest = xgb.DMatrix(X_test_baseline)\n",
    "test_predictions = final_model.predict(dtest)\n",
    "\n",
    "# Clip predictions to training range\n",
    "min_price = y.min()\n",
    "max_price = y.max()\n",
    "test_predictions = np.clip(test_predictions, min_price, max_price)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test predictions range: {test_predictions.min():.2f} - {test_predictions.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e481e7a",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "# Create submission\n",
    "submission = sample_sub.copy()\n",
    "submission['Price'] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission Price stats:\")\n",
    "print(submission['Price'].describe())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
