{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2955843",
   "metadata": {},
   "source": [
    "# Experiment 006: Original Dataset + COMBO Features\n",
    "\n",
    "**Strategy:** Implement the evolved strategy focusing on:\n",
    "1. Original dataset features (orig_price, orig_price_r7, orig_price_r8, orig_price_r9)\n",
    "2. COMBO/interaction features (NaN encoding, NaN × Weight Capacity, categorical × Weight Capacity)\n",
    "3. Optimize groupby statistics (keep mean, count, median; add skew, kurtosis, percentiles)\n",
    "4. Remove histogram bins (they hurt performance)\n",
    "5. Hyperparameter refinement\n",
    "\n",
    "**Expected improvement:** -0.148 RMSE total, targeting 38.512840 CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a0108e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:16.296619Z",
     "iopub.status.busy": "2026-01-15T22:58:16.296352Z",
     "iopub.status.idle": "2026-01-15T22:58:17.195936Z",
     "shell.execute_reply": "2026-01-15T22:58:17.195391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (300000, 11)\n",
      "Test shape: (200000, 10)\n",
      "Original dataset shape: (52500, 10)\n",
      "\n",
      "Original dataset columns: ['Brand', 'Material', 'Size', 'Compartments', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Weight Capacity (kg)', 'Price']\n",
      "\n",
      "Original dataset sample:\n",
      "          Brand Material   Size  Compartments Laptop Compartment Waterproof  \\\n",
      "0      Jansport    Nylon  Small           2.0                 No        Yes   \n",
      "1  Under Armour    Nylon  Large           4.0                Yes        Yes   \n",
      "2          Nike    Nylon  Large           NaN                 No        Yes   \n",
      "3          Nike    Nylon  Small           1.0                Yes         No   \n",
      "4  Under Armour  Leather  Small           8.0                Yes         No   \n",
      "\n",
      "       Style  Color  Weight Capacity (kg)       Price  \n",
      "0   Backpack  Green             13.340058  143.445135  \n",
      "1       Tote   Pink              5.918030   72.086319  \n",
      "2  Messenger    Red             24.088386   29.699631  \n",
      "3  Messenger   Pink              5.000000   27.181990  \n",
      "4        NaN  Black             11.258172   71.953236  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "submission = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "# Load original dataset\n",
    "original = pd.read_csv('/home/code/original_dataset/Noisy_Student_Bag_Price_Prediction_Dataset.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Original dataset shape: {original.shape}\")\n",
    "print(f\"\\nOriginal dataset columns: {original.columns.tolist()}\")\n",
    "print(f\"\\nOriginal dataset sample:\")\n",
    "print(original.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374ac14",
   "metadata": {},
   "source": [
    "## Step 1: Compute Original Dataset Features\n",
    "\n",
    "Following the winning solution, compute:\n",
    "- orig_price: mean Price by Weight Capacity\n",
    "- orig_price_r7, orig_price_r8, orig_price_r9: mean Price by rounded Weight Capacity (7, 8, 9 decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f9f979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:17.200400Z",
     "iopub.status.busy": "2026-01-15T22:58:17.200223Z",
     "iopub.status.idle": "2026-01-15T22:58:17.317663Z",
     "shell.execute_reply": "2026-01-15T22:58:17.317240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset features computed:\n",
      "   weight_capacity  orig_price  weight_capacity_r7  orig_price_r7  \\\n",
      "0         5.000000   80.693646            5.000000      80.693646   \n",
      "1         5.001061   93.862638            5.001061      93.862638   \n",
      "2         5.004444  130.627948            5.004444     130.627948   \n",
      "3         5.004837   76.920155            5.004837      76.920155   \n",
      "4         5.005468  101.682464            5.005468     101.682464   \n",
      "5         5.005485   66.103371            5.005485      66.103371   \n",
      "6         5.006887   83.136924            5.006887      83.136924   \n",
      "7         5.008382  133.934556            5.008382     133.934556   \n",
      "8         5.009264  104.682819            5.009264     104.682819   \n",
      "9         5.009461   75.432553            5.009461      75.432553   \n",
      "\n",
      "   weight_capacity_r8  orig_price_r8  weight_capacity_r9  orig_price_r9  \n",
      "0            5.000000      80.693646            5.000000      80.693646  \n",
      "1            5.001061      93.862638            5.001061      93.862638  \n",
      "2            5.004444     130.627948            5.004444     130.627948  \n",
      "3            5.004837      76.920155            5.004837      76.920155  \n",
      "4            5.005468     101.682464            5.005468     101.682464  \n",
      "5            5.005485      66.103371            5.005485      66.103371  \n",
      "6            5.006887      83.136924            5.006887      83.136924  \n",
      "7            5.008382     133.934556            5.008382     133.934556  \n",
      "8            5.009264     104.682819            5.009264     104.682819  \n",
      "9            5.009461      75.432553            5.009461      75.432553  \n",
      "\n",
      "Shape: (46957, 8)\n",
      "NaN counts: 9336\n"
     ]
    }
   ],
   "source": [
    "# Extract Weight Capacity from original dataset\n",
    "# The original dataset has 'Weight Capacity (kg)' column\n",
    "original['weight_capacity'] = original['Weight Capacity (kg)'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "# Compute orig_price: mean Price by Weight Capacity\n",
    "orig_price = original.groupby('weight_capacity')['Price'].mean().reset_index()\n",
    "orig_price.columns = ['weight_capacity', 'orig_price']\n",
    "\n",
    "# Compute rounded versions\n",
    "for decimals in [7, 8, 9]:\n",
    "    col_name = f'weight_capacity_r{decimals}'\n",
    "    orig_price[col_name] = orig_price['weight_capacity'].round(decimals)\n",
    "    \n",
    "    # Compute mean by rounded weight capacity\n",
    "    temp = original.copy()\n",
    "    temp['weight_rounded'] = temp['weight_capacity'].round(decimals)\n",
    "    rounded_price = temp.groupby('weight_rounded')['Price'].mean().reset_index()\n",
    "    rounded_price.columns = [col_name, f'orig_price_r{decimals}']\n",
    "    \n",
    "    # Merge back\n",
    "    orig_price = orig_price.merge(rounded_price, on=col_name, how='left')\n",
    "\n",
    "print(\"Original dataset features computed:\")\n",
    "print(orig_price.head(10))\n",
    "print(f\"\\nShape: {orig_price.shape}\")\n",
    "print(f\"NaN counts: {orig_price.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d3f5e",
   "metadata": {},
   "source": [
    "## Step 2: Extract Weight Capacity from Current Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f94a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:17.319057Z",
     "iopub.status.busy": "2026-01-15T22:58:17.318687Z",
     "iopub.status.idle": "2026-01-15T22:58:18.304771Z",
     "shell.execute_reply": "2026-01-15T22:58:18.304369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight capacity extracted:\n",
      "   Weight Capacity (kg)  weight_capacity  weight_capacity_r7  \\\n",
      "0             11.611723        11.611723           11.611723   \n",
      "1             27.078537        27.078537           27.078537   \n",
      "2             16.643760        16.643760           16.643760   \n",
      "3             12.937220        12.937220           12.937220   \n",
      "4             17.749338        17.749338           17.749339   \n",
      "\n",
      "   weight_capacity_r8  weight_capacity_r9  \n",
      "0           11.611723           11.611723  \n",
      "1           27.078537           27.078537  \n",
      "2           16.643760           16.643760  \n",
      "3           12.937220           12.937220  \n",
      "4           17.749338           17.749338  \n"
     ]
    }
   ],
   "source": [
    "# Extract weight capacity from current dataset\n",
    "def extract_weight_capacity(df):\n",
    "    \"\"\"Extract numeric weight capacity from Weight Capacity column\"\"\"\n",
    "    df = df.copy()\n",
    "    df['weight_capacity'] = df['Weight Capacity (kg)'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "    \n",
    "    # Create rounded versions\n",
    "    for decimals in [7, 8, 9]:\n",
    "        df[f'weight_capacity_r{decimals}'] = df['weight_capacity'].round(decimals)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = extract_weight_capacity(train)\n",
    "test = extract_weight_capacity(test)\n",
    "\n",
    "print(\"Weight capacity extracted:\")\n",
    "print(train[['Weight Capacity (kg)', 'weight_capacity', 'weight_capacity_r7', 'weight_capacity_r8', 'weight_capacity_r9']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9758cb22",
   "metadata": {},
   "source": [
    "## Step 3: Merge Original Dataset Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0d47f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:18.305920Z",
     "iopub.status.busy": "2026-01-15T22:58:18.305815Z",
     "iopub.status.idle": "2026-01-15T22:58:18.801649Z",
     "shell.execute_reply": "2026-01-15T22:58:18.801274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features merged:\n",
      "   weight_capacity  orig_price  orig_price_r7  orig_price_r8  orig_price_r9\n",
      "0        11.611723   39.770555      39.770555      39.770555      39.770555\n",
      "1        27.078537         NaN            NaN            NaN            NaN\n",
      "2        16.643760         NaN            NaN            NaN            NaN\n",
      "3        12.937220         NaN            NaN            NaN            NaN\n",
      "4        17.749338         NaN            NaN            NaN            NaN\n",
      "\n",
      "NaN counts in original features:\n",
      "orig_price       163393\n",
      "orig_price_r7    148490\n",
      "orig_price_r8    151921\n",
      "orig_price_r9    154435\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge original dataset features\n",
    "def merge_orig_features(df, orig_features):\n",
    "    \"\"\"Merge original dataset price features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Merge by exact weight capacity\n",
    "    df = df.merge(orig_features[['weight_capacity', 'orig_price']], on='weight_capacity', how='left')\n",
    "    \n",
    "    # Merge by rounded weight capacity\n",
    "    for decimals in [7, 8, 9]:\n",
    "        df = df.merge(\n",
    "            orig_features[['weight_capacity_r' + str(decimals), 'orig_price_r' + str(decimals)]],\n",
    "            on='weight_capacity_r' + str(decimals),\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = merge_orig_features(train, orig_price)\n",
    "test = merge_orig_features(test, orig_price)\n",
    "\n",
    "print(\"Original features merged:\")\n",
    "print(train[['weight_capacity', 'orig_price', 'orig_price_r7', 'orig_price_r8', 'orig_price_r9']].head())\n",
    "print(f\"\\nNaN counts in original features:\")\n",
    "print(train[['orig_price', 'orig_price_r7', 'orig_price_r8', 'orig_price_r9']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1483981",
   "metadata": {},
   "source": [
    "## Step 4: Create COMBO/Interaction Features\n",
    "\n",
    "Following winning solution pattern:\n",
    "- NaNs: Base-2 encoding of all NaN patterns\n",
    "- {col}_nan_wc: NaN status × Weight Capacity for each of 7 categorical columns\n",
    "- {col}_wc: Factorized categorical × Weight Capacity for each of 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da936f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:18.802927Z",
     "iopub.status.busy": "2026-01-15T22:58:18.802814Z",
     "iopub.status.idle": "2026-01-15T22:58:18.997158Z",
     "shell.execute_reply": "2026-01-15T22:58:18.996707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN encoding created:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs\n",
      "0     246734\n",
      "64      8370\n",
      "1       8313\n",
      "2       7277\n",
      "32      7011\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define categorical columns\n",
    "cat_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Step 4a: Base-2 NaN encoding\n",
    "def create_nan_encoding(df):\n",
    "    \"\"\"Create base-2 encoding of NaN patterns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create binary pattern for NaN status\n",
    "    nan_pattern = 0\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        is_nan = df[col].isna().astype(int)\n",
    "        nan_pattern += is_nan * (2 ** i)\n",
    "    \n",
    "    df['NaNs'] = nan_pattern\n",
    "    return df\n",
    "\n",
    "train = create_nan_encoding(train)\n",
    "test = create_nan_encoding(test)\n",
    "\n",
    "print(\"NaN encoding created:\")\n",
    "print(train['NaNs'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfb19e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:18.998402Z",
     "iopub.status.busy": "2026-01-15T22:58:18.998262Z",
     "iopub.status.idle": "2026-01-15T22:58:19.112840Z",
     "shell.execute_reply": "2026-01-15T22:58:19.112441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN × Weight Capacity features created\n",
      "New features: ['Brand_nan_wc', 'Material_nan_wc', 'Size_nan_wc', 'Laptop Compartment_nan_wc', 'Waterproof_nan_wc', 'Style_nan_wc', 'Color_nan_wc']\n"
     ]
    }
   ],
   "source": [
    "# Step 4b: {col}_nan_wc features (NaN status × Weight Capacity)\n",
    "for col in cat_cols:\n",
    "    train[f'{col}_nan_wc'] = train[col].isna().astype(int) * train['weight_capacity']\n",
    "    test[f'{col}_nan_wc'] = test[col].isna().astype(int) * test['weight_capacity']\n",
    "\n",
    "print(\"NaN × Weight Capacity features created\")\n",
    "print(f\"New features: {[f'{col}_nan_wc' for col in cat_cols]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1530528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T22:58:19.113873Z",
     "iopub.status.busy": "2026-01-15T22:58:19.113762Z",
     "iopub.status.idle": "2026-01-15T22:58:19.347523Z",
     "shell.execute_reply": "2026-01-15T22:58:19.347116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical × Weight Capacity features created\n",
      "Sample:     Brand_wc  Material_wc    Size_wc\n",
      "0  23.223446    23.223446  23.223446\n",
      "1  54.157073    27.078537  81.235610\n",
      "2  83.218800    33.287520  49.931280\n",
      "3  38.811661    38.811661  38.811661\n",
      "4  17.749338    17.749338  35.498677\n"
     ]
    }
   ],
   "source": [
    "# Step 4c: {col}_wc features (factorized categorical × Weight Capacity)\n",
    "# First, factorize categoricals (handle NaNs)\n",
    "for col in cat_cols:\n",
    "    # Combine train and test for consistent factorization\n",
    "    combined = pd.concat([train[col], test[col]], axis=0)\n",
    "    \n",
    "    # Factorize (NaN becomes -1, we add 1 to make it 0)\n",
    "    codes, categories = pd.factorize(combined, sort=True)\n",
    "    codes = codes + 1  # Shift so NaN becomes 0 instead of -1\n",
    "    \n",
    "    # Split back\n",
    "    train[f'{col}_factorized'] = codes[:len(train)]\n",
    "    test[f'{col}_factorized'] = codes[len(train):]\n",
    "    \n",
    "    # Create interaction with weight capacity\n",
    "    train[f'{col}_wc'] = train[f'{col}_factorized'] * train['weight_capacity']\n",
    "    test[f'{col}_wc'] = test[f'{col}_factorized'] * test['weight_capacity']\n",
    "\n",
    "print(\"Categorical × Weight Capacity features created\")\n",
    "print(f\"Sample: {train[['Brand_wc', 'Material_wc', 'Size_wc']].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0df45a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T23:09:31.600557Z",
     "iopub.status.busy": "2026-01-15T23:09:31.600354Z",
     "iopub.status.idle": "2026-01-15T23:09:33.033662Z",
     "shell.execute_reply": "2026-01-15T23:09:33.033205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for group key: weight_capacity\n",
      "Computing stats for group key: Brand\n",
      "Computing stats for group key: Material\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for group key: Size\n",
      "Computing stats for group key: Laptop Compartment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for group key: Waterproof\n",
      "Computing stats for group key: Style\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for group key: Color\n",
      "Groupby statistics computed\n",
      "New features added: 28\n",
      "Sample features: ['Waterproof_count_price', 'Waterproof_median_price', 'Style_mean_price', 'Style_count_price', 'Style_median_price', 'Color_mean_price', 'Color_count_price', 'Color_median_price']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define group keys (8 total: weight_capacity + 7 categoricals)\n",
    "group_keys = ['weight_capacity'] + cat_cols\n",
    "\n",
    "# Statistics to compute - simplify to avoid errors with small groups\n",
    "stats_to_compute = ['mean', 'count', 'median']\n",
    "\n",
    "def compute_groupby_stats(train_df, test_df, target_col='Price'):\n",
    "    \"\"\"Compute groupby statistics with simple approach\"\"\"\n",
    "    \n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    for group_key in group_keys:\n",
    "        print(f\"Computing stats for group key: {group_key}\")\n",
    "        \n",
    "        # Initialize columns for each statistic\n",
    "        for stat in stats_to_compute:\n",
    "            train_df[f'{group_key}_{stat}_price'] = np.nan\n",
    "            test_df[f'{group_key}_{stat}_price'] = np.nan\n",
    "        \n",
    "        # Compute statistics for each group\n",
    "        if group_key == 'weight_capacity':\n",
    "            # For numeric weight_capacity, use simple groupby\n",
    "            grouped = train_df.groupby(group_key)[target_col].agg(['mean', 'count', 'median'])\n",
    "            \n",
    "            # Map back to train and test\n",
    "            for stat in stats_to_compute:\n",
    "                train_df[f'{group_key}_{stat}_price'] = train_df[group_key].map(grouped[stat])\n",
    "                test_df[f'{group_key}_{stat}_price'] = test_df[group_key].map(grouped[stat])\n",
    "        else:\n",
    "            # For categorical columns\n",
    "            # Factorize first to handle NaNs and get numeric codes\n",
    "            combined = pd.concat([train_df[group_key], test_df[group_key]], axis=0)\n",
    "            codes, _ = pd.factorize(combined, sort=True)\n",
    "            codes = codes + 1  # Make NaN = 0\n",
    "            \n",
    "            train_df[f'{group_key}_code'] = codes[:len(train_df)]\n",
    "            test_df[f'{group_key}_code'] = codes[len(train_df):]\n",
    "            \n",
    "            # Group by the code and compute statistics\n",
    "            grouped = train_df.groupby(f'{group_key}_code')[target_col].agg(['mean', 'count', 'median'])\n",
    "            \n",
    "            # Map back\n",
    "            for stat in stats_to_compute:\n",
    "                train_df[f'{group_key}_{stat}_price'] = train_df[f'{group_key}_code'].map(grouped[stat])\n",
    "                test_df[f'{group_key}_{stat}_price'] = test_df[f'{group_key}_code'].map(grouped[stat])\n",
    "            \n",
    "            # Clean up temporary code column\n",
    "            train_df.drop(columns=[f'{group_key}_code'], inplace=True)\n",
    "            test_df.drop(columns=[f'{group_key}_code'], inplace=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Execute the groupby statistics computation\n",
    "train, test = compute_groupby_stats(train, test)\n",
    "\n",
    "print(\"Groupby statistics computed\")\n",
    "print(f\"New features added: {len([col for col in train.columns if '_price' in col and col != 'Price'])}\")\n",
    "print(f\"Sample features: {train.columns[-8:].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c01ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the groupby statistics computation from cell 12\n",
    "train, test = compute_groupby_stats(train, test)\n",
    "\n",
    "print(\"Groupby statistics computed\")\n",
    "print(f\"New features added: {len([col for col in train.columns if '_price' in col and col != 'Price'])}\")\n",
    "print(f\"Sample features: {train.columns[-8:].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc338286",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Final Feature Set\n",
    "\n",
    "Remove histogram bins (not in winning solution) and prepare features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0363e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature columns (exclude target, ID, and raw categorical columns)\n",
    "exclude_cols = ['Price', 'id', 'Weight Capacity', 'weight_capacity_r7', 'weight_capacity_r8', 'weight_capacity_r9']\n",
    "exclude_cols += cat_cols  # Exclude raw categorical columns (they're object type)\n",
    "\n",
    "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features before selection: {len(feature_cols)}\")\n",
    "\n",
    "# Count feature types\n",
    "orig_features = [col for col in feature_cols if 'orig_price' in col]\n",
    "combo_features = [col for col in feature_cols if '_nan_wc' in col or '_wc' in col or col == 'NaNs']\n",
    "groupby_features = [col for col in feature_cols if '_price' in col and col != 'Price']\n",
    "other_features = [col for col in feature_cols if col not in orig_features + combo_features + groupby_features]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"- Original dataset features: {len(orig_features)}\")\n",
    "print(f\"- COMBO/interaction features: {len(combo_features)}\")\n",
    "print(f\"- Groupby statistics: {len(groupby_features)}\")\n",
    "print(f\"- Other features: {len(other_features)}\")\n",
    "print(f\"\\nTotal selected features: {len(feature_cols)}\")\n",
    "print(f\"\\nSample features: {feature_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61ae76",
   "metadata": {},
   "source": [
    "## Step 7: Model Training with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Prepare data\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train['Price'].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Hyperparameters (optimized per strategy)\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.03,  # Reduced from 0.05\n",
    "    'max_depth': 10,        # Increased from 8\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,       # Added L1 regularization\n",
    "    'reg_lambda': 1.0,      # Added L2 regularization\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "n_folds = 20\n",
    "time_budget = 3600  # 1 hour in seconds\n",
    "start_time = time.time()\n",
    "\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "print(f\"Starting {n_folds}-fold CV training...\")\n",
    "print(f\"Time budget: {time_budget} seconds\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    # Check time budget\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed > time_budget * 0.8:  # Stop if 80% of time used\n",
    "        print(f\"Stopping early due to time budget. Completed {fold} folds.\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Fold {fold+1}/{n_folds}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    val_pred = model.predict(dval)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    print(f\"  Fold {fold+1} RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    # Store model\n",
    "    models.append(model)\n",
    "\n",
    "print(f\"\\nCV completed. Scores: {rmse_scores}\")\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores):.6f} ± {np.std(rmse_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b47aa",
   "metadata": {},
   "source": [
    "## Step 8: Generate Predictions and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using ensemble of all folds\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "predictions = np.zeros(len(X_test))\n",
    "\n",
    "for model in models:\n",
    "    predictions += model.predict(dtest)\n",
    "\n",
    "# Average predictions\n",
    "predictions /= len(models)\n",
    "\n",
    "# Create submission - ensure we use the correct index\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Price': predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('/home/code/submission_candidates/candidate_006.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved to /home/code/submission_candidates/candidate_006.csv\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Prediction range: [{predictions.min():.2f}, {predictions.max():.2f}]\")\n",
    "print(f\"Prediction mean: {predictions.mean():.2f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\nTop 20 features by importance:\")\n",
    "importance_dict = models[0].get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': list(importance_dict.keys()),\n",
    "    'importance': list(importance_dict.values())\n",
    "})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "print(importance_df.head(20))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
