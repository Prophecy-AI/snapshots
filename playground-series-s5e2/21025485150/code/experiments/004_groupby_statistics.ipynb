{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508a0b5b",
   "metadata": {},
   "source": [
    "# Experiment 004: Groupby Statistics Features\n",
    "\n",
    "## Goal\n",
    "Implement groupby statistics following the winning solution pattern that showed 0.71 correlation vs <0.02 for simple transformations.\n",
    "\n",
    "## Strategy\n",
    "Based on analysis in evolver_loop4_analysis.ipynb:\n",
    "- Groupby aggregations have 0.71 correlation with target\n",
    "- Simple weight transformations have <0.02 correlation\n",
    "- Winning pattern: `groupby(COL1)[COL2].agg(STAT)` where COL2 is the target\n",
    "\n",
    "## Implementation\n",
    "1. Groupby Weight Capacity: mean, std, count, min, max, median of Price\n",
    "2. Groupby each categorical: mean, std, count of Price  \n",
    "3. Use nested CV to prevent leakage\n",
    "4. Merge back to train/test based on the group key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5e19c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:09:15.971659Z",
     "iopub.status.busy": "2026-01-15T16:09:15.971335Z",
     "iopub.status.idle": "2026-01-15T16:09:18.119725Z",
     "shell.execute_reply": "2026-01-15T16:09:18.119226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "GPU Memory: 85.0 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa503148",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a1c877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:09:18.125593Z",
     "iopub.status.busy": "2026-01-15T16:09:18.125386Z",
     "iopub.status.idle": "2026-01-15T16:09:22.055700Z",
     "shell.execute_reply": "2026-01-15T16:09:22.055218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (300000, 11)\n",
      "Test shape: (200000, 10)\n",
      "Training extra shape: (3694318, 11)\n",
      "Combined train shape: (3994318, 11)\n",
      "\n",
      "Target stats:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.994318e+06\n",
      "mean     8.136217e+01\n",
      "std      3.893868e+01\n",
      "min      1.500000e+01\n",
      "25%      4.747002e+01\n",
      "50%      8.098495e+01\n",
      "75%      1.148550e+02\n",
      "max      1.500000e+02\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "training_extra = pd.read_csv('/home/data/training_extra.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Training extra shape: {training_extra.shape}\")\n",
    "\n",
    "# Combine train and training_extra for more data\n",
    "combined_train = pd.concat([train, training_extra], ignore_index=True)\n",
    "print(f\"Combined train shape: {combined_train.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nTarget stats:\")\n",
    "print(combined_train['Price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce351d",
   "metadata": {},
   "source": [
    "## Create Baseline Features (from exp_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b2992d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:09:22.056975Z",
     "iopub.status.busy": "2026-01-15T16:09:22.056862Z",
     "iopub.status.idle": "2026-01-15T16:09:30.476109Z",
     "shell.execute_reply": "2026-01-15T16:09:30.475540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline features shape: (3994318, 12)\n",
      "Baseline test features shape: (200000, 12)\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_features(df):\n",
    "    \"\"\"Create proven baseline features from exp_003\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Weight Capacity features (proven to work)\n",
    "    if 'Weight Capacity (kg)' in df.columns:\n",
    "        weight = df['Weight Capacity (kg)'].copy()\n",
    "        \n",
    "        # Original value\n",
    "        features['weight_original'] = weight\n",
    "        \n",
    "        # Rounding to different decimals (7-10 as per winning solutions)\n",
    "        for dec in range(7, 11):\n",
    "            features[f'weight_round_{dec}'] = np.round(weight, decimals=dec)\n",
    "        \n",
    "        # Digit extraction (1-5 digits) - handle NaN properly\n",
    "        # Replace NaN with 0 for digit extraction\n",
    "        weight_filled = weight.fillna(0)\n",
    "        weight_str = weight_filled.astype(str).str.replace('.', '', regex=False)\n",
    "        \n",
    "        # Remove decimal point and get first 5 digits\n",
    "        # Pad with zeros if needed\n",
    "        weight_str = weight_str.str.pad(width=5, side='right', fillchar='0')\n",
    "        \n",
    "        for i in range(1, 6):\n",
    "            features[f'weight_digit_{i}'] = weight_str.str[i-1].astype(float)\n",
    "        \n",
    "        # Integer and fractional parts\n",
    "        features['weight_int'] = weight.fillna(0).astype(int)\n",
    "        features['weight_frac'] = weight.fillna(0) - weight.fillna(0).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create baseline features\n",
    "X_baseline = create_baseline_features(combined_train)\n",
    "X_test_baseline = create_baseline_features(test)\n",
    "\n",
    "print(f\"Baseline features shape: {X_baseline.shape}\")\n",
    "print(f\"Baseline test features shape: {X_test_baseline.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85b956",
   "metadata": {},
   "source": [
    "## Label Encode Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Fit label encoders on combined train + test data\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in combined_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined data to handle all categories\n",
    "        combined_data = pd.concat([combined_train[col], test[col]], ignore_index=True)\n",
    "        le.fit(combined_data.astype(str).fillna('missing'))\n",
    "        \n",
    "        # Transform train and test\n",
    "        X_baseline[f'{col}_encoded'] = le.transform(combined_train[col].astype(str).fillna('missing'))\n",
    "        X_test_baseline[f'{col}_encoded'] = le.transform(test[col].astype(str).fillna('missing'))\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Added {len(categorical_cols)} label encoded features\")\n",
    "print(f\"Total baseline features: {X_baseline.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61fff",
   "metadata": {},
   "source": [
    "## Groupby Statistics - Core Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced44707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groupby_features(train_df, test_df, target_col='Price', n_folds=5):\n",
    "    \"\"\"\n",
    "    Create groupby statistics features using nested CV to prevent leakage\n",
    "    \n",
    "    Pattern: groupby(COL1)[COL2].agg(STAT) where COL2 is the target\n",
    "    \"\"\"\n",
    "    \n",
    "    # Features dataframe\n",
    "    features_train = pd.DataFrame(index=train_df.index)\n",
    "    features_test = pd.DataFrame(index=test_df.index)\n",
    "    \n",
    "    # Group keys to test\n",
    "    group_keys = ['Weight Capacity (kg)'] + categorical_cols\n",
    "    \n",
    "    # Statistics to compute\n",
    "    stats = ['mean', 'std', 'count', 'min', 'max', 'median']\n",
    "    \n",
    "    for key in group_keys:\n",
    "        print(f\"Processing groupby features for: {key}\")\n",
    "        \n",
    "        if key not in train_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # For test set, we'll use the full training data to compute statistics\n",
    "        # For training set, use nested CV to prevent leakage\n",
    "        \n",
    "        # Test set features (use full training data - no leakage risk)\n",
    "        test_group_stats = train_df.groupby(key)[target_col].agg(stats)\n",
    "        test_group_stats.columns = [f'{key}_{stat}_price' for stat in stats]\n",
    "        \n",
    "        # Merge to test features\n",
    "        test_merged = test_df[[key]].merge(\n",
    "            test_group_stats, \n",
    "            left_on=key, \n",
    "            right_index=True, \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Add to test features\n",
    "        for col in test_group_stats.columns:\n",
    "            features_test[col] = test_merged[col].values\n",
    "        \n",
    "        # Training set features (use nested CV)\n",
    "        # Initialize columns\n",
    "        for stat in stats:\n",
    "            features_train[f'{key}_{stat}_price'] = np.nan\n",
    "        \n",
    "        # Create KFold for nested CV\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "            # Compute statistics on training portion only\n",
    "            train_fold = train_df.iloc[train_idx]\n",
    "            fold_group_stats = train_fold.groupby(key)[target_col].agg(stats)\n",
    "            fold_group_stats.columns = [f'{key}_{stat}_price' for stat in stats]\n",
    "            \n",
    "            # Merge to validation portion\n",
    "            val_fold = train_df.iloc[val_idx]\n",
    "            val_merged = val_fold[[key]].merge(\n",
    "                fold_group_stats,\n",
    "                left_on=key,\n",
    "                right_index=True,\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Assign to features_train at validation indices\n",
    "            for col in fold_group_stats.columns:\n",
    "                features_train.loc[val_idx, col] = val_merged[col].values\n",
    "        \n",
    "        # Fill any remaining NaN values (for keys not seen in training)\n",
    "        for col in features_train.columns:\n",
    "            if col.startswith(f'{key}_'):\n",
    "                features_train[col] = features_train[col].fillna(train_df[target_col].mean())\n",
    "                features_test[col] = features_test[col].fillna(train_df[target_col].mean())\n",
    "    \n",
    "    return features_train, features_test\n",
    "\n",
    "print(\"Creating groupby statistics features...\")\n",
    "groupby_features_train, groupby_features_test = create_groupby_features(\n",
    "    combined_train, test, target_col='Price', n_folds=5\n",
    ")\n",
    "\n",
    "print(f\"Groupby features train shape: {groupby_features_train.shape}\")\n",
    "print(f\"Groupby features test shape: {groupby_features_test.shape}\")\n",
    "print(f\"\\nSample groupby features:\")\n",
    "print(groupby_features_train.columns.tolist()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f92d4",
   "metadata": {},
   "source": [
    "## Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine baseline and groupby features\n",
    "X = pd.concat([X_baseline, groupby_features_train], axis=1)\n",
    "X_test = pd.concat([X_test_baseline, groupby_features_test], axis=1)\n",
    "\n",
    "print(f\"Final training features shape: {X.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")\n",
    "print(f\"\\nFeature groups:\")\n",
    "print(f\"- Baseline features: {X_baseline.shape[1]}\")\n",
    "print(f\"- Groupby features: {groupby_features_train.shape[1]}\")\n",
    "print(f\"- Total: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbff62",
   "metadata": {},
   "source": [
    "## Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CV\n",
    "y = combined_train['Price'].values\n",
    "\n",
    "# 20-fold CV (consistent with previous experiments)\n",
    "n_folds = 20\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"CV folds: {n_folds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f993b",
   "metadata": {},
   "source": [
    "## Train Model with Groupby Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (from winning solutions)\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(X))\n",
    "\n",
    "print(\"Starting 20-fold CV with groupby features...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(dval)\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    fold_scores.append(rmse)\n",
    "    \n",
    "    print(f\"Fold {fold+1:2d}/{n_folds} - RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_score = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CV RMSE: {cv_score:.6f} ± {cv_std:.6f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Compare to baseline\n",
    "baseline_score = 38.825723\n",
    "improvement = baseline_score - cv_score\n",
    "print(f\"Improvement over exp_003 baseline: {improvement:.6f}\")\n",
    "print(f\"Target: 38.616280 (gap: {cv_score - 38.616280:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66f3f6",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77077407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from last fold\n",
    "importance = model.get_score(importance_type='gain')\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': [importance.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 features by importance:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Check if groupby features are at the top\n",
    "groupby_feature_importance = importance_df[importance_df['feature'].str.contains('_price')]\n",
    "print(f\"\\nGroupby feature importance (top 10):\")\n",
    "print(groupby_feature_importance.head(10))\n",
    "\n",
    "# Check for zero-variance features\n",
    "zero_var_features = X.columns[X.nunique() <= 1]\n",
    "if len(zero_var_features) > 0:\n",
    "    print(f\"\\n⚠️  Zero variance features: {zero_var_features.tolist()}\")\n",
    "else:\n",
    "    print(\"\\n✓ No zero variance features detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4a650",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "\n",
    "final_model = xgb.train(\n",
    "    params,\n",
    "    dtrain_full,\n",
    "    num_boost_round=2000,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predict on test\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "test_predictions = final_model.predict(dtest)\n",
    "\n",
    "# Clip predictions to training range\n",
    "min_price = y.min()\n",
    "max_price = y.max()\n",
    "test_predictions = np.clip(test_predictions, min_price, max_price)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test predictions range: {test_predictions.min():.2f} - {test_predictions.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa9959",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "# Create submission\n",
    "submission = sample_sub.copy()\n",
    "submission['Price'] = test_predictions\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission Price stats:\")\n",
    "print(submission['Price'].describe())\n",
    "\n",
    "# Also save to candidates folder\n",
    "candidate_path = '/home/code/submission_candidates/candidate_004.csv'\n",
    "submission.to_csv(candidate_path, index=False)\n",
    "print(f\"\\nCandidate also saved to: {candidate_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
