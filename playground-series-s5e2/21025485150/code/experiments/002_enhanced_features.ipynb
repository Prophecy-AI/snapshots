{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7763c264",
   "metadata": {},
   "source": [
    "# Enhanced Feature Engineering Experiment\n",
    "\n",
    "Implementing comprehensive feature engineering based on winning solution strategies:\n",
    "- Enhanced Weight Capacity features (50 bins, quantiles, rounding)\n",
    "- Target encoding with nested CV (Color, Material, Brand)\n",
    "- Categorical interactions (Brand_Size, Size_Color, Size_Style)\n",
    "- Count encoding for all categoricals\n",
    "- Hyperparameter tuning (lower LR, deeper trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3659e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "train1 = pd.read_csv('/home/data/train.csv')\n",
    "train2 = pd.read_csv('/home/data/training_extra.csv')\n",
    "train = pd.concat([train1, train2], ignore_index=True)\n",
    "\n",
    "print(f\"Combined training shape: {train.shape}\")\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Identify features\n",
    "cat_features = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "target_col = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "for col in cat_features:\n",
    "    train[col] = train[col].fillna('Missing')\n",
    "    test[col] = test[col].fillna('Missing')\n",
    "\n",
    "# Numerical missing values\n",
    "train['Compartments'] = train['Compartments'].fillna(train['Compartments'].median())\n",
    "test['Compartments'] = test['Compartments'].fillna(train['Compartments'].median())\n",
    "\n",
    "train['Weight Capacity (kg)'] = train['Weight Capacity (kg)'].fillna(train['Weight Capacity (kg)'].median())\n",
    "test['Weight Capacity (kg)'] = test['Weight Capacity (kg)'].fillna(train['Weight Capacity (kg)'].median())\n",
    "\n",
    "print(\"Missing values handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Enhanced Weight Capacity Features (Highest Priority)\n",
    "print(\"Creating enhanced Weight Capacity features...\")\n",
    "\n",
    "def create_weight_capacity_features(df):\n",
    "    df = df.copy()\n",
    "    wc = df['Weight Capacity (kg)']\n",
    "    \n",
    "    # 50 uniform bins (best from analysis)\n",
    "    df['weight_bin_50'] = pd.cut(wc, bins=50, labels=False, retbins=False)\n",
    "    \n",
    "    # Rounding to 7-10 decimal places (winning solution pattern)\n",
    "    for dec in [7, 8, 9, 10]:\n",
    "        df[f'weight_round_{dec}'] = wc.round(dec)\n",
    "    \n",
    "    # Digit extraction (1-5 digits)\n",
    "    for k in range(1, 6):\n",
    "        df[f'weight_digit_{k}'] = ((wc * 10**k) % 10).fillna(-1)\n",
    "    \n",
    "    # Basic components\n",
    "    df['weight_int'] = wc.astype(int)\n",
    "    df['weight_frac'] = wc - df['weight_int']\n",
    "    \n",
    "    # Quantile features from training data\n",
    "    quantiles = [0.25, 0.5, 0.75, 0.9]\n",
    "    for q in quantiles:\n",
    "        df[f'weight_q{int(q*100)}'] = wc.quantile(q)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = create_weight_capacity_features(train)\n",
    "test = create_weight_capacity_features(test)\n",
    "\n",
    "print(f\"Weight capacity features created. New shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c99aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Count Encoding (Medium Priority) - Do this before target encoding\n",
    "print(\"Creating count encoding features...\")\n",
    "\n",
    "def create_count_encoding(df, df_test, cat_cols):\n",
    "    \"\"\"Create count encoding for categorical features\"\"\"\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Compute counts from combined train+test\n",
    "        combined = pd.concat([df[col], df_test[col]], ignore_index=True)\n",
    "        counts = combined.value_counts()\n",
    "        \n",
    "        df[f'{col}_count'] = df[col].map(counts)\n",
    "        df_test[f'{col}_count'] = df_test[col].map(counts)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "train, test = create_count_encoding(train, test, cat_features)\n",
    "print(\"Count encoding completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d11187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Categorical Interaction Features (High Priority)\n",
    "print(\"Creating categorical interaction features...\")\n",
    "\n",
    "def create_interaction_features(df, df_test):\n",
    "    \"\"\"Create interaction features with strong signals\"\"\"\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy()\n",
    "    \n",
    "    # Brand_Size: Strongest signal (0.49)\n",
    "    df['Brand_Size'] = df['Brand'].astype(str) + '_' + df['Size'].astype(str)\n",
    "    df_test['Brand_Size'] = df_test['Brand'].astype(str) + '_' + df_test['Size'].astype(str)\n",
    "    \n",
    "    # Size_Color: Moderate signal (0.17)\n",
    "    df['Size_Color'] = df['Size'].astype(str) + '_' + df['Color'].astype(str)\n",
    "    df_test['Size_Color'] = df_test['Size'].astype(str) + '_' + df_test['Color'].astype(str)\n",
    "    \n",
    "    # Size_Style: Moderate signal (0.12)\n",
    "    df['Size_Style'] = df['Size'].astype(str) + '_' + df['Style'].astype(str)\n",
    "    df_test['Size_Style'] = df_test['Size'].astype(str) + '_' + df_test['Style'].astype(str)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "train, test = create_interaction_features(train, test)\n",
    "\n",
    "# Add interaction features to cat_features list for encoding\n",
    "interaction_features = ['Brand_Size', 'Size_Color', 'Size_Style']\n",
    "all_cat_features = cat_features + interaction_features\n",
    "\n",
    "print(f\"Interaction features created: {interaction_features}\")\n",
    "print(f\"Total categorical features: {len(all_cat_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e56d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Target Encoding with Nested CV (High Priority)\n",
    "print(\"Implementing target encoding with nested CV...\")\n",
    "\n",
    "def target_encode(train_df, test_df, cat_cols, target_col, n_folds=5):\n",
    "    \"\"\"\n",
    "    Target encoding with nested CV to prevent leakage\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Global target mean for smoothing\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        print(f\"Target encoding {col}...\")\n",
    "        \n",
    "        # Initialize encoded columns\n",
    "        train_df[f'{col}_target_enc'] = 0\n",
    "        test_df[f'{col}_target_enc'] = 0\n",
    "        \n",
    "        # Outer CV for training data\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Store OOF predictions for training data\n",
    "        oof_encodings = np.zeros(len(train_df))\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "            X_tr, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n",
    "            \n",
    "            # Compute target statistics on training fold\n",
    "            target_stats = X_tr.groupby(col)[target_col].agg(['mean', 'count'])\n",
    "            \n",
    "            # Apply smoothing: (count * mean + alpha * global_mean) / (count + alpha)\n",
    "            alpha = 100  # Smoothing parameter\n",
    "            smoothed_mean = (target_stats['count'] * target_stats['mean'] + alpha * global_mean) / (target_stats['count'] + alpha)\n",
    "            \n",
    "            # Map to validation fold\n",
    "            oof_encodings[val_idx] = X_val[col].map(smoothed_mean).fillna(global_mean)\n",
    "            \n",
    "            # For test data, use all training data for encoding\n",
    "            if fold == 0:  # Only need to compute once for test\n",
    "                test_target_stats = train_df.groupby(col)[target_col].agg(['mean', 'count'])\n",
    "                test_smoothed = (test_target_stats['count'] * test_target_stats['mean'] + alpha * global_mean) / (test_target_stats['count'] + alpha)\n",
    "                test_df[f'{col}_target_enc'] = test_df[col].map(test_smoothed).fillna(global_mean)\n",
    "        \n",
    "        # Assign OOF encodings to training data\n",
    "        train_df[f'{col}_target_enc'] = oof_encodings\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Apply target encoding to top features: Color, Material, Brand (from analysis)\n",
    "target_encode_features = ['Color', 'Material', 'Brand']\n",
    "train, test = target_encode(train, test, target_encode_features, target_col)\n",
    "\n",
    "print(\"Target encoding completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d938b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Label encode all categorical features (including interactions)\n",
    "print(\"Label encoding all categorical features...\")\n",
    "\n",
    "le_dict = {}\n",
    "for col in all_cat_features:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train[col], test[col]], ignore_index=True)\n",
    "    le.fit(combined.astype(str))\n",
    "    \n",
    "    train[col] = le.transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "    \n",
    "    le_dict[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} classes\")\n",
    "\n",
    "print(\"Label encoding completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final feature matrix\n",
    "feature_cols = [col for col in train.columns if col not in ['id', target_col]]\n",
    "X = train[feature_cols]\n",
    "y = train[target_col]\n",
    "X_test = test[feature_cols]\n",
    "\n",
    "print(f\"Final training features shape: {X.shape}\")\n",
    "print(f\"Final test features shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Show feature types\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"  Weight capacity features: {len([c for c in feature_cols if 'weight' in c])}\")\n",
    "print(f\"  Count encoding features: {len([c for c in feature_cols if '_count' in c])}\")\n",
    "print(f\"  Target encoding features: {len([c for c in feature_cols if '_target_enc' in c])}\")\n",
    "print(f\"  Original categoricals: {len([c for c in feature_cols if c in all_cat_features])}\")\n",
    "print(f\"  Numerical features: {len([c for c in feature_cols if c in ['Compartments', 'Weight Capacity (kg)']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b07647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Hyperparameter tuning (Medium Priority)\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.05,  # Reduced from 0.1\n",
    "    'max_depth': 8,         # Increased from 6\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 2000,   # Increased from 1000\n",
    "    'early_stopping_rounds': 100,  # Increased from 50\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "print(\"Optimized XGBoost parameters:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20-fold CV training\n",
    "n_folds = 20\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "rmse_scores = []\n",
    "oof_predictions = np.zeros(len(train))\n",
    "test_predictions = np.zeros(len(test))\n",
    "\n",
    "print(f\"Starting {n_folds}-fold CV training with enhanced features...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Store OOF predictions\n",
    "    oof_predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Accumulate test predictions\n",
    "    test_predictions += test_pred / n_folds\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    print(f\"  Fold RMSE: {fold_rmse:.6f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Overall CV RMSE: {cv_rmse:.6f}\")\n",
    "print(f\"Mean Fold RMSE: {np.mean(rmse_scores):.6f} Â± {np.std(rmse_scores):.6f}\")\n",
    "print(f\"Improvement over baseline: {38.781061 - cv_rmse:.6f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Price': test_predictions\n",
    "})\n",
    "\n",
    "# Clip predictions\n",
    "submission['Price'] = submission['Price'].clip(lower=train[target_col].min(), upper=train[target_col].max())\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(submission[target_col].describe())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
