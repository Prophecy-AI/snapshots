{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf5b9ae",
   "metadata": {},
   "source": [
    "# Baseline Experiment - iNaturalist 2019\n",
    "\n",
    "This is a simple CNN baseline using ResNet50 pretrained on ImageNet.\n",
    "\n",
    "## Strategy\n",
    "- Use ResNet50 as feature extractor\n",
    "- Fine-tune on iNaturalist 2019 dataset\n",
    "- Use standard data augmentation\n",
    "- 5-fold cross-validation\n",
    "- Adam optimizer with learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "with open('/home/data/train2019.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "with open('/home/data/val2019.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "with open('/home/data/test2019.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create category mapping\n",
    "categories = {cat['id']: cat for cat in train_data['categories']}\n",
    "num_classes = len(categories)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create image id to file name mapping\n",
    "train_images = {img['id']: img for img in train_data['images']}\n",
    "val_images = {img['id']: img for img in val_data['images']}\n",
    "test_images = {img['id']: img for img in test_data['images']}\n",
    "\n",
    "# Create annotations mapping\n",
    "train_annots = {ann['image_id']: ann['category_id'] for ann in train_data['annotations']}\n",
    "val_annots = {ann['image_id']: ann['category_id'] for ann in val_data['annotations']}\n",
    "\n",
    "print(f\"Training images: {len(train_images)}\")\n",
    "print(f\"Validation images: {len(val_images)}\")\n",
    "print(f\"Test images: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53bd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare combined dataset for cross-validation\n",
    "# Combine train and val for better CV\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "# Add training data\n",
    "for img_id, img_info in train_images.items():\n",
    "    all_images.append(img_info)\n",
    "    all_labels.append(train_annots[img_id])\n",
    "\n",
    "# Add validation data  \n",
    "for img_id, img_info in val_images.items():\n",
    "    all_images.append(img_info)\n",
    "    all_labels.append(val_annots[img_id])\n",
    "\n",
    "print(f\"Total images for CV: {len(all_images)}\")\n",
    "print(f\"Label distribution: {Counter(all_labels).most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "class iNaturalistDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        image_path = f\"/home/data/{img_info['file_name']}\"\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "class iNaturalistModel(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = iNaturalistModel(num_classes=num_classes)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{running_loss/total:.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/total:.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return running_loss/len(val_loader), 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455936cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "# For this baseline, let's use just 1 fold to keep it fast\n",
    "fold = 0\n",
    "train_idx, val_idx = list(skf.split(all_images, all_labels))[0]\n",
    "\n",
    "print(f\"Fold {fold+1}\")\n",
    "print(f\"Train samples: {len(train_idx)}\")\n",
    "print(f\"Val samples: {len(val_idx)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = iNaturalistDataset(\n",
    "    [all_images[i] for i in train_idx],\n",
    "    [all_labels[i] for i in train_idx],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = iNaturalistDataset(\n",
    "    [all_images[i] for i in val_idx],\n",
    "    [all_labels[i] for i in val_idx],\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d532dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "num_epochs = 5  # Keep it short for baseline\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), '/home/code/experiments/001_baseline/best_model.pth')\n",
    "        print(f\"Saved best model with val acc: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    cv_scores.append(val_acc)\n",
    "\n",
    "print(f\"\\nCV Score: {np.mean(cv_scores):.2f}% Â± {np.std(cv_scores):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and predict on test set\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load('/home/code/experiments/001_baseline/best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        image_path = f\"/home/data/{img_info['file_name']}\"\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, img_info['id']\n",
    "\n",
    "test_dataset = TestDataset(test_data['images'], transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Predict\n",
    "print(\"Making predictions...\")\n",
    "test_predictions = []\n",
    "test_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, ids in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_ids.extend(ids)\n",
    "\n",
    "print(f\"Predictions made: {len(test_predictions)}\")\n",
    "print(f\"Sample predictions: {test_predictions[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5959912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'predicted': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure correct format (space-separated for multiple predictions)\n",
    "submission['predicted'] = submission['predicted'].astype(str)\n",
    "\n",
    "print(\"Submission format:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nShape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "\n",
    "# Also save to experiment folder for reference\n",
    "submission.to_csv('/home/code/experiments/001_baseline/submission.csv', index=False)\n",
    "\n",
    "# Calculate CV score for logging\n",
    "cv_score = 1 - (best_val_acc / 100)  # Convert accuracy to error rate\n",
    "print(f\"CV Error Rate: {cv_score:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
