{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae0577d",
   "metadata": {},
   "source": [
    "# Transformer Baseline with Context and Target Grouping\n",
    "\n",
    "Implementing transformer-based approach based on winning solutions:\n",
    "- Model: BERT-for-patents (domain-specific)\n",
    "- Input: anchor[SEP]target[SEP]context[SEP]targets\n",
    "- Validation: GroupKFold grouping by anchor\n",
    "- Loss: Pearson correlation\n",
    "- Features: Target grouping (the \"magic\" from winning solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef79650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTrain columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Score distribution:\\n{train_df['score'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target grouping - the \"magic\" feature from winning solutions\n",
    "# Group targets by anchor and context\n",
    "\n",
    "def create_target_groups(df, train_df_full):\n",
    "    \"\"\"Create grouped targets for each row\"\"\"\n",
    "    # Group by anchor and context\n",
    "    groups = train_df_full.groupby(['anchor', 'context'])['target'].apply(list).to_dict()\n",
    "    \n",
    "    def get_targets(row):\n",
    "        key = (row['anchor'], row['context'])\n",
    "        if key in groups:\n",
    "            targets = groups[key].copy()\n",
    "            # Remove current target from the list\n",
    "            if row['target'] in targets:\n",
    "                targets.remove(row['target'])\n",
    "            # Return comma-separated targets (up to 10 to avoid too long sequences)\n",
    "            return '; '.join(targets[:10])\n",
    "        return ''\n",
    "    \n",
    "    return df.apply(get_targets, axis=1)\n",
    "\n",
    "# Create target groups for training data\n",
    "print(\"Creating target groups...\")\n",
    "train_df['targets'] = create_target_groups(train_df, train_df)\n",
    "\n",
    "# For test data, we need to use both train and test to create groups\n",
    "# (as mentioned in 2nd place solution: \"During inference, the group is performed after concatenating train-set and test-set\")\n",
    "print(\"Creating target groups for test data...\")\n",
    "combined_df = pd.concat([train_df[['anchor', 'target', 'context']], test_df], ignore_index=True)\n",
    "test_df['targets'] = create_target_groups(test_df, combined_df)\n",
    "\n",
    "print(f\"\\nExample of target grouping:\")\n",
    "print(train_df[['anchor', 'target', 'context', 'targets']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45926e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CPC text descriptions for context\n",
    "# Create a simple mapping from context codes to text descriptions\n",
    "# In practice, you'd load this from the CPC documentation, but we'll create a simple version\n",
    "\n",
    "# Sample CPC descriptions (simplified - in real solution you'd use full CPC hierarchy)\n",
    "cpc_descriptions = {\n",
    "    'A01': 'Agriculture; forestry; animal husbandry; hunting; trapping; fishing',\n",
    "    'A61': 'Medical or veterinary science; hygiene',\n",
    "    'B01': 'Physical or chemical processes or apparatus in general',\n",
    "    'B02': 'Crushing, pulverising, or disintegrating; preparing grain for milling',\n",
    "    'B23': 'Machine tools; metal-working not otherwise provided for',\n",
    "    'B29': 'Working of plastics; working of substances in a plastic state',\n",
    "    'B60': 'Vehicles in general',\n",
    "    'B65': 'Conveying; packing; storing; handling thin or filamentary material',\n",
    "    'C01': 'Inorganic chemistry',\n",
    "    'C07': 'Organic chemistry',\n",
    "    'C08': 'Organic macromolecular compounds',\n",
    "    'C12': 'Biochemistry; beer; spirits; wine; vinegar; microbiology; enzymology',\n",
    "    'D01': 'Natural or artificial threads or fibres; spinning',\n",
    "    'D06': 'Treatment of textiles or the like; laundering; flexible materials',\n",
    "    'E02': 'Hydraulic engineering; foundations; soil-shifting',\n",
    "    'E04': 'Building',\n",
    "    'E05': 'Locks; keys; window or door fittings; safes',\n",
    "    'E21': 'Earth drilling; mining',\n",
    "    'F01': 'Machines or engines in general; engine plants in general',\n",
    "    'F16': 'Engineering elements or units; general measures for producing',\n",
    "    'F21': 'Lighting',\n",
    "    'F23': 'Combustion apparatus; combustion processes',\n",
    "    'F24': 'Heating; ranges; ventilating',\n",
    "    'G01': 'Measuring; testing',\n",
    "    'G02': 'Optics',\n",
    "    'G03': 'Photography; cinematography; electrography; holography',\n",
    "    'G04': 'Horology',\n",
    "    'G05': 'Controlling; regulating',\n",
    "    'G06': 'Computing; calculating; counting',\n",
    "    'G07': 'Checking-devices',\n",
    "    'G08': 'Signalling',\n",
    "    'G09': 'Educating; cryptography; display; advertising; seals',\n",
    "    'G10': 'Musical instruments; acoustics',\n",
    "    'G11': 'Information storage',\n",
    "    'G12': 'Instrument details',\n",
    "    'G16': 'Information and communication technology [ICT] specially adapted',\n",
    "    'G21': 'Nuclear physics; nuclear engineering',\n",
    "    'H01': 'Basic electric elements',\n",
    "    'H02': 'Generation, conversion, or distribution of electric power',\n",
    "    'H03': 'Basic electronic circuitry',\n",
    "    'H04': 'Electric communication technique',\n",
    "    'H05': 'Electric techniques not otherwise provided for'\n",
    "}\n",
    "\n",
    "def get_cpc_text(context):\n",
    "    \"\"\"Get CPC text description for context\"\"\"\n",
    "    # Extract first 3 characters (sector + class)\n",
    "    key = context[:3]\n",
    "    return cpc_descriptions.get(key, f'Patent classification {context}')\n",
    "\n",
    "# Apply to data\n",
    "train_df['context_text'] = train_df['context'].apply(get_cpc_text)\n",
    "test_df['context_text'] = test_df['context'].apply(get_cpc_text)\n",
    "\n",
    "print(f\"\\nContext text examples:\")\n",
    "print(train_df[['context', 'context_text']].drop_duplicates().head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953fbc15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T19:37:28.553238Z",
     "iopub.status.busy": "2026-01-15T19:37:28.552742Z",
     "iopub.status.idle": "2026-01-15T19:37:32.527795Z",
     "shell.execute_reply": "2026-01-15T19:37:32.527196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n",
      "Train shape: (36473, 5)\n",
      "Test shape: (36, 4)\n",
      "\n",
      "Train columns: ['id', 'anchor', 'target', 'context', 'score']\n",
      "Score distribution:\n",
      "score\n",
      "0.00     7471\n",
      "0.25    11519\n",
      "0.50    12300\n",
      "0.75     4029\n",
      "1.00     1154\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nTrain columns: {train_df.columns.tolist()}\")\n",
    "print(f\"Score distribution:\\n{train_df['score'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8dc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class\n",
    "class PatentDataset(Dataset):\n",
    "    def __init__(self, texts, scores, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.scores = scores\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        score = float(self.scores[idx]) if self.scores is not None else 0.0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'score': torch.tensor(score, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Define Model class\n",
    "class PatentModel(nn.Module):\n",
    "    def __init__(self, model_name, dropout=0.1):\n",
    "        super(PatentModel, self).__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.classifier(pooled_output)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "# Pearson correlation loss\n",
    "def pearson_corr_loss(pred, target):\n",
    "    \"\"\"Pearson correlation loss (maximize correlation)\"\"\"\n",
    "    pred_mean = pred.mean()\n",
    "    target_mean = target.mean()\n",
    "    \n",
    "    pred_centered = pred - pred_mean\n",
    "    target_centered = target - target_mean\n",
    "    \n",
    "    covariance = (pred_centered * target_centered).sum()\n",
    "    pred_std = torch.sqrt((pred_centered ** 2).sum())\n",
    "    target_std = torch.sqrt((target_centered ** 2).sum())\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero\n",
    "    correlation = covariance / (pred_std * target_std + 1e-8)\n",
    "    \n",
    "    # Return negative correlation (since we want to maximize it)\n",
    "    return -correlation\n",
    "\n",
    "print(\"Model components defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "MODEL_NAME = \"anferico/bert-for-patents\"  # Domain-specific model, smaller than DeBERTa-v3-large\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
    "\n",
    "# Check max token length\n",
    "max_tokens = max(tokenizer(text, return_length=True).length for text in train_df['input_text'].head(1000))\n",
    "print(f\"Max tokens in sample: {max_tokens}\")\n",
    "MAX_LENGTH = min(128, max_tokens + 10)  # Use 128 as in winning solutions\n",
    "print(f\"Using MAX_LENGTH: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62547a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GroupKFold validation\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "groups = train_df['anchor']  # Group by anchor to prevent leakage\n",
    "\n",
    "fold_scores = []\n",
    "all_predictions = np.zeros(len(train_df))\n",
    "\n",
    "print(\"Starting 5-fold GroupKFold validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_fold = train_df.iloc[train_idx]\n",
    "    val_fold = train_df.iloc[val_idx]\n",
    "    \n",
    "    print(f\"Train size: {len(train_fold)}, Val size: {len(val_fold)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PatentDataset(\n",
    "        train_fold['input_text'].values,\n",
    "        train_fold['score'].values,\n",
    "        tokenizer,\n",
    "        MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    val_dataset = PatentDataset(\n",
    "        val_fold['input_text'].values,\n",
    "        val_fold['score'].values,\n",
    "        tokenizer,\n",
    "        MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PatentModel(MODEL_NAME)\n",
    "    model = model.cuda() if torch.cuda.is_available() else model\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    num_epochs = 3  # Start with 3 epochs for baseline\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].cuda() if torch.cuda.is_available() else batch['input_ids']\n",
    "            attention_mask = batch['attention_mask'].cuda() if torch.cuda.is_available() else batch['attention_mask']\n",
    "            scores = batch['score'].cuda() if torch.cuda.is_available() else batch['score']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = pearson_corr_loss(outputs, scores)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].cuda() if torch.cuda.is_available() else batch['input_ids']\n",
    "                attention_mask = batch['attention_mask'].cuda() if torch.cuda.is_available() else batch['attention_mask']\n",
    "                scores = batch['score'].cuda() if torch.cuda.is_available() else batch['score']\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                \n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(scores.cpu().numpy())\n",
    "        \n",
    "        # Calculate Pearson correlation\n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_targets = np.array(val_targets)\n",
    "        \n",
    "        # Clip predictions to valid range\n",
    "        val_predictions = np.clip(val_predictions, 0, 1)\n",
    "        \n",
    "        correlation = np.corrcoef(val_targets, val_predictions)[0, 1]\n",
    "        fold_scores.append(correlation)\n",
    "        \n",
    "        print(f\"Val Pearson Correlation: {correlation:.4f}\")\n",
    "        \n",
    "        # Store predictions for this fold\n",
    "        all_predictions[val_idx] = val_predictions\n",
    "        \n",
    "        # Early stopping check\n",
    "        val_loss = -correlation  # Our loss is negative correlation\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CV Results: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1398a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall Pearson correlation\n",
    "overall_correlation = np.corrcoef(train_df['score'].values, all_predictions)[0, 1]\n",
    "print(f\"Overall Pearson Correlation: {overall_correlation:.4f}\")\n",
    "\n",
    "# Add predictions to dataframe for analysis\n",
    "train_df['predictions'] = all_predictions\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(pd.Series(all_predictions).describe())\n",
    "\n",
    "print(f\"\\nScore vs Prediction correlation by context:\")\n",
    "correlations_by_context = train_df.groupby('context').apply(\n",
    "    lambda x: np.corrcoef(x['score'], x['predictions'])[0, 1] if len(x) > 1 else np.nan\n",
    ")\n",
    "print(correlations_by_context.describe())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
