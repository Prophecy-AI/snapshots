{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11702049,"sourceType":"datasetVersion","datasetId":7345142}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPU Hill Climbing\nThis notebook demonstrates how to perform hill climbing. For each model and experiment we run, we save the OOF and Test PREDS files to disk. Then we load all our OOF and all our Test PREDS into this hill climbing notebook. This demonstration notebook only loads 3 models (from public notebooks). To improve this notebook's CV score and LB score. Load all of your 100s of models!\n\nHill climbing builds an ensemble for us. It will start with the strongest single model and then try adding additional models one by one. It will only include models that improve the ensemble's CV score. The algorithm also finds the best weights to add models together.\n\nThe secret to achieving the best performing ensemble is to build diverse models. Try building models that are different from each other. Then add all the diverse models here.\n\nThis notebook uses GPU to perform hill climbing to make it run faster than CPU. This is important because when we begin to have 100s or 1000s of models, then hill climbing algorithm needs to compute millions of metrics when searching for optimal weights to combine models. Discussion about this notebook is [here][1]\n\n[1]: https://www.kaggle.com/competitions/playground-series-s5e5/discussion/576111","metadata":{}},{"cell_type":"markdown","source":"# Load Train","metadata":{}},{"cell_type":"code","source":"VER=1\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 500)\n\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\ntrue = np.log1p( train.Calories.values )\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:34.49067Z","iopub.execute_input":"2025-05-06T14:41:34.491275Z","iopub.status.idle":"2025-05-06T14:41:37.536774Z","shell.execute_reply.started":"2025-05-06T14:41:34.491247Z","shell.execute_reply":"2025-05-06T14:41:37.536069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load OOF and Test PREDS\nWe load all our OOF and Test PREDs. The code below attempts to detect if your OOF have `log1p` applied or not. It will then apply `log1p` if it is not already applied. The code below assumes that submission.csv files do not have `log1p` applied.\n\nIn this demonstration notebook, we load 3 models\n* XGBoost from Yoyololo's public notebook [here][1]\n* CatBoost from Echo's public notebook [here][2]\n* NN MLP from my notebook [here][3]\n\nEach notebook was ran locally with 5 KFold random seed 42 and I saved OOF and Test PRED files to disk. Then i uploaded to Kaggle dataset [here][4]. \n\n[1]: https://www.kaggle.com/code/jiaoyouzhang/calorie-only-xgboost\n[2]: https://www.kaggle.com/code/nieniejava/onlycatboost-score0-05684\n[3]: https://www.kaggle.com/code/cdeotte/nn-mlp-starter-cv-0-0608\n[4]: https://www.kaggle.com/datasets/cdeotte/may2025-playground-oofs-testpreds","metadata":{}},{"cell_type":"code","source":"files = []\nx_train = []\nx_test = []\nPATH = \"/kaggle/input/may2025-playground-oofs-testpreds/\"\n\nprint(\"Loading files...\")\nfor c in ['xgb','cat','nn']:\n    print(f\"=> {c} \",end=\"\")\n    oof = np.load(f\"{PATH}oof_{c}.npy\")\n    # IF NOT LOG1P THEN APPLY LOG1P\n    if oof.mean()>10: oof = np.log1p(oof)\n    x_train.append(oof)\n    files.append(f\"oof_{c}\")\n    df = pd.read_csv(f\"{PATH}submission_{c}.csv\")\n    pred = np.log1p( df.Calories.values )\n    x_test.append(pred)\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:37.537534Z","iopub.execute_input":"2025-05-06T14:41:37.537814Z","iopub.status.idle":"2025-05-06T14:41:38.398718Z","shell.execute_reply.started":"2025-05-06T14:41:37.537792Z","shell.execute_reply":"2025-05-06T14:41:38.397929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train = np.stack(x_train).T\nprint(\"Our combined OOF have shape:\",x_train.shape)\n\nx_test = np.stack(x_test).T\nprint(\"Our combined PRED have shape:\",x_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:38.400606Z","iopub.execute_input":"2025-05-06T14:41:38.400922Z","iopub.status.idle":"2025-05-06T14:41:38.411109Z","shell.execute_reply.started":"2025-05-06T14:41:38.400904Z","shell.execute_reply":"2025-05-06T14:41:38.410279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Find Best Single Model\nHill climbing begins with our strongest model. So first we will find the strongest model by computing the CV score for each model. The competition metric is RMSLE. We have converted the ground truth with `log1p` and converted all OOF with `log1p`. Therefore below we will just compute the metric RMSE.","metadata":{}},{"cell_type":"code","source":"def compute_metric_rmse(p):\n    m = np.sqrt(np.mean( (p-true)**2.0 ) )\n    return m\n\n# COMPUTE METRIC FOR EACH OOF\nbest_score = 40\nbest_index = -1\n\nfor k,name in enumerate( files ):\n    s = compute_metric_rmse(x_train[:,k])\n    if s < best_score:\n        best_score = s\n        best_index = k\n    print(f'RMSE {s:0.5f} {name}') \nprint()\nprint(f'Best single model is {files[best_index]} with RMSE = {best_score:0.5f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:38.411808Z","iopub.execute_input":"2025-05-06T14:41:38.412084Z","iopub.status.idle":"2025-05-06T14:41:38.434201Z","shell.execute_reply.started":"2025-05-06T14:41:38.412061Z","shell.execute_reply":"2025-05-06T14:41:38.433526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPU Metric Computation\nTo perform hill climbing quickly, we will compute RMSE metric in batch form on GPU. This makes a big difference when we begin to have 100s or 1000s of models in our hill climbing search.","metadata":{}},{"cell_type":"code","source":"import cupy as cp, gc\n\ndef multiple_rmse_scores(actual, predicted):\n    \"\"\"\n    Computes multiple approximate AUC scores using GPU.\n    \n    This function calculates K approximate AUC scores simultaneously for a binary classification \n    problem. The implementation does not handle ties in predictions correctly, making it an \n    approximate AUC computation. The function is based on the algorithm outlined in:\n    https://github.com/benhamner/Metrics/blob/master/R/R/metrics.r\n\n    Parameters:\n    ----------\n    actual : cupy.ndarray\n        A 1D GPU array of shape (N,), where N is the number of samples. \n        Contains binary values (0 or 1) indicating the true labels.\n    \n    predicted : cupy.ndarray\n        A 2D GPU array of shape (N, K), where K is the number of classifiers.\n        Each column contains predicted scores for the corresponding classifier.\n\n    Returns:\n    -------\n    cupy.ndarray\n        A 1D GPU array of shape (K,) containing the AUC scores for each classifier.\n\n    \"\"\"\n    if len(actual.shape)==1: \n        actual = actual[:,cp.newaxis]\n    m = cp.sqrt(cp.mean(  (actual-predicted)**2.0,axis=0 ))\n    return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:38.434967Z","iopub.execute_input":"2025-05-06T14:41:38.43543Z","iopub.status.idle":"2025-05-06T14:41:40.577503Z","shell.execute_reply.started":"2025-05-06T14:41:38.435403Z","shell.execute_reply":"2025-05-06T14:41:40.576927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hill Climb\nWe will now try adding more models one by one. When a new model improves our ensemble CV score, we keep it. Otherwise, we do not add it.","metadata":{}},{"cell_type":"code","source":"USE_NEGATIVE_WGT = True\nMAX_MODELS = 1000\nTOL = 1e-5\n\nindices = [best_index]\nold_best_score = best_score\nprint(f'0 We begin with best single model RMSE {best_score:0.5f} from \"{files[best_index]}\"')\n\n# PREPARE/MOVE VARIABLES TO GPU FOR SPEED UP\nx_train2 = cp.array( x_train ) #GPU\nbest_ensemble = x_train2[:,best_index] # GPU\ntruth = cp.array( true ) # GPU\nstart = -0.50\nif not USE_NEGATIVE_WGT: start = 0.01\nww = cp.arange(start,0.51,0.01) # GPU\nnn = len(ww)\n\n# BEGIN HILL CLIMBING\nmodels = [best_index]\nweights = []\nmetrics = [best_score]\n\nfor kk in range(1_000_000):\n\n    best_score = 40\n    best_index = -1\n    best_weight = 0\n\n    # TRY ADDING ONE MORE MODEL\n    for k,ff in enumerate(files):\n        new_model = x_train2[:,k] # GPU\n        m1 = cp.repeat(best_ensemble[:, cp.newaxis], nn, axis=1) * (1-ww) # GPU\n        m2 = cp.repeat(new_model[:, cp.newaxis], nn, axis=1) * ww # GPU\n        mm = m1+m2 # GPU\n        new_aucs = multiple_rmse_scores(truth, mm)\n        new_score = cp.min(new_aucs).item() # GPU -> CPU\n        if new_score < best_score:\n            best_score = new_score # CPU\n            best_index = k # CPU\n            ii = np.argmin(new_aucs).item() # GPU -> CPU\n            best_weight = ww[ii].item() # GPU -> CPU\n            potential_ensemble = mm[:,ii] # GPU\n    del new_model, m1, m2, mm, new_aucs, new_score\n    gc.collect()\n\n    # STOPPING CRITERIA\n    indices.append(best_index)\n    indices = list(np.unique(indices))\n    if len(indices)>MAX_MODELS:\n        print(f'=> We reached {MAX_MODELS} models')\n        indices = indices[:-1]\n        break\n    if -1*(best_score - old_best_score) < TOL: \n        print(f'=> We reached tolerance {TOL}')\n        break\n\n    # RECORD NEW RESULT\n    print(kk+1,'New best RMSE',best_score,f'adding \"{files[best_index]}\"','with weight',f'{best_weight:0.3f}')\n    models.append(best_index)\n    weights.append(best_weight)\n    metrics.append(best_score)\n    best_ensemble = potential_ensemble\n    old_best_score = best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:40.578202Z","iopub.execute_input":"2025-05-06T14:41:40.578543Z","iopub.status.idle":"2025-05-06T14:41:43.433015Z","shell.execute_reply.started":"2025-05-06T14:41:40.578523Z","shell.execute_reply":"2025-05-06T14:41:43.432302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compute Weights\nThe dataframe below shows us what models were selected and what weights are assigned to each model.","metadata":{}},{"cell_type":"code","source":"wgt = np.array([1])\nfor w in weights:\n    wgt = wgt*(1-w)\n    wgt = np.concatenate([wgt,np.array([w])])\n    \nrows = []\nt = 0\nfor m,w,s in zip(models,wgt,metrics):\n    name = files[m]\n    dd = {}\n    dd['weight'] = w\n    dd['model'] = name\n    rows.append(dd)\n    t += float( f'{w:.3f}' )\n\n# DISPLAY WEIGHT PER MODEL\ndf = pd.DataFrame(rows)\ndf = df.groupby('model').agg('sum').reset_index().sort_values('weight',ascending=False)\ndf = df.reset_index(drop=True)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:43.433832Z","iopub.execute_input":"2025-05-06T14:41:43.434084Z","iopub.status.idle":"2025-05-06T14:41:43.458125Z","shell.execute_reply.started":"2025-05-06T14:41:43.434057Z","shell.execute_reply":"2025-05-06T14:41:43.457442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SANITY CHECK\nprint('Ensemble weights sum to',df.weight.sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:43.45887Z","iopub.execute_input":"2025-05-06T14:41:43.459186Z","iopub.status.idle":"2025-05-06T14:41:43.49318Z","shell.execute_reply.started":"2025-05-06T14:41:43.459159Z","shell.execute_reply":"2025-05-06T14:41:43.492449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compute Overall CV Score","metadata":{}},{"cell_type":"code","source":"# COMBINE OOF PREDITIONS (using weights from hill climbing)\nx_map = {x:y for x,y in zip(files,np.arange(len(files)))}\nx_train3 = x_train2.get()\nensemble = x_train3[:, x_map[df.model.iloc[0]] ] * df.weight.iloc[0]\nfor k in range(1,len(df)):\n    ensemble += x_train3[:, x_map[df.model.iloc[k]] ] * df.weight.iloc[k]\nm = compute_metric_rmse(ensemble)\nprint(f'Overall Hill climbing RMSE = {m:0.6f}')\n\nnp.save(f'oof_hill_climb_v{VER}',ensemble)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:43.495203Z","iopub.execute_input":"2025-05-06T14:41:43.49539Z","iopub.status.idle":"2025-05-06T14:41:43.5309Z","shell.execute_reply.started":"2025-05-06T14:41:43.495376Z","shell.execute_reply":"2025-05-06T14:41:43.530329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission CSV","metadata":{}},{"cell_type":"code","source":"# COMBINE TEST PREDITIONS (using weights from hill climbing)\nx_map = {x:y for x,y in zip(files,np.arange(len(files)))}\npred = x_test[:, x_map[df.model.iloc[0]] ] * df.weight.iloc[0]\nfor k in range(1,len(df)):\n    pred += x_test[:, x_map[df.model.iloc[k]] ] * df.weight.iloc[k]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:43.531486Z","iopub.execute_input":"2025-05-06T14:41:43.531667Z","iopub.status.idle":"2025-05-06T14:41:43.539852Z","shell.execute_reply.started":"2025-05-06T14:41:43.531652Z","shell.execute_reply":"2025-05-06T14:41:43.538823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WRITE SUB TO CSV\nsub = pd.read_csv(\"/kaggle/input/playground-series-s5e5/sample_submission.csv\")\n\n# CLIP TO TRAIN MIN AND MAX\nmn = train.Calories.min(); mx = train.Calories.max()\nsub.Calories = np.clip( np.expm1( pred ),mn,mx )\n\nprint(\"Test shape\", sub.shape )\nprint(\"Test target mean is\", sub.Calories.mean())\nsub.to_csv(f\"submission_hill_climb_v{VER}.csv\",index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:43.540483Z","iopub.execute_input":"2025-05-06T14:41:43.540739Z","iopub.status.idle":"2025-05-06T14:41:44.148405Z","shell.execute_reply.started":"2025-05-06T14:41:43.540718Z","shell.execute_reply":"2025-05-06T14:41:44.147719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA Test Preds","metadata":{}},{"cell_type":"code","source":"plt.hist(sub.Calories,bins=100)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:41:44.149154Z","iopub.execute_input":"2025-05-06T14:41:44.149466Z","iopub.status.idle":"2025-05-06T14:41:44.444038Z","shell.execute_reply.started":"2025-05-06T14:41:44.149439Z","shell.execute_reply":"2025-05-06T14:41:44.443299Z"}},"outputs":[],"execution_count":null}]}