## Current Status
- **Best CV score**: 0.02047 from exp_000 (baseline XGBoost)
- **Best LB score**: Not yet submitted
- **CV-LB gap**: Unknown (need submission for calibration)
- **Target score**: 0.058410 (need to INCREASE CV from 0.020 to ~0.058)

## Response to Evaluator

**Technical Verdict: TRUSTWORTHY**
- The evaluator correctly identified that target encoding + product features caused 933% performance degradation (0.020 â†’ 0.212)
- Implementation was technically sound (no leakage, proper CV), but strategically flawed
- **Key insight from analysis**: Product features (Weight_Duration, Duration_Heart_Rate) have 0.94 correlation with target and cause overfitting
- **Second insight**: Target encoding on 'Sex' (2 categories) is ineffective - adds minimal signal (correlation 0.123)

**Evaluator's Top Priority: Investigate WHY Target Encoding Failed**
- **AGREE COMPLETELY**: Analysis reveals product features are the main culprit, not target encoding
- Product features alone achieve CV 0.2275 (much worse than baseline 0.020)
- Manual target encoding on low-cardinality 'Sex' adds almost no value
- Winners encoded HIGH-cardinality binned features, not 'Sex'

**Key Concerns Addressed**:
1. **Performance degradation**: Caused by overfitting from product features (too predictive) + ineffective target encoding on 'Sex'
2. **No diagnostic analysis**: Now completed - see `exploration/evolver_loop4_analysis.ipynb`
3. **Hyperparameters not adapted**: Will tune regularization parameters for new features
4. **Missing residual modeling**: Will implement as top priority (key winner technique)
5. **No feature selection**: Will run ablation studies to identify helpful vs harmful features

## Data Understanding

**Reference Notebooks**:
- `exploration/evolver_loop4_analysis.ipynb`: Diagnostic analysis of target encoding failure
- `exploration/evolver_loop1_analysis.ipynb`: Winning solution analysis
- `experiments/001_baseline.ipynb`: XGBoost baseline (CV 0.02047)
- `experiments/003_target_encoding.ipynb`: Failed target encoding experiment (CV 0.21156)

**Key Findings from Analysis**:
1. **Product features are TOO predictive**: Weight_Duration (r=0.94), Duration_Heart_Rate (r=0.94)
2. **Target encoding on 'Sex' is ineffective**: Only 2 categories, correlation 0.123
3. **Winners' approach differences**:
   - Used sklearn's TargetEncoder with internal cross-fitting (not manual)
   - Applied target encoding to binned features (high cardinality)
   - Used residual modeling (NN on LR residuals, XGB on NN residuals)
   - Used groupby z-score features
   - GBDT models worked best with MINIMAL feature engineering (per AngelosMar)

## Recommended Approaches (Priority Order)

### Priority 1: Implement Residual Modeling (CRITICAL - Key Winner Technique)

**Why**: Both Chris Deotte and AngelosMar emphasized this as crucial. It captures complementary patterns that direct modeling misses.

**Implementation**:
1. **LinearRegression baseline**
   - Use original features + basic interactions
   - 5-fold CV, get OOF predictions
   - Expected CV: ~0.059-0.060

2. **Neural Network on LR residuals**
   - Target: `new_target = actual - LR_OOF_predictions`
   - Train NN on residuals (captures non-linear patterns)
   - Final prediction: `NN_PRED + LR_PRED`
   - Expected CV: ~0.059-0.060

3. **XGBoost on NN residuals**
   - Target: `new_target = actual - NN_OOF_predictions`
   - Train XGBoost on second-level residuals
   - Final prediction: `XGB_PRED + NN_PRED + LR_PRED`
   - Expected CV: ~0.058-0.059

**Expected outcome**: 3 diverse models capturing different patterns (linear, non-linear, tree-based)

### Priority 2: Fix Feature Engineering (Based on Diagnostic Analysis)

**2.1 Remove Product Features (Temporary)**
- Weight_Duration and Duration_Heart_Rate cause overfitting (r=0.94)
- Remove from next experiments to establish baseline without them
- Can re-add later with proper regularization

**2.2 Abandon Target Encoding on 'Sex'**
- Only 2 categories - adds minimal signal
- Winners encoded binned features instead
- Skip target encoding until we implement sklearn's TargetEncoder properly

**2.3 Add GroupBy Z-Score Features**
- Group by Sex + Age bins, compute z-scores for Height, Weight, etc.
- Creates relative positioning features
- Winners found these effective
- Example: For males in 30s, how does their weight compare to other males in 30s?

**2.4 Create Binned Features for CatBoost**
- Convert numerical features to 9 equal-width bins
- Helps CatBoost handle numerical features better
- Use these for target encoding later (high cardinality)

### Priority 3: Hyperparameter Tuning for Regularization

**Why**: Added 16 new features in exp_003 but kept same hyperparameters, causing overfitting.

**For XGBoost**:
- Increase `reg_alpha` (L1 regularization): try [0.1, 0.5, 1.0]
- Increase `reg_lambda` (L2 regularization): try [1.0, 1.5, 2.0]
- Reduce `max_depth`: try [4, 5] (was 6)
- Increase `min_child_samples`: try [5, 10] (was 1)

**For CatBoost**:
- Increase `l2_leaf_reg`: try [3, 5, 7] (was default)
- Reduce `depth`: try [4, 5] (was 6)
- Increase `min_data_in_leaf`: try [5, 10]

### Priority 4: Create Diverse Base Models

**4.1 LightGBM with GOSS**
- Use histogram-based approach
- Try Gradient-based One-Side Sampling for speed
- Different feature subset than XGBoost/CatBoost
- Expected CV: ~0.059-0.061

**4.2 Neural Network (Direct - Not Residual)**
- 2-3 hidden layers with dropout
- Use original features + log transforms
- Different architecture than residual NN
- Expected CV: ~0.060-0.062

**4.3 Linear Regression with Many Features**
- Create ~200 features (products, ratios, polynomials)
- Use Ridge regularization
- Expected CV: ~0.059-0.061

### Priority 5: Proper Target Encoding (After Fixing Other Issues)

**Only after implementing above**:
1. Use sklearn's TargetEncoder with internal cross-fitting (cv=5)
2. Apply to HIGH-cardinality features (binned versions of numerical features)
3. Test on small subset first to verify no degradation
4. Add smoothing parameter tuning

## What NOT to Try

1. **Don't use product features (Weight_Duration, Duration_Heart_Rate) yet**
   - Too predictive (r=0.94), causing overfitting
   - Establish baseline without them first
   - Can re-add later with stronger regularization

2. **Don't use manual target encoding on 'Sex'**
   - Only 2 categories, adds minimal signal
   - Use sklearn's TargetEncoder on binned features instead

3. **Don't add all features at once**
   - Run ablation studies to identify helpful vs harmful features
   - Add features systematically, one group at a time

4. **Don't chase LB feedback**
   - Winners explicitly ignored LB due to unstable correlation
   - Focus on CV improvement

5. **Don't skip residual modeling**
   - This was KEY to winning solutions
   - Implement before trying other complex approaches

## Validation Notes

- **CV Scheme**: 5-fold CV with seed 42 (consistent with winners)
- **Metric**: RMSLE (Root Mean Squared Logarithmic Error)
- **Clipping**: Clip predictions to [train_min, train_max] range
- **Trust CV**: Do not chase LB feedback - focus on CV improvement
- **Diagnostic tracking**: For each experiment, analyze:
  - Feature importance to see which features are used
  - Train vs validation performance per fold (overfitting check)
  - Correlation between engineered features and target
  - Ablation results (which feature groups help vs hurt)

## Expected Timeline

**Loop 5-6**: Implement residual modeling (3 models)
**Loop 7-8**: Add groupby features + hyperparameter tuning
**Loop 9-10**: Create additional diverse models (LGBM, direct NN, Linear)
**Loop 11-12**: Implement proper target encoding on binned features
**Loop 13+**: Ensemble with hill climbing

## Success Criteria

- Generate at least 7 diverse models with CV scores in 0.058-0.065 range
- Implement residual modeling pipeline (3 sequential models)
- Run ablation studies to identify optimal feature set
- Achieve final CV score < 0.058410 (target)
- Create robust ensemble that beats best single model by >0.001