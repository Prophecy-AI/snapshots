{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396d501a",
   "metadata": {},
   "source": [
    "# Loop 3 Analysis: Diagnosing Target Encoding Failure\n",
    "\n",
    "Investigate WHY target encoding + product features degraded performance from 0.020 to 0.211."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Loading data and OOF predictions...\")\n",
    "train_df = pd.read_csv('/home/code/data/train.csv')\n",
    "test_df = pd.read_csv('/home/code/data/test.csv')\n",
    "\n",
    "# Load OOF predictions\n",
    "oof_baseline = pd.read_csv('/home/submission/oof_predictions.csv')\n",
    "oof_target_enc = pd.read_csv('/home/code/experiments/oof_003_xgb_simple.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Baseline OOF shape: {oof_baseline.shape}\")\n",
    "print(f\"Target encoding OOF shape: {oof_target_enc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aca2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fold-wise scores for both models\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "baseline_scores = []\n",
    "target_enc_scores = []\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in kf.split(train_df):\n",
    "    y_val = train_df.iloc[val_idx]['Calories'].values\n",
    "    \n",
    "    # Baseline predictions - column is 'Calories' not 'oof_prediction'\n",
    "    pred_baseline = oof_baseline.iloc[val_idx]['Calories'].values\n",
    "    score_baseline = np.sqrt(mean_squared_log_error(y_val, np.clip(pred_baseline, 0, None)))\n",
    "    baseline_scores.append(score_baseline)\n",
    "    \n",
    "    # Target encoding predictions\n",
    "    pred_target_enc = oof_target_enc.iloc[val_idx]['oof_prediction'].values\n",
    "    score_target_enc = np.sqrt(mean_squared_log_error(y_val, np.clip(pred_target_enc, 0, None)))\n",
    "    target_enc_scores.append(score_target_enc)\n",
    "    \n",
    "    print(f\"Fold {fold}: Baseline={score_baseline:.6f}, TargetEnc={score_target_enc:.6f} (diff: {score_target_enc-score_baseline:+.6f})\")\n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nMean Baseline: {np.mean(baseline_scores):.6f} ± {np.std(baseline_scores):.6f}\")\n",
    "print(f\"Mean TargetEnc: {np.mean(target_enc_scores):.6f} ± {np.std(target_enc_scores):.6f}\")\n",
    "print(f\"\\nDegradation: {np.mean(target_enc_scores) - np.mean(baseline_scores):+.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5731d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction differences\n",
    "y_true = train_df['Calories'].values\n",
    "pred_baseline = oof_baseline['oof_prediction'].values\n",
    "pred_target_enc = oof_target_enc['oof_prediction'].values\n",
    "\n",
    "# Calculate absolute errors\n",
    "error_baseline = np.abs(pred_baseline - y_true)\n",
    "error_target_enc = np.abs(pred_target_enc - y_true)\n",
    "\n",
    "# Where does target encoding perform worse?\n",
    "worse_mask = error_target_enc > error_baseline\n",
    "better_mask = error_target_enc < error_baseline\n",
    "\n",
    "print(f\"Samples where target encoding is WORSE: {worse_mask.sum()} ({worse_mask.mean()*100:.1f}%)\")\n",
    "print(f\"Samples where target encoding is BETTER: {better_mask.sum()} ({better_mask.mean()*100:.1f}%)\")\n",
    "print(f\"Samples with equal error: {(error_target_enc == error_baseline).sum()}\")\n",
    "\n",
    "# Analyze by target value\n",
    "print(\"\\n=== Analysis by Target Value ===\")\n",
    "print(f\"Mean target (worse samples): {y_true[worse_mask].mean():.2f}\")\n",
    "print(f\"Mean target (better samples): {y_true[better_mask].mean():.2f}\")\n",
    "print(f\"Mean target (all samples): {y_true.mean():.2f}\")\n",
    "\n",
    "# Analyze by Sex\n",
    "print(\"\\n=== Analysis by Sex ===\")\n",
    "for sex in ['male', 'female']:\n",
    "    mask = train_df['Sex'] == sex\n",
    "    worse_pct = (worse_mask & mask).sum() / mask.sum() * 100\n",
    "    print(f\"{sex}: {worse_pct:.1f}% of samples are worse with target encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature importance if available (we need to re-run model with feature importance)\n",
    "print(\"Note: Need to re-run model with feature importance to analyze which features are being used.\")\n",
    "print(\"\\nHypotheses for performance degradation:\")\n",
    "print(\"1. Target encoding on 'Sex' is overfitting (only 2 categories)\")\n",
    "print(\"2. Product features are adding noise\")\n",
    "print(\"3. Too many features causing overfitting with current hyperparameters\")\n",
    "print(\"4. Smoothing parameter (p_smooth=20) is inappropriate\")\n",
    "print(\"5. Binned features + target encoding + products = too much complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation between engineered features and target\n",
    "print(\"Loading engineered features from the target encoding experiment...\")\n",
    "\n",
    "# We need to recreate the features to check correlations\n",
    "num_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "\n",
    "# Add features function from the experiment\n",
    "def add_features(df, num_cols):\n",
    "    df_new = df.copy()\n",
    "    # Log transforms\n",
    "    for col in num_cols:\n",
    "        df_new[f'{col}_log1p'] = np.log1p(df_new[col])\n",
    "    # Products\n",
    "    df_new['Weight_Duration'] = df_new['Weight'] * df_new['Duration']\n",
    "    df_new['Duration_Heart_Rate'] = df_new['Duration'] * df_new['Heart_Rate']\n",
    "    df_new['Height_Weight'] = df_new['Height'] * df_new['Weight']\n",
    "    # Ratios\n",
    "    df_new['Weight_Height'] = df_new['Weight'] / (df_new['Height'] + 1e-6)\n",
    "    return df_new\n",
    "\n",
    "# Add target encoding function\n",
    "def get_target_encoding(X_train, y_train, X_val, X_test, col='Sex', smoothing=20):\n",
    "    global_mean = y_train.mean()\n",
    "    stats = pd.DataFrame({\n",
    "        'target': y_train,\n",
    "        'category': X_train[col]\n",
    "    }).groupby('category')['target'].agg(['count', 'mean'])\n",
    "    \n",
    "    def encode(series):\n",
    "        result = []\n",
    "        for val in series:\n",
    "            if val in stats.index:\n",
    "                count = stats.loc[val, 'count']\n",
    "                mean = stats.loc[val, 'mean']\n",
    "                smoothed = (count * mean + smoothing * global_mean) / (count + smoothing)\n",
    "                result.append(smoothed)\n",
    "            else:\n",
    "                result.append(global_mean)\n",
    "        return np.array(result)\n",
    "    \n",
    "    X_train_enc = X_train.copy()\n",
    "    X_val_enc = X_val.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    X_train_enc[f'{col}_target_enc'] = encode(X_train[col])\n",
    "    X_val_enc[f'{col}_target_enc'] = encode(X_val[col])\n",
    "    X_test_enc[f'{col}_target_enc'] = encode(X_test[col])\n",
    "    \n",
    "    return X_train_enc, X_val_enc, X_test_enc\n",
    "\n",
    "# Create features on full training data\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_idx, val_idx = list(kf.split(train_df))[0]  # Use first fold\n",
    "\n",
    "X_tr, X_va = train_df.iloc[train_idx].copy(), train_df.iloc[val_idx].copy()\n",
    "y_tr, y_va = X_tr['Calories'].values, X_va['Calories'].values\n",
    "\n",
    "X_tr = X_tr.drop('Calories', axis=1)\n",
    "X_va = X_va.drop('Calories', axis=1)\n",
    "\n",
    "# Add features\n",
    "X_tr = add_features(X_tr, num_features)\n",
    "X_va = add_features(X_va, num_features)\n",
    "\n",
    "# Target encoding\n",
    "X_tr_enc, X_va_enc, _ = get_target_encoding(X_tr, y_tr, X_va, test_df.copy())\n",
    "\n",
    "# Check correlations\n",
    "print(\"\\n=== Correlation Analysis ===\")\n",
    "feature_cols = [c for c in X_tr_enc.columns if c not in ['id', 'Sex']]\n",
    "correlations = {}\n",
    "for col in feature_cols:\n",
    "    corr = np.corrcoef(X_tr_enc[col], y_tr)[0, 1]\n",
    "    correlations[col] = corr\n",
    "\n",
    "# Sort by absolute correlation\n",
    "sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"Top correlations with target:\")\n",
    "for col, corr in sorted_corr[:15]:\n",
    "    print(f\"  {col}: {corr:.4f}\")\n",
    "\n",
    "print(\"\\nTarget encoding correlation:\")\n",
    "if 'Sex_target_enc' in correlations:\n",
    "    print(f\"  Sex_target_enc: {correlations['Sex_target_enc']:.4f}\")\n",
    "\n",
    "# Check if target encoding is highly correlated with other features\n",
    "print(\"\\n=== Feature Redundancy Check ===\")\n",
    "if 'Sex_target_enc' in X_tr_enc.columns:\n",
    "    sex_enc_corr = {}\n",
    "    for col in feature_cols:\n",
    "        if col != 'Sex_target_enc':\n",
    "            corr = np.corrcoef(X_tr_enc['Sex_target_enc'], X_tr_enc[col])[0, 1]\n",
    "            if abs(corr) > 0.5:\n",
    "                sex_enc_corr[col] = corr\n",
    "    \n",
    "    if sex_enc_corr:\n",
    "        print(\"Features highly correlated with Sex_target_enc:\")\n",
    "        for col, corr in sex_enc_corr.items():\n",
    "            print(f\"  {col}: {corr:.4f}\")\n",
    "    else:\n",
    "        print(\"No features highly correlated with Sex_target_enc (|corr| > 0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. PERFORMANCE DEGRADATION:\")\n",
    "print(f\"   Baseline XGBoost: 0.02047\")\n",
    "print(f\"   With target encoding + products: 0.21156\")\n",
    "print(f\"   Degradation: +0.19109 (+933% worse)\")\n",
    "\n",
    "print(\"\\n2. PREDICTION ANALYSIS:\")\n",
    "print(f\"   Target encoding is WORSE on {worse_mask.sum()} samples ({worse_mask.mean()*100:.1f}%)\")\n",
    "print(f\"   Target encoding is BETTER on {better_mask.sum()} samples ({better_mask.mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n3. KEY HYPOTHESES:\")\n",
    "print(\"   a) 'Sex' has only 2 categories - target encoding may overfit\")\n",
    "print(\"   b) Product features may add noise rather than signal\")\n",
    "print(\"   c) Too many features (23) with same hyperparameters causes overfitting\")\n",
    "print(\"   d) Smoothing=20 may be inappropriate for this dataset\")\n",
    "print(\"   e) Manual target encoding may have implementation issues\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDED NEXT STEPS:\")\n",
    "print(\"   - Run ablation study: test target encoding only, products only, both\")\n",
    "print(\"   - Try sklearn's TargetEncoder with internal cross-fitting\")\n",
    "print(\"   - Test different smoothing parameters (5, 10, 50, 100)\")\n",
    "print(\"   - Increase regularization (reg_alpha, reg_lambda, reduce depth)\")\n",
    "print(\"   - Remove binned features (may be redundant with target encoding)\")\n",
    "print(\"   - Check for data leakage in target encoding implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de594916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save findings\n",
    "findings = {\n",
    "    'baseline_score': np.mean(baseline_scores),\n",
    "    'target_enc_score': np.mean(target_enc_scores),\n",
    "    'degradation': np.mean(target_enc_scores) - np.mean(baseline_scores),\n",
    "    'worse_samples_pct': worse_mask.mean() * 100,\n",
    "    'better_samples_pct': better_mask.mean() * 100,\n",
    "    'hypotheses': [\n",
    "        'Sex has only 2 categories - target encoding may overfit',\n",
    "        'Product features may add noise rather than signal',\n",
    "        'Too many features (23) with same hyperparameters causes overfitting',\n",
    "        'Smoothing=20 may be inappropriate',\n",
    "        'Manual target encoding implementation may have issues'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Findings saved to analysis notebook.\")\n",
    "print(\"\\nNext: Run ablation studies and test alternative implementations.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
