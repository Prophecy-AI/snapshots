{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeef5508",
   "metadata": {},
   "source": [
    "# Evolver Loop 7 Analysis: Residual Pipeline Failure & Pivot Strategy\n",
    "\n",
    "## Summary of Current State\n",
    "\n",
    "**Best CV Score**: 0.020470 (exp_000, baseline XGBoost)\n",
    "**Target Score**: 0.058410\n",
    "**Submissions Made**: 0/5\n",
    "**Experiments Completed**: 7\n",
    "\n",
    "## The Residual Pipeline Catastrophe\n",
    "\n",
    "The three-stage residual pipeline (Linear → NN → XGBoost) has **FAILED**:\n",
    "- Linear only: 0.208762\n",
    "- Linear + NN: 0.201961 (improvement: -0.006801)\n",
    "- Linear + NN + XGBoost: 0.208070 (degradation: +0.006109 from NN stage)\n",
    "- **Pipeline vs Baseline**: +0.187600 WORSE\n",
    "\n",
    "### Why It Failed\n",
    "\n",
    "1. **Insufficient Linear Foundation**: Used default Ridge(alpha=1.0) without tuning\n",
    "2. **Feature Poverty**: Only 8 basic features throughout pipeline\n",
    "3. **Residuals Contain Only Noise**: After Linear+NN, no systematic signal remains\n",
    "4. **Wrong Approach Context**: Winners used residual modeling as 10-20% of solution, not 100%\n",
    "\n",
    "## What Winners Actually Did\n",
    "\n",
    "From winning solution analysis:\n",
    "- **7-12 diverse models** (XGBoost, CatBoost, LGBM, NN, Linear, KNN, etc.)\n",
    "- **200-400 engineered features** (not 8!)\n",
    "- **Target encoding** with proper cross-validation\n",
    "- **Hill climbing ensemble** (not sequential pipelines)\n",
    "- **Residual modeling as ONE component** among many\n",
    "\n",
    "## The Path Forward\n",
    "\n",
    "### Immediate Actions\n",
    "\n",
    "1. **SUBMIT baseline XGBoost** to get LB feedback (calibrate CV-LB gap)\n",
    "2. **ABANDON sequential pipeline** - return to direct modeling\n",
    "3. **Implement comprehensive feature engineering** (50-100 features)\n",
    "4. **Build diverse base models** for ensemble\n",
    "\n",
    "### Key Techniques to Implement\n",
    "\n",
    "1. **Target Encoding** (Critical!)\n",
    "   - Sex encoding with smoothing\n",
    "   - Cross-validation to prevent leakage\n",
    "   - This was a KEY winner technique\n",
    "\n",
    "2. **Advanced Feature Engineering**\n",
    "   - Log1p transforms\n",
    "   - Product features (all pairs)\n",
    "   - Ratio features\n",
    "   - Binned features\n",
    "   - Groupby z-scores\n",
    "\n",
    "3. **Model Diversity**\n",
    "   - XGBoost with different feature sets\n",
    "   - CatBoost with binned features\n",
    "   - LightGBM\n",
    "   - Neural Network\n",
    "   - Linear with polynomial features\n",
    "\n",
    "4. **Ensemble Methods**\n",
    "   - Hill climbing (primary)\n",
    "   - Ridge regression (secondary)\n",
    "\n",
    "## Analysis: Why Our CV is Too Good\n",
    "\n",
    "Our baseline XGBoost achieves 0.020470, but winners needed 0.058-0.059. This suggests:\n",
    "1. Synthetic data is too simple/easy\n",
    "2. OR we're overfitting with product features\n",
    "3. OR missing key complexity of real data\n",
    "\n",
    "The residual pipeline failure suggests #2 or #3 - we need more sophisticated modeling, not just stacking weak models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import json\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "experiments = session_state['experiments']\n",
    "\n",
    "# Create summary DataFrame\n",
    "exp_summary = pd.DataFrame([\n",
    "    {\n",
    "        'exp_id': exp['id'],\n",
    "        'model_type': exp['model_type'],\n",
    "        'cv_score': exp['score'],\n",
    "        'notes': exp['notes'][:200] + '...' if len(exp['notes']) > 200 else exp['notes']\n",
    "    }\n",
    "    for exp in experiments\n",
    "])\n",
    "\n",
    "print(\"Experiment Summary:\")\n",
    "print(exp_summary[['exp_id', 'model_type', 'cv_score']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"1. Best CV: {exp_summary['cv_score'].min():.6f} (exp_000)\")\n",
    "print(f\"2. Worst CV: {exp_summary['cv_score'].max():.6f}\")\n",
    "print(f\"3. Residual pipeline degraded: {exp_summary[exp_summary['exp_id']=='exp_006']['cv_score'].iloc[0]:.6f} → {exp_summary[exp_summary['exp_id']=='exp_007']['cv_score'].iloc[0]:.6f}\")\n",
    "print(f\"4. Target: 0.058410 (we're TOO GOOD - need to understand why)\")\n",
    "print(f\"5. Submissions: 0/5 made (need LB feedback)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976c854",
   "metadata": {},
   "source": [
    "## Feature Engineering Analysis\n",
    "\n",
    "### Current Feature Counts:\n",
    "- exp_000 (baseline): ~15 features (products, ratios, logs)\n",
    "- exp_001-004: 18-23 features (added target encoding, binned)\n",
    "- exp_005-007: 8 features (minimal for residual pipeline)\n",
    "\n",
    "### What Winners Used:\n",
    "- 200-400 engineered features\n",
    "- All pairwise products\n",
    "- Groupby statistics\n",
    "- Target encoding\n",
    "- Polynomial features\n",
    "\n",
    "### Gap Analysis:\n",
    "We're using 8-23 features while winners used 200-400. This is a **10-20x difference**!\n",
    "\n",
    "## Next Steps Priority\n",
    "\n",
    "1. **Submit exp_000** to get LB feedback\n",
    "2. **Implement proper target encoding** (cross-validated)\n",
    "3. **Create 50-100 features** for diverse models\n",
    "4. **Train 5-7 diverse models** with different:\n",
    "   - Algorithms (XGB, CatBoost, LGBM, NN, Linear)\n",
    "   - Feature sets\n",
    "   - Hyperparameters\n",
    "5. **Implement hill climbing ensemble**\n",
    "6. **Iterate based on CV-LB gap analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze residual pipeline failure in detail\n",
    "print(\"RESIDUAL PIPELINE FAILURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load residuals data\n",
    "residuals_lr = pd.read_csv('/home/code/experiments/005_linear_regression/residuals_lr.csv')\n",
    "residuals_nn = pd.read_csv('/home/code/experiments/006_neural_network_residuals/residuals_after_nn.csv')\n",
    "\n",
    "print(f\"Original target std: {residuals_lr['actual'].std():.2f}\")\n",
    "print(f\"After Linear: residual std: {residuals_lr['residual'].std():.2f}\")\n",
    "print(f\"Variance explained by Linear: {(1 - residuals_lr['residual'].std()/residuals_lr['actual'].std())*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nAfter Linear+NN: residual std: {residuals_nn['residual'].std():.2f}\")\n",
    "print(f\"Variance explained by Linear+NN: {(1 - residuals_nn['residual'].std()/residuals_lr['actual'].std())*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nResidual after Linear+NN: mean={residuals_nn['residual'].mean():.3f}, std={residuals_nn['residual'].std():.2f}\")\n",
    "print(f\"This residual is mostly noise - XGBoost overfits to it!\")\n",
    "\n",
    "# Check if residuals are normally distributed (should be for good modeling)\n",
    "from scipy import stats\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals_nn['residual'].sample(5000) if len(residuals_nn) > 5000 else residuals_nn['residual'])\n",
    "print(f\"\\nShapiro-Wilk test for normality: p={shapiro_p:.6f}\")\n",
    "print(f\"Residuals {'are' if shapiro_p > 0.05 else 'are NOT'} normally distributed\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
