{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f927cd0",
   "metadata": {},
   "source": [
    "# Loop 4 Analysis: Diagnosing Target Encoding Failure\n",
    "\n",
    "## Objective\n",
    "Investigate WHY target encoding + product features caused 933% performance degradation (CV: 0.020 → 0.212)\n",
    "\n",
    "## Key Questions\n",
    "1. Are product features leaking target information?\n",
    "2. Is manual target encoding implementation flawed?\n",
    "3. Which specific features are causing the problem?\n",
    "4. How do winners implement these features successfully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb7a0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:47:07.601638Z",
     "iopub.status.busy": "2026-01-15T16:47:07.601391Z",
     "iopub.status.idle": "2026-01-15T16:47:08.782510Z",
     "shell.execute_reply": "2026-01-15T16:47:08.782038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8000, 9)\n",
      "Test shape: (2000, 9)\n",
      "\n",
      "Target statistics:\n",
      "count    8000.000000\n",
      "mean      143.772778\n",
      "std        76.566039\n",
      "min        10.000000\n",
      "25%        91.227982\n",
      "50%       121.244149\n",
      "75%       174.789980\n",
      "max       500.000000\n",
      "Name: Calories, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/code/data/train.csv')\n",
    "test_df = pd.read_csv('/home/code/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(train_df['Calories'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4c266",
   "metadata": {},
   "source": [
    "## 1. Analyze Product Features Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38884370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:47:08.783791Z",
     "iopub.status.busy": "2026-01-15T16:47:08.783620Z",
     "iopub.status.idle": "2026-01-15T16:47:08.826636Z",
     "shell.execute_reply": "2026-01-15T16:47:08.826297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight_Duration: correlation with target = 0.9396\n",
      "Duration_Heart_Rate: correlation with target = 0.9364\n",
      "Height_Weight: correlation with target = 0.1042\n",
      "\n",
      "=== Checking for target leakage ===\n",
      "Sample of Weight_Duration vs Calories:\n",
      "   Weight_Duration    Calories\n",
      "0      1626.889988  120.317974\n",
      "1      2665.097527  153.337764\n",
      "2       500.000000   77.173770\n",
      "3      2176.925537  181.706220\n",
      "4      1901.755408   98.043957\n",
      "\n",
      "CV RMSLE using ONLY product features: 0.2275 ± 0.0048\n",
      "If this is very low, product features are leaking target information!\n"
     ]
    }
   ],
   "source": [
    "# Create product features as implemented in exp_003\n",
    "train_analysis = train_df.copy()\n",
    "\n",
    "# Product features\n",
    "train_analysis['Weight_Duration'] = train_analysis['Weight'] * train_analysis['Duration']\n",
    "train_analysis['Duration_Heart_Rate'] = train_analysis['Duration'] * train_analysis['Heart_Rate']\n",
    "train_analysis['Height_Weight'] = train_analysis['Height'] * train_analysis['Weight']\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = {}\n",
    "for col in ['Weight_Duration', 'Duration_Heart_Rate', 'Height_Weight']:\n",
    "    corr = train_analysis[col].corr(train_analysis['Calories'])\n",
    "    correlations[col] = corr\n",
    "    print(f\"{col}: correlation with target = {corr:.4f}\")\n",
    "\n",
    "# Check if these are essentially the target in disguise\n",
    "print(\"\\n=== Checking for target leakage ===\")\n",
    "print(\"Sample of Weight_Duration vs Calories:\")\n",
    "print(pd.DataFrame({\n",
    "    'Weight_Duration': train_analysis['Weight_Duration'].head(),\n",
    "    'Calories': train_analysis['Calories'].head()\n",
    "}))\n",
    "\n",
    "# Try to predict Calories from product features alone (this would indicate leakage)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_products = train_analysis[['Weight_Duration', 'Duration_Heart_Rate', 'Height_Weight']]\n",
    "y = train_analysis['Calories']\n",
    "\n",
    "# CV score using ONLY product features\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "product_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_products):\n",
    "    X_train, X_val = X_products.iloc[train_idx], X_products.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Fit linear regression on training folds\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation\n",
    "    pred_val = lr.predict(X_val)\n",
    "    pred_val = np.clip(pred_val, 0, None)  # Clip negative values\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    score = np.sqrt(mean_squared_log_error(y_val, pred_val))\n",
    "    product_scores.append(score)\n",
    "\n",
    "print(f\"\\nCV RMSLE using ONLY product features: {np.mean(product_scores):.4f} ± {np.std(product_scores):.4f}\")\n",
    "print(\"If this is very low, product features are leaking target information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9f29d",
   "metadata": {},
   "source": [
    "## 2. Analyze Target Encoding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b9243c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:49:30.837258Z",
     "iopub.status.busy": "2026-01-15T16:49:30.837091Z",
     "iopub.status.idle": "2026-01-15T16:49:30.875845Z",
     "shell.execute_reply": "2026-01-15T16:49:30.875438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual target encoding mapping:\n",
      "{'F': 132.1348050022123, 'M': 151.3127728164979}\n",
      "\n",
      "Global mean: 143.7728\n",
      "\n",
      "Correlation between encoded Sex and target: 0.1230\n",
      "\n",
      "Number of unique values in Sex: 2\n",
      "Value counts:\n",
      "Sex\n",
      "M    4859\n",
      "F    3141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Encoding Analysis ===\n",
      "Target statistics by Sex:\n",
      "     Calories      \n",
      "         mean count\n",
      "Sex                \n",
      "F    132.0607  3141\n",
      "M    151.3438  4859\n",
      "\n",
      "Encoded values (manual implementation):\n",
      "F: encoded=132.1348, actual_mean=132.0607\n",
      "M: encoded=151.3128, actual_mean=151.3438\n",
      "\n",
      "=== Smoothing Analysis ===\n",
      "p_smooth=  5: correlation = 0.1230\n",
      "p_smooth= 10: correlation = 0.1230\n",
      "p_smooth= 20: correlation = 0.1230\n",
      "p_smooth= 50: correlation = 0.1230\n",
      "p_smooth=100: correlation = 0.1230\n"
     ]
    }
   ],
   "source": [
    "# Analyze the manual target encoding implementation from exp_003\n",
    "\n",
    "def manual_target_encode(train_df, test_df, column, target_col='Calories', p_smooth=20):\n",
    "    \"\"\"Manual target encoding as implemented in exp_003\"\"\"\n",
    "    \n",
    "    # Calculate global mean and count\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    \n",
    "    # Calculate category means and counts\n",
    "    category_stats = train_df.groupby(column)[target_col].agg(['mean', 'count'])\n",
    "    \n",
    "    # Apply smoothing\n",
    "    smooth_mean = (category_stats['mean'] * category_stats['count'] + global_mean * p_smooth) / (category_stats['count'] + p_smooth)\n",
    "    \n",
    "    # Create mapping\n",
    "    mapping = smooth_mean.to_dict()\n",
    "    \n",
    "    # Apply to train and test\n",
    "    train_encoded = train_df[column].map(mapping).fillna(global_mean)\n",
    "    test_encoded = test_df[column].map(mapping).fillna(global_mean)\n",
    "    \n",
    "    return train_encoded, test_encoded, mapping\n",
    "\n",
    "# Apply manual encoding\n",
    "train_enc, test_enc, mapping = manual_target_encode(train_df, test_df, 'Sex')\n",
    "\n",
    "print(\"Manual target encoding mapping:\")\n",
    "print(mapping)\n",
    "print(f\"\\nGlobal mean: {train_df['Calories'].mean():.4f}\")\n",
    "\n",
    "# Check correlation\n",
    "enc_corr = train_enc.corr(train_df['Calories'])\n",
    "print(f\"\\nCorrelation between encoded Sex and target: {enc_corr:.4f}\")\n",
    "\n",
    "# The problem: With only 2 categories, this encoding is too coarse\n",
    "print(f\"\\nNumber of unique values in Sex: {train_df['Sex'].nunique()}\")\n",
    "print(\"Value counts:\")\n",
    "print(train_df['Sex'].value_counts())\n",
    "\n",
    "# Check if encoding is just memorizing the training data\n",
    "print(f\"\\n=== Encoding Analysis ===\")\n",
    "print(\"Target statistics by Sex:\")\n",
    "sex_stats = train_df.groupby('Sex').agg({\n",
    "    'Calories': ['mean', 'count']\n",
    "}).round(4)\n",
    "print(sex_stats)\n",
    "\n",
    "print(f\"\\nEncoded values (manual implementation):\")\n",
    "for sex, encoded_val in mapping.items():\n",
    "    actual_mean = train_df[train_df['Sex'] == sex]['Calories'].mean()\n",
    "    print(f\"{sex}: encoded={encoded_val:.4f}, actual_mean={actual_mean:.4f}\")\n",
    "\n",
    "# The issue: With only 2 categories, target encoding adds almost no information\n",
    "# and may cause overfitting if smoothing is not appropriate\n",
    "\n",
    "print(f\"\\n=== Smoothing Analysis ===\")\n",
    "for p in [5, 10, 20, 50, 100]:\n",
    "    train_enc_p, _, _ = manual_target_encode(train_df, test_df, 'Sex', p_smooth=p)\n",
    "    corr_p = train_enc_p.corr(train_df['Calories'])\n",
    "    print(f\"p_smooth={p:3d}: correlation = {corr_p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e851d",
   "metadata": {},
   "source": [
    "## 3. Compare Baseline vs Target Encoding Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85526e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:49:31.461303Z",
     "iopub.status.busy": "2026-01-15T16:49:31.461038Z",
     "iopub.status.idle": "2026-01-15T16:49:31.464967Z",
     "shell.execute_reply": "2026-01-15T16:49:31.464669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF predictions not found. Need to run experiments first.\n"
     ]
    }
   ],
   "source": [
    "# Load OOF predictions from experiments\n",
    "import os\n",
    "\n",
    "# Try to load OOF predictions from exp_003 (target encoding)\n",
    "oof_path_003 = '/home/code/experiments/003_target_encoding/oof_predictions.npy'\n",
    "oof_path_000 = '/home/code/experiments/001_baseline/oof_predictions.npy'\n",
    "\n",
    "if os.path.exists(oof_path_003) and os.path.exists(oof_path_000):\n",
    "    oof_003 = np.load(oof_path_003)\n",
    "    oof_000 = np.load(oof_path_000)\n",
    "    \n",
    "    print(\"=== Comparing Predictions ===\")\n",
    "    print(f\"Baseline OOF shape: {oof_000.shape}\")\n",
    "    print(f\"Target encoding OOF shape: {oof_003.shape}\")\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals_000 = train_df['Calories'] - oof_000\n",
    "    residuals_003 = train_df['Calories'] - oof_003\n",
    "    \n",
    "    print(f\"\\nBaseline residuals - mean: {residuals_000.mean():.4f}, std: {residuals_000.std():.4f}\")\n",
    "    print(f\"Target encoding residuals - mean: {residuals_003.mean():.4f}, std: {residuals_003.std():.4f}\")\n",
    "    \n",
    "    # Check where predictions differ most\n",
    "    pred_diff = np.abs(oof_003 - oof_000)\n",
    "    print(f\"\\nMean absolute prediction difference: {pred_diff.mean():.4f}\")\n",
    "    print(f\"Max absolute prediction difference: {pred_diff.max():.4f}\")\n",
    "    \n",
    "    # Check if target encoding model is overfitting\n",
    "    print(f\"\\n=== Overfitting Check ===\")\n",
    "    print(\"If target encoding model has much lower training error but higher CV error,\")\n",
    "    print(\"it's overfitting to the encoded features.\")\n",
    "    \n",
    "else:\n",
    "    print(\"OOF predictions not found. Need to run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f16983",
   "metadata": {},
   "source": [
    "## 4. Investigate Winners' Implementation Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccdd435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:49:54.942723Z",
     "iopub.status.busy": "2026-01-15T16:49:54.942556Z",
     "iopub.status.idle": "2026-01-15T16:49:54.947243Z",
     "shell.execute_reply": "2026-01-15T16:49:54.946888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Winners' Approach vs Our Implementation ===\n",
      "\n",
      "WINNERS (Chris Deotte, AngelosMar):\n",
      "1. Used sklearn's TargetEncoder with INTERNAL cross-fitting\n",
      "2. Applied target encoding to HIGH-cardinality features (not just Sex)\n",
      "3. Used product features BUT with careful regularization\n",
      "4. Used residual modeling (sequential approach)\n",
      "5. Used groupby z-score features\n",
      "6. Used MANY diverse models (7-12) with different feature sets\n",
      "\n",
      "OUR IMPLEMENTATION (exp_003):\n",
      "1. Manual target encoding with simple smoothing\n",
      "2. Only applied to Sex (2 categories - too low cardinality)\n",
      "3. Added product features without additional regularization\n",
      "4. No residual modeling\n",
      "5. No groupby features\n",
      "6. Only 2 models so far\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "\n",
      "PROBLEM 1: Sex has only 2 categories - target encoding adds minimal signal\n",
      "- With only 'male' and 'female', encoding just creates 2 values\n",
      "- This is essentially just a binary feature, not true target encoding\n",
      "- Winners encoded binned features (higher cardinality)\n",
      "\n",
      "PROBLEM 2: Product features may be TOO predictive (leaking target)\n",
      "- Weight_Duration correlation with Calories: VERY HIGH\n",
      "- If product features alone can predict target well, they're too strong\n",
      "- Need to verify if these are legitimate or data leakage\n",
      "\n",
      "PROBLEM 3: Manual encoding vs sklearn's TargetEncoder\n",
      "- sklearn's version uses internal K-fold cross-fitting\n",
      "- This prevents overfitting better than simple smoothing\n",
      "- Our manual implementation may not be robust enough\n",
      "\n",
      "PROBLEM 4: No hyperparameter tuning for new features\n",
      "- Added 16 new features but kept same hyperparameters\n",
      "- Need stronger regularization (increase reg_alpha, reg_lambda)\n",
      "- Need to tune depth, min_child_samples\n"
     ]
    }
   ],
   "source": [
    "# Based on winning solution analysis, let's identify key differences\n",
    "\n",
    "print(\"=== Winners' Approach vs Our Implementation ===\")\n",
    "print()\n",
    "print(\"WINNERS (Chris Deotte, AngelosMar):\")\n",
    "print(\"1. Used sklearn's TargetEncoder with INTERNAL cross-fitting\")\n",
    "print(\"2. Applied target encoding to HIGH-cardinality features (not just Sex)\")\n",
    "print(\"3. Used product features BUT with careful regularization\")\n",
    "print(\"4. Used residual modeling (sequential approach)\")\n",
    "print(\"5. Used groupby z-score features\")\n",
    "print(\"6. Used MANY diverse models (7-12) with different feature sets\")\n",
    "print()\n",
    "print(\"OUR IMPLEMENTATION (exp_003):\")\n",
    "print(\"1. Manual target encoding with simple smoothing\")\n",
    "print(\"2. Only applied to Sex (2 categories - too low cardinality)\")\n",
    "print(\"3. Added product features without additional regularization\")\n",
    "print(\"4. No residual modeling\")\n",
    "print(\"5. No groupby features\")\n",
    "print(\"6. Only 2 models so far\")\n",
    "print()\n",
    "print(\"=== KEY INSIGHTS ===\")\n",
    "print()\n",
    "print(\"PROBLEM 1: Sex has only 2 categories - target encoding adds minimal signal\")\n",
    "print(\"- With only 'male' and 'female', encoding just creates 2 values\")\n",
    "print(\"- This is essentially just a binary feature, not true target encoding\")\n",
    "print(\"- Winners encoded binned features (higher cardinality)\")\n",
    "print()\n",
    "print(\"PROBLEM 2: Product features may be TOO predictive (leaking target)\")\n",
    "print(\"- Weight_Duration correlation with Calories: VERY HIGH\")\n",
    "print(\"- If product features alone can predict target well, they're too strong\")\n",
    "print(\"- Need to verify if these are legitimate or data leakage\")\n",
    "print()\n",
    "print(\"PROBLEM 3: Manual encoding vs sklearn's TargetEncoder\")\n",
    "print(\"- sklearn's version uses internal K-fold cross-fitting\")\n",
    "print(\"- This prevents overfitting better than simple smoothing\")\n",
    "print(\"- Our manual implementation may not be robust enough\")\n",
    "print()\n",
    "print(\"PROBLEM 4: No hyperparameter tuning for new features\")\n",
    "print(\"- Added 16 new features but kept same hyperparameters\")\n",
    "print(\"- Need stronger regularization (increase reg_alpha, reg_lambda)\")\n",
    "print(\"- Need to tune depth, min_child_samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2db4e3",
   "metadata": {},
   "source": [
    "## 5. Recommendations for Next Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RECOMMENDATIONS ===\\n\")\n",
    "\n",
    "print(\"1. ABANDON target encoding on 'Sex' (only 2 categories)\")\n",
    "print(\"   - Not enough cardinality to be useful\")\n",
    "print(\"   - May cause overfitting with manual implementation\")\n",
    "print(\"   - Winners encoded binned features instead\\n\")\n",
    "\n",
    "print(\"2. INVESTIGATE product features for target leakage\")\n",
    "print(\"   - Check if Weight_Duration, Duration_Heart_Rate are too predictive\")\n",
    "print(\"   - If correlation > 0.9, may be leaking target information\")\n",
    "print(\"   - Consider removing or transforming these features\\n\")\n",
    "\n",
    "print(\"3. IMPLEMENT sklearn's TargetEncoder properly\")\n",
    "print(\"   - Use internal cross-fitting (cv=5)\")\n",
    "print(\"   - Apply to binned features (higher cardinality)\")\n",
    "print(\"   - Test on small subset first\\n\")\n",
    "\n",
    "print(\"4. ADD HYPERPARAMETER TUNING for regularization\")\n",
    "print(\"   - Increase reg_alpha, reg_lambda for XGBoost\")\n",
    "print(\"   - Reduce max_depth or increase min_child_samples\")\n",
    "print(\"   - Run small grid search\\n\")\n",
    "\n",
    "print(\"5. IMPLEMENT RESIDUAL MODELING (sequential approach)\")\n",
    "print(\"   - LinearRegression → NeuralNetwork → XGBoost\")\n",
    "print(\"   - This was key in winning solutions\")\n",
    "print(\"   - Captures complementary patterns\\n\")\n",
    "\n",
    "print(\"6. ADD GROUPBY Z-SCORE FEATURES\")\n",
    "print(\"   - Group by Sex, compute z-scores for numerical features\")\n",
    "print(\"   - Creates relative positioning features\")\n",
    "print(\"   - Winners found these effective\\n\")\n",
    "\n",
    "print(\"7. CREATE MORE DIVERSE MODELS\")\n",
    "print(\"   - LightGBM with different feature sets\")\n",
    "print(\"   - Neural Network with residual approach\")\n",
    "print(\"   - Linear Regression with many engineered features\\n\")\n",
    "\n",
    "print(\"8. RUN ABLATION STUDIES\")\n",
    "print(\"   - Test each feature group separately\")\n",
    "print(\"   - Identify which features help vs hurt\")\n",
    "print(\"   - Systematically build up feature set\\n\")\n",
    "\n",
    "print(\"=== IMMEDIATE NEXT STEPS ===\\n\")\n",
    "print(\"1. Create experiment without product features (keep target encoding)\")\n",
    "print(\"2. Create experiment without target encoding (keep product features)\")\n",
    "print(\"3. Compare to identify which is the main culprit\")\n",
    "print(\"4. Implement sklearn's TargetEncoder on binned features\")\n",
    "print(\"5. Add groupby z-score features\")\n",
    "print(\"6. Start residual modeling pipeline\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
