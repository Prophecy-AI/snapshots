{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57cff45e",
   "metadata": {},
   "source": [
    "# Loop 6 Analysis: Residual Structure Verification and Linear Component Optimization\n",
    "\n",
    "## Objectives\n",
    "1. Verify residual structure from exp_005 (Linear Regression baseline)\n",
    "2. Optimize Linear Regression hyperparameters (alpha sweep)\n",
    "3. Analyze feature importance for linear component\n",
    "4. Create direct Neural Network baseline for comparison\n",
    "5. Prepare for Neural Network on residuals\n",
    "\n",
    "## Key Questions (from Evaluator)\n",
    "- Do residuals contain predictable structure? (Critical for pipeline viability)\n",
    "- Is alpha=1.0 optimal for Ridge regression?\n",
    "- Which features are most important for linear component?\n",
    "- Does direct modeling beat residual modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('/home/code/data/train.csv')\n",
    "test_df = pd.read_csv('/home/code/data/test.csv')\n",
    "residuals_df = pd.read_csv('/home/code/experiments/005_linear_regression/residuals_lr.csv')\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"Residuals: {residuals_df.shape}\")\n",
    "print(f\"Target range: [{train_df['Calories'].min():.2f}, {train_df['Calories'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f7c33",
   "metadata": {},
   "source": [
    "## 1. Residual Structure Analysis\n",
    "\n",
    "Analyze whether residuals contain predictable patterns or are just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37383190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple model on residuals to verify predictability\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING RESIDUAL PREDICTABILITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load and prepare data\n",
    "analysis_df = residuals_df.copy()\n",
    "features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "\n",
    "# Use original features to predict residuals\n",
    "X = analysis_df[features].copy()\n",
    "y_residual = analysis_df['residual'].copy()\n",
    "\n",
    "print(f\"Residuals shape: {y_residual.shape}\")\n",
    "print(f\"Residuals range: [{y_residual.min():.2f}, {y_residual.max():.2f}]\")\n",
    "print(f\"Residuals std: {y_residual.std():.6f}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Test with Decision Tree (can capture non-linear patterns)\n",
    "dt_scores = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled), 1):\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_residual.iloc[train_idx], y_residual.iloc[val_idx]\n",
    "    \n",
    "    # Decision Tree (depth=3 - simple model)\n",
    "    dt = DecisionTreeRegressor(max_depth=3, random_state=SEED)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred_val = dt.predict(X_val)\n",
    "    \n",
    "    # For residuals, use RMSE instead of RMSLE (residuals can be negative)\n",
    "    rmse = np.sqrt(np.mean((y_val - pred_val) ** 2))\n",
    "    dt_scores.append(rmse)\n",
    "    \n",
    "    print(f\"Fold {fold}: RMSE = {rmse:.6f}\")\n",
    "\n",
    "dt_cv = np.mean(dt_scores)\n",
    "print(f\"\\nDecision Tree CV RMSE: {dt_cv:.6f}\")\n",
    "print(f\"Residual std: {y_residual.std():.6f}\")\n",
    "if dt_cv < y_residual.std():\n",
    "    print(\"✓ RESIDUALS CONTAIN PREDICTABLE STRUCTURE → Pipeline viable!\")\n",
    "else:\n",
    "    print(\"⚠️  Residuals may be mostly noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dff0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(analysis_df['residual'], kde=True, bins=50)\n",
    "plt.title('Residual Distribution')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='oof_prediction', y='residual', data=analysis_df, alpha=0.5)\n",
    "plt.title('Residuals vs Predictions')\n",
    "plt.xlabel('OOF Prediction')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/residual_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for patterns in residuals vs original features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESIDUAL CORRELATIONS WITH ORIGINAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "correlations = []\n",
    "\n",
    "for feature in features:\n",
    "    corr = analysis_df['residual'].corr(analysis_df[feature])\n",
    "    correlations.append(corr)\n",
    "    print(f\"{feature:12s}: {corr:7.4f}\")\n",
    "\n",
    "# Check if any correlations are significant\n",
    "significant_corrs = [abs(c) for c in correlations if abs(c) > 0.1]\n",
    "if len(significant_corrs) > 0:\n",
    "    print(f\"\\n✓ Found {len(significant_corrs)} features with |correlation| > 0.1\")\n",
    "    print(\"✓ Residuals contain structure that can be predicted!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No strong correlations found - residuals may be mostly noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple model on residuals to verify predictability\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING RESIDUAL PREDICTABILITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use original features to predict residuals\n",
    "X = analysis_df[features].copy()\n",
    "y_residual = analysis_df['residual'].copy()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Test with Decision Tree (can capture non-linear patterns)\n",
    "dt_scores = []\n",
    "rf_scores = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled), 1):\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_residual.iloc[train_idx], y_residual.iloc[val_idx]\n",
    "    \n",
    "    # Decision Tree (depth=3 - simple model)\n",
    "    dt = DecisionTreeRegressor(max_depth=3, random_state=SEED)\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_val)\n",
    "    dt_rmsle = np.sqrt(mean_squared_log_error(y_val, dt_pred))\n",
    "    dt_scores.append(dt_rmsle)\n",
    "    \n",
    "    print(f\"Fold {fold}: Decision Tree RMSLE = {dt_rmsle:.6f}\")\n",
    "\n",
    "dt_cv = np.mean(dt_scores)\n",
    "print(f\"\\nDecision Tree CV RMSLE on residuals: {dt_cv:.6f} ± {np.std(dt_scores):.6f}\")\n",
    "\n",
    "if dt_cv < analysis_df['residual'].std():\n",
    "    print(\"✓ Residuals are predictable! Decision Tree can reduce residual error\")\n",
    "    print(f\"✓ Potential improvement: {analysis_df['residual'].std():.6f} → {dt_cv:.6f}\")\n",
    "else:\n",
    "    print(\"⚠️  Decision Tree cannot predict residuals better than mean\")\n",
    "    print(\"⚠️  Residuals may be mostly noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0e17b",
   "metadata": {},
   "source": [
    "## 2. Linear Regression Hyperparameter Optimization\n",
    "\n",
    "Test different alpha values for Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dea78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RIDGE REGRESSION ALPHA SWEEP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define feature creation function\n",
    "def create_lr_features(df):\n",
    "    \"\"\"Create minimal features for Linear Regression\"\"\"\n",
    "    df_new = df.copy()\n",
    "    num_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "    sex_encoded = pd.get_dummies(df_new['Sex'], prefix='Sex')\n",
    "    df_new = pd.concat([df_new, sex_encoded], axis=1)\n",
    "    feature_cols = num_features + list(sex_encoded.columns)\n",
    "    return df_new, feature_cols\n",
    "\n",
    "# Prepare data\n",
    "train_feat, feature_cols = create_lr_features(train_df)\n",
    "X = train_feat[feature_cols]\n",
    "y = train_feat['Calories']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Alpha values to test\n",
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]\n",
    "alpha_results = []\n",
    "\n",
    "print(\"Testing alpha values:\", alphas)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for alpha in alphas:\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled), 1):\n",
    "        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Ridge regression\n",
    "        model = Ridge(alpha=alpha, random_state=SEED)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        pred_val = model.predict(X_val)\n",
    "        pred_val = np.clip(pred_val, y.min(), y.max())\n",
    "        rmsle = np.sqrt(mean_squared_log_error(y_val, pred_val))\n",
    "        fold_scores.append(rmsle)\n",
    "    \n",
    "    cv_score = np.mean(fold_scores)\n",
    "    alpha_results.append((alpha, cv_score, fold_scores))\n",
    "    print(f\"Alpha {alpha:5.1f}: CV = {cv_score:.6f}\")\n",
    "\n",
    "# Find best alpha\n",
    "best_alpha, best_score, _ = min(alpha_results, key=lambda x: x[1])\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best CV: {best_score:.6f}\")\n",
    "print(f\"Improvement over alpha=1.0: {[s for a,s,_ in alpha_results if a==1.0][0] - best_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143fff2",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for the linear component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff853592",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train model with best alpha on full data\n",
    "best_model = Ridge(alpha=best_alpha, random_state=SEED)\n",
    "best_model.fit(X_scaled, y)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = best_model.coef_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature coefficients (sorted by absolute value):\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=feature_importance.head(8), x='coefficient', y='feature')\n",
    "plt.title('Feature Coefficients (Best Ridge)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=feature_importance.head(8), x='abs_coefficient', y='feature')\n",
    "plt.title('Feature Importance (Absolute Coefficients)')\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify least important features\n",
    "print(\"\\nLeast important features:\")\n",
    "print(feature_importance.tail(3).to_string(index=False))\n",
    "\n",
    "# Test model without least important feature\n",
    "least_important = feature_importance.iloc[-1]['feature']\n",
    "print(f\"\\nTesting model without {least_important}...\")\n",
    "\n",
    "features_reduced = [f for f in feature_cols if f != least_important]\n",
    "X_reduced = train_feat[features_reduced]\n",
    "scaler_reduced = StandardScaler()\n",
    "X_reduced_scaled = scaler_reduced.fit_transform(X_reduced)\n",
    "\n",
    "fold_scores_reduced = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_reduced_scaled), 1):\n",
    "    X_train, X_val = X_reduced_scaled[train_idx], X_reduced_scaled[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = Ridge(alpha=best_alpha, random_state=SEED)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    pred_val = model.predict(X_val)\n",
    "    pred_val = np.clip(pred_val, y.min(), y.max())\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_val, pred_val))\n",
    "    fold_scores_reduced.append(rmsle)\n",
    "\n",
    "cv_reduced = np.mean(fold_scores_reduced)\n",
    "print(f\"CV without {least_important}: {cv_reduced:.6f}\")\n",
    "print(f\"Difference: {cv_reduced - best_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f950977",
   "metadata": {},
   "source": [
    "## 4. Direct Neural Network Baseline\n",
    "\n",
    "Train a Neural Network directly on original target for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01304630",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIRECT NEURAL NETWORK BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "train_feat, feature_cols = create_lr_features(train_df)\n",
    "X = train_feat[feature_cols]\n",
    "y = train_feat['Calories']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Simple MLP architecture\n",
    "mlp_scores = []\n",
    "oof_mlp = np.zeros(len(train_df))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled), 1):\n",
    "    print(f\"\\nFold {fold}/5\")\n",
    "    \n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # MLP with small architecture\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        alpha=0.001,  # L2 regularization\n",
    "        batch_size=32,\n",
    "        learning_rate='adaptive',\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=200,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=SEED + fold\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred_val = mlp.predict(X_val)\n",
    "    pred_val = np.clip(pred_val, y.min(), y.max())\n",
    "    \n",
    "    # Evaluate using RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_val, pred_val))\n",
    "    mlp_scores.append(rmsle)\n",
    "    oof_mlp[val_idx] = pred_val\n",
    "    \n",
    "    print(f\"  RMSLE: {rmsle:.6f}\")\n",
    "\n",
    "mlp_cv = np.mean(mlp_scores)\n",
    "print(f\"\\nMLP CV RMSLE: {mlp_cv:.6f}\")\n",
    "print(f\"Linear Regression CV: 0.208762\")\n",
    "print(f\"Improvement: {0.208762 - mlp_cv:.6f}\")\n",
    "\n",
    "if mlp_cv < 0.208762:\n",
    "    print(\"\\u2713 Direct MLP beats Linear Regression!\")\n",
    "else:\n",
    "    print(\"\\u26a0\\ufe0f Linear Regression still better - residual modeling may help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebd0b7",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the analysis, provide clear recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dfc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANALYSIS SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. RESIDUAL STRUCTURE VERIFICATION:\")\n",
    "print(f\"   - Residuals std: {analysis_df['residual'].std():.6f}\")\n",
    "print(f\"   - Variance explained: {(1 - analysis_df['residual'].std()/y.std())*100:.2f}%\")\n",
    "print(f\"   - Decision Tree can predict residuals: CV = {dt_cv:.6f}\")\n",
    "if dt_cv < analysis_df['residual'].std():\n",
    "    print(\"   ✓ RESIDUALS CONTAIN PREDICTABLE STRUCTURE → Pipeline viable!\")\n",
    "else:\n",
    "    print(\"   ⚠️  Residuals may be mostly noise\")\n",
    "\n",
    "print(\"\\n2. HYPERPARAMETER OPTIMIZATION:\")\n",
    "print(f\"   - Best alpha: {best_alpha}\")\n",
    "print(f\"   - Best CV: {best_score:.6f}\")\n",
    "print(f\"   - Improvement over alpha=1.0: {[s for a,s,_ in alpha_results if a==1.0][0] - best_score:.6f}\")\n",
    "if best_alpha != 1.0:\n",
    "    print(\"   ✓ Different alpha improves performance - use best alpha for final model\")\n",
    "else:\n",
    "    print(\"   ✓ Alpha=1.0 is already optimal\")\n",
    "\n",
    "print(\"\\n3. FEATURE IMPORTANCE:\")\n",
    "print(f\"   - Most important: {feature_importance.iloc[0]['feature']} (coeff: {feature_importance.iloc[0]['coefficient']:.4f})\")\n",
    "print(f\"   - Least important: {feature_importance.iloc[-1]['feature']} (coeff: {feature_importance.iloc[-1]['coefficient']:.4f})\")\n",
    "if cv_reduced < best_score:\n",
    "    print(\"   ✓ Removing least important feature improves CV\")\n",
    "else:\n",
    "    print(\"   ✓ All features contribute positively\")\n",
    "\n",
    "print(\"\\n4. DIRECT vs RESIDUAL MODELING:\")\n",
    "print(f\"   - Linear Regression CV: {best_score:.6f}\")\n",
    "print(f\"   - Direct MLP CV: {mlp_cv:.6f}\")\n",
    "if mlp_cv < best_score:\n",
    "    print(\"   ✓ Direct MLP beats Linear Regression - consider direct approach\")\n",
    "else:\n",
    "    print(\"   ✓ Linear Regression is strong baseline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Re-train Linear Regression with best alpha ({})\".format(best_alpha))\n",
    "print(\"2. Generate new residuals with optimized model\")\n",
    "print(\"3. Train Neural Network on residuals (if dt_cv < residual_std)\")\n",
    "print(\"4. Compare sequential approach vs direct MLP\")\n",
    "print(\"5. Proceed with full residual modeling pipeline\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
