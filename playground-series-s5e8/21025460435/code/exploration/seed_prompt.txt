## Competition Overview
Binary classification problem predicting bank term deposit subscription. Evaluation metric: ROC AUC. Target score: 0.978010+.

**Reference notebooks for data characteristics:**
- See exploration notebooks for feature distributions, missing values, target distribution, and correlations
- Dataset is synthetic but based on Bank Marketing Dataset - consider using original data for augmentation

## Core Modeling Strategy

### Model Diversity is Critical
Top solutions emphasize creating diverse models rather than hyperparameter tuning:
- **2nd place**: 59 diverse models (TabM, XGBoost, LightGBM, RealMLP, CatBoost, DeepTables, TabR, Gandalf, Random Forest, GRN, FTTransformer, CNN, Bartz)
- **19th place**: 108 models with different feature sets, encodings, and architectures
- **8th place**: Hill climbing selected 18 base models + meta-learners

**Key principle**: Models should make different errors. Diversity beats marginal hyperparameter improvements.

### Recommended Model Types
1. **Gradient Boosting (70% of ensemble)**
   - XGBoost, LightGBM, CatBoost (primary workhorses)
   - Vary hyperparameters, depths, learning rates, random seeds
   - Use different feature subsets for each

2. **Neural Networks (30% of ensemble)**
   - Error patterns highly complementary to tree models
   - RealMLP, DeepTables, TabR, FTTransformer, CNN
   - Even simple MLPs provide valuable diversity

3. **Specialized Tabular Models**
   - TabM, Gandalf, GRN - designed specifically for tabular data
   - Often outperform generic approaches

## Feature Engineering

### Essential Techniques
1. **Cyclical Features** (from 8th place solution)
   - Convert month, day to sin/cos transformations
   - Apply to duration, balance, age, pdays
   - Captures periodic patterns

2. **Interaction Features**
   - Pairwise combinations: f1+f2, f1-f2, f1/f2, f1*f2
   - Focus on numeric features: age, balance, day, duration, campaign, pdays, previous
   - Categorical combinations (bigrams): default_housing, housing_loan, etc.

3. **Target Encoding**
   - Mean, median, std, variance, min, max encodings
   - Apply within CV folds to prevent leakage
   - Count encoding (train+test combined) can work despite leakage risk

4. **Numerical to Categorical**
   - Convert selected numeric features to categorical
   - Particularly effective for features with periodic patterns or specific value clusters
   - Can boost CatBoost performance significantly (0.965 → 0.972)

5. **Binning/Qcut**
   - duration_bin_20, balance_bin_20 (20 quantile bins)
   - Day column binning: >0, >=7, >=30, >=90, >=180, >=365

### Original Data Augmentation
- Combine synthetic data with original Bank Marketing dataset
- Use groupby target means from original data as features
- Post-processing with original data can improve generalization

## Validation Strategy

### Cross-Validation
- **Stratified K-Fold (k=5)**: Most common approach
- **Consistent CV scheme**: Use same splits across ALL models
- **Seed**: 42 or 208 (used by top solutions)
- **No full refits**: Train on folds only, no final model on full data

### CV-LB Correlation
- Strong correlation between CV and LB scores is achievable
- Trust CV improvements over LB fluctuations
- When CV moves, LB typically follows same direction

## Ensembling Strategies

### Meta-Learners (Critical for Top Scores)
1. **RidgeCV** (19th place)
   - Learns optimal weights for blending
   - Regularized linear regression on OOF predictions
   - Handles 100+ models efficiently

2. **CatBoost/LightGBM as Ensembler** (2nd place)
   - Train on OOF predictions from base models
   - Often outperforms simple averaging
   - CatBoost: CV 0.977432, LB 0.97817

3. **Hill Climbing** (8th place)
   - Iteratively add models that improve ensemble
   - Can use positive/negative weights
   - Selected 18 base models → 14 meta-learners

4. **AutoGluon** (11th, 5th place)
   - Automated stacking with multiple layers
   - Configurations: (8,2,2,best) or (10,2,2,medium)
   - 16+ hour runtime for full ensemble

### Blending Approaches
- **Two-feature-set blending**: Create diverse feature pipelines, blend predictions
- **Model stacking**: 2-3 layers deep
- **50-50 blending**: Simple but effective for diverse feature sets

## Implementation Priorities

### Phase 1: Base Models
1. Create 3-5 diverse gradient boosting models
   - Different seeds, hyperparameters, feature subsets
   - Include at least one neural network

2. Generate OOF predictions consistently
   - Same CV splits across all models
   - Save OOFs for ensembling

### Phase 2: Feature Engineering
1. Implement cyclical transformations
2. Add interaction features
3. Apply target encoding (fold-aware)
4. Convert key numerics to categorical

### Phase 3: Ensembling
1. Collect OOFs from all base models
2. Train RidgeCV/CatBoost meta-learner
3. Experiment with hill climbing
4. Consider AutoGluon for final stacking

## Key Lessons from Winners

### What Worked
- **Diversity over optimization**: 59 models with suboptimal hyperparameters beat few tuned models
- **Neural networks add value**: Even if individual scores lower, complementary errors boost ensemble
- **Target encoding**: Major source of diversity when applied with different strategies
- **Original data**: Augmentation with real Bank Marketing dataset helps generalization

### What to Avoid
- **Inconsistent CV**: Using different splits prevents advanced stacking
- **Overfitting to LB**: Trust CV improvements
- **Ignoring diversity**: Same model with different seeds adds little value
- **Data leakage**: Ensure target encoding is fold-aware

## Resource Management
- **108 models**: Requires organized tracking (spreadsheets, descriptive naming)
- **GPU requirements**: Varies by model type
  - AutoGluon: L4 Colab sufficient
  - XGBoost: A6000, A5000, 4090
  - Neural nets: T4 x 2
- **Memory**: Polars with parquet partitioning (50K rows/partition) helps with large feature sets

## Expected Performance Trajectory
- Single model: 0.970-0.976
- Simple ensemble (3-5 models): 0.976-0.977
- Large diverse ensemble (20+ models): 0.977-0.978
- Advanced stacking (50+ models): 0.978+

Focus on systematic diversity building rather than chasing single-model improvements.