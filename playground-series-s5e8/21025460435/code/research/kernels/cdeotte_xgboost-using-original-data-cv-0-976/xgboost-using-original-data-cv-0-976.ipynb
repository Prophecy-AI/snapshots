{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"},{"sourceId":9293783,"sourceType":"datasetVersion","datasetId":5626665}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost Starter Notebook\nThis is a starter notebook for Kaggle's August Playground competition. It trains an XGBoost model with some feature engineering and achieves CV 0.976. This notebook shows some techniques that I have used in previous Kaggle playground competitions to perform well.\n\nThis notebook demonstrates at least these 5 things:\n* How to create new categorical features from pairs of existing categorical features\n* How to use original data as new columns\n* How to use original data as new rows\n* How to categorical encode (with CE count encoding and TE target encoding)\n* How to train XGB with big data using memory efficient QuantileDMatrix\n\nDiscussion about this notebook is [here][1]. This notebook achieves LB 0.977 (submitted [here][2] and [here][3])\n\n[1]: https://www.kaggle.com/competitions/playground-series-s5e8/discussion/600048\n[2]: https://www.kaggle.com/code/daniyalatta/binary-classification-bank-dataset-xgboost-0-977\n[3]: https://www.kaggle.com/code/nina2025/xgboost-using-original-data-cv-0-976-fe","metadata":{}},{"cell_type":"markdown","source":"# NVIDIA cuDF Pandas\nWe accelerate our notebook with the magic command `%load_ext cudf.pandas`. Afterward, all calls to Pandas will run NVIDIA cuDF on GPU (instead of Pandas on CPU). This makes our notebook run faster!","metadata":{}},{"cell_type":"code","source":"%load_ext cudf.pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:51.110234Z","iopub.execute_input":"2025-08-20T13:50:51.110489Z","iopub.status.idle":"2025-08-20T13:50:58.411648Z","shell.execute_reply.started":"2025-08-20T13:50:51.110462Z","shell.execute_reply":"2025-08-20T13:50:58.410794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data\nWe load train, test, and original datasets. In every Kaggle playground competition, the data is synthetic and it is generated from an original dataset. In this competition, the original dataset is [here][1]\n\n[1]: https://www.kaggle.com/datasets/sushant097/bank-marketing-dataset-full","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np, os\n\nPATH = \"/kaggle/input/playground-series-s5e8/\"\ntrain = pd.read_csv(f\"{PATH}train.csv\").set_index('id')\nprint(\"Train shape\", train.shape )\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:58.412446Z","iopub.execute_input":"2025-08-20T13:50:58.4129Z","iopub.status.idle":"2025-08-20T13:50:59.045653Z","shell.execute_reply.started":"2025-08-20T13:50:58.412869Z","shell.execute_reply":"2025-08-20T13:50:59.044879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(f\"{PATH}test.csv\").set_index('id')\ntest['y'] = -1\nprint(\"Test shape\", test.shape )\ntest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:59.04738Z","iopub.execute_input":"2025-08-20T13:50:59.047564Z","iopub.status.idle":"2025-08-20T13:50:59.195725Z","shell.execute_reply.started":"2025-08-20T13:50:59.04755Z","shell.execute_reply":"2025-08-20T13:50:59.19515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig = pd.read_csv(\"/kaggle/input/bank-marketing-dataset-full/bank-full.csv\",delimiter=\";\")\norig['y'] = orig.y.map({'yes':1,'no':0})\norig['id'] = (np.arange(len(orig))+1e6).astype('int')\norig = orig.set_index('id')\nprint(\"Original data shape\", orig.shape )\norig.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:59.196286Z","iopub.execute_input":"2025-08-20T13:50:59.196476Z","iopub.status.idle":"2025-08-20T13:50:59.501071Z","shell.execute_reply.started":"2025-08-20T13:50:59.19646Z","shell.execute_reply":"2025-08-20T13:50:59.500331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)\nWe now combine all data together and then explore the columns and their properties. We observe that there is no missing data. And we observe that the categorical columns have low cardinality (i.e. under 12). We observe that most numerical columns have few unique values, and two numerical columns have around 2k and 8k unique values.","metadata":{}},{"cell_type":"code","source":"combine = pd.concat([train,test,orig],axis=0)\nprint(\"Combined data shape\", combine.shape )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:59.501896Z","iopub.execute_input":"2025-08-20T13:50:59.502453Z","iopub.status.idle":"2025-08-20T13:50:59.532377Z","shell.execute_reply.started":"2025-08-20T13:50:59.502423Z","shell.execute_reply":"2025-08-20T13:50:59.531786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CATS = []\nNUMS = []\nfor c in combine.columns[:-1]:\n    t = \"CAT\"\n    if combine[c].dtype=='object':\n        CATS.append(c)\n    else:\n        NUMS.append(c)\n        t = \"NUM\"\n    n = combine[c].nunique()\n    na = combine[c].isna().sum()\n    print(f\"[{t}] {c} has {n} unique and {na} NA\")\nprint(\"CATS:\", CATS )\nprint(\"NUMS:\", NUMS )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:59.533044Z","iopub.execute_input":"2025-08-20T13:50:59.533239Z","iopub.status.idle":"2025-08-20T13:50:59.639902Z","shell.execute_reply.started":"2025-08-20T13:50:59.533225Z","shell.execute_reply":"2025-08-20T13:50:59.639121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (LE - Label Encode)\nWe will label encode all categorical columns. Also we will make a duplicate of each numerical column and treat the copy as a categorical column.","metadata":{}},{"cell_type":"code","source":"CATS1 = []\nSIZES = {}\nfor c in NUMS + CATS:\n    n = c\n    if c in NUMS: \n        n = f\"{c}2\"\n        CATS1.append(n)\n    combine[n],_ = combine[c].factorize()\n    SIZES[n] = combine[n].max()+1\n\n    combine[c] = combine[c].astype('int32')\n    combine[n] = combine[n].astype('int32')\n\nprint(\"New CATS:\", CATS1 )\nprint(\"Cardinality of all CATS:\", SIZES )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:59.640717Z","iopub.execute_input":"2025-08-20T13:50:59.640976Z","iopub.status.idle":"2025-08-20T13:50:59.920232Z","shell.execute_reply.started":"2025-08-20T13:50:59.640938Z","shell.execute_reply":"2025-08-20T13:50:59.919499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (Combine Column Pairs)\nWe will create a new categorical column from every pair of existing categorical columns. The original categorical columns have been label encoded into integers from `0 to N-1` each. Therefore we can create a new column with unique integers using the formula `new_cols[name] = combine[c1] * SIZES[c2] + combine[c2]`.","metadata":{}},{"cell_type":"code","source":"from itertools import combinations\n\npairs = combinations(CATS + CATS1, 2)\nnew_cols = {}\nCATS2 = []\n\nfor c1, c2 in pairs:\n    name = \"_\".join(sorted((c1, c2)))\n    new_cols[name] = combine[c1] * SIZES[c2] + combine[c2]\n    CATS2.append(name)\nif new_cols:\n    new_df = pd.DataFrame(new_cols)         \n    combine = pd.concat([combine, new_df], axis=1) \n\nprint(f\"Created {len(CATS2)} new CAT columns\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:50:59.921026Z","iopub.execute_input":"2025-08-20T13:50:59.921285Z","iopub.status.idle":"2025-08-20T13:51:00.14256Z","shell.execute_reply.started":"2025-08-20T13:50:59.92126Z","shell.execute_reply":"2025-08-20T13:51:00.141954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer (CE - Count Encoding)\nWe now have 136 categorical columns. We will count encode each of them and create 136 new columns.","metadata":{}},{"cell_type":"code","source":"CE = []\nCC = CATS+CATS1+CATS2\n\nprint(f\"Processing {len(CC)} columns... \",end=\"\")\nfor i,c in enumerate(CC):\n    if i%10==0: print(f\"{i}, \",end=\"\")\n    tmp = combine.groupby(c).y.count()\n    tmp = tmp.astype('int32')\n    tmp.name = f\"CE_{c}\"\n    CE.append( f\"CE_{c}\" )\n    combine = combine.merge(tmp, on=c, how='left')\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:51:00.145101Z","iopub.execute_input":"2025-08-20T13:51:00.145314Z","iopub.status.idle":"2025-08-20T13:51:10.046116Z","shell.execute_reply.started":"2025-08-20T13:51:00.145297Z","shell.execute_reply":"2025-08-20T13:51:10.045499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = combine.iloc[:len(train)]\ntest = combine.iloc[len(train):len(train)+len(test)]\norig = combine.iloc[-len(orig):]\ndel combine\nprint(\"Train shape\", train.shape,\"Test shape\", test.shape,\"Original shape\", orig.shape )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:51:10.046683Z","iopub.execute_input":"2025-08-20T13:51:10.04691Z","iopub.status.idle":"2025-08-20T13:51:10.083112Z","shell.execute_reply.started":"2025-08-20T13:51:10.046892Z","shell.execute_reply":"2025-08-20T13:51:10.082436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train XGB w/ Original Data as Rows\nWe will now train XGBoost with adding original data as new rows. Inside each KFold for loop iteration, we will concatenate the original data to the train data as new rows. \n\nWe will also use a `QuantileDMatrix` instead of a `DMatrix`. This reduces memory usage because the data will be loaded into GPU VRAM in batches and compressed in batches. This is different than the basic `DMatrix` which tries to load all the train data into GPU VRAM at the same time which spikes the memory and can cause OOM.","metadata":{}},{"cell_type":"code","source":"from cuml.preprocessing import TargetEncoder\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\n\nprint(f\"XGBoost version {xgb.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:51:10.083885Z","iopub.execute_input":"2025-08-20T13:51:10.084148Z","iopub.status.idle":"2025-08-20T13:51:13.596448Z","shell.execute_reply.started":"2025-08-20T13:51:10.084124Z","shell.execute_reply":"2025-08-20T13:51:13.595802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGB Hyperparameters","metadata":{}},{"cell_type":"code","source":"FEATURES = NUMS+CATS+CATS1+CATS2+CE\nprint(f\"We have {len(FEATURES)} features.\")\n\nFOLDS = 7\nSEED = 42\n\nparams = {\n    \"objective\": \"binary:logistic\",  \n    \"eval_metric\": \"auc\",           \n    \"learning_rate\": 0.1,\n    \"max_depth\": 0,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.7,\n    \"seed\": SEED,\n    \"device\": \"cuda\",\n    \"grow_policy\": \"lossguide\", \n    \"max_leaves\": 32,          \n    \"alpha\": 2.0,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:51:13.597131Z","iopub.execute_input":"2025-08-20T13:51:13.597636Z","iopub.status.idle":"2025-08-20T13:51:13.602365Z","shell.execute_reply.started":"2025-08-20T13:51:13.597615Z","shell.execute_reply":"2025-08-20T13:51:13.601533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loader for QuantileDMatrix\nWe need the following data loader to use XGB's memory efficient `QuantileDMatrix`. This data loader yields batches of data on GPU for XGB.","metadata":{}},{"cell_type":"code","source":"class IterLoadForDMatrix(xgb.core.DataIter):\n    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n        self.features = features\n        self.target = target\n        self.df = df\n        self.it = 0 \n        self.batch_size = batch_size\n        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n        super().__init__()\n\n    def reset(self):\n        '''Reset the iterator'''\n        self.it = 0\n\n    def next(self, input_data):\n        '''Yield next batch of data.'''\n        if self.it == self.batches:\n            return 0 # Return 0 when there's no more batch.\n        \n        a = self.it * self.batch_size\n        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n        #dt = cudf.DataFrame(self.df.iloc[a:b])\n        dt = self.df.iloc[a:b]\n        input_data(data=dt[self.features], label=dt[self.target]) \n        self.it += 1\n        return 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:51:13.603344Z","iopub.execute_input":"2025-08-20T13:51:13.604095Z","iopub.status.idle":"2025-08-20T13:51:13.629233Z","shell.execute_reply.started":"2025-08-20T13:51:13.604072Z","shell.execute_reply":"2025-08-20T13:51:13.628407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train XGB\n## w/ Feature Engineer - (TE - NVIDIA cuML)\nInside each KFold for loop iteration, we will use NVIDIA cuML Target Encoder to target encode some high cardinality categorical features","metadata":{}},{"cell_type":"code","source":"oof_preds = np.zeros(len(train))\ntest_preds = np.zeros(len(test))\n\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1} ###\")\n    print(\"#\"*25)\n\n    Xy_train = train.iloc[train_idx][ FEATURES+['y'] ].copy()\n    Xy_more = orig[ FEATURES+['y'] ]\n    for k in range(1):\n        Xy_train = pd.concat([Xy_train,Xy_more],axis=0,ignore_index=True)\n    \n    X_valid = train.iloc[val_idx][FEATURES].copy()\n    y_valid = train.iloc[val_idx]['y']\n    X_test = test[FEATURES].copy()\n\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        TE0 = TargetEncoder(n_folds=10, smooth=0, split_method='random', stat='mean')\n        Xy_train[c] = TE0.fit_transform(Xy_train[c],Xy_train['y']).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n    print()\n\n    Xy_train[CATS] = Xy_train[CATS].astype('category')\n    X_valid[CATS] = X_valid[CATS].astype('category')\n    X_test[CATS] = X_test[CATS].astype('category')\n\n    Xy_train = IterLoadForDMatrix(Xy_train, FEATURES, 'y')\n    dtrain = xgb.QuantileDMatrix(Xy_train, enable_categorical=True, max_bin=256)\n    dval   = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n    dtest  = xgb.DMatrix(X_test, enable_categorical=True)\n\n    model = xgb.train(\n        params=params,\n        dtrain=dtrain,\n        num_boost_round=10_000,\n        evals=[(dtrain, \"train\"), (dval, \"valid\")],\n        early_stopping_rounds=200,\n        verbose_eval=200\n    )\n\n    oof_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n    test_preds += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / FOLDS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T13:51:13.630139Z","iopub.execute_input":"2025-08-20T13:51:13.630367Z","execution_failed":"2025-08-20T13:55:29.849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nm = roc_auc_score(train.y, oof_preds)\nprint(f\"XGB with Original Data as rows CV = {m}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10, 5))\nxgb.plot_importance(model, max_num_features=20, importance_type='gain',ax=ax)\nplt.title(\"Top 20 Feature Importances (XGBoost)\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train XGB w/ Original Data as Columns\nWe will now train XGBoost with adding original data as new columns. We accomplish this by Target Encoding our categorical columns using the targets of the original data. Since these are not the targets of our true train.csv data, there is no leakage and we don't need to use kfold to add these new columns.\n\nWe will also use a `QuantileDMatrix` instead of a `DMatrix`. This reduces memory usage because the data will be loaded into GPU VRAM in batches and compressed in batches. This is different than the basic `DMatrix` which tries to load all the train data into GPU VRAM at the same time which spikes the memory and can cause OOM.","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering (TE - Original Data as Cols)\nBelow is a technique to add the original data as new columns.","metadata":{}},{"cell_type":"code","source":"TE_ORIG = []\nCC = CATS+CATS1+CATS2\n\nprint(f\"Processing {len(CC)} columns... \",end=\"\")\nfor i,c in enumerate(CC):\n    if i%10==0: print(f\"{i}, \",end=\"\")\n    tmp = orig.groupby(c).y.mean()\n    tmp = tmp.astype('float32')\n    tmp.name = f\"TE_ORIG_{c}\"\n    TE_ORIG.append( f\"TE_ORIG_{c}\" )\n    train = train.merge(tmp, on=c, how='left')\n    test = test.merge(tmp, on=c, how='left')\nprint()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGB Hyperparameters","metadata":{}},{"cell_type":"code","source":"FEATURES += TE_ORIG\nprint(f\"We have {len(FEATURES)} features.\")\n\nFOLDS = 7\nSEED = 42\n\nparams = {\n    \"objective\": \"binary:logistic\",  \n    \"eval_metric\": \"auc\",           \n    \"learning_rate\": 0.1,\n    \"max_depth\": 0,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.7,\n    \"seed\": SEED,\n    \"device\": \"cuda\",\n    \"grow_policy\": \"lossguide\", \n    \"max_leaves\": 32,           \n    \"alpha\": 2.0,\n}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train XGB\n## w/ Feature Engineer - (TE - NVIDIA cuML)\nInside each KFold for loop iteration, we will use NVIDIA cuML Target Encoder to target encode some high cardinality categorical features","metadata":{}},{"cell_type":"code","source":"oof_preds2 = np.zeros(len(train))\ntest_preds2 = np.zeros(len(test))\n\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train)):\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1} ###\")\n    print(\"#\"*25)\n\n    Xy_train = train.iloc[train_idx][ FEATURES+['y'] ].copy()    \n    X_valid = train.iloc[val_idx][FEATURES].copy()\n    y_valid = train.iloc[val_idx]['y']\n    X_test = test[FEATURES].copy()\n\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        TE0 = TargetEncoder(n_folds=10, smooth=0, split_method='random', stat='mean')\n        Xy_train[c] = TE0.fit_transform(Xy_train[c],Xy_train['y']).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n    print()\n\n    Xy_train[CATS] = Xy_train[CATS].astype('category')\n    X_valid[CATS] = X_valid[CATS].astype('category')\n    X_test[CATS] = X_test[CATS].astype('category')\n\n    Xy_train = IterLoadForDMatrix(Xy_train, FEATURES, 'y')\n    dtrain = xgb.QuantileDMatrix(Xy_train, enable_categorical=True, max_bin=256)\n    dval   = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n    dtest  = xgb.DMatrix(X_test, enable_categorical=True)\n\n    model = xgb.train(\n        params=params,\n        dtrain=dtrain,\n        num_boost_round=10_000,\n        evals=[(dtrain, \"train\"), (dval, \"valid\")],\n        early_stopping_rounds=200,\n        verbose_eval=200\n    )\n\n    oof_preds2[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n    test_preds2 += model.predict(dtest, iteration_range=(0, model.best_iteration + 1)) / FOLDS","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nm = roc_auc_score(train.y, oof_preds2)\nprint(f\"XGB with Original Data as columns CV = {m}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10, 5))\nxgb.plot_importance(model, max_num_features=20, importance_type='gain',ax=ax)\nplt.title(\"Top 20 Feature Importances (XGBoost)\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble XGB Orig Rows w/ XGB Orig Cols","metadata":{}},{"cell_type":"code","source":"m = roc_auc_score(train.y, oof_preds+oof_preds2)\nprint(f\"Ensemble CV = {m}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SAVE OOF PREDS\nnp.save('oof_xgb_with_orig_rows',oof_preds)\nnp.save('oof_xgb_with_orig_cols',oof_preds2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T20:02:29.527243Z","iopub.execute_input":"2025-08-23T20:02:29.527493Z","iopub.status.idle":"2025-08-23T20:02:29.532339Z","shell.execute_reply.started":"2025-08-23T20:02:29.52745Z","shell.execute_reply":"2025-08-23T20:02:29.531422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Submission CSV","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(f\"{PATH}sample_submission.csv\")\nsub['y'] = (test_preds + test_preds2)/2.\nsub.to_csv(\"submission.csv\",index=False)\nprint('Submission shape',sub.shape)\nsub.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA Test Preds","metadata":{}},{"cell_type":"code","source":"plt.hist(sub.y,bins=100)\nplt.title('Test Preds')\nplt.ylim((0,10_000))\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-20T13:55:29.85Z"}},"outputs":[],"execution_count":null}]}