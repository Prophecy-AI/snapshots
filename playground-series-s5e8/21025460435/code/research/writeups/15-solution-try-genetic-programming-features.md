# #15 solution - Try genetic programming features

**Rank:** 15
**Author:** Tilii
**Collaborators:** Tilii
**Votes:** 20

---

My solution is pretty standard: made many models and many ensembles. At some point adding more models led to slightly better CV, but not public LB scores.

Here are my best 5 types of individual models based on private LB scores:

| Model | CV | Public LB | Private LB |
| --- | --- | --- | --- |
| CatBoost Optuna | 0.976787 | 0.97772 | 0.97732 |
| xLearn FFM | 0.976567 | 0.97740 | 0.97708 |
| XGBoost Optuna | 0.976501 | 0.97736 | 0.97701 |
| Keras FM | 0.976795 | 0.97740 | 0.97701 |
| LAMA DenseLight | 0.975058 | 0.97629 | 0.97589 |


I used different ensemblers, LAMA DenseLight worked the best.

The main reason I write this it to call attention to 13 features created by genetic programming - see the notebook [**here**](https://www.kaggle.com/code/tilii7/new-features-genetic-programming). Features are kind of nutty and it is difficult to make sense of them. Here is a couple of simple ones:

```
train['GP_07'] = np.exp(np.cos(np.exp(train['duration'] - np.exp(np.exp(np.cos(np.exp(train['pdays'])))))))
train['GP_08'] = np.cos(np.exp(train['duration'] - np.exp(train['pdays']) - np.exp(np.exp(-0.240221))))
```

A complicated feature:

```
train['GP_12'] = (train['balance'] - train['duration'] + np.exp(np.sqrt(np.abs(np.exp(2.136486*np.where(np.abs(np.where(np.abs(4.411698) < 1.0e-6, 0, np.log(4.411698))) < 1.0e-6, 0, np.log(np.where(np.abs(4.411698) < 1.0e-6, 0, np.log(4.411698))))**(-1))))))*np.sqrt(np.abs(np.where(train['duration'] <= 2.641809 - np.sqrt(np.abs(np.where(train['age'] <= 2.641809, np.where(np.abs(3.880161) < 1.0e-6, 0, np.log(3.880161)), np.exp(3.17722)))), train['duration'] + np.where(np.abs(3.880161) < 1.0e-6, 0, np.log(3.880161)), np.sin(3.17722))))**(-1)
```


If you add them to the raw dataset, it will increase the score by 0.0025-0.003. That doesn't hold nearly as well after other features are generated by counting and target encoding, but it still helped the scores by ~0.0001. If you try them, please let me know whether they work at all for you.