{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0909c47",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Extraction - Baseline Model\n",
    "\n",
    "This notebook implements a baseline approach for the tweet sentiment extraction competition.\n",
    "\n",
    "## Strategy\n",
    "1. Simple rule-based approach based on sentiment\n",
    "2. For positive/negative: extract the most relevant word/phrase\n",
    "3. For neutral: use the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f52160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "sample_submission = pd.read_csv('/home/data/sample_submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nTrain data info:\")\n",
    "print(train_df.head())\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in train:\")\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Jaccard score function for evaluation\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c)) if (len(a) + len(b) - len(c)) > 0 else 0.0\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing Jaccard function:\")\n",
    "print(f\"jaccard('very good', 'very good'): {jaccard('very good', 'very good')}\")\n",
    "print(f\"jaccard('very good', 'good'): {jaccard('very good', 'good')}\")\n",
    "print(f\"jaccard('very good', 'bad'): {jaccard('very good', 'bad')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baseline: \n",
    "# - For neutral sentiment: use the entire text\n",
    "# - For positive/negative: extract the most relevant part\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing extra spaces and quotes\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    # Remove surrounding quotes if present\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "    return text\n",
    "\n",
    "# Clean the text columns\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "train_df['selected_text'] = train_df['selected_text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Sample cleaned data:\")\n",
    "print(train_df[['text', 'selected_text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what selected_text looks like for each sentiment\n",
    "print(\"Analyzing selected_text patterns by sentiment:\")\n",
    "\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    print(f\"\\n=== {sentiment.upper()} ===\")\n",
    "    subset = train_df[train_df['sentiment'] == sentiment]\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"Examples:\")\n",
    "    for i in range(min(3, len(subset))):\n",
    "        row = subset.iloc[i]\n",
    "        print(f\"  Text: '{row['text']}'\")\n",
    "        print(f\"  Selected: '{row['selected_text']}'\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    text_lengths = subset['text'].str.len()\n",
    "    selected_lengths = subset['selected_text'].str.len()\n",
    "    \n",
    "    print(f\"Avg text length: {text_lengths.mean():.1f}\")\n",
    "    print(f\"Avg selected length: {selected_lengths.mean():.1f}\")\n",
    "    print(f\"Selected is full text: {(subset['text'] == subset['selected_text']).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple baseline model\n",
    "# For neutral: use full text\n",
    "# For positive/negative: use some simple heuristics\n",
    "\n",
    "def baseline_predict(text, sentiment):\n",
    "    \"\"\"Simple baseline prediction\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # For neutral sentiment, use the entire text\n",
    "    if sentiment == 'neutral':\n",
    "        return text\n",
    "    \n",
    "    # For positive/negative, try to extract key phrases\n",
    "    # Simple approach: look for sentiment words and extract around them\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) <= 3:\n",
    "        return text\n",
    "    \n",
    "    # For very short texts, return the whole thing\n",
    "    if len(text) < 20:\n",
    "        return text\n",
    "    \n",
    "    # Otherwise, return a reasonable subset\n",
    "    # Try to extract the first sentence or first part\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    if sentences and sentences[0].strip():\n",
    "        first_sentence = sentences[0].strip()\n",
    "        if len(first_sentence) >= 10:\n",
    "            return first_sentence\n",
    "    \n",
    "    # Fallback: return first few words\n",
    "    return ' '.join(words[:min(5, len(words))])\n",
    "\n",
    "# Test on training data\n",
    "train_df['baseline_pred'] = train_df.apply(lambda row: baseline_predict(row['text'], row['sentiment']), axis=1)\n",
    "\n",
    "# Calculate Jaccard score\n",
    "scores = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    score = jaccard(row['selected_text'], row['baseline_pred'])\n",
    "    scores.append(score)\n",
    "\n",
    "train_df['jaccard_score'] = scores\n",
    "baseline_score = np.mean(scores)\n",
    "\n",
    "print(f\"Baseline Jaccard score: {baseline_score:.4f}\")\n",
    "\n",
    "# Show some predictions vs actuals\n",
    "print(\"\\nSample predictions:\")\n",
    "sample_idx = np.random.choice(len(train_df), 5, replace=False)\n",
    "for idx in sample_idx:\n",
    "    row = train_df.iloc[idx]\n",
    "    print(f\"Sentiment: {row['sentiment']}\")\n",
    "    print(f\"Text: '{row['text']}'\")\n",
    "    print(f\"Actual: '{row['selected_text']}'\")\n",
    "    print(f\"Predicted: '{row['baseline_pred']}'\")\n",
    "    print(f\"Score: {row['jaccard_score']:.3f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "test_df['selected_text'] = test_df.apply(lambda row: baseline_predict(row['text'], row['sentiment']), axis=1)\n",
    "\n",
    "# Create submission file\n",
    "submission = test_df[['textID', 'selected_text']].copy()\n",
    "\n",
    "# Ensure the selected_text is properly quoted\n",
    "submission['selected_text'] = '\"' + submission['selected_text'].astype(str) + '\"'\n",
    "\n",
    "print(\"Sample submission:\")\n",
    "print(submission.head())\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Shape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cross-validation score for more robust evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Simple CV to get a better estimate\n",
    "def cv_score(df, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(df):\n",
    "        val_df = df.iloc[val_idx]\n",
    "        \n",
    "        # Predict on validation set\n",
    "        pred_scores = []\n",
    "        for _, row in val_df.iterrows():\n",
    "            pred = baseline_predict(row['text'], row['sentiment'])\n",
    "            score = jaccard(row['selected_text'], pred)\n",
    "            pred_scores.append(score)\n",
    "        \n",
    "        cv_scores.append(np.mean(pred_scores))\n",
    "    \n",
    "    return np.mean(cv_scores), np.std(cv_scores)\n",
    "\n",
    "cv_mean, cv_std = cv_score(train_df)\n",
    "print(f\"CV Score: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "\n",
    "# Final results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BASELINE MODEL RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Training score: {baseline_score:.4f}\")\n",
    "print(f\"CV score: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Test predictions: {len(test_df)} samples\")\n",
    "print(f\"Submission saved: /home/submission/submission.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
