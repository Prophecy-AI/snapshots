{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e252d031",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Extraction - RoBERTa Span Model\n",
    "\n",
    "This notebook implements a transformer-based approach using RoBERTa with span extraction head.\n",
    "\n",
    "## Strategy\n",
    "1. Use RoBERTa-base with start/end token classification\n",
    "2. Frame as span prediction: predict start and end positions\n",
    "3. Input: tweet text + sentiment token\n",
    "4. Target: start/end indices of selected_text\n",
    "5. Expected score: 0.70+ based on winning solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed02f203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:50:13.780487Z",
     "iopub.status.busy": "2026-01-15T11:50:13.779742Z",
     "iopub.status.idle": "2026-01-15T11:50:43.911568Z",
     "shell.execute_reply": "2026-01-15T11:50:43.910821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 11:50:30.313353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-15 11:50:31.155555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-15 11:50:31.278904: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d4a63b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T11:50:43.913957Z",
     "iopub.status.busy": "2026-01-15T11:50:43.913437Z",
     "iopub.status.idle": "2026-01-15T11:50:44.046106Z",
     "shell.execute_reply": "2026-01-15T11:50:44.045485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (27481, 4)\n",
      "Test shape: (3534, 3)\n",
      "Sample cleaned data:\n",
      "                                                text  \\\n",
      "0                I`d have responded, if I were going   \n",
      "1      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                          my boss is bullying me...   \n",
      "3                     what interview! leave me alone   \n",
      "4  Sons of ****, why couldn`t they put them on th...   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    # Remove surrounding quotes if present\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "    return text\n",
    "\n",
    "# Clean text columns\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "train_df['selected_text'] = train_df['selected_text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Sample cleaned data:\")\n",
    "print(train_df[['text', 'selected_text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Jaccard score function\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c)) if (len(a) + len(b) - len(c)) > 0 else 0.0\n",
    "\n",
    "# Create a function to find start and end character positions\n",
    "def find_start_end(text, selected_text):\n",
    "    \"\"\"Find start and end character positions of selected_text in text\"\"\"\n",
    "    if pd.isna(selected_text) or selected_text == \"\":\n",
    "        return 0, len(text)\n",
    "    \n",
    "    try:\n",
    "        start = text.index(selected_text)\n",
    "        end = start + len(selected_text)\n",
    "        return start, end\n",
    "    except:\n",
    "        # Fallback: try to find approximate match\n",
    "        words = selected_text.split()\n",
    "        if len(words) > 0:\n",
    "            for i in range(len(text) - len(words[0])):\n",
    "                if text[i:i+len(words[0])] == words[0]:\n",
    "                    return i, i + len(selected_text)\n",
    "        return 0, len(text)\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing start/end extraction:\")\n",
    "test_text = \"I love this product, it's amazing!\"\n",
    "test_selected = \"love this product\"\n",
    "start, end = find_start_end(test_text, test_selected)\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Selected: '{test_selected}'\")\n",
    "print(f\"Start: {start}, End: {end}\")\n",
    "print(f\"Extracted: '{test_text[start:end]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data with start/end positions\n",
    "print(\"Preparing training data with start/end positions...\")\n",
    "\n",
    "train_data = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    text = row['text']\n",
    "    selected_text = row['selected_text']\n",
    "    sentiment = row['sentiment']\n",
    "    \n",
    "    # Find start and end positions\n",
    "    start, end = find_start_end(text, selected_text)\n",
    "    \n",
    "    # Create input text with sentiment token\n",
    "    # Format: [SENTIMENT] text\n",
    "    input_text = f\"[{sentiment.upper()}] {text}\"\n",
    "    \n",
    "    # Adjust start/end positions for the added sentiment token\n",
    "    # Sentiment token adds len(sentiment) + 3 characters (including brackets and space)\n",
    "    offset = len(sentiment) + 3\n",
    "    start += offset\n",
    "    end += offset\n",
    "    \n",
    "    train_data.append({\n",
    "        'textID': row['textID'],\n",
    "        'text': text,\n",
    "        'input_text': input_text,\n",
    "        'selected_text': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'start': start,\n",
    "        'end': end\n",
    "    })\n",
    "\n",
    "train_processed = pd.DataFrame(train_data)\n",
    "print(f\"Processed {len(train_processed)} training samples\")\n",
    "print(\"\\nSample processed data:\")\n",
    "print(train_processed[['input_text', 'selected_text', 'start', 'end']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba277b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            row['input_text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Get start and end token positions\n",
    "        offset_mapping = encoding['offset_mapping'].squeeze().numpy()\n",
    "        \n",
    "        # Find token positions that correspond to character positions\n",
    "        start_token = 0\n",
    "        end_token = 0\n",
    "        \n",
    "        char_start = row['start']\n",
    "        char_end = row['end']\n",
    "        \n",
    "        for i, (token_start, token_end) in enumerate(offset_mapping):\n",
    "            if token_start <= char_start < token_end:\n",
    "                start_token = i\n",
    "            if token_start < char_end <= token_end:\n",
    "                end_token = i\n",
    "                break\n",
    "        \n",
    "        # If end not found, use last non-pad token\n",
    "        if end_token == 0:\n",
    "            end_token = (encoding['input_ids'].squeeze() != self.tokenizer.pad_token_id).sum().item() - 1\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'start_position': start_token,\n",
    "            'end_position': end_token,\n",
    "            'text': row['text'],\n",
    "            'selected_text': row['selected_text'],\n",
    "            'sentiment': row['sentiment']\n",
    "        }\n",
    "\n",
    "# Test the dataset\n",
    "print(\"Testing dataset...\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "test_dataset = TweetDataset(train_processed.head(5), tokenizer)\n",
    "test_item = test_dataset[0]\n",
    "print(f\"Input text: {test_item['text']}\")\n",
    "print(f\"Selected text: {test_item['selected_text']}\")\n",
    "print(f\"Start token: {test_item['start_position']}\")\n",
    "print(f\"End token: {test_item['end_position']}\")\n",
    "print(f\"Input shape: {test_item['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RoBERTa span model\n",
    "class RoBERTaSpanExtractor(nn.Module):\n",
    "    def __init__(self, model_name='roberta-base'):\n",
    "        super(RoBERTaSpanExtractor, self).__init__()\n",
    "        \n",
    "        # Load pre-trained RoBERTa\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.roberta.config.hidden_size\n",
    "        \n",
    "        # Span extraction heads\n",
    "        self.start_head = nn.Linear(self.hidden_size, 1)\n",
    "        self.end_head = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Apply dropout\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        # Predict start and end positions\n",
    "        start_logits = self.start_head(sequence_output).squeeze(-1)\n",
    "        end_logits = self.end_head(sequence_output).squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing model...\")\n",
    "model = RoBERTaSpanExtractor()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input_ids = test_item['input_ids'].unsqueeze(0)\n",
    "test_attention_mask = test_item['attention_mask'].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_logits, end_logits = model(test_input_ids, test_attention_mask)\n",
    "    print(f\"Start logits shape: {start_logits.shape}\")\n",
    "    print(f\"End logits shape: {end_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, device, epochs=3, lr=2e-5):\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Loss function\n",
    "    def span_loss(start_logits, end_logits, start_positions, end_positions):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        start_loss = loss_fct(start_logits, start_positions)\n",
    "        end_loss = loss_fct(end_logits, end_positions)\n",
    "        return (start_loss + end_loss) / 2\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_position'].to(device)\n",
    "            end_positions = batch['end_position'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            start_logits, end_logits = model(input_ids, attention_mask)\n",
    "            loss = span_loss(start_logits, end_positions, start_positions, end_positions)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                start_positions = batch['start_position'].to(device)\n",
    "                end_positions = batch['end_position'].to(device)\n",
    "                \n",
    "                start_logits, end_logits = model(input_ids, attention_mask)\n",
    "                loss = span_loss(start_logits, end_logits, start_positions, end_positions)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), '/home/code/experiments/002_roberta_span/best_model.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prediction function\n",
    "def predict_span(model, tokenizer, text, sentiment, device, max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = f\"[{sentiment.upper()}] {text}\"\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt',\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    offset_mapping = encoding['offset_mapping'].squeeze().numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Get predicted start and end positions\n",
    "        start_pred = torch.argmax(start_logits, dim=1).item()\n",
    "        end_pred = torch.argmax(end_logits, dim=1).item()\n",
    "        \n",
    "        # Ensure end >= start\n",
    "        if end_pred < start_pred:\n",
    "            end_pred = start_pred\n",
    "        \n",
    "        # Extract text span using offset mapping\n",
    "        char_start = offset_mapping[start_pred][0]\n",
    "        char_end = offset_mapping[end_pred][1]\n",
    "        \n",
    "        # Adjust for sentiment token offset\n",
    "        offset = len(sentiment) + 3\n",
    "        char_start = max(0, char_start - offset)\n",
    "        char_end = max(0, char_end - offset)\n",
    "        \n",
    "        # Extract the predicted span\n",
    "        predicted_span = text[char_start:char_end]\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if predicted_span.strip() == \"\":\n",
    "            predicted_span = text\n",
    "        \n",
    "        return predicted_span\n",
    "\n",
    "print(\"Functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation training\n",
    "print(\"Starting 5-fold cross-validation...\")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "fold_predictions = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_processed)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/5\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_fold = train_processed.iloc[train_idx]\n",
    "    val_fold = train_processed.iloc[val_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TweetDataset(train_fold, tokenizer)\n",
    "    val_dataset = TweetDataset(val_fold, tokenizer)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = RoBERTaSpanExtractor()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "    model = train_model(model, train_loader, val_loader, device, epochs=3)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('/home/code/experiments/002_roberta_span/best_model.pt'))\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"Evaluating fold {fold + 1}...\")\n",
    "    fold_scores = []\n",
    "    fold_preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Get predictions for each sample in batch\n",
    "            for i in range(len(batch['text'])):\n",
    "                text = batch['text'][i]\n",
    "                sentiment = batch['sentiment'][i]\n",
    "                true_selected = batch['selected_text'][i]\n",
    "                \n",
    "                # Predict\n",
    "                pred_selected = predict_span(model, tokenizer, text, sentiment, device)\n",
    "                \n",
    "                # Calculate Jaccard score\n",
    "                score = jaccard(true_selected, pred_selected)\n",
    "                fold_scores.append(score)\n",
    "                \n",
    "                # Store predictions\n",
    "                fold_preds.append({\n",
    "                    'textID': batch['textID'][i] if 'textID' in batch else '',\n",
    "                    'text': text,\n",
    "                    'sentiment': sentiment,\n",
    "                    'true_selected': true_selected,\n",
    "                    'pred_selected': pred_selected,\n",
    "                    'jaccard': score\n",
    "                })\n",
    "    \n",
    "    fold_mean_score = np.mean(fold_scores)\n",
    "    cv_scores.append(fold_mean_score)\n",
    "    fold_predictions.extend(fold_preds)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Jaccard score: {fold_mean_score:.4f}\")\n",
    "\n",
    "# Overall CV score\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"CV Score: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d53b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fold predictions for analysis\n",
    "import json\n",
    "\n",
    "fold_preds_df = pd.DataFrame(fold_predictions)\n",
    "fold_preds_df.to_csv('/home/code/experiments/002_roberta_span/fold_predictions.csv', index=False)\n",
    "\n",
    "# Show some sample predictions\n",
    "print(\"\\nSample predictions from validation:\")\n",
    "sample_preds = fold_preds_df.sample(5, random_state=42)\n",
    "for idx, row in sample_preds.iterrows():\n",
    "    print(f\"\\nSentiment: {row['sentiment']}\")\n",
    "    print(f\"Text: '{row['text']}'\")\n",
    "    print(f\"True: '{row['true_selected']}'\")\n",
    "    print(f\"Pred: '{row['pred_selected']}'\")\n",
    "    print(f\"Score: {row['jaccard']:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training data\n",
    "print(\"Training final model on full training data...\")\n",
    "\n",
    "# Create full dataset\n",
    "train_dataset_full = TweetDataset(train_processed, tokenizer)\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize and train final model\n",
    "final_model = RoBERTaSpanExtractor()\n",
    "final_model = train_model(final_model, train_loader_full, train_loader_full, device, epochs=3)\n",
    "\n",
    "# Save final model\n",
    "torch.save(final_model.state_dict(), '/home/code/experiments/002_roberta_span/final_model.pt')\n",
    "print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    text = row['text']\n",
    "    sentiment = row['sentiment']\n",
    "    textID = row['textID']\n",
    "    \n",
    "    # Predict using the final model\n",
    "    pred_selected = predict_span(final_model, tokenizer, text, sentiment, device)\n",
    "    \n",
    "    test_predictions.append({\n",
    "        'textID': textID,\n",
    "        'selected_text': pred_selected\n",
    "    })\n",
    "    \n",
    "    if idx % 500 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)} samples\")\n",
    "\n",
    "# Create submission\n",
    "test_preds_df = pd.DataFrame(test_predictions)\n",
    "\n",
    "# Ensure proper formatting (quoted text)\n",
    "test_preds_df['selected_text'] = '\"' + test_preds_df['selected_text'].astype(str) + '\"'\n",
    "\n",
    "print(\"\\nSample test predictions:\")\n",
    "print(test_preds_df.head())\n",
    "\n",
    "# Save submission\n",
    "test_preds_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Shape: {test_preds_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results summary\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"ROBERTA SPAN MODEL RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"CV Score: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "print(f\"Baseline score: 0.5481\")\n",
    "print(f\"Improvement: +{cv_mean - 0.5481:.4f}\")\n",
    "print(f\"Test predictions: {len(test_preds_df)} samples\")\n",
    "print(f\"Submission saved: /home/submission/submission.csv\")\n",
    "\n",
    "# Performance by sentiment\n",
    "print(f\"\\nPerformance by sentiment (from validation):\")\n",
    "sentiment_perf = fold_preds_df.groupby('sentiment')['jaccard'].agg(['mean', 'count'])\n",
    "print(sentiment_perf)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
