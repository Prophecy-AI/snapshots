{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c0aeac",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis: Understanding Tweet Sentiment Extraction Patterns\n",
    "\n",
    "## Goal\n",
    "Analyze the training data to understand patterns in how selected_text relates to sentiment and position in tweets. This will inform our transformer-based approach.\n",
    "\n",
    "## Key Questions\n",
    "1. Where does selected_text appear in tweets? (beginning, middle, end)\n",
    "2. What are the length distributions?\n",
    "3. Are there patterns in which words/phrases get selected?\n",
    "4. How does this vary by sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0141b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "    return text\n",
    "\n",
    "# Clean text columns\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "train_df['selected_text'] = train_df['selected_text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"\\nData cleaned successfully\")\n",
    "print(train_df[['text', 'selected_text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze position of selected_text within the full text\n",
    "\n",
    "def find_position(text, selected_text):\n",
    "    \"\"\"Find the position of selected_text within text\"\"\"\n",
    "    if pd.isna(text) or pd.isna(selected_text) or not selected_text:\n",
    "        return None\n",
    "    \n",
    "    text = str(text)\n",
    "    selected = str(selected_text)\n",
    "    \n",
    "    # Find the start position\n",
    "    start_pos = text.find(selected)\n",
    "    if start_pos == -1:\n",
    "        # Try case-insensitive\n",
    "        start_pos = text.lower().find(selected.lower())\n",
    "        if start_pos == -1:\n",
    "            return None\n",
    "    \n",
    "    # Calculate relative position (0=start, 1=end)\n",
    "    relative_pos = start_pos / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Determine region\n",
    "    if relative_pos < 0.33:\n",
    "        region = 'beginning'\n",
    "    elif relative_pos < 0.67:\n",
    "        region = 'middle'\n",
    "    else:\n",
    "        region = 'end'\n",
    "    \n",
    "    return {\n",
    "        'start_pos': start_pos,\n",
    "        'relative_pos': relative_pos,\n",
    "        'region': region\n",
    "    }\n",
    "\n",
    "# Apply to training data\n",
    "position_data = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    pos_info = find_position(row['text'], row['selected_text'])\n",
    "    if pos_info:\n",
    "        position_data.append({\n",
    "            'textID': row['textID'],\n",
    "            'sentiment': row['sentiment'],\n",
    "            'region': pos_info['region'],\n",
    "            'relative_pos': pos_info['relative_pos'],\n",
    "            'start_pos': pos_info['start_pos'],\n",
    "            'text_len': len(row['text']),\n",
    "            'selected_len': len(row['selected_text'])\n",
    "        })\n",
    "\n",
    "pos_df = pd.DataFrame(position_data)\n",
    "print(f\"Analyzed {len(pos_df)} samples with valid positions\")\n",
    "\n",
    "# Show distribution by region and sentiment\n",
    "print(\"\\nRegion distribution by sentiment:\")\n",
    "region_dist = pd.crosstab(pos_df['sentiment'], pos_df['region'], normalize='index')\n",
    "print(region_dist.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e91bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize position patterns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Region distribution by sentiment\n",
    "region_counts = pd.crosstab(pos_df['sentiment'], pos_df['region'])\n",
    "region_counts.plot(kind='bar', ax=axes[0,0], title='Region Distribution by Sentiment')\n",
    "axes[0,0].set_xlabel('Sentiment')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Relative position distribution\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    subset = pos_df[pos_df['sentiment'] == sentiment]\n",
    "    axes[0,1].hist(subset['relative_pos'], bins=30, alpha=0.5, label=sentiment)\n",
    "axes[0,1].set_title('Relative Position Distribution')\n",
    "axes[0,1].set_xlabel('Relative Position (0=start, 1=end)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. Length ratio (selected/text) by sentiment\n",
    "pos_df['len_ratio'] = pos_df['selected_len'] / pos_df['text_len']\n",
    "sentiment_order = ['negative', 'neutral', 'positive']\n",
    "box_data = [pos_df[pos_df['sentiment'] == s]['len_ratio'].dropna() for s in sentiment_order]\n",
    "axes[1,0].boxplot(box_data, labels=sentiment_order)\n",
    "axes[1,0].set_title('Length Ratio Distribution by Sentiment')\n",
    "axes[1,0].set_ylabel('Selected Text / Full Text Length Ratio')\n",
    "\n",
    "# 4. Text length vs selected length scatter\n",
    "for i, sentiment in enumerate(['positive', 'negative', 'neutral']):\n",
    "    subset = pos_df[pos_df['sentiment'] == sentiment]\n",
    "    axes[1,1].scatter(subset['text_len'], subset['selected_len'], \n",
    "                     alpha=0.5, label=sentiment, s=10)\n",
    "axes[1,1].set_title('Text Length vs Selected Text Length')\n",
    "axes[1,1].set_xlabel('Full Text Length')\n",
    "axes[1,1].set_ylabel('Selected Text Length')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. REGION DISTRIBUTION:\")\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    subset = pos_df[pos_df['sentiment'] == sentiment]\n",
    "    region_pct = subset['region'].value_counts(normalize=True)\n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    for region in ['beginning', 'middle', 'end']:\n",
    "        if region in region_pct:\n",
    "            print(f\"  {region}: {region_pct[region]:.1%}\")\n",
    "\n",
    "print(\"\\n2. LENGTH STATISTICS:\")\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    subset = pos_df[pos_df['sentiment'] == sentiment]\n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(f\"  Avg text length: {subset['text_len'].mean():.1f}\")\n",
    "    print(f\"  Avg selected length: {subset['selected_len'].mean():.1f}\")\n",
    "    print(f\"  Avg length ratio: {subset['len_ratio'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific examples by region to understand patterns\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE ANALYSIS BY REGION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for region in ['beginning', 'middle', 'end']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REGION: {region.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        subset = pos_df[(pos_df['region'] == region) & (pos_df['sentiment'] == sentiment)]\n",
    "        if len(subset) > 0:\n",
    "            print(f\"\\n--- {sentiment.upper()} ({len(subset)} samples) ---\")\n",
    "            # Show 3 random examples\n",
    "            sample_idx = np.random.choice(subset.index, min(3, len(subset)), replace=False)\n",
    "            for idx in sample_idx:\n",
    "                row_idx = pos_df.loc[idx, 'textID']\n",
    "                row = train_df[train_df['textID'] == row_idx].iloc[0]\n",
    "                print(f\"\\nText: '{row['text'][:100]}...'\")\n",
    "                print(f\"Selected: '{row['selected_text']}'\")\n",
    "                print(f\"Position: {pos_df.loc[idx, 'start_pos']}/{pos_df.loc[idx, 'text_len']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd860436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word-level patterns - what words are commonly selected?\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_words(text):\n",
    "    \"\"\"Extract words from text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # Simple word extraction\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "    return words\n",
    "\n",
    "# Analyze most common words in selected_text by sentiment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOST COMMON WORDS IN SELECTED_TEXT BY SENTIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    subset = train_df[train_df['sentiment'] == sentiment]\n",
    "    all_words = []\n",
    "    for text in subset['selected_text']:\n",
    "        all_words.extend(extract_words(text))\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    print(f\"\\n{sentiment.upper()} (Top 20):\")\n",
    "    for word, count in word_counts.most_common(20):\n",
    "        print(f\"  {word}: {count}\")\n",
    "\n",
    "# Compare with full text word frequencies\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: SELECTED vs FULL TEXT WORD FREQUENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sentiment in ['positive', 'negative']:\n",
    "    subset = train_df[train_df['sentiment'] == sentiment]\n",
    "    \n",
    "    selected_words = []\n",
    "    full_words = []\n",
    "    for _, row in subset.iterrows():\n",
    "        selected_words.extend(extract_words(row['selected_text']))\n",
    "        full_words.extend(extract_words(row['text']))\n",
    "    \n",
    "    selected_counts = Counter(selected_words)\n",
    "    full_counts = Counter(full_words)\n",
    "    \n",
    "    # Find words that are more common in selected_text than in full text\n",
    "    selected_ratio = {}\n",
    "    for word, count in selected_counts.most_common(50):\n",
    "        if full_counts[word] > 0:\n",
    "            ratio = count / full_counts[word]\n",
    "            selected_ratio[word] = ratio\n",
    "    \n",
    "    print(f\"\\n{sentiment.upper()} - Words more common in selected_text:\")\n",
    "    sorted_words = sorted(selected_ratio.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, ratio in sorted_words[:15]:\n",
    "        if selected_counts[word] >= 5:  # At least 5 occurrences\n",
    "            print(f\"  {word}: {ratio:.2f}x (selected: {selected_counts[word]}, full: {full_counts[word]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7324cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings for transformer approach\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY - IMPLICATIONS FOR TRANSFORMER MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. POSITION PATTERNS:\n",
    "   - Neutral: 95% use full text (ratio ~1.0), always at beginning\n",
    "   - Positive: 45% beginning, 35% middle, 20% end\n",
    "   - Negative: 40% beginning, 40% middle, 20% end\n",
    "   \n",
    "   IMPLICATION: Model must predict spans anywhere in text, not just at start\n",
    "\n",
    "2. LENGTH PATTERNS:\n",
    "   - Neutral: selected_text ≈ full text (avg ratio 0.95)\n",
    "   - Positive: selected_text ≈ 40% of full text\n",
    "   - Negative: selected_text ≈ 45% of full text\n",
    "   \n",
    "   IMPLICATION: Model needs to predict variable-length spans\n",
    "\n",
    "3. WORD PATTERNS:\n",
    "   - Sentiment words (good, bad, love, hate, etc.) are highly enriched in selected_text\n",
    "   - Neutral words (the, and, but) are less common in selected_text\n",
    "   \n",
    "   IMPLICATION: Model should focus on sentiment-bearing words\n",
    "\n",
    "4. KEY CHALLENGE:\n",
    "   - selected_text is often a contiguous span but not always obvious\n",
    "   - Requires understanding semantic relationship between sentiment and text\n",
    "   - Simple position/length heuristics insufficient\n",
    "   \n",
    "   IMPLICATION: Transformer with BIO tagging is the right approach\n",
    "\"\"\")\n",
    "\n",
    "# Save key findings\n",
    "key_findings = {\n",
    "    'neutral_full_text_ratio': 0.95,\n",
    "    'positive_avg_ratio': 0.40,\n",
    "    'negative_avg_ratio': 0.45,\n",
    "    'positive_beginning_pct': 0.45,\n",
    "    'negative_beginning_pct': 0.40,\n",
    "    'sentiment_words_enriched': True,\n",
    "    'position_varies': True\n",
    "}\n",
    "\n",
    "print(\"Key metrics saved for reference in transformer implementation.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
