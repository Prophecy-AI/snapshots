{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740e33ac",
   "metadata": {},
   "source": [
    "# Experiment 004: Character-Level WaveNet Model\n",
    "\n",
    "**Objective**: Implement character-level WaveNet refinement to improve span boundary detection\n",
    "\n",
    "**Expected gain**: +0.015 to +0.025 points\n",
    "**Priority**: HIGHEST (addresses core weakness in boundary detection)\n",
    "\n",
    "**Approach**:\n",
    "1. Modify RoBERTa inference to save token start/end probability distributions\n",
    "2. Convert token probabilities to character-level probabilities using offset_mapping\n",
    "3. Build WaveNet with dilated convolutions for smoothing\n",
    "4. Train on 5-fold CV using character-level probabilities as features\n",
    "5. Generate refined predictions with better boundary detection\n",
    "\n",
    "**Reference**: Winning solution by Theo Viel (dark-of-the-moon) - Character-level WaveNet was key innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4d9a9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:24.787554Z",
     "iopub.status.busy": "2026-01-16T00:52:24.786805Z",
     "iopub.status.idle": "2026-01-16T00:52:24.794779Z",
     "shell.execute_reply": "2026-01-16T00:52:24.793992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Define Jaccard similarity metric\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Calculate Jaccard similarity between two strings.\"\"\"\n",
    "    if pd.isna(str1) or pd.isna(str2):\n",
    "        return 0.0\n",
    "    \n",
    "    str1, str2 = str(str1), str(str2)\n",
    "    \n",
    "    # Tokenize by splitting on whitespace\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    \n",
    "    # Handle empty sets\n",
    "    if len(a) == 0 and len(b) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ceabd10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:24.796677Z",
     "iopub.status.busy": "2026-01-16T00:52:24.796508Z",
     "iopub.status.idle": "2026-01-16T00:52:24.882983Z",
     "shell.execute_reply": "2026-01-16T00:52:24.882383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27481 training samples\n",
      "Columns: ['textID', 'text', 'selected_text', 'sentiment']\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample row:\n",
      "textID                                     cb774db0d1\n",
      "text              I`d have responded, if I were going\n",
      "selected_text     I`d have responded, if I were going\n",
      "sentiment                                     neutral\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_path = Path('/home/data/train.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample row:\")\n",
    "print(train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c377d347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:24.884752Z",
     "iopub.status.busy": "2026-01-16T00:52:24.884571Z",
     "iopub.status.idle": "2026-01-16T00:52:25.407025Z",
     "shell.execute_reply": "2026-01-16T00:52:25.406494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n",
      "\n",
      "Original text: ' I`d have responded, if I were going'\n",
      "Selected text: 'I`d have responded, if I were going'\n",
      "\n",
      "Token IDs shape: torch.Size([1, 12])\n",
      "Offset mapping shape: torch.Size([1, 12, 2])\n",
      "\n",
      "First 10 tokens and their offsets:\n",
      "  0: '<s>' -> offsets: tensor([0, 0])\n",
      "  1: ' I' -> offsets: tensor([1, 2])\n",
      "  2: '`' -> offsets: tensor([2, 3])\n",
      "  3: 'd' -> offsets: tensor([3, 4])\n",
      "  4: ' have' -> offsets: tensor([5, 9])\n",
      "  5: ' responded' -> offsets: tensor([10, 19])\n",
      "  6: ',' -> offsets: tensor([19, 20])\n",
      "  7: ' if' -> offsets: tensor([21, 23])\n",
      "  8: ' I' -> offsets: tensor([24, 25])\n",
      "  9: ' were' -> offsets: tensor([26, 30])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "print(f\"Tokenizer loaded: {tokenizer.name_or_path}\")\n",
    "\n",
    "# Test tokenization with offset mapping\n",
    "text = train_df.iloc[0]['text']\n",
    "selected_text = train_df.iloc[0]['selected_text']\n",
    "\n",
    "print(f\"\\nOriginal text: '{text}'\")\n",
    "print(f\"Selected text: '{selected_text}'\")\n",
    "\n",
    "# Tokenize with offset mapping\n",
    "encoding = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"\\nToken IDs shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"Offset mapping shape: {encoding['offset_mapping'].shape}\")\n",
    "print(f\"\\nFirst 10 tokens and their offsets:\")\n",
    "for i in range(min(10, len(encoding['input_ids'][0]))):\n",
    "    token_id = encoding['input_ids'][0][i].item()\n",
    "    token = tokenizer.decode([token_id])\n",
    "    offsets = encoding['offset_mapping'][0][i]\n",
    "    print(f\"  {i}: '{token}' -> offsets: {offsets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "313a3afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:25.410759Z",
     "iopub.status.busy": "2026-01-16T00:52:25.410328Z",
     "iopub.status.idle": "2026-01-16T00:52:25.651358Z",
     "shell.execute_reply": "2026-01-16T00:52:25.650620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing token probability extraction...\n",
      "Loaded pretrained RoBERTa for testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start probs shape: torch.Size([14])\n",
      "End probs shape: torch.Size([14])\n",
      "Offset mapping shape: torch.Size([14, 2])\n",
      "First 5 start probabilities: tensor([0.0778, 0.0857, 0.0822, 0.0617, 0.0889])\n",
      "Token probability extraction test completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Function to extract token start/end probabilities from RoBERTa\n",
    "@torch.no_grad()\n",
    "def extract_token_probabilities(model, text, sentiment, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Extract token-level start and end probability distributions from RoBERTa.\n",
    "    Returns probabilities for each token position.\n",
    "    \"\"\"\n",
    "    # Prepare input with sentiment token\n",
    "    input_text = f\"{sentiment} {text}\"\n",
    "    encoding = tokenizer(\n",
    "        input_text,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    offset_mapping = encoding['offset_mapping'][0]  # Remove batch dim\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    start_logits = outputs.start_logits[0]  # [seq_len]\n",
    "    end_logits = outputs.end_logits[0]      # [seq_len]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    start_probs = F.softmax(start_logits, dim=-1)\n",
    "    end_probs = F.softmax(end_logits, dim=-1)\n",
    "    \n",
    "    return start_probs, end_probs, offset_mapping\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing token probability extraction...\")\n",
    "test_text = train_df.iloc[0]['text']\n",
    "test_sentiment = train_df.iloc[0]['sentiment']\n",
    "\n",
    "# Load a simple model for testing\n",
    "try:\n",
    "    roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "    print(\"Loaded pretrained RoBERTa for testing\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    roberta_model = None\n",
    "\n",
    "if roberta_model is not None:\n",
    "    start_probs, end_probs, offset_mapping = extract_token_probabilities(\n",
    "        roberta_model, test_text, test_sentiment, tokenizer, device='cpu'\n",
    "    )\n",
    "    print(f\"Start probs shape: {start_probs.shape}\")\n",
    "    print(f\"End probs shape: {end_probs.shape}\")\n",
    "    print(f\"Offset mapping shape: {offset_mapping.shape}\")\n",
    "    print(f\"First 5 start probabilities: {start_probs[:5]}\")\n",
    "    print(\"Token probability extraction test completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96b8ded8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:25.657035Z",
     "iopub.status.busy": "2026-01-16T00:52:25.656452Z",
     "iopub.status.idle": "2026-01-16T00:52:25.722532Z",
     "shell.execute_reply": "2026-01-16T00:52:25.721774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing token to character probability conversion...\n",
      "Token probs type: <class 'torch.Tensor'>\n",
      "Offset mapping type: <class 'torch.Tensor'>\n",
      "Character-level probabilities shape: (36,)\n",
      "Conversion test completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Function to convert token probabilities to character probabilities\n",
    "def token_to_char_probabilities(token_probs, offset_mapping, text_length):\n",
    "    \"\"\"\n",
    "    Convert token-level probabilities to character-level probabilities.\n",
    "    \n",
    "    Args:\n",
    "        token_probs: [seq_len] array of token probabilities\n",
    "        offset_mapping: [seq_len, 2] array of (start_char, end_char) offsets\n",
    "        text_length: Length of original text\n",
    "        \n",
    "    Returns:\n",
    "        char_probs: [text_length] array of character-level probabilities\n",
    "    \"\"\"\n",
    "    char_probs = np.zeros(text_length)\n",
    "    char_counts = np.zeros(text_length)\n",
    "    \n",
    "    # Convert token_probs to numpy if it's a tensor\n",
    "    if isinstance(token_probs, torch.Tensor):\n",
    "        token_probs = token_probs.cpu().numpy()\n",
    "    \n",
    "    # Convert offset_mapping to numpy if it's a tensor\n",
    "    if isinstance(offset_mapping, torch.Tensor):\n",
    "        offset_mapping = offset_mapping.cpu().numpy()\n",
    "    \n",
    "    # Aggregate probabilities from tokens to characters\n",
    "    for token_idx, (char_start, char_end) in enumerate(offset_mapping):\n",
    "        # Skip special tokens (offset = (0, 0))\n",
    "        if char_start == 0 and char_end == 0:\n",
    "            continue\n",
    "            \n",
    "        # Ensure offsets are within bounds\n",
    "        char_start = max(0, min(char_start, text_length - 1))\n",
    "        char_end = max(0, min(char_end, text_length - 1))\n",
    "        \n",
    "        if char_start < char_end:\n",
    "            # Distribute token probability across characters\n",
    "            num_chars = char_end - char_start\n",
    "            char_probs[char_start:char_end] += token_probs[token_idx] / num_chars\n",
    "            char_counts[char_start:char_end] += 1\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    char_counts = np.maximum(char_counts, 1)\n",
    "    char_probs = char_probs / char_counts\n",
    "    \n",
    "    return char_probs\n",
    "\n",
    "# Simple test without complex printing\n",
    "print(\"Testing token to character probability conversion...\")\n",
    "test_start_probs, test_end_probs, test_offset_mapping = extract_token_probabilities(\n",
    "    roberta_model, test_text, test_sentiment, tokenizer, device='cpu'\n",
    ")\n",
    "\n",
    "print(f\"Token probs type: {type(test_start_probs)}\")\n",
    "print(f\"Offset mapping type: {type(test_offset_mapping)}\")\n",
    "\n",
    "char_start_probs = token_to_char_probabilities(\n",
    "    test_start_probs, test_offset_mapping, len(test_text)\n",
    ")\n",
    "\n",
    "print(f\"Character-level probabilities shape: {char_start_probs.shape}\")\n",
    "print(\"Conversion test completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f0e78c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:25.724965Z",
     "iopub.status.busy": "2026-01-16T00:52:25.724700Z",
     "iopub.status.idle": "2026-01-16T00:52:25.756674Z",
     "shell.execute_reply": "2026-01-16T00:52:25.755941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing WaveNet architecture...\n",
      "Input shape: torch.Size([2, 2, 100])\n",
      "Output shape: torch.Size([2, 2, 100])\n",
      "Output sum (should be 1.0 per position): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# WaveNet architecture for character-level probability refinement\n",
    "class CharacterWaveNet(nn.Module):\n",
    "    def __init__(self, input_channels=2, num_classes=2, num_blocks=4, num_layers=6, \n",
    "                 residual_channels=32, gate_channels=32, skip_channels=32):\n",
    "        \"\"\"\n",
    "        WaveNet for character-level span prediction refinement.\n",
    "        \n",
    "        Args:\n",
    "            input_channels: Number of input channels (start_prob, end_prob)\n",
    "            num_classes: Number of output classes (refined_start, refined_end)\n",
    "            num_blocks: Number of residual blocks\n",
    "            num_layers: Number of layers per block (dilated convolutions)\n",
    "            residual_channels: Channels in residual connections\n",
    "            gate_channels: Channels in gated activation units\n",
    "            skip_channels: Channels in skip connections\n",
    "        \"\"\"\n",
    "        super(CharacterWaveNet, self).__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Conv1d(input_channels, residual_channels, 1)\n",
    "        \n",
    "        # Dilated convolution layers\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "        # Build dilated convolution blocks\n",
    "        for b in range(num_blocks):\n",
    "            for l in range(num_layers):\n",
    "                # Dilation doubles each layer: 1, 2, 4, 8, 16, 32, ...\n",
    "                dilation = 2 ** l\n",
    "                \n",
    "                # Gated activation unit (dilated convolution)\n",
    "                self.dilated_convs.append(\n",
    "                    nn.Conv1d(residual_channels, gate_channels, kernel_size=3, \n",
    "                             padding=dilation, dilation=dilation)\n",
    "                )\n",
    "                \n",
    "                # 1x1 conv for residual connection\n",
    "                self.residual_convs.append(\n",
    "                    nn.Conv1d(gate_channels // 2, residual_channels, 1)\n",
    "                )\n",
    "                \n",
    "                # 1x1 conv for skip connection\n",
    "                self.skip_convs.append(\n",
    "                    nn.Conv1d(gate_channels // 2, skip_channels, 1)\n",
    "                )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        self.output_conv2 = nn.Conv1d(skip_channels, num_classes, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, input_channels, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            output: Output tensor [batch, num_classes, seq_len]\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)  # [batch, residual_channels, seq_len]\n",
    "        \n",
    "        # Skip connections accumulator\n",
    "        skip_connections = []\n",
    "        \n",
    "        # Apply dilated convolutions\n",
    "        layer_idx = 0\n",
    "        for b in range(self.num_blocks):\n",
    "            for l in range(self.num_layers):\n",
    "                # Gated activation: tanh(x) * sigmoid(x)\n",
    "                dilated_out = self.dilated_convs[layer_idx](x)\n",
    "                \n",
    "                # Split into two parts for gating\n",
    "                tanh_out = self.tanh(dilated_out[:, :dilated_out.shape[1]//2, :])\n",
    "                sigmoid_out = self.sigmoid(dilated_out[:, dilated_out.shape[1]//2:, :])\n",
    "                \n",
    "                gated_out = tanh_out * sigmoid_out\n",
    "                \n",
    "                # Residual connection\n",
    "                residual_out = self.residual_convs[layer_idx](gated_out)\n",
    "                x = x + residual_out\n",
    "                \n",
    "                # Skip connection\n",
    "                skip_out = self.skip_convs[layer_idx](gated_out)\n",
    "                skip_connections.append(skip_out)\n",
    "                \n",
    "                layer_idx += 1\n",
    "        \n",
    "        # Sum all skip connections\n",
    "        skip_sum = sum(skip_connections)  # [batch, skip_channels, seq_len]\n",
    "        \n",
    "        # Output layers\n",
    "        output = self.relu(skip_sum)\n",
    "        output = self.output_conv1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.output_conv2(output)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        output = F.softmax(output, dim=1)  # [batch, num_classes, seq_len]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test the WaveNet architecture\n",
    "print(\"Testing WaveNet architecture...\")\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 100\n",
    "input_channels = 2\n",
    "\n",
    "# Create dummy input (character-level start/end probabilities)\n",
    "dummy_input = torch.randn(batch_size, input_channels, seq_len)\n",
    "\n",
    "# Initialize WaveNet model (use different variable name to avoid confusion)\n",
    "wavenet_model = CharacterWaveNet(\n",
    "    input_channels=input_channels,\n",
    "    num_classes=2,\n",
    "    num_blocks=2,  # Reduced for testing\n",
    "    num_layers=4,   # Reduced for testing\n",
    "    residual_channels=16,\n",
    "    gate_channels=16,\n",
    "    skip_channels=16\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = wavenet_model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output sum (should be 1.0 per position): {output[0, :, 0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96404ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:26.267188Z",
     "iopub.status.busy": "2026-01-16T00:52:26.266949Z",
     "iopub.status.idle": "2026-01-16T00:52:26.525856Z",
     "shell.execute_reply": "2026-01-16T00:52:26.525189Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset creation...\n",
      "No trained model found, using pretrained RoBERTa\n",
      "Dataset created with 100 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features shape: torch.Size([2, 36])\n",
      "Sample target shape: torch.Size([2, 36])\n",
      "Sample text:  I`d have responded, if I were going...\n",
      "Sample selected_text: I`d have responded, if I were going\n"
     ]
    }
   ],
   "source": [
    "# Dataset for character-level training\n",
    "class CharacterLevelDataset(Dataset):\n",
    "    def __init__(self, texts, sentiments, selected_texts, roberta_model, tokenizer, device='cpu'):\n",
    "        \"\"\"\n",
    "        Dataset for character-level WaveNet training.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of tweet texts\n",
    "            sentiments: List of sentiment labels\n",
    "            selected_texts: List of selected_text spans (targets)\n",
    "            roberta_model: Trained RoBERTa model for generating token probabilities\n",
    "            tokenizer: Tokenizer for text processing\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.sentiments = sentiments\n",
    "        self.selected_texts = selected_texts\n",
    "        self.roberta_model = roberta_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        selected_text = self.selected_texts[idx]\n",
    "        \n",
    "        # Extract token probabilities from RoBERTa\n",
    "        start_probs, end_probs, offset_mapping = extract_token_probabilities(\n",
    "            self.roberta_model, text, sentiment, self.tokenizer, self.device\n",
    "        )\n",
    "        \n",
    "        # Convert to character probabilities\n",
    "        char_start_probs = token_to_char_probabilities(\n",
    "            start_probs.cpu().numpy(), offset_mapping, len(text)\n",
    "        )\n",
    "        char_end_probs = token_to_char_probabilities(\n",
    "            end_probs.cpu().numpy(), offset_mapping, len(text)\n",
    "        )\n",
    "        \n",
    "        # Create input features [2, text_length]\n",
    "        char_features = np.stack([char_start_probs, char_end_probs], axis=0)\n",
    "        \n",
    "        # Create target labels (binary masks for start/end positions)\n",
    "        start_target = np.zeros(len(text))\n",
    "        end_target = np.zeros(len(text))\n",
    "        \n",
    "        if sentiment != 'neutral':\n",
    "            # Find selected text boundaries\n",
    "            try:\n",
    "                start_idx = text.index(selected_text)\n",
    "                end_idx = start_idx + len(selected_text) - 1\n",
    "                start_target[start_idx] = 1.0\n",
    "                end_target[end_idx] = 1.0\n",
    "            except:\n",
    "                # Handle cases where selected_text is not found\n",
    "                pass\n",
    "        \n",
    "        target = np.stack([start_target, end_target], axis=0)\n",
    "        \n",
    "        return {\n",
    "            'features': torch.FloatTensor(char_features),\n",
    "            'target': torch.FloatTensor(target),\n",
    "            'text': text,\n",
    "            'selected_text': selected_text\n",
    "        }\n",
    "\n",
    "# Test dataset creation\n",
    "print(\"Testing dataset creation...\")\n",
    "\n",
    "# Load a trained RoBERTa model if available\n",
    "try:\n",
    "    # Try to load the model from experiment 002\n",
    "    model_path = Path('/home/code/experiments/002_roberta_span/fold_0_roberta_model.pt')\n",
    "    if model_path.exists():\n",
    "        roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        roberta_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        roberta_model.eval()\n",
    "        print(f\"Loaded trained RoBERTa model from {model_path}\")\n",
    "    else:\n",
    "        print(\"No trained model found, using pretrained RoBERTa\")\n",
    "        roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "\n",
    "# Create small test dataset\n",
    "test_texts = train_df['text'].iloc[:100].tolist()\n",
    "test_sentiments = train_df['sentiment'].iloc[:100].tolist()\n",
    "test_selected = train_df['selected_text'].iloc[:100].tolist()\n",
    "\n",
    "test_dataset = CharacterLevelDataset(\n",
    "    test_texts, test_sentiments, test_selected, roberta_model, tokenizer, device='cpu'\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(test_dataset)} samples\")\n",
    "sample = test_dataset[0]\n",
    "print(f\"Sample features shape: {sample['features'].shape}\")\n",
    "print(f\"Sample target shape: {sample['target'].shape}\")\n",
    "print(f\"Sample text: {sample['text'][:50]}...\")\n",
    "print(f\"Sample selected_text: {sample['selected_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d764a49d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:52:26.529727Z",
     "iopub.status.busy": "2026-01-16T00:52:26.529539Z",
     "iopub.status.idle": "2026-01-16T00:52:26.542172Z",
     "shell.execute_reply": "2026-01-16T00:52:26.541646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and prediction functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Training function for WaveNet\n",
    "def train_wavenet_model(model, train_loader, val_loader, device, epochs=10, lr=0.001):\n",
    "    \"\"\"Train WaveNet model on character-level features.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            features = batch['features'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                features = batch['features'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_wavenet_model.pt')\n",
    "            print(f\"  Saved best model with val_loss = {val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to generate predictions with WaveNet refinement\n",
    "def generate_wavenet_predictions(model, texts, sentiments, roberta_model, tokenizer, device='cpu'):\n",
    "    \"\"\"Generate refined predictions using WaveNet.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(texts)), desc=\"Generating predictions\"):\n",
    "            text = texts[i]\n",
    "            sentiment = sentiments[i]\n",
    "            \n",
    "            if sentiment == 'neutral':\n",
    "                # For neutral sentiment, predict the entire text\n",
    "                predictions.append(text)\n",
    "                continue\n",
    "            \n",
    "            # Extract token probabilities from RoBERTa\n",
    "            start_probs, end_probs, offset_mapping = extract_token_probabilities(\n",
    "                roberta_model, text, sentiment, tokenizer, device\n",
    "            )\n",
    "            \n",
    "            # Convert to character probabilities\n",
    "            char_start_probs = token_to_char_probabilities(\n",
    "                start_probs.cpu().numpy(), offset_mapping, len(text)\n",
    "            )\n",
    "            char_end_probs = token_to_char_probabilities(\n",
    "                end_probs.cpu().numpy(), offset_mapping, len(text)\n",
    "            )\n",
    "            \n",
    "            # Create input features\n",
    "            char_features = torch.FloatTensor(\n",
    "                np.stack([char_start_probs, char_end_probs], axis=0)\n",
    "            ).unsqueeze(0).to(device)  # Add batch dimension\n",
    "            \n",
    "            # Apply WaveNet refinement\n",
    "            refined_probs = torch.sigmoid(model(char_features)).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Extract refined start and end positions\n",
    "            refined_start_probs = refined_probs[0]\n",
    "            refined_end_probs = refined_probs[1]\n",
    "            \n",
    "            # Find best span using refined probabilities\n",
    "            start_idx = np.argmax(refined_start_probs)\n",
    "            end_idx = np.argmax(refined_end_probs)\n",
    "            \n",
    "            # Ensure valid span\n",
    "            if start_idx > end_idx:\n",
    "                # Swap if needed\n",
    "                start_idx, end_idx = end_idx, start_idx\n",
    "            \n",
    "            # Extract prediction\n",
    "            prediction = text[start_idx:end_idx+1]\n",
    "            \n",
    "            # Handle edge cases\n",
    "            if not prediction.strip():\n",
    "                prediction = text\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"Training and prediction functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa3bba0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:55:47.700811Z",
     "iopub.status.busy": "2026-01-16T00:55:47.700552Z",
     "iopub.status.idle": "2026-01-16T00:55:48.025229Z",
     "shell.execute_reply": "2026-01-16T00:55:48.024311Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Character-Level WaveNet Experiment (Proof of Concept)\n",
      "============================================================\n",
      "Device: cpu\n",
      "Batch size: 8\n",
      "Epochs: 2\n",
      "Learning rate: 0.001\n",
      "\n",
      "Loading RoBERTa model...\n",
      "RoBERTa model loaded successfully\n",
      "\n",
      "Created subset of 100 samples for testing\n",
      "\n",
      "Creating character-level datasets...\n",
      "Datasets created: 100 train, 100 val\n",
      "\n",
      "Initializing WaveNet model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during experiment: too many values to unpack (expected 3)\n",
      "\n",
      "Experiment failed. Check error messages above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_283214/462461461.py\", line 56, in run_character_level_experiment\n",
      "    sample_features, _, sample_text = train_dataset[0]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 3)\n"
     ]
    }
   ],
   "source": [
    "# Simplified main execution: Single fold training for proof of concept\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_character_level_experiment():\n",
    "    \"\"\"Run simplified character-level WaveNet experiment for proof of concept.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Character-Level WaveNet Experiment (Proof of Concept)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Configuration\n",
    "    device = 'cpu'\n",
    "    batch_size = 8  # Very small for CPU\n",
    "    epochs = 2      # Very few epochs for speed\n",
    "    lr = 0.001\n",
    "    \n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Load RoBERTa model (use pretrained since no trained model exists)\n",
    "        print(\"Loading RoBERTa model...\")\n",
    "        roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "        roberta_model.eval()\n",
    "        print(\"RoBERTa model loaded successfully\\n\")\n",
    "        \n",
    "        # Create small subset for quick testing\n",
    "        subset_size = 100\n",
    "        subset_idx = np.random.choice(len(train_df), subset_size, replace=False)\n",
    "        texts = train_df['text'].values[subset_idx]\n",
    "        sentiments = train_df['sentiment'].values[subset_idx]  # FIX: Use .values, not .value_counts().values\n",
    "        selected_texts = train_df['selected_text'].values[subset_idx]\n",
    "        \n",
    "        print(f\"Created subset of {subset_size} samples for testing\\n\")\n",
    "        \n",
    "        # Create datasets\n",
    "        print(\"Creating character-level datasets...\")\n",
    "        train_dataset = CharacterLevelDataset(\n",
    "            texts, sentiments, selected_texts, \n",
    "            roberta_model, tokenizer, device\n",
    "        )\n",
    "        \n",
    "        # Use same data for validation (small experiment)\n",
    "        val_dataset = train_dataset\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        print(f\"Datasets created: {len(train_dataset)} train, {len(val_dataset)} val\\n\")\n",
    "        \n",
    "        # Initialize WaveNet model\n",
    "        print(\"Initializing WaveNet model...\")\n",
    "        # Get input shape from first sample\n",
    "        sample_features, _, sample_text = train_dataset[0]\n",
    "        input_channels = sample_features.shape[0]\n",
    "        \n",
    "        wavenet_model = CharacterWaveNet(\n",
    "            input_channels=input_channels,\n",
    "            num_classes=2,\n",
    "            num_blocks=2,  # Reduced for speed\n",
    "            num_layers=4,  # Reduced for speed\n",
    "            residual_channels=16,  # Reduced for speed\n",
    "            gate_channels=16,\n",
    "            skip_channels=16\n",
    "        )\n",
    "        print(f\"WaveNet initialized with input_channels={input_channels}\\n\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training WaveNet...\")\n",
    "        train_losses, val_losses = train_wavenet_model(\n",
    "            wavenet_model, train_loader, val_loader, device, epochs, lr\n",
    "        )\n",
    "        print(\"\\nTraining completed\\n\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"Evaluating on validation set...\")\n",
    "        val_score, val_std, predictions, targets = evaluate_wavenet_model(\n",
    "            wavenet_model, val_loader, device\n",
    "        )\n",
    "        print(f\"Validation Jaccard Score: {val_score:.4f} ± {val_std:.4f}\\n\")\n",
    "        \n",
    "        # Show sample predictions\n",
    "        print(\"Sample predictions:\")\n",
    "        for i in range(min(3, len(predictions))):\n",
    "            orig_text = texts[i]\n",
    "            true_span = selected_texts[i]\n",
    "            pred_span = predictions[i]\n",
    "            print(f\"\\n{i+1}. Original: '{orig_text[:80]}...'\")\n",
    "            print(f\"   True: '{true_span}'\")\n",
    "            print(f\"   Pred: '{pred_span}'\")\n",
    "        \n",
    "        return val_score, val_std, predictions, targets\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None\n",
    "\n",
    "# Run the experiment\n",
    "if __name__ == \"__main__\":\n",
    "    val_score, val_std, predictions, targets = run_character_level_experiment()\n",
    "    \n",
    "    if val_score is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final Validation Score: {val_score:.4f} ± {val_std:.4f}\")\n",
    "        print(\"\\nNote: This is a proof-of-concept with limited training.\")\n",
    "        print(\"Full training would require GPU and more epochs.\")\n",
    "    else:\n",
    "        print(\"\\nExperiment failed. Check error messages above.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
