{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f40861",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102aaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5197dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcaa9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288b722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4344ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f4a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for WaveNet\n",
    "def train_wavenet(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    \"\"\"\n",
    "    Train WaveNet model.\n",
    "    \n",
    "    Args:\n",
    "        model: WaveNet model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        epochs: Number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            features = batch['features'].to(device)  # [batch, 2, seq_len]\n",
    "            targets = batch['targets'].to(device)    # [batch, 2, seq_len]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)  # [batch, 2, seq_len]\n",
    "            \n",
    "            # Calculate loss (cross-entropy for each position)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_train_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                features = batch['features'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Custom loss function for character-level predictions\n",
    "class CharacterLevelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterLevelLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculate loss for character-level predictions.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions [batch, 2, seq_len]\n",
    "            targets: Ground truth [batch, 2, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss: Combined loss for start and end predictions\n",
    "        \"\"\"\n",
    "        # predictions already have softmax applied in model\n",
    "        # For BCE loss, we need probabilities for the positive class\n",
    "        # predictions shape: [batch, 2, seq_len] where dim 1 is [start_prob, end_prob]\n",
    "        \n",
    "        loss = self.bce_loss(predictions, targets)\n",
    "        return loss\n",
    "\n",
    "# Test training on a small subset\n",
    "print(\"Testing WaveNet training...\")\n",
    "\n",
    "# Create small datasets for testing\n",
    "train_subset = train_df.sample(n=50, random_state=42)\n",
    "train_texts = train_subset['text'].tolist()\n",
    "train_sentiments = train_subset['sentiment'].tolist()\n",
    "train_selected_texts = train_subset['selected_text'].tolist()\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(train_texts))\n",
    "train_texts_split = train_texts[:train_size]\n",
    "train_sentiments_split = train_sentiments[:train_size]\n",
    "train_selected_texts_split = train_selected_texts[:train_size]\n",
    "\n",
    "val_texts_split = train_texts[train_size:]\n",
    "val_sentiments_split = train_sentiments[train_size:]\n",
    "val_selected_texts_split = train_selected_texts[train_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_texts_split)}\")\n",
    "print(f\"Val samples: {len(val_texts_split)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CharacterLevelDataset(\n",
    "    texts=train_texts_split,\n",
    "    sentiments=train_sentiments_split,\n",
    "    selected_texts=train_selected_texts_split,\n",
    "    roberta_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "val_dataset = CharacterLevelDataset(\n",
    "    texts=val_texts_split,\n",
    "    sentiments=val_sentiments_split,\n",
    "    selected_texts=val_selected_texts_split,\n",
    "    roberta_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "wavenet_model = CharacterWaveNet(\n",
    "    input_channels=2,\n",
    "    num_classes=2,\n",
    "    num_blocks=2,  # Reduced for testing\n",
    "    num_layers=4,   # Reduced for testing\n",
    "    residual_channels=16,\n",
    "    gate_channels=16,\n",
    "    skip_channels=16\n",
    ")\n",
    "\n",
    "criterion = CharacterLevelLoss()\n",
    "optimizer = torch.optim.Adam(wavenet_model.parameters(), lr=0.001)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Train for a few epochs (reduced for testing)\n",
    "train_losses, val_losses = train_wavenet(\n",
    "    wavenet_model, train_loader, val_loader, criterion, optimizer, device, epochs=2\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.6f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
