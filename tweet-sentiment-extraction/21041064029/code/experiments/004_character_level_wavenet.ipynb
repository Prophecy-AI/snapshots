{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f40861",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102aaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5197dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcaa9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288b722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4344ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f4a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for WaveNet\n",
    "def train_wavenet(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    \"\"\"\n",
    "    Train WaveNet model.\n",
    "    \n",
    "    Args:\n",
    "        model: WaveNet model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        epochs: Number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            features = batch['features'].to(device)  # [batch, 2, seq_len]\n",
    "            targets = batch['targets'].to(device)    # [batch, 2, seq_len]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)  # [batch, 2, seq_len]\n",
    "            \n",
    "            # Calculate loss (cross-entropy for each position)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_train_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                features = batch['features'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Custom loss function for character-level predictions\n",
    "class CharacterLevelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterLevelLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculate loss for character-level predictions.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions [batch, 2, seq_len]\n",
    "            targets: Ground truth [batch, 2, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss: Combined loss for start and end predictions\n",
    "        \"\"\"\n",
    "        # predictions already have softmax applied in model\n",
    "        # For BCE loss, we need probabilities for the positive class\n",
    "        # predictions shape: [batch, 2, seq_len] where dim 1 is [start_prob, end_prob]\n",
    "        \n",
    "        loss = self.bce_loss(predictions, targets)\n",
    "        return loss\n",
    "\n",
    "# Test training on a small subset\n",
    "print(\"Testing WaveNet training...\")\n",
    "\n",
    "# Create small datasets for testing\n",
    "train_subset = train_df.sample(n=50, random_state=42)\n",
    "train_texts = train_subset['text'].tolist()\n",
    "train_sentiments = train_subset['sentiment'].tolist()\n",
    "train_selected_texts = train_subset['selected_text'].tolist()\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(train_texts))\n",
    "train_texts_split = train_texts[:train_size]\n",
    "train_sentiments_split = train_sentiments[:train_size]\n",
    "train_selected_texts_split = train_selected_texts[:train_size]\n",
    "\n",
    "val_texts_split = train_texts[train_size:]\n",
    "val_sentiments_split = train_sentiments[train_size:]\n",
    "val_selected_texts_split = train_selected_texts[train_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_texts_split)}\")\n",
    "print(f\"Val samples: {len(val_texts_split)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CharacterLevelDataset(\n",
    "    texts=train_texts_split,\n",
    "    sentiments=train_sentiments_split,\n",
    "    selected_texts=train_selected_texts_split,\n",
    "    roberta_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "val_dataset = CharacterLevelDataset(\n",
    "    texts=val_texts_split,\n",
    "    sentiments=val_sentiments_split,\n",
    "    selected_texts=val_selected_texts_split,\n",
    "    roberta_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "wavenet_model = CharacterWaveNet(\n",
    "    input_channels=2,\n",
    "    num_classes=2,\n",
    "    num_blocks=2,  # Reduced for testing\n",
    "    num_layers=4,   # Reduced for testing\n",
    "    residual_channels=16,\n",
    "    gate_channels=16,\n",
    "    skip_channels=16\n",
    ")\n",
    "\n",
    "criterion = CharacterLevelLoss()\n",
    "optimizer = torch.optim.Adam(wavenet_model.parameters(), lr=0.001)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Train for a few epochs (reduced for testing)\n",
    "train_losses, val_losses = train_wavenet(\n",
    "    wavenet_model, train_loader, val_loader, criterion, optimizer, device, epochs=2\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fe55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full 5-fold CV training for Character-Level WaveNet\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "def extract_predictions_from_wavenet(model, features, text, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract refined character-level predictions from trained WaveNet.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained WaveNet model\n",
    "        features: Character-level features [2, seq_len]\n",
    "        text: Original text\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        refined_span: Refined selected_text prediction\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension\n",
    "        features_tensor = features.unsqueeze(0).to(device)  # [1, 2, seq_len]\n",
    "        \n",
    "        # Get refined probabilities\n",
    "        refined_probs = model(features_tensor)  # [1, 2, seq_len]\n",
    "        refined_probs = refined_probs.squeeze(0).cpu().numpy()  # [2, seq_len]\n",
    "        \n",
    "        # refined_probs[0] = start probabilities, refined_probs[1] = end probabilities\n",
    "        start_char = np.argmax(refined_probs[0])\n",
    "        end_char = np.argmax(refined_probs[1])\n",
    "        \n",
    "        # Ensure valid range\n",
    "        start_char = max(0, min(start_char, len(text) - 1))\n",
    "        end_char = max(start_char, min(end_char, len(text) - 1))\n",
    "        \n",
    "        # Extract span\n",
    "        refined_span = text[start_char:end_char + 1]\n",
    "        \n",
    "        return refined_span, refined_probs\n",
    "\n",
    "# Set up 5-fold CV\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare data\n",
    "texts = train_df['text'].tolist()\n",
    "sentiments = train_df['sentiment'].tolist()\n",
    "selected_texts = train_df['selected_text'].tolist()\n",
    "\n",
    "# Store results\n",
    "fold_scores = []\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "print(f\"Starting 5-fold CV for Character-Level WaveNet...\")\n",
    "print(f\"Total samples: {len(train_df)}\")\n",
    "\n",
    "# Create experiment directory\n",
    "exp_dir = Path('/home/code/experiments/004_character_level_wavenet')\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_texts_fold = [texts[i] for i in train_idx]\n",
    "    train_sentiments_fold = [sentiments[i] for i in train_idx]\n",
    "    train_selected_texts_fold = [selected_texts[i] for i in train_idx]\n",
    "    \n",
    "    val_texts_fold = [texts[i] for i in val_idx]\n",
    "    val_sentiments_fold = [sentiments[i] for i in val_idx]\n",
    "    val_selected_texts_fold = [selected_texts[i] for i in val_idx]\n",
    "    \n",
    "    print(f\"Train samples: {len(train_texts_fold)}\")\n",
    "    print(f\"Val samples: {len(val_texts_fold)}\")\n",
    "    \n",
    "    # Load trained RoBERTa model for this fold\n",
    "    roberta_model_path = f'/home/code/experiments/002_roberta_span/fold_{fold}_model.pt'\n",
    "    if Path(roberta_model_path).exists():\n",
    "        roberta_model = torch.load(roberta_model_path, map_location='cpu')\n",
    "        roberta_model.eval()\n",
    "        print(f\"Loaded RoBERTa model from {roberta_model_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: RoBERTa model not found at {roberta_model_path}\")\n",
    "        print(\"Using pretrained RoBERTa (performance will be suboptimal)\")\n",
    "        roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    roberta_model = roberta_model.to(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CharacterLevelDataset(\n",
    "        texts=train_texts_fold,\n",
    "        sentiments=train_sentiments_fold,\n",
    "        selected_texts=train_selected_texts_fold,\n",
    "        roberta_model=roberta_model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_dataset = CharacterLevelDataset(\n",
    "        texts=val_texts_fold,\n",
    "        sentiments=val_sentiments_fold,\n",
    "        selected_texts=val_selected_texts_fold,\n",
    "        roberta_model=roberta_model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    # Use smaller batch size due to memory constraints with character-level data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Initialize WaveNet model\n",
    "    # Use smaller architecture to fit in GPU memory\n",
    "    wavenet_model = CharacterWaveNet(\n",
    "        input_channels=2,\n",
    "        num_classes=2,\n",
    "        num_blocks=3,      # Reduced from 4\n",
    "        num_layers=5,      # Reduced from 6\n",
    "        residual_channels=64,\n",
    "        gate_channels=64,\n",
    "        skip_channels=64\n",
    "    )\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = CharacterLevelLoss()\n",
    "    optimizer = torch.optim.Adam(wavenet_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\nTraining WaveNet for fold {fold + 1}...\")\n",
    "    train_losses, val_losses = train_wavenet(\n",
    "        wavenet_model, train_loader, val_loader, criterion, optimizer, device, epochs=10\n",
    "    )\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = exp_dir / f'fold_{fold}_wavenet_model.pt'\n",
    "    torch.save(wavenet_model, model_path)\n",
    "    print(f\"Saved WaveNet model to {model_path}\")\n",
    "    \n",
    "    # Generate predictions on validation set\n",
    "    print(f\"\\nGenerating predictions for fold {fold + 1}...\")\n",
    "    fold_predictions = []\n",
    "    fold_targets = []\n",
    "    \n",
    "    wavenet_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Predicting fold {fold + 1}\"):\n",
    "            features = batch['features'].to(device)\n",
    "            targets_batch = batch['targets']\n",
    "            texts_batch = batch['text']\n",
    "            selected_texts_batch = batch['selected_text']\n",
    "            \n",
    "            # Get refined predictions\n",
    "            refined_probs = wavenet_model(features)  # [batch, 2, seq_len]\n",
    "            refined_probs = refined_probs.cpu().numpy()\n",
    "            \n",
    "            # Extract spans for each sample in batch\n",
    "            for i in range(len(texts_batch)):\n",
    "                text = texts_batch[i]\n",
    "                selected_text = selected_texts_batch[i]\n",
    "                \n",
    "                # Get start and end positions from refined probabilities\n",
    "                start_char = np.argmax(refined_probs[i, 0, :len(text)])\n",
    "                end_char = np.argmax(refined_probs[i, 1, :len(text)])\n",
    "                \n",
    "                # Ensure valid range\n",
    "                start_char = max(0, min(start_char, len(text) - 1))\n",
    "                end_char = max(start_char, min(end_char, len(text) - 1))\n",
    "                \n",
    "                # Extract span\n",
    "                refined_span = text[start_char:end_char + 1]\n",
    "                \n",
    "                fold_predictions.append(refined_span)\n",
    "                fold_targets.append(selected_text)\n",
    "    \n",
    "    # Calculate Jaccard score for this fold\n",
    "    fold_score = np.mean([\n",
    "        jaccard_similarity(pred, target)\n",
    "        for pred, target in zip(fold_predictions, val_selected_texts_fold)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Jaccard Score: {fold_score:.6f}\")\n",
    "    fold_scores.append(fold_score)\n",
    "    \n",
    "    # Store predictions\n",
    "    all_predictions.extend(fold_predictions)\n",
    "    all_targets.extend(fold_targets)\n",
    "    \n",
    "    # Clean up\n",
    "    del roberta_model, wavenet_model, train_dataset, val_dataset\n",
    "    del train_loader, val_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate overall CV score\n",
    "overall_score = np.mean(fold_scores)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Overall 5-Fold CV Score: {overall_score:.6f}\")\n",
    "print(f\"Fold scores: {fold_scores}\")\n",
    "print(f\"Std dev: {np.std(fold_scores):.6f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save CV results\n",
    "cv_results = {\n",
    "    'overall_score': overall_score,\n",
    "    'fold_scores': fold_scores,\n",
    "    'std_dev': np.std(fold_scores),\n",
    "    'num_folds': n_splits\n",
    "}\n",
    "\n",
    "with open(exp_dir / 'cv_results.json', 'w') as f:\n",
    "    json.dump(cv_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved CV results to {exp_dir / 'cv_results.json'}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
