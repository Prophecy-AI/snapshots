{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740e33ac",
   "metadata": {},
   "source": [
    "# Experiment 004: Character-Level WaveNet Model\n",
    "\n",
    "**Objective**: Implement character-level WaveNet refinement to improve span boundary detection\n",
    "\n",
    "**Expected gain**: +0.015 to +0.025 points\n",
    "**Priority**: HIGHEST (addresses core weakness in boundary detection)\n",
    "\n",
    "**Approach**:\n",
    "1. Modify RoBERTa inference to save token start/end probability distributions\n",
    "2. Convert token probabilities to character-level probabilities using offset_mapping\n",
    "3. Build WaveNet with dilated convolutions for smoothing\n",
    "4. Train on 5-fold CV using character-level probabilities as features\n",
    "5. Generate refined predictions with better boundary detection\n",
    "\n",
    "**Reference**: Winning solution by Theo Viel (dark-of-the-moon) - Character-level WaveNet was key innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Define Jaccard similarity metric\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Calculate Jaccard similarity between two strings.\"\"\"\n",
    "    if pd.isna(str1) or pd.isna(str2):\n",
    "        return 0.0\n",
    "    \n",
    "    str1, str2 = str(str1), str(str2)\n",
    "    \n",
    "    # Tokenize by splitting on whitespace\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    \n",
    "    # Handle empty sets\n",
    "    if len(a) == 0 and len(b) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceabd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_path = Path('/home/data/train.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample row:\")\n",
    "print(train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "print(f\"Tokenizer loaded: {tokenizer.name_or_path}\")\n",
    "\n",
    "# Test tokenization with offset mapping\n",
    "text = train_df.iloc[0]['text']\n",
    "selected_text = train_df.iloc[0]['selected_text']\n",
    "\n",
    "print(f\"\\nOriginal text: '{text}'\")\n",
    "print(f\"Selected text: '{selected_text}'\")\n",
    "\n",
    "# Tokenize with offset mapping\n",
    "encoding = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"\\nToken IDs shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"Offset mapping shape: {encoding['offset_mapping'].shape}\")\n",
    "print(f\"\\nFirst 10 tokens and their offsets:\")\n",
    "for i in range(min(10, len(encoding['input_ids'][0]))):\n",
    "    token_id = encoding['input_ids'][0][i].item()\n",
    "    token = tokenizer.decode([token_id])\n",
    "    offsets = encoding['offset_mapping'][0][i]\n",
    "    print(f\"  {i}: '{token}' -> offsets: {offsets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract token start/end probabilities from RoBERTa\n",
    "@torch.no_grad()\n",
    "def extract_token_probabilities(model, text, sentiment, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract token-level start and end probability distributions from RoBERTa.\n",
    "    Returns probabilities for each token position.\n",
    "    \"\"\"\n",
    "    # Prepare input with sentiment token\n",
    "    input_text = f\"{sentiment} {text}\"\n",
    "    encoding = tokenizer(\n",
    "        input_text,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    offset_mapping = encoding['offset_mapping'][0]  # Remove batch dim\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    start_logits = outputs.start_logits[0]  # [seq_len]\n",
    "    end_logits = outputs.end_logits[0]      # [seq_len]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    start_probs = F.softmax(start_logits, dim=0).cpu().numpy()\n",
    "    end_probs = F.softmax(end_logits, dim=0).cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'start_probs': start_probs,  # [seq_len]\n",
    "        'end_probs': end_probs,      # [seq_len]\n",
    "        'offset_mapping': offset_mapping.cpu().numpy(),  # [seq_len, 2]\n",
    "        'input_ids': input_ids.cpu().numpy(),\n",
    "        'attention_mask': attention_mask.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing token probability extraction...\")\n",
    "\n",
    "# Load a trained RoBERTa model (from exp_002)\n",
    "model_path = Path('/home/code/experiments/002_roberta_span/fold_0_model.pt')\n",
    "if model_path.exists():\n",
    "    model = torch.load(model_path, map_location='cpu')\n",
    "    model.eval()\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "else:\n",
    "    # If no saved model, load pretrained\n",
    "    print(\"No saved model found, loading pretrained RoBERTa...\")\n",
    "    model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Test on a sample\n",
    "test_text = train_df.iloc[0]['text']\n",
    "test_sentiment = train_df.iloc[0]['sentiment']\n",
    "\n",
    "probs = extract_token_probabilities(model, test_text, test_sentiment, tokenizer)\n",
    "print(f\"\\nStart probabilities shape: {probs['start_probs'].shape}\")\n",
    "print(f\"End probabilities shape: {probs['end_probs'].shape}\")\n",
    "print(f\"Offset mapping shape: {probs['offset_mapping'].shape}\")\n",
    "\n",
    "# Show top predictions\n",
    "start_token = np.argmax(probs['start_probs'])\n",
    "end_token = np.argmax(probs['end_probs'])\n",
    "print(f\"\\nPredicted start token: {start_token} (prob: {probs['start_probs'][start_token]:.4f})\")\n",
    "print(f\"Predicted end token: {end_token} (prob: {probs['end_probs'][end_token]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert token probabilities to character probabilities\n",
    "def token_to_char_probabilities(token_probs, offset_mapping, text_length):\n",
    "    \"\"\"\n",
    "    Convert token-level probabilities to character-level probabilities.\n",
    "    \n",
    "    Args:\n",
    "        token_probs: [seq_len] array of token probabilities\n",
    "        offset_mapping: [seq_len, 2] array of (start_char, end_char) offsets\n",
    "        text_length: Length of original text\n",
    "        \n",
    "    Returns:\n",
    "        char_probs: [text_length] array of character-level probabilities\n",
    "    \"\"\"\n",
    "    char_probs = np.zeros(text_length)\n",
    "    char_counts = np.zeros(text_length)\n",
    "    \n",
    "    # Aggregate probabilities from tokens to characters\n",
    "    for token_idx, (char_start, char_end) in enumerate(offset_mapping):\n",
    "        # Skip special tokens (offset = (0, 0))\n",
    "        if char_start == 0 and char_end == 0:\n",
    "            continue\n",
    "            \n",
    "        # Ensure offsets are within bounds\n",
    "        char_start = max(0, min(char_start, text_length - 1))\n",
    "        char_end = max(0, min(char_end, text_length))\n",
    "        \n",
    "        if char_end > char_start:\n",
    "            # Distribute token probability to characters\n",
    "            prob_per_char = token_probs[token_idx] / (char_end - char_start)\n",
    "            char_probs[char_start:char_end] += prob_per_char\n",
    "            char_counts[char_start:char_end] += 1\n",
    "    \n",
    "    # Average probabilities where multiple tokens overlap\n",
    "    mask = char_counts > 0\n",
    "    char_probs[mask] /= char_counts[mask]\n",
    "    \n",
    "    return char_probs\n",
    "\n",
    "# Test the conversion function\n",
    "print(\"Testing token to character probability conversion...\")\n",
    "\n",
    "text_length = len(test_text)\n",
    "char_start_probs = token_to_char_probabilities(\n",
    "    probs['start_probs'], \n",
    "    probs['offset_mapping'], \n",
    "    text_length\n",
    ")\n",
    "char_end_probs = token_to_char_probabilities(\n",
    "    probs['end_probs'], \n",
    "    probs['offset_mapping'], \n",
    "    text_length\n",
    ")\n",
    "\n",
    "print(f\"Character-level start probabilities shape: {char_start_probs.shape}\")\n",
    "print(f\"Character-level end probabilities shape: {char_end_probs.shape}\")\n",
    "\n",
    "# Show character probabilities for first 50 characters\n",
    "print(f\"\\nFirst 50 characters of text: '{test_text[:50]}'\")\n",
    "print(\"Character start probabilities (first 20):\")\n",
    "for i in range(min(20, text_length)):\n",
    "    print(f\"  Char {i} ('{test_text[i]}'): {char_start_probs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaveNet architecture for character-level probability refinement\n",
    "class CharacterWaveNet(nn.Module):\n",
    "    def __init__(self, input_channels=2, num_classes=2, num_blocks=4, num_layers=6, \n",
    "                 residual_channels=32, gate_channels=32, skip_channels=32):\n",
    "        \"\"\"\n",
    "        WaveNet for character-level span prediction refinement.\n",
    "        \n",
    "        Args:\n",
    "            input_channels: Number of input channels (start_prob, end_prob)\n",
    "            num_classes: Number of output classes (refined_start, refined_end)\n",
    "            num_blocks: Number of residual blocks\n",
    "            num_layers: Number of layers per block (dilated convolutions)\n",
    "            residual_channels: Channels in residual connections\n",
    "            gate_channels: Channels in gated activation units\n",
    "            skip_channels: Channels in skip connections\n",
    "        \"\"\"\n",
    "        super(CharacterWaveNet, self).__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Conv1d(input_channels, residual_channels, 1)\n",
    "        \n",
    "        # Dilated convolution layers\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "        # Build dilated convolution blocks\n",
    "        for b in range(num_blocks):\n",
    "            for l in range(num_layers):\n",
    "                # Dilation doubles each layer: 1, 2, 4, 8, 16, 32, ...\n",
    "                dilation = 2 ** l\n",
    "                \n",
    "                # Gated activation unit (dilated convolution)\n",
    "                self.dilated_convs.append(\n",
    "                    nn.Conv1d(residual_channels, gate_channels, kernel_size=3, \n",
    "                             padding=dilation, dilation=dilation)\n",
    "                )\n",
    "                \n",
    "                # 1x1 conv for residual connection\n",
    "                self.residual_convs.append(\n",
    "                    nn.Conv1d(gate_channels // 2, residual_channels, 1)\n",
    "                )\n",
    "                \n",
    "                # 1x1 conv for skip connection\n",
    "                self.skip_convs.append(\n",
    "                    nn.Conv1d(gate_channels // 2, skip_channels, 1)\n",
    "                )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        self.output_conv2 = nn.Conv1d(skip_channels, num_classes, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, input_channels, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            output: Output tensor [batch, num_classes, seq_len]\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)  # [batch, residual_channels, seq_len]\n",
    "        \n",
    "        # Skip connections accumulator\n",
    "        skip_connections = []\n",
    "        \n",
    "        # Apply dilated convolutions\n",
    "        layer_idx = 0\n",
    "        for b in range(self.num_blocks):\n",
    "            for l in range(self.num_layers):\n",
    "                # Gated activation: tanh(x) * sigmoid(x)\n",
    "                dilated_out = self.dilated_convs[layer_idx](x)\n",
    "                \n",
    "                # Split into two parts for gating\n",
    "                tanh_out = self.tanh(dilated_out[:, :dilated_out.shape[1]//2, :])\n",
    "                sigmoid_out = self.sigmoid(dilated_out[:, dilated_out.shape[1]//2:, :])\n",
    "                \n",
    "                gated_out = tanh_out * sigmoid_out\n",
    "                \n",
    "                # Residual connection\n",
    "                residual_out = self.residual_convs[layer_idx](gated_out)\n",
    "                x = x + residual_out\n",
    "                \n",
    "                # Skip connection\n",
    "                skip_out = self.skip_convs[layer_idx](gated_out)\n",
    "                skip_connections.append(skip_out)\n",
    "                \n",
    "                layer_idx += 1\n",
    "        \n",
    "        # Sum all skip connections\n",
    "        skip_sum = sum(skip_connections)  # [batch, skip_channels, seq_len]\n",
    "        \n",
    "        # Output layers\n",
    "        output = self.relu(skip_sum)\n",
    "        output = self.output_conv1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.output_conv2(output)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        output = F.softmax(output, dim=1)  # [batch, num_classes, seq_len]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test the WaveNet architecture\n",
    "print(\"Testing WaveNet architecture...\")\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 100\n",
    "input_channels = 2\n",
    "\n",
    "# Create dummy input (character-level start/end probabilities)\n",
    "dummy_input = torch.randn(batch_size, input_channels, seq_len)\n",
    "\n",
    "# Initialize model\n",
    "model = CharacterWaveNet(\n",
    "    input_channels=input_channels,\n",
    "    num_classes=2,\n",
    "    num_blocks=2,  # Reduced for testing\n",
    "    num_layers=4,   # Reduced for testing\n",
    "    residual_channels=16,\n",
    "    gate_channels=16,\n",
    "    skip_channels=16\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output sum (should be 1.0 per position): {output[0, :, 0].sum().item():.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
