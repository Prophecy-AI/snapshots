{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740e33ac",
   "metadata": {},
   "source": [
    "# Experiment 004: Character-Level WaveNet Model\n",
    "\n",
    "**Objective**: Implement character-level WaveNet refinement to improve span boundary detection\n",
    "\n",
    "**Expected gain**: +0.015 to +0.025 points\n",
    "**Priority**: HIGHEST (addresses core weakness in boundary detection)\n",
    "\n",
    "**Approach**:\n",
    "1. Modify RoBERTa inference to save token start/end probability distributions\n",
    "2. Convert token probabilities to character-level probabilities using offset_mapping\n",
    "3. Build WaveNet with dilated convolutions for smoothing\n",
    "4. Train on 5-fold CV using character-level probabilities as features\n",
    "5. Generate refined predictions with better boundary detection\n",
    "\n",
    "**Reference**: Winning solution by Theo Viel (dark-of-the-moon) - Character-level WaveNet was key innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d9a9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:02:39.937543Z",
     "iopub.status.busy": "2026-01-16T00:02:39.936825Z",
     "iopub.status.idle": "2026-01-16T00:02:39.952637Z",
     "shell.execute_reply": "2026-01-16T00:02:39.951896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Define Jaccard similarity metric\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Calculate Jaccard similarity between two strings.\"\"\"\n",
    "    if pd.isna(str1) or pd.isna(str2):\n",
    "        return 0.0\n",
    "    \n",
    "    str1, str2 = str(str1), str(str2)\n",
    "    \n",
    "    # Tokenize by splitting on whitespace\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    \n",
    "    # Handle empty sets\n",
    "    if len(a) == 0 and len(b) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ceabd10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:02:39.954864Z",
     "iopub.status.busy": "2026-01-16T00:02:39.954636Z",
     "iopub.status.idle": "2026-01-16T00:02:40.056780Z",
     "shell.execute_reply": "2026-01-16T00:02:40.056172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27481 training samples\n",
      "Columns: ['textID', 'text', 'selected_text', 'sentiment']\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample row:\n",
      "textID                                     cb774db0d1\n",
      "text              I`d have responded, if I were going\n",
      "selected_text     I`d have responded, if I were going\n",
      "sentiment                                     neutral\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_path = Path('/home/data/train.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample row:\")\n",
    "print(train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c377d347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:02:40.058951Z",
     "iopub.status.busy": "2026-01-16T00:02:40.058726Z",
     "iopub.status.idle": "2026-01-16T00:02:41.298722Z",
     "shell.execute_reply": "2026-01-16T00:02:41.297889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe902d30bb74592a2bd5813c2dae114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36153b2000a4e43b8c05d38e2d62b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaf8a33b3734351b1fcb108bd69600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc69b09162d468590b3abe8cfd4611f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: roberta-base\n",
      "\n",
      "Original text: ' I`d have responded, if I were going'\n",
      "Selected text: 'I`d have responded, if I were going'\n",
      "\n",
      "Token IDs shape: torch.Size([1, 12])\n",
      "Offset mapping shape: torch.Size([1, 12, 2])\n",
      "\n",
      "First 10 tokens and their offsets:\n",
      "  0: '<s>' -> offsets: tensor([0, 0])\n",
      "  1: ' I' -> offsets: tensor([1, 2])\n",
      "  2: '`' -> offsets: tensor([2, 3])\n",
      "  3: 'd' -> offsets: tensor([3, 4])\n",
      "  4: ' have' -> offsets: tensor([5, 9])\n",
      "  5: ' responded' -> offsets: tensor([10, 19])\n",
      "  6: ',' -> offsets: tensor([19, 20])\n",
      "  7: ' if' -> offsets: tensor([21, 23])\n",
      "  8: ' I' -> offsets: tensor([24, 25])\n",
      "  9: ' were' -> offsets: tensor([26, 30])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "print(f\"Tokenizer loaded: {tokenizer.name_or_path}\")\n",
    "\n",
    "# Test tokenization with offset mapping\n",
    "text = train_df.iloc[0]['text']\n",
    "selected_text = train_df.iloc[0]['selected_text']\n",
    "\n",
    "print(f\"\\nOriginal text: '{text}'\")\n",
    "print(f\"Selected text: '{selected_text}'\")\n",
    "\n",
    "# Tokenize with offset mapping\n",
    "encoding = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"\\nToken IDs shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"Offset mapping shape: {encoding['offset_mapping'].shape}\")\n",
    "print(f\"\\nFirst 10 tokens and their offsets:\")\n",
    "for i in range(min(10, len(encoding['input_ids'][0]))):\n",
    "    token_id = encoding['input_ids'][0][i].item()\n",
    "    token = tokenizer.decode([token_id])\n",
    "    offsets = encoding['offset_mapping'][0][i]\n",
    "    print(f\"  {i}: '{token}' -> offsets: {offsets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313a3afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:05:48.237057Z",
     "iopub.status.busy": "2026-01-16T00:05:48.236314Z",
     "iopub.status.idle": "2026-01-16T00:05:49.398420Z",
     "shell.execute_reply": "2026-01-16T00:05:49.397822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing token probability extraction...\n",
      "No saved model found, loading pretrained RoBERTa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start probabilities shape: (14,)\n",
      "End probabilities shape: (14,)\n",
      "Offset mapping shape: (14, 2)\n",
      "\n",
      "Predicted start token: 13 (prob: 0.0831)\n",
      "Predicted end token: 12 (prob: 0.0772)\n"
     ]
    }
   ],
   "source": [
    "# Function to extract token start/end probabilities from RoBERTa\n",
    "@torch.no_grad()\n",
    "def extract_token_probabilities(model, text, sentiment, tokenizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Extract token-level start and end probability distributions from RoBERTa.\n",
    "    Returns probabilities for each token position.\n",
    "    \"\"\"\n",
    "    # Prepare input with sentiment token\n",
    "    input_text = f\"{sentiment} {text}\"\n",
    "    encoding = tokenizer(\n",
    "        input_text,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    offset_mapping = encoding['offset_mapping'][0]  # Remove batch dim\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    start_logits = outputs.start_logits[0]  # [seq_len]\n",
    "    end_logits = outputs.end_logits[0]      # [seq_len]\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    start_probs = F.softmax(start_logits, dim=0).cpu().numpy()\n",
    "    end_probs = F.softmax(end_logits, dim=0).cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'start_probs': start_probs,  # [seq_len]\n",
    "        'end_probs': end_probs,      # [seq_len]\n",
    "        'offset_mapping': offset_mapping.cpu().numpy(),  # [seq_len, 2]\n",
    "        'input_ids': input_ids.cpu().numpy(),\n",
    "        'attention_mask': attention_mask.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing token probability extraction...\")\n",
    "\n",
    "# Load a trained RoBERTa model (from exp_002)\n",
    "model_path = Path('/home/code/experiments/002_roberta_span/fold_0_model.pt')\n",
    "if model_path.exists():\n",
    "    model = torch.load(model_path, map_location='cpu')\n",
    "    model.eval()\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "else:\n",
    "    # If no saved model, load pretrained\n",
    "    print(\"No saved model found, loading pretrained RoBERTa...\")\n",
    "    model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "# Test on a sample\n",
    "test_text = train_df.iloc[0]['text']\n",
    "test_sentiment = train_df.iloc[0]['sentiment']\n",
    "\n",
    "probs = extract_token_probabilities(model, test_text, test_sentiment, tokenizer)\n",
    "print(f\"\\nStart probabilities shape: {probs['start_probs'].shape}\")\n",
    "print(f\"End probabilities shape: {probs['end_probs'].shape}\")\n",
    "print(f\"Offset mapping shape: {probs['offset_mapping'].shape}\")\n",
    "\n",
    "# Show top predictions\n",
    "start_token = np.argmax(probs['start_probs'])\n",
    "end_token = np.argmax(probs['end_probs'])\n",
    "print(f\"\\nPredicted start token: {start_token} (prob: {probs['start_probs'][start_token]:.4f})\")\n",
    "print(f\"Predicted end token: {end_token} (prob: {probs['end_probs'][end_token]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b8ded8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:05:49.400378Z",
     "iopub.status.busy": "2026-01-16T00:05:49.400186Z",
     "iopub.status.idle": "2026-01-16T00:05:49.408904Z",
     "shell.execute_reply": "2026-01-16T00:05:49.408399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing token to character probability conversion...\n",
      "Character-level start probabilities shape: (36,)\n",
      "Character-level end probabilities shape: (36,)\n",
      "\n",
      "First 50 characters of text: ' I`d have responded, if I were going'\n",
      "Character start probabilities (first 20):\n",
      "  Char 0 (' '): 0.0086\n",
      "  Char 1 ('I'): 0.0086\n",
      "  Char 2 ('`'): 0.0086\n",
      "  Char 3 ('d'): 0.0086\n",
      "  Char 4 (' '): 0.0086\n",
      "  Char 5 ('h'): 0.0086\n",
      "  Char 6 ('a'): 0.0086\n",
      "  Char 7 ('v'): 0.0000\n",
      "  Char 8 ('e'): 0.0000\n",
      "  Char 9 (' '): 0.0673\n",
      "  Char 10 ('r'): 0.0597\n",
      "  Char 11 ('e'): 0.0646\n",
      "  Char 12 ('s'): 0.0000\n",
      "  Char 13 ('p'): 0.0180\n",
      "  Char 14 ('o'): 0.0180\n",
      "  Char 15 ('n'): 0.0180\n",
      "  Char 16 ('d'): 0.0180\n",
      "  Char 17 ('e'): 0.0000\n",
      "  Char 18 ('d'): 0.0081\n",
      "  Char 19 (','): 0.0081\n"
     ]
    }
   ],
   "source": [
    "# Function to convert token probabilities to character probabilities\n",
    "def token_to_char_probabilities(token_probs, offset_mapping, text_length):\n",
    "    \"\"\"\n",
    "    Convert token-level probabilities to character-level probabilities.\n",
    "    \n",
    "    Args:\n",
    "        token_probs: [seq_len] array of token probabilities\n",
    "        offset_mapping: [seq_len, 2] array of (start_char, end_char) offsets\n",
    "        text_length: Length of original text\n",
    "        \n",
    "    Returns:\n",
    "        char_probs: [text_length] array of character-level probabilities\n",
    "    \"\"\"\n",
    "    char_probs = np.zeros(text_length)\n",
    "    char_counts = np.zeros(text_length)\n",
    "    \n",
    "    # Aggregate probabilities from tokens to characters\n",
    "    for token_idx, (char_start, char_end) in enumerate(offset_mapping):\n",
    "        # Skip special tokens (offset = (0, 0))\n",
    "        if char_start == 0 and char_end == 0:\n",
    "            continue\n",
    "            \n",
    "        # Ensure offsets are within bounds\n",
    "        char_start = max(0, min(char_start, text_length - 1))\n",
    "        char_end = max(0, min(char_end, text_length))\n",
    "        \n",
    "        if char_end > char_start:\n",
    "            # Distribute token probability to characters\n",
    "            prob_per_char = token_probs[token_idx] / (char_end - char_start)\n",
    "            char_probs[char_start:char_end] += prob_per_char\n",
    "            char_counts[char_start:char_end] += 1\n",
    "    \n",
    "    # Average probabilities where multiple tokens overlap\n",
    "    mask = char_counts > 0\n",
    "    char_probs[mask] /= char_counts[mask]\n",
    "    \n",
    "    return char_probs\n",
    "\n",
    "# Test the conversion function\n",
    "print(\"Testing token to character probability conversion...\")\n",
    "\n",
    "text_length = len(test_text)\n",
    "char_start_probs = token_to_char_probabilities(\n",
    "    probs['start_probs'], \n",
    "    probs['offset_mapping'], \n",
    "    text_length\n",
    ")\n",
    "char_end_probs = token_to_char_probabilities(\n",
    "    probs['end_probs'], \n",
    "    probs['offset_mapping'], \n",
    "    text_length\n",
    ")\n",
    "\n",
    "print(f\"Character-level start probabilities shape: {char_start_probs.shape}\")\n",
    "print(f\"Character-level end probabilities shape: {char_end_probs.shape}\")\n",
    "\n",
    "# Show character probabilities for first 50 characters\n",
    "print(f\"\\nFirst 50 characters of text: '{test_text[:50]}'\")\n",
    "print(\"Character start probabilities (first 20):\")\n",
    "for i in range(min(20, text_length)):\n",
    "    print(f\"  Char {i} ('{test_text[i]}'): {char_start_probs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0e78c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:05:49.410775Z",
     "iopub.status.busy": "2026-01-16T00:05:49.410554Z",
     "iopub.status.idle": "2026-01-16T00:05:50.064048Z",
     "shell.execute_reply": "2026-01-16T00:05:50.063399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing WaveNet architecture...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 2, 100])\n",
      "Output shape: torch.Size([2, 2, 100])\n",
      "Output sum (should be 1.0 per position): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# WaveNet architecture for character-level probability refinement\n",
    "class CharacterWaveNet(nn.Module):\n",
    "    def __init__(self, input_channels=2, num_classes=2, num_blocks=4, num_layers=6, \n",
    "                 residual_channels=32, gate_channels=32, skip_channels=32):\n",
    "        \"\"\"\n",
    "        WaveNet for character-level span prediction refinement.\n",
    "        \n",
    "        Args:\n",
    "            input_channels: Number of input channels (start_prob, end_prob)\n",
    "            num_classes: Number of output classes (refined_start, refined_end)\n",
    "            num_blocks: Number of residual blocks\n",
    "            num_layers: Number of layers per block (dilated convolutions)\n",
    "            residual_channels: Channels in residual connections\n",
    "            gate_channels: Channels in gated activation units\n",
    "            skip_channels: Channels in skip connections\n",
    "        \"\"\"\n",
    "        super(CharacterWaveNet, self).__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Conv1d(input_channels, residual_channels, 1)\n",
    "        \n",
    "        # Dilated convolution layers\n",
    "        self.dilated_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "        # Build dilated convolution blocks\n",
    "        for b in range(num_blocks):\n",
    "            for l in range(num_layers):\n",
    "                # Dilation doubles each layer: 1, 2, 4, 8, 16, 32, ...\n",
    "                dilation = 2 ** l\n",
    "                \n",
    "                # Gated activation unit (dilated convolution)\n",
    "                self.dilated_convs.append(\n",
    "                    nn.Conv1d(residual_channels, gate_channels, kernel_size=3, \n",
    "                             padding=dilation, dilation=dilation)\n",
    "                )\n",
    "                \n",
    "                # 1x1 conv for residual connection\n",
    "                self.residual_convs.append(\n",
    "                    nn.Conv1d(gate_channels // 2, residual_channels, 1)\n",
    "                )\n",
    "                \n",
    "                # 1x1 conv for skip connection\n",
    "                self.skip_convs.append(\n",
    "                    nn.Conv1d(gate_channels // 2, skip_channels, 1)\n",
    "                )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        self.output_conv2 = nn.Conv1d(skip_channels, num_classes, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, input_channels, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            output: Output tensor [batch, num_classes, seq_len]\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)  # [batch, residual_channels, seq_len]\n",
    "        \n",
    "        # Skip connections accumulator\n",
    "        skip_connections = []\n",
    "        \n",
    "        # Apply dilated convolutions\n",
    "        layer_idx = 0\n",
    "        for b in range(self.num_blocks):\n",
    "            for l in range(self.num_layers):\n",
    "                # Gated activation: tanh(x) * sigmoid(x)\n",
    "                dilated_out = self.dilated_convs[layer_idx](x)\n",
    "                \n",
    "                # Split into two parts for gating\n",
    "                tanh_out = self.tanh(dilated_out[:, :dilated_out.shape[1]//2, :])\n",
    "                sigmoid_out = self.sigmoid(dilated_out[:, dilated_out.shape[1]//2:, :])\n",
    "                \n",
    "                gated_out = tanh_out * sigmoid_out\n",
    "                \n",
    "                # Residual connection\n",
    "                residual_out = self.residual_convs[layer_idx](gated_out)\n",
    "                x = x + residual_out\n",
    "                \n",
    "                # Skip connection\n",
    "                skip_out = self.skip_convs[layer_idx](gated_out)\n",
    "                skip_connections.append(skip_out)\n",
    "                \n",
    "                layer_idx += 1\n",
    "        \n",
    "        # Sum all skip connections\n",
    "        skip_sum = sum(skip_connections)  # [batch, skip_channels, seq_len]\n",
    "        \n",
    "        # Output layers\n",
    "        output = self.relu(skip_sum)\n",
    "        output = self.output_conv1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.output_conv2(output)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        output = F.softmax(output, dim=1)  # [batch, num_classes, seq_len]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test the WaveNet architecture\n",
    "print(\"Testing WaveNet architecture...\")\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 100\n",
    "input_channels = 2\n",
    "\n",
    "# Create dummy input (character-level start/end probabilities)\n",
    "dummy_input = torch.randn(batch_size, input_channels, seq_len)\n",
    "\n",
    "# Initialize model\n",
    "model = CharacterWaveNet(\n",
    "    input_channels=input_channels,\n",
    "    num_classes=2,\n",
    "    num_blocks=2,  # Reduced for testing\n",
    "    num_layers=4,   # Reduced for testing\n",
    "    residual_channels=16,\n",
    "    gate_channels=16,\n",
    "    skip_channels=16\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output sum (should be 1.0 per position): {output[0, :, 0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96404ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for character-level training\n",
    "class CharacterLevelDataset(Dataset):\n",
    "    def __init__(self, texts, sentiments, selected_texts, roberta_model, tokenizer, device='cpu'):\n",
    "        \"\"\"\n",
    "        Dataset for character-level WaveNet training.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of tweet texts\n",
    "            sentiments: List of sentiment labels\n",
    "            selected_texts: List of selected_text spans (targets)\n",
    "            roberta_model: Trained RoBERTa model for generating token probabilities\n",
    "            tokenizer: Tokenizer for text processing\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.sentiments = sentiments\n",
    "        self.selected_texts = selected_texts\n",
    "        self.roberta_model = roberta_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        selected_text = self.selected_texts[idx]\n",
    "        \n",
    "        # Get character-level start/end probabilities from RoBERTa\n",
    "        token_probs = extract_token_probabilities(\n",
    "            self.roberta_model, text, sentiment, self.tokenizer, self.device\n",
    "        )\n",
    "        \n",
    "        # Convert to character-level probabilities\n",
    "        char_start_probs = token_to_char_probabilities(\n",
    "            token_probs['start_probs'],\n",
    "            token_probs['offset_mapping'],\n",
    "            len(text)\n",
    "        )\n",
    "        char_end_probs = token_to_char_probabilities(\n",
    "            token_probs['end_probs'],\n",
    "            token_probs['offset_mapping'],\n",
    "            len(text)\n",
    "        )\n",
    "        \n",
    "        # Create character-level targets (binary: 1 if character is start/end of selected_text)\n",
    "        char_start_target = np.zeros(len(text))\n",
    "        char_end_target = np.zeros(len(text))\n",
    "        \n",
    "        # Find selected_text in original text\n",
    "        try:\n",
    "            start_idx = text.index(selected_text)\n",
    "            end_idx = start_idx + len(selected_text) - 1\n",
    "            \n",
    "            # Mark start and end positions\n",
    "            if start_idx < len(text):\n",
    "                char_start_target[start_idx] = 1.0\n",
    "            if end_idx < len(text):\n",
    "                char_end_target[end_idx] = 1.0\n",
    "        except ValueError:\n",
    "            # selected_text not found (shouldn't happen with clean data)\n",
    "            pass\n",
    "        \n",
    "        # Stack start and end probabilities as input features\n",
    "        char_features = np.stack([char_start_probs, char_end_probs], axis=0)  # [2, text_len]\n",
    "        char_targets = np.stack([char_start_target, char_end_target], axis=0)  # [2, text_len]\n",
    "        \n",
    "        return {\n",
    "            'features': torch.FloatTensor(char_features),\n",
    "            'targets': torch.FloatTensor(char_targets),\n",
    "            'text': text,\n",
    "            'selected_text': selected_text\n",
    "        }\n",
    "\n",
    "# Test the dataset\n",
    "print(\"Testing CharacterLevelDataset...\")\n",
    "\n",
    "# Use a small subset for testing\n",
    "test_texts = train_df['text'].iloc[:10].tolist()\n",
    "test_sentiments = train_df['sentiment'].iloc[:10].tolist()\n",
    "test_selected_texts = train_df['selected_text'].iloc[:10].tolist()\n",
    "\n",
    "# Create dataset\n",
    "dataset = CharacterLevelDataset(\n",
    "    texts=test_texts,\n",
    "    sentiments=test_sentiments,\n",
    "    selected_texts=test_selected_texts,\n",
    "    roberta_model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Test getting an item\n",
    "sample = dataset[0]\n",
    "print(f\"Sample features shape: {sample['features'].shape}\")\n",
    "print(f\"Sample targets shape: {sample['targets'].shape}\")\n",
    "print(f\"Text length: {len(sample['text'])}\")\n",
    "print(f\"Selected text: '{sample['selected_text']}'\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
