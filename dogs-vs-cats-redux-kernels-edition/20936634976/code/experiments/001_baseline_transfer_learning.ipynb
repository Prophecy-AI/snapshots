{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500db30a",
   "metadata": {},
   "source": [
    "# Baseline: Transfer Learning with ResNet\n",
    "\n",
    "This notebook implements a baseline using transfer learning with ResNet50 for the Dogs vs Cats classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "train_dir = '/home/data/train'\n",
    "test_dir = '/home/data/test'\n",
    "sample_submission_path = '/home/data/sample_submission.csv'\n",
    "\n",
    "# Load file names and labels\n",
    "train_files = os.listdir(train_dir)\n",
    "train_labels = [1 if 'dog' in f else 0 for f in train_files]  # 1=dog, 0=cat\n",
    "\n",
    "print(f\"Total training images: {len(train_files)}\")\n",
    "print(f\"Dog images: {sum(train_labels)}\")\n",
    "print(f\"Cat images: {len(train_labels) - sum(train_labels)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_files, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f43c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class DogsCatsDataset(Dataset):\n",
    "    def __init__(self, file_names, labels, data_dir, transform=None):\n",
    "        self.file_names = file_names\n",
    "        self.labels = labels\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.file_names[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataset = DogsCatsDataset(X_train, y_train, train_dir, transform=train_transform)\n",
    "val_dataset = DogsCatsDataset(X_val, y_val, train_dir, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903785cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze early layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57159fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = running_loss / len(loader.dataset)\n",
    "    val_logloss = log_loss(all_labels, all_preds)\n",
    "    \n",
    "    return val_loss, val_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 5\n",
    "best_logloss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_logloss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val LogLoss: {val_logloss:.4f}\")\n",
    "    \n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), '/home/code/models/best_model.pth')\n",
    "        print(\"Model saved!\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"Best Validation LogLoss: {best_logloss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('/home/code/models/best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions on test set\n",
    "test_files = sorted(os.listdir(test_dir))\n",
    "test_dataset = DogsCatsDataset(test_files, None, test_dir, transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        test_preds.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "print(f\"Generated predictions for {len(test_preds)} test images\")\n",
    "print(f\"Sample predictions: {test_preds[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': [int(f.split('.')[0]) for f in test_files],\n",
    "    'label': test_preds\n",
    "})\n",
    "\n",
    "submission = submission.sort_values('id')\n",
    "submission_path = '/home/submission/submission_001_baseline.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(submission.head())\n",
    "print(f\"\\nFinal validation LogLoss: {best_logloss:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
