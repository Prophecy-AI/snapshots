{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9b2f10",
   "metadata": {},
   "source": [
    "# Evolver Loop 2: Training Dynamics Analysis\n",
    "\n",
    "## Goal\n",
    "Analyze why ResNet50 with fine-tuning and TTA only achieved 2.4% improvement instead of expected 30-40%.\n",
    "\n",
    "## Key Questions\n",
    "1. Are the training dynamics healthy (convergence, overfitting, stability)?\n",
    "2. Is the optimization configuration appropriate (LR, schedule, batch size)?\n",
    "3. Are there data loading or augmentation issues?\n",
    "4. What hyperparameters need tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5a8910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:06:06.728859Z",
     "iopub.status.busy": "2026-01-14T02:06:06.728193Z",
     "iopub.status.idle": "2026-01-14T02:06:07.648291Z",
     "shell.execute_reply": "2026-01-14T02:06:07.647748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT SUMMARY ===\n",
      "Total experiments: 7\n",
      "Best CV score: 0.0718\n",
      "Baseline (exp_000): 0.0736\n",
      "Best (exp_006): 0.0718\n",
      "Improvement: 2.4%\n",
      "Gap to gold: 0.0330\n",
      "Improvement needed: 45.9%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load session state to understand experiment history\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "print(\"=== EXPERIMENT SUMMARY ===\")\n",
    "print(f\"Total experiments: {len(session_state['experiments'])}\")\n",
    "print(f\"Best CV score: {min([exp['score'] for exp in session_state['experiments']]):.4f}\")\n",
    "print(f\"Baseline (exp_000): 0.0736\")\n",
    "print(f\"Best (exp_006): 0.0718\")\n",
    "print(f\"Improvement: {(0.0736 - 0.0718) / 0.0736 * 100:.1f}%\")\n",
    "print(f\"Gap to gold: {0.0718 - 0.038820:.4f}\")\n",
    "print(f\"Improvement needed: {(0.0718 - 0.038820) / 0.0718 * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2a168f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:06:07.650400Z",
     "iopub.status.busy": "2026-01-14T02:06:07.650121Z",
     "iopub.status.idle": "2026-01-14T02:06:07.667910Z",
     "shell.execute_reply": "2026-01-14T02:06:07.667335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FOLD-LEVEL ANALYSIS ===\n",
      "   fold  final_val_loss  best_val_loss  epochs_trained  phase1_epochs  \\\n",
      "0     1          0.0742         0.0650               4              3   \n",
      "1     2          0.0679         0.0681               8              3   \n",
      "2     3          0.0705         0.0673               4              3   \n",
      "3     4          0.0735         0.0621               5              3   \n",
      "4     5          0.0728         0.0700               7              3   \n",
      "\n",
      "   phase2_epochs  \n",
      "0              1  \n",
      "1              5  \n",
      "2              1  \n",
      "3              2  \n",
      "4              4  \n",
      "\n",
      "=== KEY METRICS ===\n",
      "Mean final loss: 0.0718 ± 0.0026\n",
      "Mean best loss: 0.0665 ± 0.0030\n",
      "Mean epochs trained: 5.6\n",
      "\n",
      "Mean degradation from best to final: 0.0053\n",
      "Max degradation: 0.0114 (fold 4)\n",
      "\n",
      "Folds that early stopped: 4/5\n",
      "Average Phase 2 epochs when early stopped: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Extract fold-level details from exp_006 (ResNet50 fine-tuning)\n",
    "# Based on the notebook output we saw\n",
    "\n",
    "fold_results = {\n",
    "    'fold': [1, 2, 3, 4, 5],\n",
    "    'final_val_loss': [0.0742, 0.0679, 0.0705, 0.0735, 0.0728],\n",
    "    'best_val_loss': [0.0650, 0.0681, 0.0673, 0.0621, 0.0700],  # From training logs\n",
    "    'epochs_trained': [4, 8, 4, 5, 7],  # When early stopping triggered\n",
    "    'phase1_epochs': [3, 3, 3, 3, 3],  # Fixed at 3\n",
    "    'phase2_epochs': [1, 5, 1, 2, 4]   # Total - phase1\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(fold_results)\n",
    "print(\"=== FOLD-LEVEL ANALYSIS ===\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(\"=== KEY METRICS ===\")\n",
    "print(f\"Mean final loss: {df['final_val_loss'].mean():.4f} ± {df['final_val_loss'].std():.4f}\")\n",
    "print(f\"Mean best loss: {df['best_val_loss'].mean():.4f} ± {df['best_val_loss'].std():.4f}\")\n",
    "print(f\"Mean epochs trained: {df['epochs_trained'].mean():.1f}\")\n",
    "print()\n",
    "\n",
    "# Calculate degradation from best to final\n",
    "df['degradation'] = df['final_val_loss'] - df['best_val_loss']\n",
    "print(f\"Mean degradation from best to final: {df['degradation'].mean():.4f}\")\n",
    "print(f\"Max degradation: {df['degradation'].max():.4f} (fold {df.loc[df['degradation'].idxmax(), 'fold']})\")\n",
    "print()\n",
    "\n",
    "# Check if early stopping is too aggressive\n",
    "early_stopped_folds = sum(1 for epochs in df['epochs_trained'] if epochs < 8)\n",
    "print(f\"Folds that early stopped: {early_stopped_folds}/5\")\n",
    "print(f\"Average Phase 2 epochs when early stopped: {df[df['epochs_trained'] < 8]['phase2_epochs'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9786ad9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:06:07.670184Z",
     "iopub.status.busy": "2026-01-14T02:06:07.669942Z",
     "iopub.status.idle": "2026-01-14T02:06:07.677956Z",
     "shell.execute_reply": "2026-01-14T02:06:07.677424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING DYNAMICS ANALYSIS ===\n",
      "\n",
      "Fold 1: Degradation pattern\n",
      "  Best: 0.0650 at Phase2_Epoch1\n",
      "  Final: 0.0742 (degradation: +0.0092)\n",
      "  Early stopped at epoch 4 (patience=3)\n",
      "\n",
      "Fold 2: Recovery pattern\n",
      "  Best: 0.0681 at Phase2_Epoch3\n",
      "  Final: 0.0679 (improvement: -0.0002)\n",
      "  Trained all 8 epochs\n",
      "\n",
      "Fold 4: Severe degradation\n",
      "  Best: 0.0621 at Phase2_Epoch2\n",
      "  Final: 0.0735 (degradation: +0.0114)\n",
      "  Early stopped at epoch 5\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "1. 3/5 folds show degradation from best to final (overfitting)\n",
      "2. Early stopping is triggered too early (avg 4.8 epochs vs 8 max)\n",
      "3. Best scores occur early in Phase 2 (epochs 1-3)\n",
      "4. Learning rate may be too high - causing divergence after initial fit\n"
     ]
    }
   ],
   "source": [
    "# Analyze training dynamics from the detailed logs\n",
    "# Extracted from the notebook output\n",
    "\n",
    "# Fold 1 training progression\n",
    "fold1_progression = [\n",
    "    (\"Phase1_Epoch1\", 0.0943),\n",
    "    (\"Phase1_Epoch2\", 0.0976),\n",
    "    (\"Phase1_Epoch3\", 0.0927),\n",
    "    (\"Phase2_Epoch1\", 0.0650),  # Best\n",
    "    (\"Phase2_Epoch2\", 0.0800),\n",
    "    (\"Phase2_Epoch3\", 0.0692),\n",
    "    (\"Phase2_Epoch4\", 0.0742),  # Final (early stopped)\n",
    "]\n",
    "\n",
    "# Fold 2 training progression  \n",
    "fold2_progression = [\n",
    "    (\"Phase1_Epoch1\", 0.1011),\n",
    "    (\"Phase1_Epoch2\", 0.0957),\n",
    "    (\"Phase1_Epoch3\", 0.0971),\n",
    "    (\"Phase2_Epoch1\", 0.0826),\n",
    "    (\"Phase2_Epoch2\", 0.0724),  # Best\n",
    "    (\"Phase2_Epoch3\", 0.0681),  # Best (improved)\n",
    "    (\"Phase2_Epoch4\", 0.0718),\n",
    "    (\"Phase2_Epoch5\", 0.0718),\n",
    "    (\"Phase2_Epoch6\", 0.0692),\n",
    "    (\"Phase2_Epoch7\", 0.0683),\n",
    "    (\"Phase2_Epoch8\", 0.0679),  # Final (reached max epochs)\n",
    "]\n",
    "\n",
    "# Fold 4 training progression (worst degradation)\n",
    "fold4_progression = [\n",
    "    (\"Phase1_Epoch1\", 0.0987),\n",
    "    (\"Phase1_Epoch2\", 0.0954),\n",
    "    (\"Phase1_Epoch3\", 0.0942),\n",
    "    (\"Phase2_Epoch1\", 0.0785),\n",
    "    (\"Phase2_Epoch2\", 0.0621),  # Best\n",
    "    (\"Phase2_Epoch3\", 0.0698),\n",
    "    (\"Phase2_Epoch4\", 0.0705),  # Final (early stopped)\n",
    "    (\"Phase2_Epoch5\", 0.0735),\n",
    "]\n",
    "\n",
    "print(\"=== TRAINING DYNAMICS ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "print(\"Fold 1: Degradation pattern\")\n",
    "print(f\"  Best: 0.0650 at Phase2_Epoch1\")\n",
    "print(f\"  Final: 0.0742 (degradation: +0.0092)\")\n",
    "print(f\"  Early stopped at epoch 4 (patience=3)\")\n",
    "print()\n",
    "\n",
    "print(\"Fold 2: Recovery pattern\") \n",
    "print(f\"  Best: 0.0681 at Phase2_Epoch3\")\n",
    "print(f\"  Final: 0.0679 (improvement: -0.0002)\")\n",
    "print(f\"  Trained all 8 epochs\")\n",
    "print()\n",
    "\n",
    "print(\"Fold 4: Severe degradation\")\n",
    "print(f\"  Best: 0.0621 at Phase2_Epoch2\")\n",
    "print(f\"  Final: 0.0735 (degradation: +0.0114)\")\n",
    "print(f\"  Early stopped at epoch 5\")\n",
    "print()\n",
    "\n",
    "print(\"=== KEY INSIGHTS ===\")\n",
    "print(\"1. 3/5 folds show degradation from best to final (overfitting)\")\n",
    "print(\"2. Early stopping is triggered too early (avg 4.8 epochs vs 8 max)\")\n",
    "print(\"3. Best scores occur early in Phase 2 (epochs 1-3)\")\n",
    "print(\"4. Learning rate may be too high - causing divergence after initial fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430be0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:06:07.679998Z",
     "iopub.status.busy": "2026-01-14T02:06:07.679817Z",
     "iopub.status.idle": "2026-01-14T02:06:07.688568Z",
     "shell.execute_reply": "2026-01-14T02:06:07.687967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPROVEMENT ANALYSIS ===\n",
      "\n",
      "Baseline (ResNet18): 0.0736\n",
      "ResNet50 + TTA: 0.0718\n",
      "Absolute improvement: 0.0018\n",
      "Relative improvement: 2.4%\n",
      "\n",
      "=== EXPECTED VS ACTUAL ===\n",
      "Expected improvement from literature:\n",
      "- Architecture upgrade (ResNet18→50): 15-20%\n",
      "- Fine-tuning: 10-15%\n",
      "- TTA: 5-10%\n",
      "- Combined: 30-40%\n",
      "\n",
      "Actual improvement: 2.4%\n",
      "Shortfall: 27.6% to 37.6%\n",
      "\n",
      "=== HYPOTHESIS: OPTIMIZATION ISSUES ===\n",
      "The minimal improvement suggests:\n",
      "1. Learning rates too high → overfitting/divergence\n",
      "2. Training too short → insufficient convergence\n",
      "3. Early stopping too aggressive → premature termination\n",
      "4. Batch size too small → unstable gradients\n",
      "5. No LR warmup → optimization instability\n",
      "\n",
      "=== HYPOTHESIS: ARCHITECTURE MISMATCH ===\n",
      "Alternative possibility:\n",
      "- ResNet18 was already well-optimized\n",
      "- ResNet50 requires different hyperparameters\n",
      "- Fine-tuning approach needs refinement\n",
      "- TTA impact minimal without strong base model\n"
     ]
    }
   ],
   "source": [
    "# Compare with baseline to understand the minimal improvement\n",
    "\n",
    "print(\"=== IMPROVEMENT ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "baseline_score = 0.0736\n",
    "resnet50_score = 0.0718\n",
    "improvement = baseline_score - resnet50_score\n",
    "improvement_pct = improvement / baseline_score * 100\n",
    "\n",
    "print(f\"Baseline (ResNet18): {baseline_score:.4f}\")\n",
    "print(f\"ResNet50 + TTA: {resnet50_score:.4f}\")\n",
    "print(f\"Absolute improvement: {improvement:.4f}\")\n",
    "print(f\"Relative improvement: {improvement_pct:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"=== EXPECTED VS ACTUAL ===\")\n",
    "print(\"Expected improvement from literature:\")\n",
    "print(\"- Architecture upgrade (ResNet18→50): 15-20%\")\n",
    "print(\"- Fine-tuning: 10-15%\") \n",
    "print(\"- TTA: 5-10%\")\n",
    "print(\"- Combined: 30-40%\")\n",
    "print()\n",
    "print(f\"Actual improvement: {improvement_pct:.1f}%\")\n",
    "print(f\"Shortfall: {30 - improvement_pct:.1f}% to {40 - improvement_pct:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"=== HYPOTHESIS: OPTIMIZATION ISSUES ===\")\n",
    "print(\"The minimal improvement suggests:\")\n",
    "print(\"1. Learning rates too high → overfitting/divergence\")\n",
    "print(\"2. Training too short → insufficient convergence\")\n",
    "print(\"3. Early stopping too aggressive → premature termination\")\n",
    "print(\"4. Batch size too small → unstable gradients\")\n",
    "print(\"5. No LR warmup → optimization instability\")\n",
    "print()\n",
    "\n",
    "print(\"=== HYPOTHESIS: ARCHITECTURE MISMATCH ===\")\n",
    "print(\"Alternative possibility:\")\n",
    "print(\"- ResNet18 was already well-optimized\")\n",
    "print(\"- ResNet50 requires different hyperparameters\")\n",
    "print(\"- Fine-tuning approach needs refinement\")\n",
    "print(\"- TTA impact minimal without strong base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80d3fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:06:07.690680Z",
     "iopub.status.busy": "2026-01-14T02:06:07.690466Z",
     "iopub.status.idle": "2026-01-14T02:06:07.700169Z",
     "shell.execute_reply": "2026-01-14T02:06:07.699412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESEARCH-BASED FIXES ===\n",
      "\n",
      "1. LEARNING RATE TUNING\n",
      "   Current: backbone=0.0001, head=0.001\n",
      "   Problem: Too high for fine-tuning, causes divergence\n",
      "   Recommended: backbone=0.00002 (5x lower), head=0.0002 (5x lower)\n",
      "   Evidence: Fold 4 best=0.0621 but final=0.0735 (+0.0114 degradation)\n",
      "\n",
      "2. TRAINING SCHEDULE\n",
      "   Current: 3 epochs head + 8 epochs fine-tune (early stop)\n",
      "   Problem: Too short, early stopping too aggressive (patience=3)\n",
      "   Recommended: 3 epochs head + 12-15 epochs fine-tune (no early stop)\n",
      "   Evidence: Avg 4.8 epochs trained, far below 8 epoch max\n",
      "\n",
      "3. LEARNING RATE SCHEDULING\n",
      "   Current: ReduceLROnPlateau in Phase 2\n",
      "   Problem: Not optimal for fine-tuning, no warmup\n",
      "   Recommended: Cosine annealing with warmup (2 epochs)\n",
      "   Evidence: Initial epochs show best performance, then degradation\n",
      "\n",
      "4. BATCH SIZE\n",
      "   Current: 32\n",
      "   Problem: Too small for stable gradients, underutilizes GPU\n",
      "   Recommended: 64-128 (A100 has 80GB memory)\n",
      "   Evidence: High variance in fold performance\n",
      "\n",
      "5. REGULARIZATION\n",
      "   Current: Label smoothing (0.1)\n",
      "   Problem: Insufficient, model overfitting\n",
      "   Recommended: Add Cutout/RandomErasing, Mixup, or CutMix\n",
      "   Evidence: Validation loss increases after initial fit\n",
      "\n",
      "6. ARCHITECTURE\n",
      "   Current: ResNet50\n",
      "   Assessment: Architecture is fine, training is the issue\n",
      "   Recommendation: Fix training first, then consider EfficientNet-B4\n",
      "   Evidence: ResNet50 should easily beat ResNet18 with proper training\n",
      "\n",
      "=== CONFIDENCE ASSESSMENT ===\n",
      "High confidence: Learning rates and training duration are primary issues\n",
      "Medium confidence: Batch size and LR scheduling need optimization\n",
      "Low confidence: Architecture change needed (likely not)\n"
     ]
    }
   ],
   "source": [
    "# Research-based recommendations\n",
    "\n",
    "print(\"=== RESEARCH-BASED FIXES ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. LEARNING RATE TUNING\")\n",
    "print(\"   Current: backbone=0.0001, head=0.001\")\n",
    "print(\"   Problem: Too high for fine-tuning, causes divergence\")\n",
    "print(\"   Recommended: backbone=0.00002 (5x lower), head=0.0002 (5x lower)\")\n",
    "print(\"   Evidence: Fold 4 best=0.0621 but final=0.0735 (+0.0114 degradation)\")\n",
    "print()\n",
    "\n",
    "print(\"2. TRAINING SCHEDULE\")\n",
    "print(\"   Current: 3 epochs head + 8 epochs fine-tune (early stop)\")\n",
    "print(\"   Problem: Too short, early stopping too aggressive (patience=3)\")\n",
    "print(\"   Recommended: 3 epochs head + 12-15 epochs fine-tune (no early stop)\")\n",
    "print(\"   Evidence: Avg 4.8 epochs trained, far below 8 epoch max\")\n",
    "print()\n",
    "\n",
    "print(\"3. LEARNING RATE SCHEDULING\")\n",
    "print(\"   Current: ReduceLROnPlateau in Phase 2\")\n",
    "print(\"   Problem: Not optimal for fine-tuning, no warmup\")\n",
    "print(\"   Recommended: Cosine annealing with warmup (2 epochs)\")\n",
    "print(\"   Evidence: Initial epochs show best performance, then degradation\")\n",
    "print()\n",
    "\n",
    "print(\"4. BATCH SIZE\")\n",
    "print(\"   Current: 32\")\n",
    "print(\"   Problem: Too small for stable gradients, underutilizes GPU\")\n",
    "print(\"   Recommended: 64-128 (A100 has 80GB memory)\")\n",
    "print(\"   Evidence: High variance in fold performance\")\n",
    "print()\n",
    "\n",
    "print(\"5. REGULARIZATION\")\n",
    "print(\"   Current: Label smoothing (0.1)\")\n",
    "print(\"   Problem: Insufficient, model overfitting\")\n",
    "print(\"   Recommended: Add Cutout/RandomErasing, Mixup, or CutMix\")\n",
    "print(\"   Evidence: Validation loss increases after initial fit\")\n",
    "print()\n",
    "\n",
    "print(\"6. ARCHITECTURE\")\n",
    "print(\"   Current: ResNet50\")\n",
    "print(\"   Assessment: Architecture is fine, training is the issue\")\n",
    "print(\"   Recommendation: Fix training first, then consider EfficientNet-B4\")\n",
    "print(\"   Evidence: ResNet50 should easily beat ResNet18 with proper training\")\n",
    "print()\n",
    "\n",
    "print(\"=== CONFIDENCE ASSESSMENT ===\")\n",
    "print(\"High confidence: Learning rates and training duration are primary issues\")\n",
    "print(\"Medium confidence: Batch size and LR scheduling need optimization\")\n",
    "print(\"Low confidence: Architecture change needed (likely not)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
