{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb80513e",
   "metadata": {},
   "source": [
    "# ResNet50 + Mixup Training\n",
    "\n",
    "## Objective\n",
    "Train ResNet50 with Mixup regularization using the proven optimization recipe from exp_007.\n",
    "This creates a diverse model for ensembling with EfficientNet-B4 (exp_008).\n",
    "\n",
    "## Expected Results\n",
    "- Target CV: 0.055-0.057 (improvement over ResNet50 baseline of 0.0590)\n",
    "- Creates diverse architecture for two-model ensemble\n",
    "- Ensemble target: 0.032-0.034 (additional 5-10% improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb58fa",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711115be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_dir = '/home/data/train'\n",
    "test_dir = '/home/data/test'\n",
    "\n",
    "train_files = []\n",
    "train_labels = []\n",
    "\n",
    "for label, category in enumerate(['cat', 'dog']):\n",
    "    category_path = os.path.join(train_dir, category)\n",
    "    for img_file in os.listdir(category_path):\n",
    "        if img_file.endswith('.jpg'):\n",
    "            train_files.append(os.path.join(category_path, img_file))\n",
    "            train_labels.append(label)\n",
    "\n",
    "print(f\"Total training images: {len(train_files)}\")\n",
    "print(f\"Cats: {sum(1 for x in train_labels if x == 0)}\")\n",
    "print(f\"Dogs: {sum(1 for x in train_labels if x == 1)}\")\n",
    "\n",
    "# Test files\n",
    "test_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.jpg')]\n",
    "print(f\"Total test images: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mixup function\n",
    "class Mixup:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, batch_x, batch_y):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "        \n",
    "        batch_size = batch_x.size(0)\n",
    "        index = torch.randperm(batch_size).cuda() if batch_x.is_cuda else torch.randperm(batch_size)\n",
    "        \n",
    "        mixed_x = lam * batch_x + (1 - lam) * batch_x[index]\n",
    "        y_a, y_b = batch_y, batch_y[index]\n",
    "        \n",
    "        return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# Custom loss for Mixup\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83074db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset with Mixup\n",
    "class DogCatDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None, is_training=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.15)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tta_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ed7d0",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65415236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogCatClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DogCatClassifier, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Freeze all layers initially\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize new layer\n",
    "        nn.init.xavier_normal_(self.backbone.fc[1].weight)\n",
    "        nn.init.constant_(self.backbone.fc[1].bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def unfreeze_layers(self, layer_names):\n",
    "        \"\"\"Unfreeze specific layers for fine-tuning\"\"\"\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            for layer_name in layer_names:\n",
    "                if layer_name in name:\n",
    "                    param.requires_grad = True\n",
    "                    break\n",
    "\n",
    "# Learning rate setup\n",
    "def get_optimizer_params(model, backbone_lr, head_lr):\n",
    "    \"\"\"Separate parameters for different learning rates\"\"\"\n",
    "    backbone_params = []\n",
    "    head_params = []\n",
    "    \n",
    "    # Head parameters (final classifier)\n",
    "    for name, param in model.backbone.fc.named_parameters():\n",
    "        head_params.append(param)\n",
    "    \n",
    "    # Backbone parameters\n",
    "    for name, param in model.backbone.named_parameters():\n",
    "        if 'fc' not in name:  # Exclude head\n",
    "            backbone_params.append(param)\n",
    "    \n",
    "    return [\n",
    "        {'params': backbone_params, 'lr': backbone_lr},\n",
    "        {'params': head_params, 'lr': head_lr}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfd4f6",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aef99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device, mixup_fn, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        data, target_a, target_b, lam = mixup_fn(data, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(torch.sigmoid(output).cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    logloss = log_loss(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, logloss, all_preds\n",
    "\n",
    "def predict_with_tta(model, test_loader, device, n_augmentations=5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            batch_preds = []\n",
    "            \n",
    "            for _ in range(n_augmentations):\n",
    "                output = model(data).squeeze()\n",
    "                batch_preds.append(torch.sigmoid(output).cpu().numpy())\n",
    "            \n",
    "            # Average across augmentations\n",
    "            avg_pred = np.mean(batch_preds, axis=0)\n",
    "            all_preds.extend(avg_pred)\n",
    "    \n",
    "    return np.array(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1468560",
   "metadata": {},
   "source": [
    "## Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold stratified CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "fold_models = []\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS_HEAD = 3\n",
    "EPOCHS_FINETUNE = 12\n",
    "BATCH_SIZE = 64\n",
    "BACKBONE_LR = 0.00002\n",
    "HEAD_LR = 0.0002\n",
    "WARMUP_EPOCHS = 2\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"- Epochs: {EPOCHS_HEAD} (head) + {EPOCHS_FINETUNE} (fine-tune) = {EPOCHS_HEAD + EPOCHS_FINETUNE} total\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Learning rates: backbone={BACKBONE_LR}, head={HEAD_LR}\")\n",
    "print(f\"- Mixup alpha: 0.2\")\n",
    "print(f\"- TTA augmentations: 5\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_files, train_labels)):\n",
    "    print(f\"\\nFold {fold + 1}/5\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_files_fold = [train_files[i] for i in train_idx]\n",
    "    train_labels_fold = [train_labels[i] for i in train_idx]\n",
    "    val_files_fold = [train_files[i] for i in val_idx]\n",
    "    val_labels_fold = [train_labels[i] for i in val_idx]\n",
    "    \n",
    "    train_dataset = DogCatDataset(train_files_fold, train_labels_fold, train_transform, is_training=True)\n",
    "    val_dataset = DogCatDataset(val_files_fold, val_labels_fold, val_transform, is_training=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DogCatClassifier().cuda()\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    mixup_fn = Mixup(alpha=0.2)\n",
    "    \n",
    "    # Phase 1: Train head only\n",
    "    print(\"Phase 1: Training head only...\")\n",
    "    head_params = model.backbone.fc.parameters()\n",
    "    optimizer_head = optim.AdamW(head_params, lr=HEAD_LR, weight_decay=0.05)\n",
    "    \n",
    "    for epoch in range(EPOCHS_HEAD):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer_head, criterion, 'cuda', mixup_fn)\n",
    "        val_loss, val_logloss, _ = validate(model, val_loader, criterion, 'cuda')\n",
    "        print(f\"  Epoch {epoch+1}/{EPOCHS_HEAD} - Train Loss: {train_loss:.4f}, Val LogLoss: {val_logloss:.4f}\")\n",
    "    \n",
    "    # Phase 2: Fine-tune all layers\n",
    "    print(\"Phase 2: Fine-tuning all layers...\")\n",
    "    model.unfreeze_layers(['layer3', 'layer4'])\n",
    "    \n",
    "    optimizer_params = get_optimizer_params(model, BACKBONE_LR, HEAD_LR)\n",
    "    optimizer_ft = optim.AdamW(optimizer_params, weight_decay=0.05)\n",
    "    \n",
    "    # Cosine annealing with warmup\n",
    "    total_steps = EPOCHS_FINETUNE * len(train_loader)\n",
    "    warmup_steps = WARMUP_EPOCHS * len(train_loader)\n",
    "    \n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer_ft, lr_lambda)\n",
    "    \n",
    "    best_logloss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(EPOCHS_FINETUNE):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer_ft, criterion, 'cuda', mixup_fn, scheduler)\n",
    "        val_loss, val_logloss, _ = validate(model, val_loader, criterion, 'cuda')\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{EPOCHS_FINETUNE} - Train Loss: {train_loss:.4f}, Val LogLoss: {val_logloss:.4f}\")\n",
    "        \n",
    "        if val_logloss < best_logloss:\n",
    "            best_logloss = val_logloss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    fold_scores.append(best_logloss)\n",
    "    fold_models.append(model)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Best LogLoss: {best_logloss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Cross-validation completed!\")\n",
    "print(f\"Mean LogLoss: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in fold_scores]}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
