## Current Status
- Best CV: 0.0590 from exp_007 (ResNet50 with optimization fixes)
- Gap to gold: 0.0202 (34% improvement still needed)
- Experiments above gold: 0

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY**. I fully agree - the optimization fixes were correctly implemented and yielded genuine 17.8% improvement.
- **Evaluator's top priority: Train EfficientNet-B4 and begin ensembling**. I strongly agree and will prioritize this exact approach.
- **Key concerns raised**:
  - Need 34% more improvement to reach gold → Address with architecture upgrade + ensembling
  - ResNet50 is older architecture → EfficientNet-B4 is more parameter-efficient and modern
  - Single model approach limited → Ensembling provides reliable 5-10% gains
  - Regularization could be stronger → Add Mixup to proven RandomErasing

## Data Understanding
- **Reference notebooks**: See `exploration/evolver_loop3_analysis.ipynb` for detailed analysis
- **Key patterns to exploit**:
  - Optimization recipe is now proven: 5x lower LRs, 15-epoch training, cosine annealing, batch size 64
  - Training stable across all folds (σ=0.0012) - recipe is reproducible
  - ResNet50 achieved 0.0590, proving architecture is capable when properly trained
  - Gap to gold is 0.0202 - need combination of architecture upgrade + ensembling

## Recommended Approaches (Priority Order)

### 1. EfficientNet-B4 Architecture Upgrade (HIGHEST PRIORITY)
**What**: Train EfficientNet-B4 using same optimized hyperparameters from exp_007
**Why**: 
- EfficientNet-B4 achieves 82.9% ImageNet Top-1 vs ResNet50's 76.2% (+6.7% absolute)
- Uses 19.3M parameters vs ResNet50's 25.6M (25% fewer params, better efficiency)
- Compound scaling (depth, width, resolution) extracts more informative features
- Expected gain: 0.0040 (7% relative improvement) based on architecture superiority
**How**:
- Use identical training recipe: 15 epochs (3 head + 12 fine-tune), batch size 64
- Learning rates: backbone=0.00002, head=0.0002 (same 10:1 ratio)
- Cosine annealing with 2-epoch warmup
- Label smoothing (0.1) + RandomErasing (p=0.25) + add Mixup (alpha=0.2)
- Progressive unfreezing: Start with head-only (3 epochs), then unfreeze all layers
- Target score: 0.054-0.056 (8-10% improvement from current)

### 2. Two-Model Ensemble (HIGH PRIORITY - Parallel Track)
**What**: Average predictions from ResNet50 (exp_007) and EfficientNet-B4
**Why**: 
- Most reliable path to improvement - ensembling consistently provides 5-10% gains
- Models have different architectures → high diversity → better ensemble
- Simple averaging is robust and easy to implement
- Expected gain: 0.0030 (5% relative improvement)
**How**:
- Train EfficientNet-B4 as described above
- Generate predictions from both models on test set using TTA (5 augmentations each)
- Average predictions: (pred_resnet50 + pred_efficientnet) / 2
- Optionally optimize weights via CV but simple averaging usually works
- Target ensemble score: 0.051-0.053 (combined improvement)

### 3. Mixup Regularization (MEDIUM PRIORITY)
**What**: Add Mixup augmentation (alpha=0.2) to training pipeline
**Why**: 
- Strong regularization proven effective for image classification
- Works synergistically with label smoothing
- Easy to implement, low risk
- Expected gain: 0.0020 (3% relative improvement)
**How**:
- Implement Mixup: lambda ~ Beta(0.2, 0.2)
- Mix images and labels: x' = lambda*x1 + (1-lambda)*x2, y' = lambda*y1 + (1-lambda)*y2
- Apply with probability 0.5 during training
- Keep existing augmentations (RandomErasing, etc.)
- Try on both ResNet50 and EfficientNet-B4

### 4. Progressive Unfreezing Refinement (MEDIUM PRIORITY)
**What**: More gradual unfreezing strategy
**Why**: Current approach unfreezes all layers at once. More gradual approach may allow better adaptation
**How**:
- Phase 1: 3 epochs - head only (all backbone frozen)
- Phase 2: 4 epochs - unfreeze layer4 only
- Phase 3: 4 epochs - unfreeze layer3 + layer4
- Use differential LRs: deeper layers get lower LRs
- Expected gain: 0.0010 (2% relative improvement)

### 5. Advanced TTA & Test-Time Augmentation (LOW PRIORITY)
**What**: Increase TTA augmentations from 5 to 10, add test-time weight averaging
**Why**: More predictions = lower variance, weight averaging smooths predictions
**How**:
- TTA: 10 augmentations per image (more diversity)
- Test-time weight averaging: Average predictions from last 3 epochs
- Expected combined gain: 0.0025 (4% relative improvement)

## What NOT to Try
- **New architectures beyond EfficientNet**: Don't jump to ViT, RegNet, etc. yet - EfficientNet-B4 is the logical next step
- **Complex hyperparameter sweeps**: The optimization recipe is proven - just transfer it
- **AutoML**: Not needed, manual tuning based on proven recipe is more efficient
- **Three-model ensembles**: Start with 2 models, add third only if needed later
- **Progressive resizing**: Overkill for this problem, focus on core improvements

## Validation Notes
- **CV Scheme**: Continue 5-fold stratified CV (proven stable)
- **Confidence**: High - EfficientNet + ensembling are well-established techniques
- **Risk**: Low - building on proven foundation
- **Target**: Aim for 0.051-0.053 with ensemble (next 2-3 experiments)
- **Success metrics**: 
  - EfficientNet-B4 single model: < 0.056
  - Two-model ensemble: < 0.053
  - Low variance across folds (σ < 0.002)

## Expected Timeline
- **Experiment 1**: EfficientNet-B4 single model (target: 0.054-0.056)
- **Experiment 2**: Two-model ensemble (target: 0.051-0.053)
- **Experiment 3**: Add Mixup and refinements (target: < 0.050)
- **Decision point**: Submit if ensemble exceeds gold, else continue with 3-model ensemble