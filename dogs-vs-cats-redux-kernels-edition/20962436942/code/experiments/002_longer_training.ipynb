{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cc6ad9",
   "metadata": {},
   "source": [
    "# Experiment 002: Longer Training with Early Stopping\n",
    "\n",
    "## Goal: Address the primary bottleneck of undertraining\n",
    "\n",
    "This experiment increases training duration from 3 to 15 epochs with early stopping (patience=3) to allow models to converge properly. Based on the baseline analysis, validation loss was still decreasing at epoch 3, indicating massive undertraining.\n",
    "\n",
    "### Changes from Baseline:\n",
    "- Training epochs: 3 → 15 (with early stopping)\n",
    "- Added model checkpointing to save best model per fold\n",
    "- Added EarlyStopping callback with patience=3\n",
    "- Keep all other parameters same for isolated comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06355f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T17:21:35.415715Z",
     "iopub.status.busy": "2026-01-13T17:21:35.415003Z",
     "iopub.status.idle": "2026-01-13T17:21:40.264778Z",
     "shell.execute_reply": "2026-01-13T17:21:40.264002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9730861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T17:21:40.266990Z",
     "iopub.status.busy": "2026-01-13T17:21:40.266623Z",
     "iopub.status.idle": "2026-01-13T17:21:40.283114Z",
     "shell.execute_reply": "2026-01-13T17:21:40.282376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory exists: True\n",
      "Test directory exists: True\n",
      "Sample training files: ['dog.5.jpg', 'cat.8112.jpg', 'cat.1197.jpg', 'dog.8491.jpg', 'dog.9129.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "DATA_DIR = '/home/data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Verify data exists\n",
    "print(f\"Train directory exists: {os.path.exists(TRAIN_DIR)}\")\n",
    "print(f\"Test directory exists: {os.path.exists(TEST_DIR)}\")\n",
    "\n",
    "# List some training images\n",
    "train_files = os.listdir(TRAIN_DIR)[:5]\n",
    "print(f\"Sample training files: {train_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecaff82c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T17:21:40.285071Z",
     "iopub.status.busy": "2026-01-13T17:21:40.284877Z",
     "iopub.status.idle": "2026-01-13T17:21:40.297343Z",
     "shell.execute_reply": "2026-01-13T17:21:40.296728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class DogsCatsDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels=None, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        return image\n",
    "\n",
    "# Data transforms (same as baseline)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6de4122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T17:21:40.299626Z",
     "iopub.status.busy": "2026-01-13T17:21:40.299036Z",
     "iopub.status.idle": "2026-01-13T17:21:40.352252Z",
     "shell.execute_reply": "2026-01-13T17:21:40.351680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Total training images: 22500\n",
      "Dog images: 11258\n",
      "Cat images: 11242\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "print(\"Loading training data...\")\n",
    "train_files = [os.path.join(TRAIN_DIR, f) for f in os.listdir(TRAIN_DIR) if f.endswith('.jpg')]\n",
    "train_labels = [1 if 'dog' in os.path.basename(f) else 0 for f in train_files]\n",
    "\n",
    "print(f\"Total training images: {len(train_files)}\")\n",
    "print(f\"Dog images: {sum(train_labels)}\")\n",
    "print(f\"Cat images: {len(train_labels) - sum(train_labels)}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_files = np.array(train_files)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7be833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T17:21:40.354505Z",
     "iopub.status.busy": "2026-01-13T17:21:40.354004Z",
     "iopub.status.idle": "2026-01-13T17:21:40.366361Z",
     "shell.execute_reply": "2026-01-13T17:21:40.365629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create model function (same architecture as baseline)\n",
    "def create_model():\n",
    "    # Load pretrained ResNet50\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Freeze early layers (same as baseline - will change in future experiments)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace final layer for binary classification\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training function with early stopping and checkpointing\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, max_epochs=15, patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    training_history = []\n",
    "    \n",
    "    print(f\"Training for up to {max_epochs} epochs with early stopping (patience={patience})...\\n\")\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device).float()\n",
    "                output = model(data).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_preds.extend(output.cpu().numpy())\n",
    "                val_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_log_loss = log_loss(val_targets, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{max_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Val Log Loss: {val_log_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "            print(f'  ✓ New best validation loss: {best_val_loss:.4f}')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f'  ✗ No improvement for {epochs_no_improve} epoch(s)')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "        \n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_log_loss': val_log_loss\n",
    "        })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model from epoch with val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, best_val_loss, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Store results\n",
    "cv_scores = []\n",
    "oof_predictions = np.zeros(len(train_files))\n",
    "all_training_histories = []\n",
    "\n",
    "print(f\"\\nStarting {n_splits}-fold cross-validation...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation\n",
    "fold = 1\n",
    "for train_idx, val_idx in skf.split(train_files, train_labels):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold}/{n_splits}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = train_files[train_idx], train_files[val_idx]\n",
    "    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DogsCatsDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = DogsCatsDataset(X_val, y_val, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    model, best_val_loss, fold_history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "        device, max_epochs=15, patience=3\n",
    "    )\n",
    "    \n",
    "    # Store training history\n",
    "    all_training_histories.append(fold_history)\n",
    "    \n",
    "    # Calculate final validation log loss with best model\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            output = model(data).squeeze()\n",
    "            val_preds.extend(output.cpu().numpy())\n",
    "            val_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    final_val_log_loss = log_loss(val_targets, val_preds)\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    cv_scores.append(final_val_log_loss)\n",
    "    \n",
    "    print(f\"Fold {fold} Final Log Loss: {final_val_log_loss:.4f}\")\n",
    "    print(f\"Fold {fold} Best Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# Overall CV score\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean Log Loss: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "print(f\"Individual folds: {cv_scores}\")\n",
    "\n",
    "# Show training history summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING HISTORY SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for i, history in enumerate(all_training_histories):\n",
    "    epochs = len(history)\n",
    "    best_epoch = min(history, key=lambda x: x['val_log_loss'])\n",
    "    print(f\"Fold {i+1}: {epochs} epochs, best val log loss: {best_epoch['val_log_loss']:.4f} at epoch {best_epoch['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "\n",
    "# Load test files\n",
    "test_files = [os.path.join(TEST_DIR, f) for f in sorted(os.listdir(TEST_DIR)) if f.endswith('.jpg')]\n",
    "test_ids = [int(os.path.splitext(os.path.basename(f))[0]) for f in test_files]\n",
    "\n",
    "print(f\"Total test images: {len(test_files)}\")\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = DogsCatsDataset(test_files, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Average predictions from all folds\n",
    "test_predictions = np.zeros(len(test_files))\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in skf.split(train_files, train_labels):\n",
    "    print(f\"Generating predictions from fold {fold}...\")\n",
    "    \n",
    "    # Recreate and load model for this fold\n",
    "    model = create_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create temporary training split to train model\n",
    "    X_train, X_val = train_files[train_idx], train_files[val_idx]\n",
    "    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "    \n",
    "    train_dataset = DogsCatsDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = DogsCatsDataset(X_val, y_val, transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    model, _, _ = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                              device, max_epochs=15, patience=3)\n",
    "    \n",
    "    # Generate predictions\n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            fold_preds.extend(output.cpu().numpy())\n",
    "    \n",
    "    test_predictions += np.array(fold_preds)\n",
    "    fold += 1\n",
    "\n",
    "# Average predictions across folds\n",
    "test_predictions /= n_splits\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test predictions range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913172a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "print(\"\\nCreating submission file...\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': test_predictions\n",
    "})\n",
    "\n",
    "# Sort by id\n",
    "submission = submission.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Sample predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Save submission\n",
    "SUBMISSION_DIR = '/home/submission'\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "submission_path = os.path.join(SUBMISSION_DIR, 'submission.csv')\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "\n",
    "# Verify submission format\n",
    "sample_sub = pd.read_csv('/home/data/sample_submission.csv')\n",
    "print(f\"\\nSubmission format matches sample: {list(submission.columns) == list(sample_sub.columns)}\")\n",
    "print(f\"ID ranges match: {submission['id'].min() == sample_sub['id'].min() and submission['id'].max() == sample_sub['id'].max()}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
