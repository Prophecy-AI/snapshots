{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89e6ba9",
   "metadata": {},
   "source": [
    "# Transfer Learning with ResNet50\n",
    "\n",
    "This experiment implements transfer learning using ResNet50 pretrained on ImageNet to address the issues from the baseline:\n",
    "- Severe overfitting (98% train vs 86% val)\n",
    "- Numerical instability (NaN log loss)\n",
    "- No data augmentation\n",
    "- Low resolution (128x128)\n",
    "\n",
    "## Strategy\n",
    "1. Use ResNet50 backbone with frozen initial layers\n",
    "2. Add aggressive data augmentation\n",
    "3. Use BCEWithLogitsLoss for numerical stability\n",
    "4. Two-phase training: freeze backbone first, then fine-tune\n",
    "5. Increase resolution to 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72349823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:45:13.168455Z",
     "iopub.status.busy": "2026-01-13T02:45:13.167953Z",
     "iopub.status.idle": "2026-01-13T02:45:13.176535Z",
     "shell.execute_reply": "2026-01-13T02:45:13.175549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.10 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09a973a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:45:13.180591Z",
     "iopub.status.busy": "2026-01-13T02:45:13.179742Z",
     "iopub.status.idle": "2026-01-13T02:45:13.244320Z",
     "shell.execute_reply": "2026-01-13T02:45:13.243710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 22500\n",
      "Dogs: 11258\n",
      "Cats: 11242\n",
      "Class distribution: 50.04% dogs, 49.96% cats\n"
     ]
    }
   ],
   "source": [
    "# Load data information\n",
    "train_dir = '/home/data/train'\n",
    "test_dir = '/home/data/test'\n",
    "\n",
    "# Get all training files\n",
    "train_files = os.listdir(train_dir)\n",
    "train_files = [f for f in train_files if f.endswith('.jpg')]\n",
    "\n",
    "# Create labels (dog=1, cat=0)\n",
    "train_labels = []\n",
    "for f in train_files:\n",
    "    if f.startswith('dog'):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)\n",
    "\n",
    "print(f\"Total training images: {len(train_files)}\")\n",
    "print(f\"Dogs: {sum(train_labels)}\")\n",
    "print(f\"Cats: {len(train_labels) - sum(train_labels)}\")\n",
    "print(f\"Class distribution: {np.mean(train_labels):.2%} dogs, {1-np.mean(train_labels):.2%} cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdc85d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:45:13.246556Z",
     "iopub.status.busy": "2026-01-13T02:45:13.245995Z",
     "iopub.status.idle": "2026-01-13T02:45:13.293573Z",
     "shell.execute_reply": "2026-01-13T02:45:13.292906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 18000 images\n",
      "Val set: 4500 images\n",
      "Train dogs: 9006, cats: 8994\n",
      "Val dogs: 2252, cats: 2248\n",
      "Split saved to /home/code/splits/stratified_split_20pct.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create stratified split (since split file doesn't exist or is corrupted)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create stratified split (80% train, 20% val)\n",
    "train_files_split, val_files_split, train_labels_split, val_labels_split = train_test_split(\n",
    "    train_files, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_files_split)} images\")\n",
    "print(f\"Val set: {len(val_files_split)} images\")\n",
    "print(f\"Train dogs: {sum(train_labels_split)}, cats: {len(train_labels_split) - sum(train_labels_split)}\")\n",
    "print(f\"Val dogs: {sum(val_labels_split)}, cats: {len(val_labels_split) - sum(val_labels_split)}\")\n",
    "\n",
    "# Save split for future use\n",
    "os.makedirs('/home/code/splits', exist_ok=True)\n",
    "split_data = {\n",
    "    'train_files': train_files_split,\n",
    "    'val_files': val_files_split,\n",
    "    'train_labels': train_labels_split,\n",
    "    'val_labels': val_labels_split\n",
    "}\n",
    "with open('/home/code/splits/stratified_split_20pct.pkl', 'wb') as f:\n",
    "    pickle.dump(split_data, f)\n",
    "print(\"Split saved to /home/code/splits/stratified_split_20pct.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766ddfc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:45:13.295942Z",
     "iopub.status.busy": "2026-01-13T02:45:13.295685Z",
     "iopub.status.idle": "2026-01-13T02:45:13.306572Z",
     "shell.execute_reply": "2026-01-13T02:45:13.305999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class DogsCatsDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None, is_test=False):\n",
    "        self.file_list = file_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.train_dir = '/home/data/train'\n",
    "        self.test_dir = '/home/data/test'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = os.path.join(self.test_dir, self.file_list[idx])\n",
    "        else:\n",
    "            img_path = os.path.join(self.train_dir, self.file_list[idx])\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcdfb01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:45:13.308833Z",
     "iopub.status.busy": "2026-01-13T02:45:13.308383Z",
     "iopub.status.idle": "2026-01-13T02:45:13.335224Z",
     "shell.execute_reply": "2026-01-13T02:45:13.334539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 563\n",
      "Val batches: 141\n"
     ]
    }
   ],
   "source": [
    "# Define data transforms with aggressive augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = DogsCatsDataset(train_files_split, train_labels_split, transform=train_transform)\n",
    "val_dataset = DogsCatsDataset(val_files_split, val_labels_split, transform=val_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2926cfd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T02:45:13.337759Z",
     "iopub.status.busy": "2026-01-13T02:45:13.337197Z",
     "iopub.status.idle": "2026-01-13T02:45:14.935923Z",
     "shell.execute_reply": "2026-01-13T02:45:14.935201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/nonroot/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 656k/97.8M [00:00<00:15, 6.67MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9.28M/97.8M [00:00<00:01, 55.9MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▍       | 24.0M/97.8M [00:00<00:00, 101MB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 39.4M/97.8M [00:00<00:00, 125MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 55.2M/97.8M [00:00<00:00, 140MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 70.5M/97.8M [00:00<00:00, 147MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 86.1M/97.8M [00:00<00:00, 152MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 132MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 24,032,833\n",
      "Trainable parameters (before freezing): 24,032,833\n",
      "Trainable parameters (after freezing): 0\n"
     ]
    }
   ],
   "source": [
    "# Define model with ResNet50 backbone\n",
    "class DogsCatsResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(DogsCatsResNet50, self).__init__()\n",
    "        # Load pretrained ResNet50\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Get number of features from the backbone\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace the final layer with our classification head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)  # Output single logit\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create model\n",
    "model = DogsCatsResNet50(pretrained=True)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters (before freezing): {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Freeze backbone for phase 1 training\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Trainable parameters (after freezing): {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (numerically stable) and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Use BCEWithLogitsLoss for numerical stability\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer for phase 1 (only classification head)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Gradient scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions with gradient clipping and mixed precision\n",
    "def train_epoch(model, loader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        predicted = (probs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predicted = (probs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    # Calculate log loss with numerical stability\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Clamp probabilities to avoid log(0)\n",
    "    epsilon = 1e-7\n",
    "    all_probs_clamped = np.clip(all_probs, epsilon, 1 - epsilon)\n",
    "    log_loss = -np.mean(all_labels * np.log(all_probs_clamped) + (1 - all_labels) * np.log(1 - all_probs_clamped))\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, log_loss, all_probs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping and checkpointing\n",
    "import os\n",
    "os.makedirs('/home/code/models', exist_ok=True)\n",
    "\n",
    "num_epochs_phase1 = 10  # Train only classification head\n",
    "num_epochs_phase2 = 15  # Fine-tune entire network\n",
    "best_val_loss = float('inf')\n",
    "best_val_log_loss = float('inf')\n",
    "best_val_acc = 0\n",
    "early_stopping_patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_log_losses = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: Training Classification Head (Frozen Backbone)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs_phase1):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs_phase1}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "    val_loss, val_acc, val_log_loss, val_probs, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_log_losses.append(val_log_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val LogLoss: {val_log_loss:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_log_loss < best_val_log_loss:\n",
    "        best_val_log_loss = val_log_loss\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), '/home/code/models/resnet50_phase1_best.pth')\n",
    "        print(f\"✓ New best model saved! Val LogLoss: {val_log_loss:.4f}\")\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nPhase 1 completed. Best Val LogLoss: {best_val_log_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
