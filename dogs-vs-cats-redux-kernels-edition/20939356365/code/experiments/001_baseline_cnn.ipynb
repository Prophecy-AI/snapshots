{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62523d36",
   "metadata": {},
   "source": [
    "# Baseline CNN for Dogs vs Cats\n",
    "\n",
    "Simple CNN baseline to establish a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a82a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "train_dir = '/home/data/train'\n",
    "test_dir = '/home/data/test'\n",
    "sample_submission_path = '/home/data/sample_submission.csv'\n",
    "\n",
    "# Get all training images\n",
    "train_files = os.listdir(train_dir)\n",
    "train_files = [f for f in train_files if f.endswith('.jpg')]\n",
    "\n",
    "print(f\"Total training images: {len(train_files)}\")\n",
    "\n",
    "# Create labels (1 for dog, 0 for cat)\n",
    "train_labels = []\n",
    "train_paths = []\n",
    "\n",
    "for file in train_files:\n",
    "    if file.startswith('dog'):\n",
    "        train_labels.append(1)\n",
    "        train_paths.append(os.path.join(train_dir, file))\n",
    "    elif file.startswith('cat'):\n",
    "        train_labels.append(0)\n",
    "        train_paths.append(os.path.join(train_dir, file))\n",
    "\n",
    "print(f\"Dogs: {sum(train_labels)}\")\n",
    "print(f\"Cats: {len(train_labels) - sum(train_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_paths, train_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Training dogs: {sum(y_train)}\")\n",
    "print(f\"Validation dogs: {sum(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class DogsCatsDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = DogsCatsDataset(X_train, y_train, transform=train_transform)\n",
    "val_dataset = DogsCatsDataset(X_val, y_val, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Calculate feature size after convolutions\n",
    "        # Input: 128x128 -> pool -> 64x64 -> pool -> 32x32 -> pool -> 16x16\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # 128x128 -> 64x64\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # 64x64 -> 32x32\n",
    "        x = self.pool(torch.relu(self.conv3(x)))  # 32x32 -> 16x16\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model size (MB): {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), correct / total, all_probs, all_labels\n",
    "\n",
    "# Calculate log loss\n",
    "def log_loss(probs, labels):\n",
    "    probs = np.array(probs)\n",
    "    labels = np.array(labels)\n",
    "    eps = 1e-15\n",
    "    probs = np.clip(probs, eps, 1 - eps)\n",
    "    return -np.mean(labels * np.log(probs) + (1 - labels) * np.log(1 - probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0\n",
    "best_log_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_log_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_probs, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_logloss = log_loss(val_probs, val_labels)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_log_losses.append(val_logloss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val LogLoss: {val_logloss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_logloss < best_log_loss:\n",
    "        best_log_loss = val_logloss\n",
    "        torch.save(model.state_dict(), '/home/code/models/baseline_cnn_best.pth')\n",
    "        print(f\"New best model saved with LogLoss: {val_logloss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8437f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Val Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(val_accs, label='Val Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(val_log_losses, label='Val LogLoss')\n",
    "axes[2].set_title('Validation LogLoss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('LogLoss')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/figures/baseline_training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation LogLoss: {best_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc585c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and make predictions on test set\n",
    "model.load_state_dict(torch.load('/home/code/models/baseline_cnn_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get test files\n",
    "test_files = os.listdir(test_dir)\n",
    "test_files = [f for f in test_files if f.endswith('.jpg')]\n",
    "test_files.sort(key=lambda x: int(x.split('.')[0]))  # Sort by numeric ID\n",
    "\n",
    "print(f\"Total test images: {len(test_files)}\")\n",
    "\n",
    "# Create test dataset\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = DogsCatsDataset(\n",
    "    [os.path.join(test_dir, f) for f in test_files], \n",
    "    [0] * len(test_files),  # dummy labels\n",
    "    transform=test_transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Make predictions\n",
    "all_test_probs = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).squeeze()\n",
    "        all_test_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "print(f\"Generated {len(all_test_probs)} predictions\")\n",
    "print(f\"Sample predictions: {all_test_probs[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2496803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [int(f.split('.')[0]) for f in test_files],\n",
    "    'label': all_test_probs\n",
    "})\n",
    "\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('/home/submission/submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created:\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nSubmission shape: {submission_df.shape}\")\n",
    "print(f\"Prediction range: [{submission_df['label'].min():.4f}, {submission_df['label'].max():.4f}]\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
