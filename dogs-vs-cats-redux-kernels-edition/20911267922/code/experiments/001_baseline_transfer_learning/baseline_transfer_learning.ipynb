{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e16b29",
   "metadata": {},
   "source": [
    "# Baseline: Transfer Learning with ResNet\n",
    "\n",
    "This notebook implements a baseline approach using transfer learning with a pretrained ResNet model.\n",
    "\n",
    "Based on research findings:\n",
    "- Winning solutions use pretrained CNNs (VGG, ResNet, EfficientNet)\n",
    "- Aggressive data augmentation is key\n",
    "- Fine-tuning on the Kaggle dataset achieves 98%+ accuracy\n",
    "- Two-step approach: lightweight model first, then transfer to deeper architecture\n",
    "\n",
    "## Strategy\n",
    "1. Use ResNet50 pretrained on ImageNet\n",
    "2. Resize images to 224x224\n",
    "3. Apply data augmentation (flips, rotations, color jitter)\n",
    "4. Fine-tune the top layers\n",
    "5. Use Adam optimizer with learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da98830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "train_dir = '/home/data/train'\n",
    "test_dir = '/home/data/test'\n",
    "\n",
    "# Get all training images\n",
    "train_files = os.listdir(train_dir)\n",
    "train_files = [f for f in train_files if f.endswith('.jpg')]\n",
    "\n",
    "# Create labels (cat=0, dog=1)\n",
    "train_labels = []\n",
    "for file in train_files:\n",
    "    if file.startswith('cat'):\n",
    "        train_labels.append(0)\n",
    "    elif file.startswith('dog'):\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        print(f\"Warning: Unknown file {file}\")\n",
    "\n",
    "print(f\"Total training images: {len(train_files)}\")\n",
    "print(f\"Cats: {sum(1 for x in train_labels if x == 0)}\")\n",
    "print(f\"Dogs: {sum(1 for x in train_labels if x == 1)}\")\n",
    "\n",
    "# Split into train/validation\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_files, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_files)} images\")\n",
    "print(f\"Validation set: {len(val_files)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5881099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class DogsCatsDataset(Dataset):\n",
    "    def __init__(self, file_names, labels, transform=None, is_test=False):\n",
    "        self.file_names = file_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.data_dir = test_dir if is_test else train_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.file_names[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            return image, self.labels[idx]\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DogsCatsDataset(train_files, train_labels, transform=train_transform)\n",
    "val_dataset = DogsCatsDataset(val_files, val_labels, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_features, 2)\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{running_loss/total*batch_size:.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/total*batch_size:.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), '/home/code/experiments/001_baseline_transfer_learning/best_model.pth')\n",
    "        print(f\"New best model saved with val accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and make predictions on test set\n",
    "model.load_state_dict(torch.load('/home/code/experiments/001_baseline_transfer_learning/best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get test files\n",
    "test_files = sorted(os.listdir(test_dir))\n",
    "test_files = [f for f in test_files if f.endswith('.jpg')]\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = DogsCatsDataset(test_files, [0]*len(test_files), transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(test_loader, desc='Predicting'):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability of being a dog\n",
    "        predictions.extend(probs.cpu().numpy())\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")\n",
    "print(f\"Sample predictions: {predictions[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(1, len(predictions) + 1),\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nLast 10 rows:\")\n",
    "print(submission.tail(10))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
