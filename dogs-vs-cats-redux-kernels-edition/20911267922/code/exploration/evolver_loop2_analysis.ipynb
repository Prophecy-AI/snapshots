{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f96df8",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Understanding B3 Failure & Next Steps\n",
    "\n",
    "## Goal\n",
    "Analyze why EfficientNet-B3 with enhanced augmentation performed worse than B0 baseline, and identify high-ROI improvements.\n",
    "\n",
    "## Key Questions\n",
    "1. What caused the performance degradation in exp_003?\n",
    "2. How can we build on the successful B0 baseline?\n",
    "3. What ensemble strategies will give best ROI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1030696b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T18:30:49.200633Z",
     "iopub.status.busy": "2026-01-12T18:30:49.199947Z",
     "iopub.status.idle": "2026-01-12T18:30:50.415477Z",
     "shell.execute_reply": "2026-01-12T18:30:50.414831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Summary:\n",
      "==================================================\n",
      "002_baseline_efficientnet: 0.020219 (model: efficientnet_b0)\n",
      "  Notes: Baseline EfficientNet-B0 model with transfer learning from ImageNet. Used PyTorch implementation with proper GPU acceleration on NVIDIA A100. Training: 15 epochs, batch size 64, Adam optimizer with le...\n",
      "\n",
      "exp_003_efficientnet_b3_enhanced: 0.051978 (model: efficientnet-b3)\n",
      "  Notes: Enhanced EfficientNet-B3 with early stopping (patience=5), advanced augmentation (CutMix, MixUp), and improved regularization (dropout=0.3, label smoothing=0.1). Result: 0.051978 validation log loss -...\n",
      "\n",
      "\n",
      "Gold threshold: 0.038820\n",
      "Best score: 0.020219\n",
      "Improvement needed: 0.018601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Load session state to review experiments\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "experiments = session_state['experiments']\n",
    "print(\"Experiment Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for exp in experiments:\n",
    "    print(f\"{exp['name']}: {exp['score']:.6f} (model: {exp['model_type']})\")\n",
    "    print(f\"  Notes: {exp['notes'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nGold threshold: 0.038820\")\n",
    "print(f\"Best score: {min([exp['score'] for exp in experiments]):.6f}\")\n",
    "print(f\"Improvement needed: {0.038820 - min([exp['score'] for exp in experiments]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea91c4",
   "metadata": {},
   "source": [
    "## Analysis: Why Did B3 Fail?\n",
    "\n",
    "The B3 experiment degraded from 0.020219 to 0.051978 - a 157% worse performance. Let's analyze potential causes:\n",
    "\n",
    "1. **CutMix/MixUp too aggressive**: These techniques mix images and labels, which can hurt performance on clean, balanced datasets\n",
    "2. **Learning rate suboptimal**: Same LR=1e-4 may not work for larger model\n",
    "3. **Overfitting despite regularization**: B3 has 11M params vs B0's 5M - more capacity to overfit\n",
    "4. **Early stopping didn't trigger**: Trained all 20 epochs, may have overfitted\n",
    "\n",
    "Key insight: The baseline B0 already beats gold significantly. We should:\n",
    "- Build on what works (B0 architecture)\n",
    "- Add proven improvements (TTA, early stopping, simple ensembles)\n",
    "- Avoid overcomplicating with aggressive augmentation"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
