{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f96df8",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Understanding B3 Failure & Next Steps\n",
    "\n",
    "## Goal\n",
    "Analyze why EfficientNet-B3 with enhanced augmentation performed worse than B0 baseline, and identify high-ROI improvements.\n",
    "\n",
    "## Key Questions\n",
    "1. What caused the performance degradation in exp_003?\n",
    "2. How can we build on the successful B0 baseline?\n",
    "3. What ensemble strategies will give best ROI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1030696b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T18:30:49.200633Z",
     "iopub.status.busy": "2026-01-12T18:30:49.199947Z",
     "iopub.status.idle": "2026-01-12T18:30:50.415477Z",
     "shell.execute_reply": "2026-01-12T18:30:50.414831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Summary:\n",
      "==================================================\n",
      "002_baseline_efficientnet: 0.020219 (model: efficientnet_b0)\n",
      "  Notes: Baseline EfficientNet-B0 model with transfer learning from ImageNet. Used PyTorch implementation with proper GPU acceleration on NVIDIA A100. Training: 15 epochs, batch size 64, Adam optimizer with le...\n",
      "\n",
      "exp_003_efficientnet_b3_enhanced: 0.051978 (model: efficientnet-b3)\n",
      "  Notes: Enhanced EfficientNet-B3 with early stopping (patience=5), advanced augmentation (CutMix, MixUp), and improved regularization (dropout=0.3, label smoothing=0.1). Result: 0.051978 validation log loss -...\n",
      "\n",
      "\n",
      "Gold threshold: 0.038820\n",
      "Best score: 0.020219\n",
      "Improvement needed: 0.018601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Load session state to review experiments\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "experiments = session_state['experiments']\n",
    "print(\"Experiment Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for exp in experiments:\n",
    "    print(f\"{exp['name']}: {exp['score']:.6f} (model: {exp['model_type']})\")\n",
    "    print(f\"  Notes: {exp['notes'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nGold threshold: 0.038820\")\n",
    "print(f\"Best score: {min([exp['score'] for exp in experiments]):.6f}\")\n",
    "print(f\"Improvement needed: {0.038820 - min([exp['score'] for exp in experiments]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea91c4",
   "metadata": {},
   "source": [
    "## Analysis: Why Did B3 Fail?\n",
    "\n",
    "The B3 experiment degraded from 0.020219 to 0.051978 - a 157% worse performance. Let's analyze potential causes:\n",
    "\n",
    "1. **CutMix/MixUp too aggressive**: These techniques mix images and labels, which can hurt performance on clean, balanced datasets\n",
    "2. **Learning rate suboptimal**: Same LR=1e-4 may not work for larger model\n",
    "3. **Overfitting despite regularization**: B3 has 11M params vs B0's 5M - more capacity to overfit\n",
    "4. **Early stopping didn't trigger**: Trained all 20 epochs, may have overfitted\n",
    "\n",
    "Key insight: The baseline B0 already beats gold significantly. We should:\n",
    "- Build on what works (B0 architecture)\n",
    "- Add proven improvements (TTA, early stopping, simple ensembles)\n",
    "- Avoid overcomplicating with aggressive augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the prediction distributions from both experiments\n",
    "import os\n",
    "\n",
    "# Load predictions if available\n",
    "pred_files = []\n",
    "for root, dirs, files in os.walk('/home/code/submission_candidates'):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            pred_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Available prediction files:\")\n",
    "for i, f in enumerate(pred_files):\n",
    "    print(f\"{i}: {f}\")\n",
    "\n",
    "# Try to load and compare distributions\n",
    "if len(pred_files) >= 2:\n",
    "    # Load B0 predictions\n",
    "    b0_preds = pd.read_csv(pred_files[0])\n",
    "    b3_preds = pd.read_csv(pred_files[1])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICTION DISTRIBUTION COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nB0 Baseline ({os.path.basename(pred_files[0])}):\")\n",
    "    print(f\"  Mean: {b0_preds['label'].mean():.4f}\")\n",
    "    print(f\"  Std:  {b0_preds['label'].std():.4f}\")\n",
    "    print(f\"  Min:  {b0_preds['label'].min():.4f}\")\n",
    "    print(f\"  Max:  {b0_preds['label'].max():.4f}\")\n",
    "    \n",
    "    # Bimodal analysis\n",
    "    low_conf = (b0_preds['label'] < 0.1).sum()\n",
    "    high_conf = (b0_preds['label'] > 0.9).sum()\n",
    "    mid_conf = ((b0_preds['label'] >= 0.4) & (b0_preds['label'] <= 0.6)).sum()\n",
    "    print(f\"  <0.1: {low_conf} ({low_conf/len(b0_preds)*100:.1f}%)\")\n",
    "    print(f\"  >0.9: {high_conf} ({high_conf/len(b0_preds)*100:.1f}%)\")\n",
    "    print(f\"  0.4-0.6: {mid_conf} ({mid_conf/len(b0_preds)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nB3 Enhanced ({os.path.basename(pred_files[1])}):\")\n",
    "    print(f\"  Mean: {b3_preds['label'].mean():.4f}\")\n",
    "    print(f\"  Std:  {b3_preds['label'].std():.4f}\")\n",
    "    print(f\"  Min:  {b3_preds['label'].min():.4f}\")\n",
    "    print(f\"  Max:  {b3_preds['label'].max():.4f}\")\n",
    "    \n",
    "    low_conf = (b3_preds['label'] < 0.1).sum()\n",
    "    high_conf = (b3_preds['label'] > 0.9).sum()\n",
    "    mid_conf = ((b3_preds['label'] >= 0.4) & (b3_preds['label'] <= 0.6)).sum()\n",
    "    print(f\"  <0.1: {low_conf} ({low_conf/len(b3_preds)*100:.1f}%)\")\n",
    "    print(f\"  >0.9: {high_conf} ({high_conf/len(b3_preds)*100:.1f}%)\")\n",
    "    print(f\"  0.4-0.6: {mid_conf} ({mid_conf/len(b3_preds)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].hist(b0_preds['label'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0].set_title('B0 Baseline Prediction Distribution')\n",
    "    axes[0].set_xlabel('Prediction (0=cat, 1=dog)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].axvline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    axes[1].hist(b3_preds['label'], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1].set_title('B3 Enhanced Prediction Distribution')\n",
    "    axes[1].set_xlabel('Prediction (0=cat, 1=dog)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].axvline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough prediction files to analyze\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
