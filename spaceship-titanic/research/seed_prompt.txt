## Current Status
- Best CV score: 0.80927 from exp_001 (Advanced Feature Engineering)
- Best LB score: 0.79705 from exp_000 (baseline XGBoost)
- CV-LB gap: +0.00969 (CV overestimates by ~1.2%)
- Predicted LB for exp_001: ~0.7996 (if gap holds)

## CRITICAL: Target Score Reality Check
**The target of 0.9642 is IMPOSSIBLE for this competition.**
- Top LB scores for Spaceship Titanic are ~0.8066 (80.7%)
- Our CV of 0.80927 is already competitive with top 7% solutions
- Focus on maximizing within realistic bounds (~0.80-0.81)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Results can be relied upon.
- Evaluator's top priority: **3-model ensemble (XGBoost + LightGBM + CatBoost)**. **AGREE** - This is the right next step. Feature engineering is now mature (56 features, distributed importance). Ensemble is the most reliable way to squeeze out remaining gains.
- Key concerns raised:
  1. No ensemble yet despite being top priority → **Addressing now**
  2. Fold 4 variance slightly higher → Acceptable, not critical
  3. Hyperparameters unchanged → Will tune after ensemble baseline
- Secondary recommendation: Submit exp_001 to validate CV improvement → **AGREE** - Should submit after ensemble to compare

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns already exploited in exp_001:
  1. CryoSleep×HomePlanet interaction (98.9% transported for Europa+CryoSleep)
  2. Group-based imputation (100% consistency in HomePlanet within groups)
  3. Spending ratios and categories (LuxuryRatio is now top feature at 0.27)
  4. Deck/Side interactions

## Recommended Approaches (Priority Order)

### Priority 1: 3-Model Ensemble (MUST DO)
Implement ensemble of XGBoost + LightGBM + CatBoost with the exp_001 feature set:

**XGBoost** (already tuned):
```python
xgb_params = {
    'max_depth': 5,
    'learning_rate': 0.067,
    'n_estimators': 850,
    'reg_lambda': 3.06,
    'reg_alpha': 4.58,
    'colsample_bytree': 0.92,
    'subsample': 0.95,
    'random_state': 42
}
```

**LightGBM** (from kernel research):
```python
lgb_params = {
    'num_leaves': 330,
    'learning_rate': 0.087,
    'n_estimators': 739,
    'feature_fraction': 0.66,
    'bagging_fraction': 0.87,
    'lambda_l1': 6.18,
    'lambda_l2': 0.01,
    'random_state': 42
}
```

**CatBoost**:
```python
cat_params = {
    'depth': 6,
    'learning_rate': 0.05,
    'iterations': 1000,
    'l2_leaf_reg': 3.0,
    'random_seed': 42,
    'verbose': False
}
```

**Ensemble Strategy**:
1. Train each model with 5-fold StratifiedKFold
2. Get OOF predictions from each model
3. Average probabilities: `final_pred = (xgb_pred + lgb_pred + cat_pred) / 3`
4. Use threshold 0.5 for binary classification

### Priority 2: Threshold Tuning (After Ensemble)
- Default threshold is 0.5
- Try thresholds from 0.45 to 0.55 in 0.01 increments
- Match predicted distribution to training distribution (~50.4% transported)
- Can gain 0.1-0.3% accuracy

### Priority 3: Weighted Ensemble (If Simple Averaging Works)
- Use Optuna to find optimal weights for each model
- Constraint: weights sum to 1
- Expected gain: 0.1-0.2% over simple averaging

## What NOT to Try
- **Neural networks**: Tabular data, small dataset - trees are optimal
- **Complex stacking**: Diminishing returns, simple averaging is usually sufficient
- **More feature engineering**: exp_001 features are mature (56 features, distributed importance)
- **Chasing 0.9642 target**: Impossible - top solutions achieve ~0.807

## Validation Notes
- Use 5-fold StratifiedKFold (consistent with previous experiments)
- CV-LB gap of ~1% is acceptable - CV is reliable
- Focus on CV improvement; LB will follow
- Compare ensemble CV to exp_001 CV (0.80927) to measure improvement

## Feature Set (Use exp_001 features)
Copy the feature engineering from `experiments/002_feature_engineering/feature_eng.ipynb`:
- 56 features total
- Includes: spending ratios, luxury/basic categories, interaction features
- Group-based imputation for HomePlanet, Deck, Side
- Top features: LuxuryRatio (0.27), HomePlanet_enc (0.12), CryoSleep_HomePlanet_enc (0.10)

## Experiment Plan
1. **exp_002**: 3-model ensemble (XGBoost + LightGBM + CatBoost) with simple averaging
   - Expected CV: 0.810-0.815 (0.5-1% improvement over single XGBoost)
2. **exp_003**: Threshold tuning on ensemble
3. **exp_004**: Weighted ensemble with Optuna

## Submission Strategy
- Submit exp_002 (ensemble) to validate improvement
- Compare LB score to exp_000 (0.79705) and predicted exp_001 LB (~0.7996)
- If ensemble LB > 0.80, we're in excellent position