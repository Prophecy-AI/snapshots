## Current Status
- Best CV score: 0.81951 from exp_003 (CatBoost Optuna-tuned)
- Best LB score: 0.8045 from exp_003
- Latest exp_006: CV 0.8171, LB 0.8010 (CALIBRATION HYPOTHESIS REJECTED)
- CV-LB gap: ~1.6% average, increasing trend (1.2% → 1.5% → 1.6% → 2.0%)
- Target: 0.9642 (IMPOSSIBLE - top LB is ~0.8066, our best 0.8045 is competitive)

## Response to Evaluator
- Technical verdict: TRUSTWORTHY. Execution is sound.
- Evaluator's top priority was: Submit exp_006 to test calibration hypothesis. DONE.
- **RESULT**: Calibration hypothesis REJECTED. exp_006 had best calibration (50.9% vs 51.7%) but WORST LB (0.8010 vs 0.8045).
- **KEY LEARNING**: CV is the primary driver of LB performance. Higher CV = higher LB (R²=0.92 across 4 submissions).
- **NEW PRIORITY**: Focus on increasing CV above 0.8195 to beat exp_003's LB of 0.8045.

## Data Understanding
- Reference notebooks: 
  - `exploration/eda.ipynb` for initial EDA
  - `exploration/evolver_loop5_analysis.ipynb` for feature importance and regularization analysis
  - `exploration/evolver_loop7_lb_feedback.ipynb` for calibration vs CV analysis
- Key patterns:
  - CryoSleep is the strongest predictor (81.8% transported when True)
  - Spending patterns strongly predict transport (no spending = likely transported)
  - Deck and CabinNum are important features
  - **CV is the primary driver of LB** - calibration doesn't matter as much

## CV-LB Gap Analysis (4 submissions)
| Experiment | CV | LB | Gap | Gap % |
|------------|------|------|------|-------|
| exp_000 | 0.8067 | 0.7971 | 0.0096 | 1.19% |
| exp_003 | 0.8195 | 0.8045 | 0.0150 | 1.83% |
| exp_004 | 0.8193 | 0.8041 | 0.0152 | 1.86% |
| exp_006 | 0.8171 | 0.8010 | 0.0161 | 1.97% |

**Linear model**: LB ≈ 0.54 * CV + 0.36 (R²=0.92)
**To beat LB 0.8045**: Need CV > 0.8195

## Recommended Approaches (Priority Order)

### 1. Feature Selection + CatBoost (HIGH PRIORITY)
- 22 features have importance < 1.0 (see loop 5 analysis)
- Bottom features: ShoppingMall_spent (0.02), VIP_enc (0.03), IsSenior (0.04), IsChild (0.05)
- Remove low-importance features and retrain CatBoost
- Expected benefit: Reduced overfitting, potentially higher CV
- Use best CatBoost params: depth=8, lr=0.051, iterations=755, l2_leaf_reg=3.52
- **Rationale**: Feature selection is a proven technique to reduce overfitting and can improve both CV and generalization.

### 2. Target Encoding (MEDIUM PRIORITY)
- Currently using label encoding for categorical features
- Target encoding captures more signal from categorical variables
- Apply with proper CV to avoid leakage
- Could improve CV by capturing category-target relationships
- **Rationale**: Top kernels mention target encoding as effective for this problem.

### 3. Regularized CatBoost (MEDIUM PRIORITY)
- Try depth=6 (vs 8), l2_leaf_reg=5.0 (vs 3.52)
- Add subsample=0.8 for randomness
- May reduce CV slightly but could reduce CV-LB gap
- **Rationale**: The CV-LB gap is increasing (1.2% → 2.0%), suggesting overfitting.

### 4. Multi-Seed CatBoost Ensemble (MEDIUM PRIORITY)
- Train CatBoost with 5 different random seeds
- Average predictions
- Reduces variance without changing bias
- **Rationale**: CV varies by ~0.14% across seeds (loop 5 analysis). Averaging could stabilize predictions.

### 5. Additional Feature Engineering (LOWER PRIORITY)
- Interaction features not yet tried: Age × Spending, Deck × HomePlanet
- Polynomial features on spending variables
- **Rationale**: Feature engineering has shown consistent improvements in earlier experiments.

## What NOT to Try (Exhausted Approaches)
- **Threshold tuning**: Proven to hurt LB (exp_004: 0.8041 vs exp_003: 0.8045)
- **Native categorical handling**: Didn't outperform label encoding (exp_004)
- **Simple averaging ensemble**: Weighted is better (exp_006)
- **Stacking with LR meta-learner**: CV lower than CatBoost alone (exp_005: 0.8174)
- **Weighted ensemble**: Lower CV = lower LB (exp_006: 0.8010)
- **Calibration optimization**: Doesn't improve LB (exp_006 proved this)

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency
- CV-LB gap is ~1.6% and increasing - need to reduce overfitting
- **Focus on CV**: Higher CV = higher LB (proven across 4 submissions)
- LB improvement rate is ~60% of CV improvement

## Submission Strategy
- 6 submissions remaining
- Only submit if CV > 0.8195 (exp_003's CV)
- Or if trying fundamentally different approach (e.g., neural network)
- Next submission should be feature selection experiment if CV improves

## Key Insight
**CV is king.** The calibration hypothesis was rejected. To beat exp_003's LB of 0.8045, we need CV > 0.8195. Feature selection is the most promising path because:
1. It can reduce overfitting (lower CV-LB gap)
2. It can improve CV by removing noise
3. It's quick to implement and test
