# Seed Prompt: Spaceship Titanic - Loop 5 Strategy

## Current Status
- Best CV score: **0.81951** from tuned CatBoost (exp_003)
- Best LB score: 0.79705 (exp_000 baseline XGBoost)
- CV-LB gap: +0.00969 (1.2% overestimate) → CV is optimistic
- Predicted LB for tuned CatBoost: ~0.8098 (would be competitive with top ~0.8066)

## Response to Evaluator
**Technical verdict was TRUSTWORTHY with MINOR CONCERNS.** Results are valid but improvement may not generalize due to potential fold overfitting.

**Evaluator's top priority: Submit to get LB feedback, then explore threshold tuning.**
- **AGREE 100%.** We need to calibrate our CV-LB gap with the tuned CatBoost model.
- The higher variance (std 0.00685 vs 0.00431) is a valid concern, but the improvement is real.
- We have 9 submissions remaining - plenty of room for calibration.

**Key concerns raised and my response:**
1. **Higher variance in tuned model** → Valid concern. Will monitor LB score to see if it generalizes.
2. **Same CV folds for tuning and evaluation** → Acknowledged. The +0.14% improvement may be partially due to fold overfitting.
3. **Diminishing returns on hyperparameter tuning** → Agree. Pivot to other approaches after this submission.
4. **Threshold tuning not explored** → Will implement after getting LB feedback.

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Full EDA with feature distributions, correlations
- `exploration/evolver_loop1_analysis.ipynb` - CryoSleep×HomePlanet interactions, group consistency
- `exploration/evolver_loop4_analysis.ipynb` - CV-LB gap analysis, unexplored approaches

Key patterns already exploited:
1. CryoSleep×HomePlanet interaction (Europa+CryoSleep = 98.9% transported)
2. Spending patterns (non-spenders ~65% transported)
3. Deck/Side effects (Deck B/C highest, Starboard > Port)
4. Group-based imputation for missing values

## CRITICAL: Target Score Reality
- **Target: 0.9642 is IMPOSSIBLE** - Top LB scores are ~0.8066 (80.7%)
- Our CV of 0.81951 predicts LB of ~0.8098
- This would be **competitive with top solutions**
- Focus on maximizing LB score, not chasing impossible target

## Recommended Approaches (Priority Order)

### IMMEDIATE (This Loop)
1. **Submit exp_003 (tuned CatBoost)** - Get LB feedback to calibrate CV-LB gap
   - Current best CV: 0.81951
   - Predicted LB: ~0.8098
   - This will inform all subsequent decisions

### AFTER SUBMISSION FEEDBACK
2. **Threshold tuning** - Quick win, hasn't been tried
   - Use OOF predictions to find optimal threshold
   - Target distribution is 50.36% transported
   - Potential gain: 0.1-0.3%
   
3. **CatBoost native categorical handling** - Research suggests this helps
   - Currently using label encoding for categoricals
   - CatBoost's cat_features parameter can improve performance
   - Pass categorical column indices directly to model
   
4. **Feature selection** - 56 features may include noise
   - Use feature importance to identify low-value features
   - Try removing bottom 10-20% of features
   - Potential gain: 0.1-0.2%

5. **Stacking with meta-learner** - Use OOF predictions as features
   - Train XGBoost, LightGBM, CatBoost
   - Use their OOF predictions as features for logistic regression
   - Potential gain: 0.2-0.5%

6. **Blend baseline and tuned CatBoost** - Address variance concern
   - Average predictions from baseline (lower variance) and tuned (higher accuracy)
   - May provide more stable predictions

## What NOT to Try
- **Simple averaging ensemble** - Already proven worse than CatBoost alone
- **More hyperparameter tuning** - Diminishing returns (+0.14% from 50 trials)
- **Neural networks** - GBMs typically better for small tabular data
- **Chasing target of 0.9642** - Impossible, top LB is ~0.8066

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency
- CV-LB gap is ~0.97% (CV overestimates LB by 1.2%)
- CatBoost baseline has lower variance (std=0.00431) vs tuned (std=0.00685)
- Consider using different random seeds for more robust tuning

## Implementation Notes for Executor

### After LB Feedback
1. **If LB confirms CV-LB gap (~0.8098)**:
   - Proceed with threshold tuning
   - Try CatBoost with native categorical handling
   - Consider feature selection

2. **If LB is significantly worse than predicted**:
   - Investigate distribution shift
   - Try adversarial validation
   - Consider using baseline CatBoost (lower variance)

3. **If LB is better than predicted**:
   - Great! Continue with current approach
   - Focus on stacking/ensembling for final push

### Threshold Tuning Implementation
```python
# Find optimal threshold using OOF predictions
from sklearn.metrics import accuracy_score
thresholds = np.arange(0.45, 0.55, 0.01)
for t in thresholds:
    acc = accuracy_score(y, (oof_preds >= t).astype(int))
    print(f"Threshold {t:.2f}: {acc:.5f}")
```

### CatBoost Native Categorical Handling
```python
# Instead of label encoding, pass categorical indices
cat_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']
cat_indices = [feature_cols.index(col) for col in cat_features if col in feature_cols]

model = CatBoostClassifier(
    cat_features=cat_indices,
    depth=8,
    learning_rate=0.051,
    iterations=755,
    l2_leaf_reg=3.52,
    random_seed=42,
    verbose=False
)
```

## Experiment Tracking
| Exp | Model | CV | LB | Notes |
|-----|-------|-----|-----|-------|
| exp_000 | XGBoost Baseline | 0.80674 | 0.79705 | First submission |
| exp_001 | XGBoost + Features | 0.80927 | - | +0.25% from features |
| exp_002 | 3-Model Ensemble | 0.81353 | - | Worse than CatBoost alone |
| exp_003 | Tuned CatBoost | 0.81951 | ? | Best CV, submit this |
