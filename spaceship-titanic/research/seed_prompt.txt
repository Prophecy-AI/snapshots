# Seed Prompt: Spaceship Titanic - Loop 4 Strategy

## Current Status
- Best CV score: **0.81836** from CatBoost (exp_002 individual model)
- Best LB score: 0.79705 (exp_000 baseline XGBoost)
- CV-LB gap: +0.00969 (1.2% overestimate) → CV is optimistic, expect ~0.97% lower on LB
- Predicted CatBoost LB: ~0.8087 (would be competitive with top solutions ~0.8066)

## Response to Evaluator
**Technical verdict was TRUSTWORTHY.** Execution is sound, results can be relied upon.

**Evaluator's top priority: Create and submit CatBoost-only predictions.**
- **AGREE 100%.** The evaluator correctly identified that CatBoost (0.81836) significantly outperforms the simple averaging ensemble (0.81353) by 0.48%. This is a critical insight.
- The current submission candidate uses the inferior ensemble predictions - this must be fixed.

**Key concerns raised:**
1. Ensemble worse than CatBoost alone → **Addressing by creating CatBoost-only submission**
2. Weighted ensemble not implemented → **Will implement if CatBoost-only doesn't beat target**
3. CatBoost has lowest variance → **Confirms CatBoost is the right choice**
4. LightGBM underperforms → **Will deprioritize LightGBM in future ensembles**

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Full EDA with feature distributions, correlations
- `exploration/evolver_loop1_analysis.ipynb` - CryoSleep×HomePlanet interactions, group consistency
- `exploration/evolver_loop3_analysis.ipynb` - CatBoost vs ensemble analysis

Key patterns to exploit:
1. **CryoSleep×HomePlanet interaction**: Europa+CryoSleep has 98.9% transported rate vs Earth+CryoSleep at 65.6%
2. **Spending patterns**: Non-spenders have ~65% transported rate vs ~35% for spenders
3. **Deck/Side effects**: Deck B (73.4%) and C (68.0%) have highest transported rates; Starboard > Port
4. **Group consistency**: Groups have 100% consistency in HomePlanet - useful for imputation

## Recommended Approaches (Priority Order)

### IMMEDIATE (This Loop)
1. **Create CatBoost-only submission** - Because CatBoost (0.81836) beats ensemble (0.81353) by 0.48%
   - Use the CatBoost OOF predictions from exp_002
   - Simply change threshold from ensemble to CatBoost predictions
   - Expected LB: ~0.8087 (competitive with top solutions)

### NEXT PRIORITIES (If CatBoost-only doesn't suffice)
2. **Tune CatBoost hyperparameters with Optuna** - Because current params are conservative (depth=6, lr=0.05)
   - Focus on: depth (try 4-8), learning_rate (0.03-0.1), iterations (500-2000), l2_leaf_reg (1-10)
   - Potential gain: 0.2-0.5%

3. **Threshold tuning** - Because default 0.5 may not be optimal
   - Optimize threshold to match training distribution (~50.4% transported)
   - Use precision-recall curve to find optimal cutoff
   - Potential gain: 0.1-0.3%

4. **Weighted ensemble with Optuna** - Because simple averaging hurts when one model dominates
   - Use Optuna to find optimal weights for (XGBoost, LightGBM, CatBoost)
   - Expected: weights will heavily favor CatBoost (likely 0.6-0.8)
   - Only try if weighted ensemble beats CatBoost alone

5. **Additional feature engineering** - Because there may be untapped signal
   - Name-based features (surname clustering, family size from surname)
   - More interaction terms (Age×Spending, VIP×Deck, etc.)
   - Target encoding for high-cardinality categoricals

## What NOT to Try
- **Simple averaging ensemble** - Already proven worse than CatBoost alone (0.48% worse)
- **LightGBM focus** - Underperforms both XGBoost (0.80927) and CatBoost (0.81836)
- **Neural networks** - Unlikely to beat GBMs on this tabular data with ~8700 samples
- **Complex stacking** - Premature; first maximize single model performance

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency
- CV-LB gap is ~0.97% (CV overestimates LB by 1.2%)
- CatBoost has lowest fold variance (std=0.00431) - most stable model
- Predicted LB for CatBoost: ~0.8087

## Target Score Reality Check
- **Target: 0.9642 is UNREALISTIC** - Top LB scores are ~0.8066 (80.7%)
- Our CatBoost CV of 0.81836 is EXCELLENT - likely top 5% territory
- Focus on maximizing CV and ensuring good CV-LB correlation
- If predicted LB of ~0.8087 is achieved, we would be competitive with top solutions

## Implementation Notes for Executor
1. **First**: Create CatBoost-only submission from exp_002 OOF predictions
   - Load the saved CatBoost test predictions (test_cat from exp_002)
   - Apply threshold 0.5 to get binary predictions
   - Save as new submission

2. **If time permits**: Run Optuna hyperparameter tuning on CatBoost
   - Use same feature set from exp_002 (56 features)
   - Optimize for accuracy with 5-fold CV
   - 100-200 trials should be sufficient

3. **Track**: Compare CatBoost-only LB score to ensemble LB score
   - This will validate the evaluator's insight about ensemble underperformance