# Seed Prompt: Spaceship Titanic - Loop 7

## Current Status
- Best CV score: 0.81951 from exp_003 (CatBoost with Optuna tuning)
- Best LB score: 0.80453 from exp_003
- CV-LB gap: +1.50% (increasing trend: 0.97% → 1.50% → 1.52%)
- Target: 0.9642 is IMPOSSIBLE - top LB is ~0.8066. Focus on incremental improvement.

## Response to Evaluator
- **Technical verdict**: TRUSTWORTHY - stacking implementation was correct
- **Evaluator's top priority**: DO NOT submit exp_005, try weighted ensemble instead
- **Key concerns raised**: 
  1. Stacking CV (0.81744) < CatBoost alone (0.81617-0.81951)
  2. Predicted rate 52.8% is too high (exp_003 had 51.7% with best LB)
  3. CV-LB gap is increasing - we're overfitting more with each experiment
- **My response**: AGREE with all concerns. Analysis confirms exp_005 would score ~0.8022-0.8041 on LB, worse than exp_003. Will NOT submit exp_005. Will try evaluator's recommended approaches.

## Data Understanding
- Reference notebooks: `exploration/eda.ipynb`, `exploration/evolver_loop5_analysis.ipynb`
- Key patterns:
  - CryoSleep is strongest predictor (81.8% transported when True)
  - Spending features highly skewed - log transforms help
  - Group-based features important (GroupSize, Solo)
  - 22 features have importance < 1.0 (candidates for removal)
  - Top features: Deck_enc (10.7), CabinNum (10.1), HomePlanet_enc (7.1)

## Critical Insight: Prediction Rate Matters
| Experiment | Pred Rate | LB Score |
|------------|-----------|----------|
| exp_003    | 51.7%     | 0.80453 (BEST) |
| exp_004    | 53.8%     | 0.80406 |
| exp_005    | 52.8%     | ~0.8022-0.8041 (predicted) |
| Training   | 50.4%     | - |

**Pattern**: Predictions closer to training rate (50.4%) perform better on LB. exp_003 is closest and has best LB.

## Recommended Approaches (Priority Order)

### 1. Weighted Ensemble Favoring CatBoost (HIGH PRIORITY)
Instead of stacking with meta-learner, use weighted averaging:
```python
# Weight by CV performance - CatBoost dominates
weights = [0.2, 0.2, 0.6]  # XGB, LGB, CatBoost
final_prob = 0.2*test_xgb + 0.2*test_lgb + 0.6*test_cat
final_pred = (final_prob >= 0.5).astype(bool)
```
**Why**: CatBoost is clearly the best single model. Weighted ensemble preserves diversity while leveraging CatBoost's strength. Should produce prediction rate closer to exp_003.

### 2. Feature Selection + CatBoost (MEDIUM PRIORITY)
Remove bottom 20% features by importance:
```python
# Features with importance < 1.0 (22 features)
# Bottom features: ShoppingMall_spent (0.02), VIP_enc (0.03), IsSenior (0.04)
important_features = [f for f in features if importance[f] >= 1.0]
# This reduces from 56 to ~34 features
```
**Why**: Fewer features = less overfitting. The 22 low-importance features add noise without signal.

### 3. Regularized CatBoost (MEDIUM PRIORITY)
Increase regularization to reduce CV-LB gap:
```python
cat_params = {
    'depth': 6,           # Reduced from 8
    'l2_leaf_reg': 5.0,   # Increased from 3.52
    'subsample': 0.8,     # Add randomness
    'learning_rate': 0.05,
    'iterations': 800,
}
```
**Why**: Previous analysis showed l2=5.0 gives CV=0.81491 with lower variance (std=0.00464). More regularization = better generalization.

### 4. Prediction Rate Calibration (LOW PRIORITY)
If prediction rate is still too high, calibrate:
```python
# Adjust threshold to match training rate
# If pred_rate > 0.517, use threshold > 0.5
# Target: pred_rate ≈ 51.7% (exp_003's rate)
```
**Why**: exp_003's 51.7% rate gave best LB. But be careful - threshold tuning hurt exp_004.

## What NOT to Try
- **Threshold tuning on OOF**: Already proven to hurt LB (exp_004)
- **Simple averaging ensemble**: Already tried, CV=0.81364 (worse than CatBoost alone)
- **Stacking with LR meta-learner**: Already tried, CV=0.81744 (worse than CatBoost)
- **Native categorical handling**: Already tried, didn't outperform label encoding

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency
- CV-LB gap is ~1.5% - expect LB to be 0.015 lower than CV
- **CRITICAL**: Check prediction rate before submitting. Target ≤ 51.7%
- Only submit if:
  1. CV improves over 0.81951 (exp_003)
  2. OR prediction rate is closer to training (50.4%)
  3. AND approach is fundamentally different (for diversity check)

## Submission Strategy
- 7 submissions remaining
- Best LB: 0.80453 (exp_003)
- DO NOT submit exp_005 (predicted to underperform)
- Submit only when we have a clear improvement signal

## Success Criteria
Since target 0.9642 is impossible (top LB ~0.8066):
- Realistic goal: Beat 0.80453 (our best LB)
- Stretch goal: Reach 0.8066 (top LB)
- Any improvement over 0.80453 is a win
