## Current Status
- Best CV score: 0.81951 from exp_003 (CatBoost Optuna-tuned) - **NOT REPRODUCIBLE**
- Best LB score: 0.8045 from exp_003
- Latest exp_008: CV 0.81698 (multi-seed ensemble)
- **TRUE BASELINE**: ~0.817 CV (confirmed by multi-seed analysis)
- CV-LB gap: ~1.6-2.0% average
- Target: 0.9642 (IMPOSSIBLE - top LB is ~0.8066, our best 0.8045 is top ~7%)

## Response to Evaluator
- Technical verdict: TRUSTWORTHY. Multi-seed ensemble implementation is sound.
- Evaluator's top priority: **Investigate why exp_003 achieved CV=0.81951 and try to reproduce it.**
- **I AGREE** with the diagnosis but **DISAGREE** with spending time on investigation:
  - The multi-seed analysis (5 seeds) conclusively shows true CV is ~0.817
  - exp_003's CV of 0.81951 was likely a lucky Optuna run
  - Investigating further is unlikely to be productive
  - Better to try fundamentally different approaches

- Key concerns raised:
  1. exp_003 is NOT reproducible - CONFIRMED by multi-seed analysis
  2. We're stuck at CV ~0.817 - CONFIRMED
  3. Need CV > 0.82089 to beat exp_003's LB - CONFIRMED

- **SYNTHESIS**: Accept that our true baseline is ~0.817 CV. Focus on fundamentally different approaches that could unlock +0.4% improvement. Target encoding is the highest priority as it's a fundamentally different feature encoding approach used by top kernels.

## Data Understanding
- Reference notebooks:
  - `exploration/eda.ipynb` for initial EDA
  - `exploration/evolver_loop9_analysis.ipynb` for CV-LB analysis and exp_003 investigation
- Key patterns:
  - CryoSleep is the strongest predictor (81.8% transported when True)
  - Spending patterns strongly predict transport
  - **CV is the primary driver of LB** (R²=0.92)
  - **To beat LB 0.8045, need CV > 0.82089** (from linear model)
  - **True baseline is ~0.817 CV** (exp_003's 0.81951 was lucky)

## CV-LB Gap Analysis (4 submissions)
| Experiment | CV | LB | Gap | Gap % |
|------------|------|------|------|-------|
| exp_000 | 0.8067 | 0.7971 | 0.0096 | 1.19% |
| exp_003 | 0.8195 | 0.8045 | 0.0150 | 1.83% |
| exp_004 | 0.8193 | 0.8041 | 0.0152 | 1.86% |
| exp_006 | 0.8171 | 0.8010 | 0.0161 | 1.97% |

**Linear model**: LB = 0.547 * CV + 0.355 (R²=0.92)
**To beat LB 0.8045**: Need CV > 0.82089

## Recommended Approaches (Priority Order)

### 1. TARGET ENCODING (HIGH PRIORITY - DO THIS FIRST)
**Rationale**: 
- Top kernels use target encoding
- We've only used label encoding which doesn't capture category-target relationships
- Fundamentally different approach that could unlock new signal
- Could provide the +0.4% improvement we need

**Implementation**:
```python
from category_encoders import TargetEncoder

# CRITICAL: Use CV-based encoding to avoid leakage
cat_cols = ['HomePlanet', 'Destination', 'Deck', 'Side', 'CryoSleep', 'VIP']
te = TargetEncoder(cols=cat_cols, smoothing=1.0)

# Fit on train, transform both
# Use within CV loop to avoid leakage:
for train_idx, val_idx in skf.split(X, y):
    te.fit(X_train[cat_cols], y_train)
    X_train_encoded = te.transform(X_train[cat_cols])
    X_val_encoded = te.transform(X_val[cat_cols])
```

**Expected benefit**: +0.2-0.5% CV improvement (based on research)

### 2. CABIN REGION FEATURES (MEDIUM PRIORITY)
**Rationale**:
- Top kernel (complete guide) uses cabin number regions
- We only use raw CabinNum which may not capture spatial patterns
- Passengers in similar cabin regions may have similar outcomes

**Implementation**:
```python
# Create cabin region features (from complete guide kernel)
df['Cabin_region1'] = (df['CabinNum'] < 300).astype(int)
df['Cabin_region2'] = ((df['CabinNum'] >= 300) & (df['CabinNum'] < 600)).astype(int)
df['Cabin_region3'] = ((df['CabinNum'] >= 600) & (df['CabinNum'] < 900)).astype(int)
df['Cabin_region4'] = ((df['CabinNum'] >= 900) & (df['CabinNum'] < 1200)).astype(int)
df['Cabin_region5'] = ((df['CabinNum'] >= 1200) & (df['CabinNum'] < 1500)).astype(int)
df['Cabin_region6'] = ((df['CabinNum'] >= 1500) & (df['CabinNum'] < 1800)).astype(int)
df['Cabin_region7'] = (df['CabinNum'] >= 1800).astype(int)
```

### 3. 10-FOLD CV (MEDIUM PRIORITY)
**Rationale**:
- Top kernels use 10-fold CV
- More stable CV estimates
- May slightly improve generalization

**Implementation**:
```python
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
```

### 4. FAMILY SIZE FEATURE (MEDIUM PRIORITY)
**Rationale**:
- Top kernel extracts surname and calculates family size
- Family members may have correlated outcomes
- We have Group but not Family

**Implementation**:
```python
# Extract surname from Name
df['Surname'] = df['Name'].str.split().str[-1]

# Calculate family size
surname_counts = pd.concat([train['Surname'], test['Surname']]).value_counts()
df['Family_size'] = df['Surname'].map(surname_counts)
```

### 5. LGBM + CATBOOST SOFT VOTING (LOWER PRIORITY)
**Rationale**:
- Top kernel uses LGBM + CatBoost with soft voting
- We tried this in exp_002 but CatBoost alone was better
- May be worth revisiting with target encoding features

**Implementation**:
```python
# Train both models
lgbm_preds = lgbm_model.predict_proba(X_test)[:, 1]
cat_preds = cat_model.predict_proba(X_test)[:, 1]

# Soft voting (average probabilities)
final_preds = (lgbm_preds + cat_preds) / 2
```

## What NOT to Try (Exhausted Approaches)
- **Threshold tuning**: Proven to hurt LB (exp_004: 0.8041 vs exp_003: 0.8045)
- **Native categorical handling**: Didn't outperform label encoding (exp_004)
- **Simple averaging ensemble**: Weighted is better (exp_006)
- **Stacking with LR meta-learner**: CV lower than CatBoost alone (exp_005: 0.8174)
- **Weighted ensemble**: Lower CV = lower LB (exp_006: 0.8010)
- **Calibration optimization**: Doesn't improve LB (exp_006 proved this)
- **Feature selection**: HURT CV by -0.00265 (exp_007)
- **Multi-seed ensemble alone**: Only gives +0.07% over best single seed (exp_008)
- **Investigating exp_003**: Multi-seed analysis shows it was a lucky run

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency (or try 10-fold)
- CV-LB gap is ~1.6-2.0% and increasing - need to reduce overfitting
- **Focus on CV**: Higher CV = higher LB (proven across 4 submissions)
- LB improvement rate is ~55% of CV improvement

## Submission Strategy
- 6 submissions remaining
- Only submit if:
  1. CV > 0.82089 (needed to beat exp_003's LB based on linear model)
  2. OR fundamentally different approach shows promise
- Do NOT submit exp_008 (CV = 0.81698, lower than exp_003)

## Key Insight
**We're stuck at CV ~0.817.** The last 5 experiments (exp_004-008) all failed to beat exp_003's CV of 0.81951, which we now know was a lucky run.

**Target encoding is the highest priority** because:
1. It's a fundamentally different feature encoding approach
2. Top kernels use it
3. It captures category-target relationships that label encoding misses
4. It could provide the +0.4% improvement we need

**If target encoding doesn't help**, try:
- Cabin region features (spatial patterns)
- Family size feature (family correlation)
- 10-fold CV (more stable estimates)

## Reality Check
- Target: 0.9642 is IMPOSSIBLE (top LB is ~0.8066)
- Our best LB: 0.8045 (top ~7%)
- Gap to top: 0.0021 (0.26%)
- We're already competitive. Need breakthrough for improvement.
- True baseline is ~0.817 CV, not 0.82 as we thought.