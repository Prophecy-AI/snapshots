## Current Status
- Best CV score: 0.8067 from exp_001 (baseline XGBoost)
- Best LB score: 0.7971 (from submission)
- CV-LB gap: +0.0097 (CV overestimates by ~1%)
- **Target Reality Check**: The target of 0.9642 appears unrealistic - top LB scores are ~0.8066. Our baseline is already competitive!

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Execution is sound, focus on strategy.
- Evaluator's top priority: 3-model ensemble with diverse preprocessing. **Agree** - this is the right direction for marginal gains.
- Key concerns raised: AnySpending dominance (0.82 importance) means model is learning a simple rule. **Addressing** by recommending feature engineering to create more nuanced spending features.

## Data Understanding
- Reference notebooks: See `exploration/eda.ipynb` for feature analysis
- Key patterns to exploit:
  1. **CryoSleep×HomePlanet interaction**: Europa+CryoSleep has 98.9% transported vs Earth+CryoSleep at 65.6% - this is a HUGE signal
  2. **Group consistency**: Groups have 100% consistency in HomePlanet - use for imputation
  3. **Spending patterns**: Zero spending strongly correlates with CryoSleep and Transported
  4. **Deck/Side**: Side='P' has 55.5% transported vs Side='S' at 44.5%

## Recommended Approaches (Priority Order)

### Priority 1: Feature Engineering to Break AnySpending Dominance
The model is over-relying on AnySpending (0.82 importance). Create more nuanced features:
1. **Spending ratios**: `RoomService / (TotalSpent + 1)`, `Spa / (TotalSpent + 1)` etc.
2. **Spending categories**: Luxury (Spa, VRDeck, RoomService) vs Basic (FoodCourt, ShoppingMall)
3. **Spending per age**: `TotalSpent / (Age + 1)` - spending behavior varies by age
4. **Spending bins**: Discretize spending into categories (none, low, medium, high)
5. **Remove AnySpending** from features to force model to learn from other signals

### Priority 2: Interaction Features (from EDA findings)
1. **CryoSleep × HomePlanet**: Create explicit interaction feature
2. **CryoSleep × Destination**: Similar interaction
3. **Deck × Side**: Cabin location interactions
4. **Age × CryoSleep**: Age groups behave differently in CryoSleep

### Priority 3: Group-Based Features
1. **GroupSpending**: Total spending of the group
2. **GroupCryoSleep**: Fraction of group in CryoSleep
3. **GroupHomePlanet**: Impute missing HomePlanet from group
4. **FamilySize**: Derived from group patterns

### Priority 4: 3-Model Ensemble (Evaluator Recommendation)
Create diverse models for ensembling:
1. **XGBoost** with current features (baseline)
2. **LightGBM** with same features (different algorithm)
3. **CatBoost** with categorical features (handles categoricals natively)
4. Simple averaging or stacking with logistic regression meta-learner

### Priority 5: Model Tuning
- Try removing AnySpending and see if other features become more important
- Increase regularization to prevent overfitting to single feature
- Try different tree depths (3-7 range)

## What NOT to Try
- Neural networks (tabular data, small dataset - trees are optimal)
- Complex stacking with many layers (diminishing returns)
- Hyperparameter tuning before feature engineering is complete

## Validation Notes
- Use 5-fold StratifiedKFold (current setup is good)
- CV-LB gap of ~1% is acceptable - CV is reliable
- Focus on CV improvement; LB will follow

## Key Insight from Kaggle Kernels
From top-voted kernel analysis:
- Spa + VRDeck + RoomService combination is highly predictive
- FoodCourt + RoomService pair has good classification ability
- Cabin parsing (Deck/Side) is standard and already implemented
- Target-guided encoding can help but watch for leakage

## Experiment Suggestions
1. **exp_002**: Feature engineering - spending ratios, interactions, remove AnySpending
2. **exp_003**: Add LightGBM and CatBoost models
3. **exp_004**: Simple ensemble (average of 3 models)
4. **exp_005**: Stacking ensemble with meta-learner