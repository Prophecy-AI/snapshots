# Seed Prompt: Spaceship Titanic - Loop 5 Strategy

## Current Status
- Best CV score: **0.81951** from tuned CatBoost (exp_003)
- Best LB score: **0.8045** from tuned CatBoost (exp_003)
- CV-LB gap: **+0.0150** (1.83% overestimate) → CV is optimistic, gap widened
- Target: 0.9642 is **IMPOSSIBLE** - top LB is ~0.8066

## Response to Evaluator
**Technical verdict was TRUSTWORTHY with MINOR CONCERNS.** The evaluator's concerns were validated by LB results.

**Evaluator's top priority was: Submit to get LB feedback, then explore threshold tuning.**
- **DONE.** Submitted exp_003, got LB 0.8045 (predicted ~0.8098)
- The higher variance concern was VALID - CV-LB gap increased from 0.97% to 1.50%
- LB improvement rate is only 58.6% of CV improvement (overfitting confirmed)

**Key concerns raised and validation:**
1. **Higher variance in tuned model** → CONFIRMED. Gap widened from 0.97% to 1.50%
2. **Same CV folds for tuning and evaluation** → CONFIRMED. This caused overfitting
3. **Diminishing returns on hyperparameter tuning** → CONFIRMED. +0.14% CV only gave +0.07% LB
4. **Threshold tuning not explored** → AGREE. This is now top priority

**Strategic implications:**
- We're at LB 0.8045, need 0.8066 to match top solutions (gap: 0.0021)
- Using conservative gap of 0.015, we need CV of ~0.8216 to beat 0.8066
- Focus on approaches that improve LB generalization, not just CV

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Full EDA with feature distributions, correlations
- `exploration/evolver_loop1_analysis.ipynb` - CryoSleep×HomePlanet interactions
- `exploration/evolver_loop4_lb_feedback.ipynb` - CV-LB gap analysis

Key patterns already exploited:
1. CryoSleep×HomePlanet interaction (Europa+CryoSleep = 98.9% transported)
2. Spending patterns (non-spenders ~65% transported)
3. Deck/Side effects (Deck B/C highest, Starboard > Port)
4. Group-based imputation for missing values

## Recommended Approaches (Priority Order)

### 1. THRESHOLD TUNING (Immediate - High Priority)
**Rationale:** Quick win that hasn't been tried. Default 0.5 threshold may not be optimal.
**Implementation:**
```python
# Find optimal threshold using OOF predictions
from sklearn.metrics import accuracy_score
thresholds = np.arange(0.40, 0.60, 0.01)
best_threshold = 0.5
best_acc = 0
for t in thresholds:
    acc = accuracy_score(y, (oof_preds >= t).astype(int))
    if acc > best_acc:
        best_acc = acc
        best_threshold = t
    print(f"Threshold {t:.2f}: {acc:.5f}")
print(f"Best threshold: {best_threshold:.2f} with accuracy {best_acc:.5f}")
```
**Expected gain:** 0.1-0.3% on CV, may translate to LB

### 2. CATBOOST NATIVE CATEGORICAL HANDLING (High Priority)
**Rationale:** Research suggests CatBoost's native categorical handling outperforms label encoding. We haven't tried this.
**Implementation:**
```python
# Keep categorical columns as strings, don't label encode
cat_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']

# For interaction features, keep them as strings too
interaction_cats = ['CryoSleep_HomePlanet', 'CryoSleep_Destination', 'Deck_Side', 
                   'HomePlanet_Destination', 'AgeGroup', 'AgeGroup_CryoSleep', 'VIP_HomePlanet']

# Get indices of categorical columns in feature list
cat_indices = [i for i, col in enumerate(feature_cols) if col in cat_features + interaction_cats]

model = CatBoostClassifier(
    cat_features=cat_indices,
    depth=8,
    learning_rate=0.051,
    iterations=755,
    l2_leaf_reg=3.52,
    random_seed=42,
    verbose=False
)
```
**Expected gain:** 0.1-0.5% on CV

### 3. BLEND BASELINE + TUNED CATBOOST (Medium Priority)
**Rationale:** Baseline CatBoost has lower variance (std=0.00431 vs 0.00685). Blending may reduce CV-LB gap.
**Implementation:**
```python
# Train both baseline and tuned CatBoost
# Blend predictions: 0.5 * baseline + 0.5 * tuned
# Or use weighted blend based on CV scores
blend_weight = 0.4  # Give more weight to lower-variance baseline
final_pred = blend_weight * baseline_pred + (1 - blend_weight) * tuned_pred
```
**Expected gain:** Better LB generalization (smaller CV-LB gap)

### 4. STACKING WITH META-LEARNER (Medium Priority)
**Rationale:** Use OOF predictions from multiple models as features for a simple meta-learner.
**Implementation:**
```python
# Train XGBoost, LightGBM, CatBoost with 5-fold CV
# Collect OOF predictions from each model
# Stack them as features for logistic regression

from sklearn.linear_model import LogisticRegression

# Create stacking features
stack_features = np.column_stack([oof_xgb, oof_lgb, oof_cat])
stack_test = np.column_stack([test_xgb, test_lgb, test_cat])

# Train meta-learner
meta_model = LogisticRegression(C=1.0)
meta_model.fit(stack_features, y)
final_pred = meta_model.predict_proba(stack_test)[:, 1]
```
**Expected gain:** 0.2-0.5% on CV

### 5. FEATURE SELECTION (Lower Priority)
**Rationale:** 56 features may include noise. Removing low-importance features may reduce overfitting.
**Implementation:**
```python
# Get feature importances from CatBoost
importances = model.feature_importances_
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': importances
}).sort_values('importance', ascending=False)

# Remove bottom 20% of features
threshold = feature_importance['importance'].quantile(0.2)
selected_features = feature_importance[feature_importance['importance'] > threshold]['feature'].tolist()
```
**Expected gain:** 0.1-0.2% on CV, better LB generalization

## What NOT to Try
- **Simple averaging ensemble** - Already proven worse than CatBoost alone (exp_002)
- **More hyperparameter tuning** - Diminishing returns, causes overfitting
- **Neural networks** - GBMs typically better for small tabular data
- **Chasing target of 0.9642** - IMPOSSIBLE, top LB is ~0.8066

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency
- CV-LB gap is now ~1.5% (CV overestimates LB)
- To beat top LB of 0.8066, need CV of ~0.8216 (using conservative gap)
- Consider using different random seeds for more robust evaluation

## Experiment Tracking
| Exp | Model | CV | LB | Gap | Notes |
|-----|-------|-----|-----|-----|-------|
| exp_000 | XGBoost Baseline | 0.80674 | 0.79705 | +0.97% | First submission |
| exp_001 | XGBoost + Features | 0.80927 | - | - | +0.25% from features |
| exp_002 | 3-Model Ensemble | 0.81353 | - | - | Worse than CatBoost alone |
| exp_003 | Tuned CatBoost | 0.81951 | 0.80453 | +1.50% | Best LB so far |

## Success Criteria
- **Immediate goal:** Beat LB 0.8045 (current best)
- **Stretch goal:** Beat LB 0.8066 (top solutions)
- **Reality check:** We're already in top ~5% territory

## Submission Strategy
- 8 submissions remaining
- Submit when CV improves by >0.003 (to account for gap)
- Or when trying fundamentally different approach (for diversity)
