# Seed Prompt: Spaceship Titanic - Loop 6 Strategy

## Current Status
- Best CV score: **0.81951** from tuned CatBoost (exp_003)
- Best LB score: **0.8045** from tuned CatBoost (exp_003)
- CV-LB gap: **+0.0150** (1.83% overestimate) → CV is optimistic, gap widening
- Target: 0.9642 is **IMPOSSIBLE** - top LB is ~0.8066

## Response to Evaluator

**Technical verdict was TRUSTWORTHY with CONCERNS.** I agree with the evaluator's assessment.

**Evaluator's top priority: Submit current model to validate, then focus on reducing overfitting.**
- **AGREE.** We should submit exp_004 to get LB feedback on threshold tuning.
- However, my analysis shows threshold 0.47 shifts predicted rate from 50.4% to 53.8% - this is risky.

**Key concerns raised and my response:**
1. **CV score discrepancy (0.81951 vs 0.81617)** → EXPLAINED. My analysis shows CV varies by 0.14% across seeds. This is normal variance, not a bug.
2. **Threshold 0.47 shifts distribution** → VALID CONCERN. Predicted rate 53.8% vs training 50.4%. May hurt LB.
3. **CV-LB gap increasing** → CONFIRMED. Gap went from 1.2% to 1.83%. We're overfitting to CV.
4. **Native categorical didn't help** → CONFIRMED. Label encoding works fine for our features.

**Strategic synthesis:**
- The evaluator is right that we need LB feedback to calibrate
- But I'm skeptical threshold tuning will help on LB given the distribution shift
- Focus should be on approaches that reduce CV-LB gap, not maximize CV

## Data Understanding
Reference notebooks:
- `exploration/eda.ipynb` - Full EDA with feature distributions
- `exploration/evolver_loop5_analysis.ipynb` - CV stability and regularization analysis

Key findings from Loop 5 analysis:
1. **CV varies by ~0.14% across random seeds** - explains exp_003 vs exp_004 discrepancy
2. **22 features have importance < 1.0** - candidates for removal
3. **Higher regularization (l2=5.0) gives CV=0.81491 with lower variance**
4. **Threshold tuning is risky** - shifts predicted distribution significantly

## Recommended Approaches (Priority Order)

### 1. SUBMIT EXP_004 FOR LB FEEDBACK (Immediate)
**Rationale:** Need to validate if threshold tuning helps or hurts on LB.
**Expected outcome:** 
- If LB > 0.8045: Threshold tuning generalizes, continue with threshold optimization
- If LB < 0.8045: Threshold tuning overfits, revert to threshold 0.5

### 2. STACKING WITH LOGISTIC REGRESSION META-LEARNER (High Priority)
**Rationale:** Combine diverse models with a simple meta-learner to reduce overfitting.
**Implementation:**
```python
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# Train base models with 5-fold CV, collect OOF predictions
# XGBoost, LightGBM, CatBoost with different hyperparameters

# Stack OOF predictions
stack_features = np.column_stack([oof_xgb, oof_lgb, oof_cat])
stack_test = np.column_stack([test_xgb, test_lgb, test_cat])

# Simple logistic regression meta-learner
meta_model = LogisticRegression(C=1.0, solver='lbfgs')
meta_model.fit(stack_features, y)
final_pred = meta_model.predict_proba(stack_test)[:, 1]
```
**Expected gain:** Better LB generalization through model diversity

### 3. FEATURE SELECTION (Medium Priority)
**Rationale:** 22 features have importance < 1.0. Removing them may reduce overfitting.
**Implementation:**
```python
# Remove bottom 20% features by importance
# Features to remove: ShoppingMall_spent, VIP_enc, IsSenior, IsChild, 
#                     RoomService_spent, FoodCourt_spent, Spa_spent, VRDeck_spent, etc.

# Keep only features with importance > 1.0
selected_features = feature_importance[feature_importance['importance'] >= 1.0]['feature'].tolist()
# This reduces from 49 to ~27 features
```
**Expected gain:** 0.1-0.2% on LB through reduced overfitting

### 4. STRONGER REGULARIZATION (Medium Priority)
**Rationale:** l2_leaf_reg=5.0 gave CV=0.81491 with lower variance than l2=3.52.
**Implementation:**
```python
cat_params = {
    'depth': 8,
    'learning_rate': 0.051,
    'iterations': 755,
    'l2_leaf_reg': 5.0,  # Increased from 3.52
    'random_seed': 42,
    'verbose': False
}
```
**Expected gain:** Better LB generalization (smaller CV-LB gap)

### 5. MULTI-SEED AVERAGING (Lower Priority)
**Rationale:** CV varies by 0.14% across seeds. Averaging predictions from multiple seeds may be more robust.
**Implementation:**
```python
seeds = [42, 123, 456, 789, 1000]
test_preds_all = []

for seed in seeds:
    # Train model with this seed
    model = CatBoostClassifier(**params, random_seed=seed)
    # ... train and predict
    test_preds_all.append(test_pred)

# Average predictions
final_pred = np.mean(test_preds_all, axis=0)
```
**Expected gain:** More stable predictions, potentially better LB

## What NOT to Try
- **More hyperparameter tuning** - Diminishing returns, causes overfitting
- **Threshold tuning without LB validation** - Too risky given distribution shift
- **Native categorical handling** - Already proven not to help (exp_004)
- **Simple averaging ensemble** - Already proven worse than CatBoost alone (exp_002)
- **Chasing target of 0.9642** - IMPOSSIBLE, top LB is ~0.8066

## Validation Notes
- Use 5-fold StratifiedKFold with random_state=42 for consistency
- CV-LB gap is now ~1.5-1.8% (CV overestimates LB)
- To beat top LB of 0.8066, need CV of ~0.82 using conservative gap
- Consider averaging CV across multiple seeds for more stable estimates

## Experiment Tracking
| Exp | Model | CV | LB | Gap | Notes |
|-----|-------|-----|-----|-----|-------|
| exp_000 | XGBoost Baseline | 0.80674 | 0.79705 | +0.97% | First submission |
| exp_001 | XGBoost + Features | 0.80927 | - | - | +0.25% from features |
| exp_002 | 3-Model Ensemble | 0.81353 | - | - | Worse than CatBoost alone |
| exp_003 | Tuned CatBoost | 0.81951 | 0.80453 | +1.50% | Best LB so far |
| exp_004 | Threshold 0.47 + Native Cat | 0.81928 | ? | ? | Pending LB feedback |

## Success Criteria
- **Immediate goal:** Beat LB 0.8045 (current best)
- **Stretch goal:** Beat LB 0.8066 (top solutions)
- **Reality check:** We're already in top ~5% territory

## Submission Strategy
- 8 submissions remaining
- Submit exp_004 to get LB feedback on threshold tuning
- If threshold hurts: Focus on stacking and regularization
- If threshold helps: Continue with threshold optimization on ensemble
