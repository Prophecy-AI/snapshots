{
  "query": "What are the most effective feature selection techniques for gradient boosting models to reduce overfitting?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Gradient boosting models such as XGBoost or LightGBM already produce a built\u2011in ranking of features (gain, cover, or weight), so **embedded feature\u2011selection** is often the first line of defence against overfitting.  By keeping only the top\u2011ranked variables\u2014e.g., those with the highest Gini gain or chi\u2011square weight\u2014redundant or noisy predictors are dropped, which both simplifies the tree structure and improves generalisation\u202f([IJDMS](https://aircconline.com/ijdms/V11N1/11119ijdms01.pdf)).  Many practitioners also apply **permutation importance** or **SHAP values** to capture non\u2011linear interactions and verify that the retained features truly drive predictions; these methods are listed as the most common approaches for gradient\u2011boosting feature selection\u202f([LinkedIn](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting)).\n\nBeyond the native importance scores, dedicated algorithms have been proposed.  **Gradient Boosted Feature Selection (GBFS)** modifies the boosting process itself to iteratively penalise unused features, yielding a sparse model that scales linearly with the number of dimensions while still detecting non\u2011linear interactions\u202f([Cornell PDF](https://www.cs.cornell.edu/~kilian/papers/gbfs.pdf)).  A related **boosting\u2011based framework** extracts feature rankings from fast tree\u2011boosters (e.g., XGBoost) and then selects a compact, low\u2011redundancy subset, often outperforming classic filter methods on benchmark data\u202f([ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0957417421012513)).  Hybrid meta\u2011heuristics such as **Artificial Bee Colony (ABC) combined with Gradient Boosting Decision Trees** also perform a global search for an optimal feature subset before model training, further curbing overfitting on high\u2011dimensional data\u202f([ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933)).\n\nIn practice, the most effective pipeline couples an **embedded importance filter** (e.g., top\u2011k gain or SHAP) with a **redundancy\u2011removal step** (hierarchical clustering or correlation\u2011based pruning) and, when computational budget allows, a **wrapper or meta\u2011heuristic search** (GBFS, ABC\u2011GBDT).  This combination trims irrelevant variables, respects complex interactions, and keeps the boosted trees shallow enough to maintain a favorable bias\u2011variance trade\u2011off, thereby reducing overfitting while preserving predictive power.",
      "url": ""
    },
    {
      "title": "",
      "text": "Gradient Boosted Feature Selection\nZhixiang (Eddie) Xu \u2217\nWashington University in St.\nLouis\nOne Brookings Dr.\nSt. Louis, USA\nxuzx@cse.wustl.edu\nGao Huang\nTsinghua University\n30 Shuangqing Rd.\nBeijing, China\nhuang\u0002g09@mails.tsinghua.edu.cn\nKilian Q. Weinberger\nWashington University in St.\nLouis\nOne Brookings Dr.\nSt. Louis, USA\nkilian@wustl.edu\nAlice X. Zheng\n\u2217\nGraphLab\n936 N. 34th St. Ste 208\nSeattle, USA\nalicez@graphlab.com\nABSTRACT\nA feature selection algorithm should ideally satisfy four con\u0002ditions: reliably extract relevant features; be able to iden\u0002tify non-linear feature interactions; scale linearly with the\nnumber of features and dimensions; allow the incorpora\u0002tion of known sparsity structure. In this work we propose a\nnovel feature selection algorithm, Gradient Boosted Feature\nSelection (GBFS), which satisfies all four of these require\u0002ments. The algorithm is flexible, scalable, and surprisingly\nstraight-forward to implement as it is based on a modifi\u0002cation of Gradient Boosted Trees. We evaluate GBFS on\nseveral real world data sets and show that it matches or out\u0002performs other state of the art feature selection algorithms.\nYet it scales to larger data set sizes and naturally allows for\ndomain-specific side information.\nCategories and Subject Descriptors\nH.3 [Information Storage and Retrieval]: Miscellaneous;\nI.5.2 [Pattern Recognition]: Design Methodology\u2014Fea\u0002ture evaluation and selection\nGeneral Terms\nLearning\nKeywords\nFeature selection; Large-scale; Gradient boosting\n\u2217Work done while at Microsoft Research\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD\u201914, August 24\u201327, 2014, New York, NY, USA.\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\nACM 978-1-4503-2956-9/14/08 ...$15.00.\nhttp://dx.doi.org/10.1145/2623330.2623635 .\n1. INTRODUCTION\nFeature selection (FS) [8] is an important problems in ma\u0002chine learning. In many applications, e.g., bio-informatics [21]\nor neuroscience [12], researchers hope to gain insight by ana\u0002lyzing how a classifier can predict a label and what features\nit uses. Moreover, effective feature selection leads to par\u0002simonious classifiers that require less memory [25] and are\nfaster to train and test [5]. It can also reduce feature extrac\u0002tion costs [29, 30] and lead to better generalization [9].\nLinear feature selection algorithms such as LARS [7] are\nhighly effective at discovering linear dependencies between\nfeatures and labels. However, they fail when features in\u0002teract in nonlinear ways. Nonlinear feature selection algo\u0002rithms, such as Random Forest [9] or recently introduced\nkernel methods [32, 23], can cope with nonlinear interac\u0002tions. But their computational and memory complexity typ\u0002ically grow super-linearly with the training set size. As data\nsets grow in size, this is increasingly problematic. Balancing\nthe twin goals of scalability and nonlinear feature selection\nis still an open problem.\nIn this paper, we focus on the scenario where data sets\ncontain a large number of samples. Specifically, we aim to\nperform efficient feature selection when the number of data\npoints is much larger than the number of features (n \u001d d).\nWe start with the (NP-Hard) feature selection problem that\nalso motivated LARS [7] and LASSO [26]. But instead of\nusing a linear classifier and approximating the feature selec\u0002tion cost with an l1-norm, we follow [31] and use gradient\nboosted regression trees [7] for which greedy approximations\nexist [2].\nThe resulting algorithm is surprisingly simple yet very ef\u0002fective. We refer to it as Gradient Boosted Feature Selection\n(GBFS). Following the gradient boosting framework, trees\nare built with the greedy CART algorithm [2]. Features are\nselected sparsely following an important change in the im\u0002purity function: splitting on new features is penalized by\na cost \u03bb > 0, whereas re-use of previously selected features\nincurs no additional penalty.\nGBFS has several compelling properties. 1. As it learns\nan ensemble of regression trees, it can naturally discover\nnonlinear interactions between features. 2. In contrast to,\ne.g., FS with Random Forests, it unifies feature selection\nand classification into a single optimization. 3. In contrast\nto existing nonlinear FS algorithms, its time and memory\ncomplexity scales as O(dn), where d denotes the number of\nfeatures dimensionality and n the number of data points1,\nand is very fast in practice. 4. GBFS can naturally incorpo\u0002rate pre-specified feature cost structures or side-information,\ne.g., select bags of features or focus on regions of interest,\nsimilar to generalized lasso in linear FS [19].\nWe evaluate this algorithm on several real-world data sets\nof varying difficulty and size, and we demonstrate that GBFS\ntends to match or outperform the accuracy and feature se\u0002lection trade-off of Random Forest Feature Selection, the\ncurrent state-of-the-art in nonlinear feature selection.\nWe showcase the ability of GBFS to naturally incorporate\nside-information about inter-feature dependencies on a real\nworld biological classification task [1]. Here, features are\ngrouped into nine pre-specified bags with biological mean\u0002ing. GBFS can easily adapt to this setting and select entire\nfeature bags. The resulting classifier matches the best accu\u0002racy of competing methods (trained on many features) with\nonly a single bag of features.\n2. RELATED WORK\nOne of the most widely used feature selection algorithms\nis Lasso [26]. It minimizes the squared loss with l1 regu\u0002larization on the coefficient vector, which encourages sparse\nsolutions. Although scalable to very large data sets, Lasso\nmodels only linear correlations between features and labels\nand cannot discover non-linear feature dependencies.\n[17] propose the Minimum Redundancy Maximum Rel\u0002evance (mRMR) algorithm, which selects a subset of the\nmost responsive features that have high mutual information\nwith labels. Their objective function also penalizes select\u0002ing redundant features. Though elegant, computing mu\u0002tual information when the number of instance is large is\nintractable, and thus the algorithm does not scale. HSIC\nLasso [32], on the other hand, introduces non-linearity by\ncombining multiple kernel functions that each uses a single\nfeature. The resulting convex optimization problem aligns\nthis kernel with a \u201cperfect\u201d label kernel. The algorithm re\u0002quires constructing kernel matrices for all features, thus its\ntime and memory complexity scale quadratically with input\ndata set size. Moreover, both algorithms separate feature\nselection and classification, and require additional time and\ncomputation for training classifiers using the selected fea\u0002tures.\nSeveral other works avoid expensive kernel computation\nwhile maintaining non-linearity. Grafting [18] combines l1\nand l0 regularization with a non-linear classifier based on\na non-convex variant of the multi-layer perceptron. Fea\u0002ture Selection for Ranking using Boosted Trees [15] selects\nthe top features with the highest relative importance scores.\n[27] and [9] use Random Forest. Finally, while not a fea\u0002ture selection method, [31] employ Gradient Boosted Trees\nto learn cascades of classifiers to reduce test-time cost by\nincorporating feature extraction budgets into the classifier\noptimization.\n3. BACKGROUND\nThroughout this paper we type vectors in bold (xi), scalars\nin regular math type (k or C), sets in cursive (S) and ma\u00021\nIn fact, if the storage of the input data is not counted, the\nmemory com...",
      "url": "https://www.cs.cornell.edu/~kilian/papers/gbfs.pdf"
    },
    {
      "title": "A framework for feature selection through boosting",
      "text": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- [View\u00a0**PDF**](https://www.sciencedirect.com/science/article/pii/S0957417421012513/pdfft?md5=78ec26f3711afe7314848b331e5d311c&pid=1-s2.0-S0957417421012513-main.pdf)\n- Download full issue\n\nSearch ScienceDirect\n\n## [Expert Systems with Applications](https://www.sciencedirect.com/journal/expert-systems-with-applications)\n\n[Volume 187](https://www.sciencedirect.com/journal/expert-systems-with-applications/vol/187/suppl/C), January 2022, 115895\n\n# A framework for feature selection through boosting\n\nAuthor links open overlay panelAhmadAlsahafa, NicolaiPetkova, VikramShenoyb, GeorgeAzzopardia\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.eswa.2021.115895](https://doi.org/10.1016/j.eswa.2021.115895) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0957417421012513&orderBeanReset=true)\n\nUnder a Creative Commons [license](http://creativecommons.org/licenses/by/4.0/)\n\nOpen access\n\n## Abstract\n\nAs dimensions of datasets in predictive modelling continue to grow, feature selection becomes increasingly practical. Datasets with complex feature interactions and high levels of redundancy still present a challenge to existing feature selection methods. We propose a novel framework for feature selection that relies on boosting, or sample re-weighting, to select sets of informative features in classification problems. The method uses as its basis the feature rankings derived from fast and scalable tree-boosting models, such as XGBoost. We compare the proposed method to standard feature selection algorithms on 9 benchmark datasets. We show that the proposed approach reaches higher accuracies with fewer features on most of the tested datasets, and that the selected features have lower redundancy.\n\n- Previous article in issue\n- Next article in issue\n\n## Keywords\n\nFeature selection\n\nBoosting\n\nEnsemble learning\n\nXGBoost\n\nLoading...\n\nRecommended articles\n\n\u00a9 2021 The Authors. Published by Elsevier Ltd.",
      "url": "https://www.sciencedirect.com/science/article/pii/S0957417421012513"
    },
    {
      "title": "Feature selection based on artificial bee colony and gradient boosting decision tree",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS1568494618305933)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S1568494618305933/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#preview-section-snippets)\n- [References (36)](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#preview-section-references)\n- [Cited by (450)](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/applied-soft-computing)\n\n## [Applied Soft Computing](https://www.sciencedirect.com/journal/applied-soft-computing)\n\n[Volume 74](https://www.sciencedirect.com/journal/applied-soft-computing/vol/74/suppl/C), January 2019, Pages 634-642\n\n[![Applied Soft Computing](https://ars.els-cdn.com/content/image/1-s2.0-S1568494618X00118-cov150h.gif)](https://www.sciencedirect.com/journal/applied-soft-computing/vol/74/suppl/C)\n\n# Feature selection based on artificial bee colony and gradient boosting decision tree\n\nAuthor links open overlay panelHaidiRaoab, XianzhangShiab, Ahoussou KouassiRodrigueab, JuanjuanFenga, YingchunXiaa, MohamedElhosenyd, XiaohuiYuanc, LichuanGuab\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.asoc.2018.10.036](https://doi.org/10.1016/j.asoc.2018.10.036) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S1568494618305933&orderBeanReset=true)\n\n## Highlights\n\n- \u2022\nA novel method for feature selection based on bee colony and decision tree.\n\n- \u2022\nThe proposed method improves efficiency and informative quality of the selected features.\n\n- \u2022\nExperiments conducted with breast cancer datasets demonstrate superior performance.\n\n\n## Abstract\n\nData from many real-world applications can be high dimensional and features of such data are usually highly redundant. Identifying informative features has become an important step for data mining to not only circumvent the curse of dimensionality but to reduce the amount of data for processing. In this paper, we propose a novel feature selection method based on bee colony and [gradient boosting](https://www.sciencedirect.com/topics/computer-science/gradient-boosting)[decision tree](https://www.sciencedirect.com/topics/computer-science/decision-trees) aiming at addressing problems such as efficiency and informative quality of the selected features. Our method achieves global optimization of the inputs of the [decision tree](https://www.sciencedirect.com/topics/computer-science/decision-trees) using the [bee colony algorithm](https://www.sciencedirect.com/topics/computer-science/bee-colony-algorithm) to identify the informative features. The method initializes the [feature space](https://www.sciencedirect.com/topics/engineering/feature-space) spanned by the dataset. Less relevant features are suppressed according to the information they contribute to the decision making using an [artificial bee colony algorithm](https://www.sciencedirect.com/topics/computer-science/artificial-bee-colony-algorithm). Experiments are conducted with two breast cancer datasets and six datasets from the public data repository. Experimental results demonstrate that the proposed method effectively reduces the dimensions of the dataset and achieves superior [classification accuracy](https://www.sciencedirect.com/topics/computer-science/classification-accuracy) using the selected features.\n\n## Introduction\n\nData from real-world applications can be of high dimensional. This is particularly true for the applications in the fields of medicine \\[1\\], \\[2\\], \\[3\\], \\[4\\] and remote sensing\u00a0\\[5\\]. For instance, mass spectrometry is a promising diagnostic and cancer biomarker discovery tool for cancers such as prostate, ovarian, breast, and bladder cancers\u00a0\\[6\\]. The superior sensitivities and specificities in contrast to the classical cancer biomarkers attract great attention of medical professionals. However, thousands of features of mass spectrometric data make feature selection a necessary step for effective processing and analysis in computer-aided diagnosis. Features in the mass spectrometric data are usually highly redundant, which is the cause of the well-known curse of dimensionality problem in machine learning\u00a0\\[7\\]. Identifying informative features has become an important step for data mining not only to circumvent the curse of dimension but to reduce the amount of data for processing. In general, feature selection reduces the number of features while keeping the same or even better learning performance\u00a0\\[8\\]. Its advantages have been demonstrated in various data mining and machine learning applications\u00a0\\[9\\], \\[10\\]. When redundant, irrelevant, noisy features are removed from the training dataset, the efficiency of the learning process is usually improved as well.\n\nFeature selection is responsible for selecting a subset of features, which can be described as a search process in a state space. There have been many methods developed for feature selection \\[11\\]. Alickovic et\u00a0al.\u00a0\\[3\\] proposed a decision-making system of breast cancer diagnosis. In this method, genetic algorithms are used to remove insignificant features and multiple classifiers are employed to classify breast cancer. Zhu et\u00a0al.\u00a0\\[12\\] proposed an unsupervised spectral feature selection method to preserve both the local and global structures of the features when removing irrelevant ones. Mafarja et\u00a0al.\u00a0\\[13\\] proposed three improvements based on Whale Optimization Algorithm (WOA) to optimize features in a dataset. When a simple mutation operator is used, the performance of the algorithm becomes better. Wan et\u00a0al.\u00a0\\[4\\] evaluated hierarchical feature selection methods for aging-related gene datasets. Wang et\u00a0al.\u00a0\\[14\\] construct the primary features of user comments about items and select features using Gradient Boosting Decision Tree (GBDT). GBDT\u00a0\\[15\\] was developed to identify the primary features of users\u2019 comments about items and could be efficient for feature selection. However, GBDT has a high demand for initial input when building trees. Redundant initial inputs could pose a significant challenge to the efficiency in both time and space. On the other hand, Artificial Bee Colony (ABC) algorithm has demonstrated great efficiency to convergence, which compliments the disadvantage of GBDT.\n\nTo address the open issues in feature selection of high dimensional data, we propose a method that is based on the coherent integration of artificial bee colony and gradient boosting decision tree algorithm (ABCoDT). To improve the feature selection using GBDT, initial inputs with a high dimensionality is optimized. We employ the accuracy of classification by GBDT to evaluate the quality of the inputs, which minimize the potential of ABC being trapped in a local optimum. Hence, the proposed algorithm achieves a global optimization and identifies the most informative features. The general idea is that the method initializes the feature space spanned by the dataset. Less relevant features are suppressed according to the information each feature contributes to the decision making using an artificial bee colony algorithm. Our method reduces the...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1568494618305933"
    },
    {
      "title": "What are the top feature selection techniques for Gradient Boosting?",
      "text": "Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn\u2019s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n``  ``  ``  ``  ``  ``\n\nLinkedIn\n\nLinkedIn is better on the app\n\nDon\u2019t have the app? Get it in the Microsoft Store.\n\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell)\n\n``\n\n`` `` `` `` `` ``\n\n[Skip to main content](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#base-template__workspace)\n\n`` `` `` ``\n\n# What are the top feature selection techniques for Gradient Boosting?\n\nPowered by AI and the LinkedIn community\n\n### 1\n\n[Filter methods](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#filter-methods)\n\nBe the first to add your personal experience\n\n### 2\n\n[Wrapper methods](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#wrapper-methods)\n\nBe the first to add your personal experience\n\n### 3\n\n[Embedded methods](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#embedded-methods)\n\nBe the first to add your personal experience\n\n### 4\n\n[Permutation methods](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#permutation-methods)\n\nBe the first to add your personal experience\n\n### 5\n\n[Shapley values](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#shapley-values)\n\nBe the first to add your personal experience\n\n### 6\n\n[Here\u2019s what else to consider](https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting#here%E2%80%99s-what-else-to-consider)\n\nBe the first to add your personal experience\n\nGradient boosting is a powerful machine learning algorithm that can improve the accuracy and performance of your data mining models. However, it also requires careful feature selection to avoid overfitting, noise, and redundancy. In this article, you will learn about the top feature selection techniques for gradient boosting, and how to apply them in your data mining projects.\n\n``\n\nFind expert answers in this collaborative article\n\nExperts who add quality contributions will have a chance to be featured. [Learn more](https://www.linkedin.com/help/linkedin/answer/a1652832)\n\nSee what others are saying\n\n``\n\n## 1Filter methods\n\nFilter methods are the simplest and fastest way to select features for gradient boosting. They use statistical measures, such as correlation, chi-square, or information gain, to rank the features according to their relevance to the target variable. Filter methods are independent of the learning algorithm, so they can be applied before training the model. However, they do not consider the interactions between the features, or the effect of the boosting process on the feature importance.\n\n`` `` ``\n\n[Add your perspective](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gradient-boosting&trk=article-ssr-frontend-x-article)\n\n`` `` `` `` `` `` `` `` `` `` `` ``\n\nHelp others by sharing more (125 characters min.)\n\nCancel\n\nAddSave\n\n``\n\n## 2Wrapper methods\n\nWrapper methods are more complex and computationally intensive than filter methods, but they can also produce more accurate and robust feature subsets for gradient boosting. They use the learning algorithm itself to evaluate the features, by applying a search strategy, such as forward, backward, or recursive elimination, to find the optimal combination of features that maximizes the model performance. Wrapper methods can capture the interactions between the features, and the impact of the boosting process on the feature importance, but they are also prone to overfitting and high variance.\n\n`` `` ``\n\n[Add your perspective](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gradient-boosting&trk=article-ssr-frontend-x-article)\n\n`` `` `` `` `` `` `` `` `` `` `` ``\n\nHelp others by sharing more (125 characters min.)\n\nCancel\n\nAddSave\n\n``\n\n## 3Embedded methods\n\nEmbedded methods are a hybrid of filter and wrapper methods, that integrate the feature selection process within the learning algorithm. They use regularization techniques, such as Lasso, Ridge, or Elastic Net, to penalize the complexity of the model, and shrink the coefficients of the less important features to zero. Embedded methods can achieve a balance between speed and accuracy, and reduce the risk of overfitting and high variance. However, they also require tuning the regularization parameters, and may not work well with categorical or noisy features.\n\n`` `` ``\n\n[Add your perspective](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gradient-boosting&trk=article-ssr-frontend-x-article)\n\n`` `` `` `` `` `` `` `` `` `` `` ``\n\nHelp others by sharing more (125 characters min.)\n\nCancel\n\nAddSave\n\n``\n\n## 4Permutation methods\n\nPermutation methods are a recent and innovative way to select features for gradient boosting, that use the concept of feature shuffling to estimate the feature importance. They randomly permute the values of each feature, and measure the change in the model performance. The features that cause a large drop in the performance are considered important, and the ones that cause a small or no change are considered irrelevant. Permutation methods can handle both numerical and categorical features, and are robust to noise and multicollinearity. However, they are also computationally expensive and sensitive to the number of permutations.\n\n`` `` ``\n\n[Add your perspective](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gradient-boosting&trk=article-ssr-frontend-x-article)\n\n`` `` `` `` `` `` `` `` `` `` `` ``\n\nHelp others by sharing more (125 characters min.)\n\nCancel\n\nAddSave\n\n``\n\n## 5Shapley values\n\nShapley values are another novel and powerful way to select features for gradient boosting, that use the concept of cooperative game theory to estimate the feature importance. They measure the contribution of each feature to the model prediction, by averaging over all possible combinations of features. The features that have a high positive or negative impact on the prediction are considered important, and the ones that have a low or zero impact are considered irrelevant. Shapley values can capture the non-linear and interactive effects of the features, and provide a fair and consistent attribution of the feature importance. However, they are also computationally demanding and complex to interpret.\n\n`` `` ``\n\n[Add your perspective](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gradient-boosting&trk=article-ssr-frontend-x-article)\n\n`` `` `` `` `` `` `` `` `` `` `` ``\n\nHelp others by sharing more (125 characters min.)\n\nCancel\n\nAddSave\n\n``\n\n## 6Here\u2019s what else to consider\n\nThis is a space to share examples, stories, or insights that don\u2019t fit into any of the previous sections. What else would you like to add?\n\n`` `` ``\n\n[Add your perspective](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gradient-boosting&trk=article-ssr-frontend-x-article)\n\n`` `` `` `` `` `` `` `` `` `` `` ``\n\nHelp others by sharing more (125 characters min.)\n\nCancel\n\nAddSave\n\n[Data Mining](https://www.linkedin.com/showcase/skills-data-mining/)\n\n### Data Mining\n\n[\\+ Follow](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fadvice%2F1%2Fwhat-top-feature-selection-techniques-gra...",
      "url": "https://www.linkedin.com/advice/1/what-top-feature-selection-techniques-gradient-boosting"
    },
    {
      "title": "",
      "text": "International Journal of Database Management Systems (IJDMS ) Vol.11, No.1, February 2019 \nDOI : 10.5121/ijdms.2019.11101 1 \nA XGBOOST RISK MODEL VIA FEATURE \nSELECTION AND BAYESIAN HYPER-PARAMETER \nOPTIMIZATION\nYan Wang1 and Xuelei Sherry Ni2\n1Graduate College, Kennesaw State University, Kennesaw, USA \n2Department of Statistics and Analytical Sciences, Kennesaw State University, \nKennesaw, USA \nABSTRACT\nThis paper aims to explore models based on the extreme gradient boosting (XGBoost) approach for \nbusiness risk classification. Feature selection (FS) algorithms and hyper-parameter optimizations are \nsimultaneously considered during model training. The five most commonly used FS methods including \nweight by Gini, weight by Chi-square, hierarchical variable clustering, weight by correlation, and weight \nby information are applied to alleviate the effect of redundant features. Two hyper-parameter optimization \napproaches, random search (RS) and Bayesian tree-structuredParzen Estimator (TPE), are applied in \nXGBoost. The effect of different FS and hyper-parameter optimization methods on the model performance \nare investigated by the Wilcoxon Signed Rank Test. The performance of XGBoost is compared to the \ntraditionally utilized logistic regression (LR) model in terms of classification accuracy, area under the \ncurve (AUC), recall, and F1 score obtained from the 10-fold cross validation. Results show that \nhierarchical clustering is the optimal FS method for LR while weight by Chi-square achieves the best \nperformance in XG-Boost. Both TPE and RS optimization in XGBoost outperform LR significantly. TPE \noptimization shows a superiority over RS since it results in a significantly higher accuracy and a \nmarginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE tuning shows a lower \nvariability than the RS method. Finally, the ranking of feature importance based on XGBoost enhances the \nmodel interpretation. Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an \noperative while powerful approach for business risk modeling. \nKEYWORDS\nExtreme gradient boosting; XGBoost; feature selection; Bayesian tree-structured Parzen estimator; risk\nmodeling \n1. INTRODUCTION\nRisk modeling is an effective tool to assist financial institutions to properly decide \nwhether or not to grant loans to business or other applicants [1]. Thereby, the problem of \nrisk modeling is transformed into a binary classification task, i.e., grant loans to low risk \napplicants or not grant to those with high risk. Logistic regression (LR) is a traditionally \nutilized technique for binary classificationsin the financial domain because of its easy \nimplementation, explainable results, as well as the similar and often better performance \ncompared to other binary classifiers such as decision trees and neural networks [2] [3] \n[4] [5] [6]. On the other hand, it has been shown that a single classifier cannot solve all \nproblems effectively while ensemble models have been revealed to be promising in \nmany credit risk studies [7] [8] [9]. One of the state-of-the-art ensemble approach is the \nextreme gradient boosting (XGBoost). It is a novel while advanced variant of the \nInternational Journal of Database Management Systems (IJDMS ) Vol.11, No.1, February 2019 \n2\ngradient boosting algorithm and has obtained promising results in many Kaggle machine \nlearning competitions [10]. Furthermore, XGBoost has been successfully applied in \nbankruptcy prediction and credit scoringin a few studies[11][12]. \nNumerous studies have focused on offering novel mechanisms to enhance the \nperformance of credit risk modeling. It has been demonstrated that feature selection (FS) \nis one of the efficient approaches in improving model performance because of its ability \nto alleviate the effects of noise and redundant variables [13]. Another method for model\u0002improving is the hyper-parameter optimization or tuning. It is shown that careful hyper\u0002parameter tuning tends to prevent the failure and reduce the over-fitting problem of \nXGBoost. The two main strategies used for finding the proper setting of hyper\u0002parameters in XGBoost are random search (RS) and Bayesian tree-structured Parzen \nestimator (TPE). They have demonstrated substantial influence on classification \nperformance [14] [15]. \nAfter careful paper review, we find that there is seldom research aiming at exploring the \neffect of FS and hyper-parameter optimizations simultaneously on XGBoost in the \nfinancial domain. Therefore, motivated by the aforementioned studies, we set up a series \nof experiments that contain FS methods and hyper-parameter optimizations \nsimultaneously, thereby exploring an accurate and comprehensive business risk model \nbased on XGBoost. The superiority of XGBoost over the widely used LR is evaluated \nvia classification accuracy, area under the curve (AUC), recall, and F1 score. Moreover, \nthe effect of different FS as well as hyper-parameter optimization methods on the model \nperformance is comprehensively investigated through the Wilcoxon signed rank test. \nFinally, the features are ranked according to their importance score to enhance the model \ninterpretation. \nThis paper has been structured as follows. Since different FS methods and XGBoost \nmodels along with the hyper-parameter optimization are used in this study, we will first \ndescribe the relevant algorithms in Section 2. Then the experimental design is discussed \nin Section 3. Section 4 demonstrates the experimental results and discussions.Finally, \nSection 5 addresses the conclusions. \n2. ALGORITHMS\nIn this section, the algorithms related to FS and XGBoost along with hyper-parameter \noptimizations are discussed. \n2.1. FEATURE SELECTION METHODS\nFS methods aims to filter the redundant variables and select the most appropriate subset of \nfeatures. By applying FS methods to the dataset, we can decrease the effect of the noise as \nwell as reduce the computational cost during the modeling stage. Many studies have shown \nthat FS can be used to increase the classification performance [13] [16]. \nIn this study, five commonly used FS methods are applied and evaluated: weight by gini \nindex, weight by chi-square, hierarchical variable clustering, weight by correlation, and \nweight by information gain ratio. For simplicity, we use the terms with initial capitalization \nto denote different FS methods. Therefore, Gini, Chi-square, Cluster, Correlation, and \nInformation are used to represent the aforementioned five FS approaches, respectively. In the \nGini FS method, the value of an attribute is evaluated via the gini impurity index. Similarly, \nChi-square, Correlation, and Information evaluates the relevance of the feature by calculating \nits chi-squared statistic, correlation, and information gain ratio with respect to the target \nvariable [17]. Features with higher values of gini index, chi-squared statistic, correlation, and \nInternational Journal of Database Management Systems (IJDMS ) Vol.11, No.1, February 2019 \n3\ninformation gain ratio are selected in the FS results. On the other hand, the Cluster method \nbases on the variable clustering analysis and selects the best feature within each cluster \naccording to the 1-R2 ratio defined in Eq. 1 [18]. Different from the rest of the four FS \nmethods, features with lower 1-R2 ratio are selected by the Cluster method. \n1 \u2212 \u0003\n\u0004\n\u0006\u0007\b\t\n =\n1 \u2212 \u0003\f\r\u000e_\u0010\u0011\u0012\u0013\u0014\u0015\u0016\n\u0004\n1 \u2212 \u0003\u000e\u0015\u0017\u0014_\u0010\u0011\f\u0013\u0015\u0013\u0014_\u0010\u0011\u0012\u0013\u0014\u0015\u0016\n\u0004\n(1)\n2.2. LOGISTIC REGRESSION\nLR is a standard binary classification technique widely used in industry because of its \nsimplicity and balanced error distribution [19] [21]. It outputs the conditional probability p of \nan observation that belongs to a specific class using the formula defined in Eq. 2, where (\u0018\u0019, \n\u0018\u0004, ..., \u0018\u001a) denotes the input variables while (\u001b\u001c. . . , \u001b\u001a) represents the unknown parameters \nthat need to be estimated.\np = \nexp (\u001b\u001c + \u001b\u0019 \u2217 \u0018\u0019 + \u001b\u0004 \u2217 \u0018\u0004 + \u22ef + \u001b\u001a \u2217 \u0018\u001a)\n1 + exp (\u001b\u001c + \u001b\u0019 \u2217 \u0018\u0019 + \u001b\u0004 \u2217 \u0018\u0004 + \u22ef + \u001b\u001a \u2217 \u0018\u001a)\n(\n(2)...",
      "url": "https://aircconline.com/ijdms/V11N1/11119ijdms01.pdf"
    },
    {
      "title": "Gradient Boosted Feature Selection",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/1901.04055"
    },
    {
      "title": "Overfitting: Striking the Balance: Preventing Overfitting in Gradient Boosting Models - FasterCapital",
      "text": "[Home](https://fastercapital.com) [Content](https://fastercapital.com/content/index.html) [Overfitting: Striking the Balance: Preventing Overfitting in Gradient Boosting Models](https://fastercapital.com/content/Overfitting--Striking-the-Balance--Preventing-Overfitting-in-Gradient-Boosting-Models.html)\n\n# Overfitting: Striking the Balance: Preventing Overfitting in Gradient Boosting Models\n\n**Updated: 10 Apr 2025****18 minutes**\n\nTable of Content\n\n[1\\. Introduction to Overfitting in Machine Learning](https://www.fastercapital.com/www.fastercapital.com#Introduction-to-Overfitting-in-Machine-Learning)\n\n[2\\. Understanding Gradient Boosting Models](https://www.fastercapital.com/www.fastercapital.com#Understanding-Gradient-Boosting-Models)\n\n[3\\. The Risks of Overfitting in Gradient Boosting](https://www.fastercapital.com/www.fastercapital.com#The-Risks-of-Overfitting-in-Gradient-Boosting)\n\n[4\\. Key Strategies to Prevent Overfitting](https://www.fastercapital.com/www.fastercapital.com#Key-Strategies-to-Prevent-Overfitting)\n\n[5\\. Implementing Cross-Validation Techniques](https://www.fastercapital.com/www.fastercapital.com#Implementing-Cross-Validation-Techniques)\n\n[6\\. Utilizing Regularization Methods](https://www.fastercapital.com/www.fastercapital.com#Utilizing-Regularization-Methods)\n\n[7\\. Quality Over Quantity](https://www.fastercapital.com/www.fastercapital.com#Quality-Over-Quantity)\n\n[8\\. A Practical Approach](https://www.fastercapital.com/www.fastercapital.com#A-Practical-Approach)\n\n[9\\. Maintaining Model Generalizability](https://www.fastercapital.com/www.fastercapital.com#Maintaining-Model-Generalizability)\n\n# Overfitting: Striking the Balance: Preventing Overfitting in Gradient Boosting Models\n\n## 1\\. Introduction to Overfitting in Machine Learning\n\nOverfitting in machine learning is akin to a student who memorizes facts for an exam rather than understanding the concepts; they'll perform well on known questions but fail to generalize to new problems. This phenomenon occurs when a model learns the training data too well, including its noise and outliers, leading to poor performance on unseen data. It's particularly prevalent in complex models with many parameters, such as deep neural networks, which can discern intricate patterns in data. However, these patterns often fail to represent the underlying distribution, resulting in a model that's excellent at recalling its training data but inadequate at predicting.\n\nFrom a statistical perspective, overfitting is when our model has a low bias but high variance, meaning it's sensitive to fluctuations in the training set. Conversely, underfitting\u2014its counterpart\u2014is characterized by high bias and low variance, where the model is too simplistic to capture the data's complexity.\n\nHere are some insights into overfitting from different perspectives:\n\n1\\. **Statistical Viewpoint**: Overfitting can be understood as a failure to meet the assumptions of the chosen statistical model. For instance, if we assume a [linear relationship in a non-linear](https://www.fastercapital.com/Linear-relationship--Linear-Relationship-Labyrinths--Tracing-Lines-of-Synchrony.html) dataset, the model will likely overfit.\n\n2\\. **Computational Learning Theory**: This field provides a theoretical framework to understand overfitting through concepts like VC dimension, which measures a model's capacity to fit various functions. A high VC dimension indicates a greater risk of overfitting.\n\n3\\. **Practical Aspect**: Practitioners often spot overfitting through performance metrics. If a model performs exceptionally on training data but poorly on validation data, it's a red flag for overfitting.\n\nTo combat overfitting, especially in gradient boosting models, consider the following strategies:\n\n1\\. **Simplifying the Model**: Reduce the complexity by limiting the number of layers or nodes in neural networks, or by choosing a simpler algorithm.\n\n2\\. **Cross-Validation**: Use techniques like k-fold cross-validation to ensure the model's performance is consistent across different subsets of the data.\n\n3\\. **Regularization**: Techniques like L1 and L2 regularization add a penalty for larger coefficients, discouraging the model from fitting the noise.\n\n4\\. **Pruning**: In decision trees, remove branches that have little power in predicting the target variable to reduce complexity.\n\n5\\. **Early Stopping**: Monitor the validation error and stop training when it begins to increase, indicating the model starts to overfit.\n\n6\\. **Ensemble Methods**: Combine multiple models to average out their predictions, reducing the likelihood of overfitting.\n\n7\\. **Data Augmentation**: Increase the size and diversity of the training set to make the model more robust.\n\nFor example, consider a gradient boosting model trained to predict housing prices. If the model gives undue importance to an irrelevant feature like the color of the houses, it might perform well on the training set but poorly on new data. Regularization can help by penalizing the model for giving too much weight to such features, thus encouraging it to focus on more general patterns that are likely to hold true for new data as well.\n\nIn summary, overfitting is a central [challenge in machine learning](https://www.fastercapital.com/Blood-Bank-Innovation-Challenge--AI-and-Machine-Learning-in-Blood-Banking--Transforming-the-Industry.html), requiring careful balance between model complexity and generalization ability. By understanding and applying the right techniques, we can steer our models towards making predictions that hold true across various scenarios, not just the data they were trained on.\n\nIntroduction to Overfitting in Machine Learning - Overfitting: Striking the Balance: Preventing Overfitting in Gradient Boosting Models\n\n## 2\\. Understanding Gradient Boosting Models\n\nGradient boosting models stand at the forefront of machine learning algorithms, especially when it comes to dealing with structured data. These powerful models combine the predictions of several base estimators, typically decision trees, to improve robustness over a single estimator. The idea is to sequentially add predictors to an ensemble, each one correcting its predecessor. However, this sequential nature inherently makes gradient boosting prone to overfitting if not managed correctly. The key to [success with gradient boosting](https://www.fastercapital.com/Gradient-boosting--Boosting-Your-Business-Success-with-Gradient-Boosting--A-Comprehensive-Guide.html) is understanding how it works and how to effectively control its complexity.\n\nFrom a **practical standpoint**, gradient boosting models are used because they can optimize on different loss functions and provide several hyperparameters that can be fine-tuned for prediction accuracy. From a **theoretical perspective**, they are fascinating because they can be seen as a form of functional gradient descent where the goal is to minimize a loss function by adding weak learners that predict the gradients of the loss.\n\nHere are some in-depth insights into understanding gradient boosting models:\n\n1\\. **Loss Function Optimization**: At its core, gradient boosting is about optimizing a loss function. A loss function quantifies how far off a prediction is from the actual result. By continuously reducing the loss, the model is trained to make more accurate predictions. For example, in a regression task, the mean squared error (MSE) might be used as a loss function.\n\n2\\. **Weak Learners and Additivity**: The base [learners in gradient boosting](https://www.fastercapital.com/Weak-Learners--Strength-in-Numbers--The-Power-of-Weak-Learners-in-Gradient-Boosting.html) are weak, meaning they do only slightly better than random guessing. Each new base learner is added to the ensemble with the aim of correcting the errors made by previous learners. This is done by fitting the new learner to the residual errors.\n\n3\\. **Regularization Techniques**: To prevent overfitting, gradient boosting emplo...",
      "url": "https://www.fastercapital.com/content/Overfitting--Striking-the-Balance--Preventing-Overfitting-in-Gradient-Boosting-Models.html"
    },
    {
      "title": "Notes on Parameter Tuning \uf0c1",
      "text": "Notes on Parameter Tuning &mdash; xgboost 3.1.1 documentation\n* [](../index.html)\n* [XGBoost Tutorials](index.html)\n* Notes on Parameter Tuning\n* [View page source](../_sources/tutorials/param_tuning.rst.txt)\n# Notes on Parameter Tuning[\uf0c1](#notes-on-parameter-tuning)\nParameter tuning is a dark art in machine learning, the optimal parameters\nof a model can depend on many scenarios. So it is impossible to create a\ncomprehensive guide for doing so.\nThis document tries to provide some guideline for parameters in XGBoost.\n## Understanding Bias-Variance Tradeoff[\uf0c1](#understanding-bias-variance-tradeoff)\nIf you take a machine learning or statistics course, this is likely to be one\nof the most important concepts.\nWhen we allow the model to get more complicated (e.g. more depth), the model\nhas better ability to fit the training data, resulting in a less biased model.\nHowever, such complicated model requires more data to fit.\nMost of parameters in XGBoost are about bias variance tradeoff. The best model\nshould trade the model complexity with its predictive power carefully.[Parameters Documentation](../parameter.html)will tell you whether each parameter\nwill make the model more conservative or not. This can be used to help you\nturn the knob between complicated model and simple model.\n## Control Overfitting[\uf0c1](#control-overfitting)\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\nThere are in general two ways that you can control overfitting in XGBoost:\n* The first way is to directly control model complexity.\n* This includes`max\\_depth`,`min\\_child\\_weight`,`gamma`,`max\\_cat\\_threshold`and other similar regularization parameters. See[XGBoost Parameters](../parameter.html)for a comprehensive\nset of parameters.\n* Set a constant`base\\_score`based on your own criteria. See[Intercept](intercept.html)for more info.\n* The second way is to add randomness to make training robust to noise.\n* This includes`subsample`and`colsample\\_bytree`, which may be used with boosting\nRF`num\\_parallel\\_tree`.\n* You can also reduce stepsize`eta`, possibly with a training callback. Remember to\nincrease`num\\_round`when you do so.\n## Handle Imbalanced Dataset[\uf0c1](#handle-imbalanced-dataset)\nFor common cases such as ads clickthrough log, the dataset is extremely imbalanced.\nThis can affect the training of XGBoost model, and there are two ways to improve it.\n* If you care only about the overall performance metric (AUC) of your prediction\n* Balance the positive and negative weights via`scale\\_pos\\_weight`\n* Use AUC for evaluation\n* If you care about predicting the right probability\n* In such a case, you cannot re-balance the dataset\n* Set parameter`max\\_delta\\_step`to a finite number (say 1) to help convergence\n## Use Hyper Parameter Optimization (HPO) Frameworks[\uf0c1](#use-hyper-parameter-optimization-hpo-frameworks)\nTuning models is a sophisticated task and there are advanced frameworks to help you. For\nexamples, some meta estimators in scikit-learn like[`sklearn.model\\_selection.HalvingGridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV)can help guide the search\nprocess. Optuna is another great option and there are many more based on different\nbranches of statistics.\n## Know Your Data[\uf0c1](#know-your-data)\nIt cannot be stressed enough the importance of understanding the data, sometimes that\u2019s\nall it takes to get a good model. Many solutions use a simple XGBoost tree model without\nmuch tuning and emphasize the data pre-processing step. XGBoost can help feature selection\nby providing both a global feature importance score and sample feature importance with\nSHAP value. Also, there are parameters specifically targeting categorical features, and\ntasks like survival and ranking. Feel free to explore them.\n## Reducing Memory Usage[\uf0c1](#reducing-memory-usage)\nIf you are using a HPO library like[`sklearn.model\\_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV),\nplease control the number of threads it can use. It\u2019s best to let XGBoost to run in\nparallel instead of asking> GridSearchCV\nto run multiple experiments at the same\ntime. For instance, creating a fold of data for cross validation can consume a significant\namount of memory:\n```\n# This creates a copy of dataset. X and X\\_train are both in memory at the same time.# This happens for every thread at the same time if you run `GridSearchCV` with# `n\\_jobs` larger than 1X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X,y)\n```\n```\ndf=pd.DataFrame()# This creates a new copy of the dataframe, even if you specify the inplace parameternew\\_df=df.drop(...)\n```\n```\narray=np.array(...)# This may or may not make a copy of the data, depending on the type of the dataarray.astype(np.float32)\n```\n```\n# np by default uses double, do you actually need it?array=np.array(...)\n```\nYou can find some more specific memory reduction practices scattered through the documents\nFor instances:[Distributed XGBoost with Dask](dask.html),[XGBoost GPU Support](../gpu/index.html). However, before going into\nthese, being conscious about making data copies is a good starting point. It usually\nconsumes a lot more memory than people expect.",
      "url": "https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html"
    }
  ]
}