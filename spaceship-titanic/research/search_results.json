{
  "query": "What is the highest accuracy achievable on Kaggle Spaceship Titanic competition and what techniques do top solutions use?",
  "results": [
    {
      "title": "ANSWER",
      "text": "\u22480.8066\u202faccuracy (\u224880.7\u202f%) is the highest reported leaderboard score, achieved by top\u20117% solutions that use extensive preprocessing (KNN imputation, one\u2011hot/target encoding, cabin deconstruction) together with powerful gradient\u2011boosting models such as LightGBM and often ensemble them with other classifiers\u202f(as of Jan\u202f2024)\u202f([medium.com](https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6), [github.com](https://github.com/Sayak9/Kaggle-Spaceship-Titanic-Competition)).",
      "url": ""
    },
    {
      "title": "Spaceship Titanic Kaggle competition top 7% score solution",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Spaceship Titanic Kaggle competition top 7% score solution\n\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\n\n7 min read\n\n\u00b7\n\nJan 17, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nKaggle\u2019s Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an \u201canomaly\u201d using records recovered from the spaceship\u2019s damaged computer system. The \u201clore\u201d of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship\u2019s passengers.\n\n## Import libraries\n\n```\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\n```\n\n## Let\u2019s Start\n\nAs you can see, after I loaded the dataset, I removed both the \u201cPassenger Id\u201d and \u201cName\u201d columns. They are not going to provide any useful or important information to the prediction. Someone\u2019s name or Id does not change the probability of being Transported.\n\n```\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\n```\n\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the \u201cCabin\u201d column. Notice that the rows on the \u201cCabin\u201d column follow a specific pattern. Something like this: \u201cA/5/S\u201d, \u201cC/1/S\u201d, \u201cF/7/P\u201d. And I decided to investigate it. So, to make things simple I split the rows of the \u201cCabin\u201d into three columns based on both slashes (\u201d/\u201d) of the rows. For example, the \u201cA/5/S\u201d row would be transformed into three new columns: The first one is named \u201ccabin\\_code\u201d referring tothe character behind the first slash (A). The second one named \u201cid\\_cabin\u201d refers to the character behind the second slash (5). The third one named \u201ccabin\\_sector\u201d refers to the character after the second slash (S). And we end up with three new columns.\n\n```\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\n```\n\nFirst of all, I noticed that \u201ccabin\\_code\u201d only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\n\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\n\nPress enter or click to view image in full size\n\nI did the same thing with the \u201ccabin\\_sector\u201d column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\n\nPress enter or click to view image in full size\n\nThis means that this exploration of the original \u201cCabin\u201d column is worth it since new insights are being added to the model.\n\nNow, we can finally delete the \u201cCabin\u201d column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\n\nI also removed the \u201ccabin\\_id\u201d and the column that I had created. As I said before, the Id will not interfere with the model\u2019s predictive ability.\n\nSo used: df.drop(columns=\\[\u201cCabin\u201d,\u201did\\_cabin\u201d\\], inplace=True) to drop both columns\n\nBefore splitting our data, the \u201cTransported\u201d column must be in a binary format. As you can see, I switched \u201cTrue\u201d for 1 and \u201cFalse\u201d for 0.\n\nBinary transformation: df\\[\u201cTransported\u201d\\] = df\\[\u201cTransported\u201d\\].map({True:1, False:0})\n\nI also removed every row that had missing values in the \u201ccabin\\_code\u201d column.\n\n```\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\n```\n\nNow, we can finally split the data and proceed to develop our model.\n\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\n\n```\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\n```\n\n```\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n```\n\n```\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\n```\n\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don\u2019t know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\n\n```\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\n```\n\nAnd with column transformer, we can attach both transformations to one variable that I named \u201ctransformer\u201d. Observe that we also have to specify the type of data to which the pipeline will be applied t...",
      "url": "https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6"
    },
    {
      "title": "GitHub - Sayak9/Kaggle-Spaceship-Titanic-Competition: Solution for the Kaggle Spaceship Titanic competition, achieving a leaderboard rank of 645 (Top 7%). This project features in-depth data preprocessing, feature engineering, and advanced imputation using KNNImputer. A LightGBM (LGBM) classifier was used to achieve the final competition score.",
      "text": "<div><div><article><p></p><h2><strong>Kaggle's Spaceship Titanic Competition - Top 7% Finish</strong></h2><a href=\"#kaggles-spaceship-titanic-competition---top-7-finish\"></a><p></p>\n<p>This repository contains my solution for the <a href=\"https://www.kaggle.com/competitions/spaceship-titanic\">Kaggle Spaceship Titanic competition</a>. The goal of this competition was to predict whether a passenger was transported to an alternate dimension based on personal records recovered from the spaceship's damaged computer system.</p>\n<p>This solution achieved a final leaderboard rank of <strong>645</strong>.</p>\n<p></p><h2><strong>Project Pipeline</strong></h2><a href=\"#project-pipeline\"></a><p></p>\n<p>The project follows a structured machine learning pipeline from data preprocessing to model training and final submission.</p>\n<p></p><h3><strong>1. Data Preprocessing &amp; Cleaning</strong></h3><a href=\"#1-data-preprocessing--cleaning\"></a><p></p>\n<ul>\n<li><strong>Unified Processing:</strong> The training and test datasets were combined to ensure consistent transformations were applied across all data.</li>\n<li><strong>Handling Missing Data:</strong> A key part of this project was the imputation strategy.\n<ul>\n<li>For numerical and related categorical features (<code>Age</code>, <code>RoomService</code>, <code>Spa</code>, etc.), <strong>K-Nearest Neighbors (KNN) Imputer</strong> was used to estimate missing values based on the nearest neighbors.</li>\n<li>Categorical text columns (<code>HomePlanet</code>, <code>Destination</code>) were imputed with a placeholder and then <strong>one-hot encoded</strong> to be used in the model.</li>\n</ul>\n</li>\n</ul>\n<p></p><h3><strong>2. Feature Engineering</strong></h3><a href=\"#2-feature-engineering\"></a><p></p>\n<p>New features were created to provide the model with more predictive signals:</p>\n<ul>\n<li><strong>Cabin Deconstruction:</strong> The <code>Cabin</code> feature was split into three distinct features: <code>Deck</code>, <code>Num</code>, and <code>Side</code>.</li>\n<li><strong>Spending Habits:</strong> The various spending columns (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, etc.) were aggregated to create new features like <code>amt_spent</code> (total amount) and <code>mean_amt_spent</code>.</li>\n<li><strong>Composite Features:</strong> Several high-correlation features were combined to create new composite signals for the model.</li>\n</ul>\n<p></p><h3><strong>3. Model Selection &amp; Training</strong></h3><a href=\"#3-model-selection--training\"></a><p></p>\n<p>Five different classification models were trained and evaluated to find the best performer:</p>\n<ol>\n<li>Logistic Regression (<code>Accuracy: ~77.0%</code>)</li>\n<li>Decision Tree Classifier (<code>Accuracy: ~74.7%</code>)</li>\n<li>Random Forest Classifier (<code>Accuracy: ~79.1%</code>)</li>\n<li>XGBoost Classifier (<code>Accuracy: ~79.4%</code>)</li>\n<li><strong>LightGBM (LGBM) Classifier (<code>Accuracy: ~79.7%</code>)</strong></li>\n</ol>\n<p>The <strong>LGBM Classifier</strong> provided the highest accuracy on the validation set and was selected to generate the final predictions for the submission file.</p>\n<p></p><h2><strong>Key Libraries &amp; Tools</strong></h2><a href=\"#key-libraries--tools\"></a><p></p>\n<ul>\n<li><strong>Data Manipulation:</strong> <code>Pandas</code>, <code>NumPy</code></li>\n<li><strong>Machine Learning:</strong> <code>Scikit-learn</code>, <code>XGBoost</code>, <code>LightGBM</code></li>\n<li><strong>Data Visualization:</strong> <code>Matplotlib</code>, <code>Seaborn</code></li>\n</ul>\n</article></div></div>",
      "url": "https://github.com/Sayak9/Kaggle-Spaceship-Titanic-Competition"
    },
    {
      "title": "How To Score ~80% Accuracy in Kaggle's Spaceship Titanic ...",
      "text": "<div><div><div><h2>This is a step-by-step guide to walk you through submitting a \u201c.csv\u201d file of predictions to Kaggle for the new titanic competition.</h2><div><div><a href=\"https://medium.com/@ZaynabAwofeso?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/codex?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div></div></div><figure><figcaption>image by <a href=\"http://unsplash.com\">unsplash</a></figcaption></figure><h2>Introduction</h2><p>Kaggle recently launched a fun competition called Spaceship Titanic. It is designed to be an update of the popular Titanic competition which helps people new to data science learn the basics of machine learning, get acquainted with Kaggle\u2019s platform, and meet others in the community. This article is a beginner-friendly analysis of the Spaceship Titanic Kaggle Competition. It covers steps to obtain any meaningful insights from the data and to predict the \u201cground truth\u201d for the test set with an accuracy of ~80% using RandomForestClassifier.</p><h2>Index</h2><ol><li>Problem definition and metrics</li><li>About the data</li><li>Exploratory Data Analysis</li><li>Data Cleaning and preprocessing</li><li>Feature Extraction and Feature Selection</li><li>Baseline Model Performance and Model Building</li><li>Submission and Feature Importance</li></ol><h2>1. Problem definition and metrics</h2><p>As the first thing, we have to understand the problem. It\u2019s the year 2912 and the interstellar passenger liner Spaceship Titanic has collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension! To help rescue crews retrieve the lost passengers, we are challenged to use records recovered from the spaceship\u2019s damaged computer system to predict which passengers were transported to another dimension.</p><blockquote><p>This problem is a binary class classification problem where we have to predict which passengers were transported to an alternate dimension or not, and we will be using accuracy as a metric to evaluate our results.</p></blockquote><h2>2. About the data</h2><p>We will be using 3 CSV files:</p><ul><li><strong>train file</strong> (spaceship_titanic_train.csv) \u2014 contains personal records of the passengers that would be used to build the machine learning model.</li><li><strong>test file</strong> (spaceship_titanic_test.csv) \u2014 contains personal records for the remaining one-third (~4300) of the passengers, but not the target variable (i.e. the value of Transported for the passengers). It will be used to see how well our model performs on unseen data.</li><li><strong>sample submission file</strong> (sample_submission.csv) \u2014 contains the format in which we have to submit our predictions.</li></ul><p>We will be using python for this problem. You can download the dataset from Kaggle <a href=\"https://www.kaggle.com/competitions/spaceship-titanic/data\">here</a>.</p><p><strong>Import required libraries</strong></p><figure></figure><p><strong>Reading Data</strong></p><figure></figure><p>Let\u2019s make a copy of the train and test data so that even if we make any changes to these datasets it would not affect the original datasets.</p><figure></figure><p>We will look at the structure of the train and test dataset next. We will first check the features present, then we will look at their data types.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name', 'Transported'],<br/> dtype='object')</span></pre><p>We have 13 independent variables and 1 target variable (Transported) in the training dataset. Let\u2019s also look at the columns of the test dataset.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name'],<br/> dtype='object')</span></pre><p>We have similar features in the test dataset as the training dataset except Transported that we will predict using the model built by the train data.</p><p>Given below is the description for each variable.</p><ul><li><strong>PassengerId</strong> \u2014 A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</li><li><strong>HomePlanet</strong> \u2014 The planet the passenger departed from, typically their planet of permanent residence.</li><li><strong>CryoSleep </strong>\u2014 Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</li><li><strong>Cabin</strong> \u2014 The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</li><li><strong>Destination</strong> \u2014 The planet the passenger will be debarking to.</li><li><strong>Age</strong> \u2014 The age of the passenger.</li><li><strong>VIP </strong>\u2014 Whether the passenger has paid for special VIP service during the voyage.</li><li><strong>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck</strong> \u2014 Amount the passenger has billed at each of the Spaceship Titanic\u2019s many luxury amenities.</li><li><strong>Name</strong> \u2014 The first and last names of the passenger.</li><li><strong>Transported</strong> \u2014 Whether the passenger was transported to another dimension. This is the target, the column we are trying to predict.</li></ul><p>Let\u2019s print data types for each variable of the training dataset.</p><figure></figure><pre><span>PassengerId object<br/>HomePlanet object<br/>CryoSleep object<br/>Cabin object<br/>Destination object<br/>Age float64<br/>VIP object<br/>RoomService float64<br/>FoodCourt float64<br/>ShoppingMall float64<br/>Spa float64<br/>VRDeck float64<br/>Name object<br/>Transported bool<br/>dtype: object</span></pre><p>We can see there are three formats of data types in the training dataset:</p><ul><li><strong>object</strong> (Categorical variables) \u2014 The categorical variables in the training dataset are: PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP and Name</li><li><strong>float64</strong> (Float variables i.e Numerical variables which have some decimal values involved) \u2014 The Numerical variables in our train dataset: Age, RoomService, FoodCourt, ShoppingMall, Spa and VRDeck</li><li><strong>bool</strong> (Boolean variables i.e. a variable that has one of two possible values e.g. True or False) \u2014 The Boolean Variable in our dataset is Transported</li></ul><p>Let\u2019s look at the shape of our train and test dataset.</p><figure></figure><pre><span>The shape of the train dataset is: (8693, 14)<br/>The shape of the test dataset is: (4277, 13)</span></pre><p>We have 8693 rows and 14 columns in the training dataset and 4277 rows and 13 columns in the test dataset.</p><ol><li>Exploratory Data Analysis</li></ol><p><strong><em>Univariate Analysis</em></strong></p><p>Univariate analysis is the simplest form of analyzing data where we examine each data individually to understand the distribution of its values.</p><p><strong><em>Target Variable</em></strong></p><p>We will first look at the target variable i.e. Transported. Since it is a categorical variable, let us look at its percentage distribution and bar plot.</p><figure></figure><pre><span>True 0.503624<br/>False 0.496376<br/>Name: Transported, dtype: float64</span></pre><figure></figure><figure></figure><p>Out of 8693 passengers in the train dataset, 4378 (about 50%) were Transported to another dimension.</p><p>Let\u2019s visualize the Independent categorical features next.</p><p><strong><em>...",
      "url": "https://medium.com/codex/how-to-score-80-accuracy-in-kaggles-spaceship-titanic-competition-using-random-forest-classifier-e7d06ce25bad"
    },
    {
      "title": "Spaceship Titanic: A complete guide - Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p><div><a href=\"https://www.kaggle.com/cookies\"><p>Learn more</p></a><p>OK, Got it.</p></div></div><div><div><div><a href=\"https://www.kaggle.com/samuelcortinhas\"></a><p><span><span><span>Samuel Cortinhas </span></span><span> \u00b7 <span>3y ago</span> \u00b7 70,147 views</span></span></p><div><p></p></div></div></div><div><div><p></p><h2>\ud83d\ude80 Spaceship Titanic: A complete guide \ud83c\udfc6</h2><p></p></div><div><p><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/notebook\"><span>Notebook</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/input\"><span>Input</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/output\"><span>Output</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/log\"><span>Logs</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide/comments\"><span>Comments (239)</span><span></span></a></p></div></div><div><div><div><h2>Runtime</h2><div><p>22m 26s</p></div><div><h2>Input</h2><div><p><span>COMPETITIONS</span></p><div><p><span></span></p><p>Spaceship Titanic</p><p></p></div></div></div><h2>Language</h2><p>Python</p><div><p></p><h2>Table of Contents</h2><p></p><div><p><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Introduction\"><span>Introduction</span><span></span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Libraries\"><span>Libraries</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Data\"><span>Data</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#EDA\"><span>EDA</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Feature-engineering\"><span>Feature engineering</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Missing-values\"><span>Missing values</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Preprocessing\"><span>Preprocessing</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Model-selection\"><span>Model selection</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Modelling\"><span>Modelling</span></a><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide#Submission\"><span>Submission</span></a></p></div></div></div><div><div><div><p></p><div><p>Competition Notebook</p><p><a href=\"https://www.kaggle.com/competitions/spaceship-titanic\">Spaceship Titanic</a></p></div></div><div><p>Public Score</p><p>0.80874</p></div><div><p>Best Score</p><p><a href=\"https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide?scriptVersionId=92521620\">0.80874 V47</a></p></div></div><div></div></div></div><div><div><h2>License</h2><p>This Notebook has been released under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">Apache 2.0</a> open source license.</p></div><div><h2>Continue exploring</h2><ul><li><div><a><div><div><p></p></div><div><p>Input</p><p><span>1 file</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Output</p><p><span>5 files</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Logs</p><p><span>1345.5 second run - successful</span></p></div></div></a></div></li><li><div><a><div><div><p></p></div><div><p>Comments</p><p><span>239 comments</span></p></div></div></a></div></li></ul></div></div></div></div></div></div>",
      "url": "https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide"
    },
    {
      "title": "The Prediction and Feature Importance Investigation in Titanic ...",
      "text": "The Prediction and Feature Importance Investigation in Titanic \nSurvival Prediction \nChutong Huang a\nBeijing Luhe High School International Academy, Beijing, China \nKeywords: Artificial Intelligence, Random Forest, Feature Importance. \nAbstract: Predicting the survival of Titanic passengers is one of the topics scientists are focusing on. This paper explores \nthe use of the Random Forest (RF) algorithm on a Titanic dataset and analyses the key features that influence \nthe predictions. The RF algorithm is applied to a processed dataset. Feature importance scores are returned \nfor each feature to demonstrate how much it is related to the survival prediction, and the scores are then \nanalyzed in their historical context. Age, fare and sex were found to be the three most significant features in \npredicting survival. Age is significant as its correlation to survivability, the children are determined to live \nwhile the elderly are unable to survive. Fare is a crucial attribute since it is correlated with passenger class, \nmeaning that those paying more are given better information and location to survive. Sex is important because \nwomen and children are given priority to survival, while men don\u2019t have that chance. The application of \nRandom Forests shows how well Artificial Intelligence (AI) algorithms can predict problems and spot \nsignificant patterns in complex data sets. And the analysis could have useful implications for improving \npredictive models in other areas where attributes are crucial. \n1 INTRODUCTION \nOn April 15, 1912, the RMS Titanic sideswiped an \niceberg on its first voyage, causing over 1,500 of the \n2,240 passengers and staff members on board to \nperish in the disaster. This notorious disaster soon \nbecame a warning and appears in many films, articles, \nand novels, warning people the danger of nature \n(Titanic History, 2024). Enhancing passenger \nsurvival in such disasters is an issue that needs to be \ntaken seriously, and Artificial Intelligence, as an \nemerging technology with strong feature extraction \nand prediction capabilities, can be considered in \nconjunction with this task. \nAI has evolved rapidly, with significant progress \nin machine learning, deep learning, and data \nanalytics. It utilizes sophisticated algorithms like \nlogistic regression, random forests, and neural \nnetworks to analyze complex data patterns, which is \nbeneficial in forecasting outcomes and improving \nchoices. These advancements have enabled more \naccurate predictions and enhanced decision-making \nacross various fields, such as medical science. Choi et \nal. constructed a Recurrent Neural Network (RNN)-\na https://orcid.org/0009-0003-2175-9681 \nbased model that predicts future events of patients \n(Choi, 2016). Wang et al. utilize a RNN model for the \nprediction of the future statues of Alzheimer's \nDisease for patients (Wang, 2018). Among the many \nprediction tasks, one important direction is the \nprediction of classification problems like survival of \na person. Hsieh et al. illustrated a model of Fuzzy \nHyper-Rectangular Composite Neural Network that \npredicts the survival through the first 24 hours \nphysical data of patients (Hsieh, 2014). Pradeep et al. \napplied machine learning algorithms include Naive \nBayes, classification trees, and Support Vector \nMachine (SVM) to the information of lung cancer \npatients and predict their survivability rate (Pradeep, \n2018). Kakde et al. determined the impact of each \nfeature to the survival rate and compared algorithms \nbetween SVM, decision tree, random forest, and \nlogistic regression (LR). They found that SVM as \nwell as logistic regression perform nearly the best, \nand there was high influence of age on survival while \nother features like passenger class, age, fare and \n\"sibsp\"(siblings and spouses) all have influences as \nwell (Kakde, 2018). Nair et al. analyzed the \ncorrelation between factors of passenger samples and \n60\nHuang and C.\nThe Prediction and Feature Importance Investigation in Titanic Survival Prediction.\nDOI: 10.5220/0013487400004619\nIn Proceedings of the 2nd International Conference on Data Analysis and Machine Learning (DAML 2024), pages 60-63\nISBN: 978-989-758-754-2\nCopyright \u00a9 2025 by Paper published under CC license (CC BY-NC-ND 4.0)\nthe survivability of the passengers. They suggested \nthat LR perform the best with the lowest false \ndiscovery rate and the highest accuracy. In their \nmodel of logistic regression, it tells them that the top5 \ncorrelated features are \"Pclass\"(Passenger class), \nsibsp, age, children, and sex (Singh, 2017). The \neffectiveness of AI methods has been demonstrated \non many domain tasks, so this paper intends to \nconsider the use of AI algorithms in predicting \nsurvival of people in Titanic disaster, but unlike \nprevious studies on the subject, this paper's focus is \nmore on the influence of passengers' features. \nIn this paper, based on the Kaggle dataset, the \nrandom forest algorithm was used for prediction and \nthe feature importance of each feature was compared, \nas well as the correlation between features and \nsurvival was analyzed. \n2 METHOD \n2.1 Data Preparation \nIn this study, a dataset from Kaggle is used (Kaggle, \n2017). It consists of 1309 samples of passengers on \nthe Titanic, each of these passengers are marked by 8 \nfeatures such as sex, age and fare. In terms of data \npreprocessing, it is divided into two parts. First, \nmissing values are handled by imputation using \nstatistical measures, and then categorical variables are \nconverted to numerical formats using label encoding. \n2.2 Machine Learning-based \nPrediction \n2.2.1 Introduction of Machine Learning \nWorkflow \nMachine learning generally involves several key \nsteps to create an effective predictive model. First, \ndata collection gathers relevant information from \nvarious sources to ensure a comprehensive dataset. \nNext, data preprocessing cleans and transforms this \ndata by fitting missing values, transforming \ncategorical variables, and then scaling features to \nprepare it for analysis. Feature selection follows, \nidentifying and retaining the most important variables \nthat contribute to the model\u2019s performance. Model \nselection involves choosing the appropriate algorithm \nbased on the features included in the dataset and the \npractical problem. It studies from the data to identify \npatterns during training and make predictions. \nPerformance evaluation assesses the model's \naccuracy and effectiveness using separate test data. \nFinally, model tuning adjusts the settings or \nparameters of the algorithm to enhance its \nperformance, ensuring the model is well-suited to the \nspecific problem and data. \n2.2.2 Random Forest \nRandom Forest (RF) is an accurate and effective \nsupervised machine learning method which combines \nvarious decision trees to form a \"forest.\" It is \napplicable to situations involving both regression and \nclassification. An additional kind of method for data \nclassification is a decision tree. It resembles a \nflowchart that clearly illustrates the process of \nprogressing choice. It starts at one beginning tree and \nproduces two or more branches in which each tree \nbranch giving a distinct set of possible outcomes. The \nRF model can achieve high prediction accuracy by \ncombining multiple decision trees. The principle \nbehind this is that several unrelated models of \ndecision tree produce a noticeable improvement when \nused together. In detail, each 'tree' in the 'forest' casts \na 'vote' for a problem, the forest integrates all the \nvotes and chooses the one with the majority of those \nvotes. RF takes different votes from multiple trees, \nwhich helps to increase the robustness and reduce the \noverfitting problem of the algorithm, which is its \ngreatest merit (Careerfoundry, 2023). \nIn this study, Random Forest is used in Python, \nscikit-learn (sklearn) provides a random forest \nclassifier library. After applying Random Forest to \nthe dataset, the 1309 survival predictions are fitted \n...",
      "url": "https://www.scitepress.org/Papers/2024/134874/134874.pdf"
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "How to Use Kaggle: Competitions",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions"
    },
    {
      "title": "Data pre-processing of spaceship-titanic Kaggle dataset for ...",
      "text": "Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy. | by Devang Chavda | Python in Plain English\n[Sitemap](https://python.plainenglish.io/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Python in Plain English\n](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-f090e722fd42---------------------------------------)\n\u00b7Follow publication\n[\n![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)\n](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-f090e722fd42---------------------------------------)\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\nFollow publication\n# Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy.\n[\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\n](https://medium.com/@chavdadevang23?source=post_page---byline--f090e722fd42---------------------------------------)\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--f090e722fd42---------------------------------------)\n7 min read\n\u00b7May 20, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/python-in-plain-english/f090e722fd42&amp;operation=register&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--f090e722fd42---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/f090e722fd42&amp;operation=register&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;source=---header_actions--f090e722fd42---------------------bookmark_footer------------------)\nListen\nShare\nData cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact the quality and effectiveness of analytical models.\nPress enter or click to view image in full size\n![]()\nPhoto by[Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\n[Competition Link](https://www.kaggle.com/competitions/spaceship-titanic)/[Dataset Link](https://www.kaggle.com/competitions/spaceship-titanic/data)\nPress enter or click to view image in full size\n![Leaderboard position top 20%]()\nScored 80.336%, Top 20% on leader-board (Image by Author)\nAlready got the preprocessed data and want to directly jump into the model-building part here is the part-2 of this article.\n[\n## Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\n### This article will walk you through detailed forward feature selection steps and model building from scratch, improving\u2026\npub.towardsai.net\n](https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c?source=post_page-----f090e722fd42---------------------------------------)\nLet&#x27;s start with exploring the dataset. Below are the features we have, and the definition of each feature you can find[here](https://www.kaggle.com/competitions/spaceship-titanic/data).\n```\n[&#x27;PassengerId&#x27;, &#x27;HomePlanet&#x27;, &#x27;CryoSleep&#x27;, &#x27;Cabin&#x27;, &#x27;Destination&#x27;, &#x27;Age&#x27;,\n&#x27;VIP&#x27;, &#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,\n&#x27;Name&#x27;, &#x27;Transported&#x27;]\n```\n```\ndf[&#x27;&#x27;Transported&#x27;&#x27;].value\\_counts() # balanced dataset\n# True 4378\n# False 4315\n```\nThe target class is well balanced here..! as this is a practice dataset otherwise in real-life data this is often not true.\nThe below function will help us to know what all unique values a categorical column contains and its count and portion in the dataset, it can give us a good understanding of the categorical features and we could also find some features with imbalance or features with low variance.\n```\ndef showDetails(column):\nprint(&#x27;------------------------------------------&#x27;)\nprint(column +&#x27; &amp; TRANSPORTED&#x27;)\nprint(df[column].value\\_counts())\ntempDict = dict(df[column][df[&#x27;&#x27;Transported&#x27;&#x27;] == True].value\\_counts())\nfor i in tempDict.keys():\ntempDict[i] = (tempDict[i]/len(df[df[column] == i]))\\*100\nprint(tempDict)\nprint(&#x27;------------------------------------------&#x27;)\nshowDetails(&#x27;HomePlanet&#x27;)\nshowDetails(&#x27;CryoSleep&#x27;)\nshowDetails(&#x27;Destination&#x27;)\nshowDetails(&#x27;VIP&#x27;)\n```\n```\nOutput :\nHomePlanet &amp; TRANSPORTED\nEarth 4602\nEuropa 2131\nMars 1759\nName: HomePlanet, dtype: int64\n{&#x27;Earth&#x27;: 42.39461103867884, &#x27;Europa&#x27;: 65.884561238855, &#x27;Mars&#x27;: 52.30244457077885}\n------------------------------------------\n------------------------------------------\nCryoSleep &amp; TRANSPORTED\nFalse 5439\nTrue 3037\nName: CryoSleep, dtype: int64\n{True: 81.75831412578202, False: 32.892075749218606}\n------------------------------------------\n------------------------------------------\nDestination &amp; TRANSPORTED\nTRAPPIST-1e 5915\n55 Cancri e 1800\nPSO J318.5-22 796\nName: Destination, dtype: int64\n{&#x27;TRAPPIST-1e&#x27;: 47.11749788672866, &#x27;55 Cancri e&#x27;: 61.0, &#x27;PSO J318.5-22&#x27;: 50.37688442211056}\n------------------------------------------\n------------------------------------------\nVIP &amp; TRANSPORTED\nFalse 8291\nTrue 199\nName: VIP, dtype: int64\n{False: 50.63321674104451, True: 38.19095477386934}\n```\nExample:{\u2018Earth\u2019: 42.39461103867884, \u2018Europa\u2019: 65.884561238855, \u2018Mars\u2019: 52.30244457077885}\nso, 42.4% of passengers from Earth here have been transported successfully.\nLet us see if there exists any geometric pattern in the data, which can give us a clue on which type of feature engineering or which type of models will perform better for this dataset. Will be using t-sne for visualization.\n```\ndef plot\\_tsne(X,y,p=30,step=1000):\n&quot;&quot;&quot;\nX-features number (pandas dataframe)\ny-targets int (pandas series)\np-perplexity - points in neighourhood\nstep-Maximum number of iterations for the optimization. Should be at least 250.\n&quot;&quot;&quot;\ntargets = y.astype(int)\ntsne = manifold.TSNE(n\\_components=2,perplexity=p,n\\_iter=step ,random\\_state=42)\ntransformed\\_data = tsne.fit\\_transform(X.iloc[:100,:])\ntsne\\_df = pd.DataFrame(np.column\\_stack((transformed\\_data, targets[:100])),columns=[&quot;&quot;x&quot;&quot;, &quot;&quot;y&quot;&quot...",
      "url": "https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42"
    }
  ]
}