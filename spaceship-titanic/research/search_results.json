{
  "query": "What are the best KNN imputation techniques for missing values in tabular data with mixed categorical and numerical features?",
  "results": [
    {
      "title": "ANSWER",
      "text": "K\u2011Nearest\u2011Neighbors (KNN) imputation fills each missing entry with a value derived from the\u202f*K*\u202fmost similar rows in the dataset. In scikit\u2011learn the core implementation is **`sklearn.impute.KNNImputer`**, which computes distances on the features that are present in both the target row and a candidate neighbor and then averages the neighbor values (uniformly or weighted by inverse distance)\u202f([scikit\u2011learn](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)). Because the built\u2011in class only accepts numeric arrays, the most reliable way to handle mixed categorical\u2011numerical tables is to **encode the categoricals first** (e.g., ordinal or one\u2011hot encoding), **scale the numeric columns**, and then apply the KNN imputer in a pipeline\u202f([Microsoft\u202fKNNDataImputer](https://microsoft.github.io/responsible-ai-toolbox-mitigations/dataprocessing/imputer/knn.html)). The Microsoft wrapper explicitly adds an optional encoder step, allowing categorical features to be processed while still using the underlying scikit\u2011learn algorithm\u202f([Microsoft\u202fKNNDataImputer](https://microsoft.github.io/responsible-ai-toolbox-mitigations/dataprocessing/imputer/knn.html)).\n\nEmpirical studies show that KNN imputation is especially effective when a substantial proportion of data is missing. In a 2023 comparison of simple and machine\u2011learning imputation methods, KNN achieved the **lowest mean\u2011squared error** for both numerical and categorical datasets when missingness exceeded\u202f20\u202f% and remained competitive up to\u202f50\u202f% missing values\u202f([QTAnalytics\u202f2023](https://qtanalytics.in/journals/index.php/JREAS/article/view/1821)). For purely categorical clinical records, KNN also recorded the **highest prediction accuracy** among methods such as mode, random\u2011forest, and multiple imputation\u202f([ScienceDirect\u202f2023](https://www.sciencedirect.com/science/article/pii/S2352914823002289)). The same study identified **k\u202f=\u202f5** as a sweet spot, with\u202fk\u202f=\u202f10 giving similar performance, while smaller\u202fk\u202f(e.g.,\u202f3) was less stable\u202f([QTAnalytics\u202f2023](https://qtanalytics.in/journals/index.php/JREAS/article/view/1821)).\n\nPutting these findings together, the \u201cbest\u201d KNN\u2011based workflow for mixed data typically includes:  \n\n1. **Pre\u2011processing** \u2013 encode categoricals (ordinal encoding is fast; one\u2011hot can be used if distance metrics are adapted) and standardize numerics to avoid scale bias\u202f([Medium\u202f2025](https://medium.com/@tarangds/knn-imputation-the-complete-guide-146f932870a7)).  \n2. **Distance & weighting** \u2013 use the default Euclidean distance on the transformed space, optionally switch to **`weights='distance'`** so nearer neighbors influence the imputed value more.  \n3. **Neighbour count** \u2013 start with **k\u202f=\u202f5**; increase to\u202f10 if the dataset is large or missingness is very high, and validate with cross\u2011validation.  \n\nThis pipeline leverages the robust, well\u2011maintained scikit\u2011learn implementation while extending it to categorical features, delivering accurate imputations across a wide range of missing\u2011data scenarios.",
      "url": ""
    },
    {
      "title": "KNNImputer #",
      "text": "KNNImputer &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# KNNImputer[#](#knnimputer)\n*class*sklearn.impute.KNNImputer(*\\**,*missing\\_values=nan*,*n\\_neighbors=5*,*weights='uniform'*,*metric='nan\\_euclidean'*,*copy=True*,*add\\_indicator=False*,*keep\\_empty\\_features=False*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/71cfff335/sklearn/impute/_knn.py#L24)[#](#sklearn.impute.KNNImputer)\nImputation for completing missing values using k-Nearest Neighbors.\nEach sample\u2019s missing values are imputed using the mean value from`n\\_neighbors`nearest neighbors found in the training set. Two samples are\nclose if the features that neither is missing are close.\nRead more in the[User Guide](../impute.html#knnimpute).\nAdded in version 0.22.\nParameters:**missing\\_values**int, float, str, np.nan or None, default=np.nan\nThe placeholder for the missing values. All occurrences of`missing\\_values`will be imputed. For pandas\u2019 dataframes with\nnullable integer dtypes with missing values,`missing\\_values`should be set to np.nan, since`pd.NA`will be converted to np.nan.\n**n\\_neighbors**int, default=5\nNumber of neighboring samples to use for imputation.\n**weights**{\u2018uniform\u2019, \u2018distance\u2019} or callable, default=\u2019uniform\u2019\nWeight function used in prediction. Possible values:\n* \u2018uniform\u2019 : uniform weights. All points in each neighborhood are\nweighted equally.\n* \u2018distance\u2019 : weight points by the inverse of their distance.\nin this case, closer neighbors of a query point will have a\ngreater influence than neighbors which are further away.\n* callable : a user-defined function which accepts an\narray of distances, and returns an array of the same shape\ncontaining the weights.\n**metric**{\u2018nan\\_euclidean\u2019} or callable, default=\u2019nan\\_euclidean\u2019\nDistance metric for searching neighbors. Possible values:\n* \u2018nan\\_euclidean\u2019\n* callable : a user-defined function which conforms to the definition\nof`func\\_metric(x,y,\\*,missing\\_values=np.nan)`.`x`and`y`corresponds to a row (i.e. 1-D arrays) of`X`and`Y`, respectively.\nThe callable should returns a scalar distance value.\n**copy**bool, default=True\nIf True, a copy of X will be created. If False, imputation will\nbe done in-place whenever possible.\n**add\\_indicator**bool, default=False\nIf True, a[`MissingIndicator`](sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator)transform will stack onto the\noutput of the imputer\u2019s transform. This allows a predictive estimator\nto account for missingness despite imputation. If a feature has no\nmissing values at fit/train time, the feature won\u2019t appear on the\nmissing indicator even if there are missing values at transform/test\ntime.\n**keep\\_empty\\_features**bool, default=False\nIf True, features that consist exclusively of missing values when`fit`is called are returned in results when`transform`is called.\nThe imputed value is always`0`.\nAdded in version 1.2.\nAttributes:**indicator\\_**[`MissingIndicator`](sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator)\nIndicator used to add binary indicators for missing values.`None`if add\\_indicator is False.\n**n\\_features\\_in\\_**int\nNumber of features seen during[fit](../../glossary.html#term-fit).\nAdded in version 0.24.\n**feature\\_names\\_in\\_**ndarray of shape (`n\\_features\\_in\\_`,)\nNames of features seen during[fit](../../glossary.html#term-fit). Defined only when`X`has feature names that are all strings.\nAdded in version 1.0.\nSee also\n[`SimpleImputer`](sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)\nUnivariate imputer for completing missing values with simple strategies.\n[`IterativeImputer`](sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)\nMultivariate imputer that estimates values to impute for each feature with missing values from all the others.\nReferences\n* [Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor\nHastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing\nvalue estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17\nno. 6, 2001 Pages 520-525.](https://academic.oup.com/bioinformatics/article/17/6/520/272365)\nExamples\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.imputeimportKNNImputer&gt;&gt;&gt;X=[[1,2,np.nan],[3,4,3],[np.nan,6,5],[8,8,7]]&gt;&gt;&gt;imputer=KNNImputer(n\\_neighbors=2)&gt;&gt;&gt;imputer.fit\\_transform(X)array([[1. , 2. , 4. ],[3. , 4. , 3. ],[5.5, 6. , 5. ],[8. , 8. , 7. ]])\n```\nFor a more detailed example see[Imputing missing values before building an estimator](../../auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py).\nfit(*X*,*y=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/71cfff335/sklearn/impute/_knn.py#L213)[#](#sklearn.impute.KNNImputer.fit)\nFit the imputer on X.\nParameters:**X**array-like shape of (n\\_samples, n\\_features)\nInput data, where`n\\_samples`is the number of samples and`n\\_features`is the number of features.\n**y**Ignored\nNot used, present here for API consistency by convention.\nReturns:**self**object\nThe fitted`KNNImputer`class instance.\nfit\\_transform(*X*,*y=None*,*\\*\\*fit\\_params*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/71cfff335/sklearn/base.py#L851)[#](#sklearn.impute.KNNImputer.fit_transform)\nFit to data, then transform it.\nFits transformer to`X`and`y`with optional parameters`fit\\_params`and returns a transformed version of`X`.\nParameters:**X**array-like of shape (n\\_samples, n\\_features)\nInput samples.\n**y**array-like of shape (n\\_samples,) or (n\\_samples, n\\_outputs), default=None\nTarget values (None for unsupervised transformations).\n**\\*\\*fit\\_params**dict\nAdditional fit parameters.\nPass only if the estimator accepts additional params in its`fit`method.\nReturns:**X\\_new**ndarray array of shape (n\\_samples, n\\_features\\_new)\nTransformed array.\nget\\_feature\\_names\\_out(*input\\_features=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/71cfff335/sklearn/impute/_knn.py#L388)[#](#sklearn.impute.KNNImputer.get_feature_names_out)\nGet output feature names for transformation.\nParameters:**input\\_features**array-like of str or None, default=None\nInput features.\n* If`input\\_features`is`None`, then`feature\\_names\\_in\\_`is\nused as feature names in. If`feature\\_names\\_in\\_`is not defined,\nthen the following input feature names are generated:`[&quot;x0&quot;,&quot;x1&quot;,...,&quot;&quot;x(n\\_features\\_in\\_-1)&quot;]`.\n* If`input\\_features`is an array-like, then`input\\_features`must\nmatch`feature\\_names\\_in\\_`if`feature\\_names\\_in\\_`is defined.\nReturns:**feature\\_names\\_out**ndarray of str objects\nTransformed feature names.\nget\\_metadata\\_routing()[[source]](https://github.com/scikit-learn/scikit-learn/blob/71cfff335/sklearn/utils/_metadata_requests.py#L1550)[#](#sklearn.impute.KNNImputer.get_metadata_routing)\nGet metadata routing of this object.\nPlease check[User Guide](../../metadata_routing.html#metadata-routing)on how the routing\nmechanism works.\nReturns:**routing**MetadataRequest\nA[`MetadataRequest`](sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest)encapsulating\nrouting information.\nget\\_params(*deep=True*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/71cfff335/sklearn/base.py#L240)[#](#sklearn.impute.KNNImputer.get_params)\nGet parameters for this estimator.\nParameters:**deep**bool, default=True\nIf True, will return the parameters for this estimator and\ncontained subobjects that are estimators.\nReturns:**params**dict\nParameter names mapped to their values.\nset\\_output(*\\**,*transform=None*)[[source]](https://github.com/scikit-learn/scikit...",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html"
    },
    {
      "title": "KNN Imputation: The Complete Guide",
      "text": "## KNN Imputation: A Complete Guide to Handling Missing Data with Precision and Accuracy.\n\n**Imputation**\n\n## What is KNN Imputation?\n\nK-Nearest Neighbors (KNN) imputation is a data preprocessing technique used to fill in missing values in a dataset. It leverages the similarity between data points to estimate missing values by considering the _K_ most similar observations (neighbors). Instead of using simple methods like mean or median imputation, KNN takes into account the relationships between different features, often leading to more accurate imputations.\n\n## Why Use KNN Imputation?\n\nMissing data is a common issue in real-world datasets, and improper handling can lead to biased analysis or incorrect conclusions. KNN imputation is particularly useful because:\n\n1. **Preserves Data Relationships** \u2014 Unlike mean/median imputation, which can distort variance and relationships, KNN retains the structure of the data.\n2. **Non-Parametric** \u2014 KNN does not make assumptions about the data distribution, making it flexible for various types of datasets.\n3. **Can Handle Categorical and Numerical Data** \u2014 Works well across different data types.\n4. **Works Well with Multivariate Data** \u2014 Uses multiple features to predict missing values instead of just one.\n\nHowever, it can be computationally expensive for large datasets and may not perform well if data is highly sparse.\n\n\ud83d\udd34 **Limitations:**\n\n\u274c **Computationally Expensive** \u2014 Slower for large datasets due to pairwise distance calculations.\n\n\u274c **Sensitive to Outliers** \u2014 Can introduce bias if nearest neighbors contain extreme values.\n\n\u274c **Requires Complete Data for Neighbors** \u2014 If too many missing values exist, it struggles to find suitable neighbors.\n\n## How to Use KNN Imputation?\n\nK-Nearest Neighbors (KNN) imputation replaces missing values by finding the \u2018K\u2019 most similar data points (neighbors) and averaging their values.\n\n**Steps:**\n\n1. **Standardize the Data** \u2014 Normalize numerical features to avoid bias due to different scales.\n2. **Choose K** \u2014 Select an appropriate number of neighbors ( `K`). Common values range from 3 to 10.\n3. **Compute Distance** \u2014 Use Euclidean, Manhattan, or other distance metrics to find the nearest neighbors.\n4. **Impute Missing Values** \u2014 Replace missing values with the mean (for continuous data) or mode (for categorical data) of the nearest neighbors.\n5. **Verify Results** \u2014 Check if the imputed values make sense in the context of the data.\n\n## When to Use KNN Imputation?\n\nKNN imputation is best used when:\n\n\u2705 **Missing data is random (MAR or MCAR)** \u2014 Works well when missing values are not structurally dependent.\n\n\u2705 **Small to moderate missingness (\u226430%)** \u2014 Too much missing data can distort nearest neighbor calculations.\n\n\u2705 **Dataset has meaningful correlations** \u2014 The method assumes that similar points can predict missing values.\n\n\u2705 **Nonlinear relationships exist** \u2014 KNN is flexible and captures complex patterns better than mean/mode imputation.\n\nKNN imputation is best suited for situations where:\n\n- The dataset has **moderate missing values** (typically 30%).\n- The missing data mechanism is **Missing at Random (MAR)** or **Missing Completely at Random (MCAR)**.\n- The dataset has **continuous or categorical variables** that exhibit strong relationships with other variables.\n- There is **no strong collinearity** that could lead to inaccurate imputations.\n- Computational resources are **sufficient**, as KNN can be slow for very large datasets.\n\n## When Not to Use KNN Imputation?\n\n- If the dataset is **too large**, as KNN requires pairwise distance calculations.\n- If the missingness is **not random** (e.g., systematic missing values), leading to biased imputations.\n- If the data is **highly sparse**, as there may not be enough information to make reliable imputations.\n\n## Where to Use KNN Imputation?\n\nKNN imputation is commonly applied in:\n\n\ud83d\udcca **Healthcare & Epidemiology** \u2014 Filling in missing patient data, lab results, or survey responses.\n\n\ud83d\udcc8 **Finance** \u2014 Handling missing stock prices or credit scores.\n\n\ud83d\udcca **Social Science Research** \u2014 Imputing survey responses in demographic studies.\n\n\ud83d\udce1 **Sensor Data** \u2014 Reconstructing missing environmental or IoT sensor readings.\n\n\ud83e\uddea **Genomics & Biology** \u2014 Handling missing gene expression or drug response data.\n\n## What to Use in KNN Imputation?\n\n**Distance Metric:**\n\n- Euclidean (default, best for continuous data)\n- Manhattan (for categorical or sparse data)\n- Cosine similarity (for high-dimensional data)\n\n**Choice of K:**\n\n- Small K (e.g., 3\u20135) \u2192 More local influence, higher variance\n- Large K (e.g., 10\u201315) \u2192 More generalization, risk of over-smoothing\n\n**Feature Scaling:**\n\n- Use `StandardScaler` or `MinMaxScaler` to normalize numerical data before imputation.\n\n## How to Assess If KNN Imputation Is Correct?\n\nTo evaluate KNN imputation quality:\n\n\ud83d\udd39 **Compare with Known Values** (if available)\n\n- If you have a subset of data with no missing values, artificially remove some values, impute them using KNN, and compare with the original values.\n- \ud83d\udc49 **What to check?**\n- If the imputed value is close to the original, the KNN imputation is working well.\n\n\ud83d\udd39 **Compare with Other Imputation Methods**\n\n- You can compare KNN imputation with other methods like **Mean/Median Imputation** or **Multiple Imputation** to see if KNN produces better results.\n- \ud83d\udc49 **What to check?**\n- If KNN imputation provides more reasonable values than simple mean imputation, it is a better choice.\n\n\ud83d\udd39 **Check for Consistency in Relationships**\n\nSince KNN imputation uses similarity-based techniques, the relationships between variables should be maintained after imputation.\n\n- \ud83d\udc49 **What to check?**\n- If the correlation structure changes significantly, it may indicate poor imputation.\n\n\ud83d\udd39 **Visual Inspection**\n\nBefore and after imputation, compare your dataset visually using:\n\n- **Descriptive statistics** (mean, median, standard deviation)\n- **Data distributions**\n- **Box plots or histograms**\n\n\ud83d\udc49 **What to check?**\n\n- If the mean, median, and standard deviation have changed drastically, it might indicate poor imputation.\n- Ensure that imputed values do not introduce outliers or unrealistic trends.\n\n\ud83d\udd39 **Cross-Validation with a Predictive Model**\n\n- RMSE (Root Mean Squared Error)\n- MAE (Mean Absolute Error)\n- A practical way to assess imputation quality is to train a machine learning model before and after imputation and compare performance.\n- \ud83d\udc49 **What to check?**\n- If model performance improves after imputation, the imputed values likely make sense.\n\n\ud83d\udd39 **Outlier Detection After Imputation**\n\nSince KNN imputation is based on similarity, it should not introduce extreme values.\n\n\ud83d\udc49 **What to check?**\n\n- If new extreme values appear after imputation, it may be a sign of poor imputation.\n\n## Conclusion\n\nTo verify the correctness of KNN imputation:\n\n\u2705 Compare **before and after statistics** (mean, std, correlation)\n\n\u2705 Use a **validation dataset** if available\n\n\u2705 Check **relationships between variables**\n\n\u2705 Compare with **other imputation methods**\n\n\u2705 Train a **predictive model** and evaluate performance\n\n\u2705 Detect **outliers after imputation**\n\nBy following these checks, you can ensure that your KNN-imputed data is reliable and suitable for analysis. \ud83d\ude80\n\n## Challenges and Considerations\n\n1. **Computational Complexity**\n\n- KNN imputation can be slow for large datasets since it requires pairwise distance calculations.\n\n**2\\. Choice of K**\n\n- A poorly chosen K value can lead to poor imputation. Hyperparameter tuning is needed.\n\n**3\\. Risk of Overfitting**\n\n- If too many nearest neighbors are used, the imputation may be overly smoothed.\n\n**4\\. Handling Different Data Types**\n\n- Scaling may be required for numerical variables. For mixed data types, appropriate distance metrics should be chosen.\n\n## Final Thoughts\n\nKNN imputation is a powerful yet simple method for handling missing data when relationships among variables exist. However, it should be used carefully, considering computational cost and...",
      "url": "https://medium.com/@tarangds/knn-imputation-the-complete-guide-146f932870a7"
    },
    {
      "title": "COMPARISON OF SIMPLE MISSING DATA IMPUTATION TECHNIQUES FOR NUMERICAL AND CATEGORICAL DATASETS",
      "text": "# COMPARISON OF SIMPLE MISSING DATA IMPUTATION TECHNIQUES FOR NUMERICAL AND CATEGORICAL DATASETS\n\n## Authors\n\n- Ramu Gautam\nDepartment of Electrical and Computer Engineering, University of Nevada Las Vegas\n- Shahram Latifi\nDepartment of Electrical and Computer Engineering, University of Nevada Las Vegas\n\n## DOI:\n\n[https://doi.org/10.46565/jreas.202381468-475](https://doi.org/10.46565/jreas.202381468-475)\n\n## Keywords:\n\nStatistical imputation techniques;, k-nearest neighbor imputation;, sensor data imputation;, MCAR;, MNAR;\n\n## Abstract\n\nAlmost every dataset has missing data. The common reasons are sensor error, equipment malfunction, human error, or translation loss. We study the efficacy of statistical (mean, median, mode) and machine learning based (k-nearest neighbors) imputation methods in accurately imputing missing data in numerical datasets with data missing not at random (MNAR) and data missing completely at random (MCAR) as well as categorical datasets. Imputed datasets are used to make prediction on the test set and Mean squared error (MSE) in prediction is used as the measure of performance of the imputation. Mean absolute difference between the original and imputed data is also observed. When the data is MCAR, kNN imputation results in lowest MSE for all datasets, making it the most accurate method. When less than 20% of data is missing, mean and median imputations are effective in regression problems. kNN imputation is better at 20% missingness and significantly better when 50% or more data is missing. For the kNN method, k = 5 gives better results than k=3 but k=10 gives similar results to k=5. For MNAR datasets, statistical methods result in similar or lower MSE compared to kNN imputation when less than 25% of instances have a missing feature. For higher missing levels, kNN imputation is superior. Given enough data points without missing features, deleting the instances with missing data may be a better choice at lower missingness levels. For categorical data imputation, kNN and Mode imputation are both effective.\n\n## Author Biographies\n\n### Ramu Gautam, Department of Electrical and Computer Engineering, University of Nevada Las Vegas\n\n**Ramu Gautam** is a PhD student in Electrical and Computer Engineering Department at University of Nevada Las Vegas. Currently his research is in computer vision, focusing on 3D and 4D biological images. He has a master\u2019s degree in Nanotechnology and a bachelor's degree in Electronics and Communication Engineering. He likes to go hiking and play table tennis in his free time.\n\n### Shahram Latifi, Department of Electrical and Computer Engineering, University of Nevada Las Vegas\n\n**Shahram Latifi** is a Professor of Electrical Engineering at the University of Nevada, Las Vegas. Dr. Latifi is the co-director of the Center for Information Technology and Algorithms (CITA) at UNLV. He has designed and taught undergraduate and graduate courses in the broad spectrum of Computer Science and Engineering in the past four decades. He has given keynotes and seminars on machine learning/AI and IT-related topics all over the world. His research has been funded by NSF, NASA, DOE, DoD, Boeing, Lockheed, and Cray Inc. Dr. Latifi is the recipient of several research awards, the most recent being the Barrick Distinguished Research Award (2021). Dr. Latifi was recognized to be among the top 2% researchers around the world in December 2020, according to Stanford top 2% list (publication data in Scopus, Mendeley).\u00a0 He is an IEEE Fellow and a Registered Professional Engineer in the State of Nevada.\n\n## References\n\n1\\. D. B. Rubin, \u201cInference and missing data,\u201d Biometrika, vol. 63, no. 3, pp. 581\u2013592, 1976.\n\n2\\. J. M. Jerez et al., \u201cMissing data imputation using statistical and machine learning methods in a real breast cancer problem,\u201d Artificial Intelligence in Medicine, vol. 50, no. 2, pp. 105\u2013115, 2010\n\n3\\. J. W. Graham, S. M. Hofer, S. I. Donaldson, D. P. MacKinnon, and J. L. Schafer, \u201cAnalysis with missing data in prevention research.,\u201d 1997.\n\n4\\. N. Tsikriktsis, \u201cA review of techniques for treating missing data in OM survey research,\u201d Journal of operations management, vol. 24, no. 1, pp. 53\u201362, 2005.\n\n5\\. M. R. Raymond, \u201cMissing data in evaluation research,\u201d Evaluation & the health professions, vol. 9, no. 4, pp. 395\u2013420, 1986.\n\n6\\. A. Jadhav, D. Pramod, and K. Ramanathan, \u201cComparison of Performance of Data Imputation Methods for Numeric Dataset,\u201d Applied Artificial Intelligence, vol. 33, no. 10, pp. 913\u2013933, Aug. 2019\n\n7\\. G. E. Batista and M. C. Monard, \u201cA study of K-nearest neighbour as an imputation method.,\u201d His, vol. 87, no. 251\u2013260, p. 48, 2002.\n\n8\\. G. E. Batista and M. C. Monard, \u201cAn analysis of four missing data treatment methods for supervised learning,\u201d Applied Artificial Intelligence, vol. 17, no. 5\u20136, pp. 519\u2013533, May 2003\n\n9\\. A. Choudhury and M. R. Kosorok, \u201cMissing Data Imputation for Classification Problems,\u201d Feb. 2020.\n\n10\\. A. Ngueilbaye, H. Wang, D. A. Mahamat, and S. B. Junaidu, \u201cModulo 9 model-based learning for missing data imputation,\u201d Applied Soft Computing, vol. 103, p. 107167, May 2021\n\n11\\. D. Dua and C. Graff, \u201cUCI Machine Learning Repository.\u201d 2017. \\[Online\\]. Available: http://archive.ics.uci.edu/ml\n\n12\\. H. Kaya, P. T\u00fcfekci, and E. Uzun, \u201cPredicting co and no x emissions from gas turbines: novel data and a benchmark pems,\u201d Turkish Journal of Electrical Engineering & Computer Sciences, vol. 27, no. 6, pp. 4783\u20134796, 2019.\n\n13\\. S. Zhang, B. Guo, A. Dong, J. He, Z. Xu, and S. X. Chen, \u201cCautionary tales on air-quality improvement in Beijing,\u201d Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 473, no. 2205, p. 20170457, 2017.\n\n14\\. F. A. Thabtah, \u201cAutism Spectrum Disorder Screening: Machine Learning Adaptation and DSM-5 Fulfillment,\u201d Proceedings of the 1st International Conference on Medical and Health Informatics 2017, 2017.\n\n## Downloads\n\n- [PDF](https://qtanalytics.in/journals/index.php/JREAS/article/view/1821/1023)\n\n## Published\n\n2023-04-08\n\n## Issue\n\n[Vol. 8 No. 1 (2023)](https://qtanalytics.in/journals/index.php/JREAS/issue/view/142)\n\n## Section\n\nArticles\n\n## Make a Submission\n\n[Make a Submission](https://qtanalytics.in/journals/index.php/JREAS/about/submissions)\n\n[![More information about the publishing system, Platform and Workflow by OJS/PKP.](https://qtanalytics.in/journals/templates/images/ojs_brand.png)](https://qtanalytics.in/journals/index.php/JREAS/about/aboutThisPublishingSystem)",
      "url": "https://qtanalytics.in/journals/index.php/JREAS/article/view/1821"
    },
    {
      "title": "A comparison of imputation methods for categorical data",
      "text": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- View\u00a0**PDF**\n- Download full issue\n\nSearch ScienceDirect\n\n## [Informatics in Medicine Unlocked](https://www.sciencedirect.com/journal/informatics-in-medicine-unlocked)\n\n[Volume 42](https://www.sciencedirect.com/journal/informatics-in-medicine-unlocked/vol/42/suppl/C), 2023, 101382\n\n# A comparison of imputation methods for categorical data\n\nAuthor links open overlay panelShaheen MZ.Memona, RobertWamalab, Ignace H.Kabanoa\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.imu.2023.101382](https://doi.org/10.1016/j.imu.2023.101382) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S2352914823002289&orderBeanReset=true)\n\nUnder a Creative Commons [license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nOpen access\n\n## Highlights\n\n- \u2022\nClinical records are being increasingly used for prediction of morbidities.\n\n- \u2022\nClinical records are usually subject to missing data which reduces predictive power of models.\n\n- \u2022\nKNN Imputation has the highest accuracy in predicting missing categorical data.\n\n\n## Abstract\n\n### Objectives\n\nMissing data is commonplace in clinical databases, which are being increasingly used for research. Without giving any regard to missing data, results from analysis may become biased and unrepresentative. Clinical databases contain mainly categorical variables. This study aims to assess the methods used for imputation in categorical variables.\n\n### Materials and methods\n\nWe utilized data extracted from paper-based maternal health records from Kawempe National Referral Hospital, Uganda. We compared the following imputation methods for categorical data in an empirical analysis: Mode, K-Nearest Neighbors (KNN), Random Forest (RF), Sequential Hot-Deck (SHD), and Multiple Imputation by Chained Equations (MICE). The five imputation methods were first compared by accuracy of predicting the missing values. Next, the imputation methods were compared by predictive accuracy of the outcome variable in four classifiers. The consistency of performance of imputation methods across different levels of missing data (5%\u201350\u00a0%) was assessed by Kendall's W test.\n\n### Results\n\nKNN imputation had the highest precision score at levels (5%\u201350\u00a0%) of MCAR missing data. At lower proportions of missing data (5\u00a0%, 10\u00a0%, 15\u00a0%, 20\u00a0%), RF imputation had the second-highest precision score. SHD imputation had the worst precision at all levels of missing data. In the prediction of the outcome, the methods performed differently at all proportions of missing data in the four classifiers. Even though KNN imputation was the best method in predicting the missing values, it did not consistently enhance the predictive accuracy of the classifiers at all levels of missing data. Our findings show that a high precision score of an imputation method does not translate into higher predictive accuracy in classifiers.\n\n### Conclusions\n\nKNN imputation is the best method in predicting missing values in categorical variables. There is no universal best imputation method that yields the highest predictive accuracy at all proportions of missing data.\n\n- Previous article in issue\n- Next article in issue\n\n## Keywords\n\nImputation\n\nCategorical variables\n\nPrecision score\n\nSingle imputation\n\nMultiple imputation\n\nRecommended articles\n\n\u00a9 2023 The Authors. Published by Elsevier Ltd.",
      "url": "https://www.sciencedirect.com/science/article/pii/S2352914823002289"
    },
    {
      "title": "KNNDataImputer \u2014 Responsible AI Mitigations documentation",
      "text": "- [API reference](https://microsoft.github.io/api.html)\n- [DataProcessing](https://microsoft.github.io/dataprocessing.html)\n- [Imputers](https://microsoft.github.io/imputer.html)\n- KNNDataImputer\n- [Edit on GitHub](https://github.com//microsoft/responsible-ai-toolbox-mitigations/blob/main/docs/dataprocessing/imputer/knn.rst)\n\n# KNNDataImputer [\uf0c1](https://microsoft.github.io/microsoft.github.io\\#knndataimputer)\n\n_class_ raimitigations.dataprocessing.KNNDataImputer( _df:Optional\\[Union\\[DataFrame,ndarray\\]\\]=None_, _col\\_impute:Optional\\[list\\]=None_, _enable\\_encoder:bool=False_, _knn\\_params:Optional\\[dict\\]=None_, _sklearn\\_obj:Optional\\[object\\]=None_, _verbose:bool=True_) [\uf0c1](https://microsoft.github.io/microsoft.github.io#raimitigations.dataprocessing.KNNDataImputer)\n\nBases: [`DataImputer`](https://microsoft.github.io/imputer.html#raimitigations.dataprocessing.DataImputer)\n\nConcrete class that imputes missing data of a feature using K-nearest neighbours. A feature\u2019s missing values are imputed using\nthe mean value from k-nearest neighbors in the dataset. Two samples are close if the features that neither is missing are close.\nThis subclass uses the `KNNImputer` class from `sklearn` in the background.\nsklearn.impute.KNNImputer can only handle numerical data, however, this subclass allows for categorical input by applying ordinal\nencoding before calling the sklearn class. In order to use this function, use enable\\_encoder=True. Note that encoded columns are\nnot guaranteed to reverse transform if they have imputed values.\nIf you\u2019d like to use a different type of encoding before imputation, consider using the Pipeline class and call your own encoder\nbefore calling this subclass for imputation.\nFor more details see:\n[https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer).html#\n\nParameters\n\n- **df** \u2013 pandas data frame that contains the columns to be imputed;\n\n- **col\\_impute** \u2013 a list of the column names or indexes that will be imputed.\nIf None, this parameter will be set automatically as being a list of all\ncolumns with any NaN value;\n\n- **enable\\_encoder** \u2013 a boolean flag to allow for applying ordinal encoding of categorical data before applying the KNNImputer since it only accepts numerical values.\n\n- **knn\\_params** \u2013\n\na dict indicating the parameters used by\n`KNNImputer`. The dict has the following structure:\n\n\n> {\n>\n> **\u2018missing\\_values\u2019**:np.nan,\n>\n> **\u2019n\\_neighbors\u2019**:5,\n>\n> **\u2019weights\u2019**:\u2019uniform\u2019,\n>\n> **\u2019metric\u2019**:\u2019nan\\_euclidean\u2019,\n>\n> **\u2019copy\u2019**:True,\n>\n> }\n\n\nwhere these are the parameters used by sklearn\u2019s KNNImputer. If None,\nthis dict will be auto-filled as the one above.\n`Note: 'weights' can take one of these values: ['uniform', 'distance'] or callable` `and 'metric' can take one of these values: ['nan_euclidean'] or callable`\n\n- **sklearn\\_obj** \u2013 an sklearn.impute.KNNImputer object to use directly. If this parameter is used,\nknn\\_params will be overwritten.\n\n- **verbose** \u2013 indicates whether internal messages should be printed or not.\n\n\nClass Diagram\n\n## Example [\uf0c1](https://microsoft.github.io/microsoft.github.io\\#example)\n\n[KNNDataImputer Example](https://microsoft.github.io/notebooks/dataprocessing/module_tests/knn_imputation.html)",
      "url": "https://microsoft.github.io/responsible-ai-toolbox-mitigations/dataprocessing/imputer/knn.html"
    },
    {
      "title": "Imputing missing values before building an estimator #",
      "text": "Imputing missing values before building an estimator &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\nNote\n[Go to the end](#sphx-glr-download-auto-examples-impute-plot-missing-values-py)to download the full example code or to run this example in your browser via JupyterLite or Binder.\n# Imputing missing values before building an estimator[#](#imputing-missing-values-before-building-an-estimator)\nMissing values can be replaced by the mean, the median or the most frequent\nvalue using the basic[`SimpleImputer`](../../modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer).\nIn this example we will investigate different imputation techniques:\n* imputation by the constant value 0\n* imputation by the mean value of each feature\n* k nearest neighbor imputation\n* iterative imputation\nIn all the cases, for each feature, we add a new feature indicating the missingness.\nWe will use two datasets: Diabetes dataset which consists of 10 feature\nvariables collected from diabetes patients with an aim to predict disease\nprogression and California housing dataset for which the target is the median\nhouse value for California districts.\nAs neither of these datasets have missing values, we will remove some\nvalues to create new versions with artificially missing data. The performance\nof[`RandomForestRegressor`](../../modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)on the full original dataset\nis then compared the performance on the altered datasets with the artificially\nmissing values imputed using different techniques.\n```\n# Authors: The scikit-learn developers# SPDX-License-Identifier: BSD-3-Clause\n```\n## Download the data and make missing values sets[#](#download-the-data-and-make-missing-values-sets)\nFirst we download the two datasets. Diabetes dataset is shipped with\nscikit-learn. It has 442 entries, each with 10 features. California housing\ndataset is much larger with 20640 entries and 8 features. It needs to be\ndownloaded. We will only use the first 300 entries for the sake of speeding\nup the calculations but feel free to use the whole dataset.\n```\nimportnumpyasnpfromsklearn.datasetsimport[fetch\\_california\\_housing](../../modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing),[load\\_diabetes](../../modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)X\\_diabetes,y\\_diabetes=[load\\_diabetes](../../modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)(return\\_X\\_y=True)X\\_california,y\\_california=[fetch\\_california\\_housing](../../modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing)(return\\_X\\_y=True)X\\_diabetes=X\\_diabetes[:300]y\\_diabetes=y\\_diabetes[:300]X\\_california=X\\_california[:300]y\\_california=y\\_california[:300]defadd\\_missing\\_values(X\\_full,y\\_full,rng):n\\_samples,n\\_features=X\\_full.shape# Add missing values in 75% of the linesmissing\\_rate=0.75n\\_missing\\_samples=int(n\\_samples\\*missing\\_rate)missing\\_samples=[np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros)(n\\_samples,dtype=bool)missing\\_samples[:n\\_missing\\_samples]=Truerng.shuffle(missing\\_samples)missing\\_features=rng.randint(0,n\\_features,n\\_missing\\_samples)X\\_missing=X\\_full.copy()X\\_missing[missing\\_samples,missing\\_features]=[np.nan](https://numpy.org/doc/stable/reference/constants.html#numpy.nan)y\\_missing=y\\_full.copy()returnX\\_missing,y\\_missingrng=[np.random.RandomState](https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState)(42)X\\_miss\\_diabetes,y\\_miss\\_diabetes=add\\_missing\\_values(X\\_diabetes,y\\_diabetes,rng)X\\_miss\\_california,y\\_miss\\_california=add\\_missing\\_values(X\\_california,y\\_california,rng)\n```\n## Impute the missing data and score[#](#impute-the-missing-data-and-score)\nNow we will write a function which will score the results on the differently\nimputed data, including the case of no imputation for full data.\nWe will use[`RandomForestRegressor`](../../modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)for the target\nregression.\n```\nfromsklearn.ensembleimport[RandomForestRegressor](../../modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)# To use the experimental IterativeImputer, we need to explicitly ask for it:fromsklearn.experimentalimportenable\\_iterative\\_imputer# noqa: F401fromsklearn.imputeimport[IterativeImputer](../../modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer),[KNNImputer](../../modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer),[SimpleImputer](../../modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)fromsklearn.model\\_selectionimport[cross\\_val\\_score](../../modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)fromsklearn.pipelineimport[make\\_pipeline](../../modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)fromsklearn.preprocessingimport[RobustScaler](../../modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)N\\_SPLITS=4defget\\_score(X,y,imputer=None):regressor=[RandomForestRegressor](../../modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)(random\\_state=0)ifimputerisnotNone:estimator=[make\\_pipeline](../../modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)(imputer,regressor)else:estimator=regressorscores=[cross\\_val\\_score](../../modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)(estimator,X,y,scoring=&quot;&quot;neg\\_mean\\_squared\\_error&quot;&quot;,cv=N\\_SPLITS)returnscores.mean(),scores.std()x\\_labels=[]mses\\_diabetes=[np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros)(5)stds\\_diabetes=[np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros)(5)mses\\_california=[np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros)(5)stds\\_california=[np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros)(5)\n```\n### Estimate the score[#](#estimate-the-score)\nFirst, we want to estimate the score on the original data:\n```\nmses\\_diabetes[0],stds\\_diabetes[0]=get\\_score(X\\_diabetes,y\\_diabetes)mses\\_california[0],stds\\_california[0]=get\\_score(X\\_california,y\\_california)x\\_labels.append(&quot;Full Data&quot;)\n```\n### Replace missing values by 0[#](#replace-missing-values-by-0)\nNow we will estimate the score on the data where the missing values are\nreplaced by 0:\n```\nimputer=[SimpleImputer](../../modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)(strategy=&quot;constant&quot;,fill\\_value=0,add\\_indicator=True)mses\\_diabetes[1],stds\\_diabetes[1]=get\\_score(X\\_miss\\_diabetes,y\\_miss\\_diabetes,imputer)mses\\_california[1],stds\\_california[1]=get\\_score(X\\_miss\\_california,y\\_miss\\_california,imputer)x\\_labels.append(&quot;Zero Imputation&quot;)\n```\n### Impute missing values with mean[#](#impute-missing-values-with-mean)\n```\nimputer=[SimpleImputer](../../modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)(strategy=&quot;mean&quot;,add\\_indicator=True)mses\\_diabetes[2],stds\\_diabetes[2]=get\\_score(X\\_miss\\_diabetes,y\\_miss\\_diabetes,im...",
      "url": "https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html"
    },
    {
      "title": "A Guide To KNN Imputation - Kyaw Saw Htoon - Medium",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F95e2dc496e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40kyawsawhtoon%2Fa-guide-to-knn-imputation-95e2dc496e&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40kyawsawhtoon%2Fa-guide-to-knn-imputation-95e2dc496e&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n[Source](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/04_thumb.jpg)\n\n# A Guide To KNN Imputation\n\n[![Kyaw Saw Htoon](https://miro.medium.com/v2/da:true/resize:fill:88:88/0*Ap27zHRf_Xg1WLhs)](https://medium.com/@kyawsawhtoon?source=post_page-----95e2dc496e--------------------------------)\n\n[Kyaw Saw Htoon](https://medium.com/@kyawsawhtoon?source=post_page-----95e2dc496e--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd9c94fe1e113&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40kyawsawhtoon%2Fa-guide-to-knn-imputation-95e2dc496e&user=Kyaw+Saw+Htoon&userId=d9c94fe1e113&source=post_page-d9c94fe1e113----95e2dc496e---------------------post_header-----------)\n\n6 min read\n\n\u00b7\n\nJul 3, 2020\n\n--\n\n7\n\nListen\n\nShare\n\n_How to handle missing data in your dataset with Scikit-Learn\u2019s KNN Imputer_\n\nMissing values exist in almost all datasets and it is essential to handle them properly in order to construct reliable machine learning models with optimal statistical power. In this article, we will talk about what missing values are, how to identify them, and how to replace them by using the K-Nearest Neighbors imputation method. To demonstrate this method, we will use the famous Titanic dataset in this guide.\n\n# What are Missing Values?\n\nA missing value can be defined as the data value that is not captured nor stored for a variable in the observation of interest. There are 3 types of missing values -\n\n**Missing Completely at Random (MCAR)**\n\nMCAR occurs when the missing on the variable is completely unsystematic. When our dataset is missing values completely at random, the probability of missing data is unrelated to any other variable and unrelated to the variable with missing values itself. For example, MCAR would occur when data is missing because the responses to a research survey about depression are lost in the mail.\n\n**Missing at Random (MAR)**\n\nMAR occurs when the probability of the missing data on a variable is related to some other measured variable but unrelated to the variable with missing values itself. For example, the data values are missing because males are less likely to respond to a depression survey. In this case, the missing data is related to the gender of the respondents. However, the missing data is not related to the level of depression itself.\n\n**Missing Not at Random (MNAR)**\n\nMNAR occurs when the missing values on a variable are related to the variable with the missing values itself. In this case, the data values are missing because the respondents failed to fill in the survey due to their level of depression.\n\n# Effects of Missing Values\n\nHaving missing values in our datasets can have various detrimental effects. Here are a few examples -\n\n1. Missing data can limit our ability to perform important data science tasks such as converting data types or visualizing data\n2. Missing data can reduce the statistical power of our models which in turn increases the probability of Type II error. Type II error is the failure to reject a false null hypothesis.\n3. Missing data can reduce the representativeness of the samples in the dataset.\n4. Missing data can distort the validity of the scientific trials and can lead to invalid conclusions.\n\n# Identifying Missing Values\n\nFinding missing values with Python is straightforward. First, we will import Pandas and create a data frame for the Titanic dataset.\n\n```\nimport pandas as pddf = pd.read_csv(\u2018titanic.csv\u2019)\n```\n\nNext, we will remove some of the independent variable columns that have little use for KNN Imputer or the machine learning algorithm if we are building one. These columns include passenger names, passenger IDs, cabin and ticket numbers.\n\n```\ndf = df.drop(['Unnamed: 0', 'PassengerId', 'Name',\n              'Ticket', 'Cabin'], axis=1)\n```\n\nWe will then use Pandas\u2019 data frame attributes, \u2018.isna()\u2019 and \u2018.isany()\u2019, to detect missing values. These attributes will return Boolean values where \u2018True\u2019 indicates that there are missing values in the particular column.\n\n```\ndf.isna().isany()\n```\n\nAs we can see, the columns \u2018Age\u2019 and \u2018Embarked\u2019 have missing values. Instead of \u2018.isany()\u2019, we can also use \u2018.sum()\u2019 to find out the number of missing values in the columns.\n\n```\ndf.isna().sum()\n```\n\nThere you go. Now, we know that \u2018Age\u2019 has 177 and \u2018Embarked\u2019 has 2 missing values.\n\n# KNN Imputer\n\nKNN Imputer was first supported by [Scikit-Learn](https://scikit-learn.org/stable/) in December 2019 when it released its version 0.22. This imputer utilizes the k-Nearest Neighbors method to replace the missing values in the datasets with the mean value from the parameter \u2018n\\_neighbors\u2019 nearest neighbors found in the training set. By default, it uses a Euclidean distance metric to impute the missing values.\n\nTo see this imputer in action, we will import it from Scikit-Learn\u2019s impute package -\n\n```\nfrom sklearn.impute import KNNImputer\n```\n\nOne thing to note here is that the KNN Imputer does not recognize text data values. It will generate errors if we do not change these values to numerical values. For example, in our Titanic dataset, the categorical columns \u2018Sex\u2019 and \u2018Embarked\u2019 have text data.\n\nA good way to modify the text data is to perform one-hot encoding or create \u201cdummy variables\u201d. The idea is to convert each category into a binary data column by assigning a 1 or 0. Other options would be to use LabelEncoder or OrdinalEncoder from Scikit-Learn\u2019s preprocessing package.\n\nIn this tutorial, we will stick to one-hot encoding. First, we will make a list of categorical variables with text data and generate dummy variables by using \u2018.get\\_dummies\u2019 attribute of Pandas data frame package. An important caveat here is we are setting \u201cdrop\\_first\u201d parameters as True in order to prevent the Dummy Variable Trap.\n\n_Note: You can also use Scikit-Learn\u2019s LabelBinarizer method here._\n\n```\ncat_variables = df[[\u2018Sex\u2019, \u2018Embarked\u2019]]\ncat_dummies = pd.get_dummies(cat_variables, drop_first=True)\ncat_dummies.head()\n```\n\nNow we have 3 dummy variable columns. In the \u201cSex\\_male\u201d column, 1 indicates that the passenger is male and 0 is female. The \u201cSex\\_female\u201d column is dropped since the \u201cdrop\\_first\u201d parameter is set as True. Similarly, there are only 2 columns for \u201cEmbarked\u201d because the third one has been dropped.\n\nNext, we will drop the original \u201cSex\u201d and \u201cEmbarked\u201d columns from the data frame and add the dummy variables.\n\n```\ndf = df.drop(['Sex', 'Embarked'], axis=1)\ndf = pd.concat([df, cat_dummies], axis=1)\ndf.head()\n```\n\nAnother critical point here is that the KNN Imptuer is a distance-based imputation method and it requires us to normalize our data. Otherwise, the different scales of our data will lead the KNN Imputer to generate biased replacements for the missing values. For simplicity, we will use Scikit-Learn\u2019s MinMaxScaler which will scale our variables to have values between 0 and 1.\n\n```\nfrom sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()\ndf = pd.DataFrame(scaler...",
      "url": "https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e"
    },
    {
      "title": "kNN Imputation for Missing Values in Machine Learning - MachineLearningMastery.com",
      "text": "### [Navigation](https://machinelearningmastery.com/machinelearningmastery.com\\#navigation)\n\nBy[Jason Brownlee](https://machinelearningmastery.com/author/jasonb/)onAugust 17, 2020in[Data Preparation](https://machinelearningmastery.com/category/data-preparation/)[52](https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/#comments)\n\nShare _Post_Share\n\nDatasets may have missing values, and this can cause problems for many machine learning algorithms.\n\nAs such, it is good practice to identify and replace missing values for each column in your input data prior to modeling your prediction task. This is called missing data imputation, or imputing for short.\n\nA popular approach to missing data imputation is to use a model to predict the missing values. This requires a model to be created for each input variable that has missing values. Although any one among a range of different models can be used to predict the missing values, the k-nearest neighbor (KNN) algorithm has proven to be generally effective, often referred to as \u201c _nearest neighbor imputation_.\u201d\n\nIn this tutorial, you will discover how to use nearest neighbor imputation strategies for missing data in machine learning.\n\nAfter completing this tutorial, you will know:\n\n- Missing values must be marked with NaN values and can be replaced with nearest neighbor estimated values.\n- How to load a CSV file with missing values and mark the missing values with NaN values and report the number and percentage of missing values for each column.\n- How to impute missing values with nearest neighbor models as a data preparation method when evaluating models and when fitting a final model to make predictions on new data.\n\n**Kick-start your project** with my new book [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/), including _step-by-step tutorials_ and the _Python source code_ files for all examples.\n\nLet\u2019s get started.\n\n- **Updated Jun/2020**: Changed the column used for prediction in examples.\n\nkNN Imputation for Missing Values in Machine LearningPhoto by [portengaround](https://flickr.com/photos/portengaround/8318502104/), some rights reserved.\n\n## Tutorial Overview\n\nThis tutorial is divided into three parts; they are:\n\n1. k-Nearest Neighbor Imputation\n2. Horse Colic Dataset\n3. Nearest Neighbor Imputation With KNNImputer\n1. KNNImputer Data Transform\n2. KNNImputer and Model Evaluation\n3. KNNImputer and Different Number of Neighbors\n4. KNNImputer Transform When Making a Prediction\n\n## k-Nearest Neighbor Imputation\n\nA dataset may have missing values.\n\nThese are rows of data where one or more values or columns in that row are not present. The values may be missing completely or they may be marked with a special character or value, such as a question mark \u201c _?_\u201c.\n\nValues could be missing for many reasons, often specific to the problem domain, and might include reasons such as corrupt measurements or unavailability.\n\nMost machine learning algorithms require numeric input values, and a value to be present for each row and column in a dataset. As such, missing values can cause problems for machine learning algorithms.\n\nIt is common to identify missing values in a dataset and replace them with a numeric value. This is called data imputing, or missing data imputation.\n\n> \u2026 missing data can be imputed. In this case, we can use information in the training set predictors to, in essence, estimate the values of other predictors.\n\n\u2014 Page 42, [Applied Predictive Modeling](https://amzn.to/3b2LHTL), 2013.\n\nAn effective approach to data imputing is to use a model to predict the missing values. A model is created for each feature that has missing values, taking as input values of perhaps all other input features.\n\n> One popular technique for imputation is a K-nearest neighbor model. A new sample is imputed by finding the samples in the training set \u201cclosest\u201d to it and averages these nearby points to fill in the value.\n\n\u2014 Page 42, [Applied Predictive Modeling](https://amzn.to/3b2LHTL), 2013.\n\nIf input variables are numeric, then regression models can be used for prediction, and this case is quite common. A range of different models can be used, although a simple k-nearest neighbor (KNN) model has proven to be effective in experiments. The use of a KNN model to predict or fill missing values is referred to as \u201c _Nearest Neighbor Imputation_\u201d or \u201c _KNN imputation_.\u201d\n\n> We show that KNNimpute appears to provide a more robust and sensitive method for missing value estimation \\[\u2026\\] and KNNimpute surpass the commonly used row average method (as well as filling missing values with zeros).\n\n\u2014 [Missing value estimation methods for DNA microarrays](https://academic.oup.com/bioinformatics/article/17/6/520/272365), 2001.\n\nConfiguration of KNN imputation often involves selecting the distance measure (e.g. Euclidean) and the number of contributing neighbors for each prediction, the k hyperparameter of the KNN algorithm.\n\nNow that we are familiar with nearest neighbor methods for missing value imputation, let\u2019s take a look at a dataset with missing values.\n\n### Want to Get Started With Data Preparation?\n\nTake my free 7-day email crash course now (with sample code).\n\nClick to sign-up and also get a free PDF Ebook version of the course.\n\nDownload Your FREE Mini-Course\n\n## Horse Colic Dataset\n\nThe horse colic dataset describes medical characteristics of horses with colic and whether they lived or died.\n\nThere are 300 rows and 26 input variables with one output variable. It is a binary classification prediction task that involves predicting 1 if the horse lived and 2 if the horse died.\n\nThere are many fields we could select to predict in this dataset. In this case, we will predict whether the problem was surgical or not (column index 23), making it a binary classification problem.\n\nThe dataset has many missing values for many of the columns where each missing value is marked with a question mark character (\u201c?\u201d).\n\nBelow provides an example of rows from the dataset with marked missing values.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3<br>4<br>5 | 2,1,530101,38.50,66,28,3,3,?,2,5,4,4,?,?,?,3,5,45.00,8.40,?,?,2,2,11300,00000,00000,2<br>1,1,534817,39.2,88,20,?,?,4,1,3,4,2,?,?,?,4,2,50,85,2,2,3,2,02208,00000,00000,2<br>2,1,530334,38.30,40,24,1,1,3,1,3,3,1,?,?,?,1,1,33.00,6.70,?,?,1,2,00000,00000,00000,1<br>1,9,5290409,39.10,164,84,4,1,6,2,2,4,4,1,2,5.00,3,?,48.00,7.20,3,5.30,2,1,02208,00000,00000,1<br>... |\n\nYou can learn more about the dataset here:\n\n- [Horse Colic Dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv)\n- [Horse Colic Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.names)\n\nNo need to download the dataset as we will download it automatically in the worked examples.\n\nMarking missing values with a NaN (not a number) value in a loaded dataset using Python is a best practice.\n\nWe can load the dataset using the read\\_csv() Pandas function and specify the \u201cna\\_values\u201d to load values of \u2018?\u2019 as missing, marked with a NaN value.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3<br>4 | ...<br>\\# load dataset<br>url='https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'<br>dataframe=read\\_csv(url,header=None,na\\_values='?') |\n\nOnce loaded, we can review the loaded data to confirm that \u201c?\u201d values are marked as NaN.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3 | ...<br>\\# summarize the first few rows<br>print(dataframe.head()) |\n\nWe can then enumerate each column and report the number of rows with missing values for the column.\n\n|     |     |\n| --- | --- |\n| 1<br>2<br>3<br>4<br>5<br>6<br>7 | ...<br>\\# summarize the number of rows with missing values for each column<br>foriinrange(dataframe.shape\\[1\\]):<br>\\# count number of rows with missing values<br>n\\_miss=dataframe\\[\\[i\\]\\].isnull().sum()<br>perc=n\\_miss/dataframe.shape\\[0\\]\\*100<br>print('\\> %d, Missing: %d (%...",
      "url": "https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning"
    }
  ]
}