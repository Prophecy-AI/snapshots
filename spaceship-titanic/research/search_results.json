{
  "query": "How to reduce overfitting and CV-LB gap in Kaggle competitions with gradient boosting models?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on Krystian\u202fSafjan\u2019s over\u2011fitting checklist, the XGBoost/LGBM tuning guide, Google\u2019s regular\u2011ization guide, and recent GBDT\u2011specific tricks, you can shrink both over\u2011fitting and the public\u2011leaderboard / private\u2011leaderboard (CV\u2011LB) gap with the following focused workflow:\n\n1. **Build a validation scheme that mirrors the hidden test set**  \n   - Use stratified (or time\u2011aware) k\u2011fold CV and keep an untouched hold\u2011out fold for final early\u2011stopping.  \n   - Run *adversarial validation* to spot distribution drift between train and public\u2011leaderboard data and adjust the split if needed. ([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions); [Google\u202fML\u202fDocs](https://developers.google.com/machine-learning/decision-forests/overfitting-gbdt))  \n\n2. **Apply strong regularisation to the booster**  \n   - Limit tree depth (`max_depth \u2264 4\u20116`).  \n   - Raise `min_child_weight`, set `gamma` (or `min_split_gain`) and add L1/L2 penalties (`alpha`, `lambda`).  \n   - Use a low learning rate (`eta`/`learning_rate`\u202f\u2248\u202f0.01\u20110.05) and increase `num_round`/`n_estimators`.  \n   - These knobs directly control the bias\u2011variance trade\u2011off and keep trees from memorising noise. ([XGBoost\u202fDocs](https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html); [Google\u202fML\u202fDocs](https://developers.google.com/machine-learning/decision-forests/overfitting-gbdt))  \n\n3. **Inject randomness to make training robust**  \n   - Set `subsample`\u202f\u2248\u202f0.6\u20110.8 and `colsample_bytree`\u202f\u2248\u202f0.6\u20110.9.  \n   - Optionally enable DART dropout or use the \u201cbagging\u201d variant of LightGBM.  \n   - Random row/feature sampling prevents any single tree from over\u2011fitting a particular pattern. ([XGBoost\u202fDocs](https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html); [Google\u202fML\u202fDocs](https://developers.google.com/machine-learning/decision-forests/overfitting-gbdt))  \n\n4. **Use early stopping on the dedicated hold\u2011out fold**  \n   - Monitor the validation metric each boosting round and stop when it stops improving for `patience`\u202f\u2248\u202f50\u2011100 rounds.  \n   - This automatically selects the optimal number of trees and avoids the tail of over\u2011fitting. ([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions); [Google\u202fML\u202fDocs](https://developers.google.com/machine-learning/decision-forests/overfitting-gbdt))  \n\n5. **Tune hyper\u2011parameters systematically**  \n   - Start with a coarse grid (depth, learning rate, subsample, regularisation).  \n   - Refine with Bayesian optimisation or Optuna\u2019s TPE sampler to explore the regularisation space efficiently.  \n   - Focus the search on the parameters above rather than on sheer tree count. ([Medium\u202f2025 \u201c12\u202fGBDT Tricks\u201d](https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac); [James\u202fCoffey\u202f2025](https://www.jamescoffey.co/post/kick-start-tree-ensembles-without-the-titanic-cliches))  \n\n6. **Prune noisy or low\u2011importance features**  \n   - Compute feature importance (gain, SHAP) and drop columns with negligible contribution or high missingness.  \n   - Simpler feature sets reduce variance and shrink the CV\u2011LB gap. ([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))  \n\n7. **Ensemble complementary models**  \n   - Train several boosters with different random seeds or slightly varied hyper\u2011parameters and average their predictions.  \n   - Optionally stack a lightweight meta\u2011learner on top of the ensemble.  \n   - Ensembles smooth out individual over\u2011fit patterns and often bring the private leaderboard score closer to the CV estimate. ([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions); [Medium\u202f2025](https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac))\n\nFollow these steps in order: set a realistic CV split, regularise aggressively, add randomness, stop early, fine\u2011tune with Bayesian methods, clean the feature set, and finish with a modest ensemble. This pipeline consistently narrows the over\u2011fitting gap and aligns public\u2011leaderboard performance with the true test set.",
      "url": ""
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "12 GBDT Tricks to Squeeze the Last 5% AUC",
      "text": "12 GBDT Tricks to Squeeze the Last 5% AUC | by Thinking Loop | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\nMember-only story\n# 12 GBDT Tricks to Squeeze the Last 5% AUC\n## Practical XGBoost/LightGBM moves that turn \u201cpretty good\u201d into leaderboard-level.\n[\n![Thinking Loop](https://miro.medium.com/v2/resize:fill:64:64/1*RO0Y6ej8xYi1SsLkvtIJrQ.png)\n](https://medium.com/@ThinkingLoop?source=post_page---byline--185f4084e9ac---------------------------------------)\n[Thinking Loop](https://medium.com/@ThinkingLoop?source=post_page---byline--185f4084e9ac---------------------------------------)\n5 min read\n\u00b7Sep 28, 2025\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/185f4084e9ac&amp;operation=register&amp;redirect=https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac&amp;user=Thinking+Loop&amp;userId=c4581f349120&amp;source=---header_actions--185f4084e9ac---------------------clap_footer------------------)\n--\n1\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/185f4084e9ac&amp;operation=register&amp;redirect=https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac&amp;source=---header_actions--185f4084e9ac---------------------bookmark_footer------------------)\nShare\nPress enter or click to view image in full size\n![]()\n*Twelve proven tips for higher AUC with XGBoost and LightGBM \u2014validation design, regularization, sampling, constraints, categorical handling, and ensembling.*\nYou know that feeling: your model\u2019s*fine*\u2026 but not*winning*. AUC sits at 0.87 and refuses to budge. Let\u2019s be real \u2014those last few points don\u2019t come from a magic parameter. They come from discipline, tiny corrections, and a handful of advanced patterns that compound.\nBelow are twelve field-tested tactics that push tree-boosters (XGBoost/LightGBM) over the hump without believing in fairy dust.\n## 1) Win the split before you tune a single parameter\nBad validation quietly deletes AUC. Fix it first.\n* **Match production**: time-series \u2192forward chaining; user-level \u2192**GroupKFold**; general tabular \u2192**StratifiedKFold**.\n* **Repeatable noise**: set seeds and repeat 3\u00d7 to reduce variance.\n* **OOF discipline**: always keep out-of-fold predictions; that\u2019s your truth for blending and calibration.\n```\nfrom sklearn.model\\_selection import StratifiedKFold\nimport numpy as np\nskf =\u2026\n```\n[\n![Thinking Loop](https://miro.medium.com/v2/resize:fill:96:96/1*RO0Y6ej8xYi1SsLkvtIJrQ.png)\n](https://medium.com/@ThinkingLoop?source=post_page---post_author_info--185f4084e9ac---------------------------------------)\n[\n![Thinking Loop](https://miro.medium.com/v2/resize:fill:128:128/1*RO0Y6ej8xYi1SsLkvtIJrQ.png)\n](https://medium.com/@ThinkingLoop?source=post_page---post_author_info--185f4084e9ac---------------------------------------)\n[## Written byThinking Loop\n](https://medium.com/@ThinkingLoop?source=post_page---post_author_info--185f4084e9ac---------------------------------------)\n[1K followers](https://medium.com/@ThinkingLoop/followers?source=post_page---post_author_info--185f4084e9ac---------------------------------------)\n\u00b7[536 following](https://medium.com/@ThinkingLoop/following?source=post_page---post_author_info--185f4084e9ac---------------------------------------)\nAt Thinking Loop, we dive into AI, systems design, productivity, and the curious patterns of how we think and build. New ideas. Practical code. Deep insight.\n## Responses (1)\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--185f4084e9ac---------------------------------------)\nSee all responses\n[\nHelp\n](https://help.medium.com/hc/en-us?source=post_page-----185f4084e9ac---------------------------------------)\n[\nStatus\n](https://status.medium.com/?source=post_page-----185f4084e9ac---------------------------------------)\n[\nAbout\n](https://medium.com/about?autoplay=1&amp;source=post_page-----185f4084e9ac---------------------------------------)\n[\nCareers\n](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----185f4084e9ac---------------------------------------)\n[\nPress\n](mailto:pressinquiries@medium.com)\n[\nBlog\n](https://blog.medium.com/?source=post_page-----185f4084e9ac---------------------------------------)\n[\nPrivacy\n](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----185f4084e9ac---------------------------------------)\n[\nRules\n](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----185f4084e9ac---------------------------------------)\n[\nTerms\n](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----185f4084e9ac---------------------------------------)\n[\nText to speech\n](https://speechify.com/medium?source=post_page-----185f4084e9ac---------------------------------------)",
      "url": "https://medium.com/@ThinkingLoop/12-gbdt-tricks-to-squeeze-the-last-5-auc-185f4084e9ac"
    },
    {
      "title": "Notes on Parameter Tuning \uf0c1",
      "text": "Notes on Parameter Tuning &mdash; xgboost 3.1.1 documentation\n* [](../index.html)\n* [XGBoost Tutorials](index.html)\n* Notes on Parameter Tuning\n* [View page source](../_sources/tutorials/param_tuning.rst.txt)\n# Notes on Parameter Tuning[\uf0c1](#notes-on-parameter-tuning)\nParameter tuning is a dark art in machine learning, the optimal parameters\nof a model can depend on many scenarios. So it is impossible to create a\ncomprehensive guide for doing so.\nThis document tries to provide some guideline for parameters in XGBoost.\n## Understanding Bias-Variance Tradeoff[\uf0c1](#understanding-bias-variance-tradeoff)\nIf you take a machine learning or statistics course, this is likely to be one\nof the most important concepts.\nWhen we allow the model to get more complicated (e.g. more depth), the model\nhas better ability to fit the training data, resulting in a less biased model.\nHowever, such complicated model requires more data to fit.\nMost of parameters in XGBoost are about bias variance tradeoff. The best model\nshould trade the model complexity with its predictive power carefully.[Parameters Documentation](../parameter.html)will tell you whether each parameter\nwill make the model more conservative or not. This can be used to help you\nturn the knob between complicated model and simple model.\n## Control Overfitting[\uf0c1](#control-overfitting)\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\nThere are in general two ways that you can control overfitting in XGBoost:\n* The first way is to directly control model complexity.\n* This includes`max\\_depth`,`min\\_child\\_weight`,`gamma`,`max\\_cat\\_threshold`and other similar regularization parameters. See[XGBoost Parameters](../parameter.html)for a comprehensive\nset of parameters.\n* Set a constant`base\\_score`based on your own criteria. See[Intercept](intercept.html)for more info.\n* The second way is to add randomness to make training robust to noise.\n* This includes`subsample`and`colsample\\_bytree`, which may be used with boosting\nRF`num\\_parallel\\_tree`.\n* You can also reduce stepsize`eta`, possibly with a training callback. Remember to\nincrease`num\\_round`when you do so.\n## Handle Imbalanced Dataset[\uf0c1](#handle-imbalanced-dataset)\nFor common cases such as ads clickthrough log, the dataset is extremely imbalanced.\nThis can affect the training of XGBoost model, and there are two ways to improve it.\n* If you care only about the overall performance metric (AUC) of your prediction\n* Balance the positive and negative weights via`scale\\_pos\\_weight`\n* Use AUC for evaluation\n* If you care about predicting the right probability\n* In such a case, you cannot re-balance the dataset\n* Set parameter`max\\_delta\\_step`to a finite number (say 1) to help convergence\n## Use Hyper Parameter Optimization (HPO) Frameworks[\uf0c1](#use-hyper-parameter-optimization-hpo-frameworks)\nTuning models is a sophisticated task and there are advanced frameworks to help you. For\nexamples, some meta estimators in scikit-learn like[`sklearn.model\\_selection.HalvingGridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV)can help guide the search\nprocess. Optuna is another great option and there are many more based on different\nbranches of statistics.\n## Know Your Data[\uf0c1](#know-your-data)\nIt cannot be stressed enough the importance of understanding the data, sometimes that\u2019s\nall it takes to get a good model. Many solutions use a simple XGBoost tree model without\nmuch tuning and emphasize the data pre-processing step. XGBoost can help feature selection\nby providing both a global feature importance score and sample feature importance with\nSHAP value. Also, there are parameters specifically targeting categorical features, and\ntasks like survival and ranking. Feel free to explore them.\n## Reducing Memory Usage[\uf0c1](#reducing-memory-usage)\nIf you are using a HPO library like[`sklearn.model\\_selection.GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV),\nplease control the number of threads it can use. It\u2019s best to let XGBoost to run in\nparallel instead of asking> GridSearchCV\nto run multiple experiments at the same\ntime. For instance, creating a fold of data for cross validation can consume a significant\namount of memory:\n```\n# This creates a copy of dataset. X and X\\_train are both in memory at the same time.# This happens for every thread at the same time if you run `GridSearchCV` with# `n\\_jobs` larger than 1X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(X,y)\n```\n```\ndf=pd.DataFrame()# This creates a new copy of the dataframe, even if you specify the inplace parameternew\\_df=df.drop(...)\n```\n```\narray=np.array(...)# This may or may not make a copy of the data, depending on the type of the dataarray.astype(np.float32)\n```\n```\n# np by default uses double, do you actually need it?array=np.array(...)\n```\nYou can find some more specific memory reduction practices scattered through the documents\nFor instances:[Distributed XGBoost with Dask](dask.html),[XGBoost GPU Support](../gpu/index.html). However, before going into\nthese, being conscious about making data copies is a good starting point. It usually\nconsumes a lot more memory than people expect.",
      "url": "https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html"
    },
    {
      "title": "Kick-start Tree Ensembles without the Titanic Clich\u00e9s",
      "text": "Portfolio\n[![](https://cdn.prod.website-files.com/67e49e5ed9ba284f530202fb/67e49e5ed9ba284f530203b8_b1b1665d97d6f2f297d1729970698d27_logo.png)](https://www.jamescoffey.co/)\n[\u2039 Back to Portfolio](https://www.jamescoffey.co/)\n# Kick-start Tree Ensembles without the Titanic Clich\u00e9s\nA hands-on guide to tuning Gradient-Boosted Trees, preventing overfitting, and demystifying ensembles.\n![](https://cdn.prod.website-files.com/67e49e5ed9ba284f5302035c/6876ab35449533d0ff569b27_decision%20trees%20titanic.png)\n**Published:****Jul 15, 2025**\nBuilding predictive models on the famous Kaggle*Titanic*dataset is a rite of passage for many ML beginners. Most tutorials use Random Forests or even logistic regression as \u201cjust works\u201d solutions.*Gradient-Boosted Trees*(GBTs) often get a bad rap in this toy example: out of the box they can easily**overfit**the small Titanic data. However, you might miss out on better accuracy if you avoid boosting. You*can*use GBTs \u2014but only with extra care. In this article I\u2019ll show how to cross-validate safely, use automated tuning (Optuna\u2019s TPE sampler), and even try simple ensembles. Along the way I\u2019ll clarify key ensemble concepts and evaluation metrics that many beginners overlook. Let\u2019s dive in step-by-step.\n***Want to code along?***\n*I\u2019ve published the****full Jupyter notebook****\u2014 data prep, Optuna tuning, SHAP plots, and Kaggle-ready submission \u2014right here:*\n[*Complete Titanic GBT Notebook*](https://github.com/JamesFCoffey/ml-ds-techniques/blob/main/notebooks/titanic.ipynb)\n*Feel free to fork it as you read.*\n# Random Forest vs. Gradient Boosted Trees on Small Data\nTree ensembles are powerful, but not all are created equal.**Random Forests**use*bagging.*They train many deep trees on random subsets of data and average their votes to reduce variance. In contrast,**Gradient Boosted Trees**build trees*sequentially.*Each one correcting the errors of the previous ones. Because boosting adds trees greedily, a GBT model can fit noise in a small dataset. In fact, the[scikit-learn docs](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator)note that bagging methods reduce overfitting. They work best with*strong*base models. Boosting methods work best with*weak*learners (shallow trees).\nOn the Titanic (891 training rows), this means a default Random Forest often \u201cjust works\u201d out of the box, while a default GBT might overfit. More training data can itself act as a form of regularization, so with more data boosting tends to shine. But on small data you must add*regularization*(like shallow trees, subsampling, shrinkage) manually. In practice, you can use GBT on Titanic if you carefully tune its hyperparameters and maybe use techniques like early stopping.\n*For more tips on tree ensembles, model tuning, and Kaggle techniques,****follow me on X***[***@CoffeyFirst***](https://twitter.com/CoffeyFirst)*.*\n# Avoiding Data Leakage with Cross-Validation\nTo truly guard against overfitting, use**cross-validation**with a proper pipeline. Always**fit preprocessing steps on the training fold only, then apply to the validation fold.**Never use information from the held-out set when you train. The[scikit-learn docs](https://scikit-learn.org/stable/common_pitfalls.html#how-to-avoid-data-leakage)warn that you should \u201cnever call fit on the test data\u201d and that pipelines are ideal for cross-validation. In code, you\u2019d wrap your preprocessing and model in a Pipeline, then run cross\\_val\\_score or cross\\_val\\_predict. For example:\n```\n`fromsklearn.pipelineimportPipelinefromsklearn.preprocessingimportStandardScalerfromsklearn.ensembleimportGradientBoostingClassifierfromsklearn.model\\_selectionimportcross\\_val\\_scorepipe = Pipeline([(&#x27;scale&#x27;, StandardScaler()),(&#x27;gbt&#x27;, GradientBoostingClassifier())])scores = cross\\_val\\_score(pipe, X, y, cv=5)print(&quot;CV accuracy:&quot;, scores.mean())`\n```\nEach fold will automatically fit StandardScaler() and the GBT on only the training portion. Then it will transform and score on the validation portion. This prevents leakage of information (like the global mean for scaling or imputation) from contaminating your validation set. Using pipelines in this way ensures you get a realistic estimate of performance. It also prevents overly optimistic results.\n*For more tips on tree ensembles, model tuning, and Kaggle techniques,****follow me on X***[***@CoffeyFirst***](https://twitter.com/CoffeyFirst)*.*\n# Automated Hyperparameter Tuning with Optuna\nGradient-boosted models have many knobs (number of trees, learning rate/shrinkage, max depth, subsample ratio, regularization weights, etc.). Tuning them by hand or grid search can be very slow. Instead, you can use an automated search.**Optuna**is a library that implements efficient hyperparameter tuning using a[Bayesian approach (the Tree-structured Parzen Estimator, or TPE)](https://medium.com/@cris.lincoleo/a-quick-guide-to-hyperparameter-optimization-with-optuna-1980f1d185dc). Optuna\u2019s TPE sampler builds a probabilistic model of the objective scores and focuses on promising regions of the search space. This is often much faster than exhaustive grid search or blind random search. The default TPESampler in Optuna will try to maximize your validation metric (e.g. accuracy) by picking smart parameter values.\nFor example, you might write something like:\n```\n`importoptunafromoptuna.samplersimportTPESamplerdef objective(trial):n\\_trees = trial.suggest\\_int(&quot;&quot;n\\_trees&quot;&quot;,100,1000)max\\_depth = trial.suggest\\_int(&quot;&quot;max\\_depth&quot;&quot;,3,10)lr = trial.suggest\\_float(&quot;&quot;learning\\_rate&quot;&quot;,0.01,0.2, log=True)model = GradientBoostingClassifier(n\\_estimators=n\\_trees, max\\_depth=max\\_depth, learning\\_rate=lr)score = cross\\_val\\_score(model, X, y, cv=3).mean()returnscorestudy = optuna.create\\_study(direction=&quot;maximize&quot;, sampler=TPESampler(seed=42))study.optimize(objective, n\\_trials=50)print(&quot;Best params:&quot;, study.best\\_params)`\n```\nOptuna will stop when it finds a plateau (you can also use its pruning callbacks or custom early-stop callbacks). The key point is that**automated tuning with a Bayesian sampler finds good hyperparameters more efficiently than naive methods**. In my experiments, this helped GBTs converge to a strong model without endless guessing.\n*For more tips on tree ensembles, model tuning, and Kaggle techniques,****follow me on X at***[***@CoffeyFirst***](https://twitter.com/CoffeyFirst)*.*\n# Building Ensembles: Bagging, Boosting, Voting, and Stacking\nAll the tree methods I use (Random Forest, YDF\u2019s GBT, XGBoost) are[*ensembles*](https://scikit-learn.org/stable/modules/ensemble.html)of decision trees \u2014but they ensemble in different ways. To recap the terminology:\n* **Bagging (Random Forest)**: Train many independent trees on random subsets (samples and/or features) and average their predictions. Bagging mainly*reduces variance*, which is why it often guards against overfitting.\n* **Boosting (GBT / XGBoost)**: Build trees sequentially, where each new tree tries to fix errors from the previous ones. Boosting can*reduce bias*by fitting residuals, but without care it can overfit small data.\n* **Voting**: Combine different models by majority vote or by averaging probabilities (soft voting). Voting requires you have multiple pre-trained models; it doesn\u2019t train new trees, just blends predictions.\n* **Stacking**: Train multiple base models, then train a**meta-model**on their outputs. The idea is that a meta-learner (like a logistic regression) can learn how to weight each base model\u2019s vote. In practice, stacking often ends up performing about as well as the single best base model.\nI tried both soft voting and stacking with my XGBoost and YDF-GBT classifiers. For example:\n```\n`fromsklearn.ensembleimportStackingClassifier, VotingClassifierfromsklearn.linear\\_modelimportLogisticRegressionstack = StackingClassifier(estimators=[(&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;gbt&#x27;, G...",
      "url": "https://www.jamescoffey.co/post/kick-start-tree-ensembles-without-the-titanic-cliches"
    },
    {
      "title": "Overfitting, regularization, and early stopping \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Overfitting, regularization, and early stopping | Machine Learning | Google for Developers[Skip to main content](#main-content)\n* [Machine Learning](https://developers.google.com/machine-learning)\n/\n* English\n* Deutsch\n* Espa\u00f1ol\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u4e2d\u6587\u2013\u7e41\u9ad4* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4Sign in\n* [Advanced courses](https://developers.google.com/machine-learning/advanced-courses)\n* [Home](https://developers.google.com/)\n* [Products](https://developers.google.com/products)\n* [Machine Learning](https://developers.google.com/machine-learning)\n* [Advanced courses](https://developers.google.com/machine-learning/advanced-courses)\n* [Decision Forests](https://developers.google.com/machine-learning/decision-forests)\nSend feedback# Overfitting, regularization, and early stoppingStay organized with collectionsSave and categorize content based on your preferences.\n![Spark icon](https://developers.google.com/_static/images/icons/spark.svg)\n## Page Summary\noutlined\\_flag\n* Gradient boosted trees, unlike random forests, are susceptible to overfitting and may require regularization and early stopping techniques using a validation dataset.\n* Key regularization parameters for gradient boosted trees include maximum tree depth, shrinkage rate, attribute test ratio at each node, and L1/L2 coefficients on the loss.\n* Gradient boosted trees offer advantages such as native support for various feature types, generally good default hyperparameters, and efficient model size and prediction speed.\n* Training gradient boosted trees involves sequential tree construction, potentially slowing down the process, and they lack the ability to learn and reuse internal representations, potentially impacting performance on certain datasets.\nUnlike random forests, gradient boosted trees*can*overfit. Therefore, as for\nneural networks, you can apply regularization and early stopping using a\nvalidation dataset.\nFor example, the following figures show loss and accuracy curves for training\nand validation sets when training a GBT model. Notice how divergent the curves\nare, which suggests a high degree of overfitting.\n![Plots of training loss and validation loss versus the number of\ndecision trees. Training loss gradually decreases as the number\nof decision trees increases. However, validation loss only decreases\nuntil about 40 decision trees. With more than 40 decision trees,\nvalidation loss actually increases. With 400 decision trees, the\ngap between training loss and validation loss is\nenormous.](https://developers.google.com/static/machine-learning/decision-forests/images/LossVersusNumberOfDecisionTrees.png \"image_tooltip\")\n**Figure 29. Loss vs. number of decision trees.**\n![Plots of training accuracy and validation accuracy versus the number of\ndecision trees. Training accuracy gradually increases as the number\nof decision trees increases, reaching a peak of almost 1.0 at 400\ndecision trees. Validation accuracy increases to about 0.86 at 40\ndecision trees, then gradually falls to about 0.83 at 400 decision\ntrees.](https://developers.google.com/static/machine-learning/decision-forests/images/AccuracyVersusNumberOfDecisionTrees.png \"image_tooltip\")\n**Figure 30. Accuracy vs. number of decision trees.**\nCommon regularization parameters for gradient boosted trees include:\n* The maximum depth of the tree.\n* The shrinkage rate.\n* The ratio of attributes tested at each node.\n* L1 and L2 coefficient on the loss.\nNote that decision trees generally grow much shallower than random forest\nmodels. By default, gradient boosted trees trees in TF-DF are grown to depth 6.\nBecause the trees are shallow, the minimum number of examples per leaf has\nlittle impact and is generally not tuned.\nThe need for a validation dataset is an issue when the number of training\nexamples is small. Therefore, it is common to train gradient boosted trees\ninside a cross-validation loop, or to disable early stopping when the model\nis known not to overfit.\n## Usage example\nIn the previous chapter, we trained a random forest on a small dataset. In this\nexample, we will simply replace the random forest model with a gradient boosted\ntrees model:\n```\n`model = tfdf.keras.GradientBoostedTreesModel()#Part of the training dataset will be used as validation (and removed#from training).\nmodel.fit(tf\\_train\\_dataset)#The user provides the validation dataset.\nmodel.fit(tf\\_train\\_dataset, validation\\_data=tf\\_valid\\_dataset)#Disable early stopping and the validation dataset. All the examples are#used for training.\nmodel.fit(\ntf\\_train\\_dataset,\nvalidation\\_ratio=0.0,\nearly\\_stopping=\"NONE\")#Note: When \"validation\\_ratio=0\", early stopping is automatically disabled,#so early\\_stopping=\"NONE\" is redundant here.`\n```\n## Usage and limitations\nGradient boosted trees have some pros and cons.\n**Pros**\n* Like decision trees, they natively support numerical and categorical\nfeatures and often do not need feature pre-processing.\n* Gradient boosted trees have default hyperparameters that often give great\nresults. Nevertheless, tuning those hyperparameters can significantly\nimprove the model.\n* Gradient boosted tree models are generally small (in number of nodes and in\nmemory) and fast to run (often just one or a few \u00b5s / examples).\n**Cons**\n* The decision trees must be trained sequentially, which can slow training\nconsiderably. However, the training slowdown is somewhat offset by the\ndecision trees being smaller.\n* Like random forests, gradient boosted trees can&#39;t learn and reuse internal\nrepresentations. Each decision tree (and each branch of each decision tree)\nmust relearn the dataset pattern. In some datasets, notably datasets with\nunstructured data (for example, images, text), this causes gradient boosted\ntrees to show poorer results than other methods.\n[\nPrevious\narrow\\_backThe gradient boosting algorithm (optional)](https://developers.google.com/machine-learning/decision-forests/gradient-boosting)\n[\nNext\nCourse summaryarrow\\_forward](https://developers.google.com/machine-learning/decision-forests/summary)\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the[Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the[Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-08-25 UTC.\nNeed to tell us more?[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-08-25 UTC.\"],[],[]]",
      "url": "https://developers.google.com/machine-learning/decision-forests/overfitting-gbdt"
    },
    {
      "title": "",
      "text": "A Meta-Analysis of Overfitting in Machine Learning\nRebecca Roelofs\u2217\nUC Berkeley\nroelofs@berkeley.edu\nSara Fridovich-Keil\u2217\nUC Berkeley\nsfk@berkeley.edu\nJohn Miller\nUC Berkeley\nmiller_john@berkeley.edu\nVaishaal Shankar\nUC Berkeley\nvaishaal@berkeley.edu\nMoritz Hardt\nUC Berkeley\nhardt@berkeley.edu\nBenjamin Recht\nUC Berkeley\nbrecht@berkeley.edu\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nAbstract\nWe conduct the first large meta-analysis of overfitting due to test set reuse in the\nmachine learning community. Our analysis is based on over one hundred machine\nlearning competitions hosted on the Kaggle platform over the course of several\nyears. In each competition, numerous practitioners repeatedly evaluated their\nprogress against a holdout set that forms the basis of a public ranking available\nthroughout the competition. Performance on a separate test set used only once\ndetermined the final ranking. By systematically comparing the public ranking\nwith the final ranking, we assess how much participants adapted to the holdout set\nover the course of a competition. Our study shows, somewhat surprisingly, little\nevidence of substantial overfitting. These findings speak to the robustness of the\nholdout method across different data domains, loss functions, model classes, and\nhuman analysts.\n1 Introduction\nThe holdout method is central to empirical progress in the machine learning community. Competitions,\nbenchmarks, and large-scale hyperparameter search all rely on splitting a data set into multiple pieces\nto separate model training from evaluation. However, when practitioners repeatedly reuse holdout\ndata, the danger of overfitting to the holdout data arises [6, 13].\nDespite its importance, there is little empirical research into the manifested robustness and validity\nof the holdout method in practical scenarios. Real-world use cases of the holdout method often\nfall outside the guarantees of existing theoretical bounds, making questions of validity a matter of\nguesswork.\nRecent replication studies [16] demonstrated that the popular CIFAR-10 [10] and ImageNet [5, 18]\nbenchmarks continue to support progress despite years of intensive use. The longevity of these\nbenchmarks perhaps suggests that overfitting to holdout data is less of a concern than reasoning from\nfirst principles might have suggested. However, this is evidence from only two, albeit important,\ncomputer vision benchmarks. It remains unclear whether the observed phenomenon is specific to the\ndata domain, model class, or practices of vision researchers. Unfortunately, these replication studies\n\u2217Equal contribution\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nrequired assembling new test sets from scratch, resulting in a highly labor-intensive analysis that is\ndifficult to scale.\nIn this paper, we empirically study holdout reuse at a significantly larger scale by analyzing data\nfrom 120 machine learning competitions on the popular Kaggle platform [2]. Kaggle competitions\nare a particularly well-suited environment for studying overfitting since data sources are diverse,\ncontestants use a wide range of model families, and training techniques vary greatly. Moreover,\nKaggle competitions use public and private test data splits which provide a natural experimental setup\nfor measuring overfitting on various datasets.\nTo provide a detailed analysis of each competition, we introduce a coherent methodology to char\u0002acterize the extent of overfitting at three increasingly fine scales. Our approach allows us both to\ndiscuss the overall \u201chealth\u201d of a competition across all submissions and to inspect signs of overfitting\nseparately among the top submissions. In addition, we develop a statistical test specific to the\nclassification competitions on Kaggle to compare the submission scores to those arising in an ideal\nnull model that assumes no overfitting. Observed data that are close to data predicted by the null\nmodel is strong evidence against overfitting.\nOverall, we conclude that the classification competitions on Kaggle show little to no signs of\noverfitting. While there are some outlier competitions in the data, these competitions usually have\npathologies such as non-i.i.d. data splits or (effectively) small test sets. Among the remaining\ncompetitions, the public and private test scores show a remarkably good correspondence. The\npicture becomes more nuanced among the highest scoring submissions, but the overall effect sizes\nof (potential) overfitting are typically small (e.g., less than 1% classification accuracy). Thus, our\nfindings show that substantial overfitting is unlikely to occur naturally in regular machine learning\nworkflows.\n2 Background and setup\nBefore we delve into the analysis of the Kaggle data, we briefly define the type of overfitting we\nstudy and then describe how the Kaggle competition format naturally lends itself to investigating\noverfitting in machine learning competitions.\n2.1 Adaptive overfitting\n\u201cOverfitting\u201d is often used as an umbrella term to describe any unwanted performance drop of a\nmachine learning model. Here, we focus on adaptive overfitting, which is overfitting caused by test\nset reuse. While other phenomena under the overfitting umbrella are also important aspects of reliable\nmachine learning (e.g. performance drops due to distribution shifts), they are beyond the scope of our\npaper since they require an experimental setup different from ours.\nFormally, let f : X \u2192 Y be a trained model that maps examples x \u2208 X to output values y \u2208 Y\n(e.g., class labels or regression targets). The standard approach to measuring the performance of\nsuch a trained model is to define a loss function L : Y \u00d7 Y \u2192 R and to draw samples S =\n{(x1, y1), . . . ,(xn, yn)} from a data distribution D which we then use to evaluate the test loss\nLS(f) = Pn\ni=1 L(f(xi), yi). As long as the model f does not depend on the test set S, standard\nconcentration results [19] show that LS(f) is a good approximation of the true performance given by\nthe population loss LD(f) = ED[L(f(x), y)].\nHowever, machine learning practitioners often undermine the assumption that f does not depend\non the test set by selecting models and tuning hyperparameters based on the test loss. Especially\nwhen algorithm designers evaluate a large number of different models on the same test set, the final\nclassifier may only perform well on the specific examples in the test set. The failure to generalize to\nthe entire data distribution D manifests itself in a large adaptivity gap LD(f) \u2212 LS(f) and leads to\noverly optimistic performance estimates.\n2.2 Kaggle\nKaggle is the most widely used platform for machine learning competitions, currently hosting 1,461\nactive and completed competitions. Various organizations (companies, educators, etc.) provide the\n2\ndatasets and evaluation rules for the competitions, which are generally open to any participant. Each\ncompetition is centered around a dataset consisting of a training set and a test set.\nConsidering the danger of overfitting to the test set in a competitive environment, Kaggle subdivides\neach test set into public and private components. The subsets are randomly shuffled together and\nthe entire test set is released without labels, so that participants should not know which test samples\nbelong to which split. Hence participants submit predictions for the entire test set. The Kaggle server\nthen internally evaluates each submission on both public and private splits and updates the public\ncompetition leaderboard only with the score on the public split. At the end of the competition, Kaggle\nreleases the private scores, which determine the winner.\nKaggle has released the MetaKaggle dataset2, which contains detailed information about competitions,\nsubmissions, etc. on the Kaggle platform. The structure of Kaggle competitions makes MetaKaggle\na useful dataset for investigating overfitting empirically at a large scale. In particular, we can view\nthe public test ...",
      "url": "https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf"
    },
    {
      "title": "how to avoid overfitting in XGBoost model",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [how to avoid overfitting in XGBoost model](https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked4 years, 5 months ago\n\nModified [8 months ago](https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model?lastactivity)\n\nViewed\n61k times\n\n23\n\n$\\\\begingroup$\n\n[![ROC curve image on train and test data](https://i.sstatic.net/eGEcy.png)](https://i.sstatic.net/eGEcy.png)\n\nI try to classify data from a dataset of 35K data points and 12 features\n\nFirstly, I have divided the data into train and test data for cross-validation.\n\nAfter cross validation I have built a XGBoost model using below parameters:\n\n`n_estimators = 100`\n\n`max_depth=4`\n\n`scale_pos_weight = 0.2` as the data is imbalanced (85%positive class)\n\nThe model is overfitting the training data. What can be done to avoid overfitting?\n\n- [overfitting](https://stats.stackexchange.com/questions/tagged/overfitting)\n- [boosting](https://stats.stackexchange.com/questions/tagged/boosting)\n\n[Share](https://stats.stackexchange.com/q/443259)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/443259/edit)\n\nFollow\n\n[edited Oct 10, 2023 at 18:05](https://stats.stackexchange.com/posts/443259/revisions)\n\n[![Archie's user avatar](https://www.gravatar.com/avatar/e39a2852d5783a1535b5ace436289eaf?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/96626/archie)\n\n[Archie](https://stats.stackexchange.com/users/96626/archie)\n\n59577 silver badges1717 bronze badges\n\nasked Jan 4, 2020 at 12:11\n\n[![Lalit Jain's user avatar](https://graph.facebook.com/1142366799290140/picture?type=large)](https://stats.stackexchange.com/users/269979/lalit-jain)\n\n[Lalit Jain](https://stats.stackexchange.com/users/269979/lalit-jain) Lalit Jain\n\n33111 gold badge22 silver badges33 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n28\n\n$\\\\begingroup$\n\nXGBoost (and other gradient boosting machine routines too) has a number of parameters that can be tuned to avoid over-fitting. I will mention some of the most obvious ones. For example we can change:\n\n- the ratio of features used (i.e. columns used); `colsample_bytree`. Lower ratios avoid over-fitting.\n- the ratio of the training instances used (i.e. rows used); `subsample`. Lower ratios avoid over-fitting.\n- the maximum depth of a tree; `max_depth`. Lower values avoid over-fitting.\n- the minimum loss reduction required to make a further split; `gamma`. Larger values avoid over-fitting.\n- the learning rate of our GBM (i.e. how much we update our prediction with each successive tree); `eta`. Lower values avoid over-fitting.\n- the minimum sum of instance weight needed in a leaf, in certain applications this relates directly to the minimum number of instances needed in a node; `min_child_weight`. Larger values avoid over-fitting.\n\nThis list is _not exhaustive_ and I will strongly urge looking into [XGBoost docs](https://xgboost.readthedocs.io/en/latest/parameter.html) for information regarding other parameters. Please note that trying to avoid over-fitting might lead to [under-fitting](https://en.wikipedia.org/wiki/Overfitting#Underfitting), where we regularise too much and fail to learn relevant information. On that matter, one might want to consider using a separate validation set or simply cross-validation (through `xgboost.cv()` for example) to monitor the progress of the GBM as more iterations are performed (i.e. base learners are added). That way potentially over-fitting problems can be caught early on. This relates close to the use of [early-stopping](https://en.wikipedia.org/wiki/Early_stopping) as a form a regularisation; XGBoost offers an argument `early_stopping_rounds` that is relevant in this case.\n\nFinally, I would also note that the class imbalance reported (85-15) is not really severe. Using the default value `scale_pos_weight` of 1 is probably adequate.\n\n[Share](https://stats.stackexchange.com/a/443266)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/443266/edit)\n\nFollow\n\n[edited Jan 4, 2020 at 14:10](https://stats.stackexchange.com/posts/443266/revisions)\n\nanswered Jan 4, 2020 at 13:25\n\n[![us\u03b5r11852's user avatar](https://i.sstatic.net/Y88zz.png?s=64)](https://stats.stackexchange.com/users/11852/us%ce%b5r11852)\n\n[us\u03b5r11852](https://stats.stackexchange.com/users/11852/us%ce%b5r11852) us\u03b5r11852\n\n44.7k33 gold badges101101 silver badges158158 bronze badges\n\n$\\\\endgroup$\n\n6\n\n- $\\\\begingroup$+1\\. Another thing that might help is to be sure that the class imbalance is equivalent between the training set and testing set by using stratified sampling with the outcome when making the split. Even moderate discrepancies due to an unlucky split without stratification can yield results similar to what you have in your plot.$\\\\endgroup$\n\n\u2013\u00a0[dmartin](https://stats.stackexchange.com/users/21654/dmartin)\n\nCommentedSep 12, 2020 at 19:53\n\n- $\\\\begingroup$@dmartin: Thank you for you upvote but apologies as I somewhat disagree with the point you make. Unless we are looking at a severely imbalanced problem a performance degradation in terms of AUC-ROC from 90% down to 68% is extremely unlikely to be due to \" _moderate discrepancies_\" in the train-test (T-T) split. Using stratified sampling is always a reasonable idea. For the case presented though, 35K / 85-15 class imbalance, the chance of a say at 80-20 T-T split more disproportionate than an 87.5-12.5 class split is next to none. (cont.)$\\\\endgroup$\n\n\u2013\u00a0[us\u03b5r11852](https://stats.stackexchange.com/users/11852/us%ce%b5r11852)\n\nCommentedSep 12, 2020 at 21:43\n\n- $\\\\begingroup$Taking in account also that AUC-ROC is reasonably robust to class imbalances, I think the train-test split is very unlikely to be the culprit.$\\\\endgroup$\n\n\u2013\u00a0[us\u03b5r11852](https://stats.stackexchange.com/users/11852/us%ce%b5r11852)\n\nCommentedSep 12, 2020 at 21:44\n\n- $\\\\begingroup$It certainly is not likely, but something to check. The reason I commented was that I recently saw a case where a bug in someone's code caused them to erroneously have an 80-20 imbalance in train and a 90-10 in test. The AUC plot was very similar to what was posted in the question (AUC of .85 going down to .7). After spending quite some time tuning the xgboost parameters to reduce complexity with no avail, I had them check the imbalance and they found this issue. Using stratified sampling fixed it entirely.$\\\\endgroup$\n\n\u2013\u00a0[dmartin](https://stats.stackexchange.com/users/21654/dmartin)\n\nCommentedSep 13, 2020 at 22:42\n\n- $\\\\begingroup$@dmartin: Thank you for the clarification, I stand corrected it seems. Must have been a pretty unlucky run. Even with 1000 points, when starting with 80-20 class imbalance, the chance of getting a 90-10 class split given a 80-20 T-T split, is less than 0.01%.$\\\\endgroup$\n\n\u2013\u00a0[us\u03b5r11852](https://stats.stackexchange.com/users/11852/us%ce%b5r11852)\n\nCommentedSep 14, 2020 at 0:06\n\n\n\\|\u00a0[Show **1** more comment](https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model)\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f443259%2fhow-to-avoid-overfitting-in-xgboost-model%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but ...",
      "url": "https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) [2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2) 3 [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Model Training**\n\nWe can improve a model\u2019s performance by tuning its parameters. A model usually have many parameters, but only a few of them are significant to its performance. For example, the most important parameters for a random forset is the number of trees in the forest and the maximum number of features used in developing each tree.\u00a0**We need to understand how models work and what impact does each parameter have to the model\u2019s performance, be it accuracy, robustness or speed.**\n\nNormally we would find the best set of parameters by a process called\u00a0**[grid search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**. Actually what it does is simply iterating through all the possible combinations and find the best one.\n\nBy the way, random forest usually reach optimum when\u00a0`max_features`\u00a0is set to the square root of the total number of features.\n\nHere I\u2019d like to stress some points about tuning XGB. These parameters are generally considered to have real impacts on its performance:\n\n- `eta`: Step size used in updating weights. Lower\u00a0`eta`\u00a0means slower training but better convergence.\n- `num_round`: Total number of iterations.\n- `subsample`: The ratio of training data used in each iteration. This is to combat overfitting.\n- `colsample_bytree`: The ratio of features used in each iteration. This is like\u00a0`max_features`\u00a0in\u00a0`RandomForestClassifier`.\n- `max_depth`: The maximum depth of each tree. Unlike random forest,\u00a0**gradient boosting would eventually overfit if we do not limit its depth**.\n- `early_stopping_rounds`: If we don\u2019t see an increase of validation score for a given number of iterations, the algorithm will stop early. This is to combat overfitting, too.\n\nUsual tuning steps:\n\n1. Reserve a portion of training set as the validation set.\n2. Set\u00a0`eta`\u00a0to a relatively high value (e.g. 0.05 ~ 0.1),\u00a0`num_round`\u00a0to 300 ~ 500.\n3. Use grid search to find the best combination of other parameters.\n4. Gradually lower\u00a0`eta`\u00a0until we reach the optimum.\n5. **Use the validation set as\u00a0`watch_list`\u00a0to re-train the model with the best parameters. Observe how score changes on validation set in each iteration. Find the optimal value for\u00a0`early_stopping_rounds`.**\n\nFinally, note that models with randomness all have a parameter like\u00a0`seed`\u00a0or\u00a0`random_state`\u00a0to control the random seed.\u00a0**You must record this**\u00a0with all other parameters when you get a good model. Otherwise you wouldn\u2019t be able to reproduce it.\n\n**Cross Validation**\n\n**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**\u00a0is an essential step in model training. It tells us whether our model is at high risk of overfitting. In many competitions, public LB scores are not very reliable. Often when we improve the model and get a better local CV score, the LB score becomes worse.\u00a0**It is widely believed that we should trust our CV scores under such situation.** Ideally we would want\u00a0**CV scores obtained by different approaches to improve in sync with each other and with the LB score**, but this is not always possible.\n\nUsually\u00a0**5-fold CV**\u00a0is good enough. If we use more folds, the CV score would become more reliable, but the training takes longer to finish as well. However, we shouldn\u2019t use too many folds if our training data is limited. Otherwise we would have too few samples in each fold to guarantee statistical significance.\n\nHow to do CV properly is not a trivial problem. It requires constant experiment and case-by-case discussion. Many Kagglers share their CV approaches (like\u00a0[this one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)) after competitions when they feel that reliable CV is not easy.\n\n**Ensemble Generation**\n\n[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\u00a0refers to the technique of combining different models. It\u00a0**reduces both bias and variance of the final model**\u00a0(you can find a proof\u00a0[here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)), thus\u00a0**increasing the score and reducing the risk of overfitting**. Recently it became virtually impossible to win prize without using ensemble in Kaggle competitions.\n\nCommon approaches of ensemble learning are:\n\n- **Bagging**: Use different random subsets of training data to train each base model. Then all the base models vote to generate the final predictions. This is how random forest works.\n- **Boosting**: Train base models iteratively, modify the weights of training samples according to the last iteration. This is how gradient boosted trees work. (Actually it\u2019s not the whole story. Apart from boosting, GBTs try to learn the residuals of earlier iterations.) It performs better than bagging but is more prone to overfitting.\n- **Blending**: Use non-overlapping data to train different base models and take a weighted average of them to obtain the final predictions. This is easy to implement but uses less data.\n- **Stacking**: To be discussed next.\n\nIn theory, for the ensemble to perform well, two factors matter:\n\n- **Base models should be as unrelated as possibly**. This is why we tend to include non-tree-based models in the ensemble even though they don\u2019t perform as well. The math says that the greater the diversity, and less bias in the final ensemble.\n- **Performance of base models shouldn\u2019t differ to much.**\n\nActually we have a\u00a0**trade-off**\u00a0here. In practice we may end up with highly related models of comparable performances. Yet we ensemble them anyway because it usually increase the overall performance.\n\n**Stacking**\n\nCompared with blending, stacking makes better use of training data. Here\u2019s a diagram of how it works:\n\n[![Stacking](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2099%200'%3E%3C/svg%3E)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)\n\n_(Taken from\u00a0[Faron](https://www.kaggle.com/mmueller). Many thanks!)_\n\nIt\u2019s much like cross validation. Take 5-fold stacking as an example. First we split the training data into 5 folds. Next we will do 5 iterations. In each iteration, train every base model on 4 folds and predict on the hold-out fold.\u00a0**You have to keep the predictions on the testing data as well.**\u00a0This way, in each iteration every base model will make predictions on 1 fold of the training data and all of the testing data. After 5 iterations we will obtain a matrix of shape\u00a0`#(samples in training data) X #(base models)`. This matrix is then fed to the stacker (it\u2019s just another model) in the second level. After the stacker is fitted, use the predictions on testing data by base models ( **each base model is trained 5 times, therefore we have to take an average to obtain a matrix of the same shape**) as the input for the stacker and obtain our final predictions.\n\nMaybe it\u2019s better to just show the codes:\n\nPrize winners usually have larger and much more complicated ensembles. For beginner, implementing a correct 5-fold stacking is good enough.\n\n**\\*Pipeline**\n\nWe can see that the workflow for a Kaggle competition is quite complex, especially for model selection and ensemble. Ideally, we need a highly automated pipeline capable of:\n\n- **Modularized feature transformations**. We only need to write a few lines of codes (or better, rules / DSLs) and the new feature is added to the training set.\n- **Automated grid search**. We only need to set up models and parameter grid, the search w...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3"
    }
  ]
}