{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ce8910",
   "metadata": {},
   "source": [
    "# Loop 7 Analysis: Calibration vs CV Trade-off\n",
    "\n",
    "## Key Question:\n",
    "Should we submit exp_006 (weighted ensemble) to test if better calibration compensates for lower CV?\n",
    "\n",
    "## Evaluator's Recommendation:\n",
    "- Submit exp_006 to test calibration hypothesis\n",
    "- Prediction rate 50.9% is closest to training (50.4%)\n",
    "- Even if it doesn't beat exp_003, we learn something valuable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65aa1bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:05:41.327096Z",
     "iopub.status.busy": "2026-01-06T05:05:41.326273Z",
     "iopub.status.idle": "2026-01-06T05:05:41.781741Z",
     "shell.execute_reply": "2026-01-06T05:05:41.781110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Summary:\n",
      "    exp      cv     lb  pred_rate              model     gap  gap_pct\n",
      "exp_000 0.80674 0.7971        NaN   XGBoost baseline 0.00964 1.194933\n",
      "exp_003 0.81951 0.8045      0.517    CatBoost Optuna 0.01501 1.831582\n",
      "exp_004 0.81928 0.8041      0.538 CatBoost threshold 0.01518 1.852846\n",
      "exp_006 0.81709    NaN      0.509  Weighted ensemble     NaN      NaN\n",
      "\n",
      "Training transported rate: 50.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# All experiments with CV, LB, and prediction rates\n",
    "experiments = [\n",
    "    {'exp': 'exp_000', 'cv': 0.80674, 'lb': 0.7971, 'pred_rate': None, 'model': 'XGBoost baseline'},\n",
    "    {'exp': 'exp_003', 'cv': 0.81951, 'lb': 0.8045, 'pred_rate': 0.517, 'model': 'CatBoost Optuna'},\n",
    "    {'exp': 'exp_004', 'cv': 0.81928, 'lb': 0.8041, 'pred_rate': 0.538, 'model': 'CatBoost threshold'},\n",
    "    {'exp': 'exp_006', 'cv': 0.81709, 'lb': None, 'pred_rate': 0.509, 'model': 'Weighted ensemble'},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(experiments)\n",
    "df['gap'] = df['cv'] - df['lb']\n",
    "df['gap_pct'] = (df['gap'] / df['cv']) * 100\n",
    "print(\"Experiment Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nTraining transported rate: 50.36%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae45796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:05:41.783995Z",
     "iopub.status.busy": "2026-01-06T05:05:41.783436Z",
     "iopub.status.idle": "2026-01-06T05:05:41.793107Z",
     "shell.execute_reply": "2026-01-06T05:05:41.792572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREDICTION RATE vs LB PERFORMANCE ===\n",
      "Training rate: 50.36%\n",
      "\n",
      "    exp  pred_rate     lb      cv     gap\n",
      "exp_000        NaN 0.7971 0.80674 0.00964\n",
      "exp_003      0.517 0.8045 0.81951 0.01501\n",
      "exp_004      0.538 0.8041 0.81928 0.01518\n",
      "\n",
      "=== PATTERN ANALYSIS ===\n",
      "exp_003: pred_rate=51.7% (diff=1.34%) -> LB=0.8045 (BEST)\n",
      "exp_004: pred_rate=53.8% (diff=3.44%) -> LB=0.8041 (worse)\n",
      "exp_006: pred_rate=50.9% (diff=0.54%) -> LB=??? (closest to training!)\n",
      "\n",
      "Hypothesis: Closer to training rate = better LB\n"
     ]
    }
   ],
   "source": [
    "# Analyze prediction rate vs LB performance\n",
    "print(\"\\n=== PREDICTION RATE vs LB PERFORMANCE ===\")\n",
    "print(f\"Training rate: 50.36%\")\n",
    "print()\n",
    "submitted = df[df['lb'].notna()].copy()\n",
    "submitted['rate_diff'] = abs(submitted['pred_rate'] - 0.5036) if submitted['pred_rate'].notna().any() else None\n",
    "print(submitted[['exp', 'pred_rate', 'lb', 'cv', 'gap']].to_string(index=False))\n",
    "\n",
    "# Pattern analysis\n",
    "print(\"\\n=== PATTERN ANALYSIS ===\")\n",
    "print(\"exp_003: pred_rate=51.7% (diff=1.34%) -> LB=0.8045 (BEST)\")\n",
    "print(\"exp_004: pred_rate=53.8% (diff=3.44%) -> LB=0.8041 (worse)\")\n",
    "print(\"exp_006: pred_rate=50.9% (diff=0.54%) -> LB=??? (closest to training!)\")\n",
    "print(\"\\nHypothesis: Closer to training rate = better LB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f39b215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:05:41.795342Z",
     "iopub.status.busy": "2026-01-06T05:05:41.794746Z",
     "iopub.status.idle": "2026-01-06T05:05:41.802713Z",
     "shell.execute_reply": "2026-01-06T05:05:41.802072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LB PREDICTION FOR exp_006 ===\n",
      "Method 1 (avg gap 0.0133): LB = 0.8038\n",
      "Method 2 (recent gap 0.0152): LB = 0.8019\n",
      "Method 3 (calibration-adjusted gap 0.0120): LB = 0.8051\n",
      "\n",
      "Best LB so far (exp_003): 0.8045\n",
      "\n",
      "Range of predictions: 0.8019 - 0.8051\n"
     ]
    }
   ],
   "source": [
    "# Predict LB for exp_006 using different methods\n",
    "print(\"\\n=== LB PREDICTION FOR exp_006 ===\")\n",
    "\n",
    "# Method 1: Using average CV-LB gap\n",
    "mean_gap = df[df['lb'].notna()]['gap'].mean()\n",
    "pred_lb_gap = 0.81709 - mean_gap\n",
    "print(f\"Method 1 (avg gap {mean_gap:.4f}): LB = {pred_lb_gap:.4f}\")\n",
    "\n",
    "# Method 2: Using recent gap (exp_004)\n",
    "recent_gap = 0.81928 - 0.8041\n",
    "pred_lb_recent = 0.81709 - recent_gap\n",
    "print(f\"Method 2 (recent gap {recent_gap:.4f}): LB = {pred_lb_recent:.4f}\")\n",
    "\n",
    "# Method 3: Calibration-adjusted (hypothesis: better calibration reduces gap)\n",
    "# exp_003 gap was 1.50%, exp_004 gap was 1.86% (worse calibration = bigger gap)\n",
    "# exp_006 has best calibration, so gap might be smaller\n",
    "calibration_adjusted_gap = 0.012  # Conservative estimate\n",
    "pred_lb_calibration = 0.81709 - calibration_adjusted_gap\n",
    "print(f\"Method 3 (calibration-adjusted gap {calibration_adjusted_gap:.4f}): LB = {pred_lb_calibration:.4f}\")\n",
    "\n",
    "print(f\"\\nBest LB so far (exp_003): 0.8045\")\n",
    "print(f\"\\nRange of predictions: {pred_lb_recent:.4f} - {pred_lb_calibration:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8315de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:05:41.804809Z",
     "iopub.status.busy": "2026-01-06T05:05:41.804352Z",
     "iopub.status.idle": "2026-01-06T05:05:41.811247Z",
     "shell.execute_reply": "2026-01-06T05:05:41.810683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DECISION ANALYSIS ===\n",
      "\n",
      "Arguments FOR submitting exp_006:\n",
      "1. Prediction rate (50.9%) is closest to training (50.4%)\n",
      "2. We have evidence that prediction rate matters (exp_004 failure)\n",
      "3. We have 7 submissions remaining - plenty of room to test\n",
      "4. Even if it fails, we learn about calibration vs CV trade-off\n",
      "5. Evaluator recommends it\n",
      "\n",
      "Arguments AGAINST submitting exp_006:\n",
      "1. CV (0.81709) is lower than exp_003 (0.81951) by 0.24%\n",
      "2. If CV-LB gap is constant, this predicts worse LB\n",
      "3. Could waste a submission\n",
      "\n",
      "My assessment:\n",
      "- 40% chance exp_006 beats exp_003 (calibration hypothesis)\n",
      "- 60% chance exp_006 underperforms (CV dominates)\n",
      "- Either way, we learn something valuable\n",
      "- With 7 submissions remaining, this is a good use of quota\n"
     ]
    }
   ],
   "source": [
    "# Decision analysis\n",
    "print(\"\\n=== DECISION ANALYSIS ===\")\n",
    "print(\"\\nArguments FOR submitting exp_006:\")\n",
    "print(\"1. Prediction rate (50.9%) is closest to training (50.4%)\")\n",
    "print(\"2. We have evidence that prediction rate matters (exp_004 failure)\")\n",
    "print(\"3. We have 7 submissions remaining - plenty of room to test\")\n",
    "print(\"4. Even if it fails, we learn about calibration vs CV trade-off\")\n",
    "print(\"5. Evaluator recommends it\")\n",
    "\n",
    "print(\"\\nArguments AGAINST submitting exp_006:\")\n",
    "print(\"1. CV (0.81709) is lower than exp_003 (0.81951) by 0.24%\")\n",
    "print(\"2. If CV-LB gap is constant, this predicts worse LB\")\n",
    "print(\"3. Could waste a submission\")\n",
    "\n",
    "print(\"\\nMy assessment:\")\n",
    "print(\"- 40% chance exp_006 beats exp_003 (calibration hypothesis)\")\n",
    "print(\"- 60% chance exp_006 underperforms (CV dominates)\")\n",
    "print(\"- Either way, we learn something valuable\")\n",
    "print(\"- With 7 submissions remaining, this is a good use of quota\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc22677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:05:41.813028Z",
     "iopub.status.busy": "2026-01-06T05:05:41.812787Z",
     "iopub.status.idle": "2026-01-06T05:05:41.819042Z",
     "shell.execute_reply": "2026-01-06T05:05:41.818466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UNEXPLORED APPROACHES ===\n",
      "\n",
      "1. FEATURE SELECTION (high priority)\n",
      "   - 22 features have importance < 1.0\n",
      "   - Could reduce overfitting\n",
      "   - Not yet tried\n",
      "\n",
      "2. REGULARIZED CATBOOST\n",
      "   - depth=6 (vs 8), l2_leaf_reg=5.0 (vs 3.52)\n",
      "   - subsample=0.8 for randomness\n",
      "   - Partially tried in loop 5 analysis but not as full experiment\n",
      "\n",
      "3. DIFFERENT ENSEMBLE WEIGHTS\n",
      "   - Current: 0.6*CatBoost + 0.2*XGB + 0.2*LGB\n",
      "   - Could try: 0.7*CatBoost + 0.15*XGB + 0.15*LGB\n",
      "   - Or: 0.5*CatBoost + 0.25*XGB + 0.25*LGB\n",
      "\n",
      "4. NEURAL NETWORK\n",
      "   - Not tried at all\n",
      "   - Could add diversity for ensembling\n",
      "   - TabNet or simple MLP\n",
      "\n",
      "5. PSEUDO-LABELING\n",
      "   - Use confident predictions on test set\n",
      "   - Retrain with pseudo-labels\n",
      "   - Risky but could help\n"
     ]
    }
   ],
   "source": [
    "# What else should we try?\n",
    "print(\"\\n=== UNEXPLORED APPROACHES ===\")\n",
    "print(\"\\n1. FEATURE SELECTION (high priority)\")\n",
    "print(\"   - 22 features have importance < 1.0\")\n",
    "print(\"   - Could reduce overfitting\")\n",
    "print(\"   - Not yet tried\")\n",
    "\n",
    "print(\"\\n2. REGULARIZED CATBOOST\")\n",
    "print(\"   - depth=6 (vs 8), l2_leaf_reg=5.0 (vs 3.52)\")\n",
    "print(\"   - subsample=0.8 for randomness\")\n",
    "print(\"   - Partially tried in loop 5 analysis but not as full experiment\")\n",
    "\n",
    "print(\"\\n3. DIFFERENT ENSEMBLE WEIGHTS\")\n",
    "print(\"   - Current: 0.6*CatBoost + 0.2*XGB + 0.2*LGB\")\n",
    "print(\"   - Could try: 0.7*CatBoost + 0.15*XGB + 0.15*LGB\")\n",
    "print(\"   - Or: 0.5*CatBoost + 0.25*XGB + 0.25*LGB\")\n",
    "\n",
    "print(\"\\n4. NEURAL NETWORK\")\n",
    "print(\"   - Not tried at all\")\n",
    "print(\"   - Could add diversity for ensembling\")\n",
    "print(\"   - TabNet or simple MLP\")\n",
    "\n",
    "print(\"\\n5. PSEUDO-LABELING\")\n",
    "print(\"   - Use confident predictions on test set\")\n",
    "print(\"   - Retrain with pseudo-labels\")\n",
    "print(\"   - Risky but could help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95d61a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T05:05:41.821054Z",
     "iopub.status.busy": "2026-01-06T05:05:41.820505Z",
     "iopub.status.idle": "2026-01-06T05:05:41.827974Z",
     "shell.execute_reply": "2026-01-06T05:05:41.827433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STRATEGY RECOMMENDATION ===\n",
      "\n",
      "1. SUBMIT exp_006 to test calibration hypothesis\n",
      "   - Quick feedback on whether calibration > CV\n",
      "   - Uses 1 of 7 remaining submissions\n",
      "\n",
      "2. NEXT EXPERIMENT: Feature Selection + CatBoost\n",
      "   - Remove 22 low-importance features\n",
      "   - Retrain CatBoost with best params\n",
      "   - Check if reduced feature set improves generalization\n",
      "\n",
      "3. IF exp_006 beats exp_003:\n",
      "   - Calibration matters more than CV\n",
      "   - Focus on approaches that improve calibration\n",
      "   - Try different ensemble weights\n",
      "\n",
      "4. IF exp_006 doesn't beat exp_003:\n",
      "   - CV is more important than calibration\n",
      "   - Focus on feature selection + regularization\n",
      "   - Try to improve CV while maintaining reasonable calibration\n",
      "\n",
      "5. LONG-TERM: Build diverse models for final ensemble\n",
      "   - CatBoost (best single model)\n",
      "   - Feature-selected CatBoost\n",
      "   - Regularized CatBoost\n",
      "   - Maybe Neural Network for diversity\n"
     ]
    }
   ],
   "source": [
    "# Strategy recommendation\n",
    "print(\"\\n=== STRATEGY RECOMMENDATION ===\")\n",
    "print(\"\\n1. SUBMIT exp_006 to test calibration hypothesis\")\n",
    "print(\"   - Quick feedback on whether calibration > CV\")\n",
    "print(\"   - Uses 1 of 7 remaining submissions\")\n",
    "\n",
    "print(\"\\n2. NEXT EXPERIMENT: Feature Selection + CatBoost\")\n",
    "print(\"   - Remove 22 low-importance features\")\n",
    "print(\"   - Retrain CatBoost with best params\")\n",
    "print(\"   - Check if reduced feature set improves generalization\")\n",
    "\n",
    "print(\"\\n3. IF exp_006 beats exp_003:\")\n",
    "print(\"   - Calibration matters more than CV\")\n",
    "print(\"   - Focus on approaches that improve calibration\")\n",
    "print(\"   - Try different ensemble weights\")\n",
    "\n",
    "print(\"\\n4. IF exp_006 doesn't beat exp_003:\")\n",
    "print(\"   - CV is more important than calibration\")\n",
    "print(\"   - Focus on feature selection + regularization\")\n",
    "print(\"   - Try to improve CV while maintaining reasonable calibration\")\n",
    "\n",
    "print(\"\\n5. LONG-TERM: Build diverse models for final ensemble\")\n",
    "print(\"   - CatBoost (best single model)\")\n",
    "print(\"   - Feature-selected CatBoost\")\n",
    "print(\"   - Regularized CatBoost\")\n",
    "print(\"   - Maybe Neural Network for diversity\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
