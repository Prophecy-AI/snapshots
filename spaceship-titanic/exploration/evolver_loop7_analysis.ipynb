{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ce8910",
   "metadata": {},
   "source": [
    "# Loop 7 Analysis: Calibration vs CV Trade-off\n",
    "\n",
    "## Key Question:\n",
    "Should we submit exp_006 (weighted ensemble) to test if better calibration compensates for lower CV?\n",
    "\n",
    "## Evaluator's Recommendation:\n",
    "- Submit exp_006 to test calibration hypothesis\n",
    "- Prediction rate 50.9% is closest to training (50.4%)\n",
    "- Even if it doesn't beat exp_003, we learn something valuable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# All experiments with CV, LB, and prediction rates\n",
    "experiments = [\n",
    "    {'exp': 'exp_000', 'cv': 0.80674, 'lb': 0.7971, 'pred_rate': None, 'model': 'XGBoost baseline'},\n",
    "    {'exp': 'exp_003', 'cv': 0.81951, 'lb': 0.8045, 'pred_rate': 0.517, 'model': 'CatBoost Optuna'},\n",
    "    {'exp': 'exp_004', 'cv': 0.81928, 'lb': 0.8041, 'pred_rate': 0.538, 'model': 'CatBoost threshold'},\n",
    "    {'exp': 'exp_006', 'cv': 0.81709, 'lb': None, 'pred_rate': 0.509, 'model': 'Weighted ensemble'},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(experiments)\n",
    "df['gap'] = df['cv'] - df['lb']\n",
    "df['gap_pct'] = (df['gap'] / df['cv']) * 100\n",
    "print(\"Experiment Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nTraining transported rate: 50.36%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae45796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction rate vs LB performance\n",
    "print(\"\\n=== PREDICTION RATE vs LB PERFORMANCE ===\")\n",
    "print(f\"Training rate: 50.36%\")\n",
    "print()\n",
    "submitted = df[df['lb'].notna()].copy()\n",
    "submitted['rate_diff'] = abs(submitted['pred_rate'] - 0.5036) if submitted['pred_rate'].notna().any() else None\n",
    "print(submitted[['exp', 'pred_rate', 'lb', 'cv', 'gap']].to_string(index=False))\n",
    "\n",
    "# Pattern analysis\n",
    "print(\"\\n=== PATTERN ANALYSIS ===\")\n",
    "print(\"exp_003: pred_rate=51.7% (diff=1.34%) -> LB=0.8045 (BEST)\")\n",
    "print(\"exp_004: pred_rate=53.8% (diff=3.44%) -> LB=0.8041 (worse)\")\n",
    "print(\"exp_006: pred_rate=50.9% (diff=0.54%) -> LB=??? (closest to training!)\")\n",
    "print(\"\\nHypothesis: Closer to training rate = better LB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict LB for exp_006 using different methods\n",
    "print(\"\\n=== LB PREDICTION FOR exp_006 ===\")\n",
    "\n",
    "# Method 1: Using average CV-LB gap\n",
    "mean_gap = df[df['lb'].notna()]['gap'].mean()\n",
    "pred_lb_gap = 0.81709 - mean_gap\n",
    "print(f\"Method 1 (avg gap {mean_gap:.4f}): LB = {pred_lb_gap:.4f}\")\n",
    "\n",
    "# Method 2: Using recent gap (exp_004)\n",
    "recent_gap = 0.81928 - 0.8041\n",
    "pred_lb_recent = 0.81709 - recent_gap\n",
    "print(f\"Method 2 (recent gap {recent_gap:.4f}): LB = {pred_lb_recent:.4f}\")\n",
    "\n",
    "# Method 3: Calibration-adjusted (hypothesis: better calibration reduces gap)\n",
    "# exp_003 gap was 1.50%, exp_004 gap was 1.86% (worse calibration = bigger gap)\n",
    "# exp_006 has best calibration, so gap might be smaller\n",
    "calibration_adjusted_gap = 0.012  # Conservative estimate\n",
    "pred_lb_calibration = 0.81709 - calibration_adjusted_gap\n",
    "print(f\"Method 3 (calibration-adjusted gap {calibration_adjusted_gap:.4f}): LB = {pred_lb_calibration:.4f}\")\n",
    "\n",
    "print(f\"\\nBest LB so far (exp_003): 0.8045\")\n",
    "print(f\"\\nRange of predictions: {pred_lb_recent:.4f} - {pred_lb_calibration:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8315de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision analysis\n",
    "print(\"\\n=== DECISION ANALYSIS ===\")\n",
    "print(\"\\nArguments FOR submitting exp_006:\")\n",
    "print(\"1. Prediction rate (50.9%) is closest to training (50.4%)\")\n",
    "print(\"2. We have evidence that prediction rate matters (exp_004 failure)\")\n",
    "print(\"3. We have 7 submissions remaining - plenty of room to test\")\n",
    "print(\"4. Even if it fails, we learn about calibration vs CV trade-off\")\n",
    "print(\"5. Evaluator recommends it\")\n",
    "\n",
    "print(\"\\nArguments AGAINST submitting exp_006:\")\n",
    "print(\"1. CV (0.81709) is lower than exp_003 (0.81951) by 0.24%\")\n",
    "print(\"2. If CV-LB gap is constant, this predicts worse LB\")\n",
    "print(\"3. Could waste a submission\")\n",
    "\n",
    "print(\"\\nMy assessment:\")\n",
    "print(\"- 40% chance exp_006 beats exp_003 (calibration hypothesis)\")\n",
    "print(\"- 60% chance exp_006 underperforms (CV dominates)\")\n",
    "print(\"- Either way, we learn something valuable\")\n",
    "print(\"- With 7 submissions remaining, this is a good use of quota\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc22677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What else should we try?\n",
    "print(\"\\n=== UNEXPLORED APPROACHES ===\")\n",
    "print(\"\\n1. FEATURE SELECTION (high priority)\")\n",
    "print(\"   - 22 features have importance < 1.0\")\n",
    "print(\"   - Could reduce overfitting\")\n",
    "print(\"   - Not yet tried\")\n",
    "\n",
    "print(\"\\n2. REGULARIZED CATBOOST\")\n",
    "print(\"   - depth=6 (vs 8), l2_leaf_reg=5.0 (vs 3.52)\")\n",
    "print(\"   - subsample=0.8 for randomness\")\n",
    "print(\"   - Partially tried in loop 5 analysis but not as full experiment\")\n",
    "\n",
    "print(\"\\n3. DIFFERENT ENSEMBLE WEIGHTS\")\n",
    "print(\"   - Current: 0.6*CatBoost + 0.2*XGB + 0.2*LGB\")\n",
    "print(\"   - Could try: 0.7*CatBoost + 0.15*XGB + 0.15*LGB\")\n",
    "print(\"   - Or: 0.5*CatBoost + 0.25*XGB + 0.25*LGB\")\n",
    "\n",
    "print(\"\\n4. NEURAL NETWORK\")\n",
    "print(\"   - Not tried at all\")\n",
    "print(\"   - Could add diversity for ensembling\")\n",
    "print(\"   - TabNet or simple MLP\")\n",
    "\n",
    "print(\"\\n5. PSEUDO-LABELING\")\n",
    "print(\"   - Use confident predictions on test set\")\n",
    "print(\"   - Retrain with pseudo-labels\")\n",
    "print(\"   - Risky but could help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy recommendation\n",
    "print(\"\\n=== STRATEGY RECOMMENDATION ===\")\n",
    "print(\"\\n1. SUBMIT exp_006 to test calibration hypothesis\")\n",
    "print(\"   - Quick feedback on whether calibration > CV\")\n",
    "print(\"   - Uses 1 of 7 remaining submissions\")\n",
    "\n",
    "print(\"\\n2. NEXT EXPERIMENT: Feature Selection + CatBoost\")\n",
    "print(\"   - Remove 22 low-importance features\")\n",
    "print(\"   - Retrain CatBoost with best params\")\n",
    "print(\"   - Check if reduced feature set improves generalization\")\n",
    "\n",
    "print(\"\\n3. IF exp_006 beats exp_003:\")\n",
    "print(\"   - Calibration matters more than CV\")\n",
    "print(\"   - Focus on approaches that improve calibration\")\n",
    "print(\"   - Try different ensemble weights\")\n",
    "\n",
    "print(\"\\n4. IF exp_006 doesn't beat exp_003:\")\n",
    "print(\"   - CV is more important than calibration\")\n",
    "print(\"   - Focus on feature selection + regularization\")\n",
    "print(\"   - Try to improve CV while maintaining reasonable calibration\")\n",
    "\n",
    "print(\"\\n5. LONG-TERM: Build diverse models for final ensemble\")\n",
    "print(\"   - CatBoost (best single model)\")\n",
    "print(\"   - Feature-selected CatBoost\")\n",
    "print(\"   - Regularized CatBoost\")\n",
    "print(\"   - Maybe Neural Network for diversity\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
