{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d3a0a3",
   "metadata": {},
   "source": [
    "# Loop 4 LB Feedback Analysis\n",
    "\n",
    "## Submission Results\n",
    "- exp_003 (Tuned CatBoost): CV 0.8195 → LB 0.8045 (gap: +0.0150)\n",
    "- exp_000 (XGBoost Baseline): CV 0.8067 → LB 0.7971 (gap: +0.0097)\n",
    "\n",
    "## Key Questions\n",
    "1. Why did the CV-LB gap increase from 0.97% to 1.50%?\n",
    "2. What approaches can reduce this gap?\n",
    "3. What's the best path to beat top LB (~0.8066)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3595cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:37:39.167409Z",
     "iopub.status.busy": "2026-01-06T04:37:39.167067Z",
     "iopub.status.idle": "2026-01-06T04:37:40.085640Z",
     "shell.execute_reply": "2026-01-06T04:37:40.084820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-LB Gap Analysis:\n",
      "    exp            model      cv      lb     gap  gap_pct\n",
      "exp_000 XGBoost Baseline 0.80674 0.79705 0.00969 1.201130\n",
      "exp_003   Tuned CatBoost 0.81951 0.80453 0.01498 1.827922\n",
      "\n",
      "Average gap: 0.01233 (1.51%)\n",
      "\n",
      "Observation: Gap increased from 0.00969 to 0.01498\n",
      "This suggests the tuned model is slightly overfitting to CV folds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CV-LB Gap Analysis\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'model': 'XGBoost Baseline', 'cv': 0.80674, 'lb': 0.79705},\n",
    "    {'exp': 'exp_003', 'model': 'Tuned CatBoost', 'cv': 0.81951, 'lb': 0.80453}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "df['gap'] = df['cv'] - df['lb']\n",
    "df['gap_pct'] = df['gap'] / df['cv'] * 100\n",
    "\n",
    "print(\"CV-LB Gap Analysis:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nAverage gap: {df['gap'].mean():.5f} ({df['gap_pct'].mean():.2f}%)\")\n",
    "print(f\"\\nObservation: Gap increased from {df.iloc[0]['gap']:.5f} to {df.iloc[1]['gap']:.5f}\")\n",
    "print(f\"This suggests the tuned model is slightly overfitting to CV folds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3eb5307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:37:40.088251Z",
     "iopub.status.busy": "2026-01-06T04:37:40.087857Z",
     "iopub.status.idle": "2026-01-06T04:37:40.094868Z",
     "shell.execute_reply": "2026-01-06T04:37:40.094166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LB Prediction Calibration:\n",
      "==================================================\n",
      "Using average gap of 0.01233:\n",
      "  CV 0.820 → Predicted LB 0.8077\n",
      "  CV 0.825 → Predicted LB 0.8127\n",
      "  CV 0.830 → Predicted LB 0.8177\n",
      "\n",
      "Using conservative gap of 0.01498 (from tuned model):\n",
      "  CV 0.820 → Predicted LB 0.8050\n",
      "  CV 0.825 → Predicted LB 0.8100\n",
      "  CV 0.830 → Predicted LB 0.8150\n",
      "\n",
      "To beat top LB of ~0.8066, we need:\n",
      "  - CV of 0.8189 (using avg gap)\n",
      "  - CV of 0.8216 (using conservative gap)\n"
     ]
    }
   ],
   "source": [
    "# What would different CV scores predict for LB?\n",
    "print(\"\\nLB Prediction Calibration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Using average gap\n",
    "avg_gap = df['gap'].mean()\n",
    "print(f\"Using average gap of {avg_gap:.5f}:\")\n",
    "for cv in [0.82, 0.825, 0.83]:\n",
    "    predicted_lb = cv - avg_gap\n",
    "    print(f\"  CV {cv:.3f} → Predicted LB {predicted_lb:.4f}\")\n",
    "\n",
    "# Using conservative gap (from tuned model)\n",
    "conservative_gap = df.iloc[1]['gap']\n",
    "print(f\"\\nUsing conservative gap of {conservative_gap:.5f} (from tuned model):\")\n",
    "for cv in [0.82, 0.825, 0.83]:\n",
    "    predicted_lb = cv - conservative_gap\n",
    "    print(f\"  CV {cv:.3f} → Predicted LB {predicted_lb:.4f}\")\n",
    "\n",
    "print(f\"\\nTo beat top LB of ~0.8066, we need:\")\n",
    "print(f\"  - CV of {0.8066 + avg_gap:.4f} (using avg gap)\")\n",
    "print(f\"  - CV of {0.8066 + conservative_gap:.4f} (using conservative gap)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3177b4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:37:40.097099Z",
     "iopub.status.busy": "2026-01-06T04:37:40.096841Z",
     "iopub.status.idle": "2026-01-06T04:37:40.105369Z",
     "shell.execute_reply": "2026-01-06T04:37:40.104753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Trajectory Analysis:\n",
      "==================================================\n",
      "\n",
      "CV Improvements:\n",
      "  exp_000 → exp_001: +0.00253 (+0.31%)\n",
      "  exp_001 → exp_002: +0.00426 (+0.53%)\n",
      "  exp_002 → exp_003: +0.00598 (+0.74%)\n",
      "\n",
      "LB Improvement:\n",
      "  exp_000 → exp_003: +0.00748 (+0.94%)\n",
      "\n",
      "Key insight: LB improved by 0.0075 while CV improved by 0.0128\n",
      "LB improvement rate: 58.6% of CV improvement\n"
     ]
    }
   ],
   "source": [
    "# Analyze what's working and what's not\n",
    "print(\"\\nExperiment Trajectory Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "experiments = [\n",
    "    {'exp': 'exp_000', 'model': 'XGBoost Baseline', 'cv': 0.80674, 'lb': 0.79705, 'features': 35},\n",
    "    {'exp': 'exp_001', 'model': 'XGBoost + Features', 'cv': 0.80927, 'lb': None, 'features': 56},\n",
    "    {'exp': 'exp_002', 'model': '3-Model Ensemble', 'cv': 0.81353, 'lb': None, 'features': 56},\n",
    "    {'exp': 'exp_003', 'model': 'Tuned CatBoost', 'cv': 0.81951, 'lb': 0.80453, 'features': 56}\n",
    "]\n",
    "\n",
    "print(\"\\nCV Improvements:\")\n",
    "for i in range(1, len(experiments)):\n",
    "    prev = experiments[i-1]\n",
    "    curr = experiments[i]\n",
    "    cv_delta = curr['cv'] - prev['cv']\n",
    "    print(f\"  {prev['exp']} → {curr['exp']}: {cv_delta:+.5f} ({cv_delta/prev['cv']*100:+.2f}%)\")\n",
    "\n",
    "print(\"\\nLB Improvement:\")\n",
    "lb_delta = 0.80453 - 0.79705\n",
    "print(f\"  exp_000 → exp_003: {lb_delta:+.5f} ({lb_delta/0.79705*100:+.2f}%)\")\n",
    "print(f\"\\nKey insight: LB improved by {lb_delta:.4f} while CV improved by {0.81951-0.80674:.4f}\")\n",
    "print(f\"LB improvement rate: {lb_delta/(0.81951-0.80674)*100:.1f}% of CV improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957000bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:37:40.107823Z",
     "iopub.status.busy": "2026-01-06T04:37:40.107227Z",
     "iopub.status.idle": "2026-01-06T04:37:40.114795Z",
     "shell.execute_reply": "2026-01-06T04:37:40.114138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unexplored Approaches:\n",
      "==================================================\n",
      "Approach                       Rationale                                          Priority\n",
      "------------------------------------------------------------------------------------------\n",
      "Threshold tuning               Quick win, default 0.5 may not be optimal          High\n",
      "CatBoost native categoricals   Use cat_features instead of label encoding         Medium\n",
      "Feature selection              Remove low-importance features (56 may have noise) Medium\n",
      "Stacking with meta-learner     Use OOF predictions as features for LR             Medium\n",
      "Blend baseline + tuned         Average to reduce variance                         Low\n",
      "Pseudo-labeling                Use high-confidence test predictions               Low\n",
      "Different CV strategy          Use different seeds to reduce fold overfitting     Medium\n"
     ]
    }
   ],
   "source": [
    "# Unexplored approaches analysis\n",
    "print(\"\\nUnexplored Approaches:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "approaches = [\n",
    "    ('Threshold tuning', 'Quick win, default 0.5 may not be optimal', 'High'),\n",
    "    ('CatBoost native categoricals', 'Use cat_features instead of label encoding', 'Medium'),\n",
    "    ('Feature selection', 'Remove low-importance features (56 may have noise)', 'Medium'),\n",
    "    ('Stacking with meta-learner', 'Use OOF predictions as features for LR', 'Medium'),\n",
    "    ('Blend baseline + tuned', 'Average to reduce variance', 'Low'),\n",
    "    ('Pseudo-labeling', 'Use high-confidence test predictions', 'Low'),\n",
    "    ('Different CV strategy', 'Use different seeds to reduce fold overfitting', 'Medium')\n",
    "]\n",
    "\n",
    "print(f\"{'Approach':<30} {'Rationale':<50} {'Priority'}\")\n",
    "print(\"-\"*90)\n",
    "for approach, rationale, priority in approaches:\n",
    "    print(f\"{approach:<30} {rationale:<50} {priority}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c12fc81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:37:40.117044Z",
     "iopub.status.busy": "2026-01-06T04:37:40.116767Z",
     "iopub.status.idle": "2026-01-06T04:37:40.124569Z",
     "shell.execute_reply": "2026-01-06T04:37:40.123823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PRIORITY RECOMMENDATIONS FOR NEXT LOOP\n",
      "============================================================\n",
      "\n",
      "1. THRESHOLD TUNING (Immediate)\n",
      "   - We have OOF predictions from tuned CatBoost\n",
      "   - Default threshold of 0.5 may not be optimal\n",
      "   - Target distribution is 50.36% transported\n",
      "   - Potential gain: 0.1-0.3% on CV, may translate to LB\n",
      "\n",
      "2. CATBOOST NATIVE CATEGORICAL HANDLING (High Priority)\n",
      "   - Currently using label encoding for all categoricals\n",
      "   - CatBoost's cat_features parameter can improve performance\n",
      "   - This is a known best practice we haven't tried\n",
      "   - Potential gain: 0.1-0.5%\n",
      "\n",
      "3. BLEND BASELINE + TUNED CATBOOST (Medium Priority)\n",
      "   - Baseline CatBoost: CV 0.81836, std 0.00431 (lower variance)\n",
      "   - Tuned CatBoost: CV 0.81951, std 0.00685 (higher variance)\n",
      "   - Blending may reduce the CV-LB gap\n",
      "   - Potential gain: Better LB generalization\n",
      "\n",
      "4. STACKING WITH META-LEARNER (Medium Priority)\n",
      "   - Use OOF predictions from XGBoost, LightGBM, CatBoost\n",
      "   - Train logistic regression on OOF predictions\n",
      "   - May capture complementary patterns\n",
      "   - Potential gain: 0.2-0.5%\n",
      "\n",
      "5. FEATURE SELECTION (Lower Priority)\n",
      "   - 56 features may include noise\n",
      "   - Try removing bottom 10-20% by importance\n",
      "   - May reduce overfitting and improve LB\n",
      "\n",
      "\n",
      "Target: Beat LB 0.8066 (top solutions)\n",
      "Current best LB: 0.8045 (gap to target: 0.0021)\n",
      "Remaining submissions: 8\n"
     ]
    }
   ],
   "source": [
    "# Priority recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRIORITY RECOMMENDATIONS FOR NEXT LOOP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. THRESHOLD TUNING (Immediate)\n",
    "   - We have OOF predictions from tuned CatBoost\n",
    "   - Default threshold of 0.5 may not be optimal\n",
    "   - Target distribution is 50.36% transported\n",
    "   - Potential gain: 0.1-0.3% on CV, may translate to LB\n",
    "\n",
    "2. CATBOOST NATIVE CATEGORICAL HANDLING (High Priority)\n",
    "   - Currently using label encoding for all categoricals\n",
    "   - CatBoost's cat_features parameter can improve performance\n",
    "   - This is a known best practice we haven't tried\n",
    "   - Potential gain: 0.1-0.5%\n",
    "\n",
    "3. BLEND BASELINE + TUNED CATBOOST (Medium Priority)\n",
    "   - Baseline CatBoost: CV 0.81836, std 0.00431 (lower variance)\n",
    "   - Tuned CatBoost: CV 0.81951, std 0.00685 (higher variance)\n",
    "   - Blending may reduce the CV-LB gap\n",
    "   - Potential gain: Better LB generalization\n",
    "\n",
    "4. STACKING WITH META-LEARNER (Medium Priority)\n",
    "   - Use OOF predictions from XGBoost, LightGBM, CatBoost\n",
    "   - Train logistic regression on OOF predictions\n",
    "   - May capture complementary patterns\n",
    "   - Potential gain: 0.2-0.5%\n",
    "\n",
    "5. FEATURE SELECTION (Lower Priority)\n",
    "   - 56 features may include noise\n",
    "   - Try removing bottom 10-20% by importance\n",
    "   - May reduce overfitting and improve LB\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTarget: Beat LB 0.8066 (top solutions)\")\n",
    "print(f\"Current best LB: 0.8045 (gap to target: {0.8066-0.8045:.4f})\")\n",
    "print(f\"Remaining submissions: 8\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
