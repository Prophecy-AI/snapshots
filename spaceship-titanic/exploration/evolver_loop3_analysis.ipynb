{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e418480",
   "metadata": {},
   "source": [
    "# Loop 3 Analysis: CatBoost vs Ensemble Performance\n",
    "\n",
    "## Key Finding from Evaluator\n",
    "The evaluator correctly identified that CatBoost alone (0.81836) outperforms the simple averaging ensemble (0.81353) by 0.48%.\n",
    "\n",
    "## Analysis Goals\n",
    "1. Verify CatBoost superiority\n",
    "2. Explore weighted ensemble options\n",
    "3. Analyze threshold tuning potential\n",
    "4. Plan next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a897e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:20:39.426609Z",
     "iopub.status.busy": "2026-01-06T04:20:39.426243Z",
     "iopub.status.idle": "2026-01-06T04:20:39.984265Z",
     "shell.execute_reply": "2026-01-06T04:20:39.983536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Comparison:\n",
      "              Model  CV_Score  CV_Std  Rank\n",
      "            XGBoost   0.80927 0.00656   3.0\n",
      "           LightGBM   0.80743 0.00612   4.0\n",
      "           CatBoost   0.81836 0.00431   1.0\n",
      "Simple Avg Ensemble   0.81353     NaN   2.0\n",
      "\n",
      "CatBoost vs Ensemble: 0.00483 = +0.48% improvement\n",
      "CatBoost vs XGBoost: 0.00909 = +0.91% improvement\n"
     ]
    }
   ],
   "source": [
    "# Analyze the experiment results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Results from exp_002 (3-model ensemble)\n",
    "results = {\n",
    "    'Model': ['XGBoost', 'LightGBM', 'CatBoost', 'Simple Avg Ensemble'],\n",
    "    'CV_Score': [0.80927, 0.80743, 0.81836, 0.81353],\n",
    "    'CV_Std': [0.00656, 0.00612, 0.00431, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['Rank'] = df['CV_Score'].rank(ascending=False)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nCatBoost vs Ensemble: {0.81836 - 0.81353:.5f} = +0.48% improvement\")\n",
    "print(f\"CatBoost vs XGBoost: {0.81836 - 0.80927:.5f} = +0.91% improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b822ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:20:39.987150Z",
     "iopub.status.busy": "2026-01-06T04:20:39.986375Z",
     "iopub.status.idle": "2026-01-06T04:20:39.993785Z",
     "shell.execute_reply": "2026-01-06T04:20:39.993122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Weighted Ensemble Scores (approximation):\n",
      "============================================================\n",
      "Equal (1/3, 1/3, 1/3): 0.81169\n",
      "CatBoost heavy (0.6, 0.2, 0.2): 0.81436\n",
      "CatBoost heavier (0.7, 0.15, 0.15): 0.81536\n",
      "CatBoost only (0, 0, 1): 0.81836\n",
      "Drop LightGBM (0.3, 0, 0.7): 0.81563\n",
      "XGB+Cat only (0.3, 0, 0.7): 0.81563\n"
     ]
    }
   ],
   "source": [
    "# Why is simple averaging worse than CatBoost?\n",
    "# When one model is significantly better, equal weighting drags it down\n",
    "\n",
    "# Let's simulate different weighting schemes\n",
    "def simulate_weighted_ensemble(weights, scores):\n",
    "    \"\"\"Simulate weighted ensemble score (approximation)\"\"\"\n",
    "    # This is a rough approximation - actual ensemble would need OOF predictions\n",
    "    return sum(w * s for w, s in zip(weights, scores))\n",
    "\n",
    "xgb_score = 0.80927\n",
    "lgb_score = 0.80743\n",
    "cat_score = 0.81836\n",
    "\n",
    "print(\"Simulated Weighted Ensemble Scores (approximation):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Different weighting schemes\n",
    "schemes = [\n",
    "    ('Equal (1/3, 1/3, 1/3)', [1/3, 1/3, 1/3]),\n",
    "    ('CatBoost heavy (0.6, 0.2, 0.2)', [0.2, 0.2, 0.6]),\n",
    "    ('CatBoost heavier (0.7, 0.15, 0.15)', [0.15, 0.15, 0.7]),\n",
    "    ('CatBoost only (0, 0, 1)', [0, 0, 1]),\n",
    "    ('Drop LightGBM (0.3, 0, 0.7)', [0.3, 0, 0.7]),\n",
    "    ('XGB+Cat only (0.3, 0, 0.7)', [0.3, 0, 0.7]),\n",
    "]\n",
    "\n",
    "for name, weights in schemes:\n",
    "    score = simulate_weighted_ensemble(weights, [xgb_score, lgb_score, cat_score])\n",
    "    print(f\"{name}: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f2f790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:20:39.995845Z",
     "iopub.status.busy": "2026-01-06T04:20:39.995622Z",
     "iopub.status.idle": "2026-01-06T04:20:40.003475Z",
     "shell.execute_reply": "2026-01-06T04:20:40.002805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-LB Gap Analysis:\n",
      "============================================================\n",
      "exp_000: CV=0.80674, LB=0.79705, Gap=0.00969 (1.2% overestimate)\n",
      "\n",
      "Predicted LB scores (assuming 0.00969 gap):\n",
      "  CatBoost (CV=0.81836): Predicted LB ≈ 0.80867\n",
      "  Ensemble (CV=0.81353): Predicted LB ≈ 0.80384\n",
      "  XGBoost (CV=0.80927): Predicted LB ≈ 0.79958\n",
      "\n",
      "Top LB scores in competition: ~0.8066\n",
      "Our CatBoost predicted LB: 0.80867\n",
      "Difference from top: 0.00207\n"
     ]
    }
   ],
   "source": [
    "# CV-LB Gap Analysis\n",
    "print(\"CV-LB Gap Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# From exp_000 submission\n",
    "cv_exp000 = 0.80674\n",
    "lb_exp000 = 0.79705\n",
    "gap = cv_exp000 - lb_exp000\n",
    "\n",
    "print(f\"exp_000: CV={cv_exp000:.5f}, LB={lb_exp000:.5f}, Gap={gap:.5f} ({gap/cv_exp000*100:.1f}% overestimate)\")\n",
    "\n",
    "# Predicted LB scores\n",
    "print(f\"\\nPredicted LB scores (assuming {gap:.5f} gap):\")\n",
    "print(f\"  CatBoost (CV=0.81836): Predicted LB ≈ {0.81836 - gap:.5f}\")\n",
    "print(f\"  Ensemble (CV=0.81353): Predicted LB ≈ {0.81353 - gap:.5f}\")\n",
    "print(f\"  XGBoost (CV=0.80927): Predicted LB ≈ {0.80927 - gap:.5f}\")\n",
    "\n",
    "print(f\"\\nTop LB scores in competition: ~0.8066\")\n",
    "print(f\"Our CatBoost predicted LB: {0.81836 - gap:.5f}\")\n",
    "print(f\"Difference from top: {(0.81836 - gap) - 0.8066:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f748bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:20:40.005439Z",
     "iopub.status.busy": "2026-01-06T04:20:40.005204Z",
     "iopub.status.idle": "2026-01-06T04:20:40.013148Z",
     "shell.execute_reply": "2026-01-06T04:20:40.012502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Score Reality Check:\n",
      "============================================================\n",
      "Target score: 0.9642 (96.42% accuracy)\n",
      "Top LB scores: ~0.8066 (80.66% accuracy)\n",
      "Our best CV: 0.81836 (81.84% accuracy)\n",
      "\n",
      "The target of 0.9642 is UNREALISTIC for this competition.\n",
      "Top solutions achieve ~80.7% accuracy.\n",
      "Our CatBoost CV of 0.81836 is EXCELLENT - likely top 5% territory.\n",
      "\n",
      "Focus should be on:\n",
      "1. Maximizing CV score (currently 0.81836)\n",
      "2. Ensuring good CV-LB correlation\n",
      "3. Submitting best candidate to verify LB performance\n"
     ]
    }
   ],
   "source": [
    "# Target Score Reality Check\n",
    "print(\"Target Score Reality Check:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Target score: 0.9642 (96.42% accuracy)\")\n",
    "print(f\"Top LB scores: ~0.8066 (80.66% accuracy)\")\n",
    "print(f\"Our best CV: 0.81836 (81.84% accuracy)\")\n",
    "print(f\"\\nThe target of 0.9642 is UNREALISTIC for this competition.\")\n",
    "print(f\"Top solutions achieve ~80.7% accuracy.\")\n",
    "print(f\"Our CatBoost CV of 0.81836 is EXCELLENT - likely top 5% territory.\")\n",
    "print(f\"\\nFocus should be on:\")\n",
    "print(f\"1. Maximizing CV score (currently 0.81836)\")\n",
    "print(f\"2. Ensuring good CV-LB correlation\")\n",
    "print(f\"3. Submitting best candidate to verify LB performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf86ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:20:40.015397Z",
     "iopub.status.busy": "2026-01-06T04:20:40.014850Z",
     "iopub.status.idle": "2026-01-06T04:20:40.022063Z",
     "shell.execute_reply": "2026-01-06T04:20:40.021454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategic Analysis - Next Steps:\n",
      "============================================================\n",
      "\n",
      "1. IMMEDIATE WIN: Submit CatBoost-only predictions\n",
      "   - CatBoost CV: 0.81836 (best individual model)\n",
      "   - Predicted LB: ~0.8087\n",
      "   - No additional work needed - just use CatBoost predictions\n",
      "\n",
      "2. POTENTIAL IMPROVEMENTS:\n",
      "   a) Tune CatBoost hyperparameters (current: depth=6, lr=0.05)\n",
      "      - Try Optuna optimization for CatBoost specifically\n",
      "      - Potential gain: 0.2-0.5%\n",
      "   b) Weighted ensemble (0.7 CatBoost + 0.15 XGB + 0.15 LGB)\n",
      "      - May not beat CatBoost alone given performance gap\n",
      "   c) Threshold tuning\n",
      "      - Current: 0.5 default\n",
      "      - Optimize to match training distribution (~50.4%)\n",
      "   d) Additional feature engineering\n",
      "      - Name-based features (surname clustering)\n",
      "      - More interaction terms\n",
      "\n",
      "3. WHAT NOT TO TRY:\n",
      "   - Simple averaging ensemble (already proven worse)\n",
      "   - LightGBM focus (underperforms both XGB and CatBoost)\n",
      "   - Neural networks (unlikely to beat GBMs on this tabular data)\n"
     ]
    }
   ],
   "source": [
    "# Next Steps Analysis\n",
    "print(\"Strategic Analysis - Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. IMMEDIATE WIN: Submit CatBoost-only predictions\")\n",
    "print(\"   - CatBoost CV: 0.81836 (best individual model)\")\n",
    "print(\"   - Predicted LB: ~0.8087\")\n",
    "print(\"   - No additional work needed - just use CatBoost predictions\")\n",
    "\n",
    "print(\"\\n2. POTENTIAL IMPROVEMENTS:\")\n",
    "print(\"   a) Tune CatBoost hyperparameters (current: depth=6, lr=0.05)\")\n",
    "print(\"      - Try Optuna optimization for CatBoost specifically\")\n",
    "print(\"      - Potential gain: 0.2-0.5%\")\n",
    "print(\"   b) Weighted ensemble (0.7 CatBoost + 0.15 XGB + 0.15 LGB)\")\n",
    "print(\"      - May not beat CatBoost alone given performance gap\")\n",
    "print(\"   c) Threshold tuning\")\n",
    "print(\"      - Current: 0.5 default\")\n",
    "print(\"      - Optimize to match training distribution (~50.4%)\")\n",
    "print(\"   d) Additional feature engineering\")\n",
    "print(\"      - Name-based features (surname clustering)\")\n",
    "print(\"      - More interaction terms\")\n",
    "\n",
    "print(\"\\n3. WHAT NOT TO TRY:\")\n",
    "print(\"   - Simple averaging ensemble (already proven worse)\")\n",
    "print(\"   - LightGBM focus (underperforms both XGB and CatBoost)\")\n",
    "print(\"   - Neural networks (unlikely to beat GBMs on this tabular data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e3aa2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:20:40.023956Z",
     "iopub.status.busy": "2026-01-06T04:20:40.023750Z",
     "iopub.status.idle": "2026-01-06T04:20:40.030217Z",
     "shell.execute_reply": "2026-01-06T04:20:40.029602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY OF KEY FINDINGS\n",
      "============================================================\n",
      "1. CatBoost (0.81836) is the BEST model, beating ensemble (0.81353) by 0.48%\n",
      "2. Simple averaging hurts when one model dominates\n",
      "3. CatBoost has lowest variance (std=0.00431) - most stable\n",
      "4. CV-LB gap is ~0.97% (CV overestimates LB)\n",
      "5. Target of 0.9642 is unrealistic - top LB is ~0.8066\n",
      "6. Our CatBoost CV of 0.81836 is competitive with top solutions\n",
      "\n",
      "RECOMMENDED ACTION:\n",
      "Create CatBoost-only submission and submit to verify LB performance\n"
     ]
    }
   ],
   "source": [
    "# Summary of key findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "findings = [\n",
    "    \"1. CatBoost (0.81836) is the BEST model, beating ensemble (0.81353) by 0.48%\",\n",
    "    \"2. Simple averaging hurts when one model dominates\",\n",
    "    \"3. CatBoost has lowest variance (std=0.00431) - most stable\",\n",
    "    \"4. CV-LB gap is ~0.97% (CV overestimates LB)\",\n",
    "    \"5. Target of 0.9642 is unrealistic - top LB is ~0.8066\",\n",
    "    \"6. Our CatBoost CV of 0.81836 is competitive with top solutions\",\n",
    "]\n",
    "\n",
    "for f in findings:\n",
    "    print(f)\n",
    "\n",
    "print(\"\\nRECOMMENDED ACTION:\")\n",
    "print(\"Create CatBoost-only submission and submit to verify LB performance\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
