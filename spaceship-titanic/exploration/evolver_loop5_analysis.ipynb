{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a59809e",
   "metadata": {},
   "source": [
    "# Loop 5 Analysis: CV Instability and Overfitting Investigation\n",
    "\n",
    "## Key Questions:\n",
    "1. Why did same hyperparameters give different CV scores (0.81951 vs 0.81617)?\n",
    "2. Is threshold tuning overfitting to OOF predictions?\n",
    "3. What approaches might reduce CV-LB gap?\n",
    "\n",
    "## Evaluator Concerns to Address:\n",
    "- CV score discrepancy between experiments\n",
    "- Threshold 0.47 shifts predicted distribution significantly\n",
    "- CV-LB gap is increasing (0.97% → 1.50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd129224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:46:43.868713Z",
     "iopub.status.busy": "2026-01-06T04:46:43.867996Z",
     "iopub.status.idle": "2026-01-06T04:46:45.129671Z",
     "shell.execute_reply": "2026-01-06T04:46:45.128714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (8693, 14), Test: (4277, 13)\n",
      "Training transported rate: 0.5036\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
    "print(f\"Training transported rate: {train['Transported'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ad6576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:46:45.132792Z",
     "iopub.status.busy": "2026-01-06T04:46:45.132101Z",
     "iopub.status.idle": "2026-01-06T04:46:45.210850Z",
     "shell.execute_reply": "2026-01-06T04:46:45.210110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic features done\n"
     ]
    }
   ],
   "source": [
    "# Quick feature engineering (same as experiments)\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    df['Group'] = df['PassengerId'].apply(lambda x: int(x.split('_')[0]))\n",
    "    df['PassengerNum'] = df['PassengerId'].apply(lambda x: int(x.split('_')[1]))\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x.split('/')[0] if pd.notna(x) else 'Unknown')\n",
    "    df['CabinNum'] = df['Cabin'].apply(lambda x: int(x.split('/')[1]) if pd.notna(x) else np.nan)\n",
    "    df['Side'] = df['Cabin'].apply(lambda x: x.split('/')[2] if pd.notna(x) else 'Unknown')\n",
    "    return df\n",
    "\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "# GroupSize\n",
    "all_data = pd.concat([train[['Group']], test[['Group']]], ignore_index=True)\n",
    "group_sizes = all_data['Group'].value_counts().to_dict()\n",
    "for df in [train, test]:\n",
    "    df['GroupSize'] = df['Group'].map(group_sizes)\n",
    "    df['Solo'] = (df['GroupSize'] == 1).astype(int)\n",
    "\n",
    "print(\"Basic features done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ce73cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:46:45.213415Z",
     "iopub.status.busy": "2026-01-06T04:46:45.212722Z",
     "iopub.status.idle": "2026-01-06T04:46:51.158555Z",
     "shell.execute_reply": "2026-01-06T04:46:51.157789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation done\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "def group_based_imputation(train_df, test_df):\n",
    "    combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    for col in ['HomePlanet', 'Deck', 'Side']:\n",
    "        group_mode = combined.groupby('Group')[col].apply(\n",
    "            lambda x: x.mode()[0] if len(x.mode()) > 0 and x.mode()[0] != 'Unknown' else 'Unknown'\n",
    "        ).to_dict()\n",
    "        for df in [train_df, test_df]:\n",
    "            mask = (df[col].isna()) | (df[col] == 'Unknown')\n",
    "            df.loc[mask, col] = df.loc[mask, 'Group'].map(group_mode)\n",
    "    return train_df, test_df\n",
    "\n",
    "train, test = group_based_imputation(train, test)\n",
    "\n",
    "def impute_remaining(df):\n",
    "    df = df.copy()\n",
    "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    for col in spending_cols:\n",
    "        mask = (df['CryoSleep'] == True) & (df[col].isna())\n",
    "        df.loc[mask, col] = 0\n",
    "    for col in ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']:\n",
    "        if df[col].isna().any() or (df[col] == 'Unknown').any():\n",
    "            mode_val = df[col].replace('Unknown', np.nan).mode()[0]\n",
    "            df[col] = df[col].replace('Unknown', mode_val)\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "    for col in ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'CabinNum']:\n",
    "        if df[col].isna().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "train = impute_remaining(train)\n",
    "test = impute_remaining(test)\n",
    "print(\"Imputation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ce815d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:46:51.161590Z",
     "iopub.status.busy": "2026-01-06T04:46:51.160983Z",
     "iopub.status.idle": "2026-01-06T04:46:51.213777Z",
     "shell.execute_reply": "2026-01-06T04:46:51.213041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features created\n"
     ]
    }
   ],
   "source": [
    "# Create features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    df['TotalSpent'] = df[spending_cols].sum(axis=1)\n",
    "    for col in spending_cols:\n",
    "        df[f'{col}_ratio'] = df[col] / (df['TotalSpent'] + 1)\n",
    "        df[f'{col}_spent'] = (df[col] > 0).astype(int)\n",
    "        df[f'{col}_log'] = np.log1p(df[col])\n",
    "    df['LuxurySpent'] = df['Spa'] + df['VRDeck'] + df['RoomService']\n",
    "    df['BasicSpent'] = df['FoodCourt'] + df['ShoppingMall']\n",
    "    df['LuxuryRatio'] = df['LuxurySpent'] / (df['TotalSpent'] + 1)\n",
    "    df['SpentPerAge'] = df['TotalSpent'] / (df['Age'] + 1)\n",
    "    df['SpendingBin'] = pd.cut(df['TotalSpent'], bins=[-1, 0, 500, 2000, float('inf')], labels=[0, 1, 2, 3]).astype(int)\n",
    "    df['NumSpendingCategories'] = sum(df[f'{col}_spent'] for col in spending_cols)\n",
    "    df['TotalSpent_log'] = np.log1p(df['TotalSpent'])\n",
    "    df['LuxurySpent_log'] = np.log1p(df['LuxurySpent'])\n",
    "    df['BasicSpent_log'] = np.log1p(df['BasicSpent'])\n",
    "    df['Spa_VRDeck_RoomService'] = df['Spa'] + df['VRDeck'] + df['RoomService']\n",
    "    df['FoodCourt_RoomService'] = df['FoodCourt'] + df['RoomService']\n",
    "    df['IsChild'] = (df['Age'] <= 12).astype(int)\n",
    "    df['IsTeen'] = ((df['Age'] > 12) & (df['Age'] <= 17)).astype(int)\n",
    "    df['IsYoungAdult'] = ((df['Age'] > 17) & (df['Age'] <= 25)).astype(int)\n",
    "    df['IsAdult'] = ((df['Age'] > 25) & (df['Age'] <= 60)).astype(int)\n",
    "    df['IsSenior'] = (df['Age'] > 60).astype(int)\n",
    "    return df\n",
    "\n",
    "train = create_features(train)\n",
    "test = create_features(test)\n",
    "print(\"Features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512dbcbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:46:51.216390Z",
     "iopub.status.busy": "2026-01-06T04:46:51.215775Z",
     "iopub.status.idle": "2026-01-06T04:46:51.267383Z",
     "shell.execute_reply": "2026-01-06T04:46:51.266770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 49\n"
     ]
    }
   ],
   "source": [
    "# Prepare features with label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']\n",
    "num_features = [\n",
    "    'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
    "    'Group', 'PassengerNum', 'CabinNum', 'GroupSize', 'Solo',\n",
    "    'TotalSpent', 'LuxurySpent', 'BasicSpent', 'LuxuryRatio', 'SpentPerAge', 'SpendingBin',\n",
    "    'NumSpendingCategories', 'Spa_VRDeck_RoomService', 'FoodCourt_RoomService',\n",
    "    'RoomService_ratio', 'FoodCourt_ratio', 'ShoppingMall_ratio', 'Spa_ratio', 'VRDeck_ratio',\n",
    "    'RoomService_spent', 'FoodCourt_spent', 'ShoppingMall_spent', 'Spa_spent', 'VRDeck_spent',\n",
    "    'RoomService_log', 'FoodCourt_log', 'ShoppingMall_log', 'Spa_log', 'VRDeck_log',\n",
    "    'TotalSpent_log', 'LuxurySpent_log', 'BasicSpent_log',\n",
    "    'IsChild', 'IsTeen', 'IsYoungAdult', 'IsAdult', 'IsSenior'\n",
    "]\n",
    "\n",
    "# Label encode categoricals\n",
    "for col in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train[col].astype(str), test[col].astype(str)])\n",
    "    le.fit(combined)\n",
    "    train[col + '_enc'] = le.transform(train[col].astype(str))\n",
    "    test[col + '_enc'] = le.transform(test[col].astype(str))\n",
    "\n",
    "feature_cols = [col + '_enc' for col in cat_features] + num_features\n",
    "X = train[feature_cols].values\n",
    "y = train['Transported'].astype(int).values\n",
    "X_test = test[feature_cols].values\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATION 1: CV Stability with Multiple Seeds\n",
    "# Run same model with different random seeds to understand variance\n",
    "\n",
    "print(\"=== CV STABILITY ANALYSIS ===\")\n",
    "print(\"Testing same hyperparameters with different random seeds...\\n\")\n",
    "\n",
    "cat_params = {\n",
    "    'depth': 8,\n",
    "    'learning_rate': 0.051,\n",
    "    'iterations': 755,\n",
    "    'l2_leaf_reg': 3.52,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "seeds = [42, 123, 456, 789, 1000]\n",
    "cv_scores_by_seed = []\n",
    "\n",
    "for seed in seeds:\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**cat_params, random_seed=seed)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "        \n",
    "        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        fold_acc = accuracy_score(y_val, (oof_preds[val_idx] >= 0.5).astype(int))\n",
    "        fold_scores.append(fold_acc)\n",
    "    \n",
    "    cv_score = accuracy_score(y, (oof_preds >= 0.5).astype(int))\n",
    "    cv_scores_by_seed.append(cv_score)\n",
    "    print(f\"Seed {seed}: CV = {cv_score:.5f} (fold std: {np.std(fold_scores):.5f})\")\n",
    "\n",
    "print(f\"\\nMean CV across seeds: {np.mean(cv_scores_by_seed):.5f}\")\n",
    "print(f\"Std CV across seeds: {np.std(cv_scores_by_seed):.5f}\")\n",
    "print(f\"Range: {min(cv_scores_by_seed):.5f} - {max(cv_scores_by_seed):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b581959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATION 2: Threshold Analysis\n",
    "# Is threshold 0.47 really better, or is it overfitting?\n",
    "\n",
    "print(\"\\n=== THRESHOLD ANALYSIS ===\")\n",
    "print(\"Analyzing threshold sensitivity across different CV splits...\\n\")\n",
    "\n",
    "# Get OOF predictions with seed 42 (our standard)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(len(X))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**cat_params, random_seed=42)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Analyze threshold impact\n",
    "thresholds = np.arange(0.45, 0.55, 0.01)\n",
    "print(\"Threshold | Accuracy | Pred Rate | Delta from 0.5\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for t in thresholds:\n",
    "    acc = accuracy_score(y, (oof_preds >= t).astype(int))\n",
    "    pred_rate = (oof_preds >= t).mean()\n",
    "    delta = acc - accuracy_score(y, (oof_preds >= 0.5).astype(int))\n",
    "    print(f\"  {t:.2f}    |  {acc:.5f}  |  {pred_rate:.4f}   |  {delta:+.5f}\")\n",
    "\n",
    "print(f\"\\nTraining transported rate: {y.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d873de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATION 3: Feature Importance and Potential Overfitting Features\n",
    "# Check if some features might be causing overfitting\n",
    "\n",
    "print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "# Train final model\n",
    "model = CatBoostClassifier(**cat_params, random_seed=42)\n",
    "model.fit(X, y, verbose=False)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 features:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 features (candidates for removal):\")\n",
    "print(feature_importance.tail(10).to_string(index=False))\n",
    "\n",
    "# Count features with very low importance\n",
    "low_importance = feature_importance[feature_importance['importance'] < 1.0]\n",
    "print(f\"\\nFeatures with importance < 1.0: {len(low_importance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATION 4: CV-LB Gap Analysis\n",
    "# What's causing the gap to widen?\n",
    "\n",
    "print(\"\\n=== CV-LB GAP ANALYSIS ===\")\n",
    "print(\"\\nSubmission History:\")\n",
    "print(\"| Exp | CV Score | LB Score | Gap | Gap % |\")\n",
    "print(\"|-----|----------|----------|-----|-------|\")\n",
    "print(\"| exp_000 | 0.80674 | 0.79705 | +0.00969 | +1.20% |\")\n",
    "print(\"| exp_003 | 0.81951 | 0.80453 | +0.01498 | +1.83% |\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"1. Gap increased from 1.20% to 1.83% as CV improved\")\n",
    "print(\"2. CV improvement: +0.01277 (+1.58%)\")\n",
    "print(\"3. LB improvement: +0.00748 (+0.94%)\")\n",
    "print(\"4. LB improvement rate: 58.6% of CV improvement\")\n",
    "print(\"\\nThis suggests we're overfitting to CV as we tune more.\")\n",
    "\n",
    "print(\"\\nTo beat top LB of 0.8066:\")\n",
    "print(f\"- Using 1.83% gap: Need CV of {0.8066 * 1.0183:.5f}\")\n",
    "print(f\"- Using 1.50% gap: Need CV of {0.8066 * 1.015:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bef4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATION 5: Regularization Impact\n",
    "# Try stronger regularization to reduce overfitting\n",
    "\n",
    "print(\"\\n=== REGULARIZATION ANALYSIS ===\")\n",
    "print(\"Testing different l2_leaf_reg values...\\n\")\n",
    "\n",
    "l2_values = [1.0, 3.52, 5.0, 7.0, 10.0]\n",
    "\n",
    "for l2 in l2_values:\n",
    "    params = cat_params.copy()\n",
    "    params['l2_leaf_reg'] = l2\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params, random_seed=42)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        fold_scores.append(accuracy_score(y_val, (oof_preds[val_idx] >= 0.5).astype(int)))\n",
    "    \n",
    "    cv_score = accuracy_score(y, (oof_preds >= 0.5).astype(int))\n",
    "    print(f\"l2_leaf_reg={l2:.1f}: CV={cv_score:.5f} (std={np.std(fold_scores):.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY AND RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CV STABILITY:\")\n",
    "print(f\"   - CV varies by ~{np.std(cv_scores_by_seed)*100:.2f}% across random seeds\")\n",
    "print(f\"   - This explains the 0.81951 vs 0.81617 discrepancy\")\n",
    "print(\"   - Recommendation: Use average of multiple seeds for more stable estimates\")\n",
    "\n",
    "print(\"\\n2. THRESHOLD TUNING:\")\n",
    "print(\"   - Threshold 0.47 gives marginal improvement on CV\")\n",
    "print(\"   - But shifts predicted distribution significantly (53.8% vs 50.4%)\")\n",
    "print(\"   - Risk: May hurt LB if test distribution matches training\")\n",
    "print(\"   - Recommendation: Submit with threshold 0.5 as baseline\")\n",
    "\n",
    "print(\"\\n3. CV-LB GAP:\")\n",
    "print(\"   - Gap is widening (1.20% → 1.83%)\")\n",
    "print(\"   - We're overfitting to CV\")\n",
    "print(\"   - Recommendation: Focus on regularization and simpler models\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS (Priority Order):\")\n",
    "print(\"   a) Submit exp_004 (threshold 0.47) to get LB feedback\")\n",
    "print(\"   b) If LB worse: Revert to threshold 0.5, increase regularization\")\n",
    "print(\"   c) Try stacking with logistic regression meta-learner\")\n",
    "print(\"   d) Feature selection to remove low-importance features\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
