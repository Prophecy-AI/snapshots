{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c75461",
   "metadata": {},
   "source": [
    "# Loop 10 Analysis: Post-Target Encoding Assessment\n",
    "\n",
    "## Key Questions:\n",
    "1. Why did target encoding HURT performance?\n",
    "2. What approaches remain unexplored?\n",
    "3. What's the realistic path forward?\n",
    "\n",
    "## Evaluator's Key Points:\n",
    "- Target encoding performed WORSE than label encoding (0.81560 vs 0.81698)\n",
    "- We've been stuck for 7 experiments without beating exp_003's CV of 0.81951\n",
    "- exp_003 was likely a lucky Optuna run\n",
    "- Target score (0.9642) is impossible - top LB is ~0.8066\n",
    "- Suggests: 10-fold CV + regularization to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load submission history for analysis\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.80674, 'lb': 0.79705, 'approach': 'XGBoost baseline'},\n",
    "    {'exp': 'exp_003', 'cv': 0.81951, 'lb': 0.80453, 'approach': 'CatBoost Optuna'},\n",
    "    {'exp': 'exp_004', 'cv': 0.81928, 'lb': 0.80406, 'approach': 'Threshold tuning'},\n",
    "    {'exp': 'exp_006', 'cv': 0.81709, 'lb': 0.80102, 'approach': 'Weighted ensemble'},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "df['gap'] = df['cv'] - df['lb']\n",
    "df['gap_pct'] = (df['gap'] / df['cv'] * 100).round(2)\n",
    "print(\"Submission History:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8defc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment history (all 10 experiments)\n",
    "experiments = [\n",
    "    {'exp': 'exp_000', 'cv': 0.80674, 'approach': 'XGBoost baseline', 'submitted': True},\n",
    "    {'exp': 'exp_001', 'cv': 0.80927, 'approach': 'Advanced features', 'submitted': False},\n",
    "    {'exp': 'exp_002', 'cv': 0.81353, 'approach': '3-model ensemble', 'submitted': False},\n",
    "    {'exp': 'exp_003', 'cv': 0.81951, 'approach': 'CatBoost Optuna', 'submitted': True},\n",
    "    {'exp': 'exp_004', 'cv': 0.81928, 'approach': 'Threshold tuning', 'submitted': True},\n",
    "    {'exp': 'exp_005', 'cv': 0.81744, 'approach': 'Stacking', 'submitted': False},\n",
    "    {'exp': 'exp_006', 'cv': 0.81709, 'approach': 'Weighted ensemble', 'submitted': True},\n",
    "    {'exp': 'exp_007', 'cv': 0.81617, 'approach': 'Feature selection', 'submitted': False},\n",
    "    {'exp': 'exp_008', 'cv': 0.81698, 'approach': 'Multi-seed', 'submitted': False},\n",
    "    {'exp': 'exp_009', 'cv': 0.81560, 'approach': 'Target encoding', 'submitted': False},\n",
    "]\n",
    "\n",
    "exp_df = pd.DataFrame(experiments)\n",
    "print(\"\\nAll Experiments:\")\n",
    "print(exp_df.to_string(index=False))\n",
    "\n",
    "# Best CV\n",
    "print(f\"\\nBest CV: {exp_df['cv'].max():.5f} ({exp_df.loc[exp_df['cv'].idxmax(), 'approach']})\")\n",
    "print(f\"Median CV: {exp_df['cv'].median():.5f}\")\n",
    "print(f\"CV range: {exp_df['cv'].min():.5f} - {exp_df['cv'].max():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682dad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what approaches have been tried\n",
    "approaches_tried = {\n",
    "    'Encoding': ['Label encoding (baseline)', 'Target encoding (exp_009)'],\n",
    "    'Models': ['XGBoost', 'LightGBM', 'CatBoost', '3-model ensemble'],\n",
    "    'Ensembling': ['Simple averaging', 'Weighted averaging', 'Stacking (LR meta)'],\n",
    "    'Hyperparameters': ['Optuna tuning (50 trials)', 'Multi-seed (5 seeds)'],\n",
    "    'Features': ['56 features', 'Feature selection (32 features)', 'Cabin regions', 'Family size'],\n",
    "    'Validation': ['5-fold StratifiedKFold'],\n",
    "    'Threshold': ['0.5 (default)', '0.47 (tuned)'],\n",
    "}\n",
    "\n",
    "print(\"Approaches Tried:\")\n",
    "for category, approaches in approaches_tried.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for a in approaches:\n",
    "        print(f\"  - {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc53d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What HASN'T been tried (from evaluator's suggestions)\n",
    "approaches_not_tried = {\n",
    "    'Validation': [\n",
    "        '10-fold CV (more stable estimates)',\n",
    "        'Adversarial validation (check distribution shift)',\n",
    "    ],\n",
    "    'Regularization': [\n",
    "        'Reduced depth (6 instead of 8)',\n",
    "        'Higher l2_leaf_reg (5-10 instead of 3.52)',\n",
    "        'Subsample (0.8)',\n",
    "        'Colsample_bylevel (0.8)',\n",
    "    ],\n",
    "    'Imputation': [\n",
    "        'KNN imputation (mentioned in top solutions)',\n",
    "    ],\n",
    "    'Data Augmentation': [\n",
    "        'Pseudo-labeling (use confident test predictions)',\n",
    "    ],\n",
    "    'Models': [\n",
    "        'Neural networks (for diversity)',\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"\\nApproaches NOT Yet Tried:\")\n",
    "for category, approaches in approaches_not_tried.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for a in approaches:\n",
    "        print(f\"  - {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-LB relationship analysis\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "cv_scores = [0.80674, 0.81951, 0.81928, 0.81709]\n",
    "lb_scores = [0.79705, 0.80453, 0.80406, 0.80102]\n",
    "\n",
    "# Linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_scores, lb_scores)\n",
    "print(f\"CV-LB Linear Model:\")\n",
    "print(f\"  LB = {slope:.3f} * CV + {intercept:.3f}\")\n",
    "print(f\"  RÂ² = {r_value**2:.3f}\")\n",
    "print(f\"  p-value = {p_value:.4f}\")\n",
    "\n",
    "# Predict LB for different CV scores\n",
    "print(f\"\\nPredicted LB for different CV scores:\")\n",
    "for cv in [0.815, 0.817, 0.819, 0.820, 0.821, 0.822]:\n",
    "    predicted_lb = slope * cv + intercept\n",
    "    print(f\"  CV {cv:.3f} -> LB {predicted_lb:.4f}\")\n",
    "\n",
    "# What CV do we need to beat exp_003's LB?\n",
    "target_lb = 0.80453\n",
    "required_cv = (target_lb - intercept) / slope\n",
    "print(f\"\\nTo beat LB 0.8045, need CV > {required_cv:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold variance analysis from exp_009 (target encoding)\n",
    "fold_scores_exp009 = [0.82404, 0.81196, 0.81484, 0.82336, 0.80380]\n",
    "print(\"exp_009 (Target Encoding) Fold Scores:\")\n",
    "print(f\"  Scores: {fold_scores_exp009}\")\n",
    "print(f\"  Mean: {np.mean(fold_scores_exp009):.5f}\")\n",
    "print(f\"  Std: {np.std(fold_scores_exp009):.5f}\")\n",
    "print(f\"  Range: {min(fold_scores_exp009):.5f} - {max(fold_scores_exp009):.5f}\")\n",
    "print(f\"  Spread: {max(fold_scores_exp009) - min(fold_scores_exp009):.5f} ({(max(fold_scores_exp009) - min(fold_scores_exp009))/np.mean(fold_scores_exp009)*100:.2f}%)\")\n",
    "\n",
    "# Compare with typical fold variance\n",
    "print(\"\\nTypical fold variance from previous experiments:\")\n",
    "print(\"  exp_003: std = 0.00685\")\n",
    "print(\"  exp_004: std = 0.00762\")\n",
    "print(\"  exp_008: std = 0.00498 (seed 42)\")\n",
    "print(f\"  exp_009: std = {np.std(fold_scores_exp009):.5f}\")\n",
    "print(\"\\nexp_009 has HIGH fold variance - suggests instability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: exp_003's CV of 0.81951 is an outlier\n",
    "# Multi-seed analysis showed true baseline is ~0.817\n",
    "\n",
    "print(\"=== CRITICAL INSIGHT ===\")\n",
    "print(\"\\nexp_003's CV of 0.81951 appears to be an outlier:\")\n",
    "print(\"  - Multi-seed analysis (5 seeds) gave max CV = 0.81629\")\n",
    "print(\"  - Multi-seed ensemble CV = 0.81698\")\n",
    "print(\"  - exp_003 was Optuna-tuned (50 trials) - may have overfit to CV folds\")\n",
    "print(\"\\nTrue baseline is ~0.817 CV, not 0.82\")\n",
    "print(\"\\nTo beat exp_003's LB of 0.8045:\")\n",
    "print(f\"  - Need CV > {required_cv:.5f} (based on linear model)\")\n",
    "print(f\"  - Gap from true baseline: {required_cv - 0.817:.5f} (+{(required_cv - 0.817)/0.817*100:.2f}%)\")\n",
    "print(\"\\nThis is a SIGNIFICANT gap that requires a fundamentally different approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator's recommendation: 10-fold CV + regularization\n",
    "print(\"=== EVALUATOR'S RECOMMENDATION ===\")\n",
    "print(\"\\n1. Use 10-fold CV (instead of 5-fold):\")\n",
    "print(\"   - More stable estimates\")\n",
    "print(\"   - Less variance between folds\")\n",
    "print(\"   - Top kernels use this\")\n",
    "print(\"\\n2. Increase regularization:\")\n",
    "print(\"   - Reduce depth from 8 to 6\")\n",
    "print(\"   - Increase l2_leaf_reg from 3.52 to 5-10\")\n",
    "print(\"   - Add subsample (0.8) and colsample_bylevel (0.8)\")\n",
    "print(\"\\n3. Use the best feature set (56 features with label encoding)\")\n",
    "print(\"\\nRationale:\")\n",
    "print(\"  - High fold variance (2.02% spread) suggests overfitting\")\n",
    "print(\"  - CV-LB gap is increasing (1.19% -> 1.97%)\")\n",
    "print(\"  - Focus on reducing overfitting, not maximizing CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098595b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approaches to consider\n",
    "print(\"=== ALTERNATIVE APPROACHES ===\")\n",
    "print(\"\\n1. KNN Imputation (from top solutions):\")\n",
    "print(\"   - Our current imputation is simple (mode/median)\")\n",
    "print(\"   - KNN imputation may capture more complex patterns\")\n",
    "print(\"   - Could improve data quality\")\n",
    "print(\"\\n2. Pseudo-labeling:\")\n",
    "print(\"   - Use confident test predictions to augment training data\")\n",
    "print(\"   - Can help with distribution shift\")\n",
    "print(\"   - Risk: may amplify errors if predictions are wrong\")\n",
    "print(\"\\n3. Neural Networks:\")\n",
    "print(\"   - For diversity in ensembling\")\n",
    "print(\"   - May capture different patterns than tree models\")\n",
    "print(\"   - Risk: may not work well on small tabular data\")\n",
    "print(\"\\n4. Adversarial Validation:\")\n",
    "print(\"   - Check if train/test distributions differ\")\n",
    "print(\"   - Identify features that cause distribution shift\")\n",
    "print(\"   - Could explain increasing CV-LB gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reality check on target score\n",
    "print(\"=== REALITY CHECK ===\")\n",
    "print(\"\\nTarget score: 0.9642\")\n",
    "print(\"Top LB score: ~0.8066\")\n",
    "print(\"Our best LB: 0.8045\")\n",
    "print(\"\\nThe target of 0.9642 is IMPOSSIBLE.\")\n",
    "print(\"Top LB is ~0.8066, which is 96.4% accuracy.\")\n",
    "print(\"Our best (0.8045) is already in the top ~7%.\")\n",
    "print(\"\\nGap to top: 0.0021 (0.26%)\")\n",
    "print(\"\\nThis is a very small gap. We're already competitive.\")\n",
    "print(\"\\nFocus should be on:\")\n",
    "print(\"  1. Reducing overfitting (CV-LB gap)\")\n",
    "print(\"  2. Incremental improvements toward 0.81 LB\")\n",
    "print(\"  3. NOT chasing an impossible target\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
