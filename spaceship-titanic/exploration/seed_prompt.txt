# Seed Prompt: Binary Classification for Passenger Transport Prediction

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, missing values, target distribution, key correlations
- Key findings: Target is balanced (~50/50), CryoSleep is strongest predictor (81.8% transported when True), spending features highly skewed

## Feature Engineering (Critical for High Scores)

### 1. PassengerId Extraction
- Extract Group ID (first 4 digits before underscore) - passengers in same group often have similar outcomes
- Extract passenger number within group
- Create GroupSize feature (count passengers per group)
- Create Solo indicator (GroupSize == 1)

### 2. Cabin Feature Parsing
- Split Cabin into Deck/Num/Side (format: deck/num/side)
- Deck is categorical (A, B, C, D, E, F, G, T)
- Side is binary (P=Port, S=Starboard)
- Cabin number can be used for proximity features

### 3. Spending Features
- Create TotalExpenditure = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck
- Create binary indicators for each spending category (spent > 0)
- Log transform spending features (log1p) due to high skewness
- CryoSleep passengers should have 0 spending (use for imputation)
- Create spending ratios (e.g., RoomService/TotalExpenditure)

### 4. Age Features
- Create age bins/groups (0-12, 13-17, 18-25, 26-40, 41-60, 60+)
- Young passengers (0-18) more likely transported
- Create IsChild, IsTeen, IsAdult indicators

### 5. Name Features
- Extract surname for family grouping
- TF-IDF on names with PCA dimensionality reduction (advanced)
- Family size from surname count

### 6. Interaction Features (Important for Tree Models)
- CryoSleep × HomePlanet interactions
- Deck × Side interactions
- TotalSpent × VIP interactions
- Age × CryoSleep interactions
- Pairwise products and ratios of numerical features

## Missing Value Imputation

### Strategy (Important!)
1. Use group information to impute missing values (passengers in same group often share characteristics)
2. CryoSleep passengers: Set all spending to 0 (they're confined to cabins)
3. For categorical: Use mode within group, then overall mode
4. For numerical: Use median within group, then overall median
5. Consider KNN imputation for more sophisticated approach
6. Build models to predict missing values (advanced)

## Models

### Primary Models (Ensemble These)
1. **XGBoost** - Best single model performer
   - Recommended params: max_depth=5, learning_rate=0.06-0.08, n_estimators=700-900
   - Use regularization: lambda=3.0, alpha=4.5
   - colsample_bytree=0.9, subsample=0.95

2. **LightGBM** - Fast and effective
   - Recommended params: num_leaves=330, learning_rate=0.087, n_estimators=739
   - feature_fraction=0.66, bagging_fraction=0.87
   - lambda_l1=6.18, lambda_l2=0.01

3. **CatBoost** - Handles categoricals natively
   - Good for this dataset with multiple categorical features
   - Use cat_features parameter for categorical columns
   - Often best for datasets with many categorical features

### Ensemble Strategy
- Use soft voting (average probabilities) across models
- Optuna-optimized weights for model blending
- 10-fold StratifiedKFold cross-validation
- Multiple random seeds for robustness (3-5 seeds)
- Consider stacking with logistic regression as meta-learner

## Validation Strategy

### Cross-Validation
- Use StratifiedKFold with 5-10 folds
- Maintain target distribution in each fold
- Track both accuracy and log loss
- Use multiple seeds and average results

### Threshold Tuning
- Default threshold is 0.5, but can be optimized
- Match predicted distribution to training distribution (~50.4% transported)
- Use precision-recall curve to find optimal cutoff

## Hyperparameter Optimization

### Optuna Framework (Recommended)
- Use Optuna with HyperbandPruner for efficient search
- Run 100-1000 trials for thorough search
- Key parameters to tune:
  - XGBoost: max_depth, learning_rate, n_estimators, lambda, alpha, colsample_bytree, subsample
  - LightGBM: num_leaves, learning_rate, feature_fraction, bagging_fraction, min_child_samples
  - CatBoost: depth, learning_rate, iterations, l2_leaf_reg

### Early Stopping
- Use early_stopping_rounds=200 to prevent overfitting
- Monitor validation loss during training

## Post-Processing

### Ensemble Combination
- Average predictions from multiple models
- Consider OR/AND gates between different model predictions for edge cases
- Mode voting across folds for final prediction

### Threshold Calibration
- Calibrate threshold to match expected target distribution
- Test different thresholds around 0.5 (0.48-0.52 range)

## Advanced Techniques

### AutoGluon (Alternative Approach)
- AutoGluon automates model selection and ensembling
- Can achieve competitive results with minimal manual tuning
- Trains multiple models in parallel and combines with weighted averaging

### Target Encoding
- Use target encoding for categorical features with cross-validation to prevent leakage
- CatBoost encoding is a safe alternative

### Feature Selection
- Use permutation importance to identify key features
- Remove low-importance features to reduce noise
- Mutual information for feature selection

### Pseudo-Labeling (Advanced)
- Use confident predictions on test set as additional training data
- Only use predictions with high confidence (>0.9 or <0.1)

## Key Insights from Top Solutions

1. **Feature engineering is crucial** - extracting group, cabin, and spending features significantly improves scores
2. **CryoSleep is the strongest predictor** - 81.8% transported when True
3. **Group-based imputation** - passengers in same group share characteristics
4. **Ensemble of gradient boosting models** - XGBoost + LightGBM + CatBoost combination works best
5. **Optuna hyperparameter tuning** - systematic search improves over default parameters
6. **Multiple seeds and folds** - reduces variance in predictions
7. **Spending features are highly skewed** - log transform or binary indicators help

## Implementation Priority

1. **First**: Basic feature engineering (Group, Cabin split, TotalExpenditure)
2. **Second**: Proper missing value imputation using group information
3. **Third**: Train XGBoost/LightGBM/CatBoost with good hyperparameters
4. **Fourth**: Ensemble models with optimized weights
5. **Fifth**: Advanced features (interactions, TF-IDF on names)
6. **Sixth**: Hyperparameter optimization with Optuna

## Expected Performance
- Single model CV accuracy: ~80-81%
- Ensemble CV accuracy: ~81-82%
- Target to beat: 0.9642 (very high - requires careful feature engineering and ensembling)

## Notes
- The target score of 0.9642 is exceptionally high for this problem
- Focus on robust feature engineering and proper validation
- Ensemble multiple diverse models for best results
- Use multiple random seeds to reduce variance
