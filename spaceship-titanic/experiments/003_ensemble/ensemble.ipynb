{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d521c57c",
   "metadata": {},
   "source": [
    "# Experiment 003: 3-Model Ensemble (XGBoost + LightGBM + CatBoost)\n",
    "\n",
    "Following evaluator's top priority recommendation:\n",
    "1. Use the mature feature set from exp_002 (56 features)\n",
    "2. Train XGBoost, LightGBM, and CatBoost with 5-fold CV\n",
    "3. Simple averaging of probabilities\n",
    "4. Expected improvement: 0.5-1% over single XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0b4cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:16:55.629615Z",
     "iopub.status.busy": "2026-01-06T04:16:55.628872Z",
     "iopub.status.idle": "2026-01-06T04:16:57.591308Z",
     "shell.execute_reply": "2026-01-06T04:16:57.590629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8693, 14)\n",
      "Test shape: (4277, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbf4093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:16:57.593834Z",
     "iopub.status.busy": "2026-01-06T04:16:57.593303Z",
     "iopub.status.idle": "2026-01-06T04:16:57.677901Z",
     "shell.execute_reply": "2026-01-06T04:16:57.677054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic features created\n"
     ]
    }
   ],
   "source": [
    "# Copy feature engineering from exp_002\n",
    "def advanced_feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    df['Group'] = df['PassengerId'].apply(lambda x: int(x.split('_')[0]))\n",
    "    df['PassengerNum'] = df['PassengerId'].apply(lambda x: int(x.split('_')[1]))\n",
    "    df['Deck'] = df['Cabin'].apply(lambda x: x.split('/')[0] if pd.notna(x) else np.nan)\n",
    "    df['CabinNum'] = df['Cabin'].apply(lambda x: int(x.split('/')[1]) if pd.notna(x) else np.nan)\n",
    "    df['Side'] = df['Cabin'].apply(lambda x: x.split('/')[2] if pd.notna(x) else np.nan)\n",
    "    df['Surname'] = df['Name'].apply(lambda x: x.split()[-1] if pd.notna(x) else np.nan)\n",
    "    return df\n",
    "\n",
    "train = advanced_feature_engineering(train)\n",
    "test = advanced_feature_engineering(test)\n",
    "\n",
    "# GroupSize\n",
    "all_data = pd.concat([train[['Group']], test[['Group']]], ignore_index=True)\n",
    "group_sizes = all_data['Group'].value_counts().to_dict()\n",
    "for df in [train, test]:\n",
    "    df['GroupSize'] = df['Group'].map(group_sizes)\n",
    "    df['Solo'] = (df['GroupSize'] == 1).astype(int)\n",
    "\n",
    "print(\"Basic features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00bd517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:16:57.681126Z",
     "iopub.status.busy": "2026-01-06T04:16:57.680211Z",
     "iopub.status.idle": "2026-01-06T04:17:01.961969Z",
     "shell.execute_reply": "2026-01-06T04:17:01.961031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation complete\n"
     ]
    }
   ],
   "source": [
    "# Group-based imputation\n",
    "def group_based_imputation(train_df, test_df):\n",
    "    combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    \n",
    "    group_homeplanet = combined.groupby('Group')['HomePlanet'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan).to_dict()\n",
    "    group_deck = combined.groupby('Group')['Deck'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan).to_dict()\n",
    "    group_side = combined.groupby('Group')['Side'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan).to_dict()\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        for col, mapping in [('HomePlanet', group_homeplanet), ('Deck', group_deck), ('Side', group_side)]:\n",
    "            mask = df[col].isna()\n",
    "            df.loc[mask, col] = df.loc[mask, 'Group'].map(mapping)\n",
    "    return train_df, test_df\n",
    "\n",
    "train, test = group_based_imputation(train, test)\n",
    "\n",
    "# Remaining imputation\n",
    "def impute_remaining(df):\n",
    "    df = df.copy()\n",
    "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    for col in spending_cols:\n",
    "        mask = (df['CryoSleep'] == True) & (df[col].isna())\n",
    "        df.loc[mask, col] = 0\n",
    "    \n",
    "    for col in ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']:\n",
    "        if df[col].isna().any():\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    for col in ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'CabinNum']:\n",
    "        if df[col].isna().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "train = impute_remaining(train)\n",
    "test = impute_remaining(test)\n",
    "print(\"Imputation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86868a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:17:01.965176Z",
     "iopub.status.busy": "2026-01-06T04:17:01.964510Z",
     "iopub.status.idle": "2026-01-06T04:17:02.015603Z",
     "shell.execute_reply": "2026-01-06T04:17:02.014877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spending features created\n"
     ]
    }
   ],
   "source": [
    "# Spending features\n",
    "def create_spending_features(df):\n",
    "    df = df.copy()\n",
    "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    \n",
    "    df['TotalSpent'] = df[spending_cols].sum(axis=1)\n",
    "    for col in spending_cols:\n",
    "        df[f'{col}_ratio'] = df[col] / (df['TotalSpent'] + 1)\n",
    "    \n",
    "    df['LuxurySpent'] = df['Spa'] + df['VRDeck'] + df['RoomService']\n",
    "    df['BasicSpent'] = df['FoodCourt'] + df['ShoppingMall']\n",
    "    df['LuxuryRatio'] = df['LuxurySpent'] / (df['TotalSpent'] + 1)\n",
    "    df['SpentPerAge'] = df['TotalSpent'] / (df['Age'] + 1)\n",
    "    df['SpendingBin'] = pd.cut(df['TotalSpent'], bins=[-1, 0, 500, 2000, float('inf')], labels=[0, 1, 2, 3]).astype(int)\n",
    "    \n",
    "    for col in spending_cols:\n",
    "        df[f'{col}_spent'] = (df[col] > 0).astype(int)\n",
    "    \n",
    "    for col in spending_cols + ['TotalSpent', 'LuxurySpent', 'BasicSpent']:\n",
    "        df[f'{col}_log'] = np.log1p(df[col])\n",
    "    \n",
    "    df['NumSpendingCategories'] = sum(df[f'{col}_spent'] for col in spending_cols)\n",
    "    df['Spa_VRDeck_RoomService'] = df['Spa'] + df['VRDeck'] + df['RoomService']\n",
    "    df['FoodCourt_RoomService'] = df['FoodCourt'] + df['RoomService']\n",
    "    return df\n",
    "\n",
    "train = create_spending_features(train)\n",
    "test = create_spending_features(test)\n",
    "print(\"Spending features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa2fe27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:17:02.018468Z",
     "iopub.status.busy": "2026-01-06T04:17:02.017777Z",
     "iopub.status.idle": "2026-01-06T04:17:02.080773Z",
     "shell.execute_reply": "2026-01-06T04:17:02.079872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction features created\n"
     ]
    }
   ],
   "source": [
    "# Interaction features\n",
    "def create_interaction_features(df):\n",
    "    df = df.copy()\n",
    "    df['CryoSleep_HomePlanet'] = df['CryoSleep'].astype(str) + '_' + df['HomePlanet'].astype(str)\n",
    "    df['CryoSleep_Destination'] = df['CryoSleep'].astype(str) + '_' + df['Destination'].astype(str)\n",
    "    df['Deck_Side'] = df['Deck'].astype(str) + '_' + df['Side'].astype(str)\n",
    "    df['HomePlanet_Destination'] = df['HomePlanet'].astype(str) + '_' + df['Destination'].astype(str)\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 17, 25, 40, 60, 100], \n",
    "                            labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'MiddleAge', 'Senior'])\n",
    "    df['AgeGroup_CryoSleep'] = df['AgeGroup'].astype(str) + '_' + df['CryoSleep'].astype(str)\n",
    "    df['VIP_HomePlanet'] = df['VIP'].astype(str) + '_' + df['HomePlanet'].astype(str)\n",
    "    \n",
    "    df['IsChild'] = (df['Age'] <= 12).astype(int)\n",
    "    df['IsTeen'] = ((df['Age'] > 12) & (df['Age'] <= 17)).astype(int)\n",
    "    df['IsYoungAdult'] = ((df['Age'] > 17) & (df['Age'] <= 25)).astype(int)\n",
    "    df['IsAdult'] = ((df['Age'] > 25) & (df['Age'] <= 60)).astype(int)\n",
    "    df['IsSenior'] = (df['Age'] > 60).astype(int)\n",
    "    return df\n",
    "\n",
    "train = create_interaction_features(train)\n",
    "test = create_interaction_features(test)\n",
    "print(\"Interaction features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bfe252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:17:02.084030Z",
     "iopub.status.busy": "2026-01-06T04:17:02.083299Z",
     "iopub.status.idle": "2026-01-06T04:17:02.155119Z",
     "shell.execute_reply": "2026-01-06T04:17:02.154284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 13 categorical columns\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "cat_cols_to_encode = [\n",
    "    'HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side',\n",
    "    'CryoSleep_HomePlanet', 'CryoSleep_Destination', 'Deck_Side',\n",
    "    'HomePlanet_Destination', 'AgeGroup', 'AgeGroup_CryoSleep', 'VIP_HomePlanet'\n",
    "]\n",
    "\n",
    "for col in cat_cols_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train[col].astype(str), test[col].astype(str)])\n",
    "    le.fit(combined)\n",
    "    train[col + '_enc'] = le.transform(train[col].astype(str))\n",
    "    test[col + '_enc'] = le.transform(test[col].astype(str))\n",
    "\n",
    "print(f\"Encoded {len(cat_cols_to_encode)} categorical columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9788cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:17:02.157633Z",
     "iopub.status.busy": "2026-01-06T04:17:02.157385Z",
     "iopub.status.idle": "2026-01-06T04:17:02.175178Z",
     "shell.execute_reply": "2026-01-06T04:17:02.174561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8693, 56), Features: 56\n"
     ]
    }
   ],
   "source": [
    "# Define features (same as exp_002)\n",
    "feature_cols = [\n",
    "    'HomePlanet_enc', 'CryoSleep_enc', 'Destination_enc', 'VIP_enc', 'Deck_enc', 'Side_enc',\n",
    "    'CryoSleep_HomePlanet_enc', 'CryoSleep_Destination_enc', 'Deck_Side_enc',\n",
    "    'HomePlanet_Destination_enc', 'AgeGroup_enc', 'AgeGroup_CryoSleep_enc', 'VIP_HomePlanet_enc',\n",
    "    'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
    "    'Group', 'PassengerNum', 'CabinNum', 'GroupSize', 'Solo',\n",
    "    'TotalSpent', 'LuxurySpent', 'BasicSpent', 'LuxuryRatio', 'SpentPerAge', 'SpendingBin',\n",
    "    'NumSpendingCategories', 'Spa_VRDeck_RoomService', 'FoodCourt_RoomService',\n",
    "    'RoomService_ratio', 'FoodCourt_ratio', 'ShoppingMall_ratio', 'Spa_ratio', 'VRDeck_ratio',\n",
    "    'RoomService_spent', 'FoodCourt_spent', 'ShoppingMall_spent', 'Spa_spent', 'VRDeck_spent',\n",
    "    'RoomService_log', 'FoodCourt_log', 'ShoppingMall_log', 'Spa_log', 'VRDeck_log',\n",
    "    'TotalSpent_log', 'LuxurySpent_log', 'BasicSpent_log',\n",
    "    'IsChild', 'IsTeen', 'IsYoungAdult', 'IsAdult', 'IsSenior'\n",
    "]\n",
    "\n",
    "X = train[feature_cols].values\n",
    "y = train['Transported'].astype(int).values\n",
    "X_test = test[feature_cols].values\n",
    "\n",
    "print(f\"X shape: {X.shape}, Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3887ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:17:02.177471Z",
     "iopub.status.busy": "2026-01-06T04:17:02.176839Z",
     "iopub.status.idle": "2026-01-06T04:17:02.182220Z",
     "shell.execute_reply": "2026-01-06T04:17:02.181654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters defined\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters from strategy\n",
    "xgb_params = {\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.067,\n",
    "    'n_estimators': 850,\n",
    "    'reg_lambda': 3.06,\n",
    "    'reg_alpha': 4.58,\n",
    "    'colsample_bytree': 0.92,\n",
    "    'subsample': 0.95,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'num_leaves': 330,\n",
    "    'learning_rate': 0.087,\n",
    "    'n_estimators': 739,\n",
    "    'feature_fraction': 0.66,\n",
    "    'bagging_fraction': 0.87,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 6.18,\n",
    "    'lambda_l2': 0.01,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'iterations': 1000,\n",
    "    'l2_leaf_reg': 3.0,\n",
    "    'random_seed': 42,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "print(\"Model parameters defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV for all three models\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# OOF predictions for each model\n",
    "oof_xgb = np.zeros(len(X))\n",
    "oof_lgb = np.zeros(len(X))\n",
    "oof_cat = np.zeros(len(X))\n",
    "\n",
    "# Test predictions for each model\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "test_cat = np.zeros(len(X_test))\n",
    "\n",
    "# Fold scores\n",
    "xgb_scores = []\n",
    "lgb_scores = []\n",
    "cat_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # XGBoost\n",
    "    model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    oof_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n",
    "    test_xgb += model_xgb.predict_proba(X_test)[:, 1] / n_folds\n",
    "    xgb_acc = accuracy_score(y_val, (oof_xgb[val_idx] >= 0.5).astype(int))\n",
    "    xgb_scores.append(xgb_acc)\n",
    "    \n",
    "    # LightGBM\n",
    "    model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    oof_lgb[val_idx] = model_lgb.predict_proba(X_val)[:, 1]\n",
    "    test_lgb += model_lgb.predict_proba(X_test)[:, 1] / n_folds\n",
    "    lgb_acc = accuracy_score(y_val, (oof_lgb[val_idx] >= 0.5).astype(int))\n",
    "    lgb_scores.append(lgb_acc)\n",
    "    \n",
    "    # CatBoost\n",
    "    model_cat = CatBoostClassifier(**cat_params)\n",
    "    model_cat.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "    oof_cat[val_idx] = model_cat.predict_proba(X_val)[:, 1]\n",
    "    test_cat += model_cat.predict_proba(X_test)[:, 1] / n_folds\n",
    "    cat_acc = accuracy_score(y_val, (oof_cat[val_idx] >= 0.5).astype(int))\n",
    "    cat_scores.append(cat_acc)\n",
    "    \n",
    "    print(f\"Fold {fold+1}: XGB={xgb_acc:.5f}, LGB={lgb_acc:.5f}, CAT={cat_acc:.5f}\")\n",
    "\n",
    "print(f\"\\nXGBoost Mean CV: {np.mean(xgb_scores):.5f} (+/- {np.std(xgb_scores):.5f})\")\n",
    "print(f\"LightGBM Mean CV: {np.mean(lgb_scores):.5f} (+/- {np.std(lgb_scores):.5f})\")\n",
    "print(f\"CatBoost Mean CV: {np.mean(cat_scores):.5f} (+/- {np.std(cat_scores):.5f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble: Simple averaging of probabilities\n",
    "oof_ensemble = (oof_xgb + oof_lgb + oof_cat) / 3\n",
    "test_ensemble = (test_xgb + test_lgb + test_cat) / 3\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_pred = (oof_ensemble >= 0.5).astype(int)\n",
    "ensemble_acc = accuracy_score(y, ensemble_pred)\n",
    "\n",
    "print(f\"\\n=== ENSEMBLE RESULTS ===\")\n",
    "print(f\"Ensemble OOF Accuracy: {ensemble_acc:.5f}\")\n",
    "\n",
    "# Compare to individual models\n",
    "xgb_oof_acc = accuracy_score(y, (oof_xgb >= 0.5).astype(int))\n",
    "lgb_oof_acc = accuracy_score(y, (oof_lgb >= 0.5).astype(int))\n",
    "cat_oof_acc = accuracy_score(y, (oof_cat >= 0.5).astype(int))\n",
    "\n",
    "print(f\"\\nIndividual OOF Accuracies:\")\n",
    "print(f\"  XGBoost:  {xgb_oof_acc:.5f}\")\n",
    "print(f\"  LightGBM: {lgb_oof_acc:.5f}\")\n",
    "print(f\"  CatBoost: {cat_oof_acc:.5f}\")\n",
    "\n",
    "# Compare to exp_002 baseline\n",
    "exp002_acc = 0.80927\n",
    "improvement = ensemble_acc - exp002_acc\n",
    "print(f\"\\nImprovement over exp_002 (single XGBoost): {improvement:+.5f} ({improvement*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "test_binary = (test_ensemble >= 0.5).astype(bool)\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Transported': test_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved with {len(submission)} predictions\")\n",
    "print(f\"Predicted transported rate: {test_binary.mean():.4f}\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
