{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba03fb4",
   "metadata": {},
   "source": [
    "# Cassava Leaf Disease Classification - Baseline CNN\n",
    "\n",
    "This notebook implements a simple CNN baseline for the cassava leaf disease classification competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0f7830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T21:20:46.128137Z",
     "iopub.status.busy": "2026-01-15T21:20:46.127805Z",
     "iopub.status.idle": "2026-01-15T21:20:59.572227Z",
     "shell.execute_reply": "2026-01-15T21:20:59.571478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8071b2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T21:20:59.575027Z",
     "iopub.status.busy": "2026-01-15T21:20:59.574379Z",
     "iopub.status.idle": "2026-01-15T21:20:59.960207Z",
     "shell.execute_reply": "2026-01-15T21:20:59.959280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 21397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "label\n",
      "0     1087\n",
      "1     2189\n",
      "2     2386\n",
      "3    13158\n",
      "4     2577\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: {'0': 'Cassava Bacterial Blight (CBB)', '1': 'Cassava Brown Streak Disease (CBSD)', '2': 'Cassava Green Mottle (CGM)', '3': 'Cassava Mosaic Disease (CMD)', '4': 'Healthy'}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = '/home/data'\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, 'train_images')\n",
    "TEST_IMG_DIR = os.path.join(DATA_DIR, 'test_images')\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Classes: {train_df['label'].nunique()}\")\n",
    "print(f\"Class distribution:\\n{train_df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Load label mapping\n",
    "with open(os.path.join(DATA_DIR, 'label_num_to_disease_map.json'), 'r') as f:\n",
    "    label_map = json.load(f)\n",
    "print(f\"\\nLabel mapping: {label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd0cb32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T21:20:59.963149Z",
     "iopub.status.busy": "2026-01-15T21:20:59.962814Z",
     "iopub.status.idle": "2026-01-15T21:20:59.971675Z",
     "shell.execute_reply": "2026-01-15T21:20:59.970932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['image_id'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = row['label']\n",
    "        return image, label\n",
    "\n",
    "# Define data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aaa4b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T21:20:59.974265Z",
     "iopub.status.busy": "2026-01-15T21:20:59.973990Z",
     "iopub.status.idle": "2026-01-15T21:21:01.283291Z",
     "shell.execute_reply": "2026-01-15T21:21:01.282565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 26,081,605\n",
      "Model size (MB): 99.5\n"
     ]
    }
   ],
   "source": [
    "# Define simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model size (MB): {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc66a861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T21:21:01.285849Z",
     "iopub.status.busy": "2026-01-15T21:21:01.285190Z",
     "iopub.status.idle": "2026-01-15T21:21:01.293023Z",
     "shell.execute_reply": "2026-01-15T21:21:01.292448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(train_loader), correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(val_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d509aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation training\n",
    "NUM_FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Prepare stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "\n",
    "print(f\"Starting {NUM_FOLDS}-fold cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    \n",
    "    # Create fold datasets\n",
    "    train_fold = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = CassavaDataset(train_fold, TRAIN_IMG_DIR, transform=train_transform)\n",
    "    val_dataset = CassavaDataset(val_fold, TRAIN_IMG_DIR, transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize model for this fold\n",
    "    model = SimpleCNN(num_classes=5)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    fold_scores.append(best_val_acc)\n",
    "    print(f\"Fold {fold + 1} Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Report cross-validation results\n",
    "print(f\"\\nCross-validation results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.4f}' for score in fold_scores]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training data\n",
    "print(\"Training final model on full training data...\")\n",
    "\n",
    "full_train_dataset = CassavaDataset(train_df, TRAIN_IMG_DIR, transform=train_transform)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "final_model = SimpleCNN(num_classes=5)\n",
    "final_model = final_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(final_model, full_train_loader, criterion, optimizer, device)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "print(\"Final model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "test_images = [f for f in os.listdir(TEST_IMG_DIR) if f.endswith('.jpg')]\n",
    "test_df = pd.DataFrame({'image_id': test_images})\n",
    "\n",
    "test_dataset = CassavaDataset(test_df, TEST_IMG_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "final_model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = final_model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Create submission\n",
    "test_df['label'] = predictions\n",
    "submission = test_df[['image_id', 'label']].sort_values('image_id')\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Predicted class distribution:\\n{submission['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
