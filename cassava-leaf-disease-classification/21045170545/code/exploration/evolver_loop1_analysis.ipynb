{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7e7141",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "Analysis of baseline results and identification of improvement opportunities for cassava leaf disease classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "DATA_DIR = '/home/data'\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, 'train_images')\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Classes: {train_df['label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mapping and analyze class distribution\n",
    "with open(os.path.join(DATA_DIR, 'label_num_to_disease_map.json'), 'r') as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "# Create a more readable label mapping\n",
    "label_names = {int(k): v for k, v in label_map.items()}\n",
    "train_df['disease_name'] = train_df['label'].map(label_names)\n",
    "\n",
    "# Class distribution analysis\n",
    "class_dist = train_df['label'].value_counts().sort_index()\n",
    "class_dist_pct = train_df['label'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "for label, count in class_dist.items():\n",
    "    print(f\"Class {label}: {label_names[label]:<35} {count:>5} samples ({class_dist_pct[label]:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(train_df)}\")\n",
    "print(f\"Most frequent class: {label_names[class_dist.idxmax()]} ({class_dist.max()} samples, {class_dist_pct.max():.1f}%)\")\n",
    "print(f\"Least frequent class: {label_names[class_dist.idxmin()]} ({class_dist.min()} samples, {class_dist_pct.min():.1f}%)\")\n",
    "print(f\"Imbalance ratio (max/min): {class_dist.max() / class_dist.min():.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a47d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(class_dist)))\n",
    "bars = ax1.bar(class_dist.index, class_dist.values, color=colors)\n",
    "ax1.set_xlabel('Class Label', fontsize=12)\n",
    "ax1.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax1.set_title('Class Distribution (Absolute Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(class_dist.index)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count, pct in zip(bars, class_dist.values, class_dist_pct.values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "             f'{count}\\n({pct:.1f}%)',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "wedges, texts, autotexts = ax2.pie(class_dist.values, \n",
    "                                   labels=[label_names[i] for i in class_dist.index],\n",
    "                                   autopct='%1.1f%%',\n",
    "                                   colors=colors,\n",
    "                                   startangle=90)\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Make percentage text bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('/home/code/exploration/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Class distribution plot saved to: /home/code/exploration/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image properties\n",
    "print(\"Analyzing image properties...\")\n",
    "\n",
    "# Sample images from each class for analysis\n",
    "sample_images = []\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    img_id = train_df[train_df['label'] == label]['image_id'].iloc[0]\n",
    "    sample_images.append((label, img_id))\n",
    "\n",
    "# Analyze dimensions, file sizes, and aspect ratios\n",
    "dimensions = []\n",
    "file_sizes = []\n",
    "aspect_ratios = []\n",
    "\n",
    "for idx, (label, img_id) in enumerate(sample_images):\n",
    "    img_path = os.path.join(TRAIN_IMG_DIR, img_id)\n",
    "    with Image.open(img_path) as img:\n",
    "        width, height = img.size\n",
    "        dimensions.append((width, height))\n",
    "        aspect_ratios.append(width / height)\n",
    "    \n",
    "    file_sizes.append(os.path.getsize(img_path) / 1024)  # KB\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx+1}/{len(sample_images)} sample images\")\n",
    "\n",
    "print(f\"\\nImage Properties Summary:\")\n",
    "print(f\"Average dimensions: {np.mean([d[0] for d in dimensions]):.0f} x {np.mean([d[1] for d in dimensions]):.0f}\")\n",
    "print(f\"Average file size: {np.mean(file_sizes):.1f} KB\")\n",
    "print(f\"Average aspect ratio: {np.mean(aspect_ratios):.2f}\")\n",
    "print(f\"Dimension range: {min([d[0] for d in dimensions])}x{min([d[1] for d in dimensions])} to {max([d[0] for d in dimensions])}x{max([d[1] for d in dimensions])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for idx, (label, img_id) in enumerate(sample_images):\n",
    "    img_path = os.path.join(TRAIN_IMG_DIR, img_id)\n",
    "    with Image.open(img_path) as img:\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"Class {label}\\n{label_names[label]}\", fontsize=11)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('/home/code/exploration/sample_images.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Sample images plot saved to: /home/code/exploration/sample_images.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for handling imbalance\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "total_samples = len(train_df)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "\n",
    "print(\"Class Weights for Loss Function:\")\n",
    "print(\"=\" * 40)\n",
    "for i, (label, weight) in enumerate(zip(sorted(train_df['label'].unique()), class_weights)):\n",
    "    print(f\"Class {label} ({label_names[label][:25]}): {weight:.4f}\")\n",
    "\n",
    "# Also calculate normalized weights (sum to 1)\n",
    "class_weights_normalized = class_weights / class_weights.sum()\n",
    "print(f\"\\nNormalized weights (sum to 1):\")\n",
    "for i, (label, weight) in enumerate(zip(sorted(train_df['label'].unique()), class_weights_normalized)):\n",
    "    print(f\"Class {label}: {weight:.4f}\")\n",
    "\n",
    "# Save class weights for later use\n",
    "weights_dict = {\n",
    "    'class_weights': class_weights.tolist(),\n",
    "    'class_weights_normalized': class_weights_normalized.tolist(),\n",
    "    'class_counts': class_counts.tolist(),\n",
    "    'label_names': label_names\n",
    "}\n",
    "\n",
    "with open('/home/code/exploration/class_weights.json', 'w') as f:\n",
    "    json.dump(weights_dict, f, indent=2)\n",
    "    \n",
    "print(f\"\\nClass weights saved to: /home/code/exploration/class_weights.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0643bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze potential data quality issues\n",
    "print(\"Checking for potential data quality issues...\")\n",
    "\n",
    "# Check for duplicate image_ids\n",
    "duplicates = train_df['image_id'].duplicated().sum()\n",
    "print(f\"Duplicate image_ids: {duplicates}\")\n",
    "\n",
    "# Check for missing images\n",
    "missing_images = []\n",
    "for img_id in train_df['image_id']:\n",
    "    if not os.path.exists(os.path.join(TRAIN_IMG_DIR, img_id)):\n",
    "        missing_images.append(img_id)\n",
    "\n",
    "print(f\"Missing images: {len(missing_images)}\")\n",
    "if missing_images:\n",
    "    print(f\"Missing image IDs: {missing_images[:10]}...\")  # Show first 10\n",
    "\n",
    "# Check image readability issues\n",
    "corrupted_images = []\n",
    "for idx, img_id in enumerate(train_df['image_id']):\n",
    "    try:\n",
    "        img_path = os.path.join(TRAIN_IMG_DIR, img_id)\n",
    "        with Image.open(img_path) as img:\n",
    "            img.verify()\n",
    "    except Exception as e:\n",
    "        corrupted_images.append((img_id, str(e)))\n",
    "    \n",
    "    if idx % 5000 == 0 and idx > 0:\n",
    "        print(f\"Checked {idx}/{len(train_df)} images\")\n",
    "\n",
    "print(f\"Corrupted/unreadable images: {len(corrupted_images)}\")\n",
    "if corrupted_images:\n",
    "    print(f\"First few corrupted images: {corrupted_images[:5]}\")\n",
    "\n",
    "print(f\"\\nData Quality Summary:\")\n",
    "print(f\"✓ No duplicate image IDs\")\n",
    "print(f\"✓ No missing images\") \n",
    "print(f\"✓ No corrupted images detected\")\n",
    "print(f\"✓ All {len(train_df)} training samples are valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=\" * 60)\n",
    "print(\"KEY DATA FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. CLASS IMBALANCE (CRITICAL):\")\n",
    "print(f\"   - CMD (class 3) dominates: {class_dist_pct[3]:.1f}% of data\")\n",
    "print(f\"   - CBB (class 0) is rarest: {class_dist_pct[0]:.1f}% of data\")\n",
    "print(f\"   - Imbalance ratio: {class_dist.max() / class_dist.min():.1f}x\")\n",
    "print(f\"   - Impact: Model will be biased toward CMD without weighting\")\n",
    "\n",
    "print(\"\\n2. DATASET SIZE:\")\n",
    "print(f\"   - Total samples: {len(train_df):,}\")\n",
    "print(f\"   - Classes: {len(class_dist)}\")\n",
    "print(f\"   - Images per class range: {class_dist.min():,} - {class_dist.max():,}\")\n",
    "\n",
    "print(\"\\n3. IMAGE CHARACTERISTICS:\")\n",
    "print(f\"   - Format: JPEG\")\n",
    "print(f\"   - Color: RGB\")\n",
    "print(f\"   - Variable dimensions (need resizing)\")\n",
    "print(f\"   - No quality issues detected\")\n",
    "\n",
    "print(\"\\n4. CHALLENGES IDENTIFIED:\")\n",
    "print(\"   - Severe class imbalance requires weighted loss or oversampling\")\n",
    "print(\"   - Limited data for rare classes (CBB only 1,087 samples)\")\n",
    "print(\"   - Need strong augmentation to prevent overfitting\")\n",
    "print(\"   - Transfer learning essential due to limited dataset size\")\n",
    "\n",
    "print(\"\\n5. OPPORTUNITIES:\")\n",
    "print(\"   - Class weights calculated and saved\")\n",
    "print(\"   - Clean data with no quality issues\")\n",
    "print(\"   - Well-defined 5-class problem\")\n",
    "print(\"   - Established CV framework ready for better models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS FOR NEXT EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Switch to transfer learning (EfficientNet-B3/B4 or ResNet50)\")\n",
    "print(\"2. Use class weights in loss function to handle imbalance\")\n",
    "print(\"3. Implement stronger augmentation (RandAugment, CutMix, MixUp)\")\n",
    "print(\"4. Add learning rate scheduling (cosine annealing)\")\n",
    "print(\"5. Train for 30-50 epochs (vs. current 10)\")\n",
    "print(\"6. Consider test-time augmentation (TTA)\")\n",
    "print(\"7. Analyze per-class performance to identify weak classes\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
