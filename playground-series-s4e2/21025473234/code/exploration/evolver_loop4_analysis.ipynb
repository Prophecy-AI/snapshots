{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50dc93af",
   "metadata": {},
   "source": [
    "# Evolver Loop 4 Analysis: Diagnosing Declining Scores & Winning Kernel Strategy\n",
    "\n",
    "This notebook analyzes why our scores are declining and extracts the full strategy from the winning kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6185cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:07:08.777592Z",
     "iopub.status.busy": "2026-01-15T15:07:08.777134Z",
     "iopub.status.idle": "2026-01-15T15:07:09.927878Z",
     "shell.execute_reply": "2026-01-15T15:07:09.927461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPERIMENT HISTORY ===\n",
      "exp_000: 0.9071 - 001_baseline_xgboost\n",
      "  Notes: Baseline XGBoost model with basic feature engineering. Features: BMI (critical), age groups (5 bins), simple interactions (Age*Height, Age*Weight). Categorical encoding with LabelEncoder. 5-fold strat...\n",
      "\n",
      "exp_001: 0.906 - exp_002_enhanced_features\n",
      "  Notes: Enhanced feature engineering with WHO_BMI_Categories (71.88% standalone accuracy), Weight_Height_Ratio, and lifestyle interactions (FCVC_NCP, CH2O_FAF, FAF_TUE). CRITICAL FIX: Replaced LabelEncoder wi...\n",
      "\n",
      "exp_002: 0.9052 - exp_003_winning_kernel_approach\n",
      "  Notes: Implemented winning kernel approach with MEstimateEncoder on 8 categorical features, BMI calculation, column rounding, and 9-fold CV. Used XGBoost with Optuna-tuned hyperparameters (n_estimators=131, ...\n",
      "\n",
      "\n",
      "Score Trend: 0.9071 → 0.9052\n",
      "Decline: 0.0019\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Load session state to understand experiment history\n",
    "session_file = Path('/home/code/session_state.json')\n",
    "with open(session_file, 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "print(\"=== EXPERIMENT HISTORY ===\")\n",
    "for exp in session_state['experiments']:\n",
    "    print(f\"{exp['id']}: {exp['score']} - {exp['name']}\")\n",
    "    print(f\"  Notes: {exp['notes'][:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nScore Trend: {session_state['experiments'][0]['score']} → {session_state['experiments'][-1]['score']}\")\n",
    "print(f\"Decline: {session_state['experiments'][0]['score'] - session_state['experiments'][-1]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce6608",
   "metadata": {},
   "source": [
    "## 1. Why Are Scores Declining?\n",
    "\n",
    "Let's analyze the differences between experiments to understand the decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fe54c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:07:09.929202Z",
     "iopub.status.busy": "2026-01-15T15:07:09.928965Z",
     "iopub.status.idle": "2026-01-15T15:07:09.956845Z",
     "shell.execute_reply": "2026-01-15T15:07:09.956474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUBMISSION COMPARISON ===\n",
      "\n",
      "Class distribution comparison:\n",
      "exp_000 (baseline):\n",
      "NObeyesdad\n",
      "Insufficient_Weight    1715\n",
      "Normal_Weight          2122\n",
      "Obesity_Type_I         2070\n",
      "Obesity_Type_II        2116\n",
      "Obesity_Type_III       2625\n",
      "Overweight_Level_I     1442\n",
      "Overweight_Level_II    1750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "exp_001 (enhanced features):\n",
      "NObeyesdad\n",
      "Insufficient_Weight    1716\n",
      "Normal_Weight          2110\n",
      "Obesity_Type_I         2063\n",
      "Obesity_Type_II        2122\n",
      "Obesity_Type_III       2623\n",
      "Overweight_Level_I     1457\n",
      "Overweight_Level_II    1749\n",
      "Name: count, dtype: int64\n",
      "\n",
      "exp_002 (MEstimateEncoder):\n",
      "NObeyesdad\n",
      "Insufficient_Weight    1726\n",
      "Normal_Weight          2117\n",
      "Obesity_Type_I         2066\n",
      "Obesity_Type_II        2112\n",
      "Obesity_Type_III       2625\n",
      "Overweight_Level_I     1424\n",
      "Overweight_Level_II    1770\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== PREDICTION OVERLAP ===\n",
      "exp_000 vs exp_001: 98.8% same predictions\n",
      "exp_001 vs exp_002: 97.0% same predictions\n",
      "exp_000 vs exp_002: 97.2% same predictions\n"
     ]
    }
   ],
   "source": [
    "# Load experiment details\n",
    "exp_000 = pd.read_csv('/home/code/submission_candidates/candidate_000.csv')\n",
    "exp_001 = pd.read_csv('/home/code/submission_candidates/candidate_001.csv')\n",
    "exp_002 = pd.read_csv('/home/code/submission_candidates/candidate_002.csv')\n",
    "\n",
    "print(\"=== SUBMISSION COMPARISON ===\")\n",
    "print(\"\\nClass distribution comparison:\")\n",
    "print(\"exp_000 (baseline):\")\n",
    "print(exp_000['NObeyesdad'].value_counts().sort_index())\n",
    "print(\"\\nexp_001 (enhanced features):\")\n",
    "print(exp_001['NObeyesdad'].value_counts().sort_index())\n",
    "print(\"\\nexp_002 (MEstimateEncoder):\")\n",
    "print(exp_002['NObeyesdad'].value_counts().sort_index())\n",
    "\n",
    "# Check if predictions are actually different\n",
    "print(f\"\\n=== PREDICTION OVERLAP ===\")\n",
    "print(f\"exp_000 vs exp_001: {(exp_000['NObeyesdad'] == exp_001['NObeyesdad']).mean():.1%} same predictions\")\n",
    "print(f\"exp_001 vs exp_002: {(exp_001['NObeyesdad'] == exp_002['NObeyesdad']).mean():.1%} same predictions\")\n",
    "print(f\"exp_000 vs exp_002: {(exp_000['NObeyesdad'] == exp_002['NObeyesdad']).mean():.1%} same predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba54b3c",
   "metadata": {},
   "source": [
    "## 2. Winning Kernel Deep Dive\n",
    "\n",
    "Let's extract the complete strategy from the winning kernel (0.92160 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65129010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:07:40.810932Z",
     "iopub.status.busy": "2026-01-15T15:07:40.810768Z",
     "iopub.status.idle": "2026-01-15T15:07:40.825354Z",
     "shell.execute_reply": "2026-01-15T15:07:40.824995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WINNING KERNEL STRATEGY ===\n",
      "Number of cells: 69\n",
      "\n",
      "Models used:\n",
      "  - Cell 6: LGBM Pipeline\n",
      "  - Cell 45: RandomForest Pipeline\n",
      "  - Cell 48: LGBM Pipeline\n",
      "  - Cell 52: LGBM Pipeline\n",
      "  - Cell 55: XGB Pipeline\n",
      "  - Cell 57: XGB Pipeline\n",
      "  - Cell 60: CatBoost Pipeline\n",
      "  - Cell 61: CatBoost Pipeline\n",
      "\n",
      "Preprocessing steps:\n",
      "  - Cell 6: MEstimateEncoder\n",
      "  - Cell 36: Age rounding\n",
      "  - Cell 45: MEstimateEncoder\n",
      "  - Cell 48: MEstimateEncoder\n",
      "  - Cell 55: MEstimateEncoder\n",
      "  - Cell 57: MEstimateEncoder\n",
      "  - Cell 60: MEstimateEncoder\n",
      "  - Cell 61: MEstimateEncoder\n",
      "\n",
      "Ensemble method: Cell 67: Weighted ensemble\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load winning kernel\n",
    "kernel_path = '/home/code/research/kernels/chinmayadatt_obesity-risk-prediction-multi-class-0-92160/obesity-risk-prediction-multi-class-0-92160.ipynb'\n",
    "with open(kernel_path, 'r') as f:\n",
    "    winning_kernel = json.load(f)\n",
    "\n",
    "print(\"=== WINNING KERNEL STRATEGY ===\")\n",
    "print(f\"Number of cells: {len(winning_kernel['cells'])}\")\n",
    "\n",
    "# Extract key components\n",
    "models_used = []\n",
    "preprocessing_steps = []\n",
    "ensemble_method = None\n",
    "\n",
    "for i, cell in enumerate(winning_kernel['cells']):\n",
    "    if cell['cell_type'] == 'code':\n",
    "        source = ''.join(cell['source'])\n",
    "        \n",
    "        # Identify models\n",
    "        if 'LGBMClassifier' in source and 'make_pipeline' in source:\n",
    "            models_used.append(f\"Cell {i}: LGBM Pipeline\")\n",
    "        elif 'XGBClassifier' in source and 'make_pipeline' in source:\n",
    "            models_used.append(f\"Cell {i}: XGB Pipeline\")\n",
    "        elif 'CatBoostClassifier' in source and 'make_pipeline' in source:\n",
    "            models_used.append(f\"Cell {i}: CatBoost Pipeline\")\n",
    "        elif 'RandomForestClassifier' in source and 'make_pipeline' in source:\n",
    "            models_used.append(f\"Cell {i}: RandomForest Pipeline\")\n",
    "            \n",
    "        # Identify preprocessing\n",
    "        if 'MEstimateEncoder' in source:\n",
    "            preprocessing_steps.append(f\"Cell {i}: MEstimateEncoder\")\n",
    "        elif 'age_rounder' in source:\n",
    "            preprocessing_steps.append(f\"Cell {i}: Age rounding\")\n",
    "        elif 'height_rounder' in source:\n",
    "            preprocessing_steps.append(f\"Cell {i}: Height rounding\")\n",
    "        elif 'extract_features' in source:\n",
    "            preprocessing_steps.append(f\"Cell {i}: BMI extraction\")\n",
    "            \n",
    "        # Identify ensemble\n",
    "        if 'weights' in source and 'predict_list' in source:\n",
    "            ensemble_method = f\"Cell {i}: Weighted ensemble\"\n",
    "\n",
    "print(\"\\nModels used:\")\n",
    "for model in models_used:\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "print(\"\\nPreprocessing steps:\")\n",
    "for step in preprocessing_steps:\n",
    "    print(f\"  - {step}\")\n",
    "\n",
    "print(f\"\\nEnsemble method: {ensemble_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc380ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:07:40.826361Z",
     "iopub.status.busy": "2026-01-15T15:07:40.826128Z",
     "iopub.status.idle": "2026-01-15T15:07:40.833915Z",
     "shell.execute_reply": "2026-01-15T15:07:40.833592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WINNING KERNEL ENSEMBLE WEIGHTS ===\n",
      "weights = {\"rfc_\":0,\n",
      "           \"lgbm_\":3,\n",
      "           \"xgb_\":1,\n",
      "           \"cat_\":0}\n",
      "    tmp[f\"{k}\"] = (weights['rfc_']*tmp[f\"rfc_{k}\"] +\n",
      "              weights['lgbm_']*tmp[f\"lgbm_{k}\"]+\n",
      "              weights['xgb_']*tmp[f\"xgb_{k}\"]+\n",
      "              weights['cat_']*tmp[f\"cat_{k}\"])    \n",
      "\n",
      "=== KEY INSIGHT ===\n",
      "The winning kernel uses:\n",
      "- 4 different models (RandomForest, LGBM, XGBoost, CatBoost)\n",
      "- Different preprocessing for each model\n",
      "- Simple weighted averaging (not complex stacking)\n",
      "- Weights: rfc=0, lgbm=3, xgb=1, cat=0 (LGBM dominant)\n",
      "- 9-fold CV (more stable than our 5-fold)\n"
     ]
    }
   ],
   "source": [
    "# Extract the actual weights used in winning kernel\n",
    "print(\"=== WINNING KERNEL ENSEMBLE WEIGHTS ===\")\n",
    "\n",
    "# Find the weights cell\n",
    "for i, cell in enumerate(winning_kernel['cells']):\n",
    "    if cell['cell_type'] == 'code':\n",
    "        source = ''.join(cell['source'])\n",
    "        if 'weights' in source and 'rfc_' in source:\n",
    "            lines = source.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'weights' in line and '=' in line:\n",
    "                    print(line)\n",
    "                elif 'rfc_' in line or 'lgbm_' in line or 'xgb_' in line or 'cat_' in line:\n",
    "                    print(line)\n",
    "            break\n",
    "\n",
    "print(\"\\n=== KEY INSIGHT ===\")\n",
    "print(\"The winning kernel uses:\")\n",
    "print(\"- 4 different models (RandomForest, LGBM, XGBoost, CatBoost)\")\n",
    "print(\"- Different preprocessing for each model\")\n",
    "print(\"- Simple weighted averaging (not complex stacking)\")\n",
    "print(\"- Weights: rfc=0, lgbm=3, xgb=1, cat=0 (LGBM dominant)\")\n",
    "print(\"- 9-fold CV (more stable than our 5-fold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec31de5",
   "metadata": {},
   "source": [
    "## 3. Critical Differences Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e56bf33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T15:07:40.834798Z",
     "iopub.status.busy": "2026-01-15T15:07:40.834625Z",
     "iopub.status.idle": "2026-01-15T15:07:40.838222Z",
     "shell.execute_reply": "2026-01-15T15:07:40.837922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CRITICAL DIFFERENCES: US vs WINNING KERNEL ===\n",
      "\n",
      "1. MODEL DIVERSITY:\n",
      "  US: Only XGBoost\n",
      "  Winning: RandomForest + LGBM + XGBoost + CatBoost\n",
      "  Impact: HIGH - Ensemble diversity is key\n",
      "\n",
      "2. PREPROCESSING VARIETY:\n",
      "  US: Same preprocessing for all experiments\n",
      "  Winning: Different preprocessing per model (some get rounded features, some don't)\n",
      "  Impact: MEDIUM-HIGH - Captures different feature representations\n",
      "\n",
      "3. ENSEMBLING:\n",
      "  US: None (single model)\n",
      "  Winning: Weighted averaging with tuned weights\n",
      "  Impact: HIGH - Simple ensembles often beat complex single models\n",
      "\n",
      "4. HYPERPARAMETER TUNING:\n",
      "  US: Basic parameters or limited Optuna\n",
      "  Winning: Extensive Optuna tuning for each model\n",
      "  Impact: MEDIUM - Diminishing returns but still important\n",
      "\n",
      "5. CROSS-VALIDATION:\n",
      "  US: 5-fold\n",
      "  Winning: 9-fold (more stable estimates)\n",
      "  Impact: LOW-MEDIUM - More folds = more stable CV\n",
      "\n",
      "=== CONCLUSION ===\n",
      "The winning kernel's success comes from:\n",
      "1. Model diversity (4 different algorithms)\n",
      "2. Simple but effective ensembling\n",
      "3. Different preprocessing per model\n",
      "4. NOT just from MEstimateEncoder alone\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CRITICAL DIFFERENCES: US vs WINNING KERNEL ===\")\n",
    "\n",
    "print(\"\\n1. MODEL DIVERSITY:\")\n",
    "print(\"  US: Only XGBoost\")\n",
    "print(\"  Winning: RandomForest + LGBM + XGBoost + CatBoost\")\n",
    "print(\"  Impact: HIGH - Ensemble diversity is key\")\n",
    "\n",
    "print(\"\\n2. PREPROCESSING VARIETY:\")\n",
    "print(\"  US: Same preprocessing for all experiments\")\n",
    "print(\"  Winning: Different preprocessing per model (some get rounded features, some don't)\")\n",
    "print(\"  Impact: MEDIUM-HIGH - Captures different feature representations\")\n",
    "\n",
    "print(\"\\n3. ENSEMBLING:\")\n",
    "print(\"  US: None (single model)\")\n",
    "print(\"  Winning: Weighted averaging with tuned weights\")\n",
    "print(\"  Impact: HIGH - Simple ensembles often beat complex single models\")\n",
    "\n",
    "print(\"\\n4. HYPERPARAMETER TUNING:\")\n",
    "print(\"  US: Basic parameters or limited Optuna\")\n",
    "print(\"  Winning: Extensive Optuna tuning for each model\")\n",
    "print(\"  Impact: MEDIUM - Diminishing returns but still important\")\n",
    "\n",
    "print(\"\\n5. CROSS-VALIDATION:\")\n",
    "print(\"  US: 5-fold\")\n",
    "print(\"  Winning: 9-fold (more stable estimates)\")\n",
    "print(\"  Impact: LOW-MEDIUM - More folds = more stable CV\")\n",
    "\n",
    "print(\"\\n=== CONCLUSION ===\")\n",
    "print(\"The winning kernel's success comes from:\")\n",
    "print(\"1. Model diversity (4 different algorithms)\")\n",
    "print(\"2. Simple but effective ensembling\")\n",
    "print(\"3. Different preprocessing per model\")\n",
    "print(\"4. NOT just from MEstimateEncoder alone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221294c",
   "metadata": {},
   "source": [
    "## 4. Why MEstimateEncoder Hurt Our Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MESTIMATEENCODER ANALYSIS ===\")\n",
    "\n",
    "print(\"\\nHypothesis 1: Overfitting to training data\")\n",
    "print(\"- MEstimateEncoder creates target-derived features\")\n",
    "print(\"- With our enhanced features (WHO_BMI_Categories), we're double-encoding target signal\")\n",
    "print(\"- This causes overfitting → worse generalization\")\n",
    "\n",
    "print(\"\\nHypothesis 2: Implementation differences\")\n",
    "print(\"- Winning kernel uses MEstimateEncoder WITHOUT our enhanced features\")\n",
    "print(\"- They use it on raw data with different model architectures\")\n",
    "print(\"- Context matters - encoder alone doesn't guarantee improvement\")\n",
    "\n",
    "print(\"\\nHypothesis 3: CV leakage\")\n",
    "print(\"- If MEstimateEncoder is fit outside CV folds, it leaks target information\")\n",
    "print(\"- Our implementation uses ColumnTransformer, which should prevent this\")\n",
    "print(\"- But need to verify proper folding\")\n",
    "\n",
    "print(\"\\nHypothesis 4: Diminishing returns\")\n",
    "print(\"- WHO_BMI_Categories already captures 71.88% of target signal\")\n",
    "print(\"- Adding MEstimateEncoder on top provides redundant information\")\n",
    "print(\"- More features ≠ better performance if they're correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcf5da",
   "metadata": {},
   "source": [
    "## 5. Recommendations for Next Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RECOMMENDATIONS ===\")\n",
    "\n",
    "print(\"\\nIMMEDIATE ACTIONS:\")\n",
    "print(\"1. SUBMIT exp_003 for LB calibration (urgent)\")\n",
    "print(\"   - Need to understand CV-LB gap\")\n",
    "print(\"   - Will reveal if our CV is optimistic/pessimistic\")\n",
    "print(\"   - Critical for diagnosing the decline\")\n",
    "\n",
    "print(\"\\n2. FIX THE DECLINING SCORE TREND:\")\n",
    "print(\"   - Revert to exp_001 approach (enhanced features + OrdinalEncoder)\")\n",
    "print(\"   - That gave 0.9060, better than current 0.9052\")\n",
    "print(\"   - Build from stable baseline instead of declining one\")\n",
    "\n",
    "print(\"\\n3. IMPLEMENT PROPER ENSEMBLING:\")\n",
    "print(\"   - Add LGBM model (winning kernel's top performer)\")\n",
    "print(\"   - Add CatBoost model (handles categoricals natively)\")\n",
    "print(\"   - Use simple weighted averaging like winning kernel\")\n",
    "print(\"   - Start with weights: lgbm=3, xgb=1 (based on winning kernel)\")\n",
    "\n",
    "print(\"\\n4. MODEL-SPECIFIC PREPROCESSING:\")\n",
    "print(\"   - XGBoost: Use enhanced features (BMI, interactions)\")\n",
    "print(\"   - LGBM: Use raw features + MEstimateEncoder\")\n",
    "print(\"   - CatBoost: Use raw features (handles categoricals natively)\")\n",
    "print(\"   - This creates diversity like winning kernel\")\n",
    "\n",
    "print(\"\\n5. INCREASE CV FOLDS:\")\n",
    "print(\"   - Move from 5-fold to 9-fold (like winning kernel)\")\n",
    "print(\"   - More stable CV estimates\")\n",
    "print(\"   - Better hyperparameter selection\")\n",
    "\n",
    "print(\"\\n6. HYPERPARAMETER TUNING:\")\n",
    "print(\"   - Run Optuna for LGBM and CatBoost\")\n",
    "print(\"   - Use winning kernel's parameters as starting points\")\n",
    "print(\"   - Focus on key params: learning_rate, max_depth, subsample\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
