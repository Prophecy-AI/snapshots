{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d47c3e7",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis: MEstimateEncoder Investigation\n",
    "\n",
    "This notebook analyzes the winning kernel's use of MEstimateEncoder and prepares recommendations for the next experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from category_encoders import MEstimateEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/code/data/train.csv')\n",
    "test = pd.read_csv('/home/code/data/test.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['NObeyesdad'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ed786",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from category_encoders import MEstimateEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['NObeyesdad'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', \n",
    "                   'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "\n",
    "# Analyze cardinality and target relationship\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Cardinality: {train[col].nunique()}\")\n",
    "    print(f\"  Categories: {train[col].unique()}\")\n",
    "    \n",
    "    # Calculate target distribution per category\n",
    "    target_dist = pd.crosstab(train[col], train['NObeyesdad'], normalize='index')\n",
    "    print(f\"  Most dominant class per category:\")\n",
    "    dominant = target_dist.idxmax(axis=1)\n",
    "    for cat in train[col].unique():\n",
    "        if pd.notna(cat):\n",
    "            max_prob = target_dist.loc[cat].max()\n",
    "            print(f\"    {cat}: {dominant[cat]} ({max_prob:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31805e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate chi-square statistics to measure feature importance\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def calculate_chi_square(feature, target):\n",
    "    \"\"\"Calculate chi-square statistic for categorical feature vs target\"\"\"\n",
    "    contingency_table = pd.crosstab(feature, target)\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    return chi2, p_value\n",
    "\n",
    "print(\"Chi-square statistics for categorical features:\")\n",
    "chi2_results = []\n",
    "for col in categorical_cols:\n",
    "    chi2, p_value = calculate_chi_square(train[col], train['NObeyesdad'])\n",
    "    chi2_results.append((col, chi2, p_value))\n",
    "    print(f\"{col:30s}: chi2={chi2:10.2f}, p_value={p_value:.2e}\")\n",
    "\n",
    "# Sort by chi-square value\n",
    "chi2_results.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nFeatures sorted by chi-square (most important first):\")\n",
    "for col, chi2, p_value in chi2_results:\n",
    "    print(f\"{col:30s}: {chi2:10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MEstimateEncoder performance vs OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define features\n",
    "feature_cols = [col for col in train.columns if col != 'NObeyesdad']\n",
    "X = train[feature_cols]\n",
    "y = train['NObeyesdad']\n",
    "\n",
    "# Test different encoding strategies\n",
    "def test_encoding(encoder, encoder_name):\n",
    "    \"\"\"Test encoding strategy with XGBoost\"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('encoder', encoder),\n",
    "        ('xgb', XGBClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=500,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    print(f\"{encoder_name}:\")\n",
    "    print(f\"  Mean accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print(f\"  Fold scores: {scores}\")\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Test OrdinalEncoder (current approach)\n",
    "ordinal_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), \n",
    "         categorical_cols)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "ordinal_mean, ordinal_std = test_encoding(ordinal_encoder, \"OrdinalEncoder (current)\")\n",
    "\n",
    "# Test MEstimateEncoder\n",
    "mestimate_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', MEstimateEncoder(cols=categorical_cols), \n",
    "         categorical_cols)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "mestimate_mean, mestimate_std = test_encoding(mestimate_encoder, \"MEstimateEncoder\")\n",
    "\n",
    "print(f\"\\nImprovement: {mestimate_mean - ordinal_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c49651",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on this analysis and the winning kernel review:\n",
    "\n",
    "1. **MEstimateEncoder is superior**: The winning kernel achieved 0.92160 using MEstimateEncoder vs our 0.906 with OrdinalEncoder\n",
    "2. **Target encoding captures relationships**: MEstimateEncoder converts categories to target probabilities, preserving the relationship between categories and target\n",
    "3. **Appropriate features**: The kernel used MEstimateEncoder for 8 categorical features with moderate cardinality (2-6 categories each)\n",
    "4. **Must prevent leakage**: MEstimateEncoder must be fit within CV folds, which our ColumnTransformer approach handles correctly\n",
    "\n",
    "## Recommendations for Next Experiment\n",
    "\n",
    "1. **Replace OrdinalEncoder with MEstimateEncoder** for the 8 categorical features\n",
    "2. **Keep enhanced features**: WHO_BMI_Categories, Weight_Height_Ratio, lifestyle interactions\n",
    "3. **Test both encoders**: Run comparison to validate improvement\n",
    "4. **Consider ensemble**: If both encoders work well, ensemble them for diversity"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
