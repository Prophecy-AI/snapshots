# Multi-Class Classification: Obesity Risk Prediction

## Problem Type
Multi-class classification with 7 target classes. Evaluation metric is Accuracy Score.

## Data Understanding
**Reference notebooks for data characteristics:**
- `eda.ipynb` - Contains full EDA: feature distributions, target distribution (7 classes, relatively balanced 12-19% each), feature types (mixed categorical/numerical), no missing values
- Use findings from these notebooks when implementing features

## Key Datasets
- **Competition dataset**: Primary training/test data (synthetic but based on real obesity dataset)
- **Original dataset**: "Obesity or CVD risk" dataset can be incorporated to improve performance
- **Strategy**: Append original data to training set with appropriate weighting/multiplier

## Feature Engineering (Critical for Performance)

### Essential Features
1. **BMI Calculation**: `Weight / (Height ** 2)` - highly important feature
2. **Age Groups**: Bin age into categories (e.g., [0-18, 19-30, 31-45, 46-60, 60+])
3. **Interaction Terms**: `Age * Gender`, polynomial features for Age and BMI
4. **Healthy Lifestyle Score**: Create composite features based on domain knowledge
   - No smoking indicator
   - Exercise indicator (FAF >= 1)
   - Alcohol consumption indicator
   - Healthy diet indicator

### Feature Engineering Approaches
- **Polynomial Features**: Degree 2 for Age and BMI (Age², BMI², Age*BMI)
- **Log Transformations**: Log1p for Age to handle skewness
- **Scaling**: MinMaxScaler for numerical features
- **Categorical Encoding**: Multiple strategies work - one-hot encoding, target encoding

## Models and Frameworks

### Primary Models (Most Successful)
1. **XGBoost** - Consistently strong performer, excellent for stacking meta-learner
2. **LightGBM** - Fast training, good accuracy
3. **CatBoost** - Handles categorical features well
4. **AutoGluon** - Automated ensemble with multiple models

### Model Configurations
- **XGBoost**: Use log_loss for training (even though competition metric is accuracy)
- **LightGBM**: 5-fold CV typically performs better than 10-fold
- **CatBoost**: Can handle categorical features directly
- **AutoGluon**: Zero-shot HPO training, ensemble weights automatically optimized

## Ensembling Strategies

### Stacking (Highly Recommended)
- Use XGBoost as meta-learner
- Base models: XGBoost, LightGBM, CatBoost, AutoGluon predictions
- Add model predictions as features to training data
- Train meta-model on these stacked features

### Voting/Averaging
- Soft voting classifier with optimized weights
- Simple averaging of diverse model predictions
- Weight optimization crucial for performance gains

### Model Diversity
- Include 3-5 diverse models (different algorithms, different feature sets)
- Combine tree-based models with different configurations
- Consider both competition-only and competition+original data models

## Validation Strategy

### Cross-Validation
- **Stratified K-Fold**: Essential for multi-class with imbalanced targets
- **5-fold CV**: Good balance between variance and computational cost
- **10-fold CV**: More stable but computationally expensive
- **20-fold CV**: Used by top solutions but diminishing returns

### Key Principle
- **Trust CV over LB**: Especially important for Playground series with synthetic data
- Consistent train/test distribution means CV is reliable indicator
- LB can be misleading due to different test set distribution

## Advanced Techniques

### Threshold Optimization
- Optimize class thresholds post-prediction
- Can provide significant accuracy improvements
- Use saved probabilities per class for optimization

### Pseudo-Labeling
- Train on competition data, predict on test set
- Add high-confidence predictions to training data
- Retrain model with augmented dataset

### Data Augmentation
- Append original dataset multiple times (multiplier optimization)
- Best multiplier varies by model (1x for XGB, 4x for LGBM reported)
- Remove duplicates after concatenation

## Preprocessing Pipeline

### Categorical Features
- Gender, family_history_with_overweight, FAVC, CAEC, SMOKE, SCC, CALC, MTRANS
- Options: One-hot encoding, target encoding, CatBoost native handling
- One-hot encoding generally safe and effective

### Numerical Features
- Age, Height, Weight, FCVC, NCP, CH2O, FAF, TUE
- Scaling important for neural networks
- Log transforms for skewed distributions (Age)

## Implementation Notes

### Feature Consistency
- Ensure identical feature engineering for train and test
- Use same scaler/transformer for both datasets
- Align categorical encoding between datasets

### Computational Considerations
- LightGBM faster than XGBoost for large datasets
- AutoGluon can be slow but provides strong ensembles
- Polynomial features increase dimensionality significantly

### Performance Optimization
- Focus on CV score improvement rather than LB
- Experiment tracking (MLflow, etc.) helps parameter management
- Ensemble diversity more important than individual model perfection

## Common Pitfalls to Avoid
- Don't overfit to public LB
- Don't ignore the original dataset
- Don't skip feature engineering - BMI and age features are critical
- Don't use too many folds without computational justification
- Don't forget to align features between train and test sets