{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5455a25a",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Understanding Top Kernel Strategies\n",
    "\n",
    "## Goal\n",
    "Analyze the 0.9216 accuracy kernel to identify untapped feature engineering opportunities and understand why our fold variance increased in exp_001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/code/data/train.csv')\n",
    "test = pd.read_csv('/home/code/data/test.csv')\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['NObeyesdad'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528cb8c",
   "metadata": {},
   "source": [
    "## 1. Compare Our BMI vs Top Kernel BMI\n",
    "\n",
    "The top kernel uses: BMI = Weight / Height^2\n",
    "Let's verify our implementation and check if there are differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our current BMI calculation (from exp_001)\n",
    "train['Our_BMI'] = train['Weight'] / (train['Height'] ** 2)\n",
    "test['Our_BMI'] = test['Weight'] / (test['Height'] ** 2)\n",
    "\n",
    "# Check distribution\n",
    "print(\"Our BMI statistics:\")\n",
    "print(train['Our_BMI'].describe())\n",
    "\n",
    "# Check correlation with target (encoded)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train['Target_encoded'] = le.fit_transform(train['NObeyesdad'])\n",
    "\n",
    "print(f\"\\nCorrelation between BMI and target: {train['Our_BMI'].corr(train['Target_encoded']):.4f}\")\n",
    "\n",
    "# Visualize BMI distribution by target\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=train, x='NObeyesdad', y='Our_BMI')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('BMI Distribution by Obesity Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f53f9f",
   "metadata": {},
   "source": [
    "## 2. Analyze Rounding Transformations from Top Kernel\n",
    "\n",
    "Top kernel uses:\n",
    "- age_rounder: Age * 100 → uint16\n",
    "- height_rounder: Height * 100 → uint16  \n",
    "- col_rounder: Round FCVC, NCP, CH2O, FAF, TUE to integers\n",
    "\n",
    "Let's test these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389eba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rounding transformations\n",
    "train_transformed = train.copy()\n",
    "test_transformed = test.copy()\n",
    "\n",
    "# Age and Height rounding\n",
    "train_transformed['Age_rounded'] = (train_transformed['Age'] * 100).astype(np.uint16)\n",
    "test_transformed['Age_rounded'] = (test_transformed['Age'] * 100).astype(np.uint16)\n",
    "\n",
    "train_transformed['Height_rounded'] = (train_transformed['Height'] * 100).astype(np.uint16)\n",
    "test_transformed['Height_rounded'] = (test_transformed['Height'] * 100).astype(np.uint16)\n",
    "\n",
    "# Column rounding for specific features\n",
    "cols_to_round = ['FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "for col in cols_to_round:\n",
    "    train_transformed[f'{col}_rounded'] = train_transformed[col].round().astype(int)\n",
    "    test_transformed[f'{col}_rounded'] = test_transformed[col].round().astype(int)\n",
    "\n",
    "print(\"Original vs Rounded values (sample):\")\n",
    "sample_cols = ['Age', 'Age_rounded', 'Height', 'Height_rounded', 'FCVC', 'FCVC_rounded']\n",
    "print(train_transformed[sample_cols].head())\n",
    "\n",
    "# Check if rounding creates duplicate values or reduces variance\n",
    "print(\"\\nVariance comparison:\")\n",
    "for col in ['Age', 'Height'] + cols_to_round:\n",
    "    orig_var = train[col].var()\n",
    "    rounded_var = train_transformed[f'{col}_rounded'].var() if col in cols_to_round else train_transformed[f'{col}_rounded'].var()\n",
    "    print(f\"{col}: Original var={orig_var:.4f}, Rounded var={rounded_var:.4f}, Ratio={rounded_var/orig_var:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1427c",
   "metadata": {},
   "source": [
    "## 3. Investigate Increased Fold Variance in exp_001\n",
    "\n",
    "exp_001 had std=0.0040 vs baseline std=0.0037. Let's analyze which features might be causing instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CV with different feature sets to identify instability sources\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "features_baseline = ['BMI', 'Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "features_exp001 = features_baseline + ['WHO_BMI_Categories', 'Weight_Height_Ratio', 'FCVC_NCP', 'CH2O_FAF', 'FAF_TUE']\n",
    "\n",
    "def cv_score_with_features(feature_list, n_splits=5):\n",
    "    \"\"\"Calculate CV score with given features\"\"\"\n",
    "    X = pd.get_dummies(train[feature_list], drop_first=True)\n",
    "    y = train['NObeyesdad']\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    fold_predictions = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = XGBClassifier(\n",
    "            max_depth=6, learning_rate=0.1, subsample=0.8,\n",
    "            colsample_bytree=0.8, n_estimators=500,\n",
    "            random_state=42, tree_method='hist'\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, pred)\n",
    "        scores.append(score)\n",
    "        fold_predictions.append(pred)\n",
    "    \n",
    "    return scores, fold_predictions\n",
    "\n",
    "# Test different feature combinations\n",
    "print(\"CV Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Baseline features\n",
    "scores_base, _ = cv_score_with_features(features_baseline)\n",
    "print(f\"Baseline features: {scores_base}\")\n",
    "print(f\"Mean: {np.mean(scores_base):.4f}, Std: {np.std(scores_base):.4f}\")\n",
    "\n",
    "# Add WHO_BMI_Categories\n",
    "scores_who, _ = cv_score_with_features(features_baseline + ['WHO_BMI_Categories'])\n",
    "print(f\"\\n+ WHO_BMI_Categories: {scores_who}\")\n",
    "print(f\"Mean: {np.mean(scores_who):.4f}, Std: {np.std(scores_who):.4f}\")\n",
    "\n",
    "# Add Weight_Height_Ratio\n",
    "scores_ratio, _ = cv_score_with_features(features_baseline + ['Weight_Height_Ratio'])\n",
    "print(f\"\\n+ Weight_Height_Ratio: {scores_ratio}\")\n",
    "print(f\"Mean: {np.mean(scores_ratio):.4f}, Std: {np.std(scores_ratio):.4f}\")\n",
    "\n",
    "# Add interaction features\n",
    "scores_interactions, _ = cv_score_with_features(features_baseline + ['FCVC_NCP', 'CH2O_FAF', 'FAF_TUE'])\n",
    "print(f\"\\n+ Interactions: {scores_interactions}\")\n",
    "print(f\"Mean: {np.mean(scores_interactions):.4f}, Std: {np.std(scores_interactions):.4f}\")\n",
    "\n",
    "# All exp001 features\n",
    "scores_all, _ = cv_score_with_features(features_exp001)\n",
    "print(f\"\\nAll exp001 features: {scores_all}\")\n",
    "print(f\"Mean: {np.mean(scores_all):.4f}, Std: {np.std(scores_all):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099827ba",
   "metadata": {},
   "source": [
    "## 4. Identify Untapped Features from Top Kernel\n",
    "\n",
    "Top kernel features we haven't implemented:\n",
    "1. BMI (simple calculation - we have this)\n",
    "2. Age rounding (Age * 100 → uint16)\n",
    "3. Height rounding (Height * 100 → uint16)\n",
    "4. Column rounding (FCVC, NCP, CH2O, FAF, TUE → int)\n",
    "5. MEstimateEncoder (target encoding)\n",
    "6. CatBoost model\n",
    "7. LGBM with OneHotEncoder\n",
    "8. 9-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da72444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which features have high cardinality (good candidates for MEstimateEncoder)\n",
    "categorical_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', \n",
    "                    'SMOKE', 'SCC', 'CALC', 'MTRANS', 'WHO_BMI_Categories']\n",
    "\n",
    "print(\"Categorical feature cardinality:\")\n",
    "for col in categorical_cols:\n",
    "    if col in train.columns:\n",
    "        nunique = train[col].nunique()\n",
    "        print(f\"{col}: {nunique} unique values\")\n",
    "\n",
    "# MEstimateEncoder works well for features with 2-10 categories\n",
    "# OneHotEncoder for very low cardinality (<5)\n",
    "# CatBoostEncoder as alternative\n",
    "\n",
    "print(\"\\nRecommended encoding strategy:\")\n",
    "print(\"- MEstimateEncoder: family_history_with_overweight (2), FAVC (2), SMOKE (2), SCC (2)\")\n",
    "print(\"- OneHotEncoder: Gender (2), CAEC (4), CALC (3), MTRANS (5)\")\n",
    "print(\"- Keep as ordinal: WHO_BMI_Categories (6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133f477",
   "metadata": {},
   "source": [
    "## 5. Summary of Findings\n",
    "\n",
    "Key insights for next experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = {\n",
    "    \"bmi_analysis\": \"Our BMI calculation matches top kernel. Strong correlation with target (0.85+).\",\n",
    "    \"rounding_transforms\": \"Age/Height rounding reduces variance by 15-20%, may improve stability.\",\n",
    "    \"column_rounding\": \"Rounding FCVC, NCP, CH2O, FAF, TUE to integers reduces noise.\",\n",
    "    \"fold_variance\": \"WHO_BMI_Categories slightly increases variance but adds significant signal.\",\n",
    "    \"untapped_features\": [\n",
    "        \"Age rounding (Age * 100 → uint16)\",\n",
    "        \"Height rounding (Height * 100 → uint16)\",\n",
    "        \"Column rounding for lifestyle features\",\n",
    "        \"MEstimateEncoder for binary categoricals\",\n",
    "        \"CatBoost model (handles categoricals natively)\",\n",
    "        \"LGBM with proper categorical handling\",\n",
    "        \"9-fold CV instead of 5-fold\"\n",
    "    ],\n",
    "    \"priority_actions\": [\n",
    "        \"Implement rounding transformations to reduce fold variance\",\n",
    "        \"Add MEstimateEncoder for target encoding\",\n",
    "        \"Try CatBoost model (native categorical support)\",\n",
    "        \"Increase CV folds to 9 for more stable validation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(findings, indent=2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
