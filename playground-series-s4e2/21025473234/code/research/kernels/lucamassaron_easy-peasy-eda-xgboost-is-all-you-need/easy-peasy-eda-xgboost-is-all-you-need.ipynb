{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":68479,"databundleVersionId":7609535,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction to EDA/Modelling for Multi-Class Prediction of Obesity Risk**","metadata":{}},{"cell_type":"markdown","source":"![easy_peasy_EDA](https://th.bing.com/th/id/OIG4.Z3X2KIgpwXplIxzX6Z90?pid=ImgGn)","metadata":{}},{"cell_type":"markdown","source":"The notebook has the **Multiclass Classification with a Obesity Risk competition, Playground Season-4 Episode-2**, as object.","metadata":{}},{"cell_type":"markdown","source":"The notebook is structured as a series of cells and comments, from data exploration to building and optimizing a first starter model using XGBoost. By following it you can understabnd both the data problem and grasp the actions to be taken with similar tabular data problems.\n\nThe approach is to explore data using the essential commands for tabular and chart representations. Often, on Kaggle Notebooks for competitions, EDA is weighted with tons of charts and plots that you cannot understand what they serve for, based on the principle \"The More The Merrier\". The idea is instead to just go to the point and show the relevant plots and keyt understanding in a dataset that could help you to devise the better strategies and feature engineering.\n\nThe modelling part is also striving to just do the essential, hence you will find a single model devised to show how far you can go with the state of the art in tabular data modelling, i.e. XGBoost.","metadata":{}},{"cell_type":"markdown","source":"The notebook is arrange into six parts, with sections:\n\n1. Data loading and first exploration\n2. Target analysis\n3. EDA and data preparation\n4. Modelling\n5. Explainability\n6. Preparation of the submission","metadata":{}},{"cell_type":"markdown","source":"Most of the used approaches / code snippets can be found on the books:\n\n1. The Kaggle Book by Konrad Banachewicz and Luca Massaron\n2. The Kaggle Workbook by Konrad Banachewicz and Luca Massaron\n3. Developing Kaggle Notebooks by Gabriel Preda\n\nfor a wider discussion of the topics of EDA and modelling just refer to these books","metadata":{}},{"cell_type":"markdown","source":"## 1.0 Data loading and first exploration","metadata":{}},{"cell_type":"markdown","source":"First, the basic Python packages (more will be loaded later, when needed)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:50.485296Z","iopub.execute_input":"2024-02-26T07:24:50.485655Z","iopub.status.idle":"2024-02-26T07:24:51.414455Z","shell.execute_reply.started":"2024-02-26T07:24:50.485627Z","shell.execute_reply":"2024-02-26T07:24:51.413733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the data and set the id as index","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s4e2/train.csv\").set_index(\"id\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s4e2/test.csv\").set_index(\"id\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.416262Z","iopub.execute_input":"2024-02-26T07:24:51.416816Z","iopub.status.idle":"2024-02-26T07:24:51.608486Z","shell.execute_reply.started":"2024-02-26T07:24:51.416783Z","shell.execute_reply":"2024-02-26T07:24:51.607441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.609792Z","iopub.execute_input":"2024-02-26T07:24:51.610605Z","iopub.status.idle":"2024-02-26T07:24:51.617624Z","shell.execute_reply.started":"2024-02-26T07:24:51.610569Z","shell.execute_reply":"2024-02-26T07:24:51.6168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5).T","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.620331Z","iopub.execute_input":"2024-02-26T07:24:51.620952Z","iopub.status.idle":"2024-02-26T07:24:51.639415Z","shell.execute_reply.started":"2024-02-26T07:24:51.620922Z","shell.execute_reply":"2024-02-26T07:24:51.63846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(5).T","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.640611Z","iopub.execute_input":"2024-02-26T07:24:51.640961Z","iopub.status.idle":"2024-02-26T07:24:51.654737Z","shell.execute_reply.started":"2024-02-26T07:24:51.640929Z","shell.execute_reply":"2024-02-26T07:24:51.653746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset seems similar to \"Obesity based on eating habits & physical cond\" (https://www.kaggle.com/datasets/ankurbajaj9/obesity-levels) which has been derived from:\n\nPalechor, Fabio Mendoza, and Alexis de la Hoz Manotas. \"Dataset for estimation of obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico.\" Data in brief 25 (2019): 104344.\n(https://www.sciencedirect.com/science/article/pii/S2352340919306985)\n\nThe research paper introduces a dataset designed for estimating obesity levels in individuals across Mexico, Peru, and Colombia. The estimation relies on factors such as eating habits and physical condition. The dataset encompasses 17 attributes and 2111 records, each labeled with the class variable NObesity (Obesity Level). This variable facilitates the classification of data into categories including Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II, and Obesity Type III.\n\nOf the total data, 77% was synthetically generated using the Weka tool and the SMOTE filter, while the remaining 23% was directly collected from users through a web platform. The dataset serves as a valuable resource for developing intelligent computational tools capable of identifying an individual's obesity level. Additionally, it can be utilized for constructing recommender systems aimed at monitoring and addressing obesity levels. ","metadata":{}},{"cell_type":"markdown","source":"In particular we can deduct that:\n* 'Gender', 'Age', 'Height', 'Weight' are physical attributes\n* 'family_history_with_overweight' points out to familiarity with obesity\n* FAVC = Frequent consumption of high caloric food\n* FCVC = Frequency of consumption of vegetables\n* NCP = Number of main meals\n* CAEC = Consumption of food between meals\n* SMOKE = tobacco usage\n* CH20 = Consumption of water daily\n* SCC = Calories consumption monitoring\n* FAF = Physical activity frequency\n* TUE = Time using technology devices\n* CALC = Consumption of alcohol\n* MTRANS = Transportation used","metadata":{}},{"cell_type":"markdown","source":"The target is estracted from the train","metadata":{}},{"cell_type":"code","source":"target = train[\"NObeyesdad\"]\ntrain = train.drop(\"NObeyesdad\", axis=\"columns\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.655801Z","iopub.execute_input":"2024-02-26T07:24:51.656035Z","iopub.status.idle":"2024-02-26T07:24:51.669949Z","shell.execute_reply.started":"2024-02-26T07:24:51.656014Z","shell.execute_reply":"2024-02-26T07:24:51.669174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.0 Target analysis","metadata":{}},{"cell_type":"markdown","source":"By examining the distribution of the target we can figure that: \n\n1. it is a multiclass problem, with 7 class\n2. the classes are distributed differently but there are not extreme differences in their proportions (expected probability)\n3. However, the most frequent class (Obesity_Type_III) has almost the double of probability of the least frequent one (Overweight_Level_I), hence, when defining the cross-validation strategy when should consider a target stratified one thus such different prior probabilities will be reflected also in the test set as much exactly as possible.","metadata":{}},{"cell_type":"code","source":"target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.670981Z","iopub.execute_input":"2024-02-26T07:24:51.671281Z","iopub.status.idle":"2024-02-26T07:24:51.683313Z","shell.execute_reply.started":"2024-02-26T07:24:51.671257Z","shell.execute_reply":"2024-02-26T07:24:51.682449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target.value_counts() / len(target)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.684379Z","iopub.execute_input":"2024-02-26T07:24:51.684655Z","iopub.status.idle":"2024-02-26T07:24:51.69931Z","shell.execute_reply.started":"2024-02-26T07:24:51.684633Z","shell.execute_reply":"2024-02-26T07:24:51.698447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we represent the situation with a plot, ordering the classes based on overweight","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\n\ncolor_list = [\"#A5D7E8\", \"#576CBC\", \"#19376D\", \"#0B2447\"]\ncmap_custom = ListedColormap(color_list)\n\nsorted_labels = ['Insufficient_Weight', 'Normal_Weight', \n 'Overweight_Level_I', 'Overweight_Level_II', \n 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\n\nplt.figure(figsize=(8, 4))\nax = sns.countplot(x=target, order=sorted_labels, palette=color_list)\n\nplt.title('Distribution of Obesity Risk Levels')\nplt.xlabel('Obesity Risk Levels')\nplt.ylabel('Count')\n\nax.set_xticklabels(ax.get_xticklabels(), rotation=10, ha='right', fontsize=8)\nplt.tight_layout() \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:51.700516Z","iopub.execute_input":"2024-02-26T07:24:51.700765Z","iopub.status.idle":"2024-02-26T07:24:53.17606Z","shell.execute_reply.started":"2024-02-26T07:24:51.700744Z","shell.execute_reply":"2024-02-26T07:24:53.175044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for modelling purposes, we convert the target, expressed by string labels, into numeric labels, using a custom implementation of Scikit-learn LabelEncoder that orders the encoded label based on the order we proposed before.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CustomLabelEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, sorted_labels):\n        self.classes_ = sorted_labels\n        self.label_to_numeric_mapping = None\n\n    def fit(self, y):\n        self.label_to_numeric_mapping = {label: numeric for numeric, label in enumerate(self.classes_)}\n        return self\n\n    def transform(self, y):\n        if self.label_to_numeric_mapping is None:\n            raise ValueError(\"fit method must be called before transform\")\n        return y.map(self.label_to_numeric_mapping)\n\n    def inverse_transform(self, y):\n        if self.label_to_numeric_mapping is None:\n            raise ValueError(\"fit method must be called before inverse_transform\")\n        return pd.Series(y).map({numeric: label for label, numeric in self.label_to_numeric_mapping.items()})","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.181214Z","iopub.execute_input":"2024-02-26T07:24:53.181651Z","iopub.status.idle":"2024-02-26T07:24:53.293909Z","shell.execute_reply.started":"2024-02-26T07:24:53.181624Z","shell.execute_reply":"2024-02-26T07:24:53.29294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_encoder = CustomLabelEncoder(sorted_labels)\ntarget_encoder.fit(target)\ntarget_numeric = target_encoder.transform(target)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.295222Z","iopub.execute_input":"2024-02-26T07:24:53.29614Z","iopub.status.idle":"2024-02-26T07:24:53.306089Z","shell.execute_reply.started":"2024-02-26T07:24:53.296105Z","shell.execute_reply":"2024-02-26T07:24:53.305161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_numeric","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.307504Z","iopub.execute_input":"2024-02-26T07:24:53.307874Z","iopub.status.idle":"2024-02-26T07:24:53.319593Z","shell.execute_reply.started":"2024-02-26T07:24:53.307833Z","shell.execute_reply":"2024-02-26T07:24:53.318657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using our enconder, we can rever back from numeric to string anytime:","metadata":{}},{"cell_type":"code","source":"target_encoder.inverse_transform(target_numeric)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.320856Z","iopub.execute_input":"2024-02-26T07:24:53.321245Z","iopub.status.idle":"2024-02-26T07:24:53.333365Z","shell.execute_reply.started":"2024-02-26T07:24:53.321176Z","shell.execute_reply":"2024-02-26T07:24:53.332386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.0 EDA and data preparation","metadata":{}},{"cell_type":"markdown","source":"Let's proceed to indeep explore and prepare the datasets (both the train and the test in parallel) for being suitable for modelling with both GLMs and Tree-based models","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Missing values","metadata":{}},{"cell_type":"markdown","source":"Both train and test have no missing data","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.334955Z","iopub.execute_input":"2024-02-26T07:24:53.335192Z","iopub.status.idle":"2024-02-26T07:24:53.362686Z","shell.execute_reply.started":"2024-02-26T07:24:53.335172Z","shell.execute_reply":"2024-02-26T07:24:53.361535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.36385Z","iopub.execute_input":"2024-02-26T07:24:53.364256Z","iopub.status.idle":"2024-02-26T07:24:53.388587Z","shell.execute_reply.started":"2024-02-26T07:24:53.364212Z","shell.execute_reply":"2024-02-26T07:24:53.387558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Data Types and feature classification","metadata":{}},{"cell_type":"markdown","source":"Let's enumerate the numeric (continuous and ordinal) features fromt the categorical ones","metadata":{}},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.389864Z","iopub.execute_input":"2024-02-26T07:24:53.390128Z","iopub.status.idle":"2024-02-26T07:24:53.397451Z","shell.execute_reply.started":"2024-02-26T07:24:53.390106Z","shell.execute_reply":"2024-02-26T07:24:53.39656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features = train.columns[train.dtypes==\"object\"].tolist()\nnumeric_features = train.columns[train.dtypes!=\"object\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.39851Z","iopub.execute_input":"2024-02-26T07:24:53.398763Z","iopub.status.idle":"2024-02-26T07:24:53.40622Z","shell.execute_reply.started":"2024-02-26T07:24:53.398741Z","shell.execute_reply":"2024-02-26T07:24:53.405456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the categorical features have very few unique levels, making one-hot-encoding the best choice for this situation","metadata":{}},{"cell_type":"code","source":"train[categorical_features].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.409312Z","iopub.execute_input":"2024-02-26T07:24:53.409588Z","iopub.status.idle":"2024-02-26T07:24:53.438932Z","shell.execute_reply.started":"2024-02-26T07:24:53.409558Z","shell.execute_reply":"2024-02-26T07:24:53.437955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the numeric features, have enough distinct values to confirm their are continuous ones","metadata":{}},{"cell_type":"code","source":"train[numeric_features].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.439949Z","iopub.execute_input":"2024-02-26T07:24:53.440232Z","iopub.status.idle":"2024-02-26T07:24:53.453919Z","shell.execute_reply.started":"2024-02-26T07:24:53.440206Z","shell.execute_reply":"2024-02-26T07:24:53.453075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Plotting count and distribution pairs","metadata":{}},{"cell_type":"code","source":"def plot_count_pairs(train, test, feature, hue=\"set\", order=None, palette=None):\n    data_df = train.copy()\n    data_df['set'] = 'train'\n    data_df = pd.concat([data_df, test.copy()]).fillna('test')\n    f, ax = plt.subplots(1, 1, figsize=(8, 4))\n    sns.countplot(x=feature, data=data_df, hue=hue, palette=color_list, order=order)\n    plt.grid(color=\"black\", linestyle=\"-.\", linewidth=0.5, axis=\"y\", which=\"major\")\n    ax.set_title(f\"Paired train/test frequencies of {feature}\")\n    plt.show()\n    \nfor feature in categorical_features:\n    if feature in [\"CAEC\", \"CALC\"]:\n        order = [\"no\", \"Always\", \"Sometimes\", \"Frequently\"]\n    else:\n        order = sorted(train[feature].unique())\n    plot_count_pairs(train, test, feature=feature, order=order, palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:53.455358Z","iopub.execute_input":"2024-02-26T07:24:53.456244Z","iopub.status.idle":"2024-02-26T07:24:55.842391Z","shell.execute_reply.started":"2024-02-26T07:24:53.456214Z","shell.execute_reply":"2024-02-26T07:24:55.841412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\ndef plot_distribution_pairs(train, test, feature, hue=\"set\", palette=None):\n    data_df = train.copy()\n    data_df['set'] = 'train'\n    data_df = pd.concat([data_df, test.copy()]).fillna('test')\n    data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    f, ax = plt.subplots(1, 1, figsize=(8, 4))\n    for i, s in enumerate(data_df[hue].unique()):\n        selection = data_df.loc[data_df[hue]==s, feature]\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=FutureWarning)\n            g = sns.histplot(selection, color=palette[i], ax=ax, label=s)\n    ax.set_title(f\"Paired train/test distributions of {feature}\")\n    g.legend()\n    plt.show()\n\ndef plot_distribution_pairs_boxplot(train, test, feature, hue=\"set\", palette=None):\n    data_df = train.copy()\n    data_df['set'] = 'train'\n    data_df = pd.concat([data_df, test.copy()]).fillna('test')\n    data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    f, ax = plt.subplots(1, 1, figsize=(8, 4))\n    for i, s in enumerate(data_df[hue].unique()):\n        selection = data_df.loc[data_df[hue]==s, feature]\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=FutureWarning)\n            g = sns.boxplot(x=hue, y=feature, data=data_df, palette=palette, ax=ax)\n    ax.set_title(f\"Paired train/test boxplots of {feature}\")\n    g.legend()\n    plt.show()\n    \nfor feature in numeric_features:\n    plot_distribution_pairs(train, test, feature, palette=color_list)\n    plot_distribution_pairs_boxplot(train, test, feature, palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:24:55.843801Z","iopub.execute_input":"2024-02-26T07:24:55.844166Z","iopub.status.idle":"2024-02-26T07:25:03.898564Z","shell.execute_reply.started":"2024-02-26T07:24:55.844133Z","shell.execute_reply":"2024-02-26T07:25:03.897674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 Categorical encoding","metadata":{}},{"cell_type":"markdown","source":"We then proceed to process the categorical features by encoding them using the OneHotEncoder by Scikit-learn","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse_output=False)\nencoder.fit(pd.concat([train[categorical_features], test[categorical_features]], axis=0))\n\ntrain_encoded = encoder.transform(train[categorical_features])\ntrain_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(categorical_features))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:03.899788Z","iopub.execute_input":"2024-02-26T07:25:03.900052Z","iopub.status.idle":"2024-02-26T07:25:04.012175Z","shell.execute_reply.started":"2024-02-26T07:25:03.900029Z","shell.execute_reply":"2024-02-26T07:25:04.011412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_encoded = encoder.fit_transform(test[categorical_features])\ntest_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_features))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:04.013346Z","iopub.execute_input":"2024-02-26T07:25:04.013637Z","iopub.status.idle":"2024-02-26T07:25:04.061178Z","shell.execute_reply.started":"2024-02-26T07:25:04.013614Z","shell.execute_reply":"2024-02-26T07:25:04.060452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sum = train_encoded_df.sum(axis=0).astype(int)\ntest_sum = test_encoded_df.sum(axis=0).astype(int)\n\nsum_df = pd.concat([train_sum, test_sum], axis=1, keys=['Train', 'Test'])\nsum_df","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:04.062286Z","iopub.execute_input":"2024-02-26T07:25:04.062561Z","iopub.status.idle":"2024-02-26T07:25:04.078369Z","shell.execute_reply.started":"2024-02-26T07:25:04.062537Z","shell.execute_reply":"2024-02-26T07:25:04.077457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5 Adjusting categorical encodings","metadata":{}},{"cell_type":"markdown","source":"CALC_Always, being zero in training won't be used and there could be problems in the test if CALC is a relevant feature. We can then perform the combination of columns CALC_Always and CALC_Frequently in both train_encoded_df and test_encoded_df into a new column called 'CALC_Always|Frequently'. After creating the combined column, the original columns CALC_Always and CALC_Frequently are dropped.","metadata":{}},{"cell_type":"code","source":"combine_columns = ['CALC_Always', 'CALC_Frequently']\n\ntrain_encoded_df['CALC_Always|Frequently'] = train_encoded_df[combine_columns].sum(axis=1)\ntest_encoded_df['CALC_Always|Frequently'] = test_encoded_df[combine_columns].sum(axis=1)\n\ntrain_encoded_df = train_encoded_df.drop(columns=combine_columns).set_index(train.index)\ntest_encoded_df = test_encoded_df.drop(columns=combine_columns).set_index(test.index)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:04.079483Z","iopub.execute_input":"2024-02-26T07:25:04.07977Z","iopub.status.idle":"2024-02-26T07:25:04.117384Z","shell.execute_reply.started":"2024-02-26T07:25:04.079745Z","shell.execute_reply":"2024-02-26T07:25:04.116691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before proceeding, let's also consider that CALC and CAEC are indeed ordinal features. We can replace them with an ordinal feature, too, to catch any linear effect due to these two features. ","metadata":{}},{"cell_type":"code","source":"levels = {\"Always\": 3, \"Frequently\": 2, \"Sometimes\": 1, \"no\": 0}\ntrain[\"CALC_ord\"] = train[\"CALC\"].map(levels)\ntest[\"CALC_ord\"] = test[\"CALC\"].map(levels)\ntrain[\"CAEC_ord\"] = train[\"CAEC\"].map(levels)\ntest[\"CAEC_ord\"] = test[\"CAEC\"].map(levels)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:04.118571Z","iopub.execute_input":"2024-02-26T07:25:04.118876Z","iopub.status.idle":"2024-02-26T07:25:04.134192Z","shell.execute_reply.started":"2024-02-26T07:25:04.118851Z","shell.execute_reply":"2024-02-26T07:25:04.133458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now integrate the categorical encodings in our original train and test datasets, replacing the previous categorical \nfeatures","metadata":{}},{"cell_type":"code","source":"train = pd.concat([train.drop(categorical_features, axis=1), train_encoded_df], axis=1)\ntest = pd.concat([test.drop(categorical_features, axis=1), test_encoded_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:04.135279Z","iopub.execute_input":"2024-02-26T07:25:04.136198Z","iopub.status.idle":"2024-02-26T07:25:04.147209Z","shell.execute_reply.started":"2024-02-26T07:25:04.136173Z","shell.execute_reply":"2024-02-26T07:25:04.146453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a furthermore check before proceeding modelling, let's test adversarial validation between the train and the test datasets in order to be sure that they are from the same distribution and there is no particular shift between them. This will ensure us that we should expect the same target distribution too.","metadata":{}},{"cell_type":"markdown","source":"### 3.6 Adversarial validation","metadata":{}},{"cell_type":"markdown","source":"We take the code for adversarial validation from the Kaggle Book: https://github.com/PacktPublishing/The-Kaggle-Book/blob/main/chapter_06/adversarial-validation-example.ipynb","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_auc_score\n\nX =  pd.concat([train, test], axis=0)\ny = [0] * len(train) + [1] * len(test)\n\nmodel = RandomForestClassifier(random_state=0)\ncv_preds = cross_val_predict(model, X, y, cv=5, n_jobs=-1, method='predict_proba')\n\nscore = roc_auc_score(y_true=y, y_score=cv_preds[:,1])\nprint(f\"roc-auc score: {score:0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:04.154803Z","iopub.execute_input":"2024-02-26T07:25:04.155064Z","iopub.status.idle":"2024-02-26T07:25:20.247338Z","shell.execute_reply.started":"2024-02-26T07:25:04.155042Z","shell.execute_reply":"2024-02-26T07:25:20.246215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the roc-auc score is about 0.5, we have a confirmation that train and test are from the same distribution.","metadata":{}},{"cell_type":"markdown","source":"### 3.7 Feature engineering","metadata":{}},{"cell_type":"markdown","source":"At this point we try to create some smart feature engineering. As a first step we calculate the Body Mass Index using the 'Height' and 'Weight' features. BMI is often used as an indicator of obesity and it better represents the relationship between weight and height. ","metadata":{}},{"cell_type":"code","source":"train['BMI'] = train['Weight'] / (train['Height'] ** 2)\ntest['BMI'] = test['Weight'] / (test['Height'] ** 2)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:20.248998Z","iopub.execute_input":"2024-02-26T07:25:20.249339Z","iopub.status.idle":"2024-02-26T07:25:20.258358Z","shell.execute_reply.started":"2024-02-26T07:25:20.249306Z","shell.execute_reply":"2024-02-26T07:25:20.25745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_pairs(train, test, feature=\"BMI\", palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:20.259785Z","iopub.execute_input":"2024-02-26T07:25:20.260141Z","iopub.status.idle":"2024-02-26T07:25:20.712604Z","shell.execute_reply.started":"2024-02-26T07:25:20.260117Z","shell.execute_reply":"2024-02-26T07:25:20.7117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then combine 'FAF' (Physical activity frequency) with 'TUE' (Time using technology devices) to create a feature representing the balance between physical activity and sedentary behavior.","metadata":{}},{"cell_type":"code","source":"train['Physical_Activity_Level'] = train['FAF'] - train['TUE']\ntest['Physical_Activity_Level'] = test['FAF'] - test['TUE']","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:20.713882Z","iopub.execute_input":"2024-02-26T07:25:20.714251Z","iopub.status.idle":"2024-02-26T07:25:20.721174Z","shell.execute_reply.started":"2024-02-26T07:25:20.714209Z","shell.execute_reply":"2024-02-26T07:25:20.72034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_pairs(train, test, feature=\"Physical_Activity_Level\", palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:20.722332Z","iopub.execute_input":"2024-02-26T07:25:20.723174Z","iopub.status.idle":"2024-02-26T07:25:21.219271Z","shell.execute_reply.started":"2024-02-26T07:25:20.723149Z","shell.execute_reply":"2024-02-26T07:25:21.218321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combine 'FCVC' (Frequency of consumption of vegetables) and 'NCP' (Number of main meals) to create a feature reflecting overall meal habits.","metadata":{}},{"cell_type":"code","source":"train['Meal_Habits'] = train['FCVC'] * train['NCP']\ntest['Meal_Habits'] = test['FCVC'] * test['NCP']","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:21.220596Z","iopub.execute_input":"2024-02-26T07:25:21.220877Z","iopub.status.idle":"2024-02-26T07:25:21.227435Z","shell.execute_reply.started":"2024-02-26T07:25:21.220853Z","shell.execute_reply":"2024-02-26T07:25:21.226606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_pairs(train, test, feature=\"Meal_Habits\", palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:21.228616Z","iopub.execute_input":"2024-02-26T07:25:21.228941Z","iopub.status.idle":"2024-02-26T07:25:21.707894Z","shell.execute_reply.started":"2024-02-26T07:25:21.228912Z","shell.execute_reply":"2024-02-26T07:25:21.706915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also combine features like 'FCVC' (Frequency of consumption of vegetables) and 'FAVC' (Frequent consumption of high caloric food) to create a new feature representing overall nutrition habits.","metadata":{}},{"cell_type":"code","source":"(train['FAVC_yes'] - 2).unique","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:21.709071Z","iopub.execute_input":"2024-02-26T07:25:21.709376Z","iopub.status.idle":"2024-02-26T07:25:21.717741Z","shell.execute_reply.started":"2024-02-26T07:25:21.70935Z","shell.execute_reply":"2024-02-26T07:25:21.716736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Healthy_Nutrition_Habits'] = train['FCVC'] / ( 2 * train['FAVC_no'] - 1)\ntest['Healthy_Nutrition_Habits'] = test['FCVC'] / ( 2 * test['FAVC_no'] - 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:21.719005Z","iopub.execute_input":"2024-02-26T07:25:21.719329Z","iopub.status.idle":"2024-02-26T07:25:21.730491Z","shell.execute_reply.started":"2024-02-26T07:25:21.719304Z","shell.execute_reply":"2024-02-26T07:25:21.729542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_pairs(train, test, feature=\"Healthy_Nutrition_Habits\", palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:21.731687Z","iopub.execute_input":"2024-02-26T07:25:21.732025Z","iopub.status.idle":"2024-02-26T07:25:22.319589Z","shell.execute_reply.started":"2024-02-26T07:25:21.731996Z","shell.execute_reply":"2024-02-26T07:25:22.31872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We finally create a score based on the frequency of technology usage ('TUE') weighted by the age: essentially, it quantifies the average amount of time a person spends using technology per unit of their age","metadata":{}},{"cell_type":"code","source":"train['Tech_Usage_Score'] = train['TUE'] / train['Age']\ntest['Tech_Usage_Score'] = test['TUE'] / test['Age']","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:22.32061Z","iopub.execute_input":"2024-02-26T07:25:22.320868Z","iopub.status.idle":"2024-02-26T07:25:22.327428Z","shell.execute_reply.started":"2024-02-26T07:25:22.320845Z","shell.execute_reply":"2024-02-26T07:25:22.326603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_pairs(train, test, feature=\"Tech_Usage_Score\", palette=color_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:22.328482Z","iopub.execute_input":"2024-02-26T07:25:22.328742Z","iopub.status.idle":"2024-02-26T07:25:22.927049Z","shell.execute_reply.started":"2024-02-26T07:25:22.32872Z","shell.execute_reply":"2024-02-26T07:25:22.926073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since this completes our data preparation, we store the resulting transformed train and test datasets, hence we can reuse them in other Kaggle Notebooks.","metadata":{}},{"cell_type":"code","source":"train['Tech_Usage_Score'] = train['TUE'] / train['Age']\ntest['Tech_Usage_Score'] = test['TUE'] / test['Age']","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:22.928188Z","iopub.execute_input":"2024-02-26T07:25:22.928491Z","iopub.status.idle":"2024-02-26T07:25:22.934215Z","shell.execute_reply.started":"2024-02-26T07:25:22.928466Z","shell.execute_reply":"2024-02-26T07:25:22.933326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_csv(\"train.csv\")\ntest.to_csv(\"test.csv\")\ntarget.to_csv(\"target.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:22.935657Z","iopub.execute_input":"2024-02-26T07:25:22.936039Z","iopub.status.idle":"2024-02-26T07:25:24.045798Z","shell.execute_reply.started":"2024-02-26T07:25:22.936007Z","shell.execute_reply":"2024-02-26T07:25:24.04497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.0 Modelling","metadata":{}},{"cell_type":"markdown","source":"We can now proceed to modelling. First by defining our cv strategy, which is, as mentioned before, a stratified cross-validation based on the distribution of target labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ncv = StratifiedKFold(5, shuffle=True, random_state=0)\ncv_splits = cv.split(train, y=target_numeric)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:24.046821Z","iopub.execute_input":"2024-02-26T07:25:24.047071Z","iopub.status.idle":"2024-02-26T07:25:24.051997Z","shell.execute_reply.started":"2024-02-26T07:25:24.04705Z","shell.execute_reply":"2024-02-26T07:25:24.051148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(cv_splits)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:24.053036Z","iopub.execute_input":"2024-02-26T07:25:24.053292Z","iopub.status.idle":"2024-02-26T07:25:24.072069Z","shell.execute_reply.started":"2024-02-26T07:25:24.05327Z","shell.execute_reply":"2024-02-26T07:25:24.07107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 First temptative models (hand tuning)","metadata":{}},{"cell_type":"markdown","source":"XGBoost, by default, constructs an individual model for each target, a behavior reminiscent of meta-estimators in scikit-learn. This approach offers the advantage of efficient data reuse and leverages integrated features such as SHAP (SHapley Additive exPlanations). In the context of multi-label classification, XGBoost employs the binary relevance strategy. This strategy involves building separate binary classifiers for each class, treating the classification problem as a set of independent binary tasks. This allows the model to handle multi-label scenarios where each instance may belong to multiple classes.","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster='gbtree',\n                    device = \"cuda\",\n                    tree_method=\"hist\",\n                    objective='multi:softmax',  # 'multi:softmax' for multiclass classification\n                    num_class=len(set(target_numeric)),  # Number of classes\n                    n_estimators=300,\n                    max_depth=6,\n                    verbosity=0)\n\ncv_splits = cv.split(train, y=target_numeric)\nscorer = make_scorer(accuracy_score)\n\ncv_results = cross_validate(xgb, train, target_numeric, cv=cv_splits, scoring=scorer, return_train_score=False)\n\naccuracy_mean = cv_results['test_score'].mean()\nprint(f'Cross-validated Accuracy: {accuracy_mean:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:24.073231Z","iopub.execute_input":"2024-02-26T07:25:24.073546Z","iopub.status.idle":"2024-02-26T07:25:42.014191Z","shell.execute_reply.started":"2024-02-26T07:25:24.073521Z","shell.execute_reply":"2024-02-26T07:25:42.013175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One helpful step, after creating the first temptative model, is to do some error analysis and try to understand what is not working in the approach. To do so, we need to resort to cross_val_predict which will create a cross validated prediction useful to check the fit of the model.? ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_predict\n\ncv_splits = cv.split(train, y=target_numeric)\ncv_cls_preds = cross_val_predict(xgb, train, target_numeric, cv=cv_splits)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:42.015387Z","iopub.execute_input":"2024-02-26T07:25:42.015759Z","iopub.status.idle":"2024-02-26T07:25:59.853833Z","shell.execute_reply.started":"2024-02-26T07:25:42.015723Z","shell.execute_reply":"2024-02-26T07:25:59.852646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true=target_numeric, y_pred=cv_cls_preds, target_names=target_encoder.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:59.85532Z","iopub.execute_input":"2024-02-26T07:25:59.855758Z","iopub.status.idle":"2024-02-26T07:25:59.880777Z","shell.execute_reply.started":"2024-02-26T07:25:59.855718Z","shell.execute_reply":"2024-02-26T07:25:59.879876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(y_true=target_numeric, y_pred=cv_cls_preds)\n\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.2) \nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap_custom, \n            xticklabels=sorted_labels, yticklabels=sorted_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:25:59.881924Z","iopub.execute_input":"2024-02-26T07:25:59.882219Z","iopub.status.idle":"2024-02-26T07:26:00.378255Z","shell.execute_reply.started":"2024-02-26T07:25:59.882194Z","shell.execute_reply":"2024-02-26T07:26:00.377313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the overview of the error analysis, a step that always to be taken when working with tabular data, it seems that the decision boundaries between certain categories, in particular Overweight_Level_I and Overweight_Level_II, are not so well defined. In such cases it is helpful to use blended submissions between different but equally predictive models, to have better shaped class boundaries.","metadata":{}},{"cell_type":"markdown","source":"A solution, in our case, would be to blend our XGBoost with another gradient boosting solution, such as a LightGBM.","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier(boosting_type=\"gbdt\",\n                     objective=\"multiclass\",\n                     metric=\"multi_logloss\",\n                     num_class=7,\n                     learning_rate=0.025,\n                     n_estimators=500,\n                     lambda_l1=0.06,\n                     lambda_l2=0.3,\n                     max_depth=9,\n                     colsample_bytree=0.40,\n                     subsample=0.85,\n                     min_child_samples=15,\n                     verbosity=-1)\n\ncv_splits = cv.split(train, y=target_numeric)\n\ncv_results = cross_validate(lgb, train, target_numeric, cv=cv_splits, scoring=scorer, return_train_score=False)\n\naccuracy_mean = cv_results['test_score'].mean()\nprint(f'Cross-validated Accuracy: {accuracy_mean:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:26:00.379336Z","iopub.execute_input":"2024-02-26T07:26:00.379632Z","iopub.status.idle":"2024-02-26T07:26:54.83799Z","shell.execute_reply.started":"2024-02-26T07:26:00.379606Z","shell.execute_reply":"2024-02-26T07:26:54.837036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_splits = cv.split(train, y=target_numeric)\ncv_cls_lgb_preds = cross_val_predict(lgb, train, target_numeric, cv=cv_splits)\n\nprint(classification_report(y_true=target_numeric, y_pred=cv_cls_lgb_preds, target_names=target_encoder.classes_))\n\nconf_matrix = confusion_matrix(y_true=target_numeric, y_pred=cv_cls_lgb_preds)\n\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.2) \nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap_custom, \n            xticklabels=sorted_labels, yticklabels=sorted_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\nimprovement = np.sum((cv_cls_lgb_preds==target_numeric)&(cv_cls_preds!=target_numeric)) / len(target_numeric)\nprint(f\"maximum possible improvement: {improvement:0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:26:54.839593Z","iopub.execute_input":"2024-02-26T07:26:54.840153Z","iopub.status.idle":"2024-02-26T07:27:44.995691Z","shell.execute_reply.started":"2024-02-26T07:26:54.840121Z","shell.execute_reply":"2024-02-26T07:27:44.994708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training a LightGBM and doing some error analysis, does perform slightly better than XGBoost on the problematic classes. An estimation of possible maximum improvement considering only the correct estimates (not the erroneous ones) that a LightGBM could bring in a blend seems promising.","metadata":{}},{"cell_type":"markdown","source":"Before proceeding, however, let's test how the new multi output tree, which is especially designed for multiclass problem, could work.","metadata":{}},{"cell_type":"markdown","source":"As of XGBoost version 2.0, in fact, a new multi_strategy option has become available. the training parameter called multi_strategy allows you to specify how the model should handle multiple targets when constructing trees.\n\nBy default, when multi_strategy is set to \"one_output_per_tree\" (which is the default), XGBoost builds one model for each target. Each model is trained independently for its corresponding target.\n\nAlternatively, when multi_strategy is set to \"multi_output_tree,\" XGBoost constructs multi-output trees. In this mode, each leaf node of the tree is associated with all the targets, and the model learns to make joint predictions for multiple targets simultaneously. This allows to build a single, compact model.\n\nSince the multi_output_tree option is a work in progress, at the moment it only supports cpu as a device.","metadata":{}},{"cell_type":"code","source":"%%time\n\nxgb = XGBClassifier(booster='gbtree',\n                    device = \"cpu\",\n                    tree_method=\"hist\",\n                    multi_strategy=\"multi_output_tree\", # for multi-output trees\n                    objective='multi:softmax',  # 'multi:softmax' for multiclass classification\n                    num_class=len(set(target_numeric)),  # Number of classes\n                    n_estimators=300,\n                    max_depth=6,\n                    verbosity=0)\n\ncv_splits = cv.split(train, y=target_numeric)\n\ncv_results = cross_validate(xgb, train, target_numeric, cv=cv_splits, scoring=scorer, return_train_score=False)\n\naccuracy_mean = cv_results['test_score'].mean()\nprint(f'Cross-validated Accuracy: {accuracy_mean:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:27:44.996876Z","iopub.execute_input":"2024-02-26T07:27:44.997164Z","iopub.status.idle":"2024-02-26T07:28:29.230704Z","shell.execute_reply.started":"2024-02-26T07:27:44.997139Z","shell.execute_reply":"2024-02-26T07:28:29.229969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The solution is not bad, let's go for the faster binary relevance strategy, hence we can leverage the GPU, but let's consider the multi_output_strategy for a blended solution.","metadata":{}},{"cell_type":"code","source":"cv_splits = cv.split(train, y=target_numeric)\ncv_cls_multi_preds = cross_val_predict(xgb, train, target_numeric, cv=cv_splits)\n\nprint(classification_report(y_true=target_numeric, y_pred=cv_cls_multi_preds, target_names=target_encoder.classes_))\n\nconf_matrix = confusion_matrix(y_true=target_numeric, y_pred=cv_cls_multi_preds)\n\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.2) \nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap_custom, \n            xticklabels=sorted_labels, yticklabels=sorted_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:28:29.231978Z","iopub.execute_input":"2024-02-26T07:28:29.232487Z","iopub.status.idle":"2024-02-26T07:29:14.381128Z","shell.execute_reply.started":"2024-02-26T07:28:29.232459Z","shell.execute_reply":"2024-02-26T07:29:14.380202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"improvement = np.sum((cv_cls_multi_preds==target_numeric)&(cv_cls_preds!=target_numeric)) / len(target_numeric)\nprint(f\"maximum possible improvement: {improvement:0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:29:14.382244Z","iopub.execute_input":"2024-02-26T07:29:14.382532Z","iopub.status.idle":"2024-02-26T07:29:14.389312Z","shell.execute_reply.started":"2024-02-26T07:29:14.382507Z","shell.execute_reply":"2024-02-26T07:29:14.388354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Approching the problem as a regression","metadata":{}},{"cell_type":"markdown","source":"Since the target is ordinal, we can test also operating using a regression instead of a classification","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\ncv_splits = cv.split(train, y=target_numeric)\ncv_scores = list()\ncv_train_preds = np.zeros(len(train))\ncv_test_preds = np.zeros(len(test))\n\nfor train_idx, val_idx in cv_splits:\n    xgb = XGBRegressor(booster='gbtree',\n                       device = \"cpu\",\n                       tree_method=\"hist\",\n                       n_estimators=200,\n                       max_depth=4,\n                       verbosity=0)\n    xgb.fit(train.iloc[train_idx], target_numeric.iloc[train_idx])\n    preds = xgb.predict(train.iloc[val_idx])\n    cv_train_preds[val_idx] = preds\n    cv_test_preds += xgb.predict(test) / cv.n_splits\n    cls_preds = np.clip(np.round(preds).astype(int), a_min=0, a_max=6)\n    \n    cv_scores.append(accuracy_score(y_true=target_numeric.iloc[val_idx], y_pred=cls_preds))\n\naccuracy_mean = np.mean(cv_scores)\nprint(f'Cross-validated Accuracy: {accuracy_mean:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:29:14.390494Z","iopub.execute_input":"2024-02-26T07:29:14.390763Z","iopub.status.idle":"2024-02-26T07:29:16.60568Z","shell.execute_reply.started":"2024-02-26T07:29:14.39074Z","shell.execute_reply":"2024-02-26T07:29:16.604956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true=target_numeric, \n                            y_pred=np.clip(np.round(cv_train_preds).astype(int), a_min=0, a_max=6), \n                            target_names=target_encoder.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:29:16.606905Z","iopub.execute_input":"2024-02-26T07:29:16.607395Z","iopub.status.idle":"2024-02-26T07:29:16.629424Z","shell.execute_reply.started":"2024-02-26T07:29:16.607367Z","shell.execute_reply":"2024-02-26T07:29:16.628813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The problems with this predictor are again the first two overweight categories, it tend to appear a weaker solution though, than using multiclass classification. ","metadata":{}},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_true=target_numeric, \n                               y_pred=np.clip(np.round(cv_train_preds).astype(int), a_min=0, a_max=6))\n\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.2) \nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap_custom, \n            xticklabels=sorted_labels, yticklabels=sorted_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:29:16.630532Z","iopub.execute_input":"2024-02-26T07:29:16.63102Z","iopub.status.idle":"2024-02-26T07:29:17.080736Z","shell.execute_reply.started":"2024-02-26T07:29:16.630994Z","shell.execute_reply":"2024-02-26T07:29:17.079784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results from the regression are disappointing, but the regression's results could be improved and possibly be used as a feature for the classification.","metadata":{}},{"cell_type":"markdown","source":"### 4.3 Model optimization by Optuna and Bayesian search","metadata":{}},{"cell_type":"markdown","source":"As a next step we optimize XGBoost most important hyper-parameters using a bayesian search by Optuna. The TPESampler is set to first try random optimization for the first 30 trials and then perform a TPE Bayesian Search. We also use the multivariate approach (added in v2.2.0 as an experimental feature), which is reported to outperform the independent TPE.","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\n\ndef objective(trial):\n    \n    params = {\n        'grow_policy': trial.suggest_categorical('grow_policy', [\"depthwise\", \"lossguide\"]),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n        'gamma' : trial.suggest_float('gamma', 1e-9, 0.5),\n        'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 0, 16),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 100.0, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 100.0, log=True),\n        \n    }\n    \n    params['booster'] = 'gbtree'\n    params['objective'] = 'multi:softmax'\n    params[\"device\"] = \"cuda\"\n    params[\"verbosity\"] = 0\n    params['tree_method'] = \"hist\"\n    \n    xgb = XGBClassifier(**params)\n    cv_splits = cv.split(train, y=target_numeric)\n\n    cv_scores = cross_validate(estimator=xgb, \n                               X=train, \n                               y=target_numeric,\n                               scoring=scorer,\n                               cv=cv_splits)\n\n    cv_evaluation = np.mean(np.abs(cv_scores['test_score']))\n    return cv_evaluation\n\nsqlite_db = \"sqlite:///sqlite.db\"\nstudy_name = \"multi_class_prediction_of_obesity_risk\"\nstudy = optuna.create_study(storage=sqlite_db, study_name=study_name, \n                            sampler=TPESampler(n_startup_trials=30, multivariate=True, seed=0),\n                            direction=\"maximize\", load_if_exists=True)\n\nstudy.optimize(objective, n_trials=150)\nprint(f\"best optmized accuracy: {study.best_value:0.5f}\")\nprint(f\"best hyperparameters: {study.best_params}\")\n\nbest_cls_params = study.best_params\nbest_cls_params['objective'] = 'multi:softmax'\nbest_cls_params['tree_method'] = \"hist\"\nbest_cls_params[\"device\"] = \"cuda\"\nbest_cls_params[\"verbosity\"] = 0","metadata":{"execution":{"iopub.status.busy":"2024-02-26T07:29:17.082308Z","iopub.execute_input":"2024-02-26T07:29:17.082691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 Re-train on all data","metadata":{}},{"cell_type":"markdown","source":"Once we obtained the best parameters, we re-train on all our available data","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(**best_cls_params)\nxgb.fit(train, target_numeric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The we proceed to inference, convert outputted numeric labels into strings and then save everything into a submission","metadata":{}},{"cell_type":"markdown","source":"## 5.0 Explainability","metadata":{}},{"cell_type":"markdown","source":"To understand how the model actually works, the XGBoost model itself, represented by the booster, is used to calculate SHAP (SHapley Additive exPlanations) values for each feature in the training data. The feature names are extracted from the 'train' DataFrame, and a DMatrix 'Xd' is created to represent the training data in a format suitable for XGBoost. The SHAP values are obtained by predicting contributions to the predictions using the 'predict' method with 'pred_contribs=True'. ","metadata":{}},{"cell_type":"code","source":"from xgboost import DMatrix\n\nbooster = xgb.get_booster()\n\nfeature_names = train.columns\nXd = DMatrix(train)\n\nshap_values = booster.predict(Xd, pred_contribs=True)\npreds = booster.predict(Xd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"then we create a summary plot visualizing the impact of different features on the model predictions. The shap.summary_plot function is employed to generate a violin plot, where each violin represents the distribution of SHAP values for a specific feature. ","metadata":{}},{"cell_type":"code","source":"shap_values[:,0,:-1].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_encoder.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\nimport matplotlib.pyplot as plt\n\nfor k, target_label in enumerate(target_encoder.classes_):\n    print(f\"VIOLIN PLOT REPRESENTING <{target_label}>\")\n    shap.summary_plot(shap_values[:,k, :-1], train.values,\n                      plot_type=\"violin\",\n                      feature_names=feature_names, show=False)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.show()\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.0 Preparation of the submission","metadata":{}},{"cell_type":"code","source":"preds = xgb.predict(test)\nlabelled_preds = target_encoder.inverse_transform(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/playground-series-s4e2/sample_submission.csv\")\nsubmission[\"NObeyesdad\"] = labelled_preds.values\nsubmission.to_csv(\"xgb_submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.0 Submission strategies","metadata":{}},{"cell_type":"markdown","source":"Now that our single model submission is ready, let's prepare an ensembled one with multiout XGBoost and LightGBM and check if it improves our results as we figured after error analysis.","metadata":{}},{"cell_type":"code","source":"from copy import copy\n\nmulti_cls_params = copy(best_cls_params)\nmulti_cls_params[\"device\"] = \"cpu\"\nmulti_cls_params[\"tree_method\"] = \"hist\"\nmulti_cls_params[\"multi_strategy\"] = \"multi_output_tree\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi = XGBClassifier(**multi_cls_params)\nmulti.fit(train, target_numeric)\n\nlgb = LGBMClassifier(boosting_type=\"gbdt\",\n                     objective=\"multiclass\",\n                     metric=\"multi_logloss\",\n                     num_class=7,\n                     learning_rate=0.025,\n                     n_estimators=500,\n                     lambda_l1=0.06,\n                     lambda_l2=0.3,\n                     max_depth=9,\n                     colsample_bytree=0.40,\n                     subsample=0.85,\n                     min_child_samples=15,\n                     verbosity=-1)\n\nlgb.fit(train, target_numeric)\n\nxgb_preds = xgb.predict_proba(test)\nlgb_preds = lgb.predict_proba(test)\nmultixgb_preds = multi.predict_proba(test)\n\nblended_preds = np.argmax((xgb_preds + lgb_preds + multixgb_preds), axis=1).ravel()\nlabelled_blended_preds = target_encoder.inverse_transform(blended_preds)\n\nsubmission = pd.read_csv(\"/kaggle/input/playground-series-s4e2/sample_submission.csv\")\nsubmission[\"NObeyesdad\"] = labelled_blended_preds.values\nsubmission.to_csv(\"blended_submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another strategy is to bag ((Bootstrap Aggregating) multiple times your model and then blend the results. This should result in more stable results in the private leaderboard","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nboots = 300\n\nfor k in tqdm(range(boots)):\n    bootstrap_idx = np.random.choice(train.index, size=len(train), replace=True)\n\n    xgb = XGBClassifier(**best_cls_params)\n    xgb.fit(train.loc[bootstrap_idx], target_numeric.loc[bootstrap_idx])\n    if k==0:\n        preds = xgb.predict_proba(test)\n    else:\n        preds += xgb.predict_proba(test)\n        \nblended_preds = np.argmax(preds, axis=1).ravel()\nlabelled_blended_preds = target_encoder.inverse_transform(blended_preds)\n\nsubmission = pd.read_csv(\"/kaggle/input/playground-series-s4e2/sample_submission.csv\")\nsubmission[\"NObeyesdad\"] = labelled_blended_preds.values\nsubmission.to_csv(\"bagged_submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we complete our overview of this multiclass problem. The next step in the competition is to feed the data we prepared to a auto-ml engine or build a blending or stacking solution that could bring you up the leaderboard. Good luck!","metadata":{}}]}