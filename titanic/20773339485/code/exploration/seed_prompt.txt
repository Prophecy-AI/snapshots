# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival patterns by Sex/Pclass/Title

## Problem Type
Binary classification (Survived: 0 or 1) with accuracy as the evaluation metric. Small dataset (891 train, 418 test samples).

## Feature Engineering (Critical for High Accuracy)

### Title Extraction from Name
Extract title using regex: `' ([A-Za-z]+)\.'` from Name column
- Group rare titles: Map Mlle/Ms -> Miss, Mme -> Mrs, rare titles -> Rare
- Title is highly predictive: Master (young boys) has 57.5% survival despite being male

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size 2-4 has higher survival than alone or large families

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (77% missing - use as binary indicator)
- Can extract deck letter from Cabin (first character) for non-missing values

### Binning Continuous Features
- **AgeBin**: Bin Age into categories (e.g., 0-16, 16-32, 32-48, 48-64, 64+)
- **FareBin**: Bin Fare using qcut (quartiles) or custom bins

### Other Features
- **Name_length**: Length of passenger name (correlates with social status)
- **Ticket_prefix**: Extract prefix from Ticket number

## Missing Value Handling
- **Age**: Fill with median (grouped by Pclass and Sex for better accuracy)
- **Embarked**: Fill with mode ('S' - Southampton)
- **Fare**: Fill with median (grouped by Pclass)
- **Cabin**: Don't impute - use Has_Cabin binary feature instead

## Models

### Single Models (Baseline)
- **Random Forest**: Good baseline, handles categorical features well
- **Gradient Boosting**: Often best single model
- **XGBoost/LightGBM**: Fast, handles missing values natively

### Ensemble Approaches (Best Performance)
1. **Voting Classifier**: Combine predictions from multiple models
   - Hard voting: Majority vote
   - Soft voting: Average probabilities (requires predict_proba)

2. **Stacking (Recommended for best accuracy)**:
   - First level base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVM
   - Use Out-of-Fold (OOF) predictions to avoid overfitting
   - Second level: XGBoost or Logistic Regression on base model predictions
   - Key: Use K-Fold (5 folds) for OOF predictions

### Model Parameters (from top kernels)
- **RandomForest**: n_estimators=500, max_depth=6, min_samples_leaf=2, max_features='sqrt'
- **ExtraTrees**: n_estimators=500, max_depth=8, min_samples_leaf=2
- **AdaBoost**: n_estimators=500, learning_rate=0.75
- **GradientBoosting**: n_estimators=500, max_depth=5, learning_rate=0.05
- **XGBoost**: n_estimators=500, max_depth=4, learning_rate=0.02, subsample=0.8

## Validation Strategy
- Use StratifiedKFold (5 folds) to maintain class distribution
- Cross-validation score ~82-84% is typical for good models
- Watch for overfitting on small dataset

## Feature Selection
- Use Recursive Feature Elimination (RFE) with cross-validation
- Feature importance from tree models to identify key predictors
- Top features typically: Sex, Pclass, Title, Fare, Age, FamilySize

## Preprocessing Pipeline
1. Combine train and test for consistent encoding
2. Extract features (Title, FamilySize, etc.)
3. Handle missing values
4. Encode categorical variables (LabelEncoder or OneHotEncoder)
5. Create bins for continuous features
6. Drop unnecessary columns (PassengerId, Name, Ticket, Cabin)

## Key Insights from Top Kernels
- Sex is the strongest predictor (females 74% vs males 19% survival)
- Pclass strongly correlates with survival (1st: 63%, 2nd: 47%, 3rd: 24%)
- "Women and children first" policy is evident in data
- Title extraction captures both gender and age information
- Ensemble methods consistently outperform single models

## Submission Tips
- Ensure exactly 418 predictions for test set
- Format: PassengerId, Survived (0 or 1)
- Multiple submissions allowed - try different ensemble combinations
