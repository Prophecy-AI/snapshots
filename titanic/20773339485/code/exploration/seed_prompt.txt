# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution, survival patterns by Sex/Pclass/Title

## Problem Type
Binary classification (Survived: 0 or 1) with accuracy as the evaluation metric. Small dataset (891 train, 418 test samples).

**Target Score Context:** The target of 1.0 (100% accuracy) is extremely high. Legitimate ML approaches typically achieve 82-85% accuracy. Scores above 85% often involve advanced techniques like genetic programming or extensive hyperparameter tuning. Focus on maximizing accuracy through comprehensive feature engineering and ensemble methods.

## Feature Engineering (Critical for High Accuracy)

### Title Extraction from Name (MOST IMPORTANT)
Extract title using regex: `' ([A-Za-z]+)\.'` from Name column
- Group rare titles: Map Mlle/Ms -> Miss, Mme -> Mrs, rare titles -> Rare
- Title is highly predictive: Master (young boys) has 57.5% survival despite being male
- Consider mapping: {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}

### Family Features
- **FamilySize** = SibSp + Parch + 1
- **IsAlone** = 1 if FamilySize == 1, else 0
- Family size 2-4 has higher survival than alone or large families
- Consider **FamilySizeGroup**: Small (1), Medium (2-4), Large (5+)

### Cabin Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (77% missing - use as binary indicator)
- **Deck** = First character of Cabin (A-G, T, or 'U' for unknown)
- Deck B has highest survival rate (~74%), Deck U (unknown) lowest (~30%)

### Binning Continuous Features
- **AgeBin**: Bin Age into categories (e.g., 0-16, 16-32, 32-48, 48-64, 64+)
- **FareBin**: Bin Fare using qcut (quartiles) or custom bins
- Binning helps tree models find optimal splits

### Advanced Features
- **Name_length**: Length of passenger name (correlates with social status)
- **Ticket_prefix**: Extract prefix from Ticket number
- **Ticket_frequency**: Count of passengers sharing same ticket
- **Sex_Pclass**: Interaction feature combining Sex and Pclass

## Missing Value Handling
- **Age**: Fill with median grouped by Pclass, Sex, and Title for better accuracy
  - Alternative: Use XGBoost/KNN imputation based on other features
- **Embarked**: Fill with mode ('S' - Southampton) or 'C' based on Fare/Pclass analysis
- **Fare**: Fill with median grouped by Pclass
- **Cabin**: Don't impute - use Has_Cabin and Deck features instead

## Models

### Single Models (Baseline)
- **Random Forest**: Good baseline, handles categorical features well
- **Gradient Boosting**: Often best single model
- **XGBoost/LightGBM/CatBoost**: Fast, handles missing values natively
- **Logistic Regression**: Simple but effective with good feature engineering

### Ensemble Approaches (Best Performance)
1. **Voting Classifier**: Combine predictions from multiple models
   - Hard voting: Majority vote
   - Soft voting: Average probabilities (requires predict_proba)

2. **Stacking (Recommended for best accuracy)**:
   - First level base models: RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVM
   - Use Out-of-Fold (OOF) predictions to avoid overfitting
   - Second level: XGBoost or Logistic Regression on base model predictions
   - Key: Use K-Fold (5 folds) for OOF predictions

3. **Blending**: Simple average or weighted average of model predictions
   - Try weights like [0.3, 0.3, 0.2, 0.2] for top 4 models

### Model Parameters (from top kernels)
- **RandomForest**: n_estimators=500, max_depth=6, min_samples_leaf=2, max_features='sqrt'
- **ExtraTrees**: n_estimators=500, max_depth=8, min_samples_leaf=2
- **AdaBoost**: n_estimators=500, learning_rate=0.75
- **GradientBoosting**: n_estimators=500, max_depth=5, learning_rate=0.05
- **XGBoost**: n_estimators=500, max_depth=4, learning_rate=0.02, subsample=0.8, colsample_bytree=0.8
- **LightGBM**: n_estimators=500, max_depth=6, learning_rate=0.05, num_leaves=31
- **CatBoost**: iterations=500, depth=6, learning_rate=0.05

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV with cross-validation
- Bayesian optimization (Optuna, hyperopt) for more efficient search
- Key parameters to tune:
  - Tree models: max_depth, n_estimators, learning_rate, min_samples_leaf
  - Regularization: reg_alpha, reg_lambda for XGBoost/LightGBM

## Validation Strategy
- Use StratifiedKFold (5 folds) to maintain class distribution
- Cross-validation score ~82-84% is typical for good models
- Watch for overfitting on small dataset
- Consider repeated K-fold for more stable estimates

## Feature Selection
- Use Recursive Feature Elimination (RFE) with cross-validation
- Feature importance from tree models to identify key predictors
- Top features typically: Sex, Pclass, Title, Fare, Age, FamilySize
- SelectFromModel with threshold='median' or 'mean'

## Preprocessing Pipeline
1. Combine train and test for consistent encoding
2. Extract features (Title, FamilySize, Deck, etc.)
3. Handle missing values (grouped imputation)
4. Encode categorical variables (LabelEncoder or OneHotEncoder)
5. Create bins for continuous features
6. Drop unnecessary columns (PassengerId, Name, Ticket, Cabin)
7. Optionally scale features for models that need it (SVM, Logistic Regression)

## Key Insights from Top Kernels
- Sex is the strongest predictor (females 74% vs males 19% survival)
- Pclass strongly correlates with survival (1st: 63%, 2nd: 47%, 3rd: 24%)
- "Women and children first" policy is evident in data
- Title extraction captures both gender and age information
- Ensemble methods consistently outperform single models
- Feature engineering is more important than model selection for this dataset

## Advanced Techniques
- **Woman-Child-Group Model**: Rule-based model focusing on women and children, can achieve ~85%
- **Genetic Programming**: Can achieve up to 91% but requires specialized libraries
- **Neural Networks**: TabNet or simple MLPs can work but often don't outperform tree ensembles on small data
- **AutoML**: Use auto-sklearn, H2O AutoML, or TPOT for automated model selection

## Submission Tips
- Ensure exactly 418 predictions for test set
- Format: PassengerId, Survived (0 or 1)
- Multiple submissions allowed - try different ensemble combinations
- Track which features/models give best leaderboard scores
- Consider creating multiple diverse models and ensembling their predictions
