{
  "query": "What techniques achieve the highest accuracy on Kaggle Titanic competition beyond 80%?",
  "results": [
    {
      "title": "ANSWER",
      "text": "As of\u202f2025, applying extensive feature engineering (e.g., extracting titles, family size, cabin deck), XGBoost\u2011based imputation for missing ages, aggressive hyper\u2011parameter tuning (grid/random/Bayesian search), and ensemble methods such as soft\u2011voting or stacking of models like XGBoost, Random\u202fForest and Logistic\u202fRegression can push Titanic competition scores above\u202f80\u202f% (up to\u202f~84\u202f% accuracy) ([python.plainenglish.io](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb), [atlantis-press.com](https://www.atlantis-press.com/article/126015317.pdf), [dataquest.io](https://www.dataquest.io/blog/kaggle-fundamentals)).",
      "url": ""
    },
    {
      "title": "Kaggle Fundamentals: The Titanic Competition - Dataquest",
      "text": "Kaggle Fundamentals: The Titanic Competition &#8211; Dataquest\n[](https://www.dataquest.io)\n* [Profile](https://app.dataquest.io/profile)\n* [Account](https://app.dataquest.io/settings/account)\n* [Subscription](https://app.dataquest.io/settings/subscription)\n* [Teams](https://www.dataquest.io/for-business/)\n* [Help](https://support.dataquest.io/en)\n* [Logout](https://app.dataquest.io/logout)\n[](https://www.dataquest.io/#)\nNew Year Launchpad: Lift Off Your Data Career with 57% Off Lifetime Plan.\n[Save Now](https://app.dataquest.io/payment?plan_interval=lifetime)\nMarch 25, 2025\n# Kaggle Fundamentals: The Titanic Competition\n[Kaggle](https://www.kaggle.com/)is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins the competition if it's the most accurate on a particular data set. Kaggle is a fun way to practice your machine learning skills.\nIn this tutorial we'll learn learn how to:\n* Approach a Kaggle competition\n* Explore the competition data and learn about the competition topic\n* Prepare data for machine learning\n* Train a model\n* Measure the accuracy of your model\n* Prepare and make your first Kaggle submission\nThis tutorial presumes you have an understanding of Python and the pandas library. If you need to learn about these, we recommend our[pandas tutorial](https://www.dataquest.io/blog/pandas-python-tutorial/)blog post.\n## The Titanic competition\nKaggle has created a number of competitions designed for beginners. The most popular of these competitions, and the one we'll be looking at, is about predicting which passengers survived the[sinking of the Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic).\nIn this[Titanic ML competition](https://www.kaggle.com/competitions/titanic), we have data about passengers onboard the Titanic, and we'll see if we can use that information to predict whether those people survived or not. Before we start looking at this specific competition, let's take a moment to understand how Kaggle competitions work.\nEach Kaggle competition has two key data files that you will work with - a**training**set and a**testing**set.\nThe training set contains data we can use to train our model. It has a number of feature columns which contain various descriptive data, as well as a column of the target values we are trying to predict: in this case,`Survival`.\nThe testing set contains all of the same feature columns, but is missing the target value column. Additionally, the testing set usually has fewer observations (rows) than the training set.\nThis is useful because we want as much data as we can to train our model on. Once we have trained our model on the training set, we will use that model to make predictions on the data from the testing set, and submit those predictions to Kaggle.\nIn this competition, the two files are named`test.csv`and`train.csv`. We'll start by using[`pandas.read\\_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)to read both files and then inspect their size.\n```\n`import pandas as pd\ntest = pd.read\\_csv(\"test.csv\")\ntrain = pd.read\\_csv(\"train.csv\")\nprint(\"Dimensions of train: {}\".format(train.shape))\nprint(\"Dimensions of test: {}\".format(test.shape))`\n```\n```\n`Dimensions of train: (891, 12)\nDimensions of test: (418, 11)`\n```\n## Exploring the data\nThe files we just opened are available on[the data page for the Titanic competition on Kaggle](https://www.kaggle.com/competitions/titanic/data). That page also has a**data dictionary**, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n* `PassengerID`\u2014 A column added by Kaggle to identify each row and make submissions easier\n* `Survived`\u2014 Whether the passenger survived or not and the value we are predicting*(0=No, 1=Yes)*\n* `Pclass`\u2014 The class of the ticket the passenger purchased*(1=1st, 2=2nd, 3=3rd)*\n* `Sex`\u2014 The passenger's sex\n* `Age`\u2014 The passenger's age in years\n* `SibSp`\u2014 The number of siblings or spouses the passenger had aboard the Titanic\n* `Parch`\u2014 The number of parents or children the passenger had aboard the Titanic\n* `Ticket`\u2014 The passenger's ticket number\n* `Fare`\u2014 The fare the passenger paid\n* `Cabin`\u2014 The passenger's cabin number\n* `Embarked`\u2014 The port where the passenger embarked*(C=Cherbourg, Q=Queenstown, S=Southampton)*\nThe data page on Kaggle has some additional notes about some of the columns. It's always worth exploring this in detail to get a full understanding of the data.\nLet's take a look at the first few rows of the`train`dataframe.\n```\n`train.head()`\n```\n||PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n0|1|0|3|Braund, Mr. Owen Harris|male|22.0|1|0|A/5 21171|7.2500|NaN|S|\n1|2|1|1|Cumings, Mrs. John Bradley (Florence Briggs Th...|female|38.0|1|0|PC 17599|71.2833|C85|C|\n2|3|1|3|Heikkinen, Miss. Laina|female|26.0|0|0|STON/O2. 3101282|7.9250|NaN|S|\n3|4|1|1|Futrelle, Mrs. Jacques Heath (Lily May Peel)|female|35.0|1|0|113803|53.1000|C123|S|\n4|5|0|3|Allen, Mr. William Henry|male|35.0|0|0|373450|8.0500|NaN|S|\nThe type of machine learning we will be doing is called**classification**, because when we make predictions we are classifying each passenger as 'survived' or not. More specifically, we are performing**binary classification**, which means that there are only two different states we are classifying.\nIn any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie[Titanic](https://en.wikipedia.org/wiki/Titanic_(1997_film))would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\nThis indicates that`Age`,`Sex`, and`PClass`may be good predictors of survival. We'll start by exploring`Sex`and`Pclass`by visualizing the data.\nBecause the`Survived`column contains`0`if the passenger did not survive and`1`if they did, we can segment our data by sex and calculate the mean of this column. We can use`DataFrame.pivot\\_table()`to easily do this:\n```\n`import matplotlib.pyplot as plt\n%matplotlib inline\nsex\\_pivot = train.pivot\\_table(index=\"Sex\",values=\"Survived\")\nsex\\_pivot.plot.bar()\nplt.show()`\n```\nWe can immediately see that females survived in much higher proportions than males did. Let's do the same with the`Pclass`column.\n```\n`class\\_pivot = train.pivot\\_table(index=\"Pclass\",values=\"Survived\")\nclass\\_pivot.plot.bar()\nplt.show()`\n```\n## Exploring and converting the age column\nThe`Sex`and`PClass`columns are what we call**categorical**features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).\nLet's take a look at the`Age`column using[`Series.describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html).\n```\n`train[\"Age\"].describe()`\n```\n```\n`count 714.000000\nmean 29.699118\nstd 14.526497\nmin 0.420000\n25% 20.125000\n50% 28.000000\n75% 38.000000\nmax 80.000000\nName: Age, dtype: float64`\n```\nThe`Age`column contains numbers ranging from`0.42`to`80.0`(If you look at Kaggle's data page, it informs us that`Age`is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 814 rows we discovered that the`train`data set had earlier which indicates we have some missing values.\nAll of this means that the`Age`column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at distribution of values in a continuous numerical set is to use histograms. We can create two histograms to compare visuall...",
      "url": "https://www.dataquest.io/blog/kaggle-fundamentals"
    },
    {
      "title": "",
      "text": "Titanic Survival Prediction Enhanced by Innovative \nFeature Engineering and Multi-Model Ensemble \nOptimization \nHanzhi Li \n1 School of Software Engineering, South China University of Technology, Guangzhou, \nGuangdong, 510006, China \n202230321188@mail.scut.edu.cn\nAbstract. This study enhances Titanic survival prediction through advanced \nfeature engineering and ensemble model optimization. The Titanic dataset \npresents a classic binary classification problem requiring the prediction of \npassenger survival based on demographic and ticket information. Our \nmethodology employs systematic preprocessing where missing values are \nintelligently imputed, including eXtreme Gradient Boosting (XGBoost) \npredictions for age values. Novel features were extracted from passenger \nnames, cabin information, and family relationships to improve predictive \npower. Feature importance was evaluated using Random Forest and XGBoost \nalgorithms, with SelectFromModel and Recursive Feature Elimination applied \nfor effective feature selection. Three classification algorithms\u2014logistic \nregression, random forest, and XGBoost\u2014were systematically optimized using \ngrid search, random search, and Bayesian optimization techniques. Based on \nthese optimized models, soft voting and stacking ensemble approaches were \nimplemented. Cross-validation results demonstrate that ensemble methods \nachieved superior accuracy (0.8361) compared to individual models: logistic \nregression (0.8339), random forest (0.8350), and XGBoost (0.8204). This \nresearch provides valuable optimization strategies for similar classification \ntasks, particularly highlighting how feature engineering combined with \nensemble methods can substantially enhance predictive performance while \nmaintaining computational efficiency. \nKeywords: Titanic Survival Prediction, Feature Engineering, Ensemble \nLearning, Hyperparameter Tuning. \n1 Introduction \nThe collision of the Titanic with an iceberg in 1912 was the cause of this catastrophe \nthat left 1,517 lives lost and is one of the most dangerous maritime accidents of the \ntwentieth century. Its historical significance and the multidimensional nature of \npassenger data (including social class, gender, age, family relationships, etc.) have \nestablished it as a classic binary classification benchmark dataset in machine learning. \n\u00a9 The Author(s) 2025\nY. Sun (ed.), Proceedings of the 2025 3rd International Conference on Image, Algorithms, and Artificial\nIntelligence (ICIAAI 2025), Advances in Computer Science Research 122,\nhttps://doi.org/10.2991/978-94-6463-823-3_19\n208 H. Li\nDespite extensive research on this problem, traditional methods exhibit \nsignificant limitations. Early studies predominantly focused on the direct application \nof basic features, achieving prediction accuracy generally below 80% [1]. For \ninstance, simple rules based on gender and passenger class could rapidly distinguish \nsurvival groups. However, these methods failed to fully explore nonlinear interactions \nbetween features and higher-order features. Moreover, approximately 20% of the age \nfield and 68% of the cabin number field (Cabin) are missing in the dataset. Traditional \napproaches often rely on mean imputation or directly delete missing values, leading to \ninformation loss and model bias [2]. \nAccording to previous research, traditional classification methods applied to the \nTitanic dataset achieved limited accuracy. These methods were constrained by their \ninability to capture complex feature interactions and their sensitivity to missing values \n[2]. Simultaneously, traditional methods predominantly depend on single models such \nas logistic regression and decision trees without conducting feature importance \nselection, resulting in lower prediction accuracy [3]. Recent studies have introduced \nensemble models but have not systematically compared their performance with single \nmodels, nor explored how different hyperparameter tuning methods influence \nprediction model performance in this dataset. \nTo address these challenges, this study employs advanced feature engineering \ntechniques to extract new features, utilizes the relationships between features and \neXtreme Gradient Boosting (XGBoost) for missing value imputation, applies feature \nselection techniques like random forests and XGBoost to identify key features for \nsurvival prediction, and performs hyperparameter tuning of logistic regression, \nrandom forests, and XGBoost models using grid search, random search, and Bayesian \noptimization. Based on these methods, soft voting and stacking ensemble models are \ntrained, and the performance of models under various parameter conditions is further \ncompared. \n2 Methods \n2.1 DataSet \nThis study is based on the Titanic dataset provided by the Kaggle platform [4]. The \ndataset consists of two CSV files: the training set and the test set. The training set \ncontains 891 samples, while the test set contains 418 samples. The data features \ninclude PassengerId, Name, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and \nEmbarked. Additionally, the training set includes an additional target variable, \n\"Survived\". Analysis reveals missing values in the Age, Cabin, and Embarked \ncolumns in the training set, and the Age, Cabin, and Fare columns in the test set, with \nAge and Cabin having a relatively high proportion of missing values [2]. \n2.2 Data Processing \nTo facilitate data processing, this study first combined the training set and the test set\nto form a complete dataset, called the \"merged set\". The missing values in the\nTitanic Survival Prediction Enhanced by Innovative Feature \u2026 209\nEmbarked and Fare columns, which had relatively few missing values, were filled \nfirst. By analyzing the sample features corresponding to the missing values in \nEmbarked, it was found that these samples all had a Pclass of 1, a Fare of 80.0, a \nCabin of B28, and a Sex of female. Considering the correlations between features, a \nstrong relationship among Pclass, Fare, and Embarked was assumed. Based on this \nassumption, box plots of Fare distribution under different combinations of Embarked \nand Pclass were created (Figure 1), and it was determined that when Fare is 80 and \nPclass is 1, Embarked is more likely to be 'C'. Therefore, the missing Embarked \nvalues were filled with 'C'. Similarly, for cases where Embarked = 'S' and Pclass = 3, \nthe median Fare was calculated and used to fill missing Fare values in the test set [2]. \nFig. 1. Fare Distribution by Embarked and Pclass (Photo/Picture credit: Original). \nNext, the Cabin column was processed. Due to the high number of missing values, the \nanalysis was based only on available information. Records with Cabin information \nwere filtered, and the distribution of cabin classes between survivors and \nnon-survivors was compared. Cabin values containing multiple cabins were split into \nlists, and the data was expanded row-wise to generate new features. Two new features \nwere extracted: Deck (the letter part, filled with \"U\" if none) and Cabin number (the \nnumeric part, converted to a numeric type). The Cabin number was further binned, \nand survival rates within different intervals were analyzed [5]. The analysis revealed \nthat passengers in cabin numbers ranging from 0-50 had a survival rate of \napproximately 45%, while those in ranges 51-100 showed a higher survival rate of \naround 60%. Passengers in cabin numbers above 100 had mixed survival rates, \nranging from 40% to 55%, depending on the specific range. Meanwhile, specific \nsurvival rates for each Deck were calculated, which showed notable variations: Deck \nB had the highest survival rate at approximately 74%, followed by Decks D and E at \naround 75% and 75%, respectively, while Deck U (unknown) had the lowest survival \nrate at about 30%. Finally, the expanded data was re-aggregated into the original \nmerged dataset (Fig. 1). \nThe Title (such as \"Mr\", \"Mrs\") was extracted from the Name column as a new\nfeature. Rare Titl...",
      "url": "https://www.atlantis-press.com/article/126015317.pdf"
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy] - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    },
    {
      "title": "Titanic Model with 90% accuracy | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=1e4ba90eba1c8273e4fe:1:11427)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/vinothan/titanic-model-with-90-accuracy"
    },
    {
      "title": "How to further improve the kaggle titanic submission accuracy?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How to further improve the kaggle titanic submission accuracy?](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 10 months ago\n\nModified [7 years, 10 months ago](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy?lastactivity)\n\nViewed\n7k times\n\n4\n\n$\\\\begingroup$\n\nI am working on the Titanic dataset. So far my submission has 0.78 score using soft majority voting with logistic regression and random forest. As for the features, I used Pclass, Age, SibSp, Parch, Fare, Sex, Embarked.\n\nMy question is how to further boost the score for this classification problem?\n\nOne thing I tried is to add more classifiers for the majority voting, but it does not help, it even worthens the result. How do I understand this worthening effect?\n\nThanks for your insight.\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [beginner](https://datascience.stackexchange.com/questions/tagged/beginner)\n\n[Share](https://datascience.stackexchange.com/q/13104)\n\n[Improve this question](https://datascience.stackexchange.com/posts/13104/edit)\n\nFollow\n\n[edited Jul 30, 2016 at 14:03](https://datascience.stackexchange.com/posts/13104/revisions)\n\nnos\n\nasked Jul 30, 2016 at 13:15\n\n[![nos's user avatar](https://www.gravatar.com/avatar/4ba32c06ed067564a6c6512de5217657?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/21955/nos)\n\n[nos](https://datascience.stackexchange.com/users/21955/nos) nos\n\n15311 silver badge66 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n5\n\n$\\\\begingroup$\n\nbig question.\n\nOk so here's a few things I'd look at if I was you..\n\n1. Have you tried any feature engineering ?(it sounds like you've just used the features in the training set but I can't be 100%)\n2. Random Forests should do pretty well, but maybe try xgboost too? It's quite good at everything on Kaggle. SVM's could be worth a go also if you're thinking of stacking/ensembling.\n3. Check out some of the tutorials around this competition. There's hundreds of them and most of them are great.\n\n**Links:**\n\n[R #1 (my favourite)](http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/)\n\n[R #2](http://www.numbrcrunch.com/blog/the-good-ol-titanic-kaggle-competition-pt-1)\n\n[Python #1](http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/)\n\n[Python #2](https://github.com/agconti/kaggle-titanic)\n\n...Hopefully this helps\n\n[Share](https://datascience.stackexchange.com/a/13139)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/13139/edit)\n\nFollow\n\nanswered Aug 2, 2016 at 2:52\n\n[![plumbus_bouquet's user avatar](https://www.gravatar.com/avatar/031741853441d7c93e9a50ffbabffb86?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/21702/plumbus-bouquet)\n\n[plumbus\\_bouquet](https://datascience.stackexchange.com/users/21702/plumbus-bouquet) plumbus\\_bouquet\n\n34011 silver badge44 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n3\n\n$\\\\begingroup$\n\nOkay, I am currently at 0.81340 in the competiton. And I will just clear out certain things. I would suggest that you try feature engineering before you go for ensemble methods. There are actually quite a decent tutorials as mentioned before.\nOne can actually score upto at least 0.82 just relying on feature engineering and a ten-fold cross validated RandomForest.\nCertain things for you to ponder at:\n\n- Look at the Age, what other information does it give you.\n- Do SibSp and Parch actually represent different things?\n- Can you get something out of the Name of the passengers?\n\nAll the Best.\n\nCheers.\n\n[Share](https://datascience.stackexchange.com/a/13223)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/13223/edit)\n\nFollow\n\nanswered Aug 5, 2016 at 7:35\n\n[![Himanshu Rai's user avatar](https://graph.facebook.com/10205672827392400/picture?type=large)](https://datascience.stackexchange.com/users/21608/himanshu-rai)\n\n[Himanshu Rai](https://datascience.stackexchange.com/users/21608/himanshu-rai) Himanshu Rai\n\n1,8481212 silver badges1010 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f13104%2fhow-to-further-improve-the-kaggle-titanic-submission-accuracy%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [classification](https://datascience.stackexchange.com/questions/tagged/classification) - [beginner](https://datascience.stackexchange.com/questions/tagged/beginner)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Linked\n\n[0](https://datascience.stackexchange.com/q/57708) [What should be the criteria to select features using correlation factors between features?](https://datascience.stackexchange.com/questions/57708/what-should-be-the-criteria-to-select-features-using-correlation-factors-between?noredirect=1)\n\n#### Related\n\n[1](https://datascience.stackexchange.com/q/5119) [Kaggle Titanic Survival Table an example of Naive Bayes?](https://datascience.stackexchange.com/questions/5119/kaggle-titanic-survival-table-an-example-of-naive-bayes)\n\n[6](https://datascience.stackexchange.com/q/8339) [Voting combined results from different classifiers gave bad accuracy](https://datascience.stackexchange.com/questions/8339/voting-combined-results-from-different-classifiers-gave-bad-accuracy)\n\n[1](https://datascience.stackexchange.com/q/16349) [How to improve accuracy further for forest cover prediction](https://datascience.stackexchange.com/questions/16349/how-to-improve-accuracy-further-for-forest-cover-prediction)\n\n[3](https://datascience.stackexchange.com/q/65196) [Kaggle Titanic submission score is higher than local accuracy score](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\n\n[1](https://datascience.stackexchange.com/q/69193) [Feature Engineering - correlation with binary outcome - Titanic Dataset - Ticket feature](https://datascience.stackexchange.com/questions/69193...",
      "url": "https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy"
    },
    {
      "title": "Creating Predictions for Kaggle's Titanic Challenge",
      "text": "Creating Predictions for Kaggle&#39;s Titanic Challenge - Free Video Tutorial\n# Creating Predictions for Kaggle&#39;s Titanic Challenge\nCreate predictions using a random forest model, format the Kaggle submission CSV, and upload it to Kaggle for scoring.\nMarch 12, 2025\nby[Colin Jaffe](https://www.graduateschool.edu/blog/authors/colin-jaffe)\nRead more in[Machine Learning](https://www.graduateschool.edu/learn/machine-learning)\nShare\n[Share on X](<https://x.com/intent/tweet?&amp;text=Creating Predictions for Kaggle's Titanic Challenge - Graduate School USA resource&amp;url=https://www.graduateschool.edu/learn/creating-predictions-kaggle-titanic-challenge/creating-predictions-kaggle-titanic-challenge&amp;via=>)[Share on Facebook](https://www.facebook.com/sharer/sharer.php?u=https://www.graduateschool.edu/learn/creating-predictions-kaggle-titanic-challenge/creating-predictions-kaggle-titanic-challenge)[Share on LinkedIn](<https://www.linkedin.com/shareArticle?url=https://www.graduateschool.edu/learn/creating-predictions-kaggle-titanic-challenge/creating-predictions-kaggle-titanic-challenge&amp;title=Creating Predictions for Kaggle's Titanic Challenge - Graduate School USA resource>)\nCreate accurate predictions using Python and Random Forest classifiers, then evaluate your model's effectiveness by submitting results to Kaggle. Learn the complete workflow from preparing prediction arrays to formatting CSV submissions for Kaggle's Titanic competition.\n## Key Insights\n* Created a prediction array by applying model.predict on the test dataset, generating an array consisting of zeros and ones indicating passenger survival.\n* Prepared a submission CSV file conforming strictly to Kaggle's format by retrieving the passenger IDs from the original Titanic test dataset and including predictions, explicitly setting index=False to exclude unwanted index columns.\n* Submitted the CSV file to Kaggle's Titanic Machine Learning competition, achieving an accuracy score around 77\u201379%, providing students motivation to explore improvements and adjustments to further optimize the Random Forest classifier model.\nThis lesson is a preview from our[Data Science & AI Certificate Online](https://www.graduateschool.edu/certificates/data-science-online)(includes software) and[Python Certification Online](https://www.graduateschool.edu/certificates/python-certification-self-paced)(includes software &amp; exam). Enroll in a course for detailed lessons, live instructor support, and project-based training.\nOkay, let's pick up right where we left off. We're going to create a predictions array. Let's call it predictions.\nAnd it will be what happens when we run model.predict on our X\\_test. It is 400 and something zeros and ones. Not very helpful for us without context for whether we got these correct or not, but we'll use that in our next step, which is to create a data frame that will have Passenger ID and predictions.\nNow, we're going to submit this to Kaggle, and it has to be in this exact format so that its algorithm can give us, can check it against the Y\\_test answers, and give us an accuracy score. We need our Passenger ID. I foolishly removed and overwrote the X\\_test and got rid of the Passenger ID, but we can get it back.\nWhat we're going to do is simply read from the test CSV again and get that right back. So, I'm going to create a Titanic\\_test data frame, and it's reading the CSV from our base URL plus CSV/test\\_Titanic.csv. Let's double-check that. Yep, it's got the Passenger ID that I got rid of in X\\_test.\nOkay, great. Since we've got that, let's now make a data frame called Titanic\\_submission. Sure.\nTitanic\\_submission is a new data frame, and it's got a Passenger ID column that should equal the Titanic\\_test data frame\u2019s Passenger ID. Then we're going to include a Survived column, and it\u2019s going to equal our predictions from above\u2014these zeros and ones up here.\nThen we can check our submission. It looks pretty good. Passenger ID and zeros and ones.\nJust those two columns are all that Kaggle wants. Now, saving it as a CSV is a little bit of work, but not too bad. We want to save it to Google Drive in our case, then download it.\nAnd we\u2019re going to make sure to set index=False. If we don\u2019t do that, we\u2019ll get another column that contains these indexes. We don\u2019t want that.\nWe want only Passenger ID and Survived as columns in the CSV we\u2019re uploading. We\u2019re going to use Titanic\\_submission.to\\_csv, and we\u2019re going to save it to our base URL on Google Drive plus CSV/Kaggle\\_submission.csv.\nFinally, index=False so that we can only have those two columns in it. Perfect. Okay.\nRun that line of code. Now we're ready to submit that to Kaggle. It should be downloaded to your Google Drive.\nLet\u2019s check it out. Here\u2019s my Kaggle\\_submission.csv, but if you need to find it, it\u2019s in my drive. It should be in[Python](https://www.graduateschool.edu/learn/python/what-is-python)*Machine Learning Bootcamp*, CSV file, Kaggle\\_submission.csv is what I just named it.\nSo, I\u2019m going to download that now. Right-click on it. Yep.\nClick Download. And yep, it\u2019s downloaded. Now I\u2019m going to go to Kaggle, and we\u2019re going to submit it.\nIf you don\u2019t have a Kaggle account, you\u2019ll need one for this step, but you should get one anyway. Kaggle is fantastic.\nIt\u2019s a big part of the machine learning community, and it\u2019s a great place to learn. What you\u2019re going to do is find the Titanic competition. If you search at the top, let me walk through that a little bit more.\nGo to competitions and type Titanic in the search bar.\nClick on Titanic under competitions, Titanic: Machine Learning from Disaster. What you\u2019re going to do is submit our CSV.\nClick \u2018Submit Prediction\u2019 up here.\nThen, find the file you downloaded.\nNow, it\u2019ll run, and it will then give you a score. It should be around 79%. Here\u2019s the one I just did.\nOoh, down to 77%. I must have made a change.\nIt\u2019s a fine score. It\u2019s a great starting point for thinking, \u2018How do I improve my score?\u2019\nHow can I improve my results? What factors improve the score? What can I adjust? What can I fine-tune?\nAnd we\u2019ll continue with the next lesson.\n## Colin Jaffe\nColin Jaffe is a programmer, writer, and teacher with a passion for creative code, customizable computing environments, and simple puns. He loves teaching code, from the fundamentals of algorithmic thinking to the business logic and user flow of application building\u2014he particularly enjoys teaching JavaScript, Python, API design, and front-end frameworks.\nColin has taught code to a diverse group of students since learning to code himself, including young men of color at All-Star Code, elementary school kids at The Coding Space, and marginalized groups at Pursuit. He also works as an instructor for Noble Desktop, where he teaches classes in the Full-Stack Web Development Certificate and the Data Science &amp; AI Certificate.\nColin lives in Brooklyn with his wife, two kids, and many intricate board games.\n[More articles by Colin Jaffe](https://www.graduateschool.edu/blog/authors/colin-jaffe)\n### **How to Learn Machine Learning**\nBuild practical, career-focused machine learning skills through hands-on training designed for beginners and professionals alike. Learn fundamental tools and workflows that prepare you for real-world projects or industry certification.\n* Explore[machine learning classes, bootcamps, and certifications](https://www.graduateschool.edu/topics/machine-learning-classes)to find the right training format for your goals.\n* Take the[Python Data Science &amp; Machine Learning Bootcamp](https://www.graduateschool.edu/certificates/python-programming)to gain comprehensive, career-ready skills\n* Prepare for professional exams or credentials with the[Python Certification Online](https://www.graduateschool.edu/certificates/python-certification)\n* Develop your team\u2019s expertise through customized[group training](https://www.graduateschool.edu/group-training)sessions",
      "url": "https://www.graduateschool.edu/learn/machine-learning/creating-predictions-kaggle-titanic-challenge"
    },
    {
      "title": "Space Titanic Kaggle competition 0.8066 score solution",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Spaceship Titanic Kaggle competition top 7% score solution\n\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\n\n7 min read\n\n\u00b7\n\nJan 17, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nKaggle\u2019s Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an \u201canomaly\u201d using records recovered from the spaceship\u2019s damaged computer system. The \u201clore\u201d of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship\u2019s passengers.\n\n## Import libraries\n\n```\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\n```\n\n## Let\u2019s Start\n\nAs you can see, after I loaded the dataset, I removed both the \u201cPassenger Id\u201d and \u201cName\u201d columns. They are not going to provide any useful or important information to the prediction. Someone\u2019s name or Id does not change the probability of being Transported.\n\n```\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\n```\n\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the \u201cCabin\u201d column. Notice that the rows on the \u201cCabin\u201d column follow a specific pattern. Something like this: \u201cA/5/S\u201d, \u201cC/1/S\u201d, \u201cF/7/P\u201d. And I decided to investigate it. So, to make things simple I split the rows of the \u201cCabin\u201d into three columns based on both slashes (\u201d/\u201d) of the rows. For example, the \u201cA/5/S\u201d row would be transformed into three new columns: The first one is named \u201ccabin\\_code\u201d referring tothe character behind the first slash (A). The second one named \u201cid\\_cabin\u201d refers to the character behind the second slash (5). The third one named \u201ccabin\\_sector\u201d refers to the character after the second slash (S). And we end up with three new columns.\n\n```\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\n```\n\nFirst of all, I noticed that \u201ccabin\\_code\u201d only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\n\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\n\nPress enter or click to view image in full size\n\nI did the same thing with the \u201ccabin\\_sector\u201d column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\n\nPress enter or click to view image in full size\n\nThis means that this exploration of the original \u201cCabin\u201d column is worth it since new insights are being added to the model.\n\nNow, we can finally delete the \u201cCabin\u201d column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\n\nI also removed the \u201ccabin\\_id\u201d and the column that I had created. As I said before, the Id will not interfere with the model\u2019s predictive ability.\n\nSo used: df.drop(columns=\\[\u201cCabin\u201d,\u201did\\_cabin\u201d\\], inplace=True) to drop both columns\n\nBefore splitting our data, the \u201cTransported\u201d column must be in a binary format. As you can see, I switched \u201cTrue\u201d for 1 and \u201cFalse\u201d for 0.\n\nBinary transformation: df\\[\u201cTransported\u201d\\] = df\\[\u201cTransported\u201d\\].map({True:1, False:0})\n\nI also removed every row that had missing values in the \u201ccabin\\_code\u201d column.\n\n```\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\n```\n\nNow, we can finally split the data and proceed to develop our model.\n\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\n\n```\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\n```\n\n```\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n```\n\n```\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\n```\n\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don\u2019t know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\n\n```\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\n```\n\nAnd with column transformer, we can attach both transformations to one variable that I named \u201ctransformer\u201d. Observe that we also have to specify the type of data to which the pipeline will be applied t...",
      "url": "https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6"
    }
  ]
}