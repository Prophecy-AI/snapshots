{
  "query": "How do some Kaggle Titanic submissions achieve 100% accuracy or perfect score?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Some \u201cperfect\u2011score\u201d Titanic submissions aren\u2019t the product of a superior modelling trick \u2013 they rely on data leakage or outright cheating.  The competition provides a **train.csv** with known outcomes and a **test.csv** without labels; the only way to obtain 100\u202f% accuracy on the test set is to have access to those hidden labels.  Participants who download the test file after the competition ends, or who augment the training data with an external dataset that already contains the true survival values, can train a model that simply memorises the answers and then submit a flawless prediction file.  As one Reddit user observed, the only way to reach scores above ~0.85 is to \u201cdownload the test set with the info about who survived,\u201d implying illicit use of the answer key\u30102](https://www.reddit.com/r/MachineLearning/comments/lpyfwe/d_kaggle_titanic_challenge_maximum_legit)\u3011.  \n\nA related explanation appears in a Medium post that notes many top\u2011scorers \u201ctrain their model on an external dataset and thus obtain a model that classifies the test data with 100\u202f% accuracy,\u201d essentially cheating by bringing in data that already encodes the test labels\u30109](https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1)\u3011.  In short, genuine machine\u2011learning techniques (logistic regression, random forests, ensembles, etc.) typically cap out around 80\u201185\u202f% on the Titanic challenge; any claim of perfect accuracy almost always signals the use of leaked or external label information rather than a legitimate algorithmic breakthrough.",
      "url": ""
    },
    {
      "title": "[D] Kaggle Titanic Challenge - maximum (legit) optainable score?",
      "text": "<div><div><p>I am learning ML and regardless of the method I got max 77% accurate predictions in the Titanic Challenge.. But normally ~75%</p><p>I tried so far Naive Bayes with some text mining, Logistic Regression, decision trees, knn , ... everything.</p><p>The best model I could produce was with random forest and ironically it was the one which I cared less. What is the max obtainable score? With what technique did you have the most succes in the Titanic challenge, for those who participated?</p><p>Is it even possible to reach 0,85-0,90 % or are all cheaters who have downloaded the test set with the info about who survived?</p></div>||||I|||| Press J to jump to the feed. Press question mark to learn the rest of the keyboard shortcuts\nSearch all of Reddit\nGet AppLog In\nUser account menu\nFound the internet!\nFeeds\nHome Popular\nTopics\nGaming\nValheim Genshin Impact Minecraft Pokimane Halo Infinite Call of Duty: Warzone Path of Exile Hollow Knight: Silksong Escape from Tarkov Watch Dogs: Legion\nSports\nNFL NBA Megan Anderson Atlanta Hawks Los Angeles Lakers Boston Celtics Arsenal F.C. Philadelphia 76ers Premier League UFC\nBusiness, Economics, and Finance\nGameStop Moderna Pfizer Johnson & Johnson AstraZeneca Walgreens Best Buy Novavax SpaceX Tesla\nCrypto\nCardano Dogecoin Algorand Bitcoin Litecoin Basic Attention Token Bitcoin Cash\nTelevision\nThe Real Housewives of Atlanta The Bachelor Sister Wives 90 Day Fiance Wife Swap The Amazing Race Australia Married at First Sight The Real Housewives of Dallas My 600-lb Life Last Week Tonight with John Oliver\nCelebrity\nKim Kardashian Doja Cat Iggy Azalea Anya Taylor-Joy Jamie Lee Curtis Natalie Portman Henry Cavill Millie Bobby Brown Tom Hiddleston Keanu Reeves\nMore Topics\nAnimals and Pets Anime Art Cars and Motor Vehicles Crafts and DIY Culture, Race, and Ethnicity Ethics and Philosophy Fashion Food and Drink History Hobbies Law Learning and Education Military Movies Music Place Podcasts and Streamers Politics Programming Reading, Writing, and Literature Religion and Spirituality Science Tabletop Games Technology Travel\nCreate an account to follow your favorite communities and start taking part in conversations.\nJoin Reddit\nr/ MachineLearning\nPosts\nr/MachineLearning\n5\nPosted by 2 years ago\n[D] Kaggle Titanic Challenge - maximum (legit) optainable score?\nDiscussion\nI am learning ML and regardless of the method I got max 77% accurate predictions in the Titanic Challenge.. But normally ~75%\nI tried so far Naive Bayes with some text mining, Logistic Regression, decision trees, knn , ... everything.\nThe best model I could produce was with random forest and ironically it was the one which I cared less. What is the max obtainable score? With what technique did you have the most succes in the Titanic challenge, for those who participated?\nIs it even possible to reach 0,85-0,90 % or are all cheaters who have downloaded the test set with the info about who survived?\n3 comments\nshare\nsave\nhide\nreport\n73% Upvoted\nSort by: best\n|\nlevel 1\n\u00b7 2 yr. ago\nThe \"woman-child-group\" model gets 85% accuracy on Titanic, see https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688 . It is also possible to get 91% accuracy using genetic programming, see https://www.kaggle.com/scirpus/my-first-gp-in-r , but that is a very different type of method than what you tried so far.\n3\nReply\nShare\nReportSaveFollow\nlevel 1\n\u00b7 2 yr. ago \u00b7 edited 2 yr. ago\nI hate to say this but 75% accuracy on Titatic dataset is a really bad result. Some ensembles and a little bit of feature engineering and you should get easily more than that.\nJust try to understand where is your model performing worse and why. Do some feature eng with that in mind and use ensembles. Naive Bayes and knn should not be even tested. Logistic Regression is very powerful, however you shoud pay lots of attention on your features and preprocess than like removing useless categories on categorical variables, bucketizing numerical features or expanding then with some polynomial expansion.\nTry LGBM, Catboost or Xgboost and you should see better results.\n9\nReply\nShare\nReportSaveFollow\nlevel 1\n[deleted]\n\u00b7 2 yr. ago\nI got 81% by literally just removing some features that seemed unimportant and putting it into a sklearn GBclassifier with no fine tuning, so you definitely can do better than 70s with actual work. However it does seem like you hit a wall pretty quickly as you get to mid 80s.\n2\nReply\nShare\nReportSaveFollow\nAbout Community\nr/MachineLearning\nWelcome to MachineLearning\nCreated Jul 29, 2009\n2.6m\nMembers\n838\nOnline\nTop 1%\nRanked by Size\nJoin\nTop posts february 22nd 2021Top posts of february, 2021Top posts 2021\nUser AgreementPrivacy policy\nContent policyModerator Code of Conduct\nReddit Inc \u00a9 2023. All rights reserved\nBack to Top\nAdvertisement",
      "url": "https://www.reddit.com/r/MachineLearning/comments/lpyfwe/d_kaggle_titanic_challenge_maximum_legit"
    },
    {
      "title": "How to score top 3% in Kaggle\u2019s ,,Titanic \u2014 Machine Learning from Disaster\u201d competition",
      "text": "<div><div><div><a href=\"https://anelmusic13.medium.com/?source=post_page---byline--13d056e262b1---------------------------------------\"><div><p></p></div></a></div><p>Hey, fellow machine learning enthusiasts. If you\u2019ve ever had a try at a Kaggle competition, chances are you\u2019re already familiar with the Titanic dataset. <br/>This competition is about predicting whether a passenger will survive the Titanic disaster or not.</p><p>With relatively little effort it is possible to rank among the top 30% of participants. Unfortunately, many of the top scorers train their model on an external dataset and thus obtain a model that classifies the test data with 100% accuracy. I couldn\u2019t tell you why someone would want to cheat in such an obvious way but what this essentially means is that you have to make an extra effort to get into the top 3%.</p><p><strong>Aim</strong> of this article:</p><ul><li>Explain step by step the end-to-end data pipeline that is needed to score among top 3%.</li><li>Discuss the thought process of a Machine Learning Engineer / Data Scientist in Data Cleaning and Feature Engineering.</li><li>Make it more difficult for future participants to stand out :)</li></ul><p><strong>You can find the complete code at:</strong></p><p><a href=\"https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis\">https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis</a></p><h2>1. Getting the data:</h2><figure><figcaption>Import needed modules for handling tabular data, plotting and accessing common ML</figcaption></figure><figure><figcaption>Provide data paths</figcaption></figure><p>In my case the train.csv as well as test.csv are located in the current working directory. Using pandas we can read and convert the csv-files to pandas dataframes:</p><figure><figcaption>Read data csv and convert it to pandas dataframe</figcaption></figure><p>Next, we can investigate the data to get a better understanding of the available features:</p><figure><figcaption>Show train.csv pandas dataframe</figcaption></figure><h2>2. Exploratory Data Analysis</h2><p>Typically, it is not necessary to use each available feature. Many of them do not provide additional information for the model and increase the training time unnecessarily. However, a more crucial problem that is often overlooked is that additional features inevitably require more data. Now why is that?</p><p><strong>Why additional features call for additional data:</strong></p><p>If you think about it, each additional feature adds a new dimension to the feature space. This in turn increases the distance between observations because each new dimension adds a non-negative term to the calculation of the euclidean distance. Thus the feature space becomes sparse. In a sparse feature space ML algorithms tend to overfit to noise and the only way to populate your feature space is by adding additional (and rich) data.</p><p>As you can see it is essential to explore which features should be considered and which should not.</p><p>I would argue that most important skill of a Machine Learning Engineer/Data Scientist is to be unbiased, not assume things and to ask good questions.</p><p>So, let\u2019s ask some questions:</p><h2>2.1 Question 1: Does the dataset contain missing values?</h2><p>In the very first step it is helpful to check if and how many entries are missing in the dataset.</p><figure><figcaption>Dataset missing values summary</figcaption></figure><figure><figcaption>Define function that creates missing value heatmap</figcaption></figure><figure><figcaption>Missing values heatmap</figcaption></figure><p>From the above plots, it can be seen that the training and similarly the test datasets contain features with missing values. The sparsest features are \u201cage\u201d and \u201ccabin\u201d.</p><p>A naive approach to solve this problem would be to remove the feature completely from the dataset. However, since we do not know how much information these they provide they further investigation is needed. Maybe we find that it would make sense to impute the missing values using sophisticated data imputation methods.</p><h2>2.1 Question 1: How many passengers survived?</h2><figure><figcaption>Check survival rate</figcaption></figure><p>If you prefer plots you can define the function below to plot a bar chart:</p><figure><figcaption>Define function for plotting bar charts</figcaption></figure><figure><figcaption>Bar chart survival rate</figcaption></figure><p>As expected the majority of passengers in the training data died and only 38% survived. This means that the training data suffers from data imbalance but it is not severe which is why I will not consider techniques like sampling to tackle the imbalance.</p><figure><figcaption>Degree of minority class imbalance [shorturl.at/fTV37]</figcaption></figure><h2>2.2 Question 2: Is the likelihood of survival dependent on gender?</h2><p>In 1912, about 110 years ago, women were generally considered to be the weaker sex and should be protected. Let\u2019s investigate if the men of that time were so dutiful even under the fear of death.</p><figure><figcaption>Passenger count based on gender</figcaption></figure><figure><figcaption>Survival ratio based on gender</figcaption></figure><figure><figcaption>Survival rate bar chart</figcaption></figure><p>Here, we can clearly see that even though the majority of passenger were men, the majority of survivors were women. The key observation here is that the survival rate for female passengers is 4 times higher than the survival rate of male passengers. This seems to confirm that the phrase \u201cwomen and children first\u201d does indeed seem to have been a rule to which men adhered to.</p><h2>2.3 Question 3: Could it be that the class to which a passenger belonged correlates with the probability of survival??</h2><figure><figcaption>Passenger count with respect to class</figcaption></figure><figure><figcaption>Passenger class distribution</figcaption></figure><figure><figcaption>Bar chart passenger class vs dependent (target) variable ,,survived\u201d</figcaption></figure><p>From the plots and tables above it becomes clear that the Pclass is an important factor to consider.</p><p><strong>Key observations:</strong></p><ul><li>Most passenger had class 3 tickets, yet only 24% of class 3 passengers survived.</li><li>Almost 63% of the passenger from class 1 survived.</li><li>Approx 50% of the class 2 passenger survived.</li></ul><p>However, it is not clear yet weather the class or the gender is the underlying and deciding factor. Which brings another important question:</p><h2>2.4 Question 4: Is the higher survival rate in Class 1 due to the class itself or due to a skewed gender distribution in which female passengers dominate?</h2><figure><figcaption>Function that plots bar chart with multiple features</figcaption></figure><figure><figcaption>Bar chart of gender vs surival rate</figcaption></figure><figure><figcaption>Percentage of male and female survivors with respect to gender</figcaption></figure><p>Here, we can see that the question raised above was justified. Irrespective of the class the most important factor when it comes to survival was gender. (At least between the two features Sex and Pclass). However, men had a significantly higher chance of survival if they bought class 1 tickets. This just shows that we should keep both features as both yield insightful information that should help our model.</p><p><strong>Key observations:</strong></p><ul><li>Survival Rate females 1. Class: 96,8%</li><li>Survival Rate females 2. Class: 92,1%</li><li>Survival Rate females 3. Class: 50%</li><li>Survival Rate male 1. Class: 36.8% <br/>(still significantly lower than 3. class females)</li></ul><h2>2.5 Question 5: Did a passengers age influence the chance of survival?</h2><figure><figcaption>Utility function for plotting a histogram and the kernel density estimate</figcaption></figure><figure><figcaption>Age distribution</figcaption></figure><figure><figcaption>Age kernel density</f...",
      "url": "https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1"
    },
    {
      "title": "Kaggle Titanic ML Challenge: Top 9% Score",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F243b5f45c8e9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fi-like-big-data-and-i-cannot-lie%2Fhow-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fi-like-big-data-and-i-cannot-lie%2Fhow-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# How I scored in the top 9% of Kaggle\u2019s Titanic Machine Learning Challenge\n\n[![Peter [REDACTED]](https://miro.medium.com/v2/resize:fill:88:88/1*Z1Od0VPK3aVnXQYmp0i8CQ.jpeg)](https://medium.com/@0xpeter?source=post_page-----243b5f45c8e9--------------------------------)[![I like Big Data and I cannot lie](https://miro.medium.com/v2/resize:fill:48:48/1*_uX9L-b0WCwN9gN9jeUbGg.png)](https://medium.com/i-like-big-data-and-i-cannot-lie?source=post_page-----243b5f45c8e9--------------------------------)\n\n[Peter \\[REDACTED\\]](https://medium.com/@0xpeter?source=post_page-----243b5f45c8e9--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb14d97da0cfd&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fi-like-big-data-and-i-cannot-lie%2Fhow-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9&user=Peter+%5BREDACTED%5D&userId=b14d97da0cfd&source=post_page-b14d97da0cfd----243b5f45c8e9---------------------post_header-----------)\n\nPublished in\n\n[I like Big Data and I cannot lie](https://medium.com/i-like-big-data-and-i-cannot-lie?source=post_page-----243b5f45c8e9--------------------------------)\n\n\u00b7\n\n10 min read\n\n\u00b7\n\nSep 24, 2017\n\n168\n\n4\n\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D243b5f45c8e9&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fi-like-big-data-and-i-cannot-lie%2Fhow-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9&source=-----243b5f45c8e9---------------------post_audio_button-----------)\n\nShare\n\n![](https://miro.medium.com/v2/resize:fit:700/1*5DEn2LeQsrwtWVNy3w6I5w.jpeg)\n\nFor those aspiring to be Data Scientists or simply those wanting to get their feet wet with machine learning, [Kaggle](https://www.kaggle.com/) is a great site to try. Kaggle is a website that hosts a ton of machine learning competitions presented either by Kaggle itself or major companies such as Google, Intel, and Facebook. They have a few beginner competitions for newbies, including their most popular one: [The Titanic Machine Learning Challenge.](https://www.kaggle.com/c/titanic)\n\nI just recently started \u201cKaggling\u201d and I must say, the challenges can be quite addicting as you try and improve your predictions and see your name soar up the leaderboards as your prediction scores improve. I decided to take a shot at the Titanic challenge and was able to crack the top 9% with one of my submissions.\n\n![](https://miro.medium.com/v2/resize:fit:700/1*1-gKQuA6YjrgKeZQPwhVww.png)\n\nThis post will be an abbreviated walk through of some of the data wrangling, feature engineering, and modeling I tried in order to achieve that score, but for the full code, feel free to checkout my [Jupyter Notebook](https://github.com/pbegle/Kaggle-Competitions/blob/master/Titanic/kaggle_titanic_challenge.ipynb) on my GitHub account.\n\n_Disclaimer: You will probably notice that some of the features I used are borrowed from other people that have posted their results in other blogs. My work is an amalgamation of what I have read on other blogs, a quick course I took on_ [_datacamp.com_](https://www.datacamp.com/community/open-courses/kaggle-python-tutorial-on-machine-learning#gs.A=c6AaQ) _, and my own machine learning knowledge through my current Master\u2019s in Data Science program._\n\nThe Titanic challenge of course is based on the infamous sinking of the behemoth passenger ship that sank in the middle of the Atlantic on April 15, 1912. A combination of icy cold waters, insufficient life boats for all passengers onboard, and other systematic failures led to 1,502 out of the 2,224 passengers dying. The Kaggle challenge provides data on 891 passengers (the training data), including wether they survived or not and the goal is to use that data to predict the fate of 418 passengers (the test data) whose fate is unknown.\n\nHere are the modules I used for this project:\n\n```\n# classifier models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV# modules to handle data\nimport pandas as pd\nimport numpy as np\n```\n\n**I. Data Wrangling and Preprocessing**\n\nKaggle provides a test and a train dataset. The training data provides a `Survived` column which shows a 1 if the passenger survived and a 0 if they did not. This is ultimately the feature we are trying to predict so the test set will not have this column.\n\nBecause I\u2019m lazy and don\u2019t like doing things twice, I first loaded the data into a `train` and `test` variable and then created a `titanic` variable where I appended the `test` to the `train` so that I can create new features to both data sets at the same time. I also created an index for each train and test so that I can separate them out later into their respective train and test.\n\n```\n# load data\ntrain = pd.read_csv('./Data/train.csv')\ntest = pd.read_csv('./Data/test.csv')# save PassengerId for final submission\npassengerId = test.PassengerId\n\n# merge train and test\ntitanic = train.append(test, ignore_index=True)# create indexes to separate data later on\ntrain_idx = len(train)\ntest_idx = len(titanic) - len(test)\n```\n\nLet\u2019s quickly peek at the data and see what it looks like:\n\n```\n# view head of data\ntitanic.head()\n```\n\n![](https://miro.medium.com/v2/resize:fit:700/1*oq6pcJbm-kOolIoCZMUhIg.png)\n\nWith the full data in the `titanic` variable, we can use the `.info()` method to get a description of the columns in the dataframe.\n\n```\n# get info on features\ntitanic.info()\n```\n\n![](https://miro.medium.com/v2/resize:fit:339/1*Qa19kLdzLVBVxwOZUPM_2Q.png)\n\nThis shows us all the features (or columns) in the data frame along with the count of _non-null_ values. Looking at the `RangeIndex` we see that there are 1309 total entries, but the `Age`, `Cabin`, `Embarked`, `Fare`, and `Survived` have less than that, suggesting that those columns have some null, or `NaN`, values. This is a dirty dataset and we either need to drop the rows with `NaN` values or fill in the gaps by leveraging the data in the dataset to estimate what those values could have been. We will choose the latter and try to estimate those values and fill in the gaps rather than lose observations. However, one thing to note is that the `Survived` feature will not require us to fill in the gaps as the count of 891 represents the labels from the `train` data. Remember that we are trying to predict the `Survived` column and so the `test` set does not have this column at all.\n\nEven though this is technically the \u201cData Wrangling\u201d section, before we do any data wrangling and address any missing values, I first want to create a `Title` feature which simply extracts the [honorific](https://en.wikipedia.org/wiki/Honorific) from the `Name` feature. Simply put, an honorific...",
      "url": "https://medium.com/i-like-big-data-and-i-cannot-lie/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9"
    },
    {
      "title": "The Kaggle Book: Data analysis and machine learning for competitive ...",
      "text": "#### Enter the characters you see below\n\nSorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies.\n\n#### Type the characters you see in this image:\n\n![](https://images-na.ssl-images-amazon.com/captcha/cdkxpfei/Captcha_cmqbfjhgkv.jpg)\n\nTry different image\n\nContinue shopping\n\n[Conditions of Use & Sale](https://www.amazon.in/gp/help/customer/display.html/ref=footer_cou?ie=UTF8&nodeId=200545940) [Privacy Notice](https://www.amazon.in/gp/help/customer/display.html/ref=footer_privacy?ie=UTF8&nodeId=200534380)\n\n\u00a9 1996-2015, Amazon.com, Inc. or its affiliates",
      "url": "https://www.amazon.in/Data-Analysis-Machine-Learning-Kaggle-ebook/dp/B09F3STL34"
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "How to Use Kaggle: Competitions",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions"
    },
    {
      "title": "Titanic - Machine Learning from Disaster | Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.",
      "url": "https://www.kaggle.com/c/titanic"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy]",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    }
  ]
}