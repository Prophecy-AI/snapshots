{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4338fcf",
   "metadata": {},
   "source": [
    "# Titanic Baseline Experiment\n",
    "\n",
    "Simple baseline using LightGBM with basic feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921c64a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:12:22.715049Z",
     "iopub.status.busy": "2026-01-15T05:12:22.714314Z",
     "iopub.status.idle": "2026-01-15T05:12:24.877359Z",
     "shell.execute_reply": "2026-01-15T05:12:24.876773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (891, 12)\n",
      "Test shape: (418, 11)\n",
      "Columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Columns: {train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbe205b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:12:24.880665Z",
     "iopub.status.busy": "2026-01-15T05:12:24.880401Z",
     "iopub.status.idle": "2026-01-15T05:12:24.914009Z",
     "shell.execute_reply": "2026-01-15T05:12:24.913400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed\n",
      "Missing values in train:\n",
      "PassengerId        0\n",
      "Pclass             0\n",
      "Name               0\n",
      "Sex                0\n",
      "Age                0\n",
      "SibSp              0\n",
      "Parch              0\n",
      "Ticket             0\n",
      "Fare               0\n",
      "Cabin            687\n",
      "Embarked           0\n",
      "Title              0\n",
      "AgeGroup           0\n",
      "Deck               0\n",
      "FamilySize         0\n",
      "IsAlone            0\n",
      "FarePerPerson      0\n",
      "Survived           0\n",
      "dtype: int64\n",
      "Missing values in test:\n",
      "PassengerId        0\n",
      "Pclass             0\n",
      "Name               0\n",
      "Sex                0\n",
      "Age                0\n",
      "SibSp              0\n",
      "Parch              0\n",
      "Ticket             0\n",
      "Fare               0\n",
      "Cabin            327\n",
      "Embarked           0\n",
      "Title              0\n",
      "AgeGroup           0\n",
      "Deck               0\n",
      "FamilySize         0\n",
      "IsAlone            0\n",
      "FarePerPerson      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic feature engineering\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract title from Name\n",
    "    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Simplify titles\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "        'Dr': 'Other', 'Rev': 'Other', 'Col': 'Other', 'Major': 'Other',\n",
    "        'Mlle': 'Miss', 'Countess': 'Other', 'Ms': 'Miss', 'Lady': 'Other',\n",
    "        'Jonkheer': 'Other', 'Don': 'Other', 'Dona': 'Other', 'Mme': 'Mrs',\n",
    "        'Capt': 'Other', 'Sir': 'Other'\n",
    "    }\n",
    "    df['Title'] = df['Title'].map(title_mapping)\n",
    "    \n",
    "    # Fill missing Age based on Title and Pclass\n",
    "    df['Age'] = df.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    \n",
    "    # Create Age groups\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 30, 50, 80], labels=['Child', 'Teen', 'Young', 'Middle', 'Senior'])\n",
    "    \n",
    "    # Fill missing Embarked with mode\n",
    "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    \n",
    "    # Fill missing Fare with median\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    \n",
    "    # Extract deck from Cabin\n",
    "    df['Deck'] = df['Cabin'].str[0]\n",
    "    df['Deck'] = df['Deck'].fillna('Unknown')\n",
    "    \n",
    "    # Family size\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # Fare per person\n",
    "    df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess both datasets\n",
    "combined = pd.concat([train.drop('Survived', axis=1), test], axis=0, ignore_index=True)\n",
    "combined = preprocess_data(combined)\n",
    "\n",
    "# Split back\n",
    "train_processed = combined.iloc[:len(train)].copy()\n",
    "test_processed = combined.iloc[len(train):].copy()\n",
    "train_processed['Survived'] = train['Survived'].values\n",
    "\n",
    "print(\"Preprocessing completed\")\n",
    "print(f\"Missing values in train:\\n{train_processed.isnull().sum()}\")\n",
    "print(f\"Missing values in test:\\n{test_processed.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf1148c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:12:24.915975Z",
     "iopub.status.busy": "2026-01-15T05:12:24.915744Z",
     "iopub.status.idle": "2026-01-15T05:12:24.935799Z",
     "shell.execute_reply": "2026-01-15T05:12:24.935235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (891, 28)\n",
      "Test feature matrix shape: (418, 28)\n",
      "Number of features: 28\n"
     ]
    }
   ],
   "source": [
    "# Select features for modeling\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'AgeGroup', 'Deck']\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'FarePerPerson', 'IsAlone']\n",
    "\n",
    "feature_columns = categorical_features + numerical_features\n",
    "\n",
    "# One-hot encode categorical features\n",
    "train_features = pd.get_dummies(train_processed[feature_columns], columns=categorical_features, drop_first=True)\n",
    "test_features = pd.get_dummies(test_processed[feature_columns], columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Align features (ensure same columns in both datasets)\n",
    "train_features, test_features = train_features.align(test_features, join='left', axis=1, fill_value=0)\n",
    "\n",
    "X = train_features.values\n",
    "y = train_processed['Survived'].values\n",
    "X_test = test_features.values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b9ed95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:12:24.938499Z",
     "iopub.status.busy": "2026-01-15T05:12:24.938289Z",
     "iopub.status.idle": "2026-01-15T05:12:25.546021Z",
     "shell.execute_reply": "2026-01-15T05:12:25.545387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold cross-validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tval's binary_error: 0.145251\n",
      "Fold 1: Accuracy = 0.8547\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\tval's binary_error: 0.123596\n",
      "Fold 2: Accuracy = 0.8764\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tval's binary_error: 0.179775\n",
      "Fold 3: Accuracy = 0.8202\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tval's binary_error: 0.146067\n",
      "Fold 4: Accuracy = 0.8539\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tval's binary_error: 0.162921\n",
      "Fold 5: Accuracy = 0.8371\n",
      "\n",
      "Overall CV Accuracy: 0.8485\n",
      "Mean CV Accuracy: 0.8485 ± 0.0189\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros(len(train))\n",
    "test_predictions = np.zeros(len(test))\n",
    "\n",
    "# Model parameters (conservative baseline)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Starting 5-fold cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        valid_names=['val'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_pred_binary = (val_pred > 0.5).astype(int)\n",
    "    \n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store results\n",
    "    oof_predictions[val_idx] = val_pred_binary\n",
    "    test_predictions += test_pred / 5\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, val_pred_binary)\n",
    "    fold_scores.append(accuracy)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Overall CV score\n",
    "overall_accuracy = accuracy_score(y, oof_predictions)\n",
    "print(f\"\\nOverall CV Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ed7578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T05:12:25.548547Z",
     "iopub.status.busy": "2026-01-15T05:12:25.547700Z",
     "iopub.status.idle": "2026-01-15T05:12:25.560247Z",
     "shell.execute_reply": "2026-01-15T05:12:25.559706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n",
      "\n",
      "Submission shape: (418, 2)\n",
      "Survival rate in submission: 0.347\n",
      "\n",
      "OOF predictions saved. Shape: (891, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': (test_predictions > 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Survival rate in submission: {submission['Survived'].mean():.3f}\")\n",
    "\n",
    "# Also save OOF predictions for analysis\n",
    "oof_df = pd.DataFrame({\n",
    "    'PassengerId': train['PassengerId'],\n",
    "    'Survived': y,\n",
    "    'Survived_Pred': oof_predictions\n",
    "})\n",
    "oof_df.to_csv('/home/code/experiments/001_baseline/oof_predictions.csv', index=False)\n",
    "\n",
    "print(f\"\\nOOF predictions saved. Shape: {oof_df.shape}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
