{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35de5ab",
   "metadata": {},
   "source": [
    "# Titanic Baseline Model\n",
    "\n",
    "This notebook creates a baseline model for the Titanic competition using gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e624d1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:09:46.416387Z",
     "iopub.status.busy": "2026-01-14T09:09:46.414671Z",
     "iopub.status.idle": "2026-01-14T09:09:49.773345Z",
     "shell.execute_reply": "2026-01-14T09:09:49.772504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (891, 12)\n",
      "Test data shape: (418, 11)\n",
      "\n",
      "Training data info:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"\\nTraining data info:\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98094ca",
   "metadata": {},
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df8c05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:10:36.248804Z",
     "iopub.status.busy": "2026-01-14T09:10:36.247854Z",
     "iopub.status.idle": "2026-01-14T09:10:36.304093Z",
     "shell.execute_reply": "2026-01-14T09:10:36.303065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before preprocessing:\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age          263\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           1\n",
      "Cabin       1014\n",
      "Embarked       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate target and features\n",
    "X = train_df.drop(['Survived', 'PassengerId'], axis=1)\n",
    "y = train_df['Survived']\n",
    "X_test = test_df.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# Combine for preprocessing\n",
    "combined = pd.concat([X, X_test], axis=0)\n",
    "\n",
    "print(\"Missing values before preprocessing:\")\n",
    "print(combined.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9834fef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:10:36.307160Z",
     "iopub.status.busy": "2026-01-14T09:10:36.306815Z",
     "iopub.status.idle": "2026-01-14T09:10:36.375377Z",
     "shell.execute_reply": "2026-01-14T09:10:36.374155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after preprocessing:\n",
      "Pclass      0\n",
      "Name        0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Ticket      0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "HasCabin    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "# Age: fill with median\n",
    "combined['Age'].fillna(combined['Age'].median(), inplace=True)\n",
    "\n",
    "# Embarked: fill with mode\n",
    "combined['Embarked'].fillna(combined['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Fare: fill with median\n",
    "combined['Fare'].fillna(combined['Fare'].median(), inplace=True)\n",
    "\n",
    "# Cabin: create binary feature (has cabin or not)\n",
    "combined['HasCabin'] = (combined['Cabin'].notna()).astype(int)\n",
    "combined.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "print(\"Missing values after preprocessing:\")\n",
    "print(combined.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d4153b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:10:36.381530Z",
     "iopub.status.busy": "2026-01-14T09:10:36.380110Z",
     "iopub.status.idle": "2026-01-14T09:10:36.402811Z",
     "shell.execute_reply": "2026-01-14T09:10:36.401378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles: ['Mr' 'Mrs' 'Miss' 'Master' 'Other']\n",
      "Title counts: Title\n",
      "Mr        757\n",
      "Miss      264\n",
      "Mrs       198\n",
      "Master     61\n",
      "Other      29\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract titles from names\n",
    "combined['Title'] = combined['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Map rare titles to more common ones\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "    'Dr': 'Other', 'Rev': 'Other', 'Col': 'Other', 'Major': 'Other',\n",
    "    'Mlle': 'Miss', 'Countess': 'Other', 'Ms': 'Miss', 'Lady': 'Other',\n",
    "    'Jonkheer': 'Other', 'Don': 'Other', 'Dona': 'Other', 'Mme': 'Mrs',\n",
    "    'Capt': 'Other', 'Sir': 'Other'\n",
    "}\n",
    "combined['Title'] = combined['Title'].map(title_mapping)\n",
    "\n",
    "# Drop Name and Ticket (high cardinality)\n",
    "combined.drop(['Name', 'Ticket'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Unique titles:\", combined['Title'].unique())\n",
    "print(\"Title counts:\", combined['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df7c1cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:11:27.222337Z",
     "iopub.status.busy": "2026-01-14T09:11:27.221269Z",
     "iopub.status.idle": "2026-01-14T09:11:27.234251Z",
     "shell.execute_reply": "2026-01-14T09:11:27.233450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes:\n",
      "(1309, 9)\n",
      "\n",
      "Feature types:\n",
      "int64      7\n",
      "float64    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "categorical_cols = ['Sex', 'Embarked', 'Title', 'Pclass']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined[col] = le.fit_transform(combined[col])\n",
    "\n",
    "print(\"Final feature shapes:\")\n",
    "print(combined.shape)\n",
    "print(\"\\nFeature types:\")\n",
    "print(combined.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547f7d7",
   "metadata": {},
   "source": [
    "## Create Family Size Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f19a834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:11:27.237046Z",
     "iopub.status.busy": "2026-01-14T09:11:27.236761Z",
     "iopub.status.idle": "2026-01-14T09:11:27.417370Z",
     "shell.execute_reply": "2026-01-14T09:11:27.416465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family size distribution:\n",
      "FamilySizeCategory\n",
      "1    790\n",
      "2    437\n",
      "0     82\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create family size feature\n",
    "combined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1\n",
    "\n",
    "# Create family size categories\n",
    "combined['FamilySizeCategory'] = pd.cut(combined['FamilySize'], \n",
    "                                       bins=[0, 1, 4, 20], \n",
    "                                       labels=['Single', 'Small', 'Large'])\n",
    "\n",
    "# Encode family size category\n",
    "le_fs = LabelEncoder()\n",
    "combined['FamilySizeCategory'] = le_fs.fit_transform(combined['FamilySizeCategory'])\n",
    "\n",
    "print(\"Family size distribution:\")\n",
    "print(combined['FamilySizeCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5c37f",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36c9247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:12:20.319573Z",
     "iopub.status.busy": "2026-01-14T09:12:20.318574Z",
     "iopub.status.idle": "2026-01-14T09:12:20.329877Z",
     "shell.execute_reply": "2026-01-14T09:12:20.329026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed training data shape: (891, 11)\n",
      "Processed test data shape: (418, 11)\n",
      "\n",
      "Missing values in training: 0\n",
      "Missing values in test: 0\n"
     ]
    }
   ],
   "source": [
    "# Split back into train and test\n",
    "X_processed = combined.iloc[:len(X), :]\n",
    "X_test_processed = combined.iloc[len(X):, :]\n",
    "\n",
    "print(f\"Processed training data shape: {X_processed.shape}\")\n",
    "print(f\"Processed test data shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"\\nMissing values in training: {X_processed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {X_test_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6643a4d",
   "metadata": {},
   "source": [
    "## Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0a74df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:13:09.195734Z",
     "iopub.status.busy": "2026-01-14T09:13:09.194478Z",
     "iopub.status.idle": "2026-01-14T09:13:10.241499Z",
     "shell.execute_reply": "2026-01-14T09:13:10.240772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 5-fold cross-validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Accuracy = 0.8715\n",
      "Fold 2: Accuracy = 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: Accuracy = 0.8146\n",
      "Fold 4: Accuracy = 0.8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Accuracy = 0.8483\n",
      "\n",
      "Cross-validation scores: [0.8715083798882681, 0.8539325842696629, 0.8146067415730337, 0.8202247191011236, 0.848314606741573]\n",
      "Mean accuracy: 0.8417 ± 0.0213\n"
     ]
    }
   ],
   "source": [
    "# Set up cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = []\n",
    "fold = 1\n",
    "\n",
    "print(\"Training with 5-fold cross-validation...\")\n",
    "for train_idx, val_idx in skf.split(X_processed, y):\n",
    "    X_train, X_val = X_processed.iloc[train_idx], X_processed.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_val)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold}: Accuracy = {score:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81356e3",
   "metadata": {},
   "source": [
    "## Train on Full Data and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full training data\n",
    "model.fit(X_processed, y)\n",
    "\n",
    "# Generate predictions for test set\n",
    "y_pred_test = model.predict(X_test_processed)\n",
    "\n",
    "print(f\"Test predictions shape: {y_pred_test.shape}\")\n",
    "print(f\"Prediction distribution: {np.bincount(y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8082c1",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred_test\n",
    "})\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify submission format\n",
    "print(\"\\nVerifying submission format...\")\n",
    "print(f\"Columns: {list(submission.columns)}\")\n",
    "print(f\"Number of rows: {len(submission)}\")\n",
    "print(f\"Expected rows: 418\")\n",
    "print(f\"PassengerId range: {submission['PassengerId'].min()} to {submission['PassengerId'].max()}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
