{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72857882",
   "metadata": {},
   "source": [
    "# Experiment 003: Hyperparameter Tuning for Regularization\n",
    "\n",
    "**Goal**: Reduce overfitting and close the 9.2% CV-LB gap through systematic hyperparameter tuning.\n",
    "\n",
    "**Approach**:\n",
    "- Reduce XGBoost complexity (max_depth=3-4, learning_rate=0.05)\n",
    "- Add regularization (min_child_weight=3, gamma=0.1, subsample=0.8, colsample_bytree=0.8)\n",
    "- Reduce n_estimators to 200-300\n",
    "- Use early_stopping_rounds=50\n",
    "- Use RandomizedSearchCV for systematic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1427822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f67098",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"\\nTraining data info:\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fafce",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(name):\n",
    "    \"\"\"Extract title from name\"\"\"\n",
    "    title = name.split(',')[1].split('.')[0].strip()\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "        'Dr': 'Dr', 'Rev': 'Clergy', 'Col': 'Military', 'Major': 'Military',\n",
    "        'Capt': 'Military', 'Sir': 'Noble', 'Lady': 'Noble', 'Don': 'Noble',\n",
    "        'Dona': 'Noble', 'Countess': 'Noble', 'Jonkheer': 'Noble',\n",
    "        'Mme': 'Mrs', 'Ms': 'Miss', 'Mlle': 'Miss'\n",
    "    }\n",
    "    return title_mapping.get(title, 'Other')\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Engineer features for the dataset\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract title\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    \n",
    "    # Family features\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # Fare per person\n",
    "    df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "    \n",
    "    # Age bins - simplified to reduce overfitting\n",
    "    df['AgeBin'] = pd.cut(df['Age'], bins=[0, 16, 32, 100], \n",
    "                         labels=['Child', 'Adult', 'Senior'])\n",
    "    \n",
    "    # Cabin indicator\n",
    "    df['HasCabin'] = df['Cabin'].notna().astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_df = engineer_features(train_df)\n",
    "test_df = engineer_features(test_df)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"New features: Title, FamilySize, IsAlone, FarePerPerson, AgeBin, HasCabin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bd440",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cea249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'AgeBin', 'HasCabin', 'IsAlone']\n",
    "\n",
    "# Prepare training data\n",
    "X = train_df[numeric_features + categorical_features]\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_df[numeric_features + categorical_features]\n",
    "\n",
    "print(f\"Training features shape: {X.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"\\nNumeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af691bc9",
   "metadata": {},
   "source": [
    "## Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03095c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocessing pipeline created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7a373",
   "metadata": {},
   "source": [
    "## Define XGBoost Model with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost model with regularization parameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,  # Reduced from 500\n",
    "    max_depth=4,       # Keep moderate depth\n",
    "    learning_rate=0.05,  # Reduced from 0.1 for better generalization\n",
    "    min_child_weight=3,  # NEW: Regularization - minimum sum of instance weight\n",
    "    gamma=0.1,           # NEW: Regularization - minimum loss reduction\n",
    "    subsample=0.8,       # NEW: Regularization - subsample ratio\n",
    "    colsample_bytree=0.8, # NEW: Regularization - feature subsample ratio\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"XGBoost model defined with regularization parameters!\")\n",
    "print(\"\\nRegularization settings:\")\n",
    "print(f\"- n_estimators: 300 (reduced from 500)\")\n",
    "print(f\"- learning_rate: 0.05 (reduced from 0.1)\")\n",
    "print(f\"- min_child_weight: 3 (prevents overfitting to leaf nodes)\")\n",
    "print(f\"- gamma: 0.1 (minimum loss reduction)\")\n",
    "print(f\"- subsample: 0.8 (row sampling)\")\n",
    "print(f\"- colsample_bytree: 0.8 (feature sampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9062ae1",
   "metadata": {},
   "source": [
    "## Create Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_model)\n",
    "])\n",
    "\n",
    "print(\"Full pipeline created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05229f43",
   "metadata": {},
   "source": [
    "## Define Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2266f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [200, 300, 400, 500],\n",
    "    'classifier__max_depth': [3, 4, 5, 6],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__min_child_weight': [1, 3, 5],\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter search space defined!\")\n",
    "print(f\"Number of parameter combinations to try: 30 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62894000",
   "metadata": {},
   "source": [
    "## Run RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea045ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Run randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "print(f\"Searching over {len(param_dist)} parameters with 30 iterations\")\n",
    "print(f\"Using 5-fold stratified CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e36423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOMIZED SEARCH COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest CV Score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Best Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed756a",
   "metadata": {},
   "source": [
    "## Evaluate Best Model with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Run cross-validation with the best model\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS (Best Model)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "print(f\"Individual folds: {[f'{score:.2%}' for score in cv_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb418220",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model on full data to get feature importances\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "preprocessor_fit = best_model.named_steps['preprocessor']\n",
    "classifier = best_model.named_steps['classifier']\n",
    "\n",
    "# Get one-hot encoded feature names\n",
    "categorical_features_names = []\n",
    "for i, cat in enumerate(categorical_features):\n",
    "    categories = preprocessor_fit.named_transformers_['cat'].named_steps['onehot'].categories_[i]\n",
    "    categorical_features_names.extend([f\"{cat}_{category}\" for category in categories])\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = numeric_features + categorical_features_names\n",
    "\n",
    "# Get feature importances\n",
    "importances = classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(feature_importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a9fcb",
   "metadata": {},
   "source": [
    "## Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nTest predictions shape: {y_pred_test.shape}\")\n",
    "print(f\"Prediction distribution: {np.bincount(y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d044a",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d874063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred_test\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION FILE CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Columns: {list(submission.columns)}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(submission.head())\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"\\nSubmission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6145200",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This experiment performed systematic hyperparameter tuning to reduce overfitting:\n",
    "\n",
    "**Key Changes:**\n",
    "- Used RandomizedSearchCV with 30 iterations\n",
    "- Added regularization parameters (min_child_weight, gamma, subsample, colsample_bytree)\n",
    "- Reduced learning_rate from 0.1 to tuned value\n",
    "- Optimized n_estimators and max_depth\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Reduced overfitting → smaller CV-LB gap\n",
    "- Better generalization to test set\n",
    "- Maintained or improved CV score while improving LB performance"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
