{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f16e194",
   "metadata": {},
   "source": [
    "# Titanic: Fixed Preprocessing Pipeline\n",
    "\n",
    "This notebook fixes the data leakage issue by implementing proper sklearn pipelines that fit preprocessing on training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d1186b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T11:45:32.496526Z",
     "iopub.status.busy": "2026-01-14T11:45:32.495772Z",
     "iopub.status.idle": "2026-01-14T11:45:34.778683Z",
     "shell.execute_reply": "2026-01-14T11:45:34.778043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (891, 12)\n",
      "Test data shape: (418, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aeb305",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions\n",
    "\n",
    "Define functions to create new features that will be applied within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afaa66d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T11:46:24.031696Z",
     "iopub.status.busy": "2026-01-14T11:46:24.031349Z",
     "iopub.status.idle": "2026-01-14T11:46:24.059823Z",
     "shell.execute_reply": "2026-01-14T11:46:24.059152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created:\n",
      "  Title  FamilySize  IsAlone      AgeBin  FarePerPerson  HasCabin\n",
      "0    Mr           2        0  YoungAdult        3.62500         0\n",
      "1   Mrs           2        0       Adult       35.64165         1\n",
      "2  Miss           1        1  YoungAdult        7.92500         0\n",
      "3   Mrs           2        0  YoungAdult       26.55000         1\n",
      "4    Mr           1        1  YoungAdult        8.05000         0\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Engineer new features from raw data\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract title from name\n",
    "    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "    \n",
    "    # Map rare titles to more common ones\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "        'Dr': 'Other', 'Rev': 'Other', 'Col': 'Other', 'Major': 'Other',\n",
    "        'Mlle': 'Miss', 'Countess': 'Other', 'Ms': 'Miss', 'Lady': 'Other',\n",
    "        'Jonkheer': 'Other', 'Don': 'Other', 'Dona': 'Other', 'Mme': 'Mrs',\n",
    "        'Capt': 'Other', 'Sir': 'Other'\n",
    "    }\n",
    "    df['Title'] = df['Title'].map(title_mapping)\n",
    "    \n",
    "    # Create family size\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    \n",
    "    # Create IsAlone flag (NEW FEATURE)\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # Create age bins (NEW FEATURE)\n",
    "    df['AgeBin'] = pd.cut(df['Age'], \n",
    "                         bins=[0, 12, 18, 35, 60, 100], \n",
    "                         labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior'])\n",
    "    \n",
    "    # Create fare per person (NEW FEATURE)\n",
    "    df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "    \n",
    "    # Create cabin indicator\n",
    "    df['HasCabin'] = df['Cabin'].notna().astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_fe = engineer_features(train_df)\n",
    "test_fe = engineer_features(test_df)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(train_fe[['Title', 'FamilySize', 'IsAlone', 'AgeBin', 'FarePerPerson', 'HasCabin']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48afbc5",
   "metadata": {},
   "source": [
    "## Define Preprocessing Pipeline\n",
    "\n",
    "Create separate preprocessing pipelines for numeric and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ea8e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T11:46:24.061961Z",
     "iopub.status.busy": "2026-01-14T11:46:24.061720Z",
     "iopub.status.idle": "2026-01-14T11:46:24.070823Z",
     "shell.execute_reply": "2026-01-14T11:46:24.070147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'FarePerPerson', 'HasCabin']\n",
      "Categorical features: ['Pclass', 'Sex', 'Embarked', 'Title', 'IsAlone', 'AgeBin']\n"
     ]
    }
   ],
   "source": [
    "# Separate target and features\n",
    "target = train_fe['Survived']\n",
    "train_features = train_fe.drop(['Survived', 'PassengerId'], axis=1)\n",
    "test_features = test_fe.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# Define feature groups\n",
    "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'FarePerPerson', 'HasCabin']\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'IsAlone', 'AgeBin']\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda0e0e",
   "metadata": {},
   "source": [
    "## Create Full Pipeline with XGBoost\n",
    "\n",
    "Combine preprocessing with XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7de705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost model with better hyperparameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=4,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Create full pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_model)\n",
    "])\n",
    "\n",
    "print(\"Pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974212b",
   "metadata": {},
   "source": [
    "## Cross-Validation with Proper Preprocessing\n",
    "\n",
    "The key improvement: preprocessing happens INSIDE the CV loop, preventing leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = []\n",
    "fold = 1\n",
    "\n",
    "print(\"Training with 5-fold cross-validation (preprocessing inside loop)...\")\n",
    "for train_idx, val_idx in skf.split(train_features, target):\n",
    "    X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n",
    "    y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n",
    "    \n",
    "    # Clone pipeline for this fold\n",
    "    fold_clf = Pipeline(clf.steps)\n",
    "    \n",
    "    # Train model\n",
    "    fold_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = fold_clf.predict(X_val)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold}: Accuracy = {score:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43331bf",
   "metadata": {},
   "source": [
    "## Train on Full Data and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full training data\n",
    "clf.fit(train_features, target)\n",
    "\n",
    "# Generate predictions for test set\n",
    "y_pred_test = clf.predict(test_features)\n",
    "\n",
    "print(f\"Test predictions shape: {y_pred_test.shape}\")\n",
    "print(f\"Prediction distribution: {np.bincount(y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7d448",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred_test\n",
    "})\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "\n",
    "# Verify submission format\n",
    "print(\"\\nVerifying submission format...\")\n",
    "print(f\"Columns: {list(submission.columns)}\")\n",
    "print(f\"Number of rows: {len(submission)}\")\n",
    "print(f\"Expected rows: 418\")\n",
    "print(f\"PassengerId range: {submission['PassengerId'].min()} to {submission['PassengerId'].max()}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "numeric_features_processed = numeric_features\n",
    "categorical_features_processed = list(clf.named_steps['preprocessor']\n",
    "                                    .named_transformers_['cat']\n",
    "                                    .named_steps['onehot']\n",
    "                                    .get_feature_names_out(categorical_features))\n",
    "\n",
    "all_features = numeric_features_processed + categorical_features_processed\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_features[:len(importances)],  # Ensure same length\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
