{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a758b27",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "Analyzing the baseline experiment results and data patterns to inform next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train_df['Survived'].value_counts().sort_index())\n",
    "print(f\"Survival rate: {train_df['Survived'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40a0d6",
   "metadata": {},
   "source": [
    "## Analyze Data Leakage Issue\n",
    "\n",
    "The evaluator flagged that we combined train and test for preprocessing. Let me quantify the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1008f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and features\n",
    "X = train_df.drop(['Survived', 'PassengerId'], axis=1)\n",
    "y = train_df['Survived']\n",
    "X_test = test_df.drop(['PassengerId'], axis=1)\n",
    "\n",
    "print(\"=== TRAIN SET STATISTICS ===\")\n",
    "print(f\"Age - median: {X['Age'].median():.2f}, mean: {X['Age'].mean():.2f}\")\n",
    "print(f\"Fare - median: {X['Fare'].median():.2f}, mean: {X['Fare'].mean():.2f}\")\n",
    "print(f\"Embarked - mode: {X['Embarked'].mode()[0]}\")\n",
    "print(f\"Embarked distribution:\\n{X['Embarked'].value_counts()}\")\n",
    "\n",
    "print(\"\\n=== TEST SET STATISTICS ===\")\n",
    "print(f\"Age - median: {X_test['Age'].median():.2f}, mean: {X_test['Age'].mean():.2f}\")\n",
    "print(f\"Fare - median: {X_test['Fare'].median():.2f}, mean: {X_test['Fare'].mean():.2f}\")\n",
    "print(f\"Embarked - mode: {X_test['Embarked'].mode()[0]}\")\n",
    "print(f\"Embarked distribution:\\n{X_test['Embarked'].value_counts()}\")\n",
    "\n",
    "print(\"\\n=== COMBINED STATISTICS (CURRENT LEAKY APPROACH) ===\")\n",
    "combined = pd.concat([X, X_test], axis=0)\n",
    "print(f\"Age - median: {combined['Age'].median():.2f}, mean: {combined['Age'].mean():.2f}\")\n",
    "print(f\"Fare - median: {combined['Fare'].median():.2f}, mean: {combined['Fare'].mean():.2f}\")\n",
    "print(f\"Embarked - mode: {combined['Embarked'].mode()[0]}\")\n",
    "\n",
    "# Calculate differences\n",
    "train_age_median = X['Age'].median()\n",
    "test_age_median = X_test['Age'].median()\n",
    "combined_age_median = combined['Age'].median()\n",
    "\n",
    "print(f\"\\n=== DIFFERENCES ===\")\n",
    "print(f\"Age median - Train: {train_age_median:.2f}, Test: {test_age_median:.2f}, Combined: {combined_age_median:.2f}\")\n",
    "print(f\"Age: Train vs Combined difference: {abs(train_age_median - combined_age_median):.2f}\")\n",
    "print(f\"Age: Test vs Combined difference: {abs(test_age_median - combined_age_median):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035325a3",
   "metadata": {},
   "source": [
    "## Analyze Feature Patterns\n",
    "\n",
    "Let's examine what features are most predictive to guide our feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper non-leaky preprocessing pipeline for analysis\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Make a copy for proper preprocessing\n",
    "X_clean = X.copy()\n",
    "X_test_clean = X_test.copy()\n",
    "\n",
    "# Impute using TRAIN data only\n",
    "age_imputer = SimpleImputer(strategy='median')\n",
    "age_imputer.fit(X_clean[['Age']])\n",
    "X_clean['Age'] = age_imputer.transform(X_clean[['Age']])\n",
    "X_test_clean['Age'] = age_imputer.transform(X_test_clean[['Age']])\n",
    "\n",
    "fare_imputer = SimpleImputer(strategy='median')\n",
    "fare_imputer.fit(X_clean[['Fare']])\n",
    "X_clean['Fare'] = fare_imputer.transform(X_clean[['Fare']])\n",
    "X_test_clean['Fare'] = fare_imputer.transform(X_test_clean[['Fare']])\n",
    "\n",
    "embarked_imputer = SimpleImputer(strategy='most_frequent')\n",
    "embarked_imputer.fit(X_clean[['Embarked']])\n",
    "X_clean['Embarked'] = embarked_imputer.transform(X_clean[['Embarked']])\n",
    "X_test_clean['Embarked'] = embarked_imputer.transform(X_test_clean[['Embarked']])\n",
    "\n",
    "# Extract titles\n",
    "X_clean['Title'] = X_clean['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "X_test_clean['Title'] = X_test_clean['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "\n",
    "# Map titles\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "    'Dr': 'Other', 'Rev': 'Other', 'Col': 'Other', 'Major': 'Other',\n",
    "    'Mlle': 'Miss', 'Countess': 'Other', 'Ms': 'Miss', 'Lady': 'Other',\n",
    "    'Jonkheer': 'Other', 'Don': 'Other', 'Dona': 'Other', 'Mme': 'Mrs',\n",
    "    'Capt': 'Other', 'Sir': 'Other'\n",
    "}\n",
    "X_clean['Title'] = X_clean['Title'].map(title_mapping)\n",
    "X_test_clean['Title'] = X_test_clean['Title'].map(title_mapping)\n",
    "\n",
    "# Fill any missing titles\n",
    "X_clean['Title'].fillna('Other', inplace=True)\n",
    "X_test_clean['Title'].fillna('Other', inplace=True)\n",
    "\n",
    "# Create family size\n",
    "X_clean['FamilySize'] = X_clean['SibSp'] + X_clean['Parch'] + 1\n",
    "X_test_clean['FamilySize'] = X_test_clean['SibSp'] + X_test_clean['Parch'] + 1\n",
    "\n",
    "# Create cabin indicator\n",
    "X_clean['HasCabin'] = (X_clean['Cabin'].notna()).astype(int)\n",
    "X_test_clean['HasCabin'] = (X_test_clean['Cabin'].notna()).astype(int)\n",
    "\n",
    "# Drop high cardinality features\n",
    "X_clean.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "X_test_clean.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Clean data shapes:\", X_clean.shape, X_test_clean.shape)\n",
    "\n",
    "# Analyze survival rates by feature\n",
    "print(\"\\n=== SURVIVAL RATES BY FEATURE ===\")\n",
    "\n",
    "# Sex\n",
    "print(\"\\nSex:\")\n",
    "sex_survival = train_df.groupby('Sex')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "sex_survival.columns = ['Total', 'Survived', 'SurvivalRate']\n",
    "print(sex_survival)\n",
    "\n",
    "# Pclass\n",
    "print(\"\\nPclass:\")\n",
    "pclass_survival = train_df.groupby('Pclass')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "pclass_survival.columns = ['Total', 'Survived', 'SurvivalRate']\n",
    "print(pclass_survival)\n",
    "\n",
    "# Title\n",
    "print(\"\\nTitle:\")\n",
    "title_survival = pd.DataFrame({'Title': X_clean['Title'], 'Survived': y})\n",
    "title_stats = title_survival.groupby('Title')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "title_stats.columns = ['Total', 'Survived', 'SurvivalRate']\n",
    "print(title_stats)\n",
    "\n",
    "# HasCabin\n",
    "print(\"\\nHasCabin:\")\n",
    "cabin_survival = pd.DataFrame({'HasCabin': X_clean['HasCabin'], 'Survived': y})\n",
    "cabin_stats = cabin_survival.groupby('HasCabin')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "cabin_stats.columns = ['Total', 'Survived', 'SurvivalRate']\n",
    "print(cabin_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2610ad2",
   "metadata": {},
   "source": [
    "## Analyze Cabin Information\n",
    "\n",
    "The evaluator mentioned we might be losing information by only using a binary indicator. Let me analyze the cabin letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cabin letters\n",
    "print(\"=== CABIN ANALYSIS ===\")\n",
    "\n",
    "# Extract deck letters from train and test\n",
    "train_cabins = train_df['Cabin'].dropna()\n",
    "test_cabins = test_df['Cabin'].dropna()\n",
    "\n",
    "print(f\"Train cabins (non-null): {len(train_cabins)} out of {len(train_df)} ({len(train_cabins)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Test cabins (non-null): {len(test_cabins)} out of {len(test_df)} ({len(test_cabins)/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "# Extract deck letters\n",
    "train_decks = train_cabins.str[0]\n",
    "test_decks = test_cabins.str[0]\n",
    "\n",
    "print(\"\\nTrain deck distribution:\")\n",
    "print(train_decks.value_counts())\n",
    "\n",
    "print(\"\\nTest deck distribution:\")\n",
    "print(test_decks.value_counts())\n",
    "\n",
    "# Analyze survival by deck (for training data)\n",
    "print(\"\\n=== SURVIVAL RATES BY DECK ===\")\n",
    "train_with_cabin = train_df[train_df['Cabin'].notna()].copy()\n",
    "train_with_cabin['Deck'] = train_with_cabin['Cabin'].str[0]\n",
    "\n",
    "deck_survival = train_with_cabin.groupby('Deck')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "deck_survival.columns = ['Total', 'Survived', 'SurvivalRate']\n",
    "print(deck_survival)\n",
    "\n",
    "# Statistical significance\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency table for deck vs survival\n",
    "deck_crosstab = pd.crosstab(train_with_cabin['Deck'], train_with_cabin['Survived'])\n",
    "print(f\"\\nContingency table:\\n{deck_crosstab}\")\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(deck_crosstab)\n",
    "print(f\"\\nChi-square test: chi2={chi2:.3f}, p-value={p_value:.3f}\")\n",
    "print(f\"Deck has significant association with survival: {p_value < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d466a",
   "metadata": {},
   "source": [
    "## Model Feature Importance\n",
    "\n",
    "Train a model to see which features are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables properly (non-leaky)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_model = X_clean.copy()\n",
    "\n",
    "categorical_cols = ['Sex', 'Embarked', 'Title', 'Pclass']\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_model[col] = le.fit_transform(X_model[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "# Train a model to get feature importance\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_model, y)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_model.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE ===\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature')\n",
    "plt.title('Feature Importance from Gradient Boosting')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation score with clean data\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_clean = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_model, y):\n",
    "    X_train, X_val = X_model.iloc[train_idx], X_model.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model_fold = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    model_fold.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model_fold.predict(X_val)\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    cv_scores_clean.append(score)\n",
    "\n",
    "print(f\"\\n=== CLEAN CV SCORE (no leakage) ===\")\n",
    "print(f\"CV scores: {cv_scores_clean}\")\n",
    "print(f\"Mean: {np.mean(cv_scores_clean):.4f} ± {np.std(cv_scores_clean):.4f}\")\n",
    "print(f\"Original leaky CV: 0.8417 ± 0.0213\")\n",
    "print(f\"Difference: {np.mean(cv_scores_clean) - 0.8417:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
