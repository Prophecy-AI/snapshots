{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e708b8",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "Analysis to identify improvement opportunities for Titanic competition.\n",
    "Focus on: feature patterns, error analysis, and proven techniques from Kaggle meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d903fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:49:48.894763Z",
     "iopub.status.busy": "2026-01-14T02:49:48.894057Z",
     "iopub.status.idle": "2026-01-14T02:49:48.908058Z",
     "shell.execute_reply": "2026-01-14T02:49:48.907475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Train: (891, 12)\n",
      "Test: (418, 11)\n",
      "\n",
      "Target distribution: {0: 549, 1: 342}\n",
      "Survival rate: 0.384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution: {train_df['Survived'].value_counts().to_dict()}\")\n",
    "print(f\"Survival rate: {train_df['Survived'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2cb34f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:49:48.909867Z",
     "iopub.status.busy": "2026-01-14T02:49:48.909694Z",
     "iopub.status.idle": "2026-01-14T02:49:48.916889Z",
     "shell.execute_reply": "2026-01-14T02:49:48.916388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test data:\n",
      "Age       86\n",
      "Fare       1\n",
      "Cabin    327\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing values\n",
    "print(\"Missing values in training data:\")\n",
    "missing_train = train_df.isnull().sum()\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(missing_test[missing_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea30db21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:49:48.918692Z",
     "iopub.status.busy": "2026-01-14T02:49:48.918497Z",
     "iopub.status.idle": "2026-01-14T02:49:48.942678Z",
     "shell.execute_reply": "2026-01-14T02:49:48.942122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rates by key features:\n",
      "\n",
      "By Pclass:\n",
      "        count  sum      mean\n",
      "Pclass                      \n",
      "1         216  136  0.629630\n",
      "2         184   87  0.472826\n",
      "3         491  119  0.242363\n",
      "\n",
      "By Sex:\n",
      "        count  sum      mean\n",
      "Sex                         \n",
      "female    314  233  0.742038\n",
      "male      577  109  0.188908\n",
      "\n",
      "By Title:\n",
      "        count  sum      mean\n",
      "Title                       \n",
      "Master     40   23  0.575000\n",
      "Miss      185  130  0.702703\n",
      "Mr        517   81  0.156673\n",
      "Mrs       126  100  0.793651\n",
      "Other      23    8  0.347826\n",
      "\n",
      "By Deck:\n",
      "         count  sum      mean\n",
      "Deck                         \n",
      "A           15    7  0.466667\n",
      "B           47   35  0.744681\n",
      "C           59   35  0.593220\n",
      "D           33   25  0.757576\n",
      "E           32   24  0.750000\n",
      "F           13    8  0.615385\n",
      "G            4    2  0.500000\n",
      "T            1    0  0.000000\n",
      "Unknown    687  206  0.299854\n"
     ]
    }
   ],
   "source": [
    "# Analyze baseline features from exp_000\n",
    "# Recreate baseline features\n",
    "def create_baseline_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract title from name\n",
    "    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "    \n",
    "    # Simplify titles\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Mrs': 'Mrs', 'Miss': 'Miss', 'Master': 'Master',\n",
    "        'Dr': 'Other', 'Rev': 'Other', 'Col': 'Other', 'Major': 'Other',\n",
    "        'Mlle': 'Miss', 'Countess': 'Other', 'Ms': 'Miss', 'Lady': 'Other',\n",
    "        'Jonkheer': 'Other', 'Don': 'Other', 'Dona': 'Other', 'Mme': 'Mrs',\n",
    "        'Capt': 'Other', 'Sir': 'Other'\n",
    "    }\n",
    "    df['Title'] = df['Title'].map(title_mapping)\n",
    "    \n",
    "    # Family size\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    \n",
    "    # Is alone\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    # Age groups\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], \n",
    "                           labels=['Child', 'Teen', 'Adult', 'MiddleAge', 'Senior'])\n",
    "    \n",
    "    # Fare per person\n",
    "    df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "    \n",
    "    # Extract deck from cabin\n",
    "    df['Deck'] = df['Cabin'].str[0]\n",
    "    df['Deck'] = df['Deck'].fillna('Unknown')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_baseline = create_baseline_features(train_df)\n",
    "\n",
    "# Analyze feature correlations with target\n",
    "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'IsAlone', 'AgeGroup', 'Deck']\n",
    "\n",
    "print(\"Survival rates by key features:\")\n",
    "print(\"\\nBy Pclass:\")\n",
    "print(train_baseline.groupby('Pclass')['Survived'].agg(['count', 'sum', 'mean']))\n",
    "\n",
    "print(\"\\nBy Sex:\")\n",
    "print(train_baseline.groupby('Sex')['Survived'].agg(['count', 'sum', 'mean']))\n",
    "\n",
    "print(\"\\nBy Title:\")\n",
    "print(train_baseline.groupby('Title')['Survived'].agg(['count', 'sum', 'mean']))\n",
    "\n",
    "print(\"\\nBy Deck:\")\n",
    "print(train_baseline.groupby('Deck')['Survived'].agg(['count', 'sum', 'mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ticket patterns - potential improvement area\n",
    "print(\"Ticket value examples:\")\n",
    "print(train_df['Ticket'].head(20).tolist())\n",
    "\n",
    "# Extract ticket prefix patterns\n",
    "train_df['TicketPrefix'] = train_df['Ticket'].str.extract('^([A-Z/]+)', expand=False)\n",
    "train_df['TicketPrefix'] = train_df['TicketPrefix'].fillna('None')\n",
    "\n",
    "print(\"\\nTicket prefixes and survival rates:\")\n",
    "ticket_analysis = train_df.groupby('TicketPrefix')['Survived'].agg(['count', 'sum', 'mean']).sort_values('count', ascending=False)\n",
    "print(ticket_analysis.head(10))\n",
    "\n",
    "# Analyze ticket frequency (shared tickets = families/groups)\n",
    "train_df['TicketFreq'] = train_df.groupby('Ticket')['Ticket'].transform('count')\n",
    "print(f\"\\nTicket frequency distribution:\")\n",
    "print(train_df['TicketFreq'].value_counts().head())\n",
    "\n",
    "print(\"\\nSurvival by ticket frequency:\")\n",
    "print(train_df.groupby('TicketFreq')['Survived'].agg(['count', 'mean']))\n",
    "\n",
    "# Also add Deck feature for later use\n",
    "train_df['Deck'] = train_df['Cabin'].str[0]\n",
    "train_df['Deck'] = train_df['Deck'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83144132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cabin patterns more deeply\n",
    "print(\"Cabin examples:\")\n",
    "print(train_df['Cabin'].dropna().head(20).tolist())\n",
    "\n",
    "# Extract more detailed cabin information\n",
    "train_df['CabinNum'] = train_df['Cabin'].str.extract('([0-9]+)', expand=False)\n",
    "train_df['CabinNum'] = pd.to_numeric(train_df['CabinNum'], errors='coerce')\n",
    "\n",
    "# Cabin location (odd/even might indicate port/starboard)\n",
    "train_df['CabinSide'] = np.where(train_df['CabinNum'] % 2 == 0, 'Even', 'Odd')\n",
    "train_df['CabinSide'] = train_df['CabinSide'].fillna('Unknown')\n",
    "\n",
    "print(\"\\nSurvival by cabin side (odd/even):\")\n",
    "cabin_side_analysis = train_df.groupby('CabinSide')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "print(cabin_side_analysis)\n",
    "\n",
    "# Analyze cabin deck + side combination\n",
    "train_df['DeckSide'] = train_df['Deck'] + '_' + train_df['CabinSide']\n",
    "print(\"\\nSurvival by deck and side:\")\n",
    "deck_side_analysis = train_df.groupby('DeckSide')['Survived'].agg(['count', 'mean']).sort_values('mean', ascending=False)\n",
    "print(deck_side_analysis.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze name features\n",
    "print(\"Name examples:\")\n",
    "print(train_df['Name'].head(10).tolist())\n",
    "\n",
    "# Name length\n",
    "train_df['NameLength'] = train_df['Name'].str.len()\n",
    "print(f\"\\nName length statistics:\")\n",
    "print(train_df['NameLength'].describe())\n",
    "\n",
    "# Correlation with survival\n",
    "name_length_corr = train_df['NameLength'].corr(train_df['Survived'])\n",
    "print(f\"\\nCorrelation between name length and survival: {name_length_corr:.3f}\")\n",
    "\n",
    "# Analyze fare patterns more deeply\n",
    "print(\"\\nFare statistics:\")\n",
    "print(train_df['Fare'].describe())\n",
    "\n",
    "# Fare bins - more granular than baseline\n",
    "fare_bins = [0, 7.91, 14.45, 31.0, 100, 600]\n",
    "fare_labels = ['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh']\n",
    "train_df['FareBin'] = pd.cut(train_df['Fare'], bins=fare_bins, labels=fare_labels)\n",
    "\n",
    "print(\"\\nSurvival by fare bins:\")\n",
    "fare_bin_analysis = train_df.groupby('FareBin')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "print(fare_bin_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5552e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis using baseline model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare baseline features\n",
    "numeric_features_baseline = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "categorical_features_baseline = ['Pclass', 'Sex', 'Embarked', 'Title', 'IsAlone', 'AgeGroup', 'Deck']\n",
    "\n",
    "X_baseline = train_baseline[numeric_features_baseline + categorical_features_baseline]\n",
    "y = train_baseline['Survived']\n",
    "\n",
    "# Create pipeline\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features_baseline),\n",
    "        ('cat', categorical_transformer, categorical_features_baseline)\n",
    "    ])\n",
    "\n",
    "# Train baseline model\n",
    "rf_baseline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_baseline.fit(X_baseline, y)\n",
    "\n",
    "# Get feature importance\n",
    "categorical_features_encoded = list(rf_baseline.named_steps['preprocessor']\n",
    "                                   .named_transformers_['cat']\n",
    "                                   .named_steps['encoder']\n",
    "                                   .get_feature_names_out(categorical_features_baseline))\n",
    "\n",
    "all_features = numeric_features_baseline + categorical_features_encoded\n",
    "importances = rf_baseline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 feature importances:\")\n",
    "print(feature_importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27642512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications to identify patterns\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Get cross-validated predictions\n",
    "y_pred = cross_val_predict(rf_baseline, X_baseline, y, cv=5)\n",
    "\n",
    "# Identify misclassifications\n",
    "misclassified = (y_pred != y)\n",
    "misclassified_indices = misclassified[misclassified].index\n",
    "\n",
    "print(f\"Total misclassified: {misclassified.sum()} out of {len(y)} ({misclassified.mean():.1%})\")\n",
    "\n",
    "# Analyze misclassifications by groups\n",
    "print(\"\\nMisclassification rates by Pclass:\")\n",
    "print(pd.crosstab(train_baseline.loc[misclassified_indices, 'Pclass'], \n",
    "                  columns='count', normalize='index'))\n",
    "\n",
    "print(\"\\nMisclassification rates by Sex:\")\n",
    "print(pd.crosstab(train_baseline.loc[misclassified_indices, 'Sex'], \n",
    "                  columns='count', normalize='index'))\n",
    "\n",
    "print(\"\\nMisclassification rates by Title:\")\n",
    "print(pd.crosstab(train_baseline.loc[misclassified_indices, 'Title'], \n",
    "                  columns='count', normalize='index'))\n",
    "\n",
    "# Look at some specific misclassified cases\n",
    "print(\"\\nSample misclassified cases (True label -> Predicted):\")\n",
    "misclassified_df = train_baseline.loc[misclassified_indices].copy()\n",
    "misclassified_df['TrueLabel'] = y[misclassified_indices]\n",
    "misclassified_df['PredictedLabel'] = y_pred[misclassified_indices]\n",
    "\n",
    "print(misclassified_df[['Name', 'Pclass', 'Sex', 'Age', 'Title', 'Fare', 'TrueLabel', 'PredictedLabel']].head(10))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
