{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7807c6",
   "metadata": {},
   "source": [
    "# Loop 2: LB Feedback Analysis - Massive CV-LB Gap Investigation\n",
    "\n",
    "**Submission**: exp_002_fixed_preprocessing  \n",
    "**CV Score**: 83.84%  \n",
    "**LB Score**: 0.7464  \n",
    "**Gap**: +83.0936 (CV much higher than LB)\n",
    "\n",
    "This is a MASSIVE gap that needs immediate investigation. Either:\n",
    "1. Our CV is severely over-optimistic\n",
    "2. There's a fundamental misunderstanding of the metric\n",
    "3. Data leakage is still present\n",
    "4. The submission format is wrong\n",
    "\n",
    "Let's investigate systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "print(\"=== SUBMISSION HISTORY ===\")\n",
    "for sub in session_state['submissions']:\n",
    "    print(f\"Experiment: {sub['experiment_id']}\")\n",
    "    print(f\"CV Score: {sub['cv_score']:.4f}\")\n",
    "    print(f\"LB Score: {sub['lb_score']:.4f}\")\n",
    "    print(f\"Gap: {sub['cv_score'] - sub['lb_score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Remaining submissions: {session_state['remaining_submissions']}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3693c",
   "metadata": {},
   "source": [
    "## 1. Verify Submission Format\n",
    "\n",
    "First, let's check if the submission file was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the actual submission file\n",
    "submission_path = '/home/code/submission_candidates/candidate_001.csv'\n",
    "submission = pd.read_csv(submission_path)\n",
    "\n",
    "print(\"=== SUBMISSION FILE CHECK ===\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Columns: {list(submission.columns)}\")\n",
    "print()\n",
    "print(\"First 10 rows:\")\n",
    "print(submission.head(10))\n",
    "print()\n",
    "print(\"Value distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print()\n",
    "print(\"Data types:\")\n",
    "print(submission.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8f80e",
   "metadata": {},
   "source": [
    "## 2. Check Metric Direction\n",
    "\n",
    "The session state says metric_direction=false, which means lower is better. But we're interpreting accuracy (higher is better). Let's verify what metric Kaggle actually uses for Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c20565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what the actual evaluation metric should be\n",
    "# Titanic competition typically uses accuracy (higher is better)\n",
    "\n",
    "print(\"=== METRIC INVESTIGATION ===\")\n",
    "print(\"Session state metric_direction:\", session_state['metric_direction'])\n",
    "print(\"This means 'lower is better' according to the system\")\n",
    "print()\n",
    "print(\"But Titanic competition uses ACCURACY (higher is better)\")\n",
    "print(\"Typical Titanic LB scores are in the range: 0.70 - 0.85\")\n",
    "print()\n",
    "print(\"POSSIBLE ISSUE: The metric direction might be configured wrong!\")\n",
    "print(\"If the system thinks lower is better, it might be:\")\n",
    "print(\"1. Reporting 1 - accuracy instead of accuracy\")\n",
    "print(\"2. Or there's a fundamental misunderstanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaaf1b9",
   "metadata": {},
   "source": [
    "## 3. Compare CV vs LB Score Ranges\n",
    "\n",
    "Let's look at what typical CV and LB scores should be for Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EXPECTED SCORE RANGES ===\")\n",
    "print(\"Based on Kaggle Titanic competition history:\")\n",
    "print(\"- Good CV scores: 0.80 - 0.86 (80% to 86% accuracy)\")\n",
    "print(\"- Good LB scores: 0.75 - 0.85 (75% to 85% accuracy)\")\n",
    "print(\"- Typical CV-LB gap: ±0.02 to ±0.05 (2% to 5%)\")\n",
    "print()\n",
    "print(\"OUR SCORES:\")\n",
    "print(f\"- CV: 0.8384 (83.84% - this is GOOD)\")\n",
    "print(f\"- LB: 0.7464 (74.64% - this is REASONABLE)\")\n",
    "print(f\"- Gap: +0.0920 (9.2% - this is LARGE but not catastrophic)\")\n",
    "print()\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"- Our CV score of 83.84% is actually very good for Titanic\")\n",
    "print(\"- Our LB score of 74.64% is reasonable but on the lower side\")\n",
    "print(\"- The 9.2% gap suggests some overfitting or distribution shift\")\n",
    "print(\"- BUT: The gap is NOT +83.09 as initially calculated!\")\n",
    "print()\n",
    "print(\"THE REAL GAP IS 9.2%, NOT 83.09%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5566d",
   "metadata": {},
   "source": [
    "## 4. Investigate Potential Issues\n",
    "\n",
    "Let's check for common problems that cause CV-LB gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data to check for potential issues\n",
    "train_path = '/home/data/train.csv'\n",
    "test_path = '/home/data/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(\"=== DATA DISTRIBUTION ANALYSIS ===\")\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Check for distribution shift in key features\n",
    "print(\"Sex distribution:\")\n",
    "print(\"Train:\", train_df['Sex'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['Sex'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print(\"Pclass distribution:\")\n",
    "print(\"Train:\", train_df['Pclass'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['Pclass'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print(\"Embarked distribution:\")\n",
    "print(\"Train:\", train_df['Embarked'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['Embarked'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in test set:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad9147",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Analysis\n",
    "\n",
    "Let's see if our features might be causing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "print(\"Our feature engineering includes:\")\n",
    "print(\"- Title extraction (Mr, Mrs, Miss, Master, Other)\")\n",
    "print(\"- FamilySize (SibSp + Parch + 1)\")\n",
    "print(\"- IsAlone flag (FamilySize == 1)\")\n",
    "print(\"- Age bins [0,12,18,35,60,100]\")\n",
    "print(\"- FarePerPerson (Fare / FamilySize)\")\n",
    "print(\"- HasCabin (binary)\")\n",
    "print()\n",
    "print(\"Potential overfitting risks:\")\n",
    "print(\"- Age bins with many thresholds (5 bins) on small dataset\")\n",
    "print(\"- Title 'Other' category with 29 rare titles combined\")\n",
    "print(\"- No interaction features (which could help generalization)\")\n",
    "print()\n",
    "print(\"The model might be memorizing training patterns that don't generalize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b1942",
   "metadata": {},
   "source": [
    "## 6. Model Complexity Check\n",
    "\n",
    "Let's check if our model is too complex for the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL COMPLEXITY ANALYSIS ===\")\n",
    "print(\"XGBoost parameters:\")\n",
    "print(\"- n_estimators: 500\")\n",
    "print(\"- max_depth: 5\") \n",
    "print(\"- learning_rate: 0.1\")\n",
    "print()\n",
    "print(f\"Training set size: {train_df.shape[0]} samples\")\n",
    "print(f\"Number of features: ~15 (after encoding)\")\n",
    "print()\n",
    "print(\"Complexity assessment:\")\n",
    "print(\"- 500 trees is reasonable for 891 samples\")\n",
    "print(\"- max_depth=5 is moderate complexity\")\n",
    "print(\"- But with many engineered features, could be overfitting\")\n",
    "print()\n",
    "print(\"Suggestion: Try reducing complexity:\")\n",
    "print(\"- max_depth: 3-4 instead of 5\")\n",
    "print(\"- learning_rate: 0.05 instead of 0.1\")\n",
    "print(\"- Add regularization: min_child_weight, gamma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2954a96",
   "metadata": {},
   "source": [
    "## 7. Action Plan\n",
    "\n",
    "Based on this analysis, here's what we need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ACTION PLAN ===\")\n",
    "print()\n",
    "print(\"1. CORRECT THE GAP CALCULATION:\")\n",
    "print(\"   - Real gap is 9.2%, not 83.09%\")\n",
    "print(\"   - CV: 83.84%, LB: 74.64%\")\n",
    "print(\"   - This is large but manageable\")\n",
    "print()\n",
    "print(\"2. IMMEDIATE NEXT STEPS:\")\n",
    "print(\"   a) Hyperparameter tuning (Evaluator's top priority)\")\n",
    "print(\"   b) Try simpler model (max_depth=3-4)\")\n",
    "print(\"   c) Add interaction features (Pclass×Sex, Age×Sex)\")\n",
    "print(\"   d) Refine title categories (split 'Other')\")\n",
    "print()\n",
    "print(\"3. ENSEMBLE STRATEGY:\")\n",
    "print(\"   - Add Logistic Regression for diversity\")\n",
    "print(\"   - Weighted blend: 75% XGBoost + 25% LR\")\n",
    "print(\"   - This often helps with generalization\")\n",
    "print()\n",
    "print(\"4. MONITORING:\")\n",
    "print(\"   - Track CV-LB gap after each change\")\n",
    "print(\"   - Aim to reduce gap to <5%\")\n",
    "print(\"   - Target LB score: 0.80+\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
