{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7807c6",
   "metadata": {},
   "source": [
    "# Loop 2: LB Feedback Analysis - Massive CV-LB Gap Investigation\n",
    "\n",
    "**Submission**: exp_002_fixed_preprocessing  \n",
    "**CV Score**: 83.84%  \n",
    "**LB Score**: 0.7464  \n",
    "**Gap**: +83.0936 (CV much higher than LB)\n",
    "\n",
    "This is a MASSIVE gap that needs immediate investigation. Either:\n",
    "1. Our CV is severely over-optimistic\n",
    "2. There's a fundamental misunderstanding of the metric\n",
    "3. Data leakage is still present\n",
    "4. The submission format is wrong\n",
    "\n",
    "Let's investigate systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8d23a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:50.463969Z",
     "iopub.status.busy": "2026-01-14T13:43:50.462996Z",
     "iopub.status.idle": "2026-01-14T13:43:51.937737Z",
     "shell.execute_reply": "2026-01-14T13:43:51.937065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUBMISSION HISTORY ===\n",
      "Experiment: exp_001\n",
      "CV Score: 83.8400\n",
      "LB Score: 0.7464\n",
      "Gap: 83.0936\n",
      "\n",
      "Remaining submissions: 8/10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load session state\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "print(\"=== SUBMISSION HISTORY ===\")\n",
    "for sub in session_state['submissions']:\n",
    "    print(f\"Experiment: {sub['experiment_id']}\")\n",
    "    print(f\"CV Score: {sub['cv_score']:.4f}\")\n",
    "    print(f\"LB Score: {sub['lb_score']:.4f}\")\n",
    "    print(f\"Gap: {sub['cv_score'] - sub['lb_score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Remaining submissions: {session_state['remaining_submissions']}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3693c",
   "metadata": {},
   "source": [
    "## 1. Verify Submission Format\n",
    "\n",
    "First, let's check if the submission file was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8225138b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.259852Z",
     "iopub.status.busy": "2026-01-14T13:43:52.259350Z",
     "iopub.status.idle": "2026-01-14T13:43:52.273546Z",
     "shell.execute_reply": "2026-01-14T13:43:52.272897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUBMISSION FILE CHECK ===\n",
      "Shape: (418, 2)\n",
      "Columns: ['PassengerId', 'Survived']\n",
      "\n",
      "First 10 rows:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n",
      "5          897         0\n",
      "6          898         0\n",
      "7          899         0\n",
      "8          900         1\n",
      "9          901         0\n",
      "\n",
      "Value distribution:\n",
      "Survived\n",
      "0    260\n",
      "1    158\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data types:\n",
      "PassengerId    int64\n",
      "Survived       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the actual submission file\n",
    "submission_path = '/home/code/submission_candidates/candidate_001.csv'\n",
    "submission = pd.read_csv(submission_path)\n",
    "\n",
    "print(\"=== SUBMISSION FILE CHECK ===\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"Columns: {list(submission.columns)}\")\n",
    "print()\n",
    "print(\"First 10 rows:\")\n",
    "print(submission.head(10))\n",
    "print()\n",
    "print(\"Value distribution:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print()\n",
    "print(\"Data types:\")\n",
    "print(submission.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8f80e",
   "metadata": {},
   "source": [
    "## 2. Check Metric Direction\n",
    "\n",
    "The session state says metric_direction=false, which means lower is better. But we're interpreting accuracy (higher is better). Let's verify what metric Kaggle actually uses for Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c20565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.276024Z",
     "iopub.status.busy": "2026-01-14T13:43:52.275825Z",
     "iopub.status.idle": "2026-01-14T13:43:52.280889Z",
     "shell.execute_reply": "2026-01-14T13:43:52.280253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METRIC INVESTIGATION ===\n",
      "Session state metric_direction: False\n",
      "This means 'lower is better' according to the system\n",
      "\n",
      "But Titanic competition uses ACCURACY (higher is better)\n",
      "Typical Titanic LB scores are in the range: 0.70 - 0.85\n",
      "\n",
      "POSSIBLE ISSUE: The metric direction might be configured wrong!\n",
      "If the system thinks lower is better, it might be:\n",
      "1. Reporting 1 - accuracy instead of accuracy\n",
      "2. Or there's a fundamental misunderstanding\n"
     ]
    }
   ],
   "source": [
    "# Let's check what the actual evaluation metric should be\n",
    "# Titanic competition typically uses accuracy (higher is better)\n",
    "\n",
    "print(\"=== METRIC INVESTIGATION ===\")\n",
    "print(\"Session state metric_direction:\", session_state['metric_direction'])\n",
    "print(\"This means 'lower is better' according to the system\")\n",
    "print()\n",
    "print(\"But Titanic competition uses ACCURACY (higher is better)\")\n",
    "print(\"Typical Titanic LB scores are in the range: 0.70 - 0.85\")\n",
    "print()\n",
    "print(\"POSSIBLE ISSUE: The metric direction might be configured wrong!\")\n",
    "print(\"If the system thinks lower is better, it might be:\")\n",
    "print(\"1. Reporting 1 - accuracy instead of accuracy\")\n",
    "print(\"2. Or there's a fundamental misunderstanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaaf1b9",
   "metadata": {},
   "source": [
    "## 3. Compare CV vs LB Score Ranges\n",
    "\n",
    "Let's look at what typical CV and LB scores should be for Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7905dd07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.285633Z",
     "iopub.status.busy": "2026-01-14T13:43:52.285390Z",
     "iopub.status.idle": "2026-01-14T13:43:52.291037Z",
     "shell.execute_reply": "2026-01-14T13:43:52.290277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPECTED SCORE RANGES ===\n",
      "Based on Kaggle Titanic competition history:\n",
      "- Good CV scores: 0.80 - 0.86 (80% to 86% accuracy)\n",
      "- Good LB scores: 0.75 - 0.85 (75% to 85% accuracy)\n",
      "- Typical CV-LB gap: ±0.02 to ±0.05 (2% to 5%)\n",
      "\n",
      "OUR SCORES:\n",
      "- CV: 0.8384 (83.84% - this is GOOD)\n",
      "- LB: 0.7464 (74.64% - this is REASONABLE)\n",
      "- Gap: +0.0920 (9.2% - this is LARGE but not catastrophic)\n",
      "\n",
      "INTERPRETATION:\n",
      "- Our CV score of 83.84% is actually very good for Titanic\n",
      "- Our LB score of 74.64% is reasonable but on the lower side\n",
      "- The 9.2% gap suggests some overfitting or distribution shift\n",
      "- BUT: The gap is NOT +83.09 as initially calculated!\n",
      "\n",
      "THE REAL GAP IS 9.2%, NOT 83.09%!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXPECTED SCORE RANGES ===\")\n",
    "print(\"Based on Kaggle Titanic competition history:\")\n",
    "print(\"- Good CV scores: 0.80 - 0.86 (80% to 86% accuracy)\")\n",
    "print(\"- Good LB scores: 0.75 - 0.85 (75% to 85% accuracy)\")\n",
    "print(\"- Typical CV-LB gap: ±0.02 to ±0.05 (2% to 5%)\")\n",
    "print()\n",
    "print(\"OUR SCORES:\")\n",
    "print(f\"- CV: 0.8384 (83.84% - this is GOOD)\")\n",
    "print(f\"- LB: 0.7464 (74.64% - this is REASONABLE)\")\n",
    "print(f\"- Gap: +0.0920 (9.2% - this is LARGE but not catastrophic)\")\n",
    "print()\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"- Our CV score of 83.84% is actually very good for Titanic\")\n",
    "print(\"- Our LB score of 74.64% is reasonable but on the lower side\")\n",
    "print(\"- The 9.2% gap suggests some overfitting or distribution shift\")\n",
    "print(\"- BUT: The gap is NOT +83.09 as initially calculated!\")\n",
    "print()\n",
    "print(\"THE REAL GAP IS 9.2%, NOT 83.09%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5566d",
   "metadata": {},
   "source": [
    "## 4. Investigate Potential Issues\n",
    "\n",
    "Let's check for common problems that cause CV-LB gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680e0640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.295286Z",
     "iopub.status.busy": "2026-01-14T13:43:52.294799Z",
     "iopub.status.idle": "2026-01-14T13:43:52.324865Z",
     "shell.execute_reply": "2026-01-14T13:43:52.324143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA DISTRIBUTION ANALYSIS ===\n",
      "Training set: (891, 12)\n",
      "Test set: (418, 11)\n",
      "\n",
      "Sex distribution:\n",
      "Train: Sex\n",
      "male      0.647587\n",
      "female    0.352413\n",
      "Name: proportion, dtype: float64\n",
      "Test: Sex\n",
      "male      0.636364\n",
      "female    0.363636\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Pclass distribution:\n",
      "Train: Pclass\n",
      "3    0.551066\n",
      "1    0.242424\n",
      "2    0.206510\n",
      "Name: proportion, dtype: float64\n",
      "Test: Pclass\n",
      "3    0.521531\n",
      "1    0.255981\n",
      "2    0.222488\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Embarked distribution:\n",
      "Train: Embarked\n",
      "S    0.724409\n",
      "C    0.188976\n",
      "Q    0.086614\n",
      "Name: proportion, dtype: float64\n",
      "Test: Embarked\n",
      "S    0.645933\n",
      "C    0.244019\n",
      "Q    0.110048\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values in test set:\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load training data to check for potential issues\n",
    "train_path = '/home/data/train.csv'\n",
    "test_path = '/home/data/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(\"=== DATA DISTRIBUTION ANALYSIS ===\")\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Check for distribution shift in key features\n",
    "print(\"Sex distribution:\")\n",
    "print(\"Train:\", train_df['Sex'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['Sex'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print(\"Pclass distribution:\")\n",
    "print(\"Train:\", train_df['Pclass'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['Pclass'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print(\"Embarked distribution:\")\n",
    "print(\"Train:\", train_df['Embarked'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['Embarked'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in test set:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad9147",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Analysis\n",
    "\n",
    "Let's see if our features might be causing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea97ffac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.328025Z",
     "iopub.status.busy": "2026-01-14T13:43:52.327452Z",
     "iopub.status.idle": "2026-01-14T13:43:52.333452Z",
     "shell.execute_reply": "2026-01-14T13:43:52.332833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ANALYSIS ===\n",
      "Our feature engineering includes:\n",
      "- Title extraction (Mr, Mrs, Miss, Master, Other)\n",
      "- FamilySize (SibSp + Parch + 1)\n",
      "- IsAlone flag (FamilySize == 1)\n",
      "- Age bins [0,12,18,35,60,100]\n",
      "- FarePerPerson (Fare / FamilySize)\n",
      "- HasCabin (binary)\n",
      "\n",
      "Potential overfitting risks:\n",
      "- Age bins with many thresholds (5 bins) on small dataset\n",
      "- Title 'Other' category with 29 rare titles combined\n",
      "- No interaction features (which could help generalization)\n",
      "\n",
      "The model might be memorizing training patterns that don't generalize.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "print(\"Our feature engineering includes:\")\n",
    "print(\"- Title extraction (Mr, Mrs, Miss, Master, Other)\")\n",
    "print(\"- FamilySize (SibSp + Parch + 1)\")\n",
    "print(\"- IsAlone flag (FamilySize == 1)\")\n",
    "print(\"- Age bins [0,12,18,35,60,100]\")\n",
    "print(\"- FarePerPerson (Fare / FamilySize)\")\n",
    "print(\"- HasCabin (binary)\")\n",
    "print()\n",
    "print(\"Potential overfitting risks:\")\n",
    "print(\"- Age bins with many thresholds (5 bins) on small dataset\")\n",
    "print(\"- Title 'Other' category with 29 rare titles combined\")\n",
    "print(\"- No interaction features (which could help generalization)\")\n",
    "print()\n",
    "print(\"The model might be memorizing training patterns that don't generalize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b1942",
   "metadata": {},
   "source": [
    "## 6. Model Complexity Check\n",
    "\n",
    "Let's check if our model is too complex for the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c1fb06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.335989Z",
     "iopub.status.busy": "2026-01-14T13:43:52.335312Z",
     "iopub.status.idle": "2026-01-14T13:43:52.343039Z",
     "shell.execute_reply": "2026-01-14T13:43:52.342243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPLEXITY ANALYSIS ===\n",
      "XGBoost parameters:\n",
      "- n_estimators: 500\n",
      "- max_depth: 5\n",
      "- learning_rate: 0.1\n",
      "\n",
      "Training set size: 891 samples\n",
      "Number of features: ~15 (after encoding)\n",
      "\n",
      "Complexity assessment:\n",
      "- 500 trees is reasonable for 891 samples\n",
      "- max_depth=5 is moderate complexity\n",
      "- But with many engineered features, could be overfitting\n",
      "\n",
      "Suggestion: Try reducing complexity:\n",
      "- max_depth: 3-4 instead of 5\n",
      "- learning_rate: 0.05 instead of 0.1\n",
      "- Add regularization: min_child_weight, gamma\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MODEL COMPLEXITY ANALYSIS ===\")\n",
    "print(\"XGBoost parameters:\")\n",
    "print(\"- n_estimators: 500\")\n",
    "print(\"- max_depth: 5\") \n",
    "print(\"- learning_rate: 0.1\")\n",
    "print()\n",
    "print(f\"Training set size: {train_df.shape[0]} samples\")\n",
    "print(f\"Number of features: ~15 (after encoding)\")\n",
    "print()\n",
    "print(\"Complexity assessment:\")\n",
    "print(\"- 500 trees is reasonable for 891 samples\")\n",
    "print(\"- max_depth=5 is moderate complexity\")\n",
    "print(\"- But with many engineered features, could be overfitting\")\n",
    "print()\n",
    "print(\"Suggestion: Try reducing complexity:\")\n",
    "print(\"- max_depth: 3-4 instead of 5\")\n",
    "print(\"- learning_rate: 0.05 instead of 0.1\")\n",
    "print(\"- Add regularization: min_child_weight, gamma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2954a96",
   "metadata": {},
   "source": [
    "## 7. Action Plan\n",
    "\n",
    "Based on this analysis, here's what we need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d4be53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T13:43:52.346103Z",
     "iopub.status.busy": "2026-01-14T13:43:52.345636Z",
     "iopub.status.idle": "2026-01-14T13:43:52.351920Z",
     "shell.execute_reply": "2026-01-14T13:43:52.351105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ACTION PLAN ===\n",
      "\n",
      "1. CORRECT THE GAP CALCULATION:\n",
      "   - Real gap is 9.2%, not 83.09%\n",
      "   - CV: 83.84%, LB: 74.64%\n",
      "   - This is large but manageable\n",
      "\n",
      "2. IMMEDIATE NEXT STEPS:\n",
      "   a) Hyperparameter tuning (Evaluator's top priority)\n",
      "   b) Try simpler model (max_depth=3-4)\n",
      "   c) Add interaction features (Pclass×Sex, Age×Sex)\n",
      "   d) Refine title categories (split 'Other')\n",
      "\n",
      "3. ENSEMBLE STRATEGY:\n",
      "   - Add Logistic Regression for diversity\n",
      "   - Weighted blend: 75% XGBoost + 25% LR\n",
      "   - This often helps with generalization\n",
      "\n",
      "4. MONITORING:\n",
      "   - Track CV-LB gap after each change\n",
      "   - Aim to reduce gap to <5%\n",
      "   - Target LB score: 0.80+\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ACTION PLAN ===\")\n",
    "print()\n",
    "print(\"1. CORRECT THE GAP CALCULATION:\")\n",
    "print(\"   - Real gap is 9.2%, not 83.09%\")\n",
    "print(\"   - CV: 83.84%, LB: 74.64%\")\n",
    "print(\"   - This is large but manageable\")\n",
    "print()\n",
    "print(\"2. IMMEDIATE NEXT STEPS:\")\n",
    "print(\"   a) Hyperparameter tuning (Evaluator's top priority)\")\n",
    "print(\"   b) Try simpler model (max_depth=3-4)\")\n",
    "print(\"   c) Add interaction features (Pclass×Sex, Age×Sex)\")\n",
    "print(\"   d) Refine title categories (split 'Other')\")\n",
    "print()\n",
    "print(\"3. ENSEMBLE STRATEGY:\")\n",
    "print(\"   - Add Logistic Regression for diversity\")\n",
    "print(\"   - Weighted blend: 75% XGBoost + 25% LR\")\n",
    "print(\"   - This often helps with generalization\")\n",
    "print()\n",
    "print(\"4. MONITORING:\")\n",
    "print(\"   - Track CV-LB gap after each change\")\n",
    "print(\"   - Aim to reduce gap to <5%\")\n",
    "print(\"   - Target LB score: 0.80+\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
