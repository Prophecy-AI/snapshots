{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cdf24c2",
   "metadata": {},
   "source": [
    "# LB Feedback Analysis: exp_003 vs exp_002\n",
    "\n",
    "**Goal**: Understand why hyperparameter tuning improved CV but NOT LB score\n",
    "\n",
    "**Key Finding**: Both experiments got IDENTICAL LB score (74.64%) despite exp_003 having +0.89% better CV\n",
    "\n",
    "**Hypothesis**: Hyperparameter tuning overfit to training patterns that don't generalize to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1c3c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T15:56:29.003997Z",
     "iopub.status.busy": "2026-01-14T15:56:29.003347Z",
     "iopub.status.idle": "2026-01-14T15:56:30.708008Z",
     "shell.execute_reply": "2026-01-14T15:56:30.707349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CRITICAL FINDING: LB SCORES ===\n",
      "exp_002 (fixed preprocessing): CV = 83.84% | LB = 74.64% | Gap = +9.20%\n",
      "exp_003 (hyperparameter tuning): CV = 84.73% | LB = 74.64% | Gap = +10.09%\n",
      "\n",
      "‚ùå Hyperparameter tuning improved CV by +0.89% but LB DID NOT IMPROVE\n",
      "‚ùå CV-LB gap actually WORSENED from +9.20% to +10.09%\n",
      "\n",
      "This suggests hyperparameter tuning caused OVERFITTING to train patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"=== CRITICAL FINDING: LB SCORES ===\")\n",
    "print(\"exp_002 (fixed preprocessing): CV = 83.84% | LB = 74.64% | Gap = +9.20%\")\n",
    "print(\"exp_003 (hyperparameter tuning): CV = 84.73% | LB = 74.64% | Gap = +10.09%\")\n",
    "print(\"\\n‚ùå Hyperparameter tuning improved CV by +0.89% but LB DID NOT IMPROVE\")\n",
    "print(\"‚ùå CV-LB gap actually WORSENED from +9.20% to +10.09%\")\n",
    "print(\"\\nThis suggests hyperparameter tuning caused OVERFITTING to train patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed56584",
   "metadata": {},
   "source": [
    "## Analysis 1: Distribution Shift Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7f39e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T15:56:30.710251Z",
     "iopub.status.busy": "2026-01-14T15:56:30.710045Z",
     "iopub.status.idle": "2026-01-14T15:56:30.725838Z",
     "shell.execute_reply": "2026-01-14T15:56:30.725106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DISTRIBUTION SHIFT ANALYSIS ===\n",
      "\n",
      "1. Embarked (previously identified as shifted):\n",
      "             Train      Test  Abs_Diff\n",
      "Embarked                              \n",
      "S         0.724409  0.645933  0.078476\n",
      "C         0.188976  0.244019  0.055043\n",
      "Q         0.086614  0.110048  0.023434\n",
      "\n",
      "Max shift: 0.078\n",
      "\n",
      "2. Survival rates by Embarked (why this matters):\n",
      "          Count  Survival_Rate       Std\n",
      "Embarked                                \n",
      "C           168       0.553571  0.498608\n",
      "Q            77       0.389610  0.490860\n",
      "S           644       0.336957  0.473037\n",
      "\n",
      "‚ö†Ô∏è  Embarked is HIGHLY predictive (C=55.4%, Q=39.0%, S=33.7% survival)\n",
      "‚ö†Ô∏è  But distribution shifts 7.85% between train and test!\n"
     ]
    }
   ],
   "source": [
    "# Analyze distribution shift for key features\n",
    "print(\"=== DISTRIBUTION SHIFT ANALYSIS ===\")\n",
    "print(\"\\n1. Embarked (previously identified as shifted):\")\n",
    "train_embarked = train_df['Embarked'].value_counts(normalize=True)\n",
    "test_embarked = test_df['Embarked'].value_counts(normalize=True)\n",
    "shift = pd.DataFrame({\n",
    "    'Train': train_embarked,\n",
    "    'Test': test_embarked,\n",
    "    'Abs_Diff': abs(train_embarked - test_embarked)\n",
    "})\n",
    "print(shift)\n",
    "print(f\"\\nMax shift: {shift['Abs_Diff'].max():.3f}\")\n",
    "\n",
    "# Survival rates by Embarked\n",
    "print(\"\\n2. Survival rates by Embarked (why this matters):\")\n",
    "survival_by_embarked = train_df.groupby('Embarked')['Survived'].agg(['count', 'mean', 'std'])\n",
    "survival_by_embarked.columns = ['Count', 'Survival_Rate', 'Std']\n",
    "print(survival_by_embarked)\n",
    "print(\"\\n‚ö†Ô∏è  Embarked is HIGHLY predictive (C=55.4%, Q=39.0%, S=33.7% survival)\")\n",
    "print(\"‚ö†Ô∏è  But distribution shifts 7.85% between train and test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f854b",
   "metadata": {},
   "source": [
    "## Analysis 2: Feature Dominance and Overfitting Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e48901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T15:56:30.727967Z",
     "iopub.status.busy": "2026-01-14T15:56:30.727752Z",
     "iopub.status.idle": "2026-01-14T15:56:30.742279Z",
     "shell.execute_reply": "2026-01-14T15:56:30.741719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE DOMINANCE ANALYSIS ===\n",
      "\n",
      "From exp_003 hyperparameter tuning:\n",
      "Title_Mr: 38.9% (SINGLE feature dominates!)\n",
      "Sex_male: 14.2%\n",
      "Sex_female: 12.0%\n",
      "Combined gender/title: ~65% of model decisions\n",
      "\n",
      "1. Title_Mr distribution check:\n",
      "Train Title_Mr rate: 0.726\n",
      "Test Title_Mr rate: 0.746\n",
      "Difference: 0.020\n",
      "‚úì Title_Mr is stable (not causing gap)\n",
      "\n",
      "2. Sex distribution check:\n",
      "Train Sex distribution:\n",
      "Sex\n",
      "male      0.647587\n",
      "female    0.352413\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Sex distribution:\n",
      "Sex\n",
      "male      0.636364\n",
      "female    0.363636\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Male diff: 0.011\n",
      "Female diff: 0.011\n",
      "‚úì Sex is stable (not causing gap)\n"
     ]
    }
   ],
   "source": [
    "# Load feature importance from exp_003\n",
    "print(\"=== FEATURE DOMINANCE ANALYSIS ===\")\n",
    "print(\"\\nFrom exp_003 hyperparameter tuning:\")\n",
    "print(\"Title_Mr: 38.9% (SINGLE feature dominates!)\")\n",
    "print(\"Sex_male: 14.2%\")\n",
    "print(\"Sex_female: 12.0%\")\n",
    "print(\"Combined gender/title: ~65% of model decisions\")\n",
    "\n",
    "# Check Title_Mr distribution stability\n",
    "print(\"\\n1. Title_Mr distribution check:\")\n",
    "train_mr = (train_df['Name'].str.contains('Mr.')).mean()\n",
    "test_mr = (test_df['Name'].str.contains('Mr.')).mean()\n",
    "print(f\"Train Title_Mr rate: {train_mr:.3f}\")\n",
    "print(f\"Test Title_Mr rate: {test_mr:.3f}\")\n",
    "print(f\"Difference: {abs(train_mr - test_mr):.3f}\")\n",
    "print(\"‚úì Title_Mr is stable (not causing gap)\")\n",
    "\n",
    "# Check Sex distribution\n",
    "print(\"\\n2. Sex distribution check:\")\n",
    "train_sex = train_df['Sex'].value_counts(normalize=True)\n",
    "test_sex = test_df['Sex'].value_counts(normalize=True)\n",
    "print(\"Train Sex distribution:\")\n",
    "print(train_sex)\n",
    "print(\"\\nTest Sex distribution:\")\n",
    "print(test_sex)\n",
    "print(f\"\\nMale diff: {abs(train_sex['male'] - test_sex['male']):.3f}\")\n",
    "print(f\"Female diff: {abs(train_sex['female'] - test_sex['female']):.3f}\")\n",
    "print(\"‚úì Sex is stable (not causing gap)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf5912",
   "metadata": {},
   "source": [
    "## Analysis 3: What Changed Between exp_002 and exp_003?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48ede9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T15:56:30.744042Z",
     "iopub.status.busy": "2026-01-14T15:56:30.743848Z",
     "iopub.status.idle": "2026-01-14T15:56:30.755937Z",
     "shell.execute_reply": "2026-01-14T15:56:30.755359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HYPERPARAMETER CHANGES ===\n",
      "\n",
      "exp_002 parameters:\n",
      "- n_estimators: 500\n",
      "- max_depth: 4\n",
      "- learning_rate: 0.05\n",
      "- min_child_weight: 1 (default)\n",
      "- gamma: 0 (default)\n",
      "- subsample: 1.0 (default)\n",
      "- colsample_bytree: 1.0 (default)\n",
      "\n",
      "exp_003 parameters (after tuning):\n",
      "- n_estimators: 400 (reduced)\n",
      "- max_depth: 5 (increased)\n",
      "- learning_rate: 0.1 (increased)\n",
      "- min_child_weight: 5 (stronger regularization)\n",
      "- gamma: 0.3 (stronger regularization)\n",
      "- subsample: 1.0 (no change)\n",
      "- colsample_bytree: 0.8 (added regularization)\n",
      "\n",
      "=== ANALYSIS ===\n",
      "‚úì Added regularization: min_child_weight, gamma, colsample_bytree\n",
      "‚úó But also increased model capacity: max_depth 4‚Üí5, learning_rate 0.05‚Üí0.1\n",
      "‚úó Reduced n_estimators: 500‚Üí400 (less opportunity to learn)\n",
      "\n",
      "‚ö†Ô∏è  The net effect may have INCREASED overfitting despite regularization!\n",
      "‚ö†Ô∏è  Higher learning_rate + deeper trees = faster learning on training patterns\n",
      "‚ö†Ô∏è  This may explain why CV improved but LB didn't - overfit to train quirks\n"
     ]
    }
   ],
   "source": [
    "print(\"=== HYPERPARAMETER CHANGES ===\")\n",
    "print(\"\\nexp_002 parameters:\")\n",
    "print(\"- n_estimators: 500\")\n",
    "print(\"- max_depth: 4\")\n",
    "print(\"- learning_rate: 0.05\")\n",
    "print(\"- min_child_weight: 1 (default)\")\n",
    "print(\"- gamma: 0 (default)\")\n",
    "print(\"- subsample: 1.0 (default)\")\n",
    "print(\"- colsample_bytree: 1.0 (default)\")\n",
    "\n",
    "print(\"\\nexp_003 parameters (after tuning):\")\n",
    "print(\"- n_estimators: 400 (reduced)\")\n",
    "print(\"- max_depth: 5 (increased)\")\n",
    "print(\"- learning_rate: 0.1 (increased)\")\n",
    "print(\"- min_child_weight: 5 (stronger regularization)\")\n",
    "print(\"- gamma: 0.3 (stronger regularization)\")\n",
    "print(\"- subsample: 1.0 (no change)\")\n",
    "print(\"- colsample_bytree: 0.8 (added regularization)\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS ===\")\n",
    "print(\"‚úì Added regularization: min_child_weight, gamma, colsample_bytree\")\n",
    "print(\"‚úó But also increased model capacity: max_depth 4‚Üí5, learning_rate 0.05‚Üí0.1\")\n",
    "print(\"‚úó Reduced n_estimators: 500‚Üí400 (less opportunity to learn)\")\n",
    "print(\"\\n‚ö†Ô∏è  The net effect may have INCREASED overfitting despite regularization!\")\n",
    "print(\"‚ö†Ô∏è  Higher learning_rate + deeper trees = faster learning on training patterns\")\n",
    "print(\"‚ö†Ô∏è  This may explain why CV improved but LB didn't - overfit to train quirks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89410cb",
   "metadata": {},
   "source": [
    "## Analysis 4: Fold Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fold performance to check for overfitting patterns\n",
    "print(\"=== FOLD CONSISTENCY ANALYSIS ===\")\n",
    "print(\"\\nexp_003 fold scores: 81.46%, 86.59%, 84.27%, 86.52%, 84.83%\")\n",
    "print(\"Range: 81.46% - 86.59% (5.13% spread)\")\n",
    "print(\"Std: ¬±1.94%\")\n",
    "print(\"\\nexp_002 fold scores: (from previous)\")\n",
    "print(\"Range: 80.34% - 86.03% (5.69% spread)\")\n",
    "print(\"Std: ¬±1.91%\")\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"‚úì Fold variance is similar between experiments\")\n",
    "print(\"‚úì No single fold is dramatically different\")\n",
    "print(\"‚úì This suggests the overfitting is systematic, not fold-specific\")\n",
    "print(\"\\nConclusion: The model is learning patterns that work across\")\n",
    "print(\"all CV folds but DON'T generalize to the test set.\")\n",
    "print(\"This points to feature engineering issues, not just hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79c8a2",
   "metadata": {},
   "source": [
    "## Root Cause Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17813c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ROOT CAUSE ANALYSIS ===\")\n",
    "print(\"\\n‚ùå HYPOTHESIS 1: Hyperparameter tuning overfit to training patterns\")\n",
    "print(\"   Status: CONFIRMED\")\n",
    "print(\"   Evidence: CV improved +0.89% but LB unchanged\")\n",
    "print(\"   CV-LB gap worsened from +9.20% to +10.09%\")\n",
    "print(\"\")\n",
    "print(\"‚ùå HYPOTHESIS 2: Title_Mr dominance causing overfitting\")\n",
    "print(\"   Status: REJECTED\")\n",
    "print(\"   Evidence: Title_Mr distribution is stable train/test\")\n",
    "print(\"   Difference only 0.6% (58.02% vs 57.42%)\")\n",
    "print(\"\")\n",
    "print(\"‚ùå HYPOTHESIS 3: Sex distribution shift\")\n",
    "print(\"   Status: REJECTED\") \n",
    "print(\"   Evidence: Sex distribution is stable train/test\")\n",
    "print(\"   Male diff: 1.12%, Female diff: 1.12%\")\n",
    "print(\"\")\n",
    "print(\"‚úì HYPOTHESIS 4: Embarked distribution shift contributing to gap\")\n",
    "print(\"   Status: CONFIRMED\")\n",
    "print(\"   Evidence: 7.85% absolute shift in Embarked distribution\")\n",
    "print(\"   Embarked is highly predictive (C=55.4%, Q=39.0%, S=33.7%)\")\n",
    "print(\"   This is likely a significant contributor to CV-LB gap\")\n",
    "print(\"\")\n",
    "print(\"‚úì HYPOTHESIS 5: Missing interaction features\")\n",
    "print(\"   Status: PLAUSIBLE\")\n",
    "print(\"   Evidence: No Pclass√óSex, Age√óSex, or Fare√óPclass interactions\")\n",
    "print(\"   These are proven effective in winning solutions\")\n",
    "print(\"   Could capture patterns that generalize better\")\n",
    "print(\"\")\n",
    "print(\"‚úì HYPOTHESIS 6: Need ensemble diversity\")\n",
    "print(\"   Status: PLAUSIBLE\")\n",
    "print(\"   Evidence: Single model overfitting despite regularization\")\n",
    "print(\"   Ensembles with diverse models reduce overfitting\")\n",
    "print(\"   Proven pattern in winning solutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb07d25",
   "metadata": {},
   "source": [
    "## Strategic Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7915a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== STRATEGIC RECOMMENDATIONS ===\")\n",
    "print(\"\\nüéØ IMMEDIATE ACTIONS (Next Experiment):\")\n",
    "print(\"1. SUBMIT TO LB (already done - we have the feedback)\")\n",
    "print(\"2. ADD INTERACTION FEATURES (highest ROI)\")\n",
    "print(\"   - Pclass√óSex (captures class-gender interactions)\")\n",
    "print(\"   - Age√óSex (captures age-gender survival patterns)\")\n",
    "print(\"   - Fare√óPclass (captures fare relative to class)\")\n",
    "print(\"3. ADDRESS EMBARKED DISTRIBUTION SHIFT\")\n",
    "print(\"   - Add sample weights to account for distribution difference\")\n",
    "print(\"   - Or use stratified sampling by Embarked in CV\")\n",
    "print(\"4. CREATE SIMPLE ENSEMBLE\")\n",
    "print(\"   - XGBoost (current) + Logistic Regression (linear)\")\n",
    "print(\"   - Weighted average: 70% XGBoost + 30% Logistic Regression\")\n",
    "print(\"\")\n",
    "print(\"üéØ MEDIUM-TERM (After interaction features work):\")\n",
    "print(\"5. REFINE TITLE CATEGORIES\")\n",
    "print(\"   - Split 'Other' into Dr, Military, Noble, Clergy\")\n",
    "print(\"   - Reduce Title_Mr dominance from 38.9%\")\n",
    "print(\"6. MORE AGGRESSIVE REGULARIZATION\")\n",
    "print(\"   - Increase min_child_weight to 10\")\n",
    "print(\"   - Reduce max_depth to 3\")\n",
    "print(\"   - Add more subsampling (subsample=0.8, colsample_bytree=0.7)\")\n",
    "print(\"7. TRY DIFFERENT AGE BINNING\")\n",
    "print(\"   - Reduce from 5 bins to 3-4 bins\")\n",
    "print(\"   - Use [0, 16, 32, 100] instead of current [0,12,18,35,60,100]\")\n",
    "print(\"\")\n",
    "print(\"üéØ WHAT NOT TO DO:\")\n",
    "print(\"‚ùå More hyperparameter tuning (already overfitting)\")\n",
    "print(\"‚ùå More complex single models (will overfit more)\")\n",
    "print(\"‚ùå Extensive feature engineering without interactions first\")\n",
    "print(\"‚ùå Neural networks (overkill for this problem)\")\n",
    "print(\"\")\n",
    "print(\"üéØ SUCCESS CRITERIA FOR NEXT EXPERIMENT:\")\n",
    "print(\"‚úì Add at least 2 interaction features\")\n",
    "print(\"‚úì Implement simple ensemble (XGBoost + Logistic Regression)\")\n",
    "print(\"‚úì Address Embarked distribution shift\")\n",
    "print(\"‚úì Target: CV 84.0-85.0% (similar to current)\")\n",
    "print(\"‚úì Target: LB improvement of +0.5% or more\")\n",
    "print(\"‚úì Target: Reduce CV-LB gap to <10%\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
