{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7370183",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: CV-LB Gap Analysis & Submission Strategy\n",
    "\n",
    "**Goal**: Understand why we have a 10.09% CV-LB gap and decide whether to submit exp_003.\n",
    "\n",
    "**Key Questions**:\n",
    "1. Is the gap due to overfitting, distribution shift, or both?\n",
    "2. Did hyperparameter tuning (exp_003) actually help generalization?\n",
    "3. What should we try next based on LB feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d793e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import json\n",
    "\n",
    "# Load session state to track experiments\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for i, exp in enumerate(session['experiments']):\n",
    "    print(f\"Exp {i}: {exp['name']}\")\n",
    "    print(f\"  CV Score: {exp['score']:.4f}\")\n",
    "    print(f\"  Model: {exp['model_type']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION HISTORY\")\n",
    "print(\"=\"*60)\n",
    "for sub in session['submissions']:\n",
    "    print(f\"Experiment: {sub['experiment_id']}\")\n",
    "    print(f\"  CV: {sub['cv_score']:.4f}\")\n",
    "    print(f\"  LB: {sub['lb_score']:.4f}\")\n",
    "    print(f\"  Gap: {sub['cv_score'] - sub['lb_score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Remaining submissions: {session['remaining_submissions']}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3601606",
   "metadata": {},
   "source": [
    "## 1. CV-LB Gap Analysis\n",
    "\n",
    "The evaluator identified a 9.2% gap in exp_002. Let's see if exp_003 improved this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gaps\n",
    "gaps = []\n",
    "for sub in session['submissions']:\n",
    "    gaps.append({\n",
    "        'experiment': sub['experiment_id'],\n",
    "        'cv_score': sub['cv_score'],\n",
    "        'lb_score': sub['lb_score'],\n",
    "        'gap': sub['cv_score'] - sub['lb_score']\n",
    "    })\n",
    "\n",
    "gaps_df = pd.DataFrame(gaps)\n",
    "print(gaps_df)\n",
    "\n",
    "# Project exp_003 gap (assuming similar gap as exp_002)\n",
    "exp_003_cv = 84.73\n",
    "exp_002_gap = 9.20  # from evaluator feedback\n",
    "projected_lb = exp_003_cv - exp_002_gap\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXP_003 PROJECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CV Score: {exp_003_cv:.2f}%\")\n",
    "print(f\"Exp_002 Gap: {exp_002_gap:.2f}%\")\n",
    "print(f\"Projected LB: {projected_lb:.2f}%\")\n",
    "print(f\"Projected Improvement: +{projected_lb - 74.64:.2f}% vs exp_002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67aac40",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Analysis - Title_Mr Dominance\n",
    "\n",
    "The evaluator flagged Title_Mr at 38.9% importance as a potential overfitting signal. Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exp_003 data to analyze feature patterns\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "# Recreate Title feature for analysis\n",
    "def extract_title(name):\n",
    "    title = name.split(',')[1].split('.')[0].strip()\n",
    "    if title in ['Mr', 'Mrs', 'Miss', 'Master']:\n",
    "        return title\n",
    "    elif title in ['Dr']:\n",
    "        return 'Dr'\n",
    "    elif title in ['Col', 'Major', 'Capt']:\n",
    "        return 'Military'\n",
    "    elif title in ['Countess', 'Lady', 'Sir', 'Don', 'Dona', 'Jonkheer']:\n",
    "        return 'Noble'\n",
    "    elif title in ['Rev']:\n",
    "        return 'Clergy'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "train_df['Title'] = train_df['Name'].apply(extract_title)\n",
    "test_df['Title'] = test_df['Name'].apply(extract_title)\n",
    "\n",
    "print(\"TITLE DISTRIBUTION IN TRAIN vs TEST:\")\n",
    "print(\"=\"*60)\n",
    "train_titles = train_df['Title'].value_counts(normalize=True) * 100\n",
    "test_titles = test_df['Title'].value_counts(normalize=True) * 100\n",
    "\n",
    "title_comp = pd.DataFrame({\n",
    "    'Train_%': train_titles,\n",
    "    'Test_%': test_titles\n",
    "}).fillna(0)\n",
    "print(title_comp.round(2))\n",
    "\n",
    "# Calculate survival rates by title\n",
    "survival_by_title = train_df.groupby('Title')['Survived'].agg(['count', 'mean', 'std'])\n",
    "survival_by_title['survival_rate'] = survival_by_title['mean'] * 100\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SURVIVAL RATES BY TITLE (TRAIN):\")\n",
    "print(\"=\"*60)\n",
    "print(survival_by_title[['count', 'survival_rate']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ee4aa",
   "metadata": {},
   "source": [
    "## 3. Distribution Shift Analysis\n",
    "\n",
    "Check for distribution differences between train and test that could explain the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325130d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key feature distributions\n",
    "check_features = ['Pclass', 'Sex', 'Embarked', 'HasCabin']\n",
    "\n",
    "# Add HasCabin to test\n",
    "train_df['HasCabin'] = train_df['Cabin'].notna().astype(int)\n",
    "test_df['HasCabin'] = test_df['Cabin'].notna().astype(int)\n",
    "\n",
    "print(\"DISTRIBUTION SHIFT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature in check_features:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    if feature in ['Pclass', 'HasCabin']:\n",
    "        train_dist = train_df[feature].value_counts(normalize=True).sort_index() * 100\n",
    "        test_dist = test_df[feature].value_counts(normalize=True).sort_index() * 100\n",
    "    else:\n",
    "        train_dist = train_df[feature].value_counts(normalize=True) * 100\n",
    "        test_dist = test_df[feature].value_counts(normalize=True) * 100\n",
    "    \n",
    "    comp = pd.DataFrame({\n",
    "        'Train_%': train_dist,\n",
    "        'Test_%': test_dist\n",
    "    }).fillna(0)\n",
    "    print(comp.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48c33a",
   "metadata": {},
   "source": [
    "## 4. Decision: Should We Submit exp_003?\n",
    "\n",
    "**Arguments FOR submitting:**\n",
    "- Evaluator's top priority: \"SUBMIT TO LEADERBOARD NOW\"\n",
    "- We need LB feedback to validate if hyperparameter tuning helps\n",
    "- 8 submissions remaining is plenty\n",
    "- CV improved from 83.84% to 84.73% (+0.89%)\n",
    "\n",
    "**Arguments AGAINST submitting:**\n",
    "- Title_Mr dominance (38.9%) suggests potential overfitting\n",
    "- We haven't tried ensembles yet (evaluator's #3 concern)\n",
    "- Could waste a submission if the model is overfit\n",
    "\n",
    "**My recommendation:** SUBMIT. The evaluator is right - we're flying blind. We need LB feedback to guide the next $1000 of effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION RECOMMENDATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ SUBMIT exp_003 NOW\")\n",
    "print()\n",
    "print(\"Reasoning:\")\n",
    "print(\"1. Evaluator's #1 priority: 'SUBMIT TO LEADERBOARD NOW'\")\n",
    "print(\"2. After 3 experiments, we have no LB feedback on hyperparameter tuning\")\n",
    "print(\"3. CV improved +0.89% with stronger regularization\")\n",
    "print(\"4. 8 submissions remaining - we can afford to test\")\n",
    "print(\"5. LB feedback will tell us if gap is closing or if we need to pivot\")\n",
    "print()\n",
    "print(\"Expected outcomes:\")\n",
    "print(f\"  If LB ≥ 75.5%: Gap closing, continue current direction\")\n",
    "print(f\"  If LB = 74.6%: No improvement, pivot to ensembles\")\n",
    "print(f\"  If LB < 74.0%: Overfitting worse, investigate Title_Mr dominance\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
