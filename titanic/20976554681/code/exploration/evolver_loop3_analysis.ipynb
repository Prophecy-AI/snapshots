{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4b89c6",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis: Post-Fix Optimization Strategy\n",
    "\n",
    "## Objectives\n",
    "1. Analyze current state after fixing data leakage\n",
    "2. Evaluate evaluator feedback and identify highest ROI improvements\n",
    "3. Create strategic plan for next experiments\n",
    "4. Determine if we should submit for LB feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load session state to understand current progress\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "print(\"=== CURRENT STATE SUMMARY ===\")\n",
    "print(f\"Best CV Score: {session_state['experiments'][-1]['score']:.4f}\")\n",
    "print(f\"Best Model: {session_state['experiments'][-1]['name']}\")\n",
    "print(f\"Remaining Submissions: {session_state['remaining_submissions']}\")\n",
    "print(f\"Total Experiments: {len(session_state['experiments'])}\")\n",
    "\n",
    "# Show experiment history\n",
    "print(\"\\n=== EXPERIMENT HISTORY ===\")\n",
    "for exp in session_state['experiments']:\n",
    "    print(f\"{exp['id']}: {exp['name']} - {exp['score']:.4f}% - {exp['model_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5fae1",
   "metadata": {},
   "source": [
    "## Evaluator Feedback Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze evaluator feedback\n",
    "feedback = session_state['feedback_history'][-1]  # Latest feedback\n",
    "print(\"=== LATEST EVALUATOR FEEDBACK ===\")\n",
    "print(f\"Experiment: {feedback['experiment_id']}\")\n",
    "print(f\"Technical Verdict: {feedback['feedback'].split('Verdict:')[1].split('\\\\n')[0].strip()}\")\n",
    "\n",
    "# Extract key concerns\n",
    "concerns = []\n",
    "if \"hyperparameter tuning\" in feedback['feedback'].lower():\n",
    "    concerns.append(\"Hyperparameter tuning under-explored\")\n",
    "if \"ensemble\" in feedback['feedback'].lower():\n",
    "    concerns.append(\"No ensemble strategy\")\n",
    "if \"title refinement\" in feedback['feedback'].lower():\n",
    "    concerns.append(\"Title engineering can be refined\")\n",
    "if \"age bin\" in feedback['feedback'].lower():\n",
    "    concerns.append(\"Age bin thresholds are arbitrary\")\n",
    "if \"leaderboard\" in feedback['feedback'].lower():\n",
    "    concerns.append(\"No LB feedback yet\")\n",
    "\n",
    "print(f\"\\nKey Concerns Identified:\")\n",
    "for i, concern in enumerate(concerns, 1):\n",
    "    print(f\"{i}. {concern}\")\n",
    "\n",
    "# Extract top priority\n",
    "top_priority = feedback['feedback'].split('Top Priority')[1].split('\\\\n')[0].strip()\n",
    "print(f\"\\nEvaluator's Top Priority: {top_priority}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67734ee4",
   "metadata": {},
   "source": [
    "## Competitive Intelligence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70715725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data findings from previous research\n",
    "print(\"=== DATA FINDINGS SUMMARY ===\")\n",
    "for finding in session_state['data_findings']:\n",
    "    print(f\"- {finding['finding'][:100]}...\")\n",
    "\n",
    "# Calculate potential improvement from findings\n",
    "print(\"\\n=== PROJECTED IMPROVEMENTS ===\")\n",
    "current_score = session_state['experiments'][-1]['score']\n",
    "print(f\"Current Score: {current_score:.4f}%\")\n",
    "print(\"\\nPotential improvements identified:\")\n",
    "print(\"- Hyperparameter tuning: +1.0% (from research)\")\n",
    "print(\"- Simple ensemble: +0.7% (from research)\") \n",
    "print(\"- Title refinement: +0.5% (from research)\")\n",
    "print(\"- Age optimization: +0.3% (from research)\")\n",
    "print(\"- Advanced stacking: +1.5% (from research)\")\n",
    "total_potential = current_score + 1.0 + 0.7 + 0.5 + 0.3 + 1.5\n",
    "print(f\"\\nTheoretical max: {total_potential:.4f}%\")\n",
    "\n",
    "# Check if we have any writeups or kernels to reference\n",
    "print(\"\\n=== RESEARCH RESOURCES ===\")\n",
    "research_dir = '/home/code/research'\n",
    "import os\n",
    "if os.path.exists(research_dir):\n",
    "    for item in os.listdir(research_dir):\n",
    "        print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c72b73",
   "metadata": {},
   "source": [
    "## Decision: Submit or Continue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43902164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze whether we should submit now or continue experimenting\n",
    "print(\"=== SUBMISSION DECISION ANALYSIS ===\")\n",
    "\n",
    "# Criteria for submission:\n",
    "# 1. CV improved significantly vs last submission\n",
    "# 2. Need LB feedback for calibration\n",
    "# 3. Pipeline is trustworthy\n",
    "# 4. Have a competitive model\n",
    "\n",
    "last_submission_cv = None\n",
    "if session_state['submissions']:\n",
    "    last_submission_cv = session_state['submissions'][-1]['cv_score']\n",
    "\n",
    "current_cv = session_state['experiments'][-1]['score']\n",
    "\n",
    "print(f\"Current CV: {current_cv:.4f}%\")\n",
    "if last_submission_cv:\n",
    "    print(f\"Last Submission CV: {last_submission_cv:.4f}%\")\n",
    "    improvement = current_cv - last_submission_cv\n",
    "    print(f\"Improvement: {improvement:.4f}%\")\n",
    "else:\n",
    "    print(\"No previous submissions - this would be first submission\")\n",
    "    improvement = float('inf')\n",
    "\n",
    "# Check if we meet submission criteria\n",
    "print(\"\\n=== SUBMISSION CRITERIA CHECK ===\")\n",
    "criteria_met = []\n",
    "\n",
    "# Criterion 1: Significant improvement or first submission\n",
    "if improvement > 0.001 or last_submission_cv is None:\n",
    "    criteria_met.append(\"✓ Significant improvement or first submission\")\n",
    "else:\n",
    "    criteria_met.append(\"✗ No significant improvement\")\n",
    "\n",
    "# Criterion 2: Trustworthy pipeline (leakage fixed)\n",
    "if \"TRUSTWORTHY\" in session_state['feedback_history'][-1]['feedback']:\n",
    "    criteria_met.append(\"✓ Trustworthy pipeline (leakage fixed)\")\n",
    "else:\n",
    "    criteria_met.append(\"✗ Pipeline concerns remain\")\n",
    "\n",
    "# Criterion 3: Competitive score\n",
    "if current_cv > 80.0:  # Titanic competitive threshold\n",
    "    criteria_met.append(\"✓ Competitive CV score (>80%)\")\n",
    "else:\n",
    "    criteria_met.append(\"✗ Score not competitive\")\n",
    "\n",
    "# Criterion 4: Have submissions remaining\n",
    "if session_state['remaining_submissions'] > 0:\n",
    "    criteria_met.append(f\"✓ Submissions available ({session_state['remaining_submissions']}/10)\")\n",
    "else:\n",
    "    criteria_met.append(\"✗ No submissions remaining\")\n",
    "\n",
    "for criterion in criteria_met:\n",
    "    print(criterion)\n",
    "\n",
    "# Decision\n",
    "should_submit = all(\"✓\" in c for c in criteria_met[:3]) and session_state['remaining_submissions'] > 0\n",
    "print(f\"\\n=== DECISION ===\")\n",
    "if should_submit:\n",
    "    print(\"RECOMMENDATION: SUBMIT NOW\")\n",
    "    print(\"Reason: Trustworthy pipeline, competitive score, need LB calibration\")\n",
    "else:\n",
    "    print(\"RECOMMENDATION: CONTINUE EXPERIMENTING\")\n",
    "    print(\"Reason: Address evaluator concerns first\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
