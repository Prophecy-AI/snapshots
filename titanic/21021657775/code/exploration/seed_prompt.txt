## Competition Overview
This is a binary classification problem predicting passenger survival on the Titanic. The dataset is small (891 train, 418 test samples) with mixed feature types requiring careful preprocessing and feature engineering.

**Reference notebooks for data characteristics:**
- See exploration notebooks for full EDA: feature distributions, missing values, correlations, and target distribution

## Data Understanding & Preprocessing

### Critical Data Characteristics
- **Small dataset**: 891 training samples - requires careful handling to avoid overfitting
- **Missing data**: Age (~20%), Cabin (~77%), Embarked (2 entries) - multiple imputation strategies needed
- **Mixed data types**: Numerical (Age, Fare, SibSp, Parch), Categorical (Sex, Embarked, Pclass), High-cardinality text (Name, Ticket, Cabin)
- **Class imbalance**: ~38% survival rate (not severely imbalanced)

### Essential Preprocessing Steps
1. **Missing Value Imputation**:
   - Age: Use median, mean, or predictive imputation (median most robust for small samples)
   - Embarked: Fill with mode (most frequent port)
   - Cabin: Too many missing values - extract deck letter or create "HasCabin" binary feature
   - Fare: Only 1 missing value in test set - use median or Pclass-based imputation

2. **Categorical Encoding**:
   - Sex: Binary encoding (female=1, male=0) - critical feature with strong predictive power
   - Embarked: One-hot encoding or label encoding with 3 categories (S, C, Q)
   - Pclass: Treat as ordinal (1=1st, 2=2nd, 3=3rd) or one-hot encode

3. **Feature Scaling**:
   - Age and Fare: Apply log transformation or StandardScaler for neural networks
   - Tree-based models (XGBoost, Random Forest) don't require scaling

## Feature Engineering (Critical for High Performance)

### Proven High-Impact Features
1. **Title Extraction from Name**: 
   - Extract Mr, Mrs, Miss, Master, Dr, Rev, etc.
   - Group rare titles into "Rare" category
   - Creates powerful socioeconomic proxy feature

2. **Family Size**: 
   - SibSp + Parch + 1 (including passenger)
   - Create discrete categories: Single (1), Small (2-4), Large (5+)
   - Large families had lower survival rates

3. **IsAlone Feature**:
   - Binary indicator: 1 if traveling alone (FamilySize=1), 0 otherwise
   - Strong predictive power - solo travelers had lower survival

4. **Age Bins/Bands**:
   - Create categories: Infant (0-4), Child (5-12), Teen (13-18), Adult (19-59), Senior (60+)
   - Or use quantile-based bins
   - Children had higher survival rates

5. **Fare Bins**:
   - Quartile or quantile-based bands
   - Proxy for socioeconomic status

6. **Cabin Deck**:
   - Extract first letter from Cabin (A, B, C, D, E, F, G, T)
   - Missing values as "Unknown" or "M"
   - Higher decks (A-C) closer to lifeboats

7. **Ticket Frequency**:
   - Count how many passengers share the same ticket
   - Groups traveling together

### Feature Selection Strategy
- Start with 10-15 engineered features
- Use correlation analysis to remove highly correlated features
- Apply feature importance from tree models to select top features
- Avoid feature explosion (keep total features < 25-30 for this small dataset)

## Modeling Strategy

### Baseline Models (Establish Performance Floor)
1. **Logistic Regression**: Simple, interpretable baseline
2. **Random Guess**: 50% accuracy (binary classification baseline)
3. **Gender-only Model**: ~79% accuracy (women survived at much higher rates)

### Primary Model Candidates (Proven on Titanic)
1. **Gradient Boosting (XGBoost/LightGBM)**:
   - Top performer for this dataset
   - Handles mixed data types well
   - Robust to overfitting with proper regularization
   - Key hyperparameters: n_estimators (100-500), max_depth (3-5), learning_rate (0.01-0.1)

2. **Random Forest**:
   - Excellent default performance
   - Provides feature importance
   - Less prone to overfitting than single decision trees
   - Key hyperparameters: n_estimators (100-200), max_depth (5-10), min_samples_split

3. **Support Vector Machines (SVM)**:
   - Strong performance with proper kernel selection
   - Requires careful feature scaling
   - Slower training on larger datasets

4. **Neural Networks (for completeness)**:
   - Generally overkill for this small dataset
   - Requires extensive tuning
   - Use only if ensembling multiple architectures

### Model Configuration Guidelines
- **Cross-validation**: Use StratifiedKFold (5-10 folds) to preserve class distribution
- **Evaluation metric**: Accuracy (percentage of correct predictions)
- **Validation strategy**: Holdout 20-30% of training data for final validation
- **Random seed**: Always set for reproducibility (critical for small datasets)

## Ensembling & Stacking

### Proven Ensemble Strategies
1. **Voting Ensemble (Soft/Hard)**:
   - Combine predictions from 3-5 diverse models
   - Use soft voting for probability-based predictions
   - Typical combination: XGBoost + Random Forest + Logistic Regression

2. **Stacking**:
   - Level-1: Train multiple models (XGBoost, Random Forest, SVM, KNN)
   - Level-2: Meta-learner (Logistic Regression) combines predictions
   - Use cross-validation to generate meta-features (prevent overfitting)

3. **Weighted Averaging**:
   - Assign weights based on validation performance
   - Higher weight to better-performing models
   - Simple and effective for small datasets

### Ensemble Best Practices
- Ensure model diversity (different algorithms, different feature subsets)
- Limit to 3-5 base models (more models don't help with small data)
- Calibrate probabilities if using soft voting
- Validate ensemble on holdout set (not used for base model training)

## Hyperparameter Tuning

### Efficient Tuning Strategy
1. **Manual Tuning First**: Start with known good defaults from kernels
2. **Grid Search**: Systematic search over parameter combinations
3. **Random Search**: More efficient than grid search for high-dimensional spaces
4. **Bayesian Optimization**: Most efficient for limited evaluation budget

### Key Hyperparameters by Model
- **XGBoost**: n_estimators, max_depth, learning_rate, subsample, colsample_bytree
- **Random Forest**: n_estimators, max_depth, min_samples_split, min_samples_leaf
- **SVM**: C, kernel, gamma (if RBF)

### Tuning Budget
- Small dataset = fast training = can try more combinations
- Focus on 2-3 most impactful parameters per model
- Use 3-fold CV for speed during tuning, 5-fold for final evaluation

## Validation & Diagnostics

### Critical Checks
1. **Overfitting Detection**: 
   - Training accuracy >> Validation accuracy = overfitting
   - Use learning curves to diagnose
   - Apply regularization, reduce model complexity, or get more data

2. **Cross-Validation Consistency**:
   - Std dev across CV folds should be small (< 2-3%)
   - High variance indicates unstable model

3. **Feature Importance Analysis**:
   - Identify which engineered features provide most value
   - Use SHAP values or tree feature importance
   - Focus future engineering on related features

4. **Error Analysis**:
   - Analyze misclassified samples
   - Look for patterns in errors (specific passenger groups)
   - Use insights to guide feature engineering

## Submission Strategy

### Final Model Selection
1. Choose model with highest cross-validation mean accuracy
2. Ensure low std dev across folds (stable predictions)
3. Prefer simpler models if performance is similar (avoid overfitting)

### Prediction Generation
1. Train on full training set (after final validation)
2. Generate predictions on test set
3. Ensure submission format: PassengerId, Survived (0/1)
4. Verify exactly 418 predictions (no extra columns/rows)

### Performance Targets
- **Good**: 80-82% accuracy
- **Very Good**: 83-85% accuracy  
- **Excellent**: 86-88% accuracy
- **Top Tier**: 89%+ accuracy (requires careful feature engineering and ensembling)

## Common Pitfalls to Avoid

1. **Data Leakage**: Never use test set information for training/preprocessing decisions
2. **Overfitting**: Don't chase perfect training accuracy on this small dataset
3. **Feature Overload**: Too many features relative to samples causes overfitting
4. **Ignoring Domain Knowledge**: Titanic has well-known patterns (women/children first, class-based survival)
5. **Not Setting Random Seeds**: Results won't be reproducible
6. **Poor Missing Value Strategy**: Don't drop rows with missing values (lose ~20% of data)

## Implementation Roadmap

1. **Phase 1 (Baseline)**: Load data, simple preprocessing, train basic models (Logistic Regression, Random Forest)
2. **Phase 2 (Feature Engineering)**: Create Title, FamilySize, Age bands, Fare bands, Cabin features
3. **Phase 3 (Model Tuning)**: Tune XGBoost and Random Forest hyperparameters
4. **Phase 4 (Ensembling)**: Build voting ensemble or stacking model
5. **Phase 5 (Validation)**: Final cross-validation, error analysis, submission generation

Focus on Phase 2 (feature engineering) for maximum impact - this is where top solutions differentiate themselves.