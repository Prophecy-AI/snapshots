{
  "query": "What techniques achieve highest accuracy on Titanic Kaggle competition above 80%?",
  "results": [
    {
      "title": "ANSWER",
      "text": "As of\u202fMarch\u202f2025, extensive feature engineering (e.g., imputing ages, creating family\u2011size and title features, encoding categorical variables) combined with powerful ensemble models such as Random\u202fForest, Gradient\u202fBoosting or XGBoost and careful hyper\u2011parameter tuning via cross\u2011validation can push Titanic competition scores above\u202f80% accuracy ([Dataquest](https://www.dataquest.io/blog/kaggle-fundamentals), [Medium](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb)).",
      "url": ""
    },
    {
      "title": "Kaggle Fundamentals: The Titanic Competition - Dataquest",
      "text": "Kaggle Fundamentals: The Titanic Competition &#8211; Dataquest\n[](https://www.dataquest.io)\n* [Profile](https://app.dataquest.io/profile)\n* [Account](https://app.dataquest.io/settings/account)\n* [Subscription](https://app.dataquest.io/settings/subscription)\n* [Teams](https://www.dataquest.io/for-business/)\n* [Help](https://support.dataquest.io/en)\n* [Logout](https://app.dataquest.io/logout)\n[](https://www.dataquest.io/#)\nNew Year Launchpad: Lift Off Your Data Career with 57% Off Lifetime Plan.\n[Save Now](https://app.dataquest.io/payment?plan_interval=lifetime)\nMarch 25, 2025\n# Kaggle Fundamentals: The Titanic Competition\n[Kaggle](https://www.kaggle.com/)is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins the competition if it's the most accurate on a particular data set. Kaggle is a fun way to practice your machine learning skills.\nIn this tutorial we'll learn learn how to:\n* Approach a Kaggle competition\n* Explore the competition data and learn about the competition topic\n* Prepare data for machine learning\n* Train a model\n* Measure the accuracy of your model\n* Prepare and make your first Kaggle submission\nThis tutorial presumes you have an understanding of Python and the pandas library. If you need to learn about these, we recommend our[pandas tutorial](https://www.dataquest.io/blog/pandas-python-tutorial/)blog post.\n## The Titanic competition\nKaggle has created a number of competitions designed for beginners. The most popular of these competitions, and the one we'll be looking at, is about predicting which passengers survived the[sinking of the Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic).\nIn this[Titanic ML competition](https://www.kaggle.com/competitions/titanic), we have data about passengers onboard the Titanic, and we'll see if we can use that information to predict whether those people survived or not. Before we start looking at this specific competition, let's take a moment to understand how Kaggle competitions work.\nEach Kaggle competition has two key data files that you will work with - a**training**set and a**testing**set.\nThe training set contains data we can use to train our model. It has a number of feature columns which contain various descriptive data, as well as a column of the target values we are trying to predict: in this case,`Survival`.\nThe testing set contains all of the same feature columns, but is missing the target value column. Additionally, the testing set usually has fewer observations (rows) than the training set.\nThis is useful because we want as much data as we can to train our model on. Once we have trained our model on the training set, we will use that model to make predictions on the data from the testing set, and submit those predictions to Kaggle.\nIn this competition, the two files are named`test.csv`and`train.csv`. We'll start by using[`pandas.read\\_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)to read both files and then inspect their size.\n```\n`import pandas as pd\ntest = pd.read\\_csv(\"test.csv\")\ntrain = pd.read\\_csv(\"train.csv\")\nprint(\"Dimensions of train: {}\".format(train.shape))\nprint(\"Dimensions of test: {}\".format(test.shape))`\n```\n```\n`Dimensions of train: (891, 12)\nDimensions of test: (418, 11)`\n```\n## Exploring the data\nThe files we just opened are available on[the data page for the Titanic competition on Kaggle](https://www.kaggle.com/competitions/titanic/data). That page also has a**data dictionary**, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n* `PassengerID`\u2014 A column added by Kaggle to identify each row and make submissions easier\n* `Survived`\u2014 Whether the passenger survived or not and the value we are predicting*(0=No, 1=Yes)*\n* `Pclass`\u2014 The class of the ticket the passenger purchased*(1=1st, 2=2nd, 3=3rd)*\n* `Sex`\u2014 The passenger's sex\n* `Age`\u2014 The passenger's age in years\n* `SibSp`\u2014 The number of siblings or spouses the passenger had aboard the Titanic\n* `Parch`\u2014 The number of parents or children the passenger had aboard the Titanic\n* `Ticket`\u2014 The passenger's ticket number\n* `Fare`\u2014 The fare the passenger paid\n* `Cabin`\u2014 The passenger's cabin number\n* `Embarked`\u2014 The port where the passenger embarked*(C=Cherbourg, Q=Queenstown, S=Southampton)*\nThe data page on Kaggle has some additional notes about some of the columns. It's always worth exploring this in detail to get a full understanding of the data.\nLet's take a look at the first few rows of the`train`dataframe.\n```\n`train.head()`\n```\n||PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n0|1|0|3|Braund, Mr. Owen Harris|male|22.0|1|0|A/5 21171|7.2500|NaN|S|\n1|2|1|1|Cumings, Mrs. John Bradley (Florence Briggs Th...|female|38.0|1|0|PC 17599|71.2833|C85|C|\n2|3|1|3|Heikkinen, Miss. Laina|female|26.0|0|0|STON/O2. 3101282|7.9250|NaN|S|\n3|4|1|1|Futrelle, Mrs. Jacques Heath (Lily May Peel)|female|35.0|1|0|113803|53.1000|C123|S|\n4|5|0|3|Allen, Mr. William Henry|male|35.0|0|0|373450|8.0500|NaN|S|\nThe type of machine learning we will be doing is called**classification**, because when we make predictions we are classifying each passenger as 'survived' or not. More specifically, we are performing**binary classification**, which means that there are only two different states we are classifying.\nIn any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie[Titanic](https://en.wikipedia.org/wiki/Titanic_(1997_film))would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\nThis indicates that`Age`,`Sex`, and`PClass`may be good predictors of survival. We'll start by exploring`Sex`and`Pclass`by visualizing the data.\nBecause the`Survived`column contains`0`if the passenger did not survive and`1`if they did, we can segment our data by sex and calculate the mean of this column. We can use`DataFrame.pivot\\_table()`to easily do this:\n```\n`import matplotlib.pyplot as plt\n%matplotlib inline\nsex\\_pivot = train.pivot\\_table(index=\"Sex\",values=\"Survived\")\nsex\\_pivot.plot.bar()\nplt.show()`\n```\nWe can immediately see that females survived in much higher proportions than males did. Let's do the same with the`Pclass`column.\n```\n`class\\_pivot = train.pivot\\_table(index=\"Pclass\",values=\"Survived\")\nclass\\_pivot.plot.bar()\nplt.show()`\n```\n## Exploring and converting the age column\nThe`Sex`and`PClass`columns are what we call**categorical**features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).\nLet's take a look at the`Age`column using[`Series.describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html).\n```\n`train[\"Age\"].describe()`\n```\n```\n`count 714.000000\nmean 29.699118\nstd 14.526497\nmin 0.420000\n25% 20.125000\n50% 28.000000\n75% 38.000000\nmax 80.000000\nName: Age, dtype: float64`\n```\nThe`Age`column contains numbers ranging from`0.42`to`80.0`(If you look at Kaggle's data page, it informs us that`Age`is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 814 rows we discovered that the`train`data set had earlier which indicates we have some missing values.\nAll of this means that the`Age`column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at distribution of values in a continuous numerical set is to use histograms. We can create two histograms to compare visuall...",
      "url": "https://www.dataquest.io/blog/kaggle-fundamentals"
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "How to win your first Kaggle competition? - dataroots",
      "text": "How to win your first Kaggle competition?\n[![symbol](https://dataroots.io/_next/static/media/symbol-rainbow.66f0e23b.svg)](https://dataroots.io/)\n![dataroots hero](https://dataroots.io/_next/static/media/glow-bottom-green.eb20c0f6.svg)\n# How to win your first Kaggle competition?\n[Get In Touch-&gt;](https://dataroots.io/contact-us)\n[Careers](https://dataroots.io/careers)\n[DNAOur DNA](https://dataroots.io/our-dna)\n[file-articleBlog](https://dataroots.io/blog)\n[podcastPodcast](https://dataroots.io/datatopics)\nByAdrien Debray, Johannes Lootens\nYou want to get started with Kaggle competitions? You saw an interesting challenge or the big prize money but feel a bit lost about how to tackle the competition ?\nThis blog provides a broad overview of Kaggle competitions, guides you through the winning methodologies, and offers tips and tricks to help you tackle a Kaggle competition more effectively.\n## All you need to know about Kaggle competitions\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled-1.png)Kaggle competition overview page\n\ud83d\udca1Kaggle is a platform where data enthusiasts come together to explore and analyse datasets and participate in machine learning competitions. The platform is a fun and collaborative space that encourages learning, problem-solving, and innovation.\nWhile Kaggle has grown substantially over the last few years to a more all-round data science hub, the competitions were and remain Kaggle\u2019s raison d\u2019\u00eatre and come in all shapes and forms but can be divided into three main (albeit slightly arbitrary) categories: getting-started , community competitions, and cash prize competitions.\nFirstly there are the**getting-started competitions**, such as the[Titanic](https://www.kaggle.com/competitions/titanic?ref=dataroots.ghost.io)or[Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer?ref=dataroots.ghost.io)ones. These are meant more as a sandbox with a well-defined goal to allow newcomers to become familiar with ML concepts, libraries and the Kaggle ecosystem in a fun way. Together with the**community competitions**, where someone just had an idea for an interesting competition, these generally list \u201ckudos\u201d, \u201cswag\u201d or \u201cknowledge\u201d as their prizes with the reasoning that the journey and knowledge gained on the way are more important than the destination.\nWhat actually tends to attract people to Kaggle are the**competitions with cash prizes**. These \u00a0attach prize money to top leaderboard spots and are usually set up by companies or research institutes that actually want a problem solved and would like the broader audience to take a shot at this. With prizes ranging from a few 100\u2019s to multiple 100 000\u2019s of dollars, these attract some of the best in their respective fields, which makes competing challenging but very rewarding.\nEvery single one of those competitions is defined by a**dataset**and an**evaluation score**. Here the labels from the dataset define the problem to be solved and the evaluation score is the single objective measure that indicates how well a solution solves this problem (and that will be used to rank the solutions for the leaderboard).\nWhile the********************train set********************is publicly available, the data used to evaluate solutions is not and is usually divided into two parts. First there is the**public leaderboard test set**which is used to calculate leaderboard scores while the competition is still going on. This can be used by teams to check how well their solution performs on unseen data and verify their validation strategy. Secondly there is the**private leaderboard test set.**This is used to calculate the private leaderboard scores, which decides ones actual final place, and these are only disclosed after the competition ended.\nThis not only prevents fine-tuning on the test data but also keeps things exiting since leaderboards can completely change last minute if people did (either intentionally or not) do this anyway. The resulting shuffle usually makes for some interesting drama where long-reigning champions fall from grace and unnoticed underdogs, who kept best practices in mind, suddenly rise to the top.\n### Notebooks\nTo compete one can either work on private resources (such as a local machine or cloud-hosted vm or compute instance) or use a Kaggle notebook.\nPrivate resources do have some advantages since one has full freedom about the environment, packages, etc. Especially if you want to use packages such as MLFlow or Tensorboard, which do not work in the Kaggle notebooks. Next to this, not having a limit to running times, memory and disk space can be quite convenient.\nThe Kaggle notebooks are Jupyter notebooks running on a maintained and standardised environment, hosted by Kaggle and they come with unlimited, free CPU time (with session limits of 12h) and 30h of GPU time (per user) each week for most of your parallel computing needs. This ensures that everybody who wants to compete can compete and is not limited by their hardware, which makes the competitions as democratic as possible. Additionally, you can easily import Kaggle datasets in a few seconds, which is especially convenient for the larger ones which can easily exceed 100s of GBs. Finally they are also the way to submit a solution to the competition. The submission notebook will have to read in a (private )test set and generate predictions on this set that will be used to calculate the leaderboard score. So even if you work on your own resources for training and fine-tuning, you will have to convert your code to a Kaggle notebook eventually.\n## How to take the W in a Kaggle competition ?\nIt is a matter of approach taken and how many of your ideas you could try out to finish high on the leaderboard !\nWe participated in the[Vesuvius Challenge - Ink Detection](https://www.kaggle.com/c/vesuvius-challenge-ink-detection?ref=dataroots.ghost.io)competition. While we did not win any prizes, some of the top Kaggle competitors shared their solutions after the competition ended. The methodology used by the winners seems to be more or less the same across the board. Interested to know them ? Let\u2019s break them down in few simple steps !\n### 1. Have a good understanding of the competition and how to tackle the problem\nAs the people who are organising these competitions often already spend a lot of time finding a good solution themselves, a ton of material might be already available. We would recommend you to:\n* Read the competition overview and linked resources thoroughly\n* Get familiar with the data. Look at samples, plot statistics, all the usual EDA\n* Check existing literature on approaches that were tried/succeeded in solving this or similar problems### 2. Get inspired by other participants\u2019 work to get started\nTo earn Kaggle medals or because they are genuinely nice, some competitors share their knowledge through making notebooks and datasets public or sharing findings and insights in discussions to get \u201cupvotes\u201d. We recommend reading the ones that got a lot of upvotes. This step is really a must as there are so much things to try out to improve your result it is impossible to cover everything with your team.\n![](https://dataroots.ghost.io/content/images/2023/06/Untitled--1--1.png)Go in the \"Code\" section of the competition page. You can select notebooks with the most votes and check the number of upvotes on the right side of the notebook\nBased on your readings, choose a clear and simple notebook with a decent LB score as**baseline**. Try to come up with a strategy on how to improve this baseline based on your thoughts and what you read from the shared work.\n### 3. Improve your model in an efficient way\nIn this phase, you will experiment a lot in the hopes of improving your LB. The goal here is to maximise the number of experiments you will try in a limited amount of time !\n##### Create datasets for intermediate results / preprocessed data\nSaved preprocessed datasets and trained models will make your results comparis...",
      "url": "https://dataroots.io/blog/how-to-win-your-first-kaggle-competition"
    },
    {
      "title": "Titanic Model with 90% accuracy | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=1e4ba90eba1c8273e4fe:1:11427)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/vinothan/titanic-model-with-90-accuracy"
    },
    {
      "title": "The Prediction and Feature Importance Investigation in Titanic ...",
      "text": "The Prediction and Feature Importance Investigation in Titanic \nSurvival Prediction \nChutong Huang a\nBeijing Luhe High School International Academy, Beijing, China \nKeywords: Artificial Intelligence, Random Forest, Feature Importance. \nAbstract: Predicting the survival of Titanic passengers is one of the topics scientists are focusing on. This paper explores \nthe use of the Random Forest (RF) algorithm on a Titanic dataset and analyses the key features that influence \nthe predictions. The RF algorithm is applied to a processed dataset. Feature importance scores are returned \nfor each feature to demonstrate how much it is related to the survival prediction, and the scores are then \nanalyzed in their historical context. Age, fare and sex were found to be the three most significant features in \npredicting survival. Age is significant as its correlation to survivability, the children are determined to live \nwhile the elderly are unable to survive. Fare is a crucial attribute since it is correlated with passenger class, \nmeaning that those paying more are given better information and location to survive. Sex is important because \nwomen and children are given priority to survival, while men don\u2019t have that chance. The application of \nRandom Forests shows how well Artificial Intelligence (AI) algorithms can predict problems and spot \nsignificant patterns in complex data sets. And the analysis could have useful implications for improving \npredictive models in other areas where attributes are crucial. \n1 INTRODUCTION \nOn April 15, 1912, the RMS Titanic sideswiped an \niceberg on its first voyage, causing over 1,500 of the \n2,240 passengers and staff members on board to \nperish in the disaster. This notorious disaster soon \nbecame a warning and appears in many films, articles, \nand novels, warning people the danger of nature \n(Titanic History, 2024). Enhancing passenger \nsurvival in such disasters is an issue that needs to be \ntaken seriously, and Artificial Intelligence, as an \nemerging technology with strong feature extraction \nand prediction capabilities, can be considered in \nconjunction with this task. \nAI has evolved rapidly, with significant progress \nin machine learning, deep learning, and data \nanalytics. It utilizes sophisticated algorithms like \nlogistic regression, random forests, and neural \nnetworks to analyze complex data patterns, which is \nbeneficial in forecasting outcomes and improving \nchoices. These advancements have enabled more \naccurate predictions and enhanced decision-making \nacross various fields, such as medical science. Choi et \nal. constructed a Recurrent Neural Network (RNN)-\na https://orcid.org/0009-0003-2175-9681 \nbased model that predicts future events of patients \n(Choi, 2016). Wang et al. utilize a RNN model for the \nprediction of the future statues of Alzheimer's \nDisease for patients (Wang, 2018). Among the many \nprediction tasks, one important direction is the \nprediction of classification problems like survival of \na person. Hsieh et al. illustrated a model of Fuzzy \nHyper-Rectangular Composite Neural Network that \npredicts the survival through the first 24 hours \nphysical data of patients (Hsieh, 2014). Pradeep et al. \napplied machine learning algorithms include Naive \nBayes, classification trees, and Support Vector \nMachine (SVM) to the information of lung cancer \npatients and predict their survivability rate (Pradeep, \n2018). Kakde et al. determined the impact of each \nfeature to the survival rate and compared algorithms \nbetween SVM, decision tree, random forest, and \nlogistic regression (LR). They found that SVM as \nwell as logistic regression perform nearly the best, \nand there was high influence of age on survival while \nother features like passenger class, age, fare and \n\"sibsp\"(siblings and spouses) all have influences as \nwell (Kakde, 2018). Nair et al. analyzed the \ncorrelation between factors of passenger samples and \n60\nHuang and C.\nThe Prediction and Feature Importance Investigation in Titanic Survival Prediction.\nDOI: 10.5220/0013487400004619\nIn Proceedings of the 2nd International Conference on Data Analysis and Machine Learning (DAML 2024), pages 60-63\nISBN: 978-989-758-754-2\nCopyright \u00a9 2025 by Paper published under CC license (CC BY-NC-ND 4.0)\nthe survivability of the passengers. They suggested \nthat LR perform the best with the lowest false \ndiscovery rate and the highest accuracy. In their \nmodel of logistic regression, it tells them that the top5 \ncorrelated features are \"Pclass\"(Passenger class), \nsibsp, age, children, and sex (Singh, 2017). The \neffectiveness of AI methods has been demonstrated \non many domain tasks, so this paper intends to \nconsider the use of AI algorithms in predicting \nsurvival of people in Titanic disaster, but unlike \nprevious studies on the subject, this paper's focus is \nmore on the influence of passengers' features. \nIn this paper, based on the Kaggle dataset, the \nrandom forest algorithm was used for prediction and \nthe feature importance of each feature was compared, \nas well as the correlation between features and \nsurvival was analyzed. \n2 METHOD \n2.1 Data Preparation \nIn this study, a dataset from Kaggle is used (Kaggle, \n2017). It consists of 1309 samples of passengers on \nthe Titanic, each of these passengers are marked by 8 \nfeatures such as sex, age and fare. In terms of data \npreprocessing, it is divided into two parts. First, \nmissing values are handled by imputation using \nstatistical measures, and then categorical variables are \nconverted to numerical formats using label encoding. \n2.2 Machine Learning-based \nPrediction \n2.2.1 Introduction of Machine Learning \nWorkflow \nMachine learning generally involves several key \nsteps to create an effective predictive model. First, \ndata collection gathers relevant information from \nvarious sources to ensure a comprehensive dataset. \nNext, data preprocessing cleans and transforms this \ndata by fitting missing values, transforming \ncategorical variables, and then scaling features to \nprepare it for analysis. Feature selection follows, \nidentifying and retaining the most important variables \nthat contribute to the model\u2019s performance. Model \nselection involves choosing the appropriate algorithm \nbased on the features included in the dataset and the \npractical problem. It studies from the data to identify \npatterns during training and make predictions. \nPerformance evaluation assesses the model's \naccuracy and effectiveness using separate test data. \nFinally, model tuning adjusts the settings or \nparameters of the algorithm to enhance its \nperformance, ensuring the model is well-suited to the \nspecific problem and data. \n2.2.2 Random Forest \nRandom Forest (RF) is an accurate and effective \nsupervised machine learning method which combines \nvarious decision trees to form a \"forest.\" It is \napplicable to situations involving both regression and \nclassification. An additional kind of method for data \nclassification is a decision tree. It resembles a \nflowchart that clearly illustrates the process of \nprogressing choice. It starts at one beginning tree and \nproduces two or more branches in which each tree \nbranch giving a distinct set of possible outcomes. The \nRF model can achieve high prediction accuracy by \ncombining multiple decision trees. The principle \nbehind this is that several unrelated models of \ndecision tree produce a noticeable improvement when \nused together. In detail, each 'tree' in the 'forest' casts \na 'vote' for a problem, the forest integrates all the \nvotes and chooses the one with the majority of those \nvotes. RF takes different votes from multiple trees, \nwhich helps to increase the robustness and reduce the \noverfitting problem of the algorithm, which is its \ngreatest merit (Careerfoundry, 2023). \nIn this study, Random Forest is used in Python, \nscikit-learn (sklearn) provides a random forest \nclassifier library. After applying Random Forest to \nthe dataset, the 1309 survival predictions are fitted \n...",
      "url": "https://www.scitepress.org/Papers/2024/134874/134874.pdf"
    },
    {
      "title": "How to further improve the kaggle titanic submission accuracy?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How to further improve the kaggle titanic submission accuracy?](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 10 months ago\n\nModified [7 years, 10 months ago](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy?lastactivity)\n\nViewed\n7k times\n\n4\n\n$\\\\begingroup$\n\nI am working on the Titanic dataset. So far my submission has 0.78 score using soft majority voting with logistic regression and random forest. As for the features, I used Pclass, Age, SibSp, Parch, Fare, Sex, Embarked.\n\nMy question is how to further boost the score for this classification problem?\n\nOne thing I tried is to add more classifiers for the majority voting, but it does not help, it even worthens the result. How do I understand this worthening effect?\n\nThanks for your insight.\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [classification](https://datascience.stackexchange.com/questions/tagged/classification)\n- [beginner](https://datascience.stackexchange.com/questions/tagged/beginner)\n\n[Share](https://datascience.stackexchange.com/q/13104)\n\n[Improve this question](https://datascience.stackexchange.com/posts/13104/edit)\n\nFollow\n\n[edited Jul 30, 2016 at 14:03](https://datascience.stackexchange.com/posts/13104/revisions)\n\nnos\n\nasked Jul 30, 2016 at 13:15\n\n[![nos's user avatar](https://www.gravatar.com/avatar/4ba32c06ed067564a6c6512de5217657?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/21955/nos)\n\n[nos](https://datascience.stackexchange.com/users/21955/nos) nos\n\n15311 silver badge66 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n5\n\n$\\\\begingroup$\n\nbig question.\n\nOk so here's a few things I'd look at if I was you..\n\n1. Have you tried any feature engineering ?(it sounds like you've just used the features in the training set but I can't be 100%)\n2. Random Forests should do pretty well, but maybe try xgboost too? It's quite good at everything on Kaggle. SVM's could be worth a go also if you're thinking of stacking/ensembling.\n3. Check out some of the tutorials around this competition. There's hundreds of them and most of them are great.\n\n**Links:**\n\n[R #1 (my favourite)](http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/)\n\n[R #2](http://www.numbrcrunch.com/blog/the-good-ol-titanic-kaggle-competition-pt-1)\n\n[Python #1](http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/)\n\n[Python #2](https://github.com/agconti/kaggle-titanic)\n\n...Hopefully this helps\n\n[Share](https://datascience.stackexchange.com/a/13139)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/13139/edit)\n\nFollow\n\nanswered Aug 2, 2016 at 2:52\n\n[![plumbus_bouquet's user avatar](https://www.gravatar.com/avatar/031741853441d7c93e9a50ffbabffb86?s=64&d=identicon&r=PG)](https://datascience.stackexchange.com/users/21702/plumbus-bouquet)\n\n[plumbus\\_bouquet](https://datascience.stackexchange.com/users/21702/plumbus-bouquet) plumbus\\_bouquet\n\n34011 silver badge44 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n3\n\n$\\\\begingroup$\n\nOkay, I am currently at 0.81340 in the competiton. And I will just clear out certain things. I would suggest that you try feature engineering before you go for ensemble methods. There are actually quite a decent tutorials as mentioned before.\nOne can actually score upto at least 0.82 just relying on feature engineering and a ten-fold cross validated RandomForest.\nCertain things for you to ponder at:\n\n- Look at the Age, what other information does it give you.\n- Do SibSp and Parch actually represent different things?\n- Can you get something out of the Name of the passengers?\n\nAll the Best.\n\nCheers.\n\n[Share](https://datascience.stackexchange.com/a/13223)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/13223/edit)\n\nFollow\n\nanswered Aug 5, 2016 at 7:35\n\n[![Himanshu Rai's user avatar](https://graph.facebook.com/10205672827392400/picture?type=large)](https://datascience.stackexchange.com/users/21608/himanshu-rai)\n\n[Himanshu Rai](https://datascience.stackexchange.com/users/21608/himanshu-rai) Himanshu Rai\n\n1,8481212 silver badges1010 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f13104%2fhow-to-further-improve-the-kaggle-titanic-submission-accuracy%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning) - [classification](https://datascience.stackexchange.com/questions/tagged/classification) - [beginner](https://datascience.stackexchange.com/questions/tagged/beginner)   or [ask your own question](https://datascience.stackexchange.com/questions/ask).\n\nHappy 10th Anniversary!\n\n[Read more](https://datascience.meta.stackexchange.com/q/2671/145786)\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Linked\n\n[0](https://datascience.stackexchange.com/q/57708) [What should be the criteria to select features using correlation factors between features?](https://datascience.stackexchange.com/questions/57708/what-should-be-the-criteria-to-select-features-using-correlation-factors-between?noredirect=1)\n\n#### Related\n\n[1](https://datascience.stackexchange.com/q/5119) [Kaggle Titanic Survival Table an example of Naive Bayes?](https://datascience.stackexchange.com/questions/5119/kaggle-titanic-survival-table-an-example-of-naive-bayes)\n\n[6](https://datascience.stackexchange.com/q/8339) [Voting combined results from different classifiers gave bad accuracy](https://datascience.stackexchange.com/questions/8339/voting-combined-results-from-different-classifiers-gave-bad-accuracy)\n\n[1](https://datascience.stackexchange.com/q/16349) [How to improve accuracy further for forest cover prediction](https://datascience.stackexchange.com/questions/16349/how-to-improve-accuracy-further-for-forest-cover-prediction)\n\n[3](https://datascience.stackexchange.com/q/65196) [Kaggle Titanic submission score is higher than local accuracy score](https://datascience.stackexchange.com/questions/65196/kaggle-titanic-submission-score-is-higher-than-local-accuracy-score)\n\n[1](https://datascience.stackexchange.com/q/69193) [Feature Engineering - correlation with binary outcome - Titanic Dataset - Ticket feature](https://datascience.stackexchange.com/questions/69193...",
      "url": "https://datascience.stackexchange.com/questions/13104/how-to-further-improve-the-kaggle-titanic-submission-accuracy"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    },
    {
      "title": "GitHub - mrankitgupta/titanic-survival-prediction-93-xgboost: Titanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[mrankitgupta](https://github.com/mrankitgupta)/ **[titanic-survival-prediction-93-xgboost](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost) You must be signed in to change notification settings\n- [Fork\\\n1](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n- [Star\\\n1](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n\n\nTitanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.\n\n### License\n\n[MIT license](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE)\n\n[1\\\nstar](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/stargazers) [1\\\nfork](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/forks) [Branches](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/branches) [Tags](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/tags) [Activity](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/activity)\n\n[Star](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost)\n\n[Notifications](https://github.com/login?return_to=%2Fmrankitgupta%2Ftitanic-survival-prediction-93-xgboost) You must be signed in to change notification settings\n\n# mrankitgupta/titanic-survival-prediction-93-xgboost\n\nmain\n\n[Branches](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/branches) [Tags](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[6 Commits](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/commits/main/) |\n| [LICENSE](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE) | [LICENSE](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/LICENSE) |\n| [README.md](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/README.md) | [README.md](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/README.md) |\n| [titanic-survival-prediction-93-xgboost.ipynb](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/titanic-survival-prediction-93-xgboost.ipynb) | [titanic-survival-prediction-93-xgboost.ipynb](https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost/blob/main/titanic-survival-prediction-93-xgboost.ipynb) |\n| View all files |\n\n## Repository files navigation\n\n# Titanic - Machine Learning from Disaster \\| (Accuracy: 93%) XGBoost \ud83d\udef3\ufe0f\n\n### Titanic Survival Prediction: Machine Learning Model \ud83d\udef3\ufe0f\n\n### ML Models used: XGBoost, Random Forest, Logistic Regression\n\nIn this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model and Hyperparameter tunning.\n\n### **Prerequisites:**\n\n`Data Analyst Roadmap` \u231b\n\n`Python Lessons` \ud83d\udcd1\n\n`Python Libraries for Data Science` \ud83d\uddc2\ufe0f\n\n### **Overview**\n\n1. **Understand the shape of the data (Histograms, box plots, etc.)**\n\n2. **Data Cleaning**\n\n3. **Data Exploration**\n\n4. **Feature Engineering**\n\n5. **Data Preprocessing for Model**\n\n6. **Basic Model Building**\n\n7. **Model Tuning**\n\n8. **Ensemble Modle Building**\n\n9. **Results**\n\n\n### **About the Project** \ud83d\udef3\ufe0f\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\nKnowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## **Workflow stages**\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\n## Technologies used \u2699\ufe0f\n\n- [Python](https://github.com/mrankitgupta/Python-Lessons)\n\n- [Statistics](https://github.com/mrankitgupta/Statistics-for-Data-Science-using-Python)\n\n- [Jupyter](https://www.kaggle.com/mrankitgupta/titanic-survival-prediction-92-74-xgboost)\n\n\n##### Python Libraries :\n\n- [Pandas](https://github.com/mrankitgupta/Kaggle-Pandas-Solved-Exercises) \\| [NumPy](https://numpy.org/) \\| [Matplotlib](https://matplotlib.org/) \\| [Seaborn](https://seaborn.pydata.org) \\|\n\n- [Scikit-Learn](https://scikit-learn.org/) [\\|](https://scikit-learn.org/) [XGBoost](https://xgboost.readthedocs.io/en/stable/)\n\n\n## Project - Titanic Survival Prediction: Machine Learning Model \ud83d\udef3\ufe0f\n\n### **Kaggle Project Link:** **[Titanic Survival Prediction](https://www.kaggle.com/mrankitgupta/titanic-survival-prediction-93-xgboost)** \ud83d\udef3\ufe0f \ud83d\udd17\n\n### Datasets\n\nKaggle Titanic Datasets: `Titanic Train` & `Titanic Test`\n\n## Related Projects\u2753 \ud83d\udc68\u200d\ud83d\udcbb \ud83d\udef0\ufe0f\n\n`Spotify Data Analysis using Python` \ud83d\udcca\n\n`Data Analyst Roadmap` \u231b\n\n`Statistics for Data Science using Python` \ud83d\udcca\n\n`Sales Insights - Data Analysis using Tableau & SQL` \ud83d\udcca\n\n`Kaggle - Pandas Solved Exercises` \ud83d\udcca\n\n`Python Lessons` \ud83d\udcd1\n\n`Python Libraries for Data Science` \ud83d\uddc2\ufe0f\n\n### Liked my Contributions\u2753 Follow Me\ud83d\udc49 [Kaggle](https://www.kaggle.com/MrAnkitGupta) and [GitHub](https://github.com/MrAnkitGupta)\n\n[Nominate Me for GitHub Stars](https://stars.github.com/nominate/) \u2b50 \u2728\n\n## For any queries/doubts \ud83d\udd17 \ud83d\udc47\n\n### [Ankit Gupta](https://bio.link/AnkitGupta)\n\n[aggle](https://kaggle.com/mrankitgupta)\n\n## About\n\nTitanic Survival Prediction Project (93% Accuracy)\ud83d\udef3\ufe0f In this notebook, The goal is to correctly predict if someone survived the Titanic shipwreck using different Machine Learning Model & Hyperparameter tunning.\n\n### Topics\n\n[nlp](https://github.com/topics/nlp) [data-science](https://github.com/topics/data-science) [machine-learning](https://github.com/topics/machine-learning) [random-forest](https://github.com/topics/random-forest) [linear-regression](https://github.com/topics/linear-regression) [machine-learning-algorithms](https://github.com/topics/machine-learning-algorithms) [ml](https://github.com/topics/ml) [prediction](https://github.com/topi...",
      "url": "https://github.com/mrankitgupta/titanic-survival-prediction-93-xgboost"
    }
  ]
}