{
  "query": "How to achieve perfect 100% accuracy on Kaggle Titanic competition data leakage or special techniques?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on the community posts that discuss the Titanic \u201cleak\u201d and the only known way participants have obtained\u202f100\u202f% accuracy\u2014by exploiting data leakage or importing an external labeled copy of the test set\u2014here\u2019s a concise, step\u2011by\u2011step outline of the techniques that have been used\u202f([Anel Music, 2022](https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1); [David Recio, 2022](https://david-recio.com/2022/04/11/titanic-leak.html)):\n\n1. **Inspect the full dataset for hidden identifiers**  \n   - Load `train.csv` and `test.csv` together and look for columns that are unique per passenger (e.g., `Ticket`, `Cabin`, `Name`).  \n   - Check whether any of these identifiers appear in both splits; duplicated values indicate a leakage path.  \n\n2. **Create a \u201cleak\u2011mapping\u201d table**  \n   - Concatenate the two files, group by the suspected identifier, and compute the mode of the `Survived` label in the training portion.  \n   - Store this mapping (identifier \u2192 predicted label).  \n\n3. **Apply the mapping to the test set**  \n   - For each test row, if its identifier exists in the mapping, assign the corresponding label directly (no model needed).  \n   - This step alone can yield perfect predictions for any leaked rows.  \n\n4. **Supplement missing predictions with an external labeled copy**  \n   - Some top\u2011scorers have downloaded a separate copy of the original Kaggle test set that already includes the hidden `Survived` column (often shared unofficially).  \n   - Merge this external file with the official `test.csv` on a stable key (e.g., `PassengerId`) to obtain the true labels for *all* test rows. ([Anel Music, 2022](https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1))  \n\n5. **Train a trivial \u201cmemorization\u201d model (optional)**  \n   - If you prefer a model pipeline, fit a simple classifier (e.g., `LogisticRegression` or `XGBoost`) on the training data **including** the leakage column as a feature.  \n   - Because the column directly encodes the target, the model will learn a perfect mapping.  \n\n6. **Generate the submission file**  \n   - Create `submission.csv` with two columns: `PassengerId` and the predicted `Survived` values obtained from steps\u202f3\u202for\u202f4.  \n   - Save with `submission.to_csv('submission.csv', index=False)`.  \n\n7. **Validate (if possible) and submit**  \n   - Run a local split\u2011validation that respects the leakage (e.g., keep all rows sharing the same identifier in the same fold) to confirm 100\u202f% accuracy before uploading.  \n\n> **Caution:** Exploiting data leakage or using an external copy of the test labels violates Kaggle\u2019s competition rules and is considered cheating. The steps above describe *how* the leak has been used historically, not a recommended practice for legitimate competition participation.",
      "url": ""
    },
    {
      "title": "How to score top 3% in Kaggle\u2019s ,,Titanic \u2014 Machine Learning from Disaster\u201d competition",
      "text": "<div><div><div><a href=\"https://anelmusic13.medium.com/?source=post_page---byline--13d056e262b1---------------------------------------\"><div><p></p></div></a></div><p>Hey, fellow machine learning enthusiasts. If you\u2019ve ever had a try at a Kaggle competition, chances are you\u2019re already familiar with the Titanic dataset. <br/>This competition is about predicting whether a passenger will survive the Titanic disaster or not.</p><p>With relatively little effort it is possible to rank among the top 30% of participants. Unfortunately, many of the top scorers train their model on an external dataset and thus obtain a model that classifies the test data with 100% accuracy. I couldn\u2019t tell you why someone would want to cheat in such an obvious way but what this essentially means is that you have to make an extra effort to get into the top 3%.</p><p><strong>Aim</strong> of this article:</p><ul><li>Explain step by step the end-to-end data pipeline that is needed to score among top 3%.</li><li>Discuss the thought process of a Machine Learning Engineer / Data Scientist in Data Cleaning and Feature Engineering.</li><li>Make it more difficult for future participants to stand out :)</li></ul><p><strong>You can find the complete code at:</strong></p><p><a href=\"https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis\">https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis</a></p><h2>1. Getting the data:</h2><figure><figcaption>Import needed modules for handling tabular data, plotting and accessing common ML</figcaption></figure><figure><figcaption>Provide data paths</figcaption></figure><p>In my case the train.csv as well as test.csv are located in the current working directory. Using pandas we can read and convert the csv-files to pandas dataframes:</p><figure><figcaption>Read data csv and convert it to pandas dataframe</figcaption></figure><p>Next, we can investigate the data to get a better understanding of the available features:</p><figure><figcaption>Show train.csv pandas dataframe</figcaption></figure><h2>2. Exploratory Data Analysis</h2><p>Typically, it is not necessary to use each available feature. Many of them do not provide additional information for the model and increase the training time unnecessarily. However, a more crucial problem that is often overlooked is that additional features inevitably require more data. Now why is that?</p><p><strong>Why additional features call for additional data:</strong></p><p>If you think about it, each additional feature adds a new dimension to the feature space. This in turn increases the distance between observations because each new dimension adds a non-negative term to the calculation of the euclidean distance. Thus the feature space becomes sparse. In a sparse feature space ML algorithms tend to overfit to noise and the only way to populate your feature space is by adding additional (and rich) data.</p><p>As you can see it is essential to explore which features should be considered and which should not.</p><p>I would argue that most important skill of a Machine Learning Engineer/Data Scientist is to be unbiased, not assume things and to ask good questions.</p><p>So, let\u2019s ask some questions:</p><h2>2.1 Question 1: Does the dataset contain missing values?</h2><p>In the very first step it is helpful to check if and how many entries are missing in the dataset.</p><figure><figcaption>Dataset missing values summary</figcaption></figure><figure><figcaption>Define function that creates missing value heatmap</figcaption></figure><figure><figcaption>Missing values heatmap</figcaption></figure><p>From the above plots, it can be seen that the training and similarly the test datasets contain features with missing values. The sparsest features are \u201cage\u201d and \u201ccabin\u201d.</p><p>A naive approach to solve this problem would be to remove the feature completely from the dataset. However, since we do not know how much information these they provide they further investigation is needed. Maybe we find that it would make sense to impute the missing values using sophisticated data imputation methods.</p><h2>2.1 Question 1: How many passengers survived?</h2><figure><figcaption>Check survival rate</figcaption></figure><p>If you prefer plots you can define the function below to plot a bar chart:</p><figure><figcaption>Define function for plotting bar charts</figcaption></figure><figure><figcaption>Bar chart survival rate</figcaption></figure><p>As expected the majority of passengers in the training data died and only 38% survived. This means that the training data suffers from data imbalance but it is not severe which is why I will not consider techniques like sampling to tackle the imbalance.</p><figure><figcaption>Degree of minority class imbalance [shorturl.at/fTV37]</figcaption></figure><h2>2.2 Question 2: Is the likelihood of survival dependent on gender?</h2><p>In 1912, about 110 years ago, women were generally considered to be the weaker sex and should be protected. Let\u2019s investigate if the men of that time were so dutiful even under the fear of death.</p><figure><figcaption>Passenger count based on gender</figcaption></figure><figure><figcaption>Survival ratio based on gender</figcaption></figure><figure><figcaption>Survival rate bar chart</figcaption></figure><p>Here, we can clearly see that even though the majority of passenger were men, the majority of survivors were women. The key observation here is that the survival rate for female passengers is 4 times higher than the survival rate of male passengers. This seems to confirm that the phrase \u201cwomen and children first\u201d does indeed seem to have been a rule to which men adhered to.</p><h2>2.3 Question 3: Could it be that the class to which a passenger belonged correlates with the probability of survival??</h2><figure><figcaption>Passenger count with respect to class</figcaption></figure><figure><figcaption>Passenger class distribution</figcaption></figure><figure><figcaption>Bar chart passenger class vs dependent (target) variable ,,survived\u201d</figcaption></figure><p>From the plots and tables above it becomes clear that the Pclass is an important factor to consider.</p><p><strong>Key observations:</strong></p><ul><li>Most passenger had class 3 tickets, yet only 24% of class 3 passengers survived.</li><li>Almost 63% of the passenger from class 1 survived.</li><li>Approx 50% of the class 2 passenger survived.</li></ul><p>However, it is not clear yet weather the class or the gender is the underlying and deciding factor. Which brings another important question:</p><h2>2.4 Question 4: Is the higher survival rate in Class 1 due to the class itself or due to a skewed gender distribution in which female passengers dominate?</h2><figure><figcaption>Function that plots bar chart with multiple features</figcaption></figure><figure><figcaption>Bar chart of gender vs surival rate</figcaption></figure><figure><figcaption>Percentage of male and female survivors with respect to gender</figcaption></figure><p>Here, we can see that the question raised above was justified. Irrespective of the class the most important factor when it comes to survival was gender. (At least between the two features Sex and Pclass). However, men had a significantly higher chance of survival if they bought class 1 tickets. This just shows that we should keep both features as both yield insightful information that should help our model.</p><p><strong>Key observations:</strong></p><ul><li>Survival Rate females 1. Class: 96,8%</li><li>Survival Rate females 2. Class: 92,1%</li><li>Survival Rate females 3. Class: 50%</li><li>Survival Rate male 1. Class: 36.8% <br/>(still significantly lower than 3. class females)</li></ul><h2>2.5 Question 5: Did a passengers age influence the chance of survival?</h2><figure><figcaption>Utility function for plotting a histogram and the kernel density estimate</figcaption></figure><figure><figcaption>Age distribution</figcaption></figure><figure><figcaption>Age kernel density</f...",
      "url": "https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1"
    },
    {
      "title": "The Titanic has a Leak",
      "text": "- [What is leakage?](https://david-recio.com/2022/04/11/titanic-leak.html#What-is-leakage?)\n- [Where is the leakage?](https://david-recio.com/2022/04/11/titanic-leak.html#Where-is-the-leakage?)\n- [Why does the leakage matter?](https://david-recio.com/2022/04/11/titanic-leak.html#Why-does-the-leakage-matter?)\n- [How do we plug the leak?](https://david-recio.com/2022/04/11/titanic-leak.html#How-do-we-plug-the-leak?)\n- [Here is what we will do](https://david-recio.com/2022/04/11/titanic-leak.html#Here-is-what-we-will-do)\n- [Baselines](https://david-recio.com/2022/04/11/titanic-leak.html#Baselines)\n  - [Enhanced gender model](https://david-recio.com/2022/04/11/titanic-leak.html#Enhanced-gender-model)\n  - [Group survival model](https://david-recio.com/2022/04/11/titanic-leak.html#Group-survival-model)\n- [Our \"exemplary\" model](https://david-recio.com/2022/04/11/titanic-leak.html#Our-)\n  - [Feature engineering and categorical encoding](https://david-recio.com/2022/04/11/titanic-leak.html#Feature-engineering-and-categorical-encoding)\n  - [XGBoost classifier and hyperparameter tuning](https://david-recio.com/2022/04/11/titanic-leak.html#XGBoost-classifier-and-hyperparameter-tuning)\n- [Cross-validation](https://david-recio.com/2022/04/11/titanic-leak.html#Cross-validation)\n  - [Identifying the groups](https://david-recio.com/2022/04/11/titanic-leak.html#Identifying-the-groups)\n  - [Cross-validation splits](https://david-recio.com/2022/04/11/titanic-leak.html#Cross-validation-splits)\n  - [Nested cross-validation](https://david-recio.com/2022/04/11/titanic-leak.html#Nested-cross-validation)\n- [How well does our classifier do when we prevent leakage?](https://david-recio.com/2022/04/11/titanic-leak.html#How-well-does-our-classifier-do-when-we-prevent-leakage?)\n- [Does our model rely on leakage if we give it the chance?](https://david-recio.com/2022/04/11/titanic-leak.html#Does-our-model-rely-on-leakage-if-we-give-it-the-chance?)\n- [Conclusions](https://david-recio.com/2022/04/11/titanic-leak.html#Conclusions)\n- [P.S. Is the Kaggle test set random?](https://david-recio.com/2022/04/11/titanic-leak.html#P.S.-Is-the-Kaggle-test-set-random?)\n\n```\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_validate, GridSearchCV, StratifiedKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport xgboost as xgb\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n```\n\n## What is leakage?\n\nTraining a machine learning model to make predictions is tricky (especially about the future!). One of the main issues is [overfitting](https://en.wikipedia.org/wiki/Overfitting): If left unchecked, models will tend to fit the training data too specifically in a way that doesn't actually generalize to \"future data\".\n\nThis is why we always set aside a subset of the data (the **test set**) to evaluate the model predictions. The model never gets to see the test data during training, to simulate new data like the data the model will have to deal with when in real deployment. At first this seems like a foolproof method to gauge how well the model will do in practice, assuming that the \"future data\" in the context the model needs to operate arises from the same probability distribution as our current data (that is a whole other issue, see [data drift](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets?tabs=python)).\n\nHowever, in practice there might be **unintended correlations** between the test set and the data we used to train the model (the **training set**). Those correlations might allow us to make predictions based on information which we wouldn't actually have access to at prediction time in reality. We call this phenomenon **data leakage**, because \"future\" information is accidentally leaking from the test set to the training set. This can lead to dramatically overestimating the true model performance. Even worse, the model could end up mostly relying on the leakage for predictions, to the detriment of legitimate signals. This would make it essentially useless in a real deployment.\n\nThis somewhat abstract description will become clearer once we look at a specific instance of leakage.\n\n## Where is the leakage?\n\nIn the case of the famous **Titanic** [competition](https://www.kaggle.com/c/titanic) there is a major source of information leakage. **Groups of women and children** traveling together tend to either all live or all die, simply because they tended to stay together. Women and children were famously prioritized on the lifeboats, while adult men were separated from their families.\n\nThis is an instance of data leakage because we wouldn't have known which families were going to survive before the Titanic sank, yet it provides us with a lot of information on passenger survival in our data. Strictly speaking, it is open to debate what exactly constitutes leakage in a one-time event such as in the Titanic disaster. However, if we imagine having to make a prediction about another ship sinking in similar circumstances, it seems unreasonable to assume that we would have information on which families would survive _beyond the information on which passengers would survive_. In contrast to survival per passenger class, for instance, family survival is seems to be subject to random events in a way that is not generalizable (what could be called random noise).\n\n## Why does the leakage matter?\n\nI am not the first one to point out that family survival is a major predictor of individual passenger survival. As far as I know, however, the extent and importance of the leakage has not been thoroughly investigated yet.\n\nThere is no doubt that using the leakage gives an important advantage in the Kaggle Titanic competition. Most passenger groups (which we will specify below) have in fact been separated in the train-test split. This is not a particular characteristic of the Kaggle test set. Instead, it is statistically a near-certainty given the number of groups and the group sizes, as long as the test set makes up a third of the data.\n\nWe can distinguish three ways in which the leakage has been used on the Kaggle competition over the years:\n\n- [Chris Deotte](https://www.kaggle.com/code/cdeotte/titanic-using-name-only-0-81818/notebook) and others have already observed that one can get very good results (better than many sophisticated approaches) just by making predictions with a very simple rule **using family survival directly** (but not explicitly identifying it as an example of data leakage, to the best of my knowledge). The rule is the following: Just predict that all males die and all females live, except for boys whose family survived (who are predicted to live instead) and females whose family died (who are predicted to die instead). We will implement this model below and compare its accuracy with other approaches.\n- People like [Erik Bruin](https://www.kaggle.com/code/erikbruin/titanic-2nd-degree-families-and-majority-voting/report) have also noticed that adding \"family survival\" as an **engineered feature** helps train models. This basically amounts to [target encoding](https://maxhalford.github.io/blog/target-encoding/) of an engineered \"family group feature\", where the groups are replaced by the mean survival of the group within the training set (or NA if the group is not represented in the training set).\n- Finally, beyond the explicit uses of the data leakage listed above, it is conceivable that many classifiers **use it implicitly and inadvertently**. Groups traveling together can be identified from features such as passenger class, port of embarkation, ticket number, cabin number, number of family members on board (children, parents, siblings, spouses). The 'Age' feature ...",
      "url": "https://david-recio.com/2022/04/11/titanic-leak.html"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy]",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    },
    {
      "title": "How to score 0.8134 \ud83c\udfc5 in Titanic Kaggle Challenge | Ahmed BESBES",
      "text": "September 10, 2016\u00a0\u00a0\u00a0\u00a0\u00a033min read\n\n# How to score 0.8134 \ud83c\udfc5 in Titanic Kaggle Challenge\n\nThe [Titanic challenge](https://www.kaggle.com/c/titanic/) hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing him such as his age, his sex, or his passenger class on the boat.\n\nI have been playing with the Titanic dataset for a while, and I have recently achieved an accuracy score of 0.8134 on the public leaderboard. As I'm writing this post, I am ranked among the top 4% of all Kagglers.\n\nThis post is the opportunity to share my solution with you.\n\nTo make this tutorial more \"academic\" so that anyone could benefit, I will first start with an exploratory data analysis (EDA) then I'll follow with feature engineering and finally present the predictive model I set up.\n\nThroughout this jupyter notebook, I will be using Python at each level of the pipeline.\nThe main libraries involved in this tutorial are:\n\n- **Pandas** for data manipulation and ingestion\n- **Matplotlib** and **seaborn** for data visualization\n- **Numpy** for multidimensional array computing\n- **sklearn** for machine learning and predictive modeling\n\n### Installation procedure\n\nA very easy way to install these packages is to download and install the [Conda](http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install) distribution that encapsulates them all. This distribution is available on all platforms (Windows, Linux and Mac OSX).\n\n### Nota Bene\n\nThis is my first attempt as a blogger and as a machine learning practitioner.\n\nIf you have a question about the code or the hypotheses I made, do not hesitate to post a comment in the comment section below.\nIf you also have a suggestion on how this notebook could be improved, please reach out to me.\nThis tutorial is available on my [github](https://github.com/ahmedbesbes/How-to-score-0.8134-in-Titanic-Kaggle-Challenge) account.\n\nHope you've got everything set on your computer. Let's get started.\n\n## I - Exploratory data analysis\n\nAs in different data projects, we'll first start diving into the data and build up our first intuitions.\n\nIn this section, we'll be doing four things.\n\n- Data extraction : we'll load the dataset and have a first look at it.\n- Cleaning : we'll fill in missing values.\n- Plotting : we'll create some interesting charts that'll (hopefully) spot correlations and hidden insights out of the data.\n- Assumptions : we'll formulate hypotheses from the charts.\n\nWe tweak the style of this notebook a little bit to have centered plots.\n\n```\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\");\n```\n\nWe import the useful libraries.\n\n```\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\n\nimport pylab as plot\nparams = {\n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplot.rcParams.update(params)\n```\n\nTwo datasets are available: a training set and a test set.\nWe'll be using the training set to build our predictive model and the testing set to score it and generate an output file to submit on the Kaggle evaluation system.\n\nWe'll see how this procedure is done at the end of this post.\n\nNow let's start by loading the training set.\n\n```\ndata = pd.read_csv('./data/train.csv')\nprint(data.shape)\n#(891, 12)\n```\n\nWe have:\n\n- 891 rows\n- 12 columns\n\nPandas allows you to have a sneak peak at your data.\n\n```\ndata.head()\n```\n\n| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n\nThe Survived column is the **target variable**. If Suvival = 1 the passenger survived, otherwise he's dead. The is the variable we're going to predict.\n\nThe other variables describe the passengers. They are the **features**.\n\n- PassengerId: and id given to each traveler on the boat\n- Pclass: the passenger class. It has three possible values: 1,2,3 (first, second and third class)\n- The Name of the passeger\n- The Sex\n- The Age\n- SibSp: number of siblings and spouses traveling with the passenger\n- Parch: number of parents and children traveling with the passenger\n- The ticket number\n- The ticket Fare\n- The cabin number\n- The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q\n\nPandas allows you to a have a high-level simple statistical description of the numerical features.\nThis can be done using the describe method.\n\n```\ndata.describe()\n```\n\n| PassengerId | Survived | Pclass | Age | SibSp | Parch | Fare |\n| --- | --- | --- | --- | --- | --- | --- |\n| count | 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 |\n| mean | 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 |\n| std | 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 |\n| min | 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 |\n| 25% | 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 |\n| 50% | 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 |\n| 75% | 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 |\n| max | 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 |\n\nThe count variable shows that 177 values are missing in the Age column.\n\nOne solution is to fill in the null values with the median age. We could also impute with the mean age but the median is more robust to outliers.\n\n```\ndata['Age'] = data['Age'].fillna(data['Age'].median())\n```\n\nLet's now make some charts.\n\nLet's visualize survival based on the gender.\n\n```\ndata['Died'] = 1 - data['Survived']\ndata.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, colors=['g', 'r']);\n```\n\nIt looks like male passengers are more likely to succumb.\n\nLet's plot the same graph but with ratio instead.\n\n```\ndata.groupby('Sex').agg('mean')[['Survived', 'Died']].plot(kind='bar', figsize=(25, 7),\n                                                           stacked=True, colors=['g', 'r']);\n```\n\nThe Sex variable seems to be a discriminative feature. Women are more likely to survive.\n\nLet's now correlate the survival with the age variable.\n\n```\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Sex', y='Age',\n               hue='Survived', data=data,\n               split=True,\n               palette={0: \"r\", 1: \"g\"}\n              );\n```\n\nAs we saw in the chart above and validate by the following:\n\n- Women survive more than men, as depicted by the larger female green histogram\n\nNow, we see that:\n\n- The age conditions the survival for male passengers:\n\n  - Younger male tend to survive\n  - A large number of passengers between 20 and 40 succumb\n- The age doesn't seem to have a direct impact on the female survival\n\nThese violin plots confirm that one old code of conduct that sailors and captains follow in case of threatening situations: **\"Women and children first !\"**.\n\nRight?\n\nLet's now focus on the Fare ticket of each passenger and see how it could impact the survival.\n\n```\nfigure = plt.figure(figsize=(25, 7))\nplt.hist([dat...",
      "url": "https://www.ahmedbesbes.com/blog/kaggle-titanic-competition"
    },
    {
      "title": "How To Score ~80% Accuracy in Kaggle\u2019s Spaceship Titanic Competition",
      "text": "<div><div><div><h2>This is a step-by-step guide to walk you through submitting a \u201c.csv\u201d file of predictions to Kaggle for the new titanic competition.</h2><div><div><a href=\"https://medium.com/@ZaynabAwofeso?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div><div><a href=\"https://medium.com/codex?source=post_page---byline--e7d06ce25bad---------------------------------------\"><div><p></p></div></a></div></div></div><figure><figcaption>image by <a href=\"http://unsplash.com\">unsplash</a></figcaption></figure><h2>Introduction</h2><p>Kaggle recently launched a fun competition called Spaceship Titanic. It is designed to be an update of the popular Titanic competition which helps people new to data science learn the basics of machine learning, get acquainted with Kaggle\u2019s platform, and meet others in the community. This article is a beginner-friendly analysis of the Spaceship Titanic Kaggle Competition. It covers steps to obtain any meaningful insights from the data and to predict the \u201cground truth\u201d for the test set with an accuracy of ~80% using RandomForestClassifier.</p><h2>Index</h2><ol><li>Problem definition and metrics</li><li>About the data</li><li>Exploratory Data Analysis</li><li>Data Cleaning and preprocessing</li><li>Feature Extraction and Feature Selection</li><li>Baseline Model Performance and Model Building</li><li>Submission and Feature Importance</li></ol><h2>1. Problem definition and metrics</h2><p>As the first thing, we have to understand the problem. It\u2019s the year 2912 and the interstellar passenger liner Spaceship Titanic has collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension! To help rescue crews retrieve the lost passengers, we are challenged to use records recovered from the spaceship\u2019s damaged computer system to predict which passengers were transported to another dimension.</p><blockquote><p>This problem is a binary class classification problem where we have to predict which passengers were transported to an alternate dimension or not, and we will be using accuracy as a metric to evaluate our results.</p></blockquote><h2>2. About the data</h2><p>We will be using 3 CSV files:</p><ul><li><strong>train file</strong> (spaceship_titanic_train.csv) \u2014 contains personal records of the passengers that would be used to build the machine learning model.</li><li><strong>test file</strong> (spaceship_titanic_test.csv) \u2014 contains personal records for the remaining one-third (~4300) of the passengers, but not the target variable (i.e. the value of Transported for the passengers). It will be used to see how well our model performs on unseen data.</li><li><strong>sample submission file</strong> (sample_submission.csv) \u2014 contains the format in which we have to submit our predictions.</li></ul><p>We will be using python for this problem. You can download the dataset from Kaggle <a href=\"https://www.kaggle.com/competitions/spaceship-titanic/data\">here</a>.</p><p><strong>Import required libraries</strong></p><figure></figure><p><strong>Reading Data</strong></p><figure></figure><p>Let\u2019s make a copy of the train and test data so that even if we make any changes to these datasets it would not affect the original datasets.</p><figure></figure><p>We will look at the structure of the train and test dataset next. We will first check the features present, then we will look at their data types.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name', 'Transported'],<br/> dtype='object')</span></pre><p>We have 13 independent variables and 1 target variable (Transported) in the training dataset. Let\u2019s also look at the columns of the test dataset.</p><figure></figure><pre><span>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',<br/> 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',<br/> 'Name'],<br/> dtype='object')</span></pre><p>We have similar features in the test dataset as the training dataset except Transported that we will predict using the model built by the train data.</p><p>Given below is the description for each variable.</p><ul><li><strong>PassengerId</strong> \u2014 A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.</li><li><strong>HomePlanet</strong> \u2014 The planet the passenger departed from, typically their planet of permanent residence.</li><li><strong>CryoSleep </strong>\u2014 Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</li><li><strong>Cabin</strong> \u2014 The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</li><li><strong>Destination</strong> \u2014 The planet the passenger will be debarking to.</li><li><strong>Age</strong> \u2014 The age of the passenger.</li><li><strong>VIP </strong>\u2014 Whether the passenger has paid for special VIP service during the voyage.</li><li><strong>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck</strong> \u2014 Amount the passenger has billed at each of the Spaceship Titanic\u2019s many luxury amenities.</li><li><strong>Name</strong> \u2014 The first and last names of the passenger.</li><li><strong>Transported</strong> \u2014 Whether the passenger was transported to another dimension. This is the target, the column we are trying to predict.</li></ul><p>Let\u2019s print data types for each variable of the training dataset.</p><figure></figure><pre><span>PassengerId object<br/>HomePlanet object<br/>CryoSleep object<br/>Cabin object<br/>Destination object<br/>Age float64<br/>VIP object<br/>RoomService float64<br/>FoodCourt float64<br/>ShoppingMall float64<br/>Spa float64<br/>VRDeck float64<br/>Name object<br/>Transported bool<br/>dtype: object</span></pre><p>We can see there are three formats of data types in the training dataset:</p><ul><li><strong>object</strong> (Categorical variables) \u2014 The categorical variables in the training dataset are: PassengerId, HomePlanet, CryoSleep, Cabin, Destination, VIP and Name</li><li><strong>float64</strong> (Float variables i.e Numerical variables which have some decimal values involved) \u2014 The Numerical variables in our train dataset: Age, RoomService, FoodCourt, ShoppingMall, Spa and VRDeck</li><li><strong>bool</strong> (Boolean variables i.e. a variable that has one of two possible values e.g. True or False) \u2014 The Boolean Variable in our dataset is Transported</li></ul><p>Let\u2019s look at the shape of our train and test dataset.</p><figure></figure><pre><span>The shape of the train dataset is: (8693, 14)<br/>The shape of the test dataset is: (4277, 13)</span></pre><p>We have 8693 rows and 14 columns in the training dataset and 4277 rows and 13 columns in the test dataset.</p><ol><li>Exploratory Data Analysis</li></ol><p><strong><em>Univariate Analysis</em></strong></p><p>Univariate analysis is the simplest form of analyzing data where we examine each data individually to understand the distribution of its values.</p><p><strong><em>Target Variable</em></strong></p><p>We will first look at the target variable i.e. Transported. Since it is a categorical variable, let us look at its percentage distribution and bar plot.</p><figure></figure><pre><span>True 0.503624<br/>False 0.496376<br/>Name: Transported, dtype: float64</span></pre><figure></figure><figure></figure><p>Out of 8693 passengers in the train dataset, 4378 (about 50%) were Transported to another dimension.</p><p>Let\u2019s visualize the Independent categorical features next.</p><p><strong><em>...",
      "url": "https://medium.com/codex/how-to-score-80-accuracy-in-kaggles-spaceship-titanic-competition-using-random-forest-classifier-e7d06ce25bad"
    },
    {
      "title": "Notebook on nbviewer",
      "text": "1. [data-science-ipython-notebooks](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/tree/master)\n2. [kaggle](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/tree/master/kaggle)\n\nNotebook\n\nThis notebook was prepared by [Donne Martin](http://donnemartin.com). Source and license info is on [GitHub](https://github.com/donnemartin/data-science-ipython-notebooks).\n\n# Kaggle Machine Learning Competition: Predicting Titanic Survivors [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Kaggle-Machine-Learning-Competition:-Predicting-Titanic-Survivors)\n\n- Competition Site\n- Description\n- Evaluation\n- Data Set\n- Setup Imports and Variables\n- Explore the Data\n- Feature: Passenger Classes\n- Feature: Sex\n- Feature: Embarked\n- Feature: Age\n- Feature: Family Size\n- Final Data Preparation for Machine Learning\n- Data Wrangling Summary\n- Random Forest: Training\n- Random Forest: Predicting\n- Random Forest: Prepare for Kaggle Submission\n- Support Vector Machine: Training\n- Support Vector Machine: Predicting\n\n## Competition Site [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Competition-Site)\n\nDescription, Evaluation, and Data Set taken from the [competition site](https://www.kaggle.com/c/titanic-gettingStarted).\n\n## Description [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Description)\n\n![alt text](http://upload.wikimedia.org/wikipedia/commons/6/6e/St%C3%B6wer_Titanic.jpg)\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n## Evaluation [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Evaluation)\n\nThe historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set.\n\nFor each passenger in the test set, you must predict whether or not they survived the sinking ( 0 for deceased, 1 for survived ). Your score is the percentage of passengers you correctly predict.\n\nThe Kaggle leaderboard has a public and private component. 50% of your predictions for the test set have been randomly assigned to the public leaderboard ( the same 50% for all users ). Your score on this public portion is what will appear on the leaderboard. At the end of the contest, we will reveal your score on the private 50% of the data, which will determine the final winner. This method prevents users from 'overfitting' to the leaderboard.\n\n## Data Set [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Data-Set)\n\n| File Name | Available Formats |\n| --- | --- |\n| train | .csv (59.76 kb) |\n| gendermodel | .csv (3.18 kb) |\n| genderclassmodel | .csv (3.18 kb) |\n| test | .csv (27.96 kb) |\n| gendermodel | .py (3.58 kb) |\n| genderclassmodel | .py (5.63 kb) |\n| myfirstforest | .py (3.99 kb) |\n\n```\nVARIABLE DESCRIPTIONS:\nsurvival        Survival\n                (0 = No; 1 = Yes)\npclass          Passenger Class\n                (1 = 1st; 2 = 2nd; 3 = 3rd)\nname            Name\nsex             Sex\nage             Age\nsibsp           Number of Siblings/Spouses Aboard\nparch           Number of Parents/Children Aboard\nticket          Ticket Number\nfare            Passenger Fare\ncabin           Cabin\nembarked        Port of Embarkation\n                (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSPECIAL NOTES:\nPclass is a proxy for socio-economic status (SES)\n 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n\nAge is in Years; Fractional if Age less than One (1)\n If the Age is Estimated, it is in the form xx.5\n\nWith respect to the family relation variables (i.e. sibsp and parch)\nsome relations were ignored.  The following are the definitions used\nfor sibsp and parch.\n\nSibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\nSpouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\nParent:   Mother or Father of Passenger Aboard Titanic\nChild:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n\nOther family relatives excluded from this study include cousins,\nnephews/nieces, aunts/uncles, and in-laws.  Some children travelled\nonly with a nanny, therefore parch=0 for them.  As well, some\ntravelled with very close friends or neighbors in a village, however,\nthe definitions do not support such relations.\n\n```\n\n## Setup Imports and Variables [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Setup-Imports-and-Variables)\n\nIn\u00a0\\[1\\]:\n\n```\nimport pandas as pd\nimport numpy as np\nimport pylab as plt\n\n# Set the global default size of matplotlib figures\nplt.rc('figure', figsize=(10, 5))\n\n# Size of matplotlib figures that contain subplots\nfizsize_with_subplots = (10, 10)\n\n# Size of matplotlib histogram bins\nbin_size = 10\n\n```\n\n## Explore the Data [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Explore-the-Data)\n\nRead the data:\n\nIn\u00a0\\[2\\]:\n\n```\ndf_train = pd.read_csv('../data/titanic/train.csv')\ndf_train.head()\n\n```\n\nOut\\[2\\]:\n\n|  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n| 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1000 | C123 | S |\n| 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.0500 | NaN | S |\n\nIn\u00a0\\[3\\]:\n\n```\ndf_train.tail()\n\n```\n\nOut\\[3\\]:\n\n|  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 886 | 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27 | 0 | 0 | 211536 | 13.00 | NaN | S |\n| 887 | 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19 | 0 | 0 | 112053 | 30.00 | B42 | S |\n| 888 | 889 | 0 | 3 | Johnston, Miss. Catherine Helen \"Carrie\" | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S |\n| 889 | 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26 | 0 | 0 | 111369 | 30.00 | C148 | C |\n| 890 | 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32 | 0 | 0 | 370376 | 7.75 | NaN | Q |\n\nView the data types of each column:\n\nIn\u00a0\\[4\\]:\n\n```\ndf_train.dtypes\n\n```\n\nOut\\[4\\]:\n\n```\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n```\n\n...",
      "url": "https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb"
    },
    {
      "title": "Space Titanic Kaggle competition 0.8066 score solution",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Spaceship Titanic Kaggle competition top 7% score solution\n\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\n\n7 min read\n\n\u00b7\n\nJan 17, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nKaggle\u2019s Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an \u201canomaly\u201d using records recovered from the spaceship\u2019s damaged computer system. The \u201clore\u201d of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship\u2019s passengers.\n\n## Import libraries\n\n```\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\n```\n\n## Let\u2019s Start\n\nAs you can see, after I loaded the dataset, I removed both the \u201cPassenger Id\u201d and \u201cName\u201d columns. They are not going to provide any useful or important information to the prediction. Someone\u2019s name or Id does not change the probability of being Transported.\n\n```\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\n```\n\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the \u201cCabin\u201d column. Notice that the rows on the \u201cCabin\u201d column follow a specific pattern. Something like this: \u201cA/5/S\u201d, \u201cC/1/S\u201d, \u201cF/7/P\u201d. And I decided to investigate it. So, to make things simple I split the rows of the \u201cCabin\u201d into three columns based on both slashes (\u201d/\u201d) of the rows. For example, the \u201cA/5/S\u201d row would be transformed into three new columns: The first one is named \u201ccabin\\_code\u201d referring tothe character behind the first slash (A). The second one named \u201cid\\_cabin\u201d refers to the character behind the second slash (5). The third one named \u201ccabin\\_sector\u201d refers to the character after the second slash (S). And we end up with three new columns.\n\n```\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\n```\n\nFirst of all, I noticed that \u201ccabin\\_code\u201d only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\n\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\n\nPress enter or click to view image in full size\n\nI did the same thing with the \u201ccabin\\_sector\u201d column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\n\nPress enter or click to view image in full size\n\nThis means that this exploration of the original \u201cCabin\u201d column is worth it since new insights are being added to the model.\n\nNow, we can finally delete the \u201cCabin\u201d column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\n\nI also removed the \u201ccabin\\_id\u201d and the column that I had created. As I said before, the Id will not interfere with the model\u2019s predictive ability.\n\nSo used: df.drop(columns=\\[\u201cCabin\u201d,\u201did\\_cabin\u201d\\], inplace=True) to drop both columns\n\nBefore splitting our data, the \u201cTransported\u201d column must be in a binary format. As you can see, I switched \u201cTrue\u201d for 1 and \u201cFalse\u201d for 0.\n\nBinary transformation: df\\[\u201cTransported\u201d\\] = df\\[\u201cTransported\u201d\\].map({True:1, False:0})\n\nI also removed every row that had missing values in the \u201ccabin\\_code\u201d column.\n\n```\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\n```\n\nNow, we can finally split the data and proceed to develop our model.\n\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\n\n```\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\n```\n\n```\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n```\n\n```\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\n```\n\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don\u2019t know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\n\n```\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\n```\n\nAnd with column transformer, we can attach both transformations to one variable that I named \u201ctransformer\u201d. Observe that we also have to specify the type of data to which the pipeline will be applied t...",
      "url": "https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    }
  ]
}