# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Key findings: 891 train samples, 418 test samples, binary classification (Survived: 0/1)
- Target is imbalanced: 38.4% survived, 61.6% died
- Missing values: Age (~20%), Cabin (~77%), Embarked (2 samples)

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract title from passenger name using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Miss, Mrs, Master
- Group rare titles (Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona) into "Rare"
- Map Mlle/Ms to Miss, Mme to Mrs
- Title is highly predictive of survival (Master = young boys, Miss/Mrs = females)

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- Medium-sized families (2-4) had better survival rates than singles or large families

### Additional Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (proxy for wealth/deck location)
- **Name_length** = length of name string (correlates with social status)
- **Age*Class** interaction feature (young + upper class = highest survival)

### Binning Continuous Variables
- **AgeBand**: Bin Age into 5 categories (0-16, 16-32, 32-48, 48-64, 64+)
- **FareBand**: Bin Fare into 4 quartiles using pd.qcut

### Handling Missing Values
- **Age**: Fill with median grouped by Sex, Pclass, and/or Title
- **Embarked**: Fill with mode ('S' - Southampton)
- **Fare**: Fill with median
- **Cabin**: Create Has_Cabin feature, then drop original

### Feature Encoding
- Sex: female=0, male=1
- Embarked: S=0, C=1, Q=2
- Title: Mr=1, Miss=2, Mrs=3, Master=4, Rare=5

### Features to Drop
- PassengerId, Name, Ticket, Cabin (after extracting Has_Cabin)
- SibSp (after creating FamilySize)

## Models

### Single Models (Baseline)
For this small dataset (891 samples), tree-based models work well:
- **Random Forest**: n_estimators=100, good baseline (~86% CV accuracy)
- **Gradient Boosting**: learning_rate=0.25, max_depth=2, n_estimators=50
- **XGBoost**: learning_rate=0.01, max_depth=4, n_estimators=300
- **Decision Tree**: criterion='entropy', max_depth=6 (achieved 0.77990 on LB)
- **SVM**: Can achieve good results with proper feature scaling

### Ensemble Methods (Best Results)

#### Hard Voting Classifier
Combine multiple models with majority voting:
- Gradient Boosting + Random Forest + AdaBoost + Extra Trees + SVC
- Achieved 0.77990 on leaderboard

#### Stacking (Top 9% - 0.808)
Two-level stacking approach:
1. **First Level Base Models** (with out-of-fold predictions):
   - Random Forest
   - Extra Trees
   - AdaBoost
   - Gradient Boosting
   - SVC
2. **Second Level Meta-Model**:
   - XGBoost trained on first-level predictions
   - Parameters: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8, colsample_bytree=0.8

**Key Insight**: More uncorrelated base models produce better stacking results. Use diverse model types.

## Validation Strategy
- Use Stratified K-Fold (k=5 or k=10) to preserve class distribution
- Small dataset means high variance in CV scores
- Cross-validation accuracy ~82-86% typically

## Hyperparameter Tuning
- Use GridSearchCV with scoring='roc_auc' or 'accuracy'
- Key parameters to tune:
  - Tree models: n_estimators, max_depth, min_samples_split
  - XGBoost: learning_rate, max_depth, n_estimators, gamma, subsample

## Domain Knowledge
- "Women and children first" policy: Sex and Age are most important features
- Pclass is proxy for socio-economic status (1st class = wealthy = better survival)
- Embarked port correlates with passenger demographics
- Cabin location (deck) affected survival chances

## Expected Scores
- Simple baseline (all females survive): ~0.76
- Good feature engineering + single model: ~0.77-0.78
- Stacking ensemble: ~0.80-0.81
- Target to beat: 1.0 (perfect accuracy - likely requires manual inspection or data leakage)

## Implementation Tips
1. Combine train and test for consistent feature engineering
2. Create features before encoding/binning
3. Use out-of-fold predictions for stacking to avoid overfitting
4. Try multiple random seeds and average predictions
5. Consider pseudo-labeling with high-confidence test predictions
