# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Key findings: 891 train samples, 418 test samples, binary classification (Survived: 0/1)
- Target is imbalanced: 38.4% survived, 61.6% died
- Missing values: Age (~20%), Cabin (~77%), Embarked (2 samples)

## CRITICAL: Achieving Perfect/Near-Perfect Accuracy

### Family/Group Survival Leakage (Key to 100% Accuracy)
The most important insight for achieving very high accuracy is exploiting **family/group survival patterns**:

1. **Groups traveling together tend to either ALL LIVE or ALL DIE**
   - Women and children were prioritized for lifeboats
   - Families stayed together during evacuation
   - This creates a data leakage opportunity

2. **How to identify groups:**
   - **Surname**: Extract from Name field (text before comma)
   - **Ticket number**: Passengers with same ticket traveled together
   - **Cabin**: Passengers in same cabin were likely family/group
   - Combine these to create a unique group identifier

3. **Group Survival Feature Engineering:**
   ```
   - Create GroupId from Surname + FamilySize or Ticket
   - For each group in training data, compute survival rate
   - Apply this as a feature to both train and test
   - For groups appearing in both train and test, use training survival rate
   ```

4. **Simple Rule-Based Approach (achieves ~0.82):**
   - Predict all males die, all females live
   - EXCEPT: Boys whose family survived → predict LIVE
   - EXCEPT: Females whose family died → predict DIE

5. **Target Encoding of Groups:**
   - Encode family groups with their mean survival rate from training
   - This is essentially target encoding and very powerful

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract title from passenger name using regex pattern ` ([A-Za-z]+)\.`:
- Common titles: Mr, Miss, Mrs, Master
- Group rare titles (Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona) into "Rare"
- Map Mlle/Ms to Miss, Mme to Mrs
- Title is highly predictive of survival (Master = young boys, Miss/Mrs = females)

### Family Features
- **FamilySize** = SibSp + Parch + 1 (total family members including self)
- **IsAlone** = 1 if FamilySize == 1, else 0
- Medium-sized families (2-4) had better survival rates than singles or large families
- **Surname** = Name.split(',')[0] - for family grouping

### Ticket-Based Features
- **TicketPrefix**: Extract alphabetic prefix from Ticket
- **TicketGroup**: Group passengers by exact Ticket number
- **TicketGroupSize**: Count of passengers with same ticket
- **TicketGroupSurvival**: Mean survival of ticket group (from training data)

### Additional Features
- **Has_Cabin** = 1 if Cabin is not null, else 0 (proxy for wealth/deck location)
- **Deck** = First letter of Cabin (A, B, C, D, E, F, G, T)
- **Name_length** = length of name string (correlates with social status)
- **Age*Class** interaction feature (young + upper class = highest survival)

### Binning Continuous Variables
- **AgeBand**: Bin Age into 5 categories (0-16, 16-32, 32-48, 48-64, 64+)
- **FareBand**: Bin Fare into 4 quartiles using pd.qcut

### Handling Missing Values
- **Age**: Fill with median grouped by Sex, Pclass, and/or Title
- **Embarked**: Fill with mode ('S' - Southampton)
- **Fare**: Fill with median
- **Cabin**: Create Has_Cabin and Deck features, then drop original

### Feature Encoding
- Sex: female=0, male=1
- Embarked: S=0, C=1, Q=2
- Title: Mr=1, Miss=2, Mrs=3, Master=4, Rare=5

### Features to Drop
- PassengerId, Name (after extracting Title/Surname), Ticket (after extracting features), Cabin (after extracting Has_Cabin/Deck)

## Models

### Single Models (Baseline)
For this small dataset (891 samples), tree-based models work well:
- **Random Forest**: n_estimators=100, good baseline (~86% CV accuracy)
- **Gradient Boosting**: learning_rate=0.25, max_depth=2, n_estimators=50
- **XGBoost**: learning_rate=0.01, max_depth=4, n_estimators=300
- **Decision Tree**: criterion='entropy', max_depth=6 (achieved 0.77990 on LB)
- **SVM**: Can achieve good results with proper feature scaling

### Ensemble Methods (Best Results)

#### Hard Voting Classifier
Combine multiple models with majority voting:
- Gradient Boosting + Random Forest + AdaBoost + Extra Trees + SVC
- Achieved 0.77990 on leaderboard

#### Stacking (Top 9% - 0.808)
Two-level stacking approach:
1. **First Level Base Models** (with out-of-fold predictions):
   - Random Forest
   - Extra Trees
   - AdaBoost
   - Gradient Boosting
   - SVC
2. **Second Level Meta-Model**:
   - XGBoost trained on first-level predictions
   - Parameters: n_estimators=2000, max_depth=4, min_child_weight=2, gamma=0.9, subsample=0.8, colsample_bytree=0.8

**Key Insight**: More uncorrelated base models produce better stacking results. Use diverse model types.

## Validation Strategy
- Use Stratified K-Fold (k=5 or k=10) to preserve class distribution
- **CRITICAL**: Use GroupKFold or StratifiedGroupKFold to keep family groups together
- This prevents leakage during validation and gives more realistic CV scores
- Small dataset means high variance in CV scores

## Hyperparameter Tuning
- Use GridSearchCV with scoring='roc_auc' or 'accuracy'
- Key parameters to tune:
  - Tree models: n_estimators, max_depth, min_samples_split
  - XGBoost: learning_rate, max_depth, n_estimators, gamma, subsample

## Domain Knowledge
- "Women and children first" policy: Sex and Age are most important features
- Pclass is proxy for socio-economic status (1st class = wealthy = better survival)
- Embarked port correlates with passenger demographics
- Cabin location (deck) affected survival chances
- Families stayed together - this is the key insight for high accuracy

## Expected Scores
- Simple baseline (all females survive): ~0.76
- Good feature engineering + single model: ~0.77-0.78
- Stacking ensemble: ~0.80-0.81
- With family survival features: ~0.82-0.85
- Target to beat: 1.0 (requires exploiting group survival leakage fully)

## Implementation Strategy for Maximum Accuracy

1. **Combine train and test for consistent feature engineering**
2. **Create comprehensive group identifiers:**
   - Surname from Name
   - Ticket groups
   - Cabin groups (if available)
3. **Compute group survival statistics from training data**
4. **Apply group survival as features**
5. **Use ensemble of diverse models**
6. **Consider pseudo-labeling with high-confidence predictions**
7. **Try multiple random seeds and average predictions**

## Key Code Patterns

### Extract Surname
```python
df['Surname'] = df['Name'].str.split(',').str[0]
```

### Create Family Group
```python
df['FamilyGroup'] = df['Surname'] + '_' + df['FamilySize'].astype(str)
```

### Compute Group Survival (Target Encoding)
```python
group_survival = train.groupby('FamilyGroup')['Survived'].mean()
df['GroupSurvival'] = df['FamilyGroup'].map(group_survival)
```

### Ticket Group Survival
```python
ticket_survival = train.groupby('Ticket')['Survived'].mean()
df['TicketSurvival'] = df['Ticket'].map(ticket_survival)
```
