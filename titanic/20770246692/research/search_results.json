{
  "query": "What are the best feature engineering techniques for Titanic survival prediction to achieve high accuracy?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Feature engineering is the biggest driver of performance on the Titanic Kaggle competition.  First, treat the four columns with missing values (\u202fAge,\u202fFare,\u202fCabin,\u202fEmbarked\u202f) by either imputing sensible statistics or, as many practitioners find, turning the missingness itself into a signal \u2013 for example, assigning a separate \u201cMissing\u201d category for\u202fAge\u202fand\u202fCabin\u202for filling\u202fFare\u202fwith the median of the passenger\u2019s\u202fPclass\u202fand then scaling it ([towardsdatascience](https://towardsdatascience.com/titanic-feature-engineering-modeling-in-python-6749e6e87bf4)).  Next, convert raw variables into richer descriptors: bucket\u202fAge\u202finto bins (e.g., 0\u201112\u202f=\u202fchild, 13\u201118\u202f=\u202fteen, etc.), extract the passenger\u2019s title from\u202fName\u202f(\u201cMr\u201d, \u201cMrs\u201d, \u201cMiss\u201d, \u201cMaster\u201d, \u201cDr\u201d, etc.) and map rare titles to a common group, derive a\u202fFamilySize\u202f=\u202fSibSp\u202f+\u202fParch\u202f+\u202f1 and an\u202fIsAlone\u202fflag, and split the\u202fTicket\u202ffield to capture ticket prefixes that often indicate travel class or group bookings.  Cabin letters can be reduced to deck information (A\u2011G) and combined with\u202fFare\u202fto create a\u202fFarePerPerson\u202ffeature.  Finally, encode all categorical attributes with one\u2011hot or label encoding and standard\u2011scale numeric columns before feeding them to tree\u2011based models (XGBoost, Random Forest) or linear models with interaction terms ([plainenglish](https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb),\u202f[medium](https://medium.com/@vishal8092009/in-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21)).\n\nThese engineered features consistently push validation accuracy above the 80\u202f% threshold and are highlighted in recent high\u2011scoring notebooks (e.g., a 2025 GitHub project that emphasizes family\u2011size, title, and deck features as the most predictive) ([github](https://github.com/PaoloVeliz/titanic-survival-prediction-ml)).  By systematically handling missing data, creating relational attributes (family, title, deck), and applying appropriate encoding and scaling, you give models the nuanced information they need to achieve top\u2011tier performance on the Titanic survival prediction task.",
      "url": ""
    },
    {
      "title": "Feature Engineering & Modeling in Python",
      "text": "[Data Science](https://towardsdatascience.com/category/data-science/)\n\n# Feature Engineering & Modeling in Python\n\nHow to predict the survival status in Titanic passengers?\n\n[Jessie Xu](https://towardsdatascience.com/author/jessiexxu/)\n\nJul 31, 2021\n\n5 min read\n\nShare\n\nPhoto by [Maximilian Weisbecker](https://unsplash.com/@maxweisbecker?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)\n\nIn this article, I\u2019ll walk through how to build models to predict the survival status of the Titanic passengers in Python. The methodology used here could also be applied to other similar use cases.\n\nFor added context, you can refer to the previous article, [Exploratory Data Analysis & Visualization in Python](https://towardsdatascience.com/exploratory-data-analysis-visualization-in-python-1bf7a817bc01), to read more about the Titanic data exploration and visualization in Python.\n\nThe titanic data can be downloaded from [the Kaggle website](https://www.kaggle.com/c/titanic/data).\n\n1. **Handling Missing Data**\n\nThere are 4 predictors that contain missing data \u2013 Age, Fare, Cabin, and Embarked.\n\nMissing value count\n\n**Age**\n\nFor the missing data in age, we can (1) refill the n/a values with mean or median, or (2) categorize the n/a values as a separate category. I chose the second option given that the n/a category could have useful information for predicting the survival status.\n\nAnalyzed in [the previous article](https://towardsdatascience.com/exploratory-data-analysis-visualization-in-python-1bf7a817bc01)\n\nThe missing data in age was categorized as a category. For the remaining non-missing values, one way to categorize them is by manually defining the number of age bins and the bin size.\n\nSurvival rates by age bins\n\nIn the chart above, the non-missing age values were categorized into 9 age bins. It\u2019s easy to change the age\\_bin\\_size input in the categorize\\_age function to test out a different number of age bins and see which one makes sense. This approach is a bit arbitrary. I prefer another way to select age bins \u2013 using the K-Means clustering method.\n\nApply K-Means clustering to the non-missing age data.\n\nAge range for each age bin defined by k-meansSurvival rates by age bin\n\nAs shown in the output table, children (age bin #1 with age ranged from 0 to 13) had the highest survival rates compared to the other age groups.\n\nBased on the age bins and bin sizes above, I created a new variable called \"age bin\".\n\nSurvival rates by age bins\n\n**Fare**\n\nOnly one passenger didn\u2019t have fare data.\n\nPassenger without fare data\n\nThis passenger had a third class ticket. I refilled the fare data by the average fare in the 3rd class tickets.\n\nNext, create the fare bins using the K-Means clustering method. In the previous article, I found that passengers with complementary tickets had a very low survival rate. So I coded the $0 fare as a separate category.\n\nSurvival rates by fare bins\n\n**Cabin**\n\nThere are too many missing values in Cabin. I decided to drop this column.\n\n**Embarked**\n\nTwo passengers didn\u2019t have Embarked data, and they both had the first class tickets.\n\nPassengers without Embarked data\n\nWe can check the Embarked median by Pclass.\n\nPclass by Embarked metrics\n\nThe median of Embarked in Pclass = 1 is S. Refill the missing Embarked data with S.\n\n**2\\. Engineering New Features**\n\n**Travel group size**\n\nIt\u2019d be interesting to group people by the ticket number.\n\nSurvival rate by travel size\n\nNote that the survival rate increased as the travel size increased except for the traveling group with 5+ people.\n\nNext, use SibSp and Parch to double check if the travel size variable is defined correctly.\n\nThe number of people traveling together (travel group size) should be \u2265 the number of siblings / spouses aboard (SibSp) + 1. Here we can see that this was not the case for a few passengers. This indicated that a few passengers traveled together as a family but they didn\u2019t purchase the same ticket numbers.\n\nUpdate the travel size based on SibSp.\n\nI updated the travel size based on Parch using the same approach.\n\n**Travel group type**\n\nI created a new variable to specify if the passenger was traveling alone, with family or with friend.\n\nSurvival rate by travel type\n\n**Title**\n\nThere might be some information in the Name variable. I extracted the title from Name and created a new variable called \"title\".\n\nPassenger count by title\n\nGroup all the other titles except for the top 4 into an \"other\" category.\n\nSurvival rate by title group\n\nThe title group variable is probably highly correlated with Sex and age bin.\n\n**3\\. One-Hot Encoding**\n\nOne-hot encoding is used to convert categorical variables that contain label values (e.g. male, female) to numeric values (0 or 1).\n\nFirst, let\u2019s drop the columns that won\u2019t be used in modeling. I dropped SibSp and Parch because they are probably be highly correlated with travel size.\n\nExample data in df\\_modelData types\n\nNext, apply the one-hot encoding to columns that have the object type.\n\nI dropped some columns to prevent high correlations in the data. Using Sex as an example, keeping Sex\\_female only would be sufficient \u2013 if Sex\\_female = 0, then that indicates the gender is male.\n\nExample data in data\\_dummies\n\n**4\\. Correlation**\n\nI used Cramer\u2019s V correlation given that the variables are either nominal or ordinal.\n\nCorrelation heatmap\n\nIt\u2019s not surprising to see that travel type and travel size variables are highly correlated. I decided to drop the travel type variables.\n\n**5\\. Modeling**\n\nFinally, the data is ready for modeling!\n\nData shape for training and testing data\n\nFit the training data for a selected list of models. Use cross validation to calculate the accuracy scores.\n\nModel accuracy scores\n\nRandom Forest gave the highest accuracy score \u2013 ~84%.\n\nVisualize the feature coefficients using the Logistic Regression model to get a sense of how the model works.\n\nFeature coefficients in Logistic Regression modelFeature coefficients to predict survival status\n\nAs to the feature coefficients, female passengers were much more likely to survive compared to the male passengers, and children were much more likely to survive compared to the other age groups. On the other hand, passengers with a lower ticket class were less likely to survive compared to the other passengers. This validated the hypotheses formed from [the exploratory analysis](https://towardsdatascience.com/exploratory-data-analysis-visualization-in-python-1bf7a817bc01).\n\nThanks for reading! Feel free to leave a comment and let me know what you think.\n\nWritten By\n\nJessie Xu\n\n[See all from Jessie Xu](https://towardsdatascience.com/author/jessiexxu/)\n\n[Data Science](https://towardsdatascience.com/tag/data-science/), [Features](https://towardsdatascience.com/tag/features/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/), [Model](https://towardsdatascience.com/tag/model/), [Python](https://towardsdatascience.com/tag/python/)\n\nShare This Article\n\n- [Share on Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Ftitanic-feature-engineering-modeling-in-python-6749e6e87bf4%2F&title=Feature%20Engineering%20%26%23038%3B%20Modeling%20in%20Python)\n- [Share on LinkedIn](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Ftitanic-feature-engineering-modeling-in-python-6749e6e87bf4%2F&title=Feature%20Engineering%20%26%23038%3B%20Modeling%20in%20Python)\n- [Share on X](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Ftitanic-feature-engineering-modeling-in-python-6749e6e87bf4%2F&text=Feature%20Engineering%20%26%23038%3B%20Modeling%20in%20Python)\n\nTowards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.\n\n[Write for TDS](https://towardsdatascience.com/questions-96667b06af5/)\n\n## Related Articles\n\n- ## [Implementing Convolutional Neural N...",
      "url": "https://towardsdatascience.com/titanic-feature-engineering-modeling-in-python-6749e6e87bf4"
    },
    {
      "title": "Kaggle Challenge - Titanic Survival Prediction Feature Engineering | Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd48875ccba21&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vishal8092009%2Fin-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vishal8092009%2Fin-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Vishal Gupta](https://medium.com/@vishal8092009?source=post_page---byline--d48875ccba21---------------------------------------)\n\n10 min read\n\n\u00b7\n\nJun 2, 2020\n\n--\n\nListen\n\nShare\n\nTITANIC SURVIVAL PREDICTION\n\nIn this tutorial, we will learn about one of the most popular datasets in data science. It will give you idea about how to analyze and relate with real conditions.\n\n## The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive compared to others.\n\nIn this challenge, we will need to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n_This post will help you start with data science and familiarize yourself with Machine Learning. The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck._\n\n## A. Variable Information here\n\n**PassengerId** is the unique id of the row (no effect on target)\n\n**Survived** is the target we are trying to predict 1 : Survived 0 : Not Survived\n\n**Pclass** (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has three unique values (1, 2 or 3)\n\n**Name**, **Sex** and **Age** are self-explanatory\n\n**SibSp** is the total number of the passengers\u2019 siblings and spouse\n\n**Parch** is the total number of the passengers\u2019 parents and children\n\n**Ticket** is the ticket number of the passenger\n\n**Fare** is the passenger fare\n\n**Cabin** is the cabin number of the passenger\n\n**Embarked** is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n\n```\nC = CherbourgQ = QueenstownS = Southampton\n```\n\n## Variable Notes\n\n**Pclass:** 1st = Upper 2nd = Middle 3rd = Lower\n\n**SibSp:** The dataset defines relationship as, Sibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**Parch:** The dataset defines relationship as, Parent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n## Below are the few questions we would like to answer after our analysis\n\n- Is there any relation between given info of passengers and their survival?\n- What is the survival rate for different age groups?\n- Was preference given to women and children for saving?\n- Did having higher social status help people improve survival chances?\n- What are the effects of being alone or with family?\n- Was the survival rate affected by passenger class\n- Can we predict if a passenger survived from the disaster with using machine-learning techniques?\n\n## Let\u2019s Get Started\n\n```\n# importing basic librariesimport warningswarnings.filterwarnings(\u2018ignore\u2019)import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport plotly as plyfrom scipy import statsimport math# reading the titanic train and test datatrain = pd.read_csv(\u2018https://bit.ly/kaggletrain')test = pd.read_csv(\u2018https://bit.ly/kaggletest')train.shape # (891,12)test.shape # (418,11)\n```\n\nWe can check the data head to see what kind of variables we have. Mainly we have two kinds of variables i.e. continuous and discrete.\n\nHere Survived is our target variable, so we save it in the variable y and save the _PassengerID_ for sake of result submission on _kaggle_.\n\n```\ny , testpassenger = train[\u2018Survived\u2019], test[\u2018PassengerID\u2019]\n```\n\nWe also drop the _passenger id_, as it is unique for each record and doesn\u2019t contribute anything for survival prediction. Also concatenate the train and test into one for the ease of performing EDA and missing value operations and feature engineering.\n\n```\ntrain = train.drop(\u2018PassengerId\u2019, axis = 1)test = test.drop(\u2018PassengerId\u2019, axis = 1)total = pd.concat([train,test],axis = 0, sort=False)total = total.reset_index(drop=True)# Saving variables into different listscat_var = [\u2018Pclass\u2019,\u2019Sex\u2019,\u2019SibSp\u2019,\u2019Parch\u2019,\u2019Embarked\u2019] # categoricaltarget = [\u2018Survived\u2019] # targetcon_var = [\u2018Age\u2019,\u2019Fare\u2019] # continuous\n```\n\n## B. Visualization\n\nTo plot categorical variables:\n\n```\ndef catplot(x, df):   fig, ax = plt.subplots(math.ceil(len(x)/3), 3, figsize = (20,10))   ax = ax.flatten()   for axes, cat in zip(ax, x):   if (cat == \u2018Survived\u2019):      sns.countplot(df[cat], ax = axes)      axes.set_ylabel(\u2018Count\u2019, fontsize=12)#, weight=\u2019bold\u2019)   else:      sns.countplot(df[cat], hue=\u2019Survived\u2019, data = df, ax = axes)      axes.legend(title=\u2019Survived ?\u2019, labels = [\u2018No\u2019,\u2019Yes\u2019],                      loc= \u2018upper right\u2019)      axes.set_ylabel(\u2018Count\u2019, fontsize=12)#, weight=\u2019bold\u2019)# call the plotcatplot(target + cat_var, total)\n```\n\nPress enter or click to view image in full size\n\nCategorical Variable Plots\n\nTo plot the continuous variables:\n\n```\ndef cont(x, df):   fig, ax = plt.subplots(1,3, figsize = (18,4))   sns.distplot( df[df[x].notna()][x] , fit = stats.norm,ax =ax[0],                     bins = 12)   # fit parameter in distplot fits a function of distribution   # passed in argumentstats.probplot( df[x].fillna(df[x].median()), plot=ax[1])   sns.boxplot(y = df[x], ax = ax[2])   plt.show()# call the plotcont(\u2018Age\u2019, total)\n```\n\n```\ncont(\u2018Fare\u2019, total)\n```\n\nDensity plots:\n\nFor Age and Fare variables we make density plots, to see if they have any impact on the survival\n\n```\nplot_con = [\u2018Age\u2019,\u2019Fare\u2019]for i in plot_con:   plt.figure(figsize=(12,4))   sns.distplot(train[train[\u2018Survived\u2019]==0][i], color=\u2019r\u2019, bins = 5)   sns.distplot(train[train[\u2018Survived\u2019]==1][i], color=\u2019g\u2019, bins = 5)   plt.legend(title=\u2019Survived vs \u2018+i, labels = [\u2018Dead\u2019,\u2019Alive\u2019],                    loc=\u2019upper right\u2019)   plt.show()\n```\n\n- People with lower age survived more\n- It can be concluded that higher fare leads to better survival chance, the binning of fare might improve decision boundary.\n- However that relationship might be result of other variables also.\n\n## C. Feature Engineering\n\nNow our next steps will be towards feature engineering\n\n**1\\. Feature Engineering \u2014 Name**:\n\nAfter a glimpse we can see that name itself cannot help us in any way, so we extract new variables from it i.e. Title and Last Name.\n\n```\ndef map_title(x):   if x in [\u2018Mr\u2019]:      return \u2018Mr\u2019   elif x in [\u2018Mrs\u2019, \u2018Mme\u2019]:      return \u2018Mrs\u2019   elif x in [\u2018Miss\u2019, \u2018Mlle\u2019, \u2018Ms\u2019]:      return \u2018Miss\u2019   # Master taken separate as all have children ageel...",
      "url": "https://medium.com/@vishal8092009/in-this-tutorial-we-will-learn-about-one-of-the-most-popular-datasets-in-data-science-d48875ccba21"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "<div><div><article><p></p><h2>Titanic Survival Prediction \u2014 Kaggle Competition</h2><a href=\"#titanic-survival-prediction--kaggle-competition\"></a><p></p>\n<p>This project is my first Kaggle challenge as part of my Machine Learning Engineer learning path.\nThe goal of the competition is to predict whether a passenger survived the Titanic disaster using a dataset containing demographic and socio-economic information.\nYou can check the competition here: <a href=\"https://www.kaggle.com/competitions/titanic/\">https://www.kaggle.com/competitions/titanic/</a></p>\n<p>This repository includes:</p>\n<ul>\n<li>Exploratory Data Analysis (EDA)</li>\n<li>Feature Engineering</li>\n<li>Model Selection</li>\n<li>Training and Validation Pipelines</li>\n<li>A Kaggle-ready <code>submission.csv</code></li>\n<li>Notebook documentation</li>\n</ul>\n<hr/>\n<p></p><h2>Project Structure</h2><a href=\"#project-structure\"></a><p></p>\n<div><pre><code>\u251c\u2500\u2500 data/ # Training and test datasets\n\u251c\u2500\u2500 notebooks/\n\u2502 \u251c\u2500\u2500 01_data_exploration.ipynb\n\u2502 \u2514\u2500\u2500 02_feature_engineering_and_model.ipynb\n\u251c\u2500\u2500 submission/\n\u2502 \u2514\u2500\u2500 submission.csv # Kaggle submission file\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre></div>\n<hr/>\n<p></p><h2>Exploratory Data Analysis (EDA)</h2><a href=\"#exploratory-data-analysis-eda\"></a><p></p>\n<p>During the analysis, I explored key aspects of the dataset, including:</p>\n<ul>\n<li>Distribution of variables such as Age, Fare, Pclass, SibSp, and Parch</li>\n<li>Survival differences by Sex and Pclass</li>\n<li>Missing values and their treatment</li>\n<li>Pearson correlation to check linear relationships</li>\n<li>Visualizations using bar charts, boxplots, and heatmaps</li>\n</ul>\n<p>A major insight:\n<strong>Most features do not have strong linear relationships with the target</strong>, which is important when choosing a model.\nThis suggested that linear models would not perform well, so tree-based models were preferred.</p>\n<hr/>\n<p></p><h2>Feature Engineering</h2><a href=\"#feature-engineering\"></a><p></p>\n<p>To improve model performance, I created several new features:</p>\n<p></p><h3>Family Size</h3><a href=\"#family-size\"></a><p></p>\n<p>Combined <code>SibSp + Parch + 1</code> to represent group sizes:</p>\n<ul>\n<li><strong>Alone</strong>: 1</li>\n<li><strong>Small family</strong>: 2\u20134</li>\n<li><strong>Large family</strong>: 5+</li>\n</ul>\n<p></p><h3>Age Groups</h3><a href=\"#age-groups\"></a><p></p>\n<p>Bucketized ages into categories:</p>\n<ul>\n<li><strong>Child</strong>: 0\u201312</li>\n<li><strong>Teen</strong>: 13\u201319</li>\n<li><strong>Adult</strong>: 20\u201364</li>\n<li><strong>Senior</strong>: 65+</li>\n</ul>\n<p></p><h3>One-Hot Encoding</h3><a href=\"#one-hot-encoding\"></a><p></p>\n<p>Applied to:</p>\n<ul>\n<li><code>Sex</code></li>\n<li><code>Pclass</code></li>\n<li><code>Embarked</code></li>\n</ul>\n<p>These transformations helped the model capture non-linear and categorical patterns more effectively.</p>\n<hr/>\n<p></p><h2>Model Selection</h2><a href=\"#model-selection\"></a><p></p>\n<p>Since the dataset does not show strong linear relationships, I chose <strong>Decision Trees</strong> for the first model.</p>\n<p>Initial model:</p>\n<ul>\n<li><strong>DecisionTreeClassifier</strong></li>\n<li>Achieved ~0.798 validation accuracy</li>\n<li>Scored ~0.753 on Kaggle (public leaderboard)</li>\n</ul>\n<p>Later, I tested <strong>XGBoost</strong>, a more powerful non-linear model, and improved the score to ~0.77 on Kaggle.</p>\n<p>More improvements are planned (hyperparameter tuning, ensembles, etc.).</p>\n<hr/>\n<p></p><h2>Results</h2><a href=\"#results\"></a><p></p>\n<p>This validates that tree-based models work significantly better for this dataset.</p>\n<hr/>\n<p></p><h2>Next Steps</h2><a href=\"#next-steps\"></a><p></p>\n<ul>\n<li>Hyperparameter tuning of XGBoost</li>\n<li>Trying Random Forest and Gradient Boosting</li>\n<li>Building ensembles</li>\n<li>Applying more advanced FE (e.g., Title extraction from names)</li>\n<li>Starting the <strong>Spaceship Titanic</strong> challenge</li>\n</ul>\n<hr/>\n<p></p><h2>Usage</h2><a href=\"#usage\"></a><p></p>\n<p>Run the notebooks in order:</p>\n<ol>\n<li><code>01_data_exploration.ipynb</code></li>\n<li><code>02_feature_engineering_and_model.ipynb</code></li>\n</ol>\n<p>Then generate your <code>submission.csv</code> and upload it to Kaggle.</p>\n<hr/>\n<p></p><h2>Author</h2><a href=\"#author\"></a><p></p>\n<p><strong>Paolo Giovanni Veliz Sulecio</strong>\nMachine Learning Engineer in progress\nPassionate about data, automation, and real-world problem solving.</p>\n<hr/>\n<p></p><h2>Support</h2><a href=\"#support\"></a><p></p>\n<p>If you found this useful, feel free to:</p>\n<ul>\n<li>Star the repository</li>\n<li>Fork it</li>\n<li>Connect with me on LinkedIn</li>\n</ul>\n</article></div></div>",
      "url": "https://github.com/PaoloVeliz/titanic-survival-prediction-ml"
    },
    {
      "title": "\ud83d\udea2 Titanic Survival Prediction using XGBoost - hackerdawn - Medium",
      "text": "<div><div><div><h2>\ud83d\udea2 Titanic Survival Prediction using XGBoost</h2><div><a href=\"https://medium.com/@sidharthpandita?source=post_page---byline--ed2ceea3a1c7---------------------------------------\"><div><p></p></div></a></div></div><figure><figcaption>Photo by <a href=\"https://unsplash.com/@alonsoreyes?utm_source=medium&amp;utm_medium=referral\">Alonso Reyes</a> on <a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>The Titanic struck an iceberg on 15 April 1912 that caused its sinking, leading to the death of more than 1,500 people. This made it one of the deadliest sinking of a single ship. We will try to predict whether a particular person on Titanic survived or not using 11 features about them. For this purpose, we will use the <a href=\"https://www.kaggle.com/c/titanic\">Titanic</a> dataset from Kaggle.</p><h2>Importing Libraries</h2><p>Let\u2019s first import the required libraries. If you don\u2019t have a particular library installed, run the command <em>\u2018pip install &lt;package_name&gt;\u2019</em> to install it.</p><pre><span>import os<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.model_selection import train_test_split<br/>from xgboost import XGBClassifier<br/>from sklearn.model_selection import GridSearchCV</span></pre><h2>Loading the Data</h2><p>We\u2019ll have downloaded the data from Kaggle and unzipped it in a directory named <em>titanic</em>. Let\u2019s see what went inside the directory where we unzipped the data.</p><pre><span>for dirname, _, filenames in os.walk('./titanic'):<br/> for filename in filenames:<br/> print(filename)</span></pre><figure><figcaption>Filenames</figcaption></figure><p>We will load the train and test dataset into our notebook.</p><pre><span>train = pd.read_csv('./titanic/train.csv')<br/>test = pd.read_csv('./titanic/test.csv')</span></pre><h2>Exploring the Data</h2><p>We\u2019ll print the head of <em>train</em> to see how the dataset looks like.</p><pre><span>train.head()</span></pre></div><div><p>We\u2019ll print the shape of the train and test datasets to see their dimensions.</p><pre><span>print(train.shape)<br/>print(test.shape)</span></pre><figure><figcaption>Shape of train &amp; test</figcaption></figure><p>We will drop the columns <em>\u2018PassengerId\u2019</em>, <em>\u2019Name\u2019</em>, <em>\u2019Ticket\u2019</em>, <em>\u2019Cabin\u2019 </em>because they do not play any significant role in the survival of a person.</p><pre><span>train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)<br/>test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)</span></pre><p>Let\u2019s see the head of <em>train</em> dataframe now.</p><pre><span>train.head()</span></pre><figure><figcaption>After dropping the columns</figcaption></figure><p>We will the <em>info()</em> method to see information like non-null counts and data types for the columns.</p><pre><span>train.info()</span></pre><figure><figcaption>Information about train columns</figcaption></figure><p>Let\u2019s now see some stats about the numeric columns using <em>describe()</em>.</p><pre><span>train.describe()</span></pre><figure></figure><p>We\u2019ll now get the number of null values in each column.</p><pre><span>train.isna().sum()</span></pre><figure><figcaption>Column-wise Null values Count</figcaption></figure><h2>Visualizing the Data</h2><p>Let\u2019s visualize the null values in different columns using a heatmap. We can observe in the output that the column<em> \u2018Age\u2019</em> has many null values.</p><pre><span><strong>#Plotting null values heatmap</strong><br/>sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')</span></pre><figure><figcaption>Column-wise Null value Heatmap</figcaption></figure><p>We will plot a heatmap to see the correlation between different features. The higher the number in the block, the higher is the correlation.</p><pre><span><strong>#Plotting correlation heatmap</strong><br/>corr = train.corr()<br/>corr = corr.apply(abs)<br/>plt.figure(figsize=(7,7))<br/>map = sns.heatmap(corr,annot=True,cmap=\"RdYlGn\")</span></pre><figure><figcaption>Feature Correlation Heatmap</figcaption></figure><p>Let\u2019s see the survived v/s died percentage by plotting a pie chart.</p><pre><span><strong>#Plotting the Pie chart</strong><br/>fig = plt.figure(figsize=(5,5))<br/>colors = [\"skyblue\",'pink']<br/>surv = train[train['Survived']==1]<br/>died = train[train['Survived']==0]<br/>ck = [surv['Survived'].count(),died['Survived'].count()]<br/>piechart = plt.pie(ck,labels=[\"Survived\",\"Died\"],<br/>autopct ='%1.1f%%',<br/>shadow = True,<br/>colors = colors,<br/>startangle = 45,<br/>explode=(0, 0.1))<br/>train.Survived.value_counts()</span></pre><figure><figcaption>Percentage of Survived v/s Died</figcaption></figure><p>Let\u2019s see the distribution of the <em>\u2018Age\u2019 </em>column<em> </em>using a histogram. The distribution looks right-skewed.</p><pre><span>sns.histplot(train['Age'].dropna())</span></pre><figure><figcaption>Distribution of \u2018Age\u2019</figcaption></figure><p>Let\u2019s see the distribution of the <em>\u2018Parch\u2019 </em>column.</p><pre><span>sns.histplot(train['Parch'].dropna(),bins=15)</span></pre><figure><figcaption>Distribution of \u2018Parch\u2019</figcaption></figure><p>Let\u2019s see the distribution of the <em>\u2018Pclass\u2019 </em>column.</p><pre><span>sns.histplot(train['Pclass'].dropna(),bins=15)</span></pre><figure><figcaption>Distribution of \u2018Pclass\u2019</figcaption></figure><p>Let\u2019s see the distribution of the <em>\u2018Fare\u2019 </em>column. The distribution looks right-skewed.</p><pre><span>sns.histplot(train['Fare'].dropna(),bins=20)</span></pre><figure><figcaption>Distribution of \u2018<em>Fare\u2019</em></figcaption></figure><p>Let\u2019s see the distribution of the <em>\u2018Sex\u2019 </em>column.</p><pre><span>sns.countplot(train['Sex'].dropna())</span></pre><figure><figcaption>Count of Males v/s Females</figcaption></figure><h2>Feature Engineering</h2><p>We\u2019ll start off the feature engineering by taking care of the null values. We will first replace the null values in the <em>\u2018Age\u2019</em> column with the column mean.</p><pre><span><strong>#Replacing null values by column mean</strong><br/>train['Age'].fillna(train['Age'].mean(),inplace=True)<br/>test['Age'].fillna(test['Age'].mean(),inplace=True)</span></pre><p>We\u2019ll now replace the 0\u2019s and nulls in the <em>\u2018Fare\u2019</em> column with the column mean.</p><pre><span><strong>#Replacing 0\u2019s &amp; nulls by column mean</strong><br/>train['Fare'] = train['Fare'].replace(0, train['Fare'].mean())<br/>train['Fare'].replace('nan',np.nan,inplace=True)<br/>train['Fare'].fillna(train['Fare'].mean(),inplace=True)</span><span>test['Fare'] = test['Fare'].replace(0, test['Fare'].mean())<br/>test['Fare'].replace('nan',np.nan,inplace=True)<br/>test['Fare'].fillna(test['Fare'].mean(),inplace=True)</span></pre><p>We will replace the null values in the <em>\u2018Embarked\u2019</em> column with the column mean.</p><pre><span><strong>#Replacing nulls by column mean</strong> <br/>train['Embarked'].replace('nan',np.nan,inplace=True)<br/>train['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)</span><span>test['Embarked'].replace('nan',np.nan,inplace=True)<br/>test['Embarked'].fillna(test['Embarked'].mode()[0],inplace=True)</span></pre><p>Let\u2019s print the unique values in the <em>\u2018Sex\u2019</em> column. As you can see in the output, there are 2 unique values in this column.</p><pre><span><strong>#Printing unique values in 'Sex' column</strong><br/>train['Sex'].unique()</span></pre><figure><figcaption>Unique values in \u2018Sex\u2019</figcaption></figure><p>We\u2019ll map the categorical values in the <em>\u2018Sex\u2019</em> column to numeric values (0 &amp; 1).</p><pre><span><strong>#Mappping categorical to numeric</strong><br/>train['Sex']=train['Sex'].map({'male':0,'female':1})<br/>test['Sex']=test['Sex'].map({'male':0,'female':1})</span></pre><p>Let\u2019s print the unique values in the <em>\u2018Embarked\u2019</em> columns. As you can see in the output, there are 3 unique values in this column.</p><pre><span><strong>#Printing unique values ...",
      "url": "https://medium.com/hackerdawn/titanic-survival-prediction-using-xgboost-ed2ceea3a1c7"
    },
    {
      "title": "How to Predict %80 Accuracy in the Titanic Disaster Competition",
      "text": "<div><div><div><a href=\"https://medium.com/@ozzgur.sanli?source=post_page---byline--762f5c0f4bfb---------------------------------------\"><div><p></p></div></a></div><p>In this post, I will explain how to achieve accuracy over 80% in Kaggle\u2019s Titanic Disaster Competition by using data manipulation, feature engineering techniques, and machine learning algorithms. Even though achieving the highest accuracy is our main objective, I believe this project is highly \"core\" for people who are new to data science or simply want to refresh their memory on the subject.</p><figure></figure><p>For those who do not know about Kaggle\u2019s competition, let me quickly inform you that our goal is to predict the survivors of the Titanic disaster. On Kaggle's page, you can find comprehensive information about this subject. This page includes graphs that provide a detailed description of the data set's attributes.</p><p>First of all, we will quickly examine the data set (EDA), and then we will determine if there are outliers. We will fill in any missing data, if there are any. We interpret the notable results based on the correlations found in the dataset between the independent and dependent variables. A machine-learning model is going to be developed based on these findings. The process is displayed below.</p><figure><figcaption>The workflow of Project</figcaption></figure><p>Let\u2019s begin by importing the packages we need.</p><pre><span>This Python 3 environment comes with many helpful analytics libraries installed<br/># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python<br/>For example, here are several helpful packages to load<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>from datetime import date<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/>import joblib<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, and AdaBoostClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.svm import SVC<br/>from sklearn.tree import DecisionTreeClassifier<br/>from xgboost import XGBClassifier<br/>import warnings<br/>warnings.filterwarnings(\"ignore\")<br/># Input data files are available in the read-only \"../input/\" directory<br/>For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory<p>import os<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/> for filename in filenames:<br/> print(os.path.join(dirname, filename))</p></span></pre><p>Now load the dataset. There\u2019re 2 datasets: one is the \u201ctrain.csv\u201d dataset, which has the \u201cSurvived\u201d variable filled with 1\u20130, and the other is the \"test.csv,\" from which we will predict the survived ones.</p><pre><span>train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")<br/>test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")<br/>df = pd.concat([train, test], axis=0)<br/></span></pre><h2><strong>Exploratory Data Analysis</strong></h2><p>For exploratory data analysis, we will use .head(), info(), describe() functions.</p><pre><span>df.head()<br/>Out[4]: <br/> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked<br/>0 1 0.000 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S<br/>1 2 1.000 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C<br/>2 3 1.000 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S<br/>3 4 1.000 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S<br/>4 5 0.000 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S</span></pre><pre><span>df.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>Int64Index: 1309 entries, 0 to 417<br/>Data columns (total 12 columns):<br/> # Column Non-Null Count Dtype <br/>--- ------ -------------- ----- <br/> 0 PassengerId 1309 non-null int64 <br/> 1 Survived 891 non-null float64<br/> 2 Pclass 1309 non-null int64 <br/> 3 Name 1309 non-null object <br/> 4 Sex 1309 non-null object <br/> 5 Age 1046 non-null float64<br/> 6 SibSp 1309 non-null int64 <br/> 7 Parch 1309 non-null int64 <br/> 8 Ticket 1309 non-null object <br/> 9 Fare 1308 non-null float64<br/> 10 Cabin 295 non-null object <br/> 11 Embarked 1307 non-null object <br/>dtypes: float64(3), int64(4), object(5)<br/>memory usage: 132.9+ KB</span></pre><pre><span>df.describe().T<br/>Out[5]: <br/> count mean std min 25% 50% 75% max<br/>PassengerId 1309.000 655.000 378.020 1.000 328.000 655.000 982.000 1309.000<br/>Survived 891.000 0.384 0.487 0.000 0.000 0.000 1.000 1.000<br/>Pclass 1309.000 2.295 0.838 1.000 2.000 3.000 3.000 3.000<br/>Age 1046.000 29.881 14.413 0.170 21.000 28.000 39.000 80.000<br/>SibSp 1309.000 0.499 1.042 0.000 0.000 0.000 1.000 8.000<br/>Parch 1309.000 0.385 0.866 0.000 0.000 0.000 0.000 9.000<br/>Fare 1308.000 33.295 51.759 0.000 7.896 14.454 31.275 512.329</span></pre><p>Considering that you have reviewed the data before, I am going through these parts a little fast because of not extending the reading time.</p><p>In order to visually examine the dataset, we will first divide the columns of the data into numerical columns and categorical columns. Thus, we can more easily show the relationship of these columns with the \u201cSurvived\u201d variable, that is, the target variable.</p><p>We use the function \u201cgrab_col_names()\u201d to separate the columns according to their types. This function separates the columns in the data into numerical, categorical, numerical but originally categorical, and categorical but originally cardinal.</p><pre><span>def grab_col_names(dataframe, cat_th=10, car_th=20):<br/> \"\"\"<br/> It provides the names of categorical, numerical, and categorical but cardinal variables in the dataset.<br/> Note: Numerical-looking categorical variables are also included in the categorical variables.<p> Parameters<br/> ------<br/> dataframe: dataframe<br/> The dataframe from which variable names are to be retrieved<br/> cat_th: int, optional<br/> Threshold value for variables that are numerical but categorical.<br/> car_th: int, optinal<br/> Threshold value for variables that are categorical but cardinal.<br/> Returns<br/> ------<br/> cat_cols: list<br/> Categorical variable list<br/> num_cols: list<br/> Numeric variable list<br/> cat_but_car: list<br/> List of categorical variables that appear to be cardinal</p><p> Examples<br/> ------<br/> import seaborn as sns<br/> df = sns.load_dataset(\"iris\")<br/> print(grab_col_names(df))</p><p> Notes<br/> ------<br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> num_but_cat is within cat_cols<br/> The sum of the 3 lists returned equals the total number of variables: <br/> cat_cols + num_cols + cat_but_car = total number of variables<br/> \"\"\"</p><p> # cat_cols, cat_but_car<br/> cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]<br/> num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and<br/> dataframe[col].dtypes != \"O\"]<br/> cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and<br/> dataframe[col].dtypes == \"O\"]<br/> cat_cols = cat_cols + num_but_cat<br/> cat_cols = [col for col in cat_cols if col not in cat_but_car]</p><p> # num_cols<br/> num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]<br/> num_cols = [col for col in num_cols if col not in num_but_cat]</p><p> print(f\"Observations: {dataframe.shape[0]}\")<br/> print(f\"Variables: {dataframe.shape[1]}\")<br/> print(f'cat_cols: {len(cat_cols)}')<br/> print(f'num_cols: {len(num_cols)}')<br/> print(f'cat_but_car: {len...",
      "url": "https://python.plainenglish.io/how-to-predict-80-accuracy-in-titanic-disaster-competition-762f5c0f4bfb?gi=189b98032ad3"
    },
    {
      "title": "Kaggle Titanic Challenge: Features Creation",
      "text": "Kaggle Titanic Challenge: Create New Features Using Extracted Data | Python in Plain English\n[Sitemap](https://python.plainenglish.io/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Python in Plain English\n](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-a324ba577812---------------------------------------)\n\u00b7Follow publication\n[\n![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)\n](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-a324ba577812---------------------------------------)\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\nFollow publication\n# Kaggle Titanic Challenge: Features Creation\n## Better Data, Better Model!\n[\n![Aman Krishna](https://miro.medium.com/v2/resize:fill:64:64/1*l76xlm-uG_86SWPTz0iYqw.jpeg)\n](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n[Aman Krishna](https://medium.com/@mnkrishn?source=post_page---byline--a324ba577812---------------------------------------)\n5 min read\n\u00b7Apr 7, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/python-in-plain-english/a324ba577812&amp;operation=register&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;user=Aman+Krishna&amp;userId=bda5c8d97e1a&amp;source=---header_actions--a324ba577812---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/a324ba577812&amp;operation=register&amp;redirect=https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812&amp;source=---header_actions--a324ba577812---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\n![]()\nPhoto by[Glen Carrie](https://unsplash.com/@glencarrie?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)## Overview\nHey folks,\nIn the previous section of[**EDA**](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941),****we looked at the distribution of different features. And their relationship with the prediction label,**Survived**, as well as among each other.\nWe noticed that some of the features like**Name**contain extra information about a passenger\u2019s**title**which can be useful for our model.\n**In this section,**we will extract useful details from different features and create new features. Let\u2019s get started!\nYou can find the complete notebook here:\n[\n## kaggle\\_titanic/feature\\_engineering.ipynb at main \u00b7AmanKrishna/kaggle\\_titanic\n### Machine Learning model build for survivor prediction for Kaggle&#x27;s titanic competition \u2026github.com\n](https://github.com/AmanKrishna/kaggle_titanic/blob/main/notebooks/feature_engineering.ipynb?source=post_page-----a324ba577812---------------------------------------)\nFirst, let\u2019s import and combine our train &amp; test data to avoid redoing the same steps for both separately.\n### Code\n## **New Feature:**Family Name\nThe**Name**feature looks something like this:\nPress enter or click to view image in full size\n![]()\nFamily Names\n**Family Name**followed by the rest of the name. We can retrieve the family name and add it as a new feature.\n### Why Family Name?\n*Hypothesis:*If some members of a family survive then chances are others will too.\n### Code\n## New Feature: Title\nEach passenger has a title in our database (**Mr./Mrs./Master**etc.). We can extract them and add them as a new feature**Title**.\nPress enter or click to view image in full size\n![]()\nTitle associated with Name\nLet\u2019s group some of these Titles together using the below dictionary:\nTitle Dictionary\nHere is the final list of titles we will be using:\n![]()\nList of Titles### Why Title?\nLooking at the below graph we can see that survival probability changes with Title\nPress enter or click to view image in full size\n![]()\nTitle vs Survival### **Code**\n## New Feature: Cabin Section\nIf we look at the**Cabin**feature we can see that each cabin starts with an alphabet. These could be**Cabin sections**similar to the ones we have on trains or flights.\nPress enter or click to view image in full size\n![]()\nCabin Starts with Alphabet\nAfter extracting and combining a few we get the following set of**Cabin Sections**\n![]()\nSet of Cabins### Code\n## New Feature: Family Size\nIn[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)we discovered that**family size**played a pivotal role in determining survival.\nPress enter or click to view image in full size\n![]()\nFamily Size vs Survival Rate\nLet\u2019s create a new feature**Family\\_Size**by summing**Sibsp**&amp;**Parch**features.\n### Code\n## New Feature: Family Size Grouped\nIn this feature we will assigning group names to passengers based on their Family Group Size using the following code\n### Code\n### Why grouping?\nWe will be using**Boosting and Bagging Tree-**based models for classification. Binning family sizes into groups will help our model learn better.\n## New Feature: Grouping Ticket\nDuring[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)we noticed that the number of Unique**Tickets**was 681. This means that some passengers are sharing the same ticket numbers. Grouping them based on the number of**passengers per ticket**we get the following survival rates:\nPress enter or click to view image in full size\n![]()\nTicket\\_Group\\_Size vs Survival### Why group tickets?\n*Hypothesis:*The same ticket will be shared among the family group as well as the friends group.\n### Code\n## Age\\_Bin &amp;&amp; Fare\\_Bin\nWe have observed during[EDA](https://medium.com/@mnkrishn/kaggle-titanic-challenge-know-thy-data-8f1dae4de941)that Survival probability varied across**Age**and**Fare**segments. Let\u2019s bin them into separate features.\nPress enter or click to view image in full size![]()\nPress enter or click to view image in full size![]()\nAge/Fare Bins vs Survival Rates\n### Why Binning?\nWe will be using**Boosting and Bagging Tree-**based models for classification. Using Bins will help our model learn better. I will be discussing the rationale for using these models then.\n## GetAman Krishna\u2019s stories in\u00a0your\u00a0inbox\nJoin Medium for free to get updates from\u00a0this\u00a0writer.\nSubscribe\nSubscribe\n**Note:**The**age**feature is missing for many passengers. We will be updating their bin values in the next section where we will be handling missing values.\n### Code\n## New Feature: Survival Rates\nThis feature will contain the survival probability for each**Ticket**,**Family Name**&amp;**Cabin number**.**We will create 3 new features: Ticket\\_Survival\\_Rate, Cabin\\_Survival\\_R...",
      "url": "https://python.plainenglish.io/kaggle-titanic-challenge-create-them-features-a324ba577812"
    },
    {
      "title": "Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy.",
      "text": "Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy. | by Devang Chavda | Python in Plain English\n[Sitemap](https://python.plainenglish.io/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Python in Plain English\n](https://python.plainenglish.io/?source=post_page---publication_nav-78073def27b8-f090e722fd42---------------------------------------)\n\u00b7Follow publication\n[\n![Python in Plain English](https://miro.medium.com/v2/resize:fill:76:76/1*VA3oGfprJgj5fRsTjXp6fA@2x.png)\n](https://python.plainenglish.io/?source=post_page---post_publication_sidebar-78073def27b8-f090e722fd42---------------------------------------)\nNew Python content every day. Follow to join our 3.5M+ monthly readers.\nFollow publication\n# Data pre-processing of spaceship-titanic Kaggle dataset for achieving 80+% accuracy.\n[\n![Devang Chavda](https://miro.medium.com/v2/resize:fill:64:64/1*-gyTgptyecYNtM89jd5EKA.jpeg)\n](https://medium.com/@chavdadevang23?source=post_page---byline--f090e722fd42---------------------------------------)\n[Devang Chavda](https://medium.com/@chavdadevang23?source=post_page---byline--f090e722fd42---------------------------------------)\n7 min read\n\u00b7May 20, 2023\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/python-in-plain-english/f090e722fd42&amp;operation=register&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;user=Devang+Chavda&amp;userId=1099c9234a27&amp;source=---header_actions--f090e722fd42---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/f090e722fd42&amp;operation=register&amp;redirect=https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42&amp;source=---header_actions--f090e722fd42---------------------bookmark_footer------------------)\nListen\nShare\nData cleaning and feature engineering are crucial steps in the data pre-processing pipeline that significantly impact the quality and effectiveness of analytical models.\nPress enter or click to view image in full size\n![]()\nPhoto by[Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)\n[Competition Link](https://www.kaggle.com/competitions/spaceship-titanic)/[Dataset Link](https://www.kaggle.com/competitions/spaceship-titanic/data)\nPress enter or click to view image in full size\n![Leaderboard position top 20%]()\nScored 80.336%, Top 20% on leader-board (Image by Author)\nAlready got the preprocessed data and want to directly jump into the model-building part here is the part-2 of this article.\n[\n## Building a Classification Model To Score 80+% Accuracy on the Spaceship Titanic Kaggle Dataset\n### This article will walk you through detailed forward feature selection steps and model building from scratch, improving\u2026\npub.towardsai.net\n](https://pub.towardsai.net/building-a-classification-model-to-score-80-accuracy-on-the-spaceship-titanic-kaggle-dataset-40a0aa184c1c?source=post_page-----f090e722fd42---------------------------------------)\nLet&#x27;s start with exploring the dataset. Below are the features we have, and the definition of each feature you can find[here](https://www.kaggle.com/competitions/spaceship-titanic/data).\n```\n[&#x27;PassengerId&#x27;, &#x27;HomePlanet&#x27;, &#x27;CryoSleep&#x27;, &#x27;Cabin&#x27;, &#x27;Destination&#x27;, &#x27;Age&#x27;,\n&#x27;VIP&#x27;, &#x27;RoomService&#x27;, &#x27;FoodCourt&#x27;, &#x27;ShoppingMall&#x27;, &#x27;Spa&#x27;, &#x27;VRDeck&#x27;,\n&#x27;Name&#x27;, &#x27;Transported&#x27;]\n```\n```\ndf[&#x27;&#x27;Transported&#x27;&#x27;].value\\_counts() # balanced dataset\n# True 4378\n# False 4315\n```\nThe target class is well balanced here..! as this is a practice dataset otherwise in real-life data this is often not true.\nThe below function will help us to know what all unique values a categorical column contains and its count and portion in the dataset, it can give us a good understanding of the categorical features and we could also find some features with imbalance or features with low variance.\n```\ndef showDetails(column):\nprint(&#x27;------------------------------------------&#x27;)\nprint(column +&#x27; &amp; TRANSPORTED&#x27;)\nprint(df[column].value\\_counts())\ntempDict = dict(df[column][df[&#x27;&#x27;Transported&#x27;&#x27;] == True].value\\_counts())\nfor i in tempDict.keys():\ntempDict[i] = (tempDict[i]/len(df[df[column] == i]))\\*100\nprint(tempDict)\nprint(&#x27;------------------------------------------&#x27;)\nshowDetails(&#x27;HomePlanet&#x27;)\nshowDetails(&#x27;CryoSleep&#x27;)\nshowDetails(&#x27;Destination&#x27;)\nshowDetails(&#x27;VIP&#x27;)\n```\n```\nOutput :\nHomePlanet &amp; TRANSPORTED\nEarth 4602\nEuropa 2131\nMars 1759\nName: HomePlanet, dtype: int64\n{&#x27;Earth&#x27;: 42.39461103867884, &#x27;Europa&#x27;: 65.884561238855, &#x27;Mars&#x27;: 52.30244457077885}\n------------------------------------------\n------------------------------------------\nCryoSleep &amp; TRANSPORTED\nFalse 5439\nTrue 3037\nName: CryoSleep, dtype: int64\n{True: 81.75831412578202, False: 32.892075749218606}\n------------------------------------------\n------------------------------------------\nDestination &amp; TRANSPORTED\nTRAPPIST-1e 5915\n55 Cancri e 1800\nPSO J318.5-22 796\nName: Destination, dtype: int64\n{&#x27;TRAPPIST-1e&#x27;: 47.11749788672866, &#x27;55 Cancri e&#x27;: 61.0, &#x27;PSO J318.5-22&#x27;: 50.37688442211056}\n------------------------------------------\n------------------------------------------\nVIP &amp; TRANSPORTED\nFalse 8291\nTrue 199\nName: VIP, dtype: int64\n{False: 50.63321674104451, True: 38.19095477386934}\n```\nExample:{\u2018Earth\u2019: 42.39461103867884, \u2018Europa\u2019: 65.884561238855, \u2018Mars\u2019: 52.30244457077885}\nso, 42.4% of passengers from Earth here have been transported successfully.\nLet us see if there exists any geometric pattern in the data, which can give us a clue on which type of feature engineering or which type of models will perform better for this dataset. Will be using t-sne for visualization.\n```\ndef plot\\_tsne(X,y,p=30,step=1000):\n&quot;&quot;&quot;\nX-features number (pandas dataframe)\ny-targets int (pandas series)\np-perplexity - points in neighourhood\nstep-Maximum number of iterations for the optimization. Should be at least 250.\n&quot;&quot;&quot;\ntargets = y.astype(int)\ntsne = manifold.TSNE(n\\_components=2,perplexity=p,n\\_iter=step ,random\\_state=42)\ntransformed\\_data = tsne.fit\\_transform(X.iloc[:100,:])\ntsne\\_df = pd.DataFrame(np.column\\_stack((transformed\\_data, targets[:100])),columns=[&quot;&quot;x&quot;&quot;, &quot;&quot;y&quot;&quot...",
      "url": "https://python.plainenglish.io/data-pre-processing-of-spaceship-titanic-kaggle-dataset-for-achieving-80-accuracy-f090e722fd42"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    }
  ]
}