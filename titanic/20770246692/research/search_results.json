{
  "query": "How do top Kaggle Titanic solutions reduce overfitting and improve generalization to test set?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Titanic notebooks keep the model from memorising the training data by combining rigorous validation, careful regularisation, smart feature work and ensembles.\u202fThe most\u2011effective tricks mentioned across leading write\u2011ups (e.g., Safjan\u2019s over\u2011fitting guide and Coffey\u2019s 2025 GBT notebook) are:\n\n1. **Stratified K\u2011fold cross\u2011validation** \u2013 split the training set into multiple folds (often 5\u2011 or 10\u2011fold) that preserve the survival rate, and evaluate every model on out\u2011of\u2011fold predictions. This gives a reliable estimate of how the model will behave on unseen data.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\u202f([Coffey](https://www.jamescoffey.co/post/kick-start-tree-ensembles-without-the-titanic-cliches))\n\n2. **Early stopping on a validation fold** \u2013 while training tree\u2011based models (e.g., Gradient\u2011Boosted Trees), monitor the validation loss and stop adding trees as soon as the score stops improving. This prevents the model from fitting noise.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n3. **Regularisation of tree learners** \u2013 limit depth, set `min_child_weight`, apply L1/L2 penalties, and use subsampling/column\u2011sampling. These constraints reduce variance and keep the model from becoming overly complex.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n4. **Feature engineering & selection** \u2013 extract high\u2011value signals (e.g., ticket number patterns, cabin deck, family size, title from name) and drop noisy or redundant columns. Selecting only the most predictive features lowers dimensionality and curbs overfitting.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n5. **Adversarial validation** \u2013 train a classifier to distinguish the original training rows from the Kaggle test rows. Features that the classifier uses strongly indicate a distribution shift; those can be down\u2011weighted, transformed, or removed to improve generalisation.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\n\n6. **Ensembling (bagging, stacking, simple averages)** \u2013 combine several diverse models (e.g., Random Forest, LightGBM, Logistic Regression) and average their predictions or stack them with a meta\u2011learner. Ensembles reduce variance and smooth out idiosyncratic errors.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\u202f([Coffey](https://www.jamescoffey.co/post/kick-start-tree-ensembles-without-the-titanic-cliches))\n\n7. **Bayesian / automated hyper\u2011parameter optimisation** \u2013 use tools like Optuna\u2019s TPE sampler to search the hyper\u2011parameter space efficiently, guided by cross\u2011validation scores. This finds well\u2011regularised settings without manual trial\u2011and\u2011error.\u202f([Safjan](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions))\u202f([Coffey](https://www.jamescoffey.co/post/kick-start-tree-ensembles-without-the-titanic-cliches))\n\nApplying these steps together\u2014robust validation, early stopping, regularised models, thoughtful feature work, detection of train\u2011test drift, diverse ensembles, and systematic hyper\u2011parameter tuning\u2014has been shown to cut over\u2011fitting and boost the final Kaggle Titanic leaderboard score.",
      "url": ""
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "Kick-start Tree Ensembles without the Titanic Clich\u00e9s",
      "text": "Portfolio\n[![](https://cdn.prod.website-files.com/67e49e5ed9ba284f530202fb/67e49e5ed9ba284f530203b8_b1b1665d97d6f2f297d1729970698d27_logo.png)](https://www.jamescoffey.co/)\n[\u2039 Back to Portfolio](https://www.jamescoffey.co/)\n# Kick-start Tree Ensembles without the Titanic Clich\u00e9s\nA hands-on guide to tuning Gradient-Boosted Trees, preventing overfitting, and demystifying ensembles.\n![](https://cdn.prod.website-files.com/67e49e5ed9ba284f5302035c/6876ab35449533d0ff569b27_decision%20trees%20titanic.png)\n**Published:****Jul 15, 2025**\nBuilding predictive models on the famous Kaggle*Titanic*dataset is a rite of passage for many ML beginners. Most tutorials use Random Forests or even logistic regression as \u201cjust works\u201d solutions.*Gradient-Boosted Trees*(GBTs) often get a bad rap in this toy example: out of the box they can easily**overfit**the small Titanic data. However, you might miss out on better accuracy if you avoid boosting. You*can*use GBTs \u2014but only with extra care. In this article I\u2019ll show how to cross-validate safely, use automated tuning (Optuna\u2019s TPE sampler), and even try simple ensembles. Along the way I\u2019ll clarify key ensemble concepts and evaluation metrics that many beginners overlook. Let\u2019s dive in step-by-step.\n***Want to code along?***\n*I\u2019ve published the****full Jupyter notebook****\u2014 data prep, Optuna tuning, SHAP plots, and Kaggle-ready submission \u2014right here:*\n[*Complete Titanic GBT Notebook*](https://github.com/JamesFCoffey/ml-ds-techniques/blob/main/notebooks/titanic.ipynb)\n*Feel free to fork it as you read.*\n# Random Forest vs. Gradient Boosted Trees on Small Data\nTree ensembles are powerful, but not all are created equal.**Random Forests**use*bagging.*They train many deep trees on random subsets of data and average their votes to reduce variance. In contrast,**Gradient Boosted Trees**build trees*sequentially.*Each one correcting the errors of the previous ones. Because boosting adds trees greedily, a GBT model can fit noise in a small dataset. In fact, the[scikit-learn docs](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator)note that bagging methods reduce overfitting. They work best with*strong*base models. Boosting methods work best with*weak*learners (shallow trees).\nOn the Titanic (891 training rows), this means a default Random Forest often \u201cjust works\u201d out of the box, while a default GBT might overfit. More training data can itself act as a form of regularization, so with more data boosting tends to shine. But on small data you must add*regularization*(like shallow trees, subsampling, shrinkage) manually. In practice, you can use GBT on Titanic if you carefully tune its hyperparameters and maybe use techniques like early stopping.\n*For more tips on tree ensembles, model tuning, and Kaggle techniques,****follow me on X***[***@CoffeyFirst***](https://twitter.com/CoffeyFirst)*.*\n# Avoiding Data Leakage with Cross-Validation\nTo truly guard against overfitting, use**cross-validation**with a proper pipeline. Always**fit preprocessing steps on the training fold only, then apply to the validation fold.**Never use information from the held-out set when you train. The[scikit-learn docs](https://scikit-learn.org/stable/common_pitfalls.html#how-to-avoid-data-leakage)warn that you should \u201cnever call fit on the test data\u201d and that pipelines are ideal for cross-validation. In code, you\u2019d wrap your preprocessing and model in a Pipeline, then run cross\\_val\\_score or cross\\_val\\_predict. For example:\n```\n`fromsklearn.pipelineimportPipelinefromsklearn.preprocessingimportStandardScalerfromsklearn.ensembleimportGradientBoostingClassifierfromsklearn.model\\_selectionimportcross\\_val\\_scorepipe = Pipeline([(&#x27;scale&#x27;, StandardScaler()),(&#x27;gbt&#x27;, GradientBoostingClassifier())])scores = cross\\_val\\_score(pipe, X, y, cv=5)print(&quot;CV accuracy:&quot;, scores.mean())`\n```\nEach fold will automatically fit StandardScaler() and the GBT on only the training portion. Then it will transform and score on the validation portion. This prevents leakage of information (like the global mean for scaling or imputation) from contaminating your validation set. Using pipelines in this way ensures you get a realistic estimate of performance. It also prevents overly optimistic results.\n*For more tips on tree ensembles, model tuning, and Kaggle techniques,****follow me on X***[***@CoffeyFirst***](https://twitter.com/CoffeyFirst)*.*\n# Automated Hyperparameter Tuning with Optuna\nGradient-boosted models have many knobs (number of trees, learning rate/shrinkage, max depth, subsample ratio, regularization weights, etc.). Tuning them by hand or grid search can be very slow. Instead, you can use an automated search.**Optuna**is a library that implements efficient hyperparameter tuning using a[Bayesian approach (the Tree-structured Parzen Estimator, or TPE)](https://medium.com/@cris.lincoleo/a-quick-guide-to-hyperparameter-optimization-with-optuna-1980f1d185dc). Optuna\u2019s TPE sampler builds a probabilistic model of the objective scores and focuses on promising regions of the search space. This is often much faster than exhaustive grid search or blind random search. The default TPESampler in Optuna will try to maximize your validation metric (e.g. accuracy) by picking smart parameter values.\nFor example, you might write something like:\n```\n`importoptunafromoptuna.samplersimportTPESamplerdef objective(trial):n\\_trees = trial.suggest\\_int(&quot;&quot;n\\_trees&quot;&quot;,100,1000)max\\_depth = trial.suggest\\_int(&quot;&quot;max\\_depth&quot;&quot;,3,10)lr = trial.suggest\\_float(&quot;&quot;learning\\_rate&quot;&quot;,0.01,0.2, log=True)model = GradientBoostingClassifier(n\\_estimators=n\\_trees, max\\_depth=max\\_depth, learning\\_rate=lr)score = cross\\_val\\_score(model, X, y, cv=3).mean()returnscorestudy = optuna.create\\_study(direction=&quot;maximize&quot;, sampler=TPESampler(seed=42))study.optimize(objective, n\\_trials=50)print(&quot;Best params:&quot;, study.best\\_params)`\n```\nOptuna will stop when it finds a plateau (you can also use its pruning callbacks or custom early-stop callbacks). The key point is that**automated tuning with a Bayesian sampler finds good hyperparameters more efficiently than naive methods**. In my experiments, this helped GBTs converge to a strong model without endless guessing.\n*For more tips on tree ensembles, model tuning, and Kaggle techniques,****follow me on X at***[***@CoffeyFirst***](https://twitter.com/CoffeyFirst)*.*\n# Building Ensembles: Bagging, Boosting, Voting, and Stacking\nAll the tree methods I use (Random Forest, YDF\u2019s GBT, XGBoost) are[*ensembles*](https://scikit-learn.org/stable/modules/ensemble.html)of decision trees \u2014but they ensemble in different ways. To recap the terminology:\n* **Bagging (Random Forest)**: Train many independent trees on random subsets (samples and/or features) and average their predictions. Bagging mainly*reduces variance*, which is why it often guards against overfitting.\n* **Boosting (GBT / XGBoost)**: Build trees sequentially, where each new tree tries to fix errors from the previous ones. Boosting can*reduce bias*by fitting residuals, but without care it can overfit small data.\n* **Voting**: Combine different models by majority vote or by averaging probabilities (soft voting). Voting requires you have multiple pre-trained models; it doesn\u2019t train new trees, just blends predictions.\n* **Stacking**: Train multiple base models, then train a**meta-model**on their outputs. The idea is that a meta-learner (like a logistic regression) can learn how to weight each base model\u2019s vote. In practice, stacking often ends up performing about as well as the single best base model.\nI tried both soft voting and stacking with my XGBoost and YDF-GBT classifiers. For example:\n```\n`fromsklearn.ensembleimportStackingClassifier, VotingClassifierfromsklearn.linear\\_modelimportLogisticRegressionstack = StackingClassifier(estimators=[(&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;gbt&#x27;, G...",
      "url": "https://www.jamescoffey.co/post/kick-start-tree-ensembles-without-the-titanic-cliches"
    },
    {
      "title": "Introduction to Kaggle and Scoring Top 7% in the Titanic Competition",
      "text": "Introduction to Kaggle and Scoring Top 7% in the Titanic Competition | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Data Science](https://towardsdatascience.com/category/data-science/)\n# Introduction to Kaggle and Scoring Top 7% in the Titanic Competition\nGet started with Kaggle and submit a (good) first solution\n[Oliver S](https://towardsdatascience.com/author/hrmnmichaels/)\nApr 23, 2024\n16 min read\nShare\n[Kaggle](https://www.kaggle.com/)is a fun platform hosting a variety of data science and machine learning competitions &#8211; covering topics such as[sports](https://www.kaggle.com/competitions/nfl-big-data-bowl-2024),[energy](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers)or[autonomous driving](https://www.kaggle.com/competitions/lyft-motion-prediction-autonomous-vehicles).\nIn this post we will give an introduction to Kaggle, and tackle the introductory[&quot;Titanic&quot; challenge](https://www.kaggle.com/competitions/titanic). We will explain how to approach and solve such a challenge, and demonstrate this with a top 7% solution for &quot;Titanic&quot;.\n![Photo by Derek Oyen on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/04/0BguEdndpTyZl35ae.jpg)Photo by[Derek Oyen](https://unsplash.com/@goosegrease?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash)on[Unsplash](https://unsplash.com/photos/glacier-during-daytime-4ReskwNsh68?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash)\nYou can find the full code on[Github](https://github.com/hermanmichaels/titanic), and with that following along while reading this article, as well as reproduce my exact score. In it, we follow some things I consider[best practice for Python](https://towardsdatascience.com/best-practices-for-python-development-bf74c2880f87)and use helpful tools, such as[mypy](https://towardsdatascience.com/introduction-to-mypy-3d32fc96db75)and[poetry](https://towardsdatascience.com/dependency-management-with-poetry-f1d598591161). With that being said, let&#8217;s dive right into it.\n## Kaggle\n[Kaggle](https://www.kaggle.com/)offers a wide variety of data science / machine learning competitions, see the intro for examples. It is a great way to test and improve your data science / ML knowledge and learn how to solve problems hands-on. Plus, you can even win monetary prices! However, Kaggle is populated by some of the best data scientists and ML people out there &#8211; and prices are only given to the few top solutions (out of several hundreds or thousands) &#8211; thus winning here is extremely hard and rare, and should not be your main motivation when starting.\nEach (most?) competition comes with a story &#8211; a purpose &#8211; and a dataset. You are then tasked to understand the data, and solve the desired problem. If you want, you can submit your solutions to the platform, and get ranked on a public leaderboard &#8211; that is, your solution is ranked on a held-out test set. However, to avoid cheating or optimizing against this by simply spamming submissions, once the competition time (usually a few weeks to months) has expired, all competitors / teams are ranked vs a private test set &#8211; deciding the ultimate winners.\nIn the following, we will show how to understand the data, create a model, and submit to Kaggle following the introductory[Titanic competition](https://www.kaggle.com/competitions/titanic).\n## Titanic &#8211; Machine Learning from Disaster\nMost probably know the story of the cruise ship[Titanic](https://en.wikipedia.org/wiki/Titanic)and its infamous demise: a long time ago, although sadly still a tragedy with many lives lost.\nKaggle offers &quot;tutorial&quot; competitions, in which you can learn and train without any time constraints. One of these is the mentioned &quot;Titanic&quot; competition, for which you have to predict which passengers will survive.\nWe get two csv files &#8211; train.csv and test.csv. Both contain different features, such as Sex and Name of the passengers, and train contains an extra column &quot;Survived&quot;. Thus, we are tasked to learn a model, which, given the test features, can predict whether that passenger survived or not.\nTo make a submission, we simply make our own csv file consisting of PassengerID: survived, and upload this to Kaggle (either via drag &amp; drop, or from the command line &#8211; as we will see later).\nIn details, we have the following features:\n* **pclass**: ticket class (1, 2 or 3) &#8211; something similar to economy or business class in today&#8217;s travel\n* **Sex**: sex of the passenger (male / female)\n* **Age**: age in years\n* **sibsp**: # siblings / spouses aboard\n* **parch**: # parents / children aboard\n* **ticket**: the ticket number\n* **fare**: passenger fare &#8211; how much the passenger paid for the ticket\n* **cabin**: cabin number\n* **embarked**: port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)### Data Analysis and Feature Selection\nFirst step of every data science / ML problem, every competition &#8230; you tackle, should always be understanding and visualizing the data &#8211; also known as[Exploratory Data Analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis). Every good ML project stands and falls with the amount and quality of data available, and engineers spend a large amount of time collecting and preparing this data &#8211; and not necessarily working on complex models.\nThus, let&#8217;s do exactly that with the Titanic dataset. I have downloaded the materials from the Kaggle competition into my project folder, in a subfolder called &quot;titanic&quot;.\nLet&#8217;s start with the discrete data. For this, we create a function plotting the survival ratio per class (reminder, you can find the full code on[Github](https://github.com/hermanmichaels/titanic)):\n```\n`def plot\\_survival\\_ratio(df: pd.DataFrame, column: str) -&gt;&gt; None:\n# Replace NaN tokens by &quot;&quot;Unk(nown)&quot;&quot;\ndf[column].fillna(&quot;&quot;Unk&quot;&quot;, inplace=True)\n# Calculate survival ratio per existing value in the column\ntotal\\_counts = df[column].value\\_counts()\nsurvival\\_counts = (\ndf.groupby([column, &quot;&quot;Survived&quot;&quot;]).size().unstack().fillna(0)\n)\ncolumn\\_values = df[column].unique()\nsurvival\\_ratios = [\nsurvival\\_counts.loc[value, 1] / total\\_counts[value]\nfor value in column\\_values\n]\ncolor\\_mapping = plt.cm.get\\_cmap(&quot;&quot;tab10&quot;&quot;, len(survival\\_counts))\nbar\\_colors = color\\_mapping(np.arange(len(column\\_values)))\n# Plot the survival ratios\nplt.bar(column\\_values, survival\\_ratios, color=bar\\_colors)\nplt.title(f&quot;&quot;Survival Ratio by {column}&quot;&quot;)\nplt.xlabel(column)\nplt.ylabel(&quot;&quot;Survival Ratio&quot;&quot;)\nplt.ylim(0, 1)\nplt.show()`\n```\nNow, let&#8217;s exemplary look at a few available features, starting with**Sex**:\n```\n`df = pd.read\\_csv(&quot;&quot;titanic/train.csv&quot;&quot;)\nplot\\_survival\\_ratio(df, &quot;&quot;Sex&quot;&quot;)`\n```\n![Figure by author](https://towardsdatascience.com/wp-content/uploads/2024/04/1dr4w5JNnpXInfZheO6dvlA.png)Figure by author\nAs we can see, Sex makes a big difference on survival &#8211; seemingly the famous line of &quot;Women (and children)&quot; first was followed flawlessly.\nNext, let&#8217;s look at**passenger class**:\n![Figure by author](https://towardsdatascience.com/wp-content/uploads/2024/04/186WISQQsSc5Cvnla5n3-Qg.png)Figure by author\nAs one could suspect, passengers with a higher (lower in numbers) class were probably given priority treatment.\nNow, let&#82...",
      "url": "https://towardsdatascience.com/introduction-to-kaggle-and-scoring-top-7-in-the-titanic-competition-7a29ce9c24ae"
    },
    {
      "title": "Comprehensive Beginner\u2019s Guide to Kaggle & The Titanic Survival Prediction Competition",
      "text": "Comprehensive Beginner&#039;s Guide to Kaggle &amp; The Titanic Survival Prediction Competition | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Data Science](https://towardsdatascience.com/category/data-science/)\n# Comprehensive Beginner&#8217;s Guide to Kaggle &#038; The Titanic Survival Prediction Competition\nMy solution and analysis of the Titanic survival prediction competition on Kaggle\n[Jason Chong](https://towardsdatascience.com/author/chongjason/)\nNov 10, 2020\n16 min read\nShare\n![Photo by Annie Spratt on Unsplash](https://towardsdatascience.com/wp-content/uploads/2020/11/0_Xu59JMa3GaXv-wR-scaled.jpg)Photo by[Annie Spratt](https://unsplash.com/@anniespratt?utm_source=medium&amp;utm_medium=referral)on[Unsplash](https://unsplash.com?utm_source=medium&amp;utm_medium=referral)\nIf you know me, I am a big fan of Kaggle. God only knows how many times I have brought up Kaggle in my previous articles here on Medium.\nBut my journey on Kaggle wasn&#8217;t always filled with roses and sunshine, especially in the beginning. Due to my lack of experience, I was initially struggling to grasp the workflow behind a machine learning project as well as the different terminologies that were being used.\nIt was only after months of sifting through online resources which included many technical articles, documentation and tutorial videos that I slowly started to learn concepts like label encoding, cross-validation and hyperparameter tuning.\nI wished back then that there was a one-stop-shop where I could learn not only the steps behind a machine learning project but more importantly the rationale behind doing them. So I thought, why not use the knowledge that I have accumulated over the last few months to create that one-stop-shop to help others who might be going through the same thing as I did?\nIn this article, I will explain what a machine learning problem is as well as the steps behind an end-to-end machine learning project, from importing and reading a dataset to building a predictive model with reference to one of the most popular beginner&#8217;s competitions on Kaggle, that is the Titanic survival prediction competition.\nThis was the project that introduced me to the world of machine learning and also my first competition on Kaggle.\n## What is machine learning?\nActually, before we even get into machine learning, I think it is important that we first understand the purpose behind building a model.\n> What is a model and why is it important?\nA model is a simulation and simplification of the real-world.\nWhether you are an engineer building a skyscraper, a hedge fund analyst picking a stock or even a politician who is running for an election, we all deal with uncertainty in this world. A model allows us to minimise this uncertainty and make more informed decisions.\nNow, a model is not perfect and it won&#8217;t help us consistently predict the actual outcome of a particular event but we can definitely come very close to it. With large amounts of data, together with the power of artificial intelligence, humans have become increasingly proficient at making high-quality, accurate predictions using models.\nThis is where machine learning comes in.\nMachine learning is when computers learn to derive trends and patterns that are present in data. A machine learning model is &quot;trained&quot; using historical data. Once the model has been built and tested, we can then use it to make future predictions on other data points.\nMachine learning models differ from statistical models such that they require a minimal human effort but more importantly, there are fewer assumptions that that go into a machine learning model than a statistical model. Statistical models are mathematics intensive and based on coefficient estimation. It requires the modeller to understand the relationship between different variables before they can be included in the model.\nFor those reasons stated above, machine learning models not only make more accurate predictions but also predictions that are more robust and far exceed the capabilities of any ordinary model.\n## Supervised vs unsupervised learning\nFor the sake of keeping this article as comprehensive as possible for beginners, I want to also highlight the two main branches of machine learning, that is supervised and unsupervised learning. Understanding the distinction between the two branches will help us think more carefully about the type of problem that we are trying to solve using machine learning.\n> Supervised learning is training a model using labelled data. In other words, we are explicitly telling our model the sample outcome of our predictions.\nSupervised learning can be further broken into classification and regression. Classification problems are when the outcome of our predictions and discrete and categorical. Regression, on the other hand, has predictions that are of a continuous distribution.\nTo illustrate a classification problem, imagine that you are building a spam classifier which helps classify your emails as either spam or non-spam. What you would do is prepare a training set which contains the different features of your emails like their word count, use of particular words and punctuations and most importantly their labels, spam or non-spam. Then, you can train a machine learning model using this training set and the model will start to learn the relationship between the features and their labels. Once the model has been built to your desired accuracy, you can use this classifier to classify your other emails.\nRegression differs from classification in that, rather than having a discrete target variable such as spam or non-spam, the outcome in a regression problem is continuous. A perfect example of regression is predicting house prices. Here, house prices is a continuous variable. Again, we need to prepare a training set which contains house observations along with their features such as the number of bedrooms, distance away from the city, living room area and of course, their sale prices. A model will then learn how the different house features correlate with the final sale price. Subsequently, we can use this model to make predictions on other houses.\nNow, let&#8217;s move on to unsupervised learning.\nAs we saw in the spam classifier and house price prediction examples, we included the sample outcome (spam vs non-spam and final sale price) as part of our training set to train our machine learning model.\n> In unsupervised learning, we don&#8217;t include any sample outcome but instead simply let the model derive any underlying patterns that are present in the data and group them accordingly.\nSince most Kaggle competitions use supervised learning, I won&#8217;t go into unsupervised learning in too much detail in this article.\n## Aim of this competition\nThe aim of this competition is to build a machine learning model that will help us predict the survival outcome of the passengers on the Titanic.\nThis is an example of a binary classification problem in supervised learning as we are classifying the outcome of the passengers as either one of two classes, survived or did not survive the Titanic.\nMore specifically, we would like to investigate how different passenger features like their age, gender, ticket class etc impact their survival outcome.\nThe evaluation metric of this competition is the percentage of passengers in the test set that are correctly predicted by our classifier.\n## Notebook walkthrough\nIn this section, I will briefly discuss the steps and results of my analysis of the competition. You can find the compl...",
      "url": "https://towardsdatascience.com/comprehensive-beginners-guide-to-kaggle-titanic-survival-prediction-competition-solution-21c5be2cec2c"
    },
    {
      "title": "GitHub - selimamrouni/kaggle-titanic: This is my solution for the Kaggle Titanic competition.",
      "text": "[Skip to content](https://github.com/selimamrouni/kaggle-titanic#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/selimamrouni/kaggle-titanic) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/selimamrouni/kaggle-titanic) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/selimamrouni/kaggle-titanic) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[selimamrouni](https://github.com/selimamrouni)/ **[kaggle-titanic](https://github.com/selimamrouni/kaggle-titanic)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fselimamrouni%2Fkaggle-titanic) You must be signed in to change notification settings\n- [Fork\\\n13](https://github.com/login?return_to=%2Fselimamrouni%2Fkaggle-titanic)\n- [Star\\\n4](https://github.com/login?return_to=%2Fselimamrouni%2Fkaggle-titanic)\n\n\nThis is my solution for the Kaggle Titanic competition.\n\n[4\\\nstars](https://github.com/selimamrouni/kaggle-titanic/stargazers) [13\\\nforks](https://github.com/selimamrouni/kaggle-titanic/forks) [Branches](https://github.com/selimamrouni/kaggle-titanic/branches) [Tags](https://github.com/selimamrouni/kaggle-titanic/tags) [Activity](https://github.com/selimamrouni/kaggle-titanic/activity)\n\n[Star](https://github.com/login?return_to=%2Fselimamrouni%2Fkaggle-titanic)\n\n[Notifications](https://github.com/login?return_to=%2Fselimamrouni%2Fkaggle-titanic) You must be signed in to change notification settings\n\n# selimamrouni/kaggle-titanic\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/selimamrouni/kaggle-titanic/branches) [Tags](https://github.com/selimamrouni/kaggle-titanic/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[7 Commits](https://github.com/selimamrouni/kaggle-titanic/commits/master/) |\n| [README.md](https://github.com/selimamrouni/kaggle-titanic/blob/master/README.md) | [README.md](https://github.com/selimamrouni/kaggle-titanic/blob/master/README.md) |  |  |\n| [corr\\_matrix.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/corr_matrix.png) | [corr\\_matrix.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/corr_matrix.png) |  |  |\n| [criterion.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/criterion.png) | [criterion.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/criterion.png) |  |  |\n| [model\\_evaluation.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/model_evaluation.png) | [model\\_evaluation.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/model_evaluation.png) |  |  |\n| [survive\\_or\\_not.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/survive_or_not.png) | [survive\\_or\\_not.png](https://github.com/selimamrouni/kaggle-titanic/blob/master/survive_or_not.png) |  |  |\n| [titanic-predictions.csv](https://github.com/selimamrouni/kaggle-titanic/blob/master/titanic-predictions.csv) | [titanic-predictions.csv](https://github.com/selimamrouni/kaggle-titanic/blob/master/titanic-predictions.csv) |  |  |\n| [titanic.ipynb](https://github.com/selimamrouni/kaggle-titanic/blob/master/titanic.ipynb) | [titanic.ipynb](https://github.com/selimamrouni/kaggle-titanic/blob/master/titanic.ipynb) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# Kaggle Titanic\n\nThis repository presents my submission in the [Titanic: Machine Learning from Disaster, Kaggle Competition](https://www.kaggle.com/c/titanic).\n\nIn this competition, the **goal** is to perform a 2-label **classification problem**: predict which **passengers survived** the tragedy.\n\n[Kaggle](https://www.kaggle.com) offers two datasets. One training (the labels are known) and one testing (the labels are unknown). The goal is to submit a file with our predicted labels saying who survived or not.\n\nWe have access to a bunch of 9 features (numerical, text, categorical). The **big challenge** with this competition is the size of the data we have. The **training set** is composed of only **891 samples**. The testing set is composed of 418 samples.\n\nTherefore, the main issue is to design an algorithm which generalizes enough in order to avoid **over-fitting**. To do so, a bunch of features is generated. Then, an ensemble modeling method with voting is used in order to get the most generalized model.\n\nThis is a multi-label classification, with 2 labels:\n\n- 0: passenger did not survive\n- 1: passenger survived\n\n[Kaggle](https://www.kaggle.com) offers 2 datasets:\n\n- One Training set: 891 passengers whose label is known\n- One Test set (TS0): 418 passengers whose label is unknown\n\nGoal: For each passenger, predict the label (0 or 1).\n\nThe evaluation metric is accuracy score.\n\nThe project is decomposed in 3 parts:\n\nThe framework of this notebook is:\n\n- Preliminary Work:\n  - General Exploration\n  - NaN values\n  - Feature Exploration\n- Analysis of the features\n  - Categorical and Integer Numerical\n  - Numerical\n  - Text\n- Feature Engineering\n- Modeling\n  - Simple Models & Selection\n  - Hyper-Parameters Optimization\n  - Ensemble Modeling\n- Submission\n\nFor this competition, the current Kaggle Leaderboard accuracy I reached is 0.79904.\n\n## Preliminary Work\n\n[![SurvivalHistogram](https://github.com/selimamrouni/kaggle-titanic/raw/master/survive_or_not.png)](https://github.com/selimamrouni/kaggle-titanic/blob/master/survive_or_not.png)\n\nIn the training dataframe, we observe that the 2 label are slightly balanced (61% labeled as 0). We also see we have access to 16 different features per passengers.\n\n4 of the features have missing values:\n\n- Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 -> Numerical Variable\n- Cabin: Cabin number -> Categorical variable\n- Embarked: Port of Embarkation (3 categories) -> Categorical variable\n- Fare: Numerical: Only 1 missing in the test dataset. Can be replace by the mean in the training set.\n\nThe notebook details how the NaNs are treated.\n\n## Analysis of the features\n\n### Categorical & Integer Numerical Features\n\nFor this type of feature, we can observe the average survival of passengers within each categories. The observed features are:\n\n- PClass\n- Sex\n- Embarked\n- SibSp\n- Parch\n\nConclusion:\n\n- Pclass and Sex has a great correlation with the survival of people -> Keep directly them as features\n- SibSp & Parch have a sort of correlation but feature engineering is required: We can sum the two and then know:\n  - if the passenger was alone\n  - if the passenger was with a big or a small family\n- Embarked: 3 labels with no assumed order -> one-hot encoding\n\n### Numerical Features\n\nFor this type of feature, we can observe the distribution of the passengers given the survival. The observed features are:\n\n- Age\n- Fare\n\nConclusion:\n\n- Younger and older people survived.\n- The middle age (20-40) people did not survive.\n- We should consider the age as predictor\n- Fare is less clear\n- Age is skewed and Fare is highly skewed -> [Box-Cox Transformation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) is required\n\n### Text Features\n\nFor this type of features, we don't directly do analysis. A first transformation is needed. The observed features are:\n\n- Cabin\n- Name\n\nEngineering:\n\n- Cabin: We can extract two informations:\n  - if a passenger has a cabin\n  - what is the letter of the cabin deck and so we have an estimate of the position of the passenger in the boat\n- Name: We can extract one information:\n  - the title of the passenger\n  - Group the title to reduce the number of categories\n\n## Feature Engineering\n\nIn this part, I designed the features following the previous part. I ended with the following features:\n\n- PClass\n- Title (engineered from Name)\n- Sex\n- Age\n- FamilyType (engineered from SibSp + Pa...",
      "url": "https://github.com/selimamrouni/kaggle-titanic"
    },
    {
      "title": "Spaceship Titanic Kaggle competition top 7% score solution",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a9c401281c6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fernandao.lacerda.dantas%2Fspace-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Spaceship Titanic Kaggle competition top 7% score solution\n\n[Fernandao Lacerda Dantas](https://medium.com/@fernandao.lacerda.dantas?source=post_page---byline--7a9c401281c6---------------------------------------)\n\n7 min read\n\n\u00b7\n\nJan 17, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nKaggle\u2019s Space Titanic machine learning competition is quite similar to the well-known Titanic competition. Given a dataset, we are required to predict which passengers were transported or not by an \u201canomaly\u201d using records recovered from the spaceship\u2019s damaged computer system. The \u201clore\u201d of the competition is not so important, what you need to know is to develop a machine learning algorithm capable of correctly predicting the outcome of the spaceship\u2019s passengers.\n\n## Import libraries\n\n```\nimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFoldimport xgboost as xgbimport category_encoders as cefrom sklearn.impute import SimpleImputerfrom sklearn.pipeline import Pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn.compose import ColumnTransformerfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score,f1_score, confusion_matrixfrom category_encoders import TargetEncoderfrom sklearn.impute import SimpleImputer,KNNImputerfrom lightgbm import LGBMClassifier\n```\n\n## Let\u2019s Start\n\nAs you can see, after I loaded the dataset, I removed both the \u201cPassenger Id\u201d and \u201cName\u201d columns. They are not going to provide any useful or important information to the prediction. Someone\u2019s name or Id does not change the probability of being Transported.\n\n```\ndf = pd.read_csv(\"train.csv\")df.drop(columns=[\"PassengerId\",\"Name\"],inplace=True)\n```\n\nNow, we are going to discuss a fundamental step I came across after trying to improve my score a thousand times. This step relies on exploring the \u201cCabin\u201d column. Notice that the rows on the \u201cCabin\u201d column follow a specific pattern. Something like this: \u201cA/5/S\u201d, \u201cC/1/S\u201d, \u201cF/7/P\u201d. And I decided to investigate it. So, to make things simple I split the rows of the \u201cCabin\u201d into three columns based on both slashes (\u201d/\u201d) of the rows. For example, the \u201cA/5/S\u201d row would be transformed into three new columns: The first one is named \u201ccabin\\_code\u201d referring tothe character behind the first slash (A). The second one named \u201cid\\_cabin\u201d refers to the character behind the second slash (5). The third one named \u201ccabin\\_sector\u201d refers to the character after the second slash (S). And we end up with three new columns.\n\n```\n#Splittingdf[[\"cabin_code\",\"id_cabin\",\"cabin_sector\"]] = df[\"Cabin\"].str.split(\"/\", n=2, expand=True)df.head(4)\n```\n\nFirst of all, I noticed that \u201ccabin\\_code\u201d only has 8 different characters which means that the cabins are, somehow, divided into 8 sections.\n\nAlso, I asked myself if passengers from a specific section had a higher chance of being transported or if this statement was not true. With the plot below we can conclude that passengers from the B and C sections have a greater chance of surviving and passengers from the E section have a lower chance of surviving.\n\nPress enter or click to view image in full size\n\nI did the same thing with the \u201ccabin\\_sector\u201d column and also noticed that there was a difference between the sectors. Passengers from the P sector have a lower chance of being transported, while in the S sector, the opposite happens.\n\nPress enter or click to view image in full size\n\nThis means that this exploration of the original \u201cCabin\u201d column is worth it since new insights are being added to the model.\n\nNow, we can finally delete the \u201cCabin\u201d column. It will not provide any useful information for the model anymore. We have already extracted everything useful from it.\n\nI also removed the \u201ccabin\\_id\u201d and the column that I had created. As I said before, the Id will not interfere with the model\u2019s predictive ability.\n\nSo used: df.drop(columns=\\[\u201cCabin\u201d,\u201did\\_cabin\u201d\\], inplace=True) to drop both columns\n\nBefore splitting our data, the \u201cTransported\u201d column must be in a binary format. As you can see, I switched \u201cTrue\u201d for 1 and \u201cFalse\u201d for 0.\n\nBinary transformation: df\\[\u201cTransported\u201d\\] = df\\[\u201cTransported\u201d\\].map({True:1, False:0})\n\nI also removed every row that had missing values in the \u201ccabin\\_code\u201d column.\n\n```\n#BINARY TRANSFORMATIONdf[\"Transported\"] = df[\"Transported\"].map({True:1, False:0})#DROPPING COLUMNSdf.drop(columns=[\"Cabin\",\"id_cabin\"], inplace=True)#DROPPING NULLSdf.dropna(subset=[\"cabin_code\"], inplace=True)\n```\n\nNow, we can finally split the data and proceed to develop our model.\n\nAfter splitting in train and test, I separated the test data into two categories: numerical and categorical. Why is that? We are going to perform different operations depending on the type of the variable. Categorical data must be encoded since most models are not able to understand categorical values and it must be converted to numerical values. Also, we are going to apply different techniques to fill the null values in our dataset, but I will talk more about it later on.\n\n```\n#Define X and yX = df.iloc[:,0:12]y = df[\"Transported\"]\n```\n\n```\n#Splitting DataX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n```\n\n```\n#Separate categorical and numerical featurescat_feat = np.array([coluna for coluna in X_train.columns if X_train[coluna].dtype.name == 'object'])num_feat = np.array([coluna for coluna in X_train.columns if coluna not in cat_feat])\n```\n\nWe can now create our pipeline. There are going to be two pipelines: one is going to handle the categorical data and the other one is going to handle numerical data. The missing values of the categorical data will be filled with the most frequent value (mode) and after the Target Encoder will be applied to transform categorical variables into numerical variables. The numerical data missing values will be filled with a strategy called **K-nearest neighbors,** which uses the Euclidean distance between the data points to find the best number to fill the missing values. If don\u2019t know how this Pipeline technique works, I recommend you check [my article about Pipelines.](https://medium.com/@fernandao.lacerda.dantas/boost-your-pipelines-with-columntransformer-b2c009db096f)\n\n```\n#Categorical and numerical pipelinescat_pipe = Pipeline([(\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),(\"encoder\", ce.TargetEncoder()),                    ])num_pipe = Pipeline([(\"imputer_num\", KNNImputer(n_neighbors=3))])\n```\n\nAnd with column transformer, we can attach both transformations to one variable that I named \u201ctransformer\u201d. Observe that we also have to specify the type of data to which the pipeline will be applied t...",
      "url": "https://medium.com/@fernandao.lacerda.dantas/space-titanic-kaggle-competition-0-8066-score-solution-7a9c401281c6"
    },
    {
      "title": "Titanic - Advanced Feature Engineering Tutorial",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8eb8d216ba71e97900f6:1:11100)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial"
    },
    {
      "title": "Notebook on nbviewer",
      "text": "1. [data-science-ipython-notebooks](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/tree/master)\n2. [kaggle](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/tree/master/kaggle)\n\nNotebook\n\nThis notebook was prepared by [Donne Martin](http://donnemartin.com). Source and license info is on [GitHub](https://github.com/donnemartin/data-science-ipython-notebooks).\n\n# Kaggle Machine Learning Competition: Predicting Titanic Survivors [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Kaggle-Machine-Learning-Competition:-Predicting-Titanic-Survivors)\n\n- Competition Site\n- Description\n- Evaluation\n- Data Set\n- Setup Imports and Variables\n- Explore the Data\n- Feature: Passenger Classes\n- Feature: Sex\n- Feature: Embarked\n- Feature: Age\n- Feature: Family Size\n- Final Data Preparation for Machine Learning\n- Data Wrangling Summary\n- Random Forest: Training\n- Random Forest: Predicting\n- Random Forest: Prepare for Kaggle Submission\n- Support Vector Machine: Training\n- Support Vector Machine: Predicting\n\n## Competition Site [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Competition-Site)\n\nDescription, Evaluation, and Data Set taken from the [competition site](https://www.kaggle.com/c/titanic-gettingStarted).\n\n## Description [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Description)\n\n![alt text](http://upload.wikimedia.org/wikipedia/commons/6/6e/St%C3%B6wer_Titanic.jpg)\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n## Evaluation [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Evaluation)\n\nThe historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set.\n\nFor each passenger in the test set, you must predict whether or not they survived the sinking ( 0 for deceased, 1 for survived ). Your score is the percentage of passengers you correctly predict.\n\nThe Kaggle leaderboard has a public and private component. 50% of your predictions for the test set have been randomly assigned to the public leaderboard ( the same 50% for all users ). Your score on this public portion is what will appear on the leaderboard. At the end of the contest, we will reveal your score on the private 50% of the data, which will determine the final winner. This method prevents users from 'overfitting' to the leaderboard.\n\n## Data Set [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Data-Set)\n\n| File Name | Available Formats |\n| --- | --- |\n| train | .csv (59.76 kb) |\n| gendermodel | .csv (3.18 kb) |\n| genderclassmodel | .csv (3.18 kb) |\n| test | .csv (27.96 kb) |\n| gendermodel | .py (3.58 kb) |\n| genderclassmodel | .py (5.63 kb) |\n| myfirstforest | .py (3.99 kb) |\n\n```\nVARIABLE DESCRIPTIONS:\nsurvival        Survival\n                (0 = No; 1 = Yes)\npclass          Passenger Class\n                (1 = 1st; 2 = 2nd; 3 = 3rd)\nname            Name\nsex             Sex\nage             Age\nsibsp           Number of Siblings/Spouses Aboard\nparch           Number of Parents/Children Aboard\nticket          Ticket Number\nfare            Passenger Fare\ncabin           Cabin\nembarked        Port of Embarkation\n                (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSPECIAL NOTES:\nPclass is a proxy for socio-economic status (SES)\n 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n\nAge is in Years; Fractional if Age less than One (1)\n If the Age is Estimated, it is in the form xx.5\n\nWith respect to the family relation variables (i.e. sibsp and parch)\nsome relations were ignored.  The following are the definitions used\nfor sibsp and parch.\n\nSibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\nSpouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\nParent:   Mother or Father of Passenger Aboard Titanic\nChild:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n\nOther family relatives excluded from this study include cousins,\nnephews/nieces, aunts/uncles, and in-laws.  Some children travelled\nonly with a nanny, therefore parch=0 for them.  As well, some\ntravelled with very close friends or neighbors in a village, however,\nthe definitions do not support such relations.\n\n```\n\n## Setup Imports and Variables [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Setup-Imports-and-Variables)\n\nIn\u00a0\\[1\\]:\n\n```\nimport pandas as pd\nimport numpy as np\nimport pylab as plt\n\n# Set the global default size of matplotlib figures\nplt.rc('figure', figsize=(10, 5))\n\n# Size of matplotlib figures that contain subplots\nfizsize_with_subplots = (10, 10)\n\n# Size of matplotlib histogram bins\nbin_size = 10\n\n```\n\n## Explore the Data [\u00b6](https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb\\#Explore-the-Data)\n\nRead the data:\n\nIn\u00a0\\[2\\]:\n\n```\ndf_train = pd.read_csv('../data/titanic/train.csv')\ndf_train.head()\n\n```\n\nOut\\[2\\]:\n\n|  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S |\n| 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |\n| 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S |\n| 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35 | 1 | 0 | 113803 | 53.1000 | C123 | S |\n| 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35 | 0 | 0 | 373450 | 8.0500 | NaN | S |\n\nIn\u00a0\\[3\\]:\n\n```\ndf_train.tail()\n\n```\n\nOut\\[3\\]:\n\n|  | PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket | Fare | Cabin | Embarked |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 886 | 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27 | 0 | 0 | 211536 | 13.00 | NaN | S |\n| 887 | 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19 | 0 | 0 | 112053 | 30.00 | B42 | S |\n| 888 | 889 | 0 | 3 | Johnston, Miss. Catherine Helen \"Carrie\" | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S |\n| 889 | 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26 | 0 | 0 | 111369 | 30.00 | C148 | C |\n| 890 | 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32 | 0 | 0 | 370376 | 7.75 | NaN | Q |\n\nView the data types of each column:\n\nIn\u00a0\\[4\\]:\n\n```\ndf_train.dtypes\n\n```\n\nOut\\[4\\]:\n\n```\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n```\n\n...",
      "url": "https://nbviewer.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb"
    }
  ]
}