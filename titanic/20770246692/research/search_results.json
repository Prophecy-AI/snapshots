{
  "query": "How do people achieve 100% accuracy on Kaggle Titanic competition? Is it possible to get perfect score?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Achieving a perfect 100\u202f% score on the Titanic competition is only possible by exploiting data leakage or cheating (e.g., using the test labels or external data), not by a genuine model, so a legitimate perfect score is effectively impossible ([kaggle.com](https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy), [medium.com](https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1) as of March\u202f2025).",
      "url": ""
    },
    {
      "title": "How to score top 3% in Kaggle's ,,Titanic \u2014 Machine Learning from ...",
      "text": "<div><div><div><a href=\"https://anelmusic13.medium.com/?source=post_page---byline--13d056e262b1---------------------------------------\"><div><p></p></div></a></div><p>Hey, fellow machine learning enthusiasts. If you\u2019ve ever had a try at a Kaggle competition, chances are you\u2019re already familiar with the Titanic dataset. <br/>This competition is about predicting whether a passenger will survive the Titanic disaster or not.</p><p>With relatively little effort it is possible to rank among the top 30% of participants. Unfortunately, many of the top scorers train their model on an external dataset and thus obtain a model that classifies the test data with 100% accuracy. I couldn\u2019t tell you why someone would want to cheat in such an obvious way but what this essentially means is that you have to make an extra effort to get into the top 3%.</p><p><strong>Aim</strong> of this article:</p><ul><li>Explain step by step the end-to-end data pipeline that is needed to score among top 3%.</li><li>Discuss the thought process of a Machine Learning Engineer / Data Scientist in Data Cleaning and Feature Engineering.</li><li>Make it more difficult for future participants to stand out :)</li></ul><p><strong>You can find the complete code at:</strong></p><p><a href=\"https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis\">https://github.com/AnelMusic/Kaggle_Titanic_Advanced_DataAnalysis</a></p><h2>1. Getting the data:</h2><figure><figcaption>Import needed modules for handling tabular data, plotting and accessing common ML</figcaption></figure><figure><figcaption>Provide data paths</figcaption></figure><p>In my case the train.csv as well as test.csv are located in the current working directory. Using pandas we can read and convert the csv-files to pandas dataframes:</p><figure><figcaption>Read data csv and convert it to pandas dataframe</figcaption></figure><p>Next, we can investigate the data to get a better understanding of the available features:</p><figure><figcaption>Show train.csv pandas dataframe</figcaption></figure><h2>2. Exploratory Data Analysis</h2><p>Typically, it is not necessary to use each available feature. Many of them do not provide additional information for the model and increase the training time unnecessarily. However, a more crucial problem that is often overlooked is that additional features inevitably require more data. Now why is that?</p><p><strong>Why additional features call for additional data:</strong></p><p>If you think about it, each additional feature adds a new dimension to the feature space. This in turn increases the distance between observations because each new dimension adds a non-negative term to the calculation of the euclidean distance. Thus the feature space becomes sparse. In a sparse feature space ML algorithms tend to overfit to noise and the only way to populate your feature space is by adding additional (and rich) data.</p><p>As you can see it is essential to explore which features should be considered and which should not.</p><p>I would argue that most important skill of a Machine Learning Engineer/Data Scientist is to be unbiased, not assume things and to ask good questions.</p><p>So, let\u2019s ask some questions:</p><h2>2.1 Question 1: Does the dataset contain missing values?</h2><p>In the very first step it is helpful to check if and how many entries are missing in the dataset.</p><figure><figcaption>Dataset missing values summary</figcaption></figure><figure><figcaption>Define function that creates missing value heatmap</figcaption></figure><figure><figcaption>Missing values heatmap</figcaption></figure><p>From the above plots, it can be seen that the training and similarly the test datasets contain features with missing values. The sparsest features are \u201cage\u201d and \u201ccabin\u201d.</p><p>A naive approach to solve this problem would be to remove the feature completely from the dataset. However, since we do not know how much information these they provide they further investigation is needed. Maybe we find that it would make sense to impute the missing values using sophisticated data imputation methods.</p><h2>2.1 Question 1: How many passengers survived?</h2><figure><figcaption>Check survival rate</figcaption></figure><p>If you prefer plots you can define the function below to plot a bar chart:</p><figure><figcaption>Define function for plotting bar charts</figcaption></figure><figure><figcaption>Bar chart survival rate</figcaption></figure><p>As expected the majority of passengers in the training data died and only 38% survived. This means that the training data suffers from data imbalance but it is not severe which is why I will not consider techniques like sampling to tackle the imbalance.</p><figure><figcaption>Degree of minority class imbalance [shorturl.at/fTV37]</figcaption></figure><h2>2.2 Question 2: Is the likelihood of survival dependent on gender?</h2><p>In 1912, about 110 years ago, women were generally considered to be the weaker sex and should be protected. Let\u2019s investigate if the men of that time were so dutiful even under the fear of death.</p><figure><figcaption>Passenger count based on gender</figcaption></figure><figure><figcaption>Survival ratio based on gender</figcaption></figure><figure><figcaption>Survival rate bar chart</figcaption></figure><p>Here, we can clearly see that even though the majority of passenger were men, the majority of survivors were women. The key observation here is that the survival rate for female passengers is 4 times higher than the survival rate of male passengers. This seems to confirm that the phrase \u201cwomen and children first\u201d does indeed seem to have been a rule to which men adhered to.</p><h2>2.3 Question 3: Could it be that the class to which a passenger belonged correlates with the probability of survival??</h2><figure><figcaption>Passenger count with respect to class</figcaption></figure><figure><figcaption>Passenger class distribution</figcaption></figure><figure><figcaption>Bar chart passenger class vs dependent (target) variable ,,survived\u201d</figcaption></figure><p>From the plots and tables above it becomes clear that the Pclass is an important factor to consider.</p><p><strong>Key observations:</strong></p><ul><li>Most passenger had class 3 tickets, yet only 24% of class 3 passengers survived.</li><li>Almost 63% of the passenger from class 1 survived.</li><li>Approx 50% of the class 2 passenger survived.</li></ul><p>However, it is not clear yet weather the class or the gender is the underlying and deciding factor. Which brings another important question:</p><h2>2.4 Question 4: Is the higher survival rate in Class 1 due to the class itself or due to a skewed gender distribution in which female passengers dominate?</h2><figure><figcaption>Function that plots bar chart with multiple features</figcaption></figure><figure><figcaption>Bar chart of gender vs surival rate</figcaption></figure><figure><figcaption>Percentage of male and female survivors with respect to gender</figcaption></figure><p>Here, we can see that the question raised above was justified. Irrespective of the class the most important factor when it comes to survival was gender. (At least between the two features Sex and Pclass). However, men had a significantly higher chance of survival if they bought class 1 tickets. This just shows that we should keep both features as both yield insightful information that should help our model.</p><p><strong>Key observations:</strong></p><ul><li>Survival Rate females 1. Class: 96,8%</li><li>Survival Rate females 2. Class: 92,1%</li><li>Survival Rate females 3. Class: 50%</li><li>Survival Rate male 1. Class: 36.8% <br/>(still significantly lower than 3. class females)</li></ul><h2>2.5 Question 5: Did a passengers age influence the chance of survival?</h2><figure><figcaption>Utility function for plotting a histogram and the kernel density estimate</figcaption></figure><figure><figcaption>Age distribution</figcaption></figure><figure><figcaption>Age kernel density</f...",
      "url": "https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1"
    },
    {
      "title": "Titanic Competition [Journey to 100% Accuracy] - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=8cd3a1b73a88961fadc8:1:10922)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/amerwafiy/titanic-competition-journey-to-100-accuracy"
    },
    {
      "title": "Titanic Model with 90% accuracy | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat f.onerror.f.onload (https://www.kaggle.com/static/assets/runtime.js?v=1e4ba90eba1c8273e4fe:1:11427)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/vinothan/titanic-model-with-90-accuracy"
    },
    {
      "title": "Titanic - Beginner Friendly with 92% Accuracy",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)</p></div>ChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=13a2e59ca90baa13b10c:1:10686)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/code/devbilalkhan/titanic-beginner-friendly-with-92-accuracy"
    },
    {
      "title": "Creating Predictions for Kaggle's Titanic Challenge",
      "text": "Creating Predictions for Kaggle&#39;s Titanic Challenge - Free Video Tutorial\n# Creating Predictions for Kaggle&#39;s Titanic Challenge\nCreate predictions using a random forest model, format the Kaggle submission CSV, and upload it to Kaggle for scoring.\nMarch 12, 2025\nby[Colin Jaffe](https://www.graduateschool.edu/blog/authors/colin-jaffe)\nRead more in[Machine Learning](https://www.graduateschool.edu/learn/machine-learning)\nShare\n[Share on X](<https://x.com/intent/tweet?&amp;text=Creating Predictions for Kaggle's Titanic Challenge - Graduate School USA resource&amp;url=https://www.graduateschool.edu/learn/creating-predictions-kaggle-titanic-challenge/creating-predictions-kaggle-titanic-challenge&amp;via=>)[Share on Facebook](https://www.facebook.com/sharer/sharer.php?u=https://www.graduateschool.edu/learn/creating-predictions-kaggle-titanic-challenge/creating-predictions-kaggle-titanic-challenge)[Share on LinkedIn](<https://www.linkedin.com/shareArticle?url=https://www.graduateschool.edu/learn/creating-predictions-kaggle-titanic-challenge/creating-predictions-kaggle-titanic-challenge&amp;title=Creating Predictions for Kaggle's Titanic Challenge - Graduate School USA resource>)\nCreate accurate predictions using Python and Random Forest classifiers, then evaluate your model's effectiveness by submitting results to Kaggle. Learn the complete workflow from preparing prediction arrays to formatting CSV submissions for Kaggle's Titanic competition.\n## Key Insights\n* Created a prediction array by applying model.predict on the test dataset, generating an array consisting of zeros and ones indicating passenger survival.\n* Prepared a submission CSV file conforming strictly to Kaggle's format by retrieving the passenger IDs from the original Titanic test dataset and including predictions, explicitly setting index=False to exclude unwanted index columns.\n* Submitted the CSV file to Kaggle's Titanic Machine Learning competition, achieving an accuracy score around 77\u201379%, providing students motivation to explore improvements and adjustments to further optimize the Random Forest classifier model.\nThis lesson is a preview from our[Data Science & AI Certificate Online](https://www.graduateschool.edu/certificates/data-science-online)(includes software) and[Python Certification Online](https://www.graduateschool.edu/certificates/python-certification-self-paced)(includes software &amp; exam). Enroll in a course for detailed lessons, live instructor support, and project-based training.\nOkay, let's pick up right where we left off. We're going to create a predictions array. Let's call it predictions.\nAnd it will be what happens when we run model.predict on our X\\_test. It is 400 and something zeros and ones. Not very helpful for us without context for whether we got these correct or not, but we'll use that in our next step, which is to create a data frame that will have Passenger ID and predictions.\nNow, we're going to submit this to Kaggle, and it has to be in this exact format so that its algorithm can give us, can check it against the Y\\_test answers, and give us an accuracy score. We need our Passenger ID. I foolishly removed and overwrote the X\\_test and got rid of the Passenger ID, but we can get it back.\nWhat we're going to do is simply read from the test CSV again and get that right back. So, I'm going to create a Titanic\\_test data frame, and it's reading the CSV from our base URL plus CSV/test\\_Titanic.csv. Let's double-check that. Yep, it's got the Passenger ID that I got rid of in X\\_test.\nOkay, great. Since we've got that, let's now make a data frame called Titanic\\_submission. Sure.\nTitanic\\_submission is a new data frame, and it's got a Passenger ID column that should equal the Titanic\\_test data frame\u2019s Passenger ID. Then we're going to include a Survived column, and it\u2019s going to equal our predictions from above\u2014these zeros and ones up here.\nThen we can check our submission. It looks pretty good. Passenger ID and zeros and ones.\nJust those two columns are all that Kaggle wants. Now, saving it as a CSV is a little bit of work, but not too bad. We want to save it to Google Drive in our case, then download it.\nAnd we\u2019re going to make sure to set index=False. If we don\u2019t do that, we\u2019ll get another column that contains these indexes. We don\u2019t want that.\nWe want only Passenger ID and Survived as columns in the CSV we\u2019re uploading. We\u2019re going to use Titanic\\_submission.to\\_csv, and we\u2019re going to save it to our base URL on Google Drive plus CSV/Kaggle\\_submission.csv.\nFinally, index=False so that we can only have those two columns in it. Perfect. Okay.\nRun that line of code. Now we're ready to submit that to Kaggle. It should be downloaded to your Google Drive.\nLet\u2019s check it out. Here\u2019s my Kaggle\\_submission.csv, but if you need to find it, it\u2019s in my drive. It should be in[Python](https://www.graduateschool.edu/learn/python/what-is-python)*Machine Learning Bootcamp*, CSV file, Kaggle\\_submission.csv is what I just named it.\nSo, I\u2019m going to download that now. Right-click on it. Yep.\nClick Download. And yep, it\u2019s downloaded. Now I\u2019m going to go to Kaggle, and we\u2019re going to submit it.\nIf you don\u2019t have a Kaggle account, you\u2019ll need one for this step, but you should get one anyway. Kaggle is fantastic.\nIt\u2019s a big part of the machine learning community, and it\u2019s a great place to learn. What you\u2019re going to do is find the Titanic competition. If you search at the top, let me walk through that a little bit more.\nGo to competitions and type Titanic in the search bar.\nClick on Titanic under competitions, Titanic: Machine Learning from Disaster. What you\u2019re going to do is submit our CSV.\nClick \u2018Submit Prediction\u2019 up here.\nThen, find the file you downloaded.\nNow, it\u2019ll run, and it will then give you a score. It should be around 79%. Here\u2019s the one I just did.\nOoh, down to 77%. I must have made a change.\nIt\u2019s a fine score. It\u2019s a great starting point for thinking, \u2018How do I improve my score?\u2019\nHow can I improve my results? What factors improve the score? What can I adjust? What can I fine-tune?\nAnd we\u2019ll continue with the next lesson.\n## Colin Jaffe\nColin Jaffe is a programmer, writer, and teacher with a passion for creative code, customizable computing environments, and simple puns. He loves teaching code, from the fundamentals of algorithmic thinking to the business logic and user flow of application building\u2014he particularly enjoys teaching JavaScript, Python, API design, and front-end frameworks.\nColin has taught code to a diverse group of students since learning to code himself, including young men of color at All-Star Code, elementary school kids at The Coding Space, and marginalized groups at Pursuit. He also works as an instructor for Noble Desktop, where he teaches classes in the Full-Stack Web Development Certificate and the Data Science &amp; AI Certificate.\nColin lives in Brooklyn with his wife, two kids, and many intricate board games.\n[More articles by Colin Jaffe](https://www.graduateschool.edu/blog/authors/colin-jaffe)\n### **How to Learn Machine Learning**\nBuild practical, career-focused machine learning skills through hands-on training designed for beginners and professionals alike. Learn fundamental tools and workflows that prepare you for real-world projects or industry certification.\n* Explore[machine learning classes, bootcamps, and certifications](https://www.graduateschool.edu/topics/machine-learning-classes)to find the right training format for your goals.\n* Take the[Python Data Science &amp; Machine Learning Bootcamp](https://www.graduateschool.edu/certificates/python-programming)to gain comprehensive, career-ready skills\n* Prepare for professional exams or credentials with the[Python Certification Online](https://www.graduateschool.edu/certificates/python-certification)\n* Develop your team\u2019s expertise through customized[group training](https://www.graduateschool.edu/group-training)sessions",
      "url": "https://www.graduateschool.edu/learn/machine-learning/creating-predictions-kaggle-titanic-challenge"
    },
    {
      "title": "How to score 100% Accuracy in Kaggle Competition - YouTube",
      "text": "\ud83d\udd12 Unveiling Kaggle Competition Loopholes: Achieve 100% Accuracy! \ud83d\ude80\\n\\nWelcome to our channel, where we explore unconventional pathways to reach the pinnacle of success in Kaggle competitions! \ud83c\udfc6 Are you intrigued by the idea of discovering hidden strategies to secure that 100% accuracy score? Look no further! \ud83c\udf1f Whether you're a data prodigy, a machine learning enthusiast, or a code-breaking genius, we're here to guide you through the world of Kaggle with a unique twist.\\n\\n\ud83d\udd25 Join us on a journey that goes beyond conventional approaches, as we delve into the realm of loopholes, hacks, and creative tactics to outmaneuver the competition. \ud83d\udd73\ufe0f Our videos are designed to challenge the status quo and empower you with the knowledge to unlock unexplored avenues of success.\\n\\nAre you ready to redefine the way you approach Kaggle competitions? Hit that subscribe button and turn on the notification bell to stay in the loop with our unique content. \ud83d\udece\ufe0f Get ready to witness your competition results transformed as we equip you with the tools and mindset to explore uncharted territories.\\n\\nConnect with our community of like-minded Kaggle enthusiasts, where we discuss, experiment, and push the boundaries of competition dynamics. Let's challenge each other to reimagine possibilities and set new benchmarks for unconventional success.\\n\\nRemember, breaking the mold isn't just a strategy\u2014it's a mindset shift. \ud83d\ude80 Embark on this exciting journey with us and discover how to harness the power of Kaggle competition loopholes to your advantage!\\n\\nStay ingenious, stay ahead! \ud83d\udd12\ud83d\ude80\\n\\n#kaggle #datascience #machinelearning #competition #artificialintelligence\n| view_count: 3,144 views | short_view_count: 3.1K views | num_likes: 19 | num_subscribers: 59",
      "url": "https://www.youtube.com/watch?v=XJ_b19nr-oc"
    },
    {
      "title": "Top 1% Titanic solution",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/nikitakudriashov/top-1-titanic-solution"
    },
    {
      "title": "This will help you score 95 percentile in the Kaggle Titanic ML competition",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faa2b3fd1b79b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40praveen.orvakanti%2Fthis-will-help-you-score-95-percentile-in-the-kaggle-titanic-ml-competition-aa2b3fd1b79b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40praveen.orvakanti%2Fthis-will-help-you-score-95-percentile-in-the-kaggle-titanic-ml-competition-aa2b3fd1b79b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# This will help you score 95 percentile in the Kaggle Titanic ML competition\n\n[![Praveen kumar Orvakanti](https://miro.medium.com/v2/resize:fill:64:64/1*VFzKjU6WLhlBd07KW-ZHzA.jpeg)](https://medium.com/@praveen.orvakanti?source=post_page---byline--aa2b3fd1b79b---------------------------------------)\n\n[Praveen kumar Orvakanti](https://medium.com/@praveen.orvakanti?source=post_page---byline--aa2b3fd1b79b---------------------------------------)\n\nFollow\n\n7 min read\n\n\u00b7\n\nOct 16, 2018\n\n--\n\n1\n\nListen\n\nShare\n\nPredict the survival of the Titanic passengers\n\n> In this blog-post, we will take a closer look at the [**Titanic Machine Learning From Disaster**](https://www.kaggle.com/c/titanic) data set from [**Kaggle**](https://kaggle.com). I will try to briefly explain my approach/analysis and I sincerely hope to provide an example of a competitive analysis for those interested in getting into the field of data analytics or using python for Kaggle\u2019s Data Science competitions .\n\n> RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early hours of 15 April 1912, after colliding with an iceberg during her maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line.\n\n> Let\u2019s take a closer look at the data provided:\n\n> **Step 1: Import the necessary libraries**\n\n```\n# Import our libraries\nimport pandas as pd\nimport numpy as np# Import sklearn libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, precision_recall_curve, auc, make_scorer, confusion_matrix, f1_score, fbeta_score# Import the Naive Bayes, logistic regression, Bagging, RandomForest, AdaBoost, GradientBoost, Decision Trees and SVM Classifierfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom xgboost import XGBClassifierimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#from matplotlib import style\n#plt.style.use('bmh')\n#plt.style.use('ggplot')\nplt.style.use('seaborn-notebook')from matplotlib.ticker import StrMethodFormatterfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n```\n\n> **Step 2: Read the data**\n\n```\ntrain_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\n```\n\n> **Step 3: Exploratory data analysis**\n\n[**Exploratory data analysis - Wikipedia** \\\n\\\n**In statistics, exploratory data analysis ( EDA) is an approach to analyzing data sets to summarize their main\u2026**\\\n\\\nen.wikipedia.org](https://en.wikipedia.org/wiki/Exploratory_data_analysis?source=post_page-----aa2b3fd1b79b---------------------------------------)\n\nLets explore the data and ask questions to better understand the data.\n\nThe dataset has the following features:\n\n- `PassengerId`: Unique Id of a passenger\n- `survival`: Whether a passenger survived or not; 1 if survived and 0 if not.\n- `pclass`: Ticket class\n- `sex`: Sex\n- `Age`: Age in years\n- `sibsp`: # of siblings / spouses aboard the Titanic\n- `parch`: # of parents / children aboard the Titanic\n- `ticket`: Ticket number\n- `fare`: Passenger fare\n- `cabin`: Cabin number\n- `embarked`: Port of Embarkation\n\nSnapshot of the data:\n\nQuestion 1: How many people embarked from different ports? Is there a correlation between port of embarkment and survival?\n\nOver 72% of the passengers embarked from the port \u2018Southampton\u2019, 18% from the port \u2018Cherbourg\u2019 and the rest from the port \u2018Queenstown\u2019.\n\nPassengers from port \u2018Southampton\u2019 have a low survival rate of 34%, while those from the port \u2018Cherbourg\u2019 have a survival rate of 55%.\n\nQuestion 2: Does survival depend upon the gender?\n\nThe survival rate for the female passengers is very high for those who embarked from the port \u2018Southampton\u2019 or \u2018 Queenstown\u2019.\n\nThe survival rate for the male passengers is high for those who embarked from the port \u2018Cherbourg\u2019.\n\nSo there seems to be a correlation between embarked port and survival depending upon the gender.\n\nWomen have a survival rate of 74%, while men have a survival rate of about 19%.\n\nMen below the age 10 and between 30 and 35 have a higher survival rate while the women in general seem to have a high survival rate across age groups compared to that of men.\n\n> **Step 4: Data Preprocessing**\n\n**Missing values:**\n\nWe see that the features `Age` and `Embarked ` have missing values. Since `Age` is a numeric variable, lets fill the null values with random numbers computed based on mean and the standard deviation of the column.\n\n```\ndata = [train_df, test_df]for dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n```\n\nFor `Embarked` we will use the \u2018mode\u2019 to replace the null values.\n\n```\nembarked_mode = train_df['Embarked'].mode()\ndata = [train_df, test_df]for dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(embarked_mode)\n```\n\n**Feature Engineering:**\n\nThe features `SibSp` and `Parch` capture the number of parents / children / siblings that the passengers have travelled with. Let\u2019s combine them both to engineer a new feature:\n\n```\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'\n```\n\nWe will skip the other feature engineering steps for the sake of this post. For more details, please refer to my [**github repository**](https://github.com/porvakanti/Kaggle-Competition-TitanicSurvival).\n\nQuestion 3: Do the passengers have better chance at survival while travelling alone?\n\n```\naxes = sns.factorplot('relatives','Survived',\n                      data=train_df, aspect = 2.5, );\n```\n\nPassengers travelling with 1 to 3 relatives have a higher survival rate than those travelling with no relatives or those travelling with more than 3 relatives.\n\n> **St...",
      "url": "https://medium.com/@praveen.orvakanti/this-will-help-you-score-95-percentile-in-the-kaggle-titanic-ml-competition-aa2b3fd1b79b"
    }
  ]
}