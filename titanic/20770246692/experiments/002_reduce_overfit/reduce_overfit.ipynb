{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449488e3",
   "metadata": {},
   "source": [
    "# Experiment 002: Reduce Overfitting + Ticket Features\n",
    "\n",
    "Addressing the CV-LB gap (0.8361 CV vs 0.7799 LB = 5.6% gap):\n",
    "1. Add Ticket_Group_Size feature\n",
    "2. Use simpler/regularized models\n",
    "3. Remove potentially overfitting features (AgeBin, FareBin)\n",
    "4. Try ensemble of regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168152d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test for consistent feature engineering\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['Survived'] = np.nan\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Combined shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfde243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# 1. Extract Title from Name\n",
    "import re\n",
    "def extract_title(name):\n",
    "    match = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return 'Unknown'\n",
    "\n",
    "df['Title'] = df['Name'].apply(extract_title)\n",
    "\n",
    "# Group rare titles\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
    "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Capt': 'Rare',\n",
    "    'Lady': 'Rare', 'Countess': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Dona': 'Rare',\n",
    "    'Jonkheer': 'Rare'\n",
    "}\n",
    "df['Title'] = df['Title'].map(lambda x: title_mapping.get(x, 'Rare'))\n",
    "\n",
    "print(\"Title distribution:\")\n",
    "print(df['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Family Features\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# 3. Cabin Features\n",
    "df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "\n",
    "# 4. NEW: Ticket Group Size - passengers sharing same ticket\n",
    "ticket_counts = df.groupby('Ticket')['PassengerId'].transform('count')\n",
    "df['Ticket_Group_Size'] = ticket_counts\n",
    "\n",
    "print(\"Ticket_Group_Size distribution:\")\n",
    "print(df['Ticket_Group_Size'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FarePerPerson - divide fare by ticket group size\n",
    "df['FarePerPerson'] = df['Fare'] / df['Ticket_Group_Size']\n",
    "\n",
    "# 6. Age Imputation - using median by (Sex, Pclass, Title)\n",
    "train_mask = df['is_train'] == 1\n",
    "age_medians = df[train_mask].groupby(['Sex', 'Pclass', 'Title'])['Age'].median()\n",
    "\n",
    "def impute_age(row):\n",
    "    if pd.isna(row['Age']):\n",
    "        try:\n",
    "            return age_medians.loc[(row['Sex'], row['Pclass'], row['Title'])]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return df[train_mask].groupby(['Sex', 'Pclass'])['Age'].median().loc[(row['Sex'], row['Pclass'])]\n",
    "            except KeyError:\n",
    "                return df[train_mask]['Age'].median()\n",
    "    return row['Age']\n",
    "\n",
    "df['Age'] = df.apply(impute_age, axis=1)\n",
    "print(f\"Missing Age after imputation: {df['Age'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fare Handling\n",
    "fare_medians = df[train_mask].groupby('Pclass')['Fare'].median()\n",
    "\n",
    "def impute_fare(row):\n",
    "    if pd.isna(row['Fare']):\n",
    "        return fare_medians.loc[row['Pclass']]\n",
    "    return row['Fare']\n",
    "\n",
    "df['Fare'] = df.apply(impute_fare, axis=1)\n",
    "df['FarePerPerson'] = df['Fare'] / df['Ticket_Group_Size']  # Recalculate after imputation\n",
    "\n",
    "# 8. Embarked - fill with mode\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "\n",
    "# Encode categorical features\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "df['Title'] = df['Title'].map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5})\n",
    "\n",
    "print(\"Feature encoding complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5911b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select SIMPLER features - removing AgeBin and FareBin to reduce overfitting\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', \n",
    "            'Title', 'FamilySize', 'IsAlone', 'Has_Cabin',\n",
    "            'Ticket_Group_Size', 'FarePerPerson']\n",
    "\n",
    "# Split back to train and test\n",
    "train_df = df[df['is_train'] == 1].copy()\n",
    "test_df = df[df['is_train'] == 0].copy()\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df['Survived'].values\n",
    "X_test = test_df[features].values\n",
    "test_ids = test_df['PassengerId'].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Logistic Regression (simple, regularized)\n",
    "print(\"=\" * 50)\n",
    "print(\"Model 1: Logistic Regression (L2 regularized)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_scores_lr = []\n",
    "test_preds_lr = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale features for LR\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(C=0.5, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores_lr.append(fold_acc)\n",
    "    test_preds_lr += model.predict_proba(X_test_scaled)[:, 1] / 5\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "mean_cv_lr = np.mean(fold_scores_lr)\n",
    "std_cv_lr = np.std(fold_scores_lr)\n",
    "print(f\"\\nLogistic Regression CV: {mean_cv_lr:.4f} ± {std_cv_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Shallow Random Forest (max_depth=4)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model 2: Shallow Random Forest (max_depth=4)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fold_scores_rf = []\n",
    "test_preds_rf = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,  # Shallow!\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        criterion='entropy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    val_pred = model.predict(X_val)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores_rf.append(fold_acc)\n",
    "    test_preds_rf += model.predict_proba(X_test)[:, 1] / 5\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "mean_cv_rf = np.mean(fold_scores_rf)\n",
    "std_cv_rf = np.std(fold_scores_rf)\n",
    "print(f\"\\nShallow RF CV: {mean_cv_rf:.4f} ± {std_cv_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Gradient Boosting with regularization\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model 3: Gradient Boosting (regularized)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fold_scores_gb = []\n",
    "test_preds_gb = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,  # Shallow\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    val_pred = model.predict(X_val)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores_gb.append(fold_acc)\n",
    "    test_preds_gb += model.predict_proba(X_test)[:, 1] / 5\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "mean_cv_gb = np.mean(fold_scores_gb)\n",
    "std_cv_gb = np.std(fold_scores_gb)\n",
    "print(f\"\\nGradient Boosting CV: {mean_cv_gb:.4f} ± {std_cv_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: SVC with RBF kernel\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model 4: SVC (RBF kernel)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fold_scores_svc = []\n",
    "test_preds_svc = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = SVC(C=1.0, kernel='rbf', probability=True, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    fold_acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores_svc.append(fold_acc)\n",
    "    test_preds_svc += model.predict_proba(X_test_scaled)[:, 1] / 5\n",
    "    print(f\"Fold {fold+1}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "mean_cv_svc = np.mean(fold_scores_svc)\n",
    "std_cv_svc = np.std(fold_scores_svc)\n",
    "print(f\"\\nSVC CV: {mean_cv_svc:.4f} ± {std_cv_svc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of individual models\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY OF INDIVIDUAL MODELS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Logistic Regression: {mean_cv_lr:.4f} ± {std_cv_lr:.4f}\")\n",
    "print(f\"Shallow RF:          {mean_cv_rf:.4f} ± {std_cv_rf:.4f}\")\n",
    "print(f\"Gradient Boosting:   {mean_cv_gb:.4f} ± {std_cv_gb:.4f}\")\n",
    "print(f\"SVC:                 {mean_cv_svc:.4f} ± {std_cv_svc:.4f}\")\n",
    "print(f\"\\nBaseline RF (exp_001): 0.8361 ± 0.0069\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0225c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble: Average predictions from all models\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ENSEMBLE: Soft Voting (Average Probabilities)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Average all model predictions\n",
    "ensemble_preds = (test_preds_lr + test_preds_rf + test_preds_gb + test_preds_svc) / 4\n",
    "ensemble_binary = (ensemble_preds >= 0.5).astype(int)\n",
    "\n",
    "# For CV score of ensemble, we need to do proper OOF\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_ensemble = np.zeros(len(X))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Scale for LR and SVC\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train all models\n",
    "    lr = LogisticRegression(C=0.5, max_iter=1000, random_state=42)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=4, min_samples_split=10,\n",
    "                                 min_samples_leaf=4, criterion='entropy', random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=3,\n",
    "                                     min_samples_split=10, min_samples_leaf=4, subsample=0.8, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    svc = SVC(C=1.0, kernel='rbf', probability=True, random_state=42)\n",
    "    svc.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Average predictions\n",
    "    val_pred_proba = (lr.predict_proba(X_val_scaled)[:, 1] + \n",
    "                      rf.predict_proba(X_val)[:, 1] + \n",
    "                      gb.predict_proba(X_val)[:, 1] + \n",
    "                      svc.predict_proba(X_val_scaled)[:, 1]) / 4\n",
    "    oof_ensemble[val_idx] = val_pred_proba\n",
    "\n",
    "ensemble_oof_binary = (oof_ensemble >= 0.5).astype(int)\n",
    "ensemble_cv = accuracy_score(y, ensemble_oof_binary)\n",
    "print(f\"\\nEnsemble CV Accuracy: {ensemble_cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission with ensemble\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_ids.astype(int),\n",
    "    'Survived': ensemble_binary\n",
    "})\n",
    "\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(f\"Submission saved with {len(submission)} predictions\")\n",
    "print(f\"Predicted survival rate: {ensemble_binary.mean():.3f}\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Previous baseline (exp_001): CV = 0.8361, LB = 0.7799\")\n",
    "print(f\"This experiment ensemble:    CV = {ensemble_cv:.4f}\")\n",
    "print(f\"\\nChanges made:\")\n",
    "print(\"- Added Ticket_Group_Size and FarePerPerson features\")\n",
    "print(\"- Removed AgeBin and FareBin (potential overfitting)\")\n",
    "print(\"- Used simpler/regularized models\")\n",
    "print(\"- Ensemble of LR, shallow RF, GB, SVC\")\n",
    "print(f\"\\nExpected: Lower CV but hopefully better LB correlation\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
