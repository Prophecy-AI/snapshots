{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b25db1",
   "metadata": {},
   "source": [
    "# Loop 1 LB Feedback Analysis\n",
    "\n",
    "## Submission Results\n",
    "- **CV Score**: 0.8361\n",
    "- **LB Score**: 0.7799\n",
    "- **Gap**: +0.0562 (CV is overly optimistic by ~5.6%)\n",
    "\n",
    "## Key Questions\n",
    "1. Why is CV so much higher than LB?\n",
    "2. Is there distribution shift between train and test?\n",
    "3. What can we do to close this gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ff3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions between train and test\n",
    "print(\"=== Distribution Comparison ===\")\n",
    "\n",
    "print(\"\\n1. Sex Distribution:\")\n",
    "print(\"Train:\", train['Sex'].value_counts(normalize=True).to_dict())\n",
    "print(\"Test:\", test['Sex'].value_counts(normalize=True).to_dict())\n",
    "\n",
    "print(\"\\n2. Pclass Distribution:\")\n",
    "print(\"Train:\", train['Pclass'].value_counts(normalize=True).sort_index().to_dict())\n",
    "print(\"Test:\", test['Pclass'].value_counts(normalize=True).sort_index().to_dict())\n",
    "\n",
    "print(\"\\n3. Age Statistics:\")\n",
    "print(f\"Train: mean={train['Age'].mean():.1f}, std={train['Age'].std():.1f}\")\n",
    "print(f\"Test: mean={test['Age'].mean():.1f}, std={test['Age'].std():.1f}\")\n",
    "\n",
    "print(\"\\n4. Fare Statistics:\")\n",
    "print(f\"Train: mean={train['Fare'].mean():.1f}, std={train['Fare'].std():.1f}\")\n",
    "print(f\"Test: mean={test['Fare'].mean():.1f}, std={test['Fare'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the gap more deeply\n",
    "# The CV-LB gap of 0.056 is significant\n",
    "# This suggests either:\n",
    "# 1. Overfitting to training data patterns\n",
    "# 2. Distribution shift between train and test\n",
    "# 3. Features that work well on train but not test\n",
    "\n",
    "# Let's check the actual survival rate we'd expect\n",
    "# If test has similar distribution to train, expected survival should be ~38%\n",
    "\n",
    "# Our model's predictions on test\n",
    "# From the submission, we can check what we predicted\n",
    "submission = pd.read_csv('/home/code/submission_candidates/candidate_000.csv')\n",
    "print(f\"Predicted survival rate in test: {submission['Survived'].mean():.3f}\")\n",
    "print(f\"Training survival rate: {train['Survived'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze what the LB score of 0.7799 means\n",
    "# 0.7799 accuracy on 418 test samples = ~326 correct predictions\n",
    "# 0.8361 CV accuracy on 891 train samples = ~745 correct predictions (in CV)\n",
    "\n",
    "lb_score = 0.7799\n",
    "cv_score = 0.8361\n",
    "test_size = 418\n",
    "train_size = 891\n",
    "\n",
    "print(f\"LB: {lb_score:.4f} accuracy = ~{int(lb_score * test_size)} correct out of {test_size}\")\n",
    "print(f\"CV: {cv_score:.4f} accuracy = ~{int(cv_score * train_size)} correct out of {train_size}\")\n",
    "print(f\"\\nGap: {cv_score - lb_score:.4f} = {(cv_score - lb_score) * 100:.1f}% points\")\n",
    "print(f\"\\nThis gap is LARGE for Titanic. Typical gaps are 0.01-0.03.\")\n",
    "print(f\"A gap of 0.056 suggests significant overfitting or distribution shift.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900eab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there's a pattern in what we might be getting wrong\n",
    "# Key insight: The test set might have different characteristics\n",
    "\n",
    "# Check Embarked distribution\n",
    "print(\"Embarked Distribution:\")\n",
    "print(\"Train:\", train['Embarked'].value_counts(normalize=True).to_dict())\n",
    "print(\"Test:\", test['Embarked'].value_counts(normalize=True).to_dict())\n",
    "\n",
    "# Check SibSp/Parch distribution\n",
    "print(\"\\nSibSp Distribution:\")\n",
    "print(\"Train:\", train['SibSp'].value_counts(normalize=True).sort_index().head().to_dict())\n",
    "print(\"Test:\", test['SibSp'].value_counts(normalize=True).sort_index().head().to_dict())\n",
    "\n",
    "print(\"\\nParch Distribution:\")\n",
    "print(\"Train:\", train['Parch'].value_counts(normalize=True).sort_index().head().to_dict())\n",
    "print(\"Test:\", test['Parch'].value_counts(normalize=True).sort_index().head().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Title and compare\n",
    "import re\n",
    "\n",
    "def extract_title(name):\n",
    "    match = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return 'Unknown'\n",
    "\n",
    "train['Title'] = train['Name'].apply(extract_title)\n",
    "test['Title'] = test['Name'].apply(extract_title)\n",
    "\n",
    "# Group rare titles\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
    "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare', 'Capt': 'Rare',\n",
    "    'Lady': 'Rare', 'Countess': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Dona': 'Rare',\n",
    "    'Jonkheer': 'Rare'\n",
    "}\n",
    "train['Title'] = train['Title'].map(lambda x: title_mapping.get(x, 'Rare'))\n",
    "test['Title'] = test['Title'].map(lambda x: title_mapping.get(x, 'Rare'))\n",
    "\n",
    "print(\"Title Distribution:\")\n",
    "print(\"Train:\", train['Title'].value_counts(normalize=True).to_dict())\n",
    "print(\"Test:\", test['Title'].value_counts(normalize=True).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ba7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The distributions look similar\n",
    "# The gap is likely due to:\n",
    "# 1. Random variation in small test set (418 samples)\n",
    "# 2. Model overfitting to training patterns\n",
    "# 3. Some edge cases in test that don't match training patterns\n",
    "\n",
    "# Let's check the gender submission baseline\n",
    "gender_sub = pd.read_csv('/home/data/gender_submission.csv')\n",
    "print(f\"Gender submission survival rate: {gender_sub['Survived'].mean():.3f}\")\n",
    "print(f\"This is the 'all females survive' baseline\")\n",
    "\n",
    "# Compare with our predictions\n",
    "print(f\"\\nOur predicted survival rate: {submission['Survived'].mean():.3f}\")\n",
    "print(f\"Difference: {submission['Survived'].mean() - gender_sub['Survived'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8826f",
   "metadata": {},
   "source": [
    "## Analysis Summary\n",
    "\n",
    "### CV-LB Gap Interpretation\n",
    "- **Gap of +0.056 is significant** - suggests our model is overfitting to training patterns\n",
    "- The distributions between train and test look similar, so it's not obvious distribution shift\n",
    "- The gap could be due to:\n",
    "  1. Model complexity (RF with depth 6 might be overfitting)\n",
    "  2. Features that capture training-specific patterns\n",
    "  3. Random variation in the small test set\n",
    "\n",
    "### Implications for Strategy\n",
    "1. **Simpler models might generalize better** - consider reducing complexity\n",
    "2. **Ensemble diversity is key** - different models might capture different patterns\n",
    "3. **Feature selection** - some features might be overfitting\n",
    "4. **The target of 1.0 is still unrealistic** - best legitimate scores are ~0.80-0.82\n",
    "\n",
    "### Next Steps\n",
    "1. Try simpler models (Logistic Regression, shallow trees)\n",
    "2. Implement ensemble with diverse models\n",
    "3. Add ticket-based features (might help with group survival patterns)\n",
    "4. Consider regularization to reduce overfitting"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
