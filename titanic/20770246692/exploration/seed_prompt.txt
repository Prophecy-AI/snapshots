# Titanic Survival Prediction - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution
- Key insight: 891 train samples, 418 test samples, binary classification (38.4% survived)
- Critical survival factors: Sex (female 74% vs male 19%), Pclass (1st: 63%, 2nd: 47%, 3rd: 24%)

## Feature Engineering (Critical for High Scores)

### Title Extraction from Name
Extract titles using regex pattern ` ([A-Za-z]+)\.` from Name field:
- Common titles: Mr, Mrs, Miss, Master
- Group rare titles (Lady, Countess, Capt, Col, Don, Dr, Major, Rev, Sir, Jonkheer, Dona) into "Rare"
- Map Mlle/Ms -> Miss, Mme -> Mrs
- Title is highly predictive of survival (Master = young male, Miss = unmarried female)

### Family Features
- FamilySize = SibSp + Parch + 1
- IsAlone = 1 if FamilySize == 1, else 0
- Family size of 2-4 has higher survival rate than alone or large families

### Cabin Features
- Has_Cabin = 1 if Cabin is not null, else 0 (proxy for wealth/class)
- Cabin deck letter (first character) can be extracted but many missing values

### Age Handling
- ~20% missing values - impute using median Age per (Sex, Pclass) group
- Alternative: random values within mean ± std
- Bin Age into 5 categories (0-16, 16-32, 32-48, 48-64, 64+)
- Children (Age < 16) have higher survival rates

### Fare Handling
- Fill missing with median
- Bin into 4 quantile-based categories using pd.qcut
- Higher fare correlates with higher survival

### Name Length
- Name_length = len(Name) - can be weakly predictive

## Missing Value Handling
- Age: Impute with median by (Sex, Pclass) group or random within mean±std
- Cabin: Create Has_Cabin binary feature, then drop Cabin
- Embarked: Fill with mode ('S' - Southampton)
- Fare: Fill with median

## Feature Selection
**Features to DROP:**
- PassengerId (identifier only)
- Name (after extracting Title)
- Ticket (too many unique values, hard to encode meaningfully)
- Cabin (after creating Has_Cabin)
- SibSp (replaced by FamilySize)

**Final features typically used:**
- Pclass, Sex, Age (binned), Fare (binned), Embarked
- Title, FamilySize, IsAlone, Has_Cabin

## Models

### Single Models (baseline ~77-80% accuracy)
1. **Random Forest** - Most popular, handles feature interactions well
   - n_estimators: 100-500
   - max_depth: 4-8 (prevent overfitting)
   - criterion: 'entropy' often better than 'gini'

2. **Gradient Boosting** - Strong performer
   - learning_rate: 0.01-0.25
   - n_estimators: 50-300
   - max_depth: 2-6

3. **XGBoost** - Often best single model
   - learning_rate: 0.01-0.1
   - max_depth: 4-6
   - n_estimators: 100-2000
   - min_child_weight: 2
   - subsample: 0.8
   - colsample_bytree: 0.8

4. **SVM** - Good with proper feature scaling
   - Requires feature normalization
   - RBF kernel typically works well

5. **Logistic Regression** - Good baseline, interpretable
   - Useful for understanding feature importance via coefficients

### Ensemble Methods (for higher scores ~80-82%)

#### Voting Ensemble
- Combine 5+ diverse models (RF, ET, Ada, GB, SVC)
- Hard voting: majority vote
- Soft voting: average probabilities (requires predict_proba)
- Tune each base model individually first

#### Stacking (Best approach for this competition)
**Two-level stacking architecture:**

Level 1 (Base models with out-of-fold predictions):
- RandomForestClassifier
- ExtraTreesClassifier  
- AdaBoostClassifier
- GradientBoostingClassifier
- SVC (with probability=True)

Level 2 (Meta-learner):
- XGBoost or Logistic Regression
- Train on out-of-fold predictions from Level 1

**Key insight:** Use diverse, uncorrelated base models for better stacking results.

## Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV
- Cross-validation: StratifiedKFold with k=5 or k=10
- Scoring metric: 'accuracy' (matches competition metric)
- Important: Tune on CV score, not train score

## Validation Strategy
- StratifiedKFold (k=5 or k=10) to maintain class balance
- Watch for overfitting: large gap between train and CV scores indicates overfit
- Small dataset (891 samples) - be careful not to overfit

## Common Pitfalls
1. Overfitting to train set - use proper CV
2. Data leakage - ensure test set preprocessing uses only train statistics
3. Not combining train+test for consistent encoding of categorical features
4. Ignoring feature interactions (tree models handle this naturally)

## Expected Scores
- Baseline (gender only): ~0.766
- Good feature engineering + single model: ~0.77-0.78
- Tuned ensemble/stacking: ~0.80-0.82
- Top scores require careful feature engineering + ensemble

## Implementation Tips
1. Combine train and test DataFrames for consistent feature engineering
2. Apply same transformations to both datasets
3. Use pipelines to prevent data leakage
4. Try multiple random seeds and average predictions
