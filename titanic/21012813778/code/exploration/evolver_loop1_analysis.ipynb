{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a6fbcb",
   "metadata": {},
   "source": [
    "# Evolver Loop 1 Analysis\n",
    "\n",
    "Analysis of current state and identification of improvement opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('/home/data/train.csv')\n",
    "test_df = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(\"\\nTrain info:\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous experiment results\n",
    "with open('/home/code/experiments/001_baseline_results.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "print(\"Baseline Results:\")\n",
    "print(f\"Mean CV Score: {baseline_results['mean_cv_score']:.4f} Â± {baseline_results['std_cv_score']:.4f}\")\n",
    "print(f\"OOF Accuracy: {baseline_results['oof_accuracy']:.4f}\")\n",
    "print(f\"CV Scores: {[f'{score:.4f}' for score in baseline_results['cv_scores']]}\")\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame(baseline_results['feature_importance'])\n",
    "print(\"\\nTop Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "print(\"Missing Values in Train:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nMissing Values in Test:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Analyze target distribution\n",
    "print(f\"\\nTarget distribution in train:\")\n",
    "print(train_df['Survived'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63edd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions by target\n",
    "# Sex feature\n",
    "print(\"Survival by Sex:\")\n",
    "sex_survival = train_df.groupby('Sex')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "sex_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(sex_survival)\n",
    "\n",
    "# Pclass feature\n",
    "print(\"\\nSurvival by Pclass:\")\n",
    "pclass_survival = train_df.groupby('Pclass')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "pclass_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(pclass_survival)\n",
    "\n",
    "# Embarked feature\n",
    "print(\"\\nSurvival by Embarked:\")\n",
    "embarked_survival = train_df.groupby('Embarked')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "embarked_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(embarked_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a22422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Age patterns\n",
    "print(\"Age statistics:\")\n",
    "print(train_df['Age'].describe())\n",
    "\n",
    "# Create age groups and analyze survival\n",
    "age_bins = [0, 12, 18, 35, 60, 100]\n",
    "age_labels = ['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior']\n",
    "train_df['AgeGroup'] = pd.cut(train_df['Age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "print(\"\\nSurvival by AgeGroup:\")\n",
    "age_survival = train_df.groupby('AgeGroup')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "age_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(age_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e700ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze family features\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "print(\"Survival by FamilySize:\")\n",
    "family_survival = train_df.groupby('FamilySize')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "family_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(family_survival.head(10))\n",
    "\n",
    "# Create IsAlone feature\n",
    "train_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\n",
    "print(\"\\nSurvival by IsAlone:\")\n",
    "alone_survival = train_df.groupby('IsAlone')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "alone_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(alone_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Fare patterns\n",
    "print(\"Fare statistics:\")\n",
    "print(train_df['Fare'].describe())\n",
    "\n",
    "# Create fare bins\n",
    "train_df['FareBin'] = pd.qcut(train_df['Fare'], q=5, labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'])\n",
    "print(\"\\nSurvival by FareBin:\")\n",
    "fare_survival = train_df.groupby('FareBin')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "fare_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(fare_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Title patterns (extracted from Name)\n",
    "train_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "\n",
    "print(\"Title distribution:\")\n",
    "print(train_df['Title'].value_counts())\n",
    "\n",
    "print(\"\\nSurvival by Title:\")\n",
    "title_survival = train_df.groupby('Title')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "title_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(title_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Cabin patterns\n",
    "train_df['HasCabin'] = train_df['Cabin'].notna().astype(int)\n",
    "train_df['CabinLetter'] = train_df['Cabin'].str[0]\n",
    "train_df['CabinLetter'] = train_df['CabinLetter'].fillna('Unknown')\n",
    "\n",
    "print(\"Survival by HasCabin:\")\n",
    "cabin_survival = train_df.groupby('HasCabin')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "cabin_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(cabin_survival)\n",
    "\n",
    "print(\"\\nSurvival by CabinLetter:\")\n",
    "cabin_letter_survival = train_df.groupby('CabinLetter')['Survived'].agg(['count', 'sum', 'mean'])\n",
    "cabin_letter_survival.columns = ['Total', 'Survived', 'Survival_Rate']\n",
    "print(cabin_letter_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential new features\n",
    "print(\"Potential new feature ideas:\")\n",
    "\n",
    "# 1. Name length\n",
    "train_df['NameLength'] = train_df['Name'].str.len()\n",
    "print(f\"\\n1. NameLength correlation with survival: {train_df['NameLength'].corr(train_df['Survived']):.4f}\")\n",
    "\n",
    "# 2. Ticket patterns - check if ticket has numbers only or mixed\n",
    "train_df['TicketPrefix'] = train_df['Ticket'].str.extract('([A-Za-z]+)', expand=False)\n",
    "train_df['TicketPrefix'] = train_df['TicketPrefix'].fillna('None')\n",
    "print(\"\\n2. TicketPrefix distribution:\")\n",
    "print(train_df['TicketPrefix'].value_counts().head())\n",
    "\n",
    "# 3. Fare per person (already in baseline)\n",
    "train_df['FarePerPerson'] = train_df['Fare'] / train_df['FamilySize']\n",
    "print(f\"\\n3. FarePerPerson correlation with survival: {train_df['FarePerPerson'].corr(train_df['Survived']):.4f}\")\n",
    "\n",
    "# 4. Family survival rate (would need cross-validation to avoid leakage)\n",
    "print(\"\\n4. Family survival rate feature - needs careful implementation to avoid leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data leakage concerns\n",
    "print(\"Data Leakage Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1. Current preprocessing approach:\")\n",
    "print(\"- Median imputation: Done globally before CV\")\n",
    "print(\"- Label encoding: Fit on combined train+test before CV\")\n",
    "print(\"- This leaks test distribution information into training\")\n",
    "\n",
    "print(\"\\n2. Impact on CV scores:\")\n",
    "print(f\"- Current CV score: {baseline_results['mean_cv_score']:.4f}\")\n",
    "print(\"- This may be optimistic due to leakage\")\n",
    "print(\"- LB score may be lower than CV\")\n",
    "\n",
    "print(\"\\n3. What needs to be fixed:\")\n",
    "print(\"- Move ALL preprocessing inside CV loop\")\n",
    "print(\"- Fit imputers/encoders on train fold only\")\n",
    "print(\"- Transform val/test with fitted transformers\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
