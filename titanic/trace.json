{
  "spans": [
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "ea9d04c98ae90fb4",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684829692779158,
      "end_time": 1767684834100779902,
      "duration": 4.408000744,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}]",
        "llm.input_tokens": 4710,
        "llm.output_tokens": 220,
        "llm.total_tokens": 4930,
        "llm.latency_ms": 4408.04386138916,
        "llm.completion": "REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\n[Tool Use: Read]\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]",
        "llm.reasoning": "REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.",
        "llm.cost_usd": 0.01743
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "a0912e6d9e1b2630",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767684834101028569,
      "end_time": 1767684834101677126,
      "duration": 0.000648557,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x742e515b9c90>\", \"input\": {\"file_path\": \"../research/description.md\"}}",
        "tool.output": "{\"content\": \"# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\\\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\\\" target=\\\"_blank\\\">\\\"Join Competition button</a> to create an account and gain access to the <a href=\\\"https://www.kaggle.com/c/titanic/data\\\" target=\\\"_blank\\\">competition data</a>. Then check out <a href=\\\"https://www.kaggle.com/alexisbcook/titanic-tutorial\\\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\\\"699\\\" height=\\\"368\\\" src=\\\"https://www.youtube.com/embed/8yZMXCaFshs\\\" title=\\\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\\\" frameborder=\\\"0\\\" allow=\\\"encrypted-media\\\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\" target=\\\"_blank\\\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\\\"https://www.kaggle.com/c/titanic/data\\\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\\\"699\\\" align=\\\"center\\\" src=\\\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\\\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\\\"699\\\" align=\\\"center\\\" src=\\\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\\\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\\\"https://www.kaggle.com/c/titanic/notebooks\\\">here</a>.\\n\\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\\\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\\\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\\\"_blank\\\" href=\\\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\\\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\\\"https://www.kaggle.com/c/titanic/data\\\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\\\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\\\" target=\\\"_blank\\\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\\\"width: 100%;\\\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\\\"https://www.kaggle.com/c/titanic/team\\\" target=\\\"_blank\\\">Team</a> page.</p>\\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\" target=\\\"_blank\\\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\\\"https://www.kaggle.com/c/titanic/notebooks\\\" target=\\\"_blank\\\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\\\"https://www.kaggle.com/learn/overview\\\" target=\\\"&quot;_blank\\\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\\\"I worked so hard to get that score! Give it back!\\\"</i> Read more about our decision to implement a rolling leaderboard <a href=\\\"https://www.kaggle.com/c/titanic/discussion/6240\\\" target=\\\"_blank\\\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\\\"https://www.kaggle.com/c/titanic/discussion\\\" target=\\\"_blank\\\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\\\"https://www.kaggle.com/contact\\\" target=\\\"_blank\\\">contact us</a>.</p>\", \"is_error\": false}",
        "tool.duration_ms": 0.48923492431640625
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "233e8073ee596306",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684834101820054,
      "end_time": 1767684834101960892,
      "duration": 0.000140838,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"../research/writeups\"}}",
        "tool.output": "{\"content\": \"Not a directory: /home/code/exploration/../research/writeups\", \"is_error\": true}",
        "tool.duration_ms": 0.1804828643798828
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "2fc191e3a75137cb",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684834102043401,
      "end_time": 1767684834102247781,
      "duration": 0.00020438,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"../research/kernels\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/\", \"is_error\": false}",
        "tool.duration_ms": 0.21529197692871094
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "52740a8275ab22e8",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684834102356011,
      "end_time": 1767684834102653862,
      "duration": 0.000297851,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"/home/data\"}}",
        "tool.output": "{\"content\": \"/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)\", \"is_error\": false}",
        "tool.duration_ms": 0.3063678741455078
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "b3093f5986e3a38b",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684834127637253,
      "end_time": 1767684839779381101,
      "duration": 5.651743848,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}]",
        "llm.input_tokens": 9288,
        "llm.output_tokens": 254,
        "llm.total_tokens": 9542,
        "llm.latency_ms": 5651.725292205811,
        "llm.completion": "REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\n[Tool Use: Read]\n[Tool Use: Read]",
        "llm.reasoning": "REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.",
        "llm.cost_usd": 0.031674
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "ed29581aa67d5e8e",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767684839779646138,
      "end_time": 1767684839779907447,
      "duration": 0.000261309,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x742e515b9c90>\", \"input\": {\"file_path\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py\"}}",
        "tool.output": "{\"content\": \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\", \"is_error\": true}",
        "tool.duration_ms": 0.31065940856933594
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "218b83700db23a00",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767684839780015731,
      "end_time": 1767684839780178814,
      "duration": 0.000163083,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x742e515b9c90>\", \"input\": {\"file_path\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py\"}}",
        "tool.output": "{\"content\": \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\", \"is_error\": true}",
        "tool.duration_ms": 0.17261505126953125
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "75badf0489aea4de",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684840667881439,
      "end_time": 1767684844485541479,
      "duration": 3.81766004,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}]",
        "llm.input_tokens": 9737,
        "llm.output_tokens": 210,
        "llm.total_tokens": 9947,
        "llm.latency_ms": 3817.6486492156982,
        "llm.completion": "REASONING: Let me check the actual file names in these kernel directories.\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]\n[Tool Use: ListFiles]",
        "llm.reasoning": "REASONING: Let me check the actual file names in these kernel directories.",
        "llm.cost_usd": 0.032361
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "f82e89361ef4059e",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684844485777597,
      "end_time": 1767684844486045304,
      "duration": 0.000267707,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)\", \"is_error\": false}",
        "tool.duration_ms": 0.32210350036621094
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "5a2420bb713a0a4e",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684844486142620,
      "end_time": 1767684844486292417,
      "duration": 0.000149797,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)\", \"is_error\": false}",
        "tool.duration_ms": 0.17261505126953125
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "c85edc23f5736776",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684844486366768,
      "end_time": 1767684844486600868,
      "duration": 0.0002341,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"../research/kernels/startupsci_titanic-data-science-solutions\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\", \"is_error\": false}",
        "tool.duration_ms": 0.209808349609375
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "eee89279f831ce4a",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684845302554773,
      "end_time": 1767684851461411499,
      "duration": 6.158856726,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}]",
        "llm.input_tokens": 10237,
        "llm.output_tokens": 261,
        "llm.total_tokens": 10498,
        "llm.latency_ms": 6158.971309661865,
        "llm.completion": "REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\n[Tool Use: Bash]",
        "llm.reasoning": "REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.",
        "llm.cost_usd": 0.034626000000000004
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "3216def8519894a9",
      "parent_span_id": "5b39285dbbf762da",
      "name": "BashTool",
      "kind": "INTERNAL",
      "start_time": 1767684851461654870,
      "end_time": 1767684851491721342,
      "duration": 0.030066472,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "BashTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.BashTool object at 0x742e515b9f10>\", \"input\": {\"command\": \"cd /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy && python3 -c \\\"\\nimport json\\nwith open('a-data-science-framework-to-achieve-99-accuracy.ipynb') as f:\\n    nb = json.load(f)\\nfor i, cell in enumerate(nb['cells'][:30]):  # First 30 cells\\n    if cell['cell_type'] == 'markdown':\\n        print(f'=== MARKDOWN {i} ===')\\n        print(''.join(cell['source'])[:500])\\n    else:\\n        print(f'=== CODE {i} ===')\\n        print(''.join(cell['source'])[:500])\\n    print()\\n\\\"\"}}",
        "tool.output": "{\"content\": \"=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\\\"ch1\\\"></a>\\n# How a Data Scientist Beat the Odds\\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\\\"ch2\\\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\\\"ch3\\\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\\\"ch4\\\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\\\"Python version: {}\\\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\\\"pandas version: {}\\\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv('../input/train.csv')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv('../input/test.csv')\\n\\n\\n#to play with our data we'll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\\\"ch5\\\"></a>\\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint('Train columns with null values:\\\\n', data1.isnull().sum())\\nprint(\\\"-\\\"*10)\\n\\nprint('Test/Validation columns with null values:\\\\n', data_val.isnull().sum())\\nprint(\\\"-\\\"*10)\\n\\ndata_raw.describe(include = 'all')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let's execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\\n\\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset['Title'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\\n    dataset['FareBin_Code'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we've cleaned our data, let's do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint('Train columns with null values: \\\\n', data1.isnull().sum())\\nprint(\\\"-\\\"*10)\\nprint (data1.info())\\nprint(\\\"-\\\"*10)\\n\\nprint('Test/Validation columns with null values: \\\\n', data_val.isnull().sum())\\nprint(\\\"-\\\"*10)\\nprint (data_val.info())\\nprint(\\\"-\\\"*10)\\n\\ndata_raw.describe(include = 'all')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\\\"ch6\\\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != 'float64' :\\n        print('Survival Correlation by:', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print('-'*10, '\\\\n')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1['Title'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let's compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\\naxis1.set_title('Pclass vs Fare Survival Comparison')\\n\\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\\naxis2.set_title('Pclass vs Age Survival Comparison')\\n\\nsns.boxplot(x = 'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\\naxis1.set_title('Sex vs Embarked Survival Comparison')\\n\\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\\naxis1.set_title('Sex vs Pclass Survival Comparison')\\n\\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\\\"FamilySize\\\", y=\\\"Survived\\\", hue=\\\"Sex\\\", data=data1,\\n              palette={\\\"male\\\": \\\"blue\\\", \\\"female\\\": \\\"pink\\\"},\\n              markers=[\\\"*\\\", \\\"o\\\"], linestyles=[\\\"-\\\", \\\"--\\\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\\\"Pclass\\\", y=\\\"Survived\\\", hue=\\\"Sex\\\", data=data1,\\n              palette={\\\"male\\\": \\\"blue\\\", \\\"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = 'Embarked')\\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\\na.map(sns.kdeplot, 'Age', shade= True )\\na.set(xlim=(0 , data1['Age'].max()))\\na.add_legend()\\n\\n\", \"is_error\": false}",
        "tool.duration_ms": 29.956817626953125
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "fc1f3f6a18819366",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684852346204115,
      "end_time": 1767684861181448327,
      "duration": 8.835244212,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}]",
        "llm.input_tokens": 14687,
        "llm.output_tokens": 476,
        "llm.total_tokens": 15163,
        "llm.latency_ms": 8835.250854492188,
        "llm.completion": "REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.\n[Tool Use: Bash]\n[Tool Use: Bash]",
        "llm.reasoning": "REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.",
        "llm.cost_usd": 0.051200999999999997
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "fdb0ba0c88319e15",
      "parent_span_id": "5b39285dbbf762da",
      "name": "BashTool",
      "kind": "INTERNAL",
      "start_time": 1767684861181726908,
      "end_time": 1767684861211444167,
      "duration": 0.029717259,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "BashTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.BashTool object at 0x742e515b9f10>\", \"input\": {\"command\": \"cd /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy && python3 -c \\\"\\nimport json\\nwith open('a-data-science-framework-to-achieve-99-accuracy.ipynb') as f:\\n    nb = json.load(f)\\nfor i, cell in enumerate(nb['cells'][30:50]):  # Cells 30-50\\n    if cell['cell_type'] == 'markdown':\\n        print(f'=== MARKDOWN {i+30} ===')\\n        print(''.join(cell['source'])[:600])\\n    else:\\n        print(f'=== CODE {i+30} ===')\\n        print(''.join(cell['source'])[:600])\\n    print()\\n\\\"\"}}",
        "tool.output": "{\"content\": \"=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\\nh.map(plt.hist, 'Age', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={'shrink':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor='white',\\n        annot_kws={'fontsize':12 }\\n    )\\n    \\n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\\\"ch7\\\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title('Machine Learning Algorithm Accuracy Score \\\\n')\\nplt.xlabel('Accuracy Score (%)')\\nplt.ylabel('Algorithm')\\n\\n=== MARKDOWN 36 ===\\n<a id=\\\"ch8\\\"></a>\\n## 5.1 Evaluate Model Performance\\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\\nprint('Survival Decision Tree w/Female Node: \\\\n',pivot_female)\\n\\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\\nprint('\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {'Predict':[]})\\n    male_title = ['Master'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, 'Predict'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, 'Sex'] == 'female'):\\n                  Model.loc[index, 'Predict'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title='Confusion matrix',\\n                          cmap=plt.cm.Blues):\\n    \\\"\\\"\\\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \\\"\\\"\\\"\\n    if normalize:\\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\\\"Normalized confusion matrix\\\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \\\"predicting\\\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\\\"ch9\\\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint('BEFORE DT Parameters: ', dtree.get_params())\\nprint(\\\"BEFORE DT Training w/bin score mean: {:.2f}\\\". format(base_results['train_score'].mean()*100)) \\nprint(\\\"BEFORE DT Test w/bin score mean: {:.2f}\\\". format(base_results['test_score'].mean()*100))\\nprint(\\\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\\\". format(base_results['test_score'].std()*100*3))\\n#print(\\\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\\\"ch10\\\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \\nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\\n\\nprint(\\\"BEFORE DT RFE Training w/bin score mean: {:.2f}\\\". format(base_results['train_score'].mean()*100)) \\nprint(\\\"BEFORE DT RFE Test w/bin score mean: {:.2f}\\\". format(base_results['test_score'].mean()*100))\\nprint(\\\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\\\". format(base_results['test_score'].std()*100*3))\\nprint('-'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\\\"ch11\\\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1's, but enough blues and light reds to create a \\\"super algorithm\\\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    ('ada', ensemble.AdaBoostClassifier()),\\n    ('bc', ensemble.BaggingClassifier()),\\n    ('etc',ensemble.ExtraTreesClassifier()),\\n    ('gbc', ensemble.GradientBoostingClassifier()),\\n    ('rfc', ensemble.RandomForestClassif\\n\\n\", \"is_error\": false}",
        "tool.duration_ms": 29.668331146240234
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "4e7a8d7606b126e9",
      "parent_span_id": "5b39285dbbf762da",
      "name": "BashTool",
      "kind": "INTERNAL",
      "start_time": 1767684861211609259,
      "end_time": 1767684861240181870,
      "duration": 0.028572611,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "BashTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.BashTool object at 0x742e515b9f10>\", \"input\": {\"command\": \"cd /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python && python3 -c \\\"\\nimport json\\nwith open('introduction-to-ensembling-stacking-in-python.ipynb') as f:\\n    nb = json.load(f)\\nfor i, cell in enumerate(nb['cells']):\\n    if cell['cell_type'] == 'markdown':\\n        print(f'=== MARKDOWN {i} ===')\\n        print(''.join(cell['source'])[:600])\\n    else:\\n        print(f'=== CODE {i} ===')\\n        print(''.join(cell['source'])[:600])\\n    print()\\n\\\"\"}}",
        "tool.output": "{\"content\": \"=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv('../input/train.csv')\\ntest = pd.read_csv('../input/test.csv')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test['PassengerId']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain['Name_length'] = train['Name'].apply(len)\\ntest['Name_length'] = test['Name'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain['Has_Cabin'] = train[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest['Has_Cabin'] = test[\\\"Cabin\\\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset['FamilySize'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor='white', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params['random_state'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \\\"seen\\\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    'n_jobs': -1,\\n    'n_estimators': 500,\\n     'warm_start': True, \\n     #'max_features': 0.2,\\n    'max_depth': 6,\\n    'min_samples_leaf': 2,\\n    'max_features' : 'sqrt',\\n    'verbose': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    'n_jobs': -1,\\n    'n_estimators':500,\\n    #'max_features': 0.5,\\n    'max_depth': 8,\\n    'min_samples_leaf': 2,\\n    'verbose': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    'n_estimators': 500,\\n    'learning_rate' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train['Survived'].ravel()\\ntrain = train.drop(['Survived'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\\\"Training is complete\\\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {'features': cols,\\n     'Random Forest feature importances': rf_features,\\n     'Extra Trees  feature importances': et_features,\\n      'AdaBoost feature importances': ada_features,\\n    'Gradient Boost feature importances': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \\\"Scatter\\\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe['Random Forest feature importances'].values,\\n    x = feature_dataframe['features'].values,\\n    mode='markers',\\n    marker=dict(\\n        sizemode = 'diameter',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe['AdaBoost feature importances'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe['Random Forest feature importances'].values,\\n        colorscale='Portland',\\n        showscale=True\\n    ),\\n    text = feature_dataframe['features'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe['mean'].values\\nx = feature_dataframe['features'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe['mean'].values,\\n            colorscale='Portland',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= 'Barplots of Mean Feature Importance',\\n    hovermode= 'closest',\\n#     xaxis= dict(\\n#         title= 'Pop',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\\n     'ExtraTrees': et_oof_train.ravel(),\\n     'AdaBoost': ada_oof_train.ravel(),\\n      'GradientBoost': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale='Viridis',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename='labelled-heatmap')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= 'binary:logistic',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\\n                            'Survived': predictions })\\nStackingSubmission.to_csv(\\\"StackingSubmission.csv\\\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one's score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n\", \"is_error\": false}",
        "tool.duration_ms": 28.421640396118164
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "6bd1eb504a263dfd",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684862109716862,
      "end_time": 1767684873611675705,
      "duration": 11.501958843,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}]",
        "llm.input_tokens": 24382,
        "llm.output_tokens": 508,
        "llm.total_tokens": 24890,
        "llm.latency_ms": 11501.973152160645,
        "llm.completion": "REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\n[Tool Use: RecordFinding]\n[Tool Use: RecordFinding]\n[Tool Use: RecordFinding]",
        "llm.reasoning": "REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.",
        "llm.cost_usd": 0.080766
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "9aaaa5886f1b99e8",
      "parent_span_id": "5b39285dbbf762da",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767684873611899945,
      "end_time": 1767684873612849096,
      "duration": 0.000949151,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x742e515f0f90>\", \"input\": {\"finding\": \"Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted from Name; AgeBin and FareBin for binning continuous variables; Has_Cabin binary feature; Name_length\", \"source\": \"../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\", \"is_error\": false}",
        "tool.duration_ms": 0.972747802734375
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "62cb2d8c78d38fba",
      "parent_span_id": "5b39285dbbf762da",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767684873612969302,
      "end_time": 1767684873613660292,
      "duration": 0.00069099,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x742e515f0f90>\", \"input\": {\"finding\": \"Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) with Out-of-Fold predictions, then XGBoost as second-level meta-learner. Key params: RF/ET n_estimators=500, max_depth=6-8; XGB n_estimators=2000, max_depth=4, gamma=0.9\", \"source\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\", \"is_error\": false}",
        "tool.duration_ms": 0.7100105285644531
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "25e54a3380cf6699",
      "parent_span_id": "5b39285dbbf762da",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767684873613753476,
      "end_time": 1767684873614373547,
      "duration": 0.000620071,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x742e515f0f90>\", \"input\": {\"finding\": \"Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, Fare, FamilySize. Title is most predictive for AdaBoost (0.74 importance)\", \"source\": \"../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\", \"is_error\": false}",
        "tool.duration_ms": 0.6289482116699219
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "efb0124e373d84d0",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684874453427403,
      "end_time": 1767684880158787276,
      "duration": 5.705359873,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}]",
        "llm.input_tokens": 25192,
        "llm.output_tokens": 272,
        "llm.total_tokens": 25464,
        "llm.latency_ms": 5705.396413803101,
        "llm.completion": "REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.\n[Tool Use: CreateNotebook]",
        "llm.reasoning": "REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.",
        "llm.cost_usd": 0.079656
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "636584babba47a54",
      "parent_span_id": "5b39285dbbf762da",
      "name": "CreateNotebookTool",
      "kind": "INTERNAL",
      "start_time": 1767684880159068893,
      "end_time": 1767684881275649531,
      "duration": 1.116580638,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "CreateNotebookTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.CreateNotebookTool object at 0x742e515b9f90>\", \"input\": {\"notebook_path\": \"eda.ipynb\", \"cells\": [{\"cell_type\": \"code\", \"content\": \"import pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\"}]}}",
        "tool.output": "{\"content\": \"Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\", \"is_error\": false}",
        "tool.duration_ms": 1116.607427597046
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "4f321eaf235b48d4",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684882146646815,
      "end_time": 1767684886447710812,
      "duration": 4.301063997,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}]",
        "llm.input_tokens": 25510,
        "llm.output_tokens": 92,
        "llm.total_tokens": 25602,
        "llm.latency_ms": 4301.105737686157,
        "llm.completion": "[Tool Use: ExecuteCellRange]",
        "llm.cost_usd": 0.07791000000000001
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "022a201de84b2330",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767684886447955001,
      "end_time": 1767684886880692424,
      "duration": 0.432737423,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x742e515b9e10>\", \"input\": {\"start_cell\": 0, \"end_cell\": 0, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\", \"is_error\": false}",
        "tool.duration_ms": 432.75904655456543
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "6e5318dea78ab38f",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684887782798038,
      "end_time": 1767684892946762212,
      "duration": 5.163964174,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}]",
        "llm.input_tokens": 26092,
        "llm.output_tokens": 212,
        "llm.total_tokens": 26304,
        "llm.latency_ms": 5163.975954055786,
        "llm.completion": "[Tool Use: AddNotebookCell]",
        "llm.cost_usd": 0.081456
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "67cb2ab7c0f8fa15",
      "parent_span_id": "5b39285dbbf762da",
      "name": "AddNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767684892947026387,
      "end_time": 1767684892948840841,
      "duration": 0.001814454,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "AddNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.AddNotebookCellTool object at 0x742e515d4d50>\", \"input\": {\"cell_type\": \"code\", \"content\": \"# Target distribution and key statistics\\nprint('Target distribution:')\\nprint(train['Survived'].value_counts())\\\\nprint(f'\\\\\\\\nSurvival rate: {train[\\\"Survived\\\"].mean():.2%}')\\\\n\\\\nprint('\\\\\\\\nSurvival by Sex:')\\\\nprint(train.groupby('Sex')['Survived'].mean())\\\\n\\\\nprint('\\\\\\\\nSurvival by Pclass:')\\\\nprint(train.groupby('Pclass')['Survived'].mean())\\\\n\\\\nprint('\\\\\\\\nSample data:')\\\\nprint(train.head())\"}}",
        "tool.output": "{\"content\": \"Added code cell at 1\", \"is_error\": false}",
        "tool.duration_ms": 1.8737316131591797
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "dc7be82dd9dbb617",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684894663053356,
      "end_time": 1767684897935211285,
      "duration": 3.272157929,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}]",
        "llm.input_tokens": 26322,
        "llm.output_tokens": 92,
        "llm.total_tokens": 26414,
        "llm.latency_ms": 3272.1917629241943,
        "llm.completion": "[Tool Use: ExecuteCellRange]",
        "llm.cost_usd": 0.080346
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "d0b651c0cf3ba3af",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767684897935489167,
      "end_time": 1767684897944304876,
      "duration": 0.008815709,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x742e515b9e10>\", \"input\": {\"start_cell\": 1, \"end_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\", \"is_error\": true}",
        "tool.duration_ms": 8.89134407043457
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "3aa56a37133d15e1",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684897978313198,
      "end_time": 1767684902508158436,
      "duration": 4.529845238,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}]",
        "llm.input_tokens": 26449,
        "llm.output_tokens": 201,
        "llm.total_tokens": 26650,
        "llm.latency_ms": 4529.885053634644,
        "llm.completion": "[Tool Use: EditNotebookCell]",
        "llm.cost_usd": 0.082362
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "52cbc02186a5f229",
      "parent_span_id": "5b39285dbbf762da",
      "name": "EditNotebookCellTool",
      "kind": "INTERNAL",
      "start_time": 1767684902508380584,
      "end_time": 1767684902509913751,
      "duration": 0.001533167,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "EditNotebookCellTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.EditNotebookCellTool object at 0x742e515e6fd0>\", \"input\": {\"cell_index\": 1, \"new_content\": \"# Target distribution and key statistics\\nprint('Target distribution:')\\nprint(train['Survived'].value_counts())\\nprint(f'\\\\nSurvival rate: {train[\\\"Survived\\\"].mean():.2%}')\\n\\nprint('\\\\nSurvival by Sex:')\\nprint(train.groupby('Sex')['Survived'].mean())\\n\\nprint('\\\\nSurvival by Pclass:')\\nprint(train.groupby('Pclass')['Survived'].mean())\\n\\nprint('\\\\nSample data:')\\nprint(train.head())\"}}",
        "tool.output": "{\"content\": \"Edited cell 1. Re-run to update kernel state.\", \"is_error\": false}",
        "tool.duration_ms": 1.5642642974853516
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "0da18660114f3e84",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684904387135698,
      "end_time": 1767684907074247262,
      "duration": 2.687111564,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\ntool: Edited cell 1. Re-run to update kernel state.",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Edited cell 1. Re-run to update kernel state.'}]",
        "llm.input_tokens": 26675,
        "llm.output_tokens": 92,
        "llm.total_tokens": 26767,
        "llm.latency_ms": 2687.155246734619,
        "llm.completion": "[Tool Use: ExecuteCellRange]",
        "llm.cost_usd": 0.081405
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "2c9b08ca5c205bf9",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ExecuteCellRangeTool",
      "kind": "INTERNAL",
      "start_time": 1767684907074480754,
      "end_time": 1767684907091954971,
      "duration": 0.017474217,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ExecuteCellRangeTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.notebook.ExecuteCellRangeTool object at 0x742e515b9e10>\", \"input\": {\"start_cell\": 1, \"end_cell\": 1, \"duration\": 30}}",
        "tool.output": "{\"content\": \"Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Target distribution and key statistics\\nprint('Target distribution:')\\nprint(train['Survived'].value_counts())\\nprint(f'\\\\nSurvival rate: {train[\\\"Survived\\\"].mean():.2%}')\\n\\nprint('\\\\nSurvival by Sex:')\\nprint(train.groupby('Sex')['Survived'].mean())\\n\\nprint('\\\\nSurvival by Pclass:')\\nprint(train.groupby('Pclass')['Survived'].mean())\\n\\nprint('\\\\nSample data:')\\nprint(train.head())\\n```\\nOutput:\\nTarget distribution:\\nSurvived\\n0    549\\n1    342\\nName: count, dtype: int64\\n\\nSurvival rate: 38.38%\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\\nSample data:\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \\n\", \"is_error\": false}",
        "tool.duration_ms": 17.499446868896484
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "cafee0a9f8b9fb1d",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684907924597446,
      "end_time": 1767684914269916145,
      "duration": 6.345318699,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\ntool: Edited cell 1. Re-run to update kernel state.\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Target distribution and key statistics\nprint('Target distribution:')\nprint(train['Survived'].value_counts())\nprint(f'\\nSurvival rate: {train[\"Survived\"].mean():.2%}')\n\nprint('\\nSurvival by Sex:')\nprint(train.groupby('Sex')['Survived'].mean())\n\nprint('\\nSurvival by Pclass:')\nprint(train.groupby('Pclass')['Survived'].mean())\n\nprint('\\nSample data:')\nprint(train.head())\n```\nOutput:\nTarget distribution:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nSurvival rate: 38.38%\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\nSample data:\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Edited cell 1. Re-run to update kernel state.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Target distribution and key statistics\\nprint(\\'Target distribution:\\')\\nprint(train[\\'Survived\\'].value_counts())\\nprint(f\\'\\\\nSurvival rate: {train[\"Survived\"].mean():.2%}\\')\\n\\nprint(\\'\\\\nSurvival by Sex:\\')\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSurvival by Pclass:\\')\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSample data:\\')\\nprint(train.head())\\n```\\nOutput:\\nTarget distribution:\\nSurvived\\n0    549\\n1    342\\nName: count, dtype: int64\\n\\nSurvival rate: 38.38%\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\\nSample data:\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \\n'}]",
        "llm.input_tokens": 27359,
        "llm.output_tokens": 223,
        "llm.total_tokens": 27582,
        "llm.latency_ms": 6345.326900482178,
        "llm.completion": "REASONING: Now I have a good understanding of the data. Let me record key findings and write the seed_prompt.txt based on all the research.\n[Tool Use: RecordFinding]",
        "llm.reasoning": "REASONING: Now I have a good understanding of the data. Let me record key findings and write the seed_prompt.txt based on all the research.",
        "llm.cost_usd": 0.08542200000000001
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "0e699821692401f5",
      "parent_span_id": "5b39285dbbf762da",
      "name": "RecordFindingTool",
      "kind": "INTERNAL",
      "start_time": 1767684914270153967,
      "end_time": 1767684914271245279,
      "duration": 0.001091312,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "RecordFindingTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.findings.RecordFindingTool object at 0x742e515f0f90>\", \"input\": {\"finding\": \"Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Females 74.2% survival vs males 18.9%. Pclass 1: 63%, Pclass 2: 47%, Pclass 3: 24% survival. Missing: Age (177 train, 86 test), Cabin (687 train, 327 test), Embarked (2 train), Fare (1 test)\", \"source\": \"exploration/eda.ipynb\"}}",
        "tool.output": "{\"content\": \"‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\\n  Source: exploration/eda.ipynb\", \"is_error\": false}",
        "tool.duration_ms": 1.1255741119384766
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "1c9642bf22ffb52c",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684915237650937,
      "end_time": 1767684918575511024,
      "duration": 3.337860087,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\ntool: Edited cell 1. Re-run to update kernel state.\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Target distribution and key statistics\nprint('Target distribution:')\nprint(train['Survived'].value_counts())\nprint(f'\\nSurvival rate: {train[\"Survived\"].mean():.2%}')\n\nprint('\\nSurvival by Sex:')\nprint(train.groupby('Sex')['Survived'].mean())\n\nprint('\\nSurvival by Pclass:')\nprint(train.groupby('Pclass')['Survived'].mean())\n\nprint('\\nSample data:')\nprint(train.head())\n```\nOutput:\nTarget distribution:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nSurvival rate: 38.38%\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\nSample data:\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\ntool: ‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\n  Source: exploration/eda.ipynb",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Edited cell 1. Re-run to update kernel state.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Target distribution and key statistics\\nprint(\\'Target distribution:\\')\\nprint(train[\\'Survived\\'].value_counts())\\nprint(f\\'\\\\nSurvival rate: {train[\"Survived\"].mean():.2%}\\')\\n\\nprint(\\'\\\\nSurvival by Sex:\\')\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSurvival by Pclass:\\')\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSample data:\\')\\nprint(train.head())\\n```\\nOutput:\\nTarget distribution:\\nSurvived\\n0    549\\n1    342\\nName: count, dtype: int64\\n\\nSurvival rate: 38.38%\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\\nSample data:\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data. Let me record key findings and write the seed_prompt.txt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\\n  Source: exploration/eda.ipynb'}]",
        "llm.input_tokens": 27647,
        "llm.output_tokens": 83,
        "llm.total_tokens": 27730,
        "llm.latency_ms": 3337.877035140991,
        "llm.completion": "[Tool Use: Read]",
        "llm.cost_usd": 0.084186
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "2b237f665eab3b08",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ReadTool",
      "kind": "INTERNAL",
      "start_time": 1767684918575794779,
      "end_time": 1767684918576358786,
      "duration": 0.000564007,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ReadTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ReadTool object at 0x742e515b9c90>\", \"input\": {\"file_path\": \"../research/kernels/startupsci_titanic-data-science-solutions/titanic-data-science-solutions.ipynb\"}}",
        "tool.output": "{\"content\": \"{\\\"cells\\\":[{\\\"metadata\\\":{\\\"_uuid\\\":\\\"fed5696c67bf55a553d6d04313a77e8c617cad99\\\",\\\"_cell_guid\\\":\\\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\\\",\\\"_cell_guid\\\":\\\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\\\",\\\"_cell_guid\\\":\\\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\\\",\\\"_cell_guid\\\":\\\"6b5dc743-15b1-aac6-405e-081def6ecca1\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"13f38775c12ad6f914254a08f0d1ef948a2bd453\\\",\\\"_cell_guid\\\":\\\"e7319668-86fe-8adc-438d-0eef3fd0a982\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"train_df = pd.read_csv('../input/train.csv')\\\\ntest_df = pd.read_csv('../input/test.csv')\\\\ncombine = [train_df, test_df]\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"79282222056237a52bbbb1dbd831f057f1c23d69\\\",\\\"_cell_guid\\\":\\\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"ef106f38a00e162a80c523778af6dcc778ccc1c2\\\",\\\"_cell_guid\\\":\\\"ce473d29-8d19-76b8-24a4-48c217286e42\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"print(train_df.columns.values)\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1d7acf42af29a63bc038f14eded24e8b8146f541\\\",\\\"_cell_guid\\\":\\\"cd19a6f6-347f-be19-607b-dca950590b37\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\\\",\\\"_cell_guid\\\":\\\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# preview the data\\\\ntrain_df.head()\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"c34fa51a38336d97d5f6a184908cca37daebd584\\\",\\\"_cell_guid\\\":\\\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\\\",\\\"_cell_guid\\\":\\\"7a63bf04-a410-9c81-5310-bdef7963298f\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\\\",\\\"_cell_guid\\\":\\\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"54d86cd45703d459d452f89572771deaa8877999\\\",\\\"_cell_guid\\\":\\\"ca14ae53-f05e-eb73-201c-064d7c3ed610\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1535f18113f851e480cd53e0c612dc05835690f3\\\",\\\"_cell_guid\\\":\\\"810f723d-2313-8dfd-e3e2-26673b9caa90\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"723c835c29e8727bc9bad4b564731f2ca98025d0\\\",\\\"_cell_guid\\\":\\\"50378071-7043-ed8d-a782-70c947520dae\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\\\",\\\"_cell_guid\\\":\\\"1e286e19-b714-385a-fcfa-8cf5ec19956a\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\\\",\\\"_cell_guid\\\":\\\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"52ea4f44dd626448dd2199cb284b592670b1394b\\\",\\\"_cell_guid\\\":\\\"a4d56857-9432-55bb-14c0-52ebeb64d198\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"3a016c1f24da59c85648204302d61ea15920e740\\\",\\\"_cell_guid\\\":\\\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\\\",\\\"_cell_guid\\\":\\\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"1f94308b23b934123c03067e84027b507b989e52\\\",\\\"_cell_guid\\\":\\\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"24f4e46f202a858076be91752170cad52aa9aefa\\\",\\\"_cell_guid\\\":\\\"85693668-0cd5-4319-7768-eddb62d2b7d0\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"483c647d2759a2703d20785a44f51b6dee47d0db\\\",\\\"_cell_guid\\\":\\\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"2c1428d022430ea594af983a433757e11b47c50c\\\",\\\"_cell_guid\\\":\\\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"06a52babe50e0dd837b553c78fc73872168e1c7d\\\",\\\"_cell_guid\\\":\\\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"models = pd.DataFrame({\\\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\\\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\\\n              'Stochastic Gradient Decent', 'Linear SVC', \\\\n              'Decision Tree'],\\\\n    'Score': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by='Score', ascending=False)\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\\\",\\\"_cell_guid\\\":\\\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\\\",\\\"collapsed\\\":true,\\\"trusted\\\":false},\\\"cell_type\\\":\\\"code\\\",\\\"source\\\":\\\"submission = pd.DataFrame({\\\\n        \\\\\\\"PassengerId\\\\\\\": test_df[\\\\\\\"PassengerId\\\\\\\"],\\\\n        \\\\\\\"Survived\\\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv('../output/submission.csv', index=False)\\\",\\\"execution_count\\\":null,\\\"outputs\\\":[]},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\\\",\\\"_cell_guid\\\":\\\"fcfc8d9f-e955-cf70-5843-1fb764c54699\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\\\"},{\\\"metadata\\\":{\\\"_uuid\\\":\\\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\\\",\\\"_cell_guid\\\":\\\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\\\"},\\\"cell_type\\\":\\\"markdown\\\",\\\"source\\\":\\\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\\\"}],\\\"metadata\\\":{\\\"_is_fork\\\":false,\\\"language_info\\\":{\\\"name\\\":\\\"python\\\",\\\"version\\\":\\\"3.6.6\\\",\\\"mimetype\\\":\\\"text/x-python\\\",\\\"codemirror_mode\\\":{\\\"name\\\":\\\"ipython\\\",\\\"version\\\":3},\\\"pygments_lexer\\\":\\\"ipython3\\\",\\\"nbconvert_exporter\\\":\\\"python\\\",\\\"file_extension\\\":\\\".py\\\"},\\\"_change_revision\\\":0,\\\"kernelspec\\\":{\\\"display_name\\\":\\\"Python 3\\\",\\\"language\\\":\\\"python\\\",\\\"name\\\":\\\"python3\\\"}},\\\"nbformat\\\":4,\\\"nbformat_minor\\\":1}\", \"is_error\": false}",
        "tool.duration_ms": 0.4773139953613281
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "59050c18c7773076",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684919554643956,
      "end_time": 1767684954575923111,
      "duration": 35.021279155,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\ntool: Edited cell 1. Re-run to update kernel state.\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Target distribution and key statistics\nprint('Target distribution:')\nprint(train['Survived'].value_counts())\nprint(f'\\nSurvival rate: {train[\"Survived\"].mean():.2%}')\n\nprint('\\nSurvival by Sex:')\nprint(train.groupby('Sex')['Survived'].mean())\n\nprint('\\nSurvival by Pclass:')\nprint(train.groupby('Pclass')['Survived'].mean())\n\nprint('\\nSample data:')\nprint(train.head())\n```\nOutput:\nTarget distribution:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nSurvival rate: 38.38%\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\nSample data:\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\ntool: ‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\n  Source: exploration/eda.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Edited cell 1. Re-run to update kernel state.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Target distribution and key statistics\\nprint(\\'Target distribution:\\')\\nprint(train[\\'Survived\\'].value_counts())\\nprint(f\\'\\\\nSurvival rate: {train[\"Survived\"].mean():.2%}\\')\\n\\nprint(\\'\\\\nSurvival by Sex:\\')\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSurvival by Pclass:\\')\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSample data:\\')\\nprint(train.head())\\n```\\nOutput:\\nTarget distribution:\\nSurvived\\n0    549\\n1    342\\nName: count, dtype: int64\\n\\nSurvival rate: 38.38%\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\\nSample data:\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data. Let me record key findings and write the seed_prompt.txt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\\n  Source: exploration/eda.ipynb'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}]",
        "llm.input_tokens": 33860,
        "llm.output_tokens": 1599,
        "llm.total_tokens": 35459,
        "llm.latency_ms": 35021.2984085083,
        "llm.completion": "REASONING: I've gathered extensive information from the kernels. Now let me write a comprehensive seed_prompt.txt based on all the research. The target is 1.0 (100% accuracy), which is extremely high - this suggests we need to focus on perfect feature engineering and possibly ensemble methods.\n[Tool Use: Write]",
        "llm.reasoning": "REASONING: I've gathered extensive information from the kernels. Now let me write a comprehensive seed_prompt.txt based on all the research. The target is 1.0 (100% accuracy), which is extremely high - this suggests we need to focus on perfect feature engineering and possibly ensemble methods.",
        "llm.cost_usd": 0.125565
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "71a4433a9cb2c380",
      "parent_span_id": "5b39285dbbf762da",
      "name": "WriteTool",
      "kind": "INTERNAL",
      "start_time": 1767684954576345627,
      "end_time": 1767684954576844087,
      "duration": 0.00049846,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "WriteTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.WriteTool object at 0x742e515b9e90>\", \"input\": {\"file_path\": \"seed_prompt.txt\", \"content\": \"## Data Understanding\\n**Reference notebooks for data characteristics:**\\n- `exploration/eda.ipynb` - Contains EDA: feature distributions, missing values, target distribution\\n- Binary classification problem predicting survival (0/1)\\n- Evaluation metric: Accuracy (percentage of correct predictions)\\n- Target score to beat: 1.0 (100% accuracy)\\n\\n## Feature Engineering (Critical for High Scores)\\n\\n### Title Extraction from Name\\nExtract title from Name field - this is one of the most predictive features:\\n- Common titles: Mr, Mrs, Miss, Master, Dr, Rev, etc.\\n- Group rare titles into 'Rare' category\\n- 'Master' title indicates young male children (high survival)\\n- Title captures both gender and social status information\\n\\n### Family Features\\n- **FamilySize** = SibSp + Parch + 1 (total family members including self)\\n- **IsAlone** = 1 if FamilySize == 1, else 0\\n- Family size of 2-4 has higher survival rates than alone or large families\\n\\n### Cabin Features\\n- **Has_Cabin** = 1 if Cabin is not null, else 0 (indicates wealth/class)\\n- Cabin deck letter can be extracted (A, B, C, etc.) - deck position matters\\n\\n### Binning Continuous Variables\\n- **AgeBin**: Bin Age into categories (e.g., Child, Teen, Adult, Senior)\\n- **FareBin**: Bin Fare into quartiles or quintiles\\n- Binning helps tree models and reduces noise\\n\\n### Other Features\\n- **Name_length**: Length of name (correlates with social status)\\n- **Ticket prefix**: Extract alphabetic prefix from ticket number\\n\\n## Missing Value Handling\\n- **Age**: Fill with median (grouped by Pclass and Sex for better accuracy)\\n- **Embarked**: Fill with mode ('S' - Southampton)\\n- **Fare**: Fill with median (grouped by Pclass)\\n- **Cabin**: Create Has_Cabin feature, then can drop\\n\\n## Models for Binary Classification\\n\\n### Single Models (Baseline)\\n- **Random Forest**: n_estimators=100-500, max_depth=6-8, good baseline ~82-84%\\n- **Gradient Boosting**: Often outperforms RF slightly\\n- **XGBoost**: n_estimators=2000, max_depth=4, learning_rate=0.01-0.1\\n- **SVM**: With RBF kernel, requires feature scaling\\n- **Logistic Regression**: Good interpretability, ~80% accuracy\\n\\n### Ensemble Methods (For Higher Scores)\\n\\n#### Voting Classifier\\nCombine multiple models with soft voting:\\n- RandomForest + GradientBoosting + XGBoost + SVC + LogisticRegression\\n\\n#### Stacking (Recommended for Best Scores)\\nTwo-level stacking approach:\\n1. **First Level (Base Models)**:\\n   - Random Forest (n_estimators=500, max_depth=6)\\n   - Extra Trees (n_estimators=500, max_depth=8)\\n   - AdaBoost (n_estimators=500, learning_rate=0.75)\\n   - Gradient Boosting (n_estimators=500, max_depth=5)\\n   - SVC (with probability=True)\\n\\n2. **Second Level (Meta-Learner)**:\\n   - XGBoost (n_estimators=2000, max_depth=4, gamma=0.9, subsample=0.8)\\n   - Use out-of-fold predictions from first level as features\\n\\n3. **Out-of-Fold Prediction Process**:\\n   - Use K-Fold (k=5) cross-validation\\n   - For each fold, train on k-1 folds, predict on held-out fold\\n   - Concatenate predictions to form new feature set\\n   - This prevents data leakage\\n\\n### Key Insight: Model Diversity\\n- Uncorrelated base models produce better ensemble results\\n- Mix tree-based (RF, GBM) with linear (LogReg) and kernel (SVM) methods\\n\\n## Hyperparameter Tuning\\n- Use GridSearchCV or RandomizedSearchCV\\n- Key parameters for tree models: max_depth, n_estimators, min_samples_leaf\\n- For XGBoost: learning_rate, max_depth, subsample, colsample_bytree, gamma\\n\\n## Feature Selection\\n- Use Recursive Feature Elimination (RFE) with cross-validation\\n- Check feature importances from tree models\\n- Most important features typically: Sex, Title, Pclass, Fare, Age\\n\\n## Validation Strategy\\n- Use Stratified K-Fold (k=5 or k=10) to maintain class balance\\n- Small dataset (891 samples) - be careful of overfitting\\n- Cross-validation score is more reliable than single train/test split\\n\\n## Domain Knowledge\\n- \\\"Women and children first\\\" - Sex and Age are critical\\n- Higher class passengers had better access to lifeboats\\n- Embarked port correlates with class/wealth\\n- Family dynamics matter - small families survived better\\n\\n## Code Patterns\\n\\n### Feature Engineering Pipeline\\n1. Combine train and test for consistent encoding\\n2. Extract Title from Name\\n3. Create FamilySize, IsAlone, Has_Cabin\\n4. Fill missing values\\n5. Bin Age and Fare\\n6. Encode categorical variables (LabelEncoder or OneHotEncoder)\\n7. Drop unused columns (Name, Ticket, Cabin, PassengerId)\\n\\n### Stacking Implementation\\n```\\n# Pseudo-code structure:\\n# 1. Define base models with parameters\\n# 2. Generate OOF predictions for each base model\\n# 3. Stack OOF predictions as new features\\n# 4. Train meta-learner on stacked features\\n# 5. Generate final predictions\\n```\\n\\n## Tips for Achieving High Accuracy\\n1. **Feature engineering is key** - Title extraction alone can boost accuracy significantly\\n2. **Ensemble diverse models** - Stacking typically adds 1-3% over single models\\n3. **Tune hyperparameters carefully** - Use cross-validation\\n4. **Don't overfit** - Small dataset is prone to overfitting\\n5. **Try multiple random seeds** - Results can vary with initialization\\n\"}}",
        "tool.output": "{\"content\": \"Wrote to /home/code/exploration/seed_prompt.txt\", \"is_error\": false}",
        "tool.duration_ms": 0.5688667297363281
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "1dace3fb8d34f7a9",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684955604249084,
      "end_time": 1767684960566374085,
      "duration": 4.962125001,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\ntool: Edited cell 1. Re-run to update kernel state.\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Target distribution and key statistics\nprint('Target distribution:')\nprint(train['Survived'].value_counts())\nprint(f'\\nSurvival rate: {train[\"Survived\"].mean():.2%}')\n\nprint('\\nSurvival by Sex:')\nprint(train.groupby('Sex')['Survived'].mean())\n\nprint('\\nSurvival by Pclass:')\nprint(train.groupby('Pclass')['Survived'].mean())\n\nprint('\\nSample data:')\nprint(train.head())\n```\nOutput:\nTarget distribution:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nSurvival rate: 38.38%\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\nSample data:\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\ntool: ‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\n  Source: exploration/eda.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: Wrote to /home/code/exploration/seed_prompt.txt",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Edited cell 1. Re-run to update kernel state.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Target distribution and key statistics\\nprint(\\'Target distribution:\\')\\nprint(train[\\'Survived\\'].value_counts())\\nprint(f\\'\\\\nSurvival rate: {train[\"Survived\"].mean():.2%}\\')\\n\\nprint(\\'\\\\nSurvival by Sex:\\')\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSurvival by Pclass:\\')\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSample data:\\')\\nprint(train.head())\\n```\\nOutput:\\nTarget distribution:\\nSurvived\\n0    549\\n1    342\\nName: count, dtype: int64\\n\\nSurvival rate: 38.38%\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\\nSample data:\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data. Let me record key findings and write the seed_prompt.txt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\\n  Source: exploration/eda.ipynb'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': \"REASONING: I've gathered extensive information from the kernels. Now let me write a comprehensive seed_prompt.txt based on all the research. The target is 1.0 (100% accuracy), which is extremely high - this suggests we need to focus on perfect feature engineering and possibly ensemble methods.\"}, {'role': 'tool', 'content': 'Wrote to /home/code/exploration/seed_prompt.txt'}]",
        "llm.input_tokens": 35486,
        "llm.output_tokens": 105,
        "llm.total_tokens": 35591,
        "llm.latency_ms": 4962.143182754517,
        "llm.completion": "REASONING: Now let me check if there are any additional kernels or techniques I should explore. Let me also look at the Titanic tutorial kernel for any additional insights.\n[Tool Use: ListFiles]",
        "llm.reasoning": "REASONING: Now let me check if there are any additional kernels or techniques I should explore. Let me also look at the Titanic tutorial kernel for any additional insights.",
        "llm.cost_usd": 0.10803299999999999
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "7d10e5a4bee45b06",
      "parent_span_id": "5b39285dbbf762da",
      "name": "ListFilesTool",
      "kind": "INTERNAL",
      "start_time": 1767684960566765321,
      "end_time": 1767684960567065692,
      "duration": 0.000300371,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "ListFilesTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.ListFilesTool object at 0x742e515b9ed0>\", \"input\": {\"directory_path\": \"../research/kernels/alexisbcook_titanic-tutorial\"}}",
        "tool.output": "{\"content\": \"/home/code/exploration/../research/kernels/alexisbcook_titanic-tutorial:\\n[FILE] kernel-metadata.json (568.0B)\\n[FILE] titanic-tutorial.ipynb (15.4KB)\", \"is_error\": false}",
        "tool.duration_ms": 0.39315223693847656
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "7db0c065e0581b21",
      "parent_span_id": "5b39285dbbf762da",
      "name": "llm.anthropic/claude-opus-4.5",
      "kind": "CLIENT",
      "start_time": 1767684961631022506,
      "end_time": 1767684967873034711,
      "duration": 6.242012205,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "llm",
        "llm.model": "anthropic/claude-opus-4.5",
        "llm.provider": "openai",
        "cascade.agent_name": "ExplorerAgent",
        "llm.max_tokens": 16000,
        "llm.prompt": "user: Welcome to autonomous exploration!\n\n\nKaggle Submissions: 0/10 used, 9 remaining today.\n## TARGET SCORE\nBeat **1.000000** (higher is better).\n\n**Paths:**\n- Data: /home/data\n- Exploration workspace: /home/code/exploration\n\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\n\n\n---\n\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\n\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\n\n## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`\n\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\n\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\n\n---\n\n**CRITICAL:** All your files go in the exploration workspace.\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\n\n**Workflow:**\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\n2. Quick data check: problem type? key characteristics?\n3. WRITE seed_prompt.txt based on writeups + data findings\n4. More exploration if needed\n5. UPDATE seed_prompt.txt with additional techniques\n6. Call EndExploration() when comprehensive\n\n**File naming:**\n- Data files: Absolute paths like \"/home/data/train.csv\"\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\n  (automatically saved to exploration/ directory)\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\n\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.\ntool: # Titanic - Machine Learning from Disaster\n\n**Competition:** titanic\n**Category:** None\n**Deadline:** 2030-01-01T00:00:00Z\n**Reward:** None\n**Evaluation Metric:** Categorization Accuracy\n**Teams:** 14518\n\n---\n\n## Description\n\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\n**If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\n\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\n\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\n\n> #### Recommended Tutorial\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\n\n## How Kaggle‚Äôs Competitions Work\n1. **Join the Competition**\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\n1.  **Get to Work**\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\n1.  **Make a Submission**\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\n1.  **Check the Leaderboard**\nSee how your model ranks against other Kagglers on our leaderboard. \n1.  **Improve Your Score**\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\n\n> #### Kaggle Lingo Video\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\n\n## What Data Will I Use in This Competition?\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\n\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\n\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\n\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n\n## How to Submit your Prediction to Kaggle\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\n\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\n\n## Submission File Format:\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\n\nThe file should have exactly 2 columns:\n- `PassengerId` (sorted in any order)\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\n\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\n\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\n## A Last Word on Kaggle Notebooks\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\n\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\n\n## üèÉ‚Äç‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\n\n## Evaluation\n\n<h2>Goal</h2>\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\n<h2>Metric</h2>\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\n<h2>Submission File Format</h2>\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\n<ul>\n<li>PassengerId (sorted in any order)</li>\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\n</ul>\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\n\n## data-description\n\n<h3>Overview</h3>\n<p>The data has been split into two groups:</p>\n<ul>\n<li>training set (train.csv)</li>\n<li>test set (test.csv)</li>\n</ul>\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\n<h3>Data Dictionary</h3>\n<table style=\"width: 100%;\">\n<tbody>\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n<tr>\n<td>survival</td>\n<td>Survival</td>\n<td>0 = No, 1 = Yes</td>\n</tr>\n<tr>\n<td>pclass</td>\n<td>Ticket class</td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n</tr>\n<tr>\n<td>sex</td>\n<td>Sex</td>\n<td></td>\n</tr>\n<tr>\n<td>Age</td>\n<td>Age in years</td>\n<td></td>\n</tr>\n<tr>\n<td>sibsp</td>\n<td># of siblings / spouses aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>parch</td>\n<td># of parents / children aboard the Titanic</td>\n<td></td>\n</tr>\n<tr>\n<td>ticket</td>\n<td>Ticket number</td>\n<td></td>\n</tr>\n<tr>\n<td>fare</td>\n<td>Passenger fare</td>\n<td></td>\n</tr>\n<tr>\n<td>cabin</td>\n<td>Cabin number</td>\n<td></td>\n</tr>\n<tr>\n<td>embarked</td>\n<td>Port of Embarkation</td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n</tr>\n</tbody>\n</table>\n<h3>Variable Notes</h3>\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\n\n## Frequently Asked Questions\n\n<h2>What is a Getting Started competition?</h2>\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\n<h2>How do I create and manage a team?</h2>\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\n<p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\n<h2>What are Notebooks?</h2>\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\n<h2>Why did my team disappear from the leaderboard?</h2>\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\n<h2>How do I contact Support?</h2>\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>\ntool: Not a directory: /home/code/exploration/../research/writeups\ntool: /home/code/exploration/../research/kernels:\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\n[DIR]  alexisbcook_titanic-tutorial/\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\n[FILE] metadata.json (1.7KB)\n[DIR]  startupsci_titanic-data-science-solutions/\ntool: /home/data:\n[FILE] description.md (13.6KB)\n[FILE] gender_submission.csv (3.2KB)\n[FILE] test.csv (28.0KB)\n[FILE] train.csv (59.8KB)\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\ntool: Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\ntool: /home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\n[FILE] kernel-metadata.json (732.0B)\ntool: /home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\n[FILE] kernel-metadata.json (687.0B)\ntool: /home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\n[FILE] kernel-metadata.json (661.0B)\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)\ntool: === MARKDOWN 0 ===\n\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\n\n=== MARKDOWN 1 ===\n<a id=\"ch1\"></a>\n# How a Data Scientist Beat the Odds\nIt's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interest\n\n=== MARKDOWN 2 ===\n<a id=\"ch2\"></a>\n# A Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\n\n=== MARKDOWN 3 ===\n<a id=\"ch3\"></a>\n# Step 1: Define the Problem\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\n\n......\n\n**Project Summary:**\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\n\n=== MARKDOWN 4 ===\n<a id=\"ch4\"></a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \n\n=== CODE 5 ===\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__)\n\n=== MARKDOWN 6 ===\n## 3.11 Load Data Modelling Libraries\n\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\n\n=== CODE 7 ===\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pa\n\n=== MARKDOWN 8 ===\n## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\n\n=== CODE 9 ===\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\ndata_raw = pd.read_csv('../input/train.csv')\n\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('../input/test.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal \n\n=== MARKDOWN 10 ===\n<a id=\"ch5\"></a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\n\n=== CODE 11 ===\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 12 ===\n## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\n\n=== CODE 13 ===\n###COMPLETING: complete or delete missing values in train and test/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the cabin feature/column and others previo\n\n=== CODE 14 ===\n###CREATE: Feature Engineering for train and test/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = data\n\n=== MARKDOWN 15 ===\n## 3.23 Convert Formats\n\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\n\n** Developer Documentation: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncode\n\n=== CODE 16 ===\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(datase\n\n=== MARKDOWN 17 ===\n## 3.24 Da-Double Check Cleaned Data\n\nNow that we've cleaned our data, let's do a discount da-double check!\n\n=== CODE 18 ===\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\n\n=== MARKDOWN 19 ===\n## 3.25 Split Training and Testing Data\n\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \n\n=== CODE 20 ===\n#split train and test data with function defaults\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \n\n=== MARKDOWN 21 ===\n<a id=\"ch6\"></a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\n\n=== CODE 22 ===\n#Discrete Variable Correlation by Survival using\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],dat\n\n=== CODE 23 ===\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \n\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: h\n\n=== CODE 24 ===\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[\n\n=== CODE 25 ===\n#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclas\n\n=== CODE 26 ===\n#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone',\n\n=== CODE 27 ===\n#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"f\n\n=== CODE 28 ===\n#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n=== CODE 29 ===\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()\n\n\ntool: === CODE 30 ===\n#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()\n\n=== CODE 31 ===\n#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])\n\n=== CODE 32 ===\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)\n\n=== MARKDOWN 33 ===\n<a id=\"ch7\"></a>\n# Step 5: Model Data\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\n\n=== CODE 34 ===\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes\n\n=== CODE 35 ===\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\n\n=== MARKDOWN 36 ===\n<a id=\"ch8\"></a>\n## 5.1 Evaluate Model Performance\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\n\n=== CODE 37 ===\n#IMPORTANT: This is a handmade model for learning purposes only.\n#However, it is possible to create your own predictive model without a fancy algorithm :)\n\n#coin flip model with random 1/survived 0/died\n\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\nfor index, row in data1.iterrows(): \n    #random number generator: https://docs.python.org/2/library/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived/1\n\n\n=== CODE 38 ===\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n',pivot_male)\n\n=== CODE 39 ===\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        #Question 3A Female - Class and Question 4 Em\n\n=== CODE 40 ===\n#Plot Accuracy Summary\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        pri\n\n=== MARKDOWN 41 ===\n## 5.11 Model Performance with Cross-Validation (CV)\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\n\nRemember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at \"predicting\" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%,\n\n=== MARKDOWN 42 ===\n<a id=\"ch9\"></a>\n# 5.12 Tune Model with Hyper-Parameters\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\n\nHowever, in order to tune a model, we need to actually understand it. That's why I took the time in the previ\n\n=== CODE 43 ===\n#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set\n\n=== MARKDOWN 44 ===\n<a id=\"ch10\"></a>\n## 5.13 Tune Model with Feature Selection\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\n\n=== CODE 45 ===\n#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_spli\n\n=== CODE 46 ===\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph\n\n=== MARKDOWN 47 ===\n<a id=\"ch11\"></a>\n# Step 6: Validate and Implement\nThe next step is to prepare for submission using the validation data. \n\n=== CODE 48 ===\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n\n=== CODE 49 ===\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassif\n\n\ntool: === MARKDOWN 0 ===\n# Introduction\n\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\n\n=== CODE 1 ===\n# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom \n\n=== MARKDOWN 2 ===\n# Feature Exploration, Engineering and Cleaning \n\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\n\n=== CODE 3 ===\n# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)\n\n=== MARKDOWN 4 ===\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \n\n**Feature Engineering**\n\nHere, credit must be extended to Sina's very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\n\n=== CODE 5 ===\nfull_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = data\n\n=== CODE 6 ===\n# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n=== MARKDOWN 7 ===\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\n\n## Visualisations \n\n=== CODE 8 ===\ntrain.head(3)\n\n=== MARKDOWN 9 ===\n**Pearson Correlation Heatmap**\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\n\n=== CODE 10 ===\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n=== MARKDOWN 11 ===\n**Takeaway from the Plots**\n\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n\n**Pairplots**\n\n\n=== CODE 12 ===\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])\n\n=== MARKDOWN 13 ===\n# Ensembling & Stacking models\n\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!\n\n=== MARKDOWN 14 ===\n### Helpers via Python Classes\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\n\n=== CODE 15 ===\n# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y)\n\n=== MARKDOWN 16 ===\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\n\n=== MARKDOWN 17 ===\n### Out-of-Fold Predictions\n\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n=== CODE 18 ===\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n=== MARKDOWN 19 ===\n# Generating our Base First-Level Models \n\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine\n\n=== MARKDOWN 20 ===\n**Parameters**\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \n\n=== CODE 21 ===\n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_esti\n\n=== MARKDOWN 22 ===\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\n\n=== CODE 23 ===\n# Create 5 objects that\n...(truncated)...\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data\n\n=== MARKDOWN 26 ===\n**Output of the First level Predictions** \n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\n\n=== CODE 27 ===\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")\n\n=== MARKDOWN 28 ===\n**Feature importances generated from the different classifiers**\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\n\n=== CODE 29 ===\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n\n=== MARKDOWN 30 ===\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I'll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\n\n=== CODE 31 ===\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.074\n\n=== MARKDOWN 32 ===\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n\n=== CODE 33 ===\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n\n=== MARKDOWN 34 ===\n**Interactive feature importances via Plotly scatterplots**\n\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\n\n=== CODE 35 ===\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = \n\n=== MARKDOWN 36 ===\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\n\n=== CODE 37 ===\n# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)\n\n=== MARKDOWN 38 ===\n**Plotly Barplot of Average Feature Importances**\n\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\n\n=== CODE 39 ===\ny = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#       \n\n=== MARKDOWN 40 ===\n# Second-Level Predictions from the First-level Output\n\n=== MARKDOWN 41 ===\n**First-level output as new features**\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\n\n=== CODE 42 ===\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n=== MARKDOWN 43 ===\n**Correlation Heatmap of the Second Level Training set**\n\n=== CODE 44 ===\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')\n\n=== MARKDOWN 45 ===\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\n\n=== CODE 46 ===\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n\n=== MARKDOWN 47 ===\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\n\n=== MARKDOWN 48 ===\n### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\n\n=== CODE 49 ===\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\n\n=== MARKDOWN 50 ===\nJust a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\n\n=== MARKDOWN 51 ===\n**Producing the Submission file**\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\n\n=== CODE 52 ===\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\n=== MARKDOWN 53 ===\n**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning.\n\n=== MARKDOWN 54 ===\n### Conclusion\n\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \n\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \n\nTill next time, Peace Out\n\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\n\n=== CODE 55 ===\n\n\n\ntool: ‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy\ntool: ‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: ‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\ntool: Created: /home/code/exploration/eda.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.\ntool: Executed cells 0-0\n==================================================\n\nCell 0:\n```\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('/home/data/train.csv')\ntest = pd.read_csv('/home/data/test.csv')\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('\\nTrain columns:', train.columns.tolist())\nprint('\\nTrain dtypes:')\nprint(train.dtypes)\nprint('\\nMissing values in train:')\nprint(train.isnull().sum())\nprint('\\nMissing values in test:')\nprint(test.isnull().sum())\n```\nOutput:\nTrain shape: (891, 12)\nTest shape: (418, 11)\n\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nTrain dtypes:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\nMissing values in train:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing values in test:\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\ntool: Added code cell at 1\ntool: Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)\ntool: Edited cell 1. Re-run to update kernel state.\ntool: Executed cells 1-1\n==================================================\n\nCell 1:\n```\n# Target distribution and key statistics\nprint('Target distribution:')\nprint(train['Survived'].value_counts())\nprint(f'\\nSurvival rate: {train[\"Survived\"].mean():.2%}')\n\nprint('\\nSurvival by Sex:')\nprint(train.groupby('Sex')['Survived'].mean())\n\nprint('\\nSurvival by Pclass:')\nprint(train.groupby('Pclass')['Survived'].mean())\n\nprint('\\nSample data:')\nprint(train.head())\n```\nOutput:\nTarget distribution:\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nSurvival rate: 38.38%\n\nSurvival by Sex:\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\nSurvival by Pclass:\nPclass\n1    0.629630\n2    0.472826\n3    0.242363\nName: Survived, dtype: float64\n\nSample data:\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\ntool: ‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\n  Source: exploration/eda.ipynb\ntool: {\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\n...(truncated)...\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}\ntool: Wrote to /home/code/exploration/seed_prompt.txt\ntool: /home/code/exploration/../research/kernels/alexisbcook_titanic-tutorial:\n[FILE] kernel-metadata.json (568.0B)\n[FILE] titanic-tutorial.ipynb (15.4KB)",
        "llm.messages": "[{'role': 'user', 'content': 'Welcome to autonomous exploration!\\n\\n\\nKaggle Submissions: 0/10 used, 9 remaining today.\\n## TARGET SCORE\\nBeat **1.000000** (higher is better).\\n\\n**Paths:**\\n- Data: /home/data\\n- Exploration workspace: /home/code/exploration\\n\\n**Your mission:** Research techniques to beat the target score and create seed_prompt.txt\\n\\n\\n---\\n\\n## PRE-FETCHED COMPETITIVE INTELLIGENCE\\n\\nThe following resources have been downloaded to your workspace. **READ THESE FIRST** before doing web searches - they contain winning solutions and top notebooks from this competition.\\n\\n## Available Competitive Intelligence\\n\\n- Competition description: `../research/description.md`\\n- 5 top kernel notebooks in `../research/kernels/`\\n\\n**IMPORTANT:** Start by reading the writeups in `../research/writeups/` - they contain detailed explanations of winning approaches. This will inform your exploration and reduce the need for web searches.\\n\\n**Record insights from writeups/kernels:** After reading a writeup or kernel, use `RecordFinding` to save key techniques. Set `source` to the file path (e.g., `../research/writeups/2nd-place-solution.md`). This ensures insights persist for future agents.\\n\\n---\\n\\n**CRITICAL:** All your files go in the exploration workspace.\\nWrite \"seed_prompt.txt\" (simple filename) - it will be saved to exploration/.\\n\\n**Workflow:**\\n1. READ the pre-fetched writeups in ../research/writeups/ (winning solutions!)\\n2. Quick data check: problem type? key characteristics?\\n3. WRITE seed_prompt.txt based on writeups + data findings\\n4. More exploration if needed\\n5. UPDATE seed_prompt.txt with additional techniques\\n6. Call EndExploration() when comprehensive\\n\\n**File naming:**\\n- Data files: Absolute paths like \"/home/data/train.csv\"\\n- Your files: Simple names like \"seed_prompt.txt\", \"eda.ipynb\"\\n  (automatically saved to exploration/ directory)\\n- Research files: Use \"../research/\" paths (e.g., \"../research/description.md\")\\n\\nWrite seed_prompt.txt EARLY (iteration 3-4), update as you learn.'}, {'role': 'assistant', 'content': \"REASONING: Let me start by reading the competition description and understanding the problem type. Then I'll check the pre-fetched research materials including writeups and kernels to understand winning approaches.\"}, {'role': 'tool', 'content': '# Titanic - Machine Learning from Disaster\\n\\n**Competition:** titanic\\n**Category:** None\\n**Deadline:** 2030-01-01T00:00:00Z\\n**Reward:** None\\n**Evaluation Metric:** Categorization Accuracy\\n**Teams:** 14518\\n\\n---\\n\\n## Description\\n\\n## üëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place. \\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\n\\n**If you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here:** https://discord.gg/kaggle\\n\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\n\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\" target=\"_blank\">\"Join Competition button</a> to create an account and gain access to the <a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\">competition data</a>. Then check out <a href=\"https://www.kaggle.com/alexisbcook/titanic-tutorial\">Alexis Cook‚Äôs Titanic Tutorial</a> that walks you through step by step how to make your first submission!\\n\\n<iframe width=\"699\" height=\"368\" src=\"https://www.youtube.com/embed/8yZMXCaFshs\" title=\"YouTube video | How to Get Started with Kaggle‚Äôs Titanic Competition | Kaggle\" frameborder=\"0\" allow=\"encrypted-media\" allowfullscreen></iframe>\\n\\n## The Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\n\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\n\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\n\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\n\\n> #### Recommended Tutorial\\n> We highly recommend [Alexis Cook‚Äôs Titanic Tutorial](https://www.kaggle.com/alexisbcook/titanic-tutorial) that walks you through making your very first submission step by step and [this starter notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests) to get started.\\n\\n## How Kaggle‚Äôs Competitions Work\\n1. **Join the Competition**\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\n1.  **Get to Work**\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\n1.  **Make a Submission**\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\n1.  **Check the Leaderboard**\\nSee how your model ranks against other Kagglers on our leaderboard. \\n1.  **Improve Your Score**\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a> to find lots of tutorials and insights from other competitors.\\n\\n> #### Kaggle Lingo Video\\n> You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out  Dr. Rachael Tatman‚Äôs [video on Kaggle Lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s) to get up to speed!\\n\\n## What Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\\n\\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\n\\nThe `test.csv` dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\n\\nUsing the patterns you find in the `train.csv` data, predict whether the other 418 passengers on board (found in `test.csv`) survived.\\n\\nCheck out the <a href=\"https://www.kaggle.com/c/titanic/data\">‚ÄúData‚Äù tab</a> to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\n\\n## How to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\n\\n1. Click on the ‚ÄúSubmit Predictions‚Äù button\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen1.png\">\\n1. Upload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\n<img width=\"699\" align=\"center\" src=\"https://storage.googleapis.com/kaggle-media/welcome/screen2.png\">\\n\\n## Submission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond `PassengerId` and `Survived`) or rows.\\n\\nThe file should have exactly 2 columns:\\n- `PassengerId` (sorted in any order)\\n- `Survived` (contains your binary predictions: 1 for survived, 0 for deceased)\\n\\n## Got it! I‚Äôm ready to get started. Where do I get help if I need it?\\n- For Competition Help: <a href=\"https://www.kaggle.com/c/titanic/discussion\">Titanic Discussion Forum</a>\\n\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\n\\n## A Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.\\n\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks <a href=\"https://www.kaggle.com/c/titanic/notebooks\">here</a>.\\n\\n## üèÉ\\u200d‚ôÄReady to Compete? <a href=\"https://www.kaggle.com/account/login?returnUrl=%2Fc%2Ftitanic\">Join the Competition Here!</a>\\n\\n## Evaluation\\n\\n<h2>Goal</h2>\\n<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. <br>For each  in the test set, you must predict a 0 or 1 value for the  variable.</p>\\n<h2>Metric</h2>\\n<p>Your score is the percentage of passengers you correctly predict. This is known as <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification\">accuracy</a>.</p>\\n<h2>Submission File Format</h2>\\n<p>You should submit a csv file with exactly 418 entries <b>plus</b> a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.<br><br>The file should have exactly 2 columns:</p>\\n<ul>\\n<li>PassengerId (sorted in any order)</li>\\n<li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li>\\n</ul>\\n<pre><b>PassengerId,Survived</b><br>892,0<br>893,1<br>894,0<br>Etc.</pre>\\n<p>You can download an example submission file (gender_submission.csv) on the <a href=\"https://www.kaggle.com/c/titanic/data\">Data page</a>.</p>\\n\\n## data-description\\n\\n<h3>Overview</h3>\\n<p>The data has been split into two groups:</p>\\n<ul>\\n<li>training set (train.csv)</li>\\n<li>test set (test.csv)</li>\\n</ul>\\n<p><b> The training set </b>should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use <a href=\"https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\" target=\"_blank\"> feature engineering </a>to create new features.</p>\\n<p><b>The test set </b>should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p>\\n<p>We also include <b>gender_submission.csv</b>, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>\\n<h3>Data Dictionary</h3>\\n<table style=\"width: 100%;\">\\n<tbody>\\n<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\\n<tr>\\n<td>survival</td>\\n<td>Survival</td>\\n<td>0 = No, 1 = Yes</td>\\n</tr>\\n<tr>\\n<td>pclass</td>\\n<td>Ticket class</td>\\n<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\\n</tr>\\n<tr>\\n<td>sex</td>\\n<td>Sex</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>Age</td>\\n<td>Age in years</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>sibsp</td>\\n<td># of siblings / spouses aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>parch</td>\\n<td># of parents / children aboard the Titanic</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>ticket</td>\\n<td>Ticket number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>fare</td>\\n<td>Passenger fare</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>cabin</td>\\n<td>Cabin number</td>\\n<td></td>\\n</tr>\\n<tr>\\n<td>embarked</td>\\n<td>Port of Embarkation</td>\\n<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3>Variable Notes</h3>\\n<p><b>pclass</b>: A proxy for socio-economic status (SES)<br /> 1st = Upper<br /> 2nd = Middle<br /> 3rd = Lower<br /><br /> <b>age</b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br /><br /> <b>sibsp</b>: The dataset defines family relations in this way...<br /> Sibling = brother, sister, stepbrother, stepsister<br /> Spouse = husband, wife (mistresses and fianc√©s were ignored)<br /><br /> <b>parch</b>: The dataset defines family relations in this way...<br /> Parent = mother, father<br /> Child = daughter, son, stepdaughter, stepson<br /> Some children travelled only with a nanny, therefore parch=0 for them.</p>\\n\\n## Frequently Asked Questions\\n\\n<h2>What is a Getting Started competition?</h2>\\n<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p>\\n<p>Getting Started competitions are a non-competitive way to get familiar with Kaggle‚Äôs platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>\\n<h2>How do I create and manage a team?</h2>\\n<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; <a href=\"https://www.kaggle.com/c/titanic/team\" target=\"_blank\">Team</a> page.</p>\\n<p>We\\'ve heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don\\'t have a teammate already, consider asking if anyone wants to team up in the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">discussion forum</a>.</p>\\n<h2>What are Notebooks?</h2>\\n<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the <a href=\"https://www.kaggle.com/c/titanic/notebooks\" target=\"_blank\">Notebooks</a> tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our <a href=\"https://www.kaggle.com/learn/overview\" target=\"&quot;_blank\">Courses</a>!</p>\\n<h2>Why did my team disappear from the leaderboard?</h2>\\n<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p>\\n<p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p>\\n<p><i>\"I worked so hard to get that score! Give it back!\"</i> Read more about our decision to implement a rolling leaderboard <a href=\"https://www.kaggle.com/c/titanic/discussion/6240\" target=\"_blank\">here</a>.</p>\\n<h2>How do I contact Support?</h2>\\n<p>Kaggle does not have a dedicated support team so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you‚Äôll want to use the <a href=\"https://www.kaggle.com/c/titanic/discussion\" target=\"_blank\">Titanic discussion forum)</a>.</p>\\n<p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can‚Äôt find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!</p>\\n<p>If your problem persists or it seems to be effective all participants then please <a href=\"https://www.kaggle.com/contact\" target=\"_blank\">contact us</a>.</p>'}, {'role': 'tool', 'content': 'Not a directory: /home/code/exploration/../research/writeups'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels:\\n[DIR]  alexisbcook_exercise-arithmetic-and-variables/\\n[DIR]  alexisbcook_titanic-tutorial/\\n[DIR]  arthurtok_introduction-to-ensembling-stacking-in-python/\\n[DIR]  ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/\\n[FILE] metadata.json (1.7KB)\\n[DIR]  startupsci_titanic-data-science-solutions/'}, {'role': 'tool', 'content': '/home/data:\\n[FILE] description.md (13.6KB)\\n[FILE] gender_submission.csv (3.2KB)\\n[FILE] test.csv (28.0KB)\\n[FILE] train.csv (59.8KB)'}, {'role': 'assistant', 'content': \"REASONING: Good, I can see this is the classic Titanic survival prediction problem - binary classification with accuracy as the metric. The target is 1.0 (100% accuracy), which is very high. Let me read the top kernels to understand winning approaches. I'll start with the high-accuracy framework and ensembling notebooks.\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy.py'\"}, {'role': 'tool', 'content': \"Error: [Errno 2] No such file or directory: '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python/arthurtok_introduction-to-ensembling-stacking-in-python.py'\"}, {'role': 'assistant', 'content': 'REASONING: Let me check the actual file names in these kernel directories.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy:\\n[FILE] a-data-science-framework-to-achieve-99-accuracy.ipynb (107.1KB)\\n[FILE] kernel-metadata.json (732.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python:\\n[FILE] introduction-to-ensembling-stacking-in-python.ipynb (39.3KB)\\n[FILE] kernel-metadata.json (687.0B)'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/startupsci_titanic-data-science-solutions:\\n[FILE] kernel-metadata.json (661.0B)\\n[FILE] titanic-data-science-solutions.ipynb (55.5KB)'}, {'role': 'assistant', 'content': \"REASONING: Let me read these notebooks. They are ipynb files so I'll use Bash to extract the content.\"}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n\\n>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don\\'t understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you‚Äôll be able to solve any problem\\n\\n=== MARKDOWN 1 ===\\n<a id=\"ch1\"></a>\\n# How a Data Scientist Beat the Odds\\nIt\\'s the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare\\'s mortality rate or survival analysis. Binary events create an interest\\n\\n=== MARKDOWN 2 ===\\n<a id=\"ch2\"></a>\\n# A Data Science Framework\\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don\\'t put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the\\n\\n=== MARKDOWN 3 ===\\n<a id=\"ch3\"></a>\\n# Step 1: Define the Problem\\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\\n\\n......\\n\\n**Project Summary:**\\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the inter\\n\\n=== MARKDOWN 4 ===\\n<a id=\"ch4\"></a>\\n# Step 3: Prepare Data for Consumption\\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\\n\\n## 3.1 Import Libraries\\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. \\n\\n=== CODE 5 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n\\n#load packages\\nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\\nprint(\"Python version: {}\". format(sys.version))\\n\\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\\nprint(\"pandas version: {}\". format(pd.__version__)\\n\\n=== MARKDOWN 6 ===\\n## 3.11 Load Data Modelling Libraries\\n\\nWe will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.\\n\\n=== CODE 7 ===\\n#Common Model Algorithms\\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\\nfrom xgboost import XGBClassifier\\n\\n#Common Model Helpers\\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\\nfrom sklearn import feature_selection\\nfrom sklearn import model_selection\\nfrom sklearn import metrics\\n\\n#Visualization\\nimport matplotlib as mpl\\nimport matplotlib.pyplot as plt\\nimport matplotlib.pylab as pylab\\nimport seaborn as sns\\nfrom pa\\n\\n=== MARKDOWN 8 ===\\n## 3.2 Meet and Greet Data\\n\\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what\\'s its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\\n\\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview\\n\\n=== CODE 9 ===\\n#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\\ndata_raw = pd.read_csv(\\'../input/train.csv\\')\\n\\n\\n#a dataset should be broken into 3 splits: train, test, and (final) validation\\n#the test file provided is the validation file for competition submission\\n#we will split the train set into train and test data in future sections\\ndata_val  = pd.read_csv(\\'../input/test.csv\\')\\n\\n\\n#to play with our data we\\'ll create a copy\\n#remember python assignment or equal \\n\\n=== MARKDOWN 10 ===\\n<a id=\"ch5\"></a>\\n## 3.21 The 4 C\\'s of Data Cleaning: Correcting, Completing, Creating, and Converting\\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\\n\\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential out\\n\\n=== CODE 11 ===\\nprint(\\'Train columns with null values:\\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values:\\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 12 ===\\n## 3.22 Clean Data\\n\\nNow that we know what to clean, let\\'s execute our code.\\n\\n** Developer Documentation: **\\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-do\\n\\n=== CODE 13 ===\\n###COMPLETING: complete or delete missing values in train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #complete missing age with median\\n    dataset[\\'Age\\'].fillna(dataset[\\'Age\\'].median(), inplace = True)\\n\\n    #complete embarked with mode\\n    dataset[\\'Embarked\\'].fillna(dataset[\\'Embarked\\'].mode()[0], inplace = True)\\n\\n    #complete missing fare with median\\n    dataset[\\'Fare\\'].fillna(dataset[\\'Fare\\'].median(), inplace = True)\\n    \\n#delete the cabin feature/column and others previo\\n\\n=== CODE 14 ===\\n###CREATE: Feature Engineering for train and test/validation dataset\\nfor dataset in data_cleaner:    \\n    #Discrete variables\\n    dataset[\\'FamilySize\\'] = dataset [\\'SibSp\\'] + dataset[\\'Parch\\'] + 1\\n\\n    dataset[\\'IsAlone\\'] = 1 #initialize to yes/1 is alone\\n    dataset[\\'IsAlone\\'].loc[dataset[\\'FamilySize\\'] > 1] = 0 # now update to no/0 if family size is greater than 1\\n\\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\\n    dataset[\\'Title\\'] = data\\n\\n=== MARKDOWN 15 ===\\n## 3.23 Convert Formats\\n\\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\\n\\nIn this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.\\n\\n** Developer Documentation: **\\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\\n* [Sklearn LabelEncode\\n\\n=== CODE 16 ===\\n#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\\n\\n#code categorical data\\nlabel = LabelEncoder()\\nfor dataset in data_cleaner:    \\n    dataset[\\'Sex_Code\\'] = label.fit_transform(dataset[\\'Sex\\'])\\n    dataset[\\'Embarked_Code\\'] = label.fit_transform(dataset[\\'Embarked\\'])\\n    dataset[\\'Title_Code\\'] = label.fit_transform(dataset[\\'Title\\'])\\n    dataset[\\'AgeBin_Code\\'] = label.fit_transform(dataset[\\'AgeBin\\'])\\n    dataset[\\'FareBin_Code\\'] = label.fit_transform(datase\\n\\n=== MARKDOWN 17 ===\\n## 3.24 Da-Double Check Cleaned Data\\n\\nNow that we\\'ve cleaned our data, let\\'s do a discount da-double check!\\n\\n=== CODE 18 ===\\nprint(\\'Train columns with null values: \\\\n\\', data1.isnull().sum())\\nprint(\"-\"*10)\\nprint (data1.info())\\nprint(\"-\"*10)\\n\\nprint(\\'Test/Validation columns with null values: \\\\n\\', data_val.isnull().sum())\\nprint(\"-\"*10)\\nprint (data_val.info())\\nprint(\"-\"*10)\\n\\ndata_raw.describe(include = \\'all\\')\\n\\n=== MARKDOWN 19 ===\\n## 3.25 Split Training and Testing Data\\n\\nAs mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don\\'t [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the \\n\\n=== CODE 20 ===\\n#split train and test data with function defaults\\n#random_state -> seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy \\n\\n=== MARKDOWN 21 ===\\n<a id=\"ch6\"></a>\\n# Step 4: Perform Exploratory Analysis with Statistics\\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.\\n\\n=== CODE 22 ===\\n#Discrete Variable Correlation by Survival using\\n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\nfor x in data1_x:\\n    if data1[x].dtype != \\'float64\\' :\\n        print(\\'Survival Correlation by:\\', x)\\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\\n        print(\\'-\\'*10, \\'\\\\n\\')\\n        \\n\\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\\nprint(pd.crosstab(data1[\\'Title\\'],dat\\n\\n=== CODE 23 ===\\n#IMPORTANT: Intentionally plotted different ways for learning purposes only. \\n\\n#optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\\n\\n#we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\\n\\n#to organize our graphics will use figure: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\\n#and subplotS: h\\n\\n=== CODE 24 ===\\n#we will use seaborn graphics for multi-variable comparison: https://seaborn.pydata.org/api.html\\n\\n#graph individual features by survival\\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\\n\\nsns.barplot(x = \\'Embarked\\', y = \\'Survived\\', data=data1, ax = saxis[0,0])\\nsns.barplot(x = \\'Pclass\\', y = \\'Survived\\', order=[1,2,3], data=data1, ax = saxis[0,1])\\nsns.barplot(x = \\'IsAlone\\', y = \\'Survived\\', order=[1,0], data=data1, ax = saxis[0,2])\\n\\nsns.pointplot(x = \\'FareBin\\', y = \\'Survived\\',  data=data1, ax = saxis[\\n\\n=== CODE 25 ===\\n#graph distribution of qualitative data: Pclass\\n#we know class mattered in survival, now let\\'s compare class and a 2nd feature\\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.boxplot(x = \\'Pclass\\', y = \\'Fare\\', hue = \\'Survived\\', data = data1, ax = axis1)\\naxis1.set_title(\\'Pclass vs Fare Survival Comparison\\')\\n\\nsns.violinplot(x = \\'Pclass\\', y = \\'Age\\', hue = \\'Survived\\', data = data1, split = True, ax = axis2)\\naxis2.set_title(\\'Pclass vs Age Survival Comparison\\')\\n\\nsns.boxplot(x = \\'Pclas\\n\\n=== CODE 26 ===\\n#graph distribution of qualitative data: Sex\\n#we know sex mattered in survival, now let\\'s compare sex and a 2nd feature\\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Embarked\\', data=data1, ax = qaxis[0])\\naxis1.set_title(\\'Sex vs Embarked Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'Pclass\\', data=data1, ax  = qaxis[1])\\naxis1.set_title(\\'Sex vs Pclass Survival Comparison\\')\\n\\nsns.barplot(x = \\'Sex\\', y = \\'Survived\\', hue = \\'IsAlone\\',\\n\\n=== CODE 27 ===\\n#more side-by-side comparisons\\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\\n\\n#how does family size factor with sex & survival compare\\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\\n\\n#how does class factor with sex & survival compare\\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\\n              palette={\"male\": \"blue\", \"f\\n\\n=== CODE 28 ===\\n#how does embark port factor with class, sex, and survival compare\\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\\ne = sns.FacetGrid(data1, col = \\'Embarked\\')\\ne.map(sns.pointplot, \\'Pclass\\', \\'Survived\\', \\'Sex\\', ci=95.0, palette = \\'deep\\')\\ne.add_legend()\\n\\n=== CODE 29 ===\\n#plot distributions of age of passengers who survived or did not survive\\na = sns.FacetGrid( data1, hue = \\'Survived\\', aspect=4 )\\na.map(sns.kdeplot, \\'Age\\', shade= True )\\na.set(xlim=(0 , data1[\\'Age\\'].max()))\\na.add_legend()\\n\\n'}, {'role': 'assistant', 'content': 'REASONING: This kernel shows excellent feature engineering techniques. Let me continue reading more of this notebook and also check the ensembling notebook.'}, {'role': 'tool', 'content': '=== CODE 30 ===\\n#histogram comparison of sex, class, and age by survival\\nh = sns.FacetGrid(data1, row = \\'Sex\\', col = \\'Pclass\\', hue = \\'Survived\\')\\nh.map(plt.hist, \\'Age\\', alpha = .75)\\nh.add_legend()\\n\\n=== CODE 31 ===\\n#pair plots of entire dataset\\npp = sns.pairplot(data1, hue = \\'Survived\\', palette = \\'deep\\', size=1.2, diag_kind = \\'kde\\', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\\npp.set(xticklabels=[])\\n\\n=== CODE 32 ===\\n#correlation heatmap of dataset\\ndef correlation_heatmap(df):\\n    _ , ax = plt.subplots(figsize =(14, 12))\\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\\n    \\n    _ = sns.heatmap(\\n        df.corr(), \\n        cmap = colormap,\\n        square=True, \\n        cbar_kws={\\'shrink\\':.9 }, \\n        ax=ax,\\n        annot=True, \\n        linewidths=0.1,vmax=1.0, linecolor=\\'white\\',\\n        annot_kws={\\'fontsize\\':12 }\\n    )\\n    \\n    plt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\n\\ncorrelation_heatmap(data1)\\n\\n=== MARKDOWN 33 ===\\n<a id=\"ch7\"></a>\\n# Step 5: Model Data\\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don‚Äôt worry, we only need a high-level\\n\\n=== CODE 34 ===\\n#Machine Learning Algorithm (MLA) Selection and Initialization\\nMLA = [\\n    #Ensemble Methods\\n    ensemble.AdaBoostClassifier(),\\n    ensemble.BaggingClassifier(),\\n    ensemble.ExtraTreesClassifier(),\\n    ensemble.GradientBoostingClassifier(),\\n    ensemble.RandomForestClassifier(),\\n\\n    #Gaussian Processes\\n    gaussian_process.GaussianProcessClassifier(),\\n    \\n    #GLM\\n    linear_model.LogisticRegressionCV(),\\n    linear_model.PassiveAggressiveClassifier(),\\n    linear_model.RidgeClassifierCV(),\\n    linear_model.SGDClassifier(),\\n    linear_model.Perceptron(),\\n    \\n    #Navies Bayes\\n    naive_bayes\\n\\n=== CODE 35 ===\\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\\nsns.barplot(x=\\'MLA Test Accuracy Mean\\', y = \\'MLA Name\\', data = MLA_compare, color = \\'m\\')\\n\\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\\nplt.title(\\'Machine Learning Algorithm Accuracy Score \\\\n\\')\\nplt.xlabel(\\'Accuracy Score (%)\\')\\nplt.ylabel(\\'Algorithm\\')\\n\\n=== MARKDOWN 36 ===\\n<a id=\"ch8\"></a>\\n## 5.1 Evaluate Model Performance\\nLet\\'s recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we\\'re only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So,\\n\\n=== CODE 37 ===\\n#IMPORTANT: This is a handmade model for learning purposes only.\\n#However, it is possible to create your own predictive model without a fancy algorithm :)\\n\\n#coin flip model with random 1/survived 0/died\\n\\n#iterate over dataFrame rows as (index, Series) pairs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html\\nfor index, row in data1.iterrows(): \\n    #random number generator: https://docs.python.org/2/library/random.html\\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \\n        data1.set_value(index, \\'Random_Predict\\', 1) #predict survived/1\\n\\n\\n=== CODE 38 ===\\n#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\npivot_female = data1[data1.Sex==\\'female\\'].groupby([\\'Sex\\',\\'Pclass\\', \\'Embarked\\',\\'FareBin\\'])[\\'Survived\\'].mean()\\nprint(\\'Survival Decision Tree w/Female Node: \\\\n\\',pivot_female)\\n\\npivot_male = data1[data1.Sex==\\'male\\'].groupby([\\'Sex\\',\\'Title\\'])[\\'Survived\\'].mean()\\nprint(\\'\\\\n\\\\nSurvival Decision Tree w/Male Node: \\\\n\\',pivot_male)\\n\\n=== CODE 39 ===\\n#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\\ndef mytree(df):\\n    \\n    #initialize table to store predictions\\n    Model = pd.DataFrame(data = {\\'Predict\\':[]})\\n    male_title = [\\'Master\\'] #survived titles\\n\\n    for index, row in df.iterrows():\\n\\n        #Question 1: Were you on the Titanic; majority died\\n        Model.loc[index, \\'Predict\\'] = 0\\n\\n        #Question 2: Are you female; majority survived\\n        if (df.loc[index, \\'Sex\\'] == \\'female\\'):\\n                  Model.loc[index, \\'Predict\\'] = 1\\n\\n        #Question 3A Female - Class and Question 4 Em\\n\\n=== CODE 40 ===\\n#Plot Accuracy Summary\\n#Credit: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\\nimport itertools\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\\'Confusion matrix\\',\\n                          cmap=plt.cm.Blues):\\n    \"\"\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \"\"\"\\n    if normalize:\\n        cm = cm.astype(\\'float\\') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\"Normalized confusion matrix\")\\n    else:\\n        pri\\n\\n=== MARKDOWN 41 ===\\n## 5.11 Model Performance with Cross-Validation (CV)\\nIn step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.\\n\\nRemember, it\\'s important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it\\'s great at \"predicting\" data it\\'s already seen, but terrible at predicting data it has not seen; which is not prediction at all. It\\'s like cheating on a school quiz to get 100%,\\n\\n=== MARKDOWN 42 ===\\n<a id=\"ch9\"></a>\\n# 5.12 Tune Model with Hyper-Parameters\\nWhen we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)\\n\\nHowever, in order to tune a model, we need to actually understand it. That\\'s why I took the time in the previ\\n\\n=== CODE 43 ===\\n#base model\\ndtree = tree.DecisionTreeClassifier(random_state = 0)\\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\\ndtree.fit(data1[data1_x_bin], data1[Target])\\n\\nprint(\\'BEFORE DT Parameters: \\', dtree.get_params())\\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\n#print(\"BEFORE DT Test w/bin set\\n\\n=== MARKDOWN 44 ===\\n<a id=\"ch10\"></a>\\n## 5.13 Tune Model with Feature Selection\\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).\\n\\n=== CODE 45 ===\\n#base model\\nprint(\\'BEFORE DT RFE Training Shape Old: \\', data1[data1_x_bin].shape) \\nprint(\\'BEFORE DT RFE Training Columns Old: \\', data1[data1_x_bin].columns.values)\\n\\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results[\\'train_score\\'].mean()*100)) \\nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results[\\'test_score\\'].mean()*100))\\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results[\\'test_score\\'].std()*100*3))\\nprint(\\'-\\'*10)\\n\\n\\n\\n#feature selection\\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = \\'accuracy\\', cv = cv_spli\\n\\n=== CODE 46 ===\\n#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\\nimport graphviz \\ndot_data = tree.export_graphviz(dtree, out_file=None, \\n                                feature_names = data1_x_bin, class_names = True,\\n                                filled = True, rounded = True)\\ngraph = graphviz.Source(dot_data) \\ngraph\\n\\n=== MARKDOWN 47 ===\\n<a id=\"ch11\"></a>\\n# Step 6: Validate and Implement\\nThe next step is to prepare for submission using the validation data. \\n\\n=== CODE 48 ===\\n#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\\n#there are some 1\\'s, but enough blues and light reds to create a \"super algorithm\" by combining them\\ncorrelation_heatmap(MLA_predict)\\n\\n=== CODE 49 ===\\n#why choose one model, when you can pick them all with voting classifier\\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\\n#removed models w/o attribute \\'predict_proba\\' required for vote classifier and models with a 1.0 correlation to another model\\nvote_est = [\\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\\n    (\\'ada\\', ensemble.AdaBoostClassifier()),\\n    (\\'bc\\', ensemble.BaggingClassifier()),\\n    (\\'etc\\',ensemble.ExtraTreesClassifier()),\\n    (\\'gbc\\', ensemble.GradientBoostingClassifier()),\\n    (\\'rfc\\', ensemble.RandomForestClassif\\n\\n'}, {'role': 'tool', 'content': '=== MARKDOWN 0 ===\\n# Introduction\\n\\nThis notebook is a very basic and simple introductory primer to the method of ensembling (combining) base learning models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions.\\n\\nThe Titanic dataset is a prime candidate for introducing this concept as many newcomers to Kaggle start out here. Furthermore even though stacking has been responsible for many a team winning K\\n\\n=== CODE 1 ===\\n# Load in our libraries\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport sklearn\\nimport xgboost as xgb\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimport plotly.offline as py\\npy.init_notebook_mode(connected=True)\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\n\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Going to use these 5 base models for the stacking\\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \\n                              GradientBoostingClassifier, ExtraTreesClassifier)\\nfrom sklearn.svm import SVC\\nfrom \\n\\n=== MARKDOWN 2 ===\\n# Feature Exploration, Engineering and Cleaning \\n\\nNow we will proceed much like how most kernels in general are structured, and that is to first explore the data on hand, identify possible feature engineering opportunities as well as numerically encode any categorical features.\\n\\n=== CODE 3 ===\\n# Load in the train and test datasets\\ntrain = pd.read_csv(\\'../input/train.csv\\')\\ntest = pd.read_csv(\\'../input/test.csv\\')\\n\\n# Store our passenger ID for easy access\\nPassengerId = test[\\'PassengerId\\']\\n\\ntrain.head(3)\\n\\n=== MARKDOWN 4 ===\\nWell it is no surprise that our task is to somehow extract the information out of the categorical variables \\n\\n**Feature Engineering**\\n\\nHere, credit must be extended to Sina\\'s very comprehensive and well-thought out notebook for the feature engineering ideas so please check out his work \\n\\n[Titanic Best Working Classfier][1] : by Sina\\n\\n\\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier\\n\\n=== CODE 5 ===\\nfull_data = [train, test]\\n\\n# Some features of my own that I have added in\\n# Gives the length of the name\\ntrain[\\'Name_length\\'] = train[\\'Name\\'].apply(len)\\ntest[\\'Name_length\\'] = test[\\'Name\\'].apply(len)\\n# Feature that tells whether a passenger had a cabin on the Titanic\\ntrain[\\'Has_Cabin\\'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\ntest[\\'Has_Cabin\\'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\\n\\n# Feature engineering steps taken from Sina\\n# Create new feature FamilySize as a combination of SibSp and Parch\\nfor dataset in full_data:\\n    dataset[\\'FamilySize\\'] = data\\n\\n=== CODE 6 ===\\n# Feature selection\\ndrop_elements = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\', \\'SibSp\\']\\ntrain = train.drop(drop_elements, axis = 1)\\ntrain = train.drop([\\'CategoricalAge\\', \\'CategoricalFare\\'], axis = 1)\\ntest  = test.drop(drop_elements, axis = 1)\\n\\n=== MARKDOWN 7 ===\\nAll right so now having cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. However before we proceed let us generate some simple correlation and distribution plots of our transformed dataset to observe ho\\n\\n## Visualisations \\n\\n=== CODE 8 ===\\ntrain.head(3)\\n\\n=== MARKDOWN 9 ===\\n**Pearson Correlation Heatmap**\\n\\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows\\n\\n=== CODE 10 ===\\ncolormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title(\\'Pearson Correlation of Features\\', y=1.05, size=15)\\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor=\\'white\\', annot=True)\\n\\n=== MARKDOWN 11 ===\\n**Takeaway from the Plots**\\n\\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn\\'t much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I\\'ll still leave both features in for the purposes of this exercise.\\n\\n**Pairplots**\\n\\n\\n=== CODE 12 ===\\ng = sns.pairplot(train[[u\\'Survived\\', u\\'Pclass\\', u\\'Sex\\', u\\'Age\\', u\\'Parch\\', u\\'Fare\\', u\\'Embarked\\',\\n       u\\'FamilySize\\', u\\'Title\\']], hue=\\'Survived\\', palette = \\'seismic\\',size=1.2,diag_kind = \\'kde\\',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\\ng.set(xticklabels=[])\\n\\n=== MARKDOWN 13 ===\\n# Ensembling & Stacking models\\n\\nFinally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\\n\\nCreating a Stacking ensemble!\\n\\n=== MARKDOWN 14 ===\\n### Helpers via Python Classes\\n\\nHere we invoke the use of Python\\'s classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\\n\\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn c\\n\\n=== CODE 15 ===\\n# Some useful parameters which will come in handy later on\\nntrain = train.shape[0]\\nntest = test.shape[0]\\nSEED = 0 # for reproducibility\\nNFOLDS = 5 # set folds for out-of-fold prediction\\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\\n\\n# Class to extend the Sklearn classifier\\nclass SklearnHelper(object):\\n    def __init__(self, clf, seed=0, params=None):\\n        params[\\'random_state\\'] = seed\\n        self.clf = clf(**params)\\n\\n    def train(self, x_train, y_train):\\n        self.clf.fit(x_train, y_train)\\n\\n    def predict(self, x):\\n        return self.clf.predict(x)\\n    \\n    def fit(self,x,y)\\n\\n=== MARKDOWN 16 ===\\nBear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\\n\\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\\n\\n\\n=== MARKDOWN 17 ===\\n### Out-of-Fold Predictions\\n\\nNow as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\\n\\n=== CODE 18 ===\\ndef get_oof(clf, x_train, y_train, x_test):\\n    oof_train = np.zeros((ntrain,))\\n    oof_test = np.zeros((ntest,))\\n    oof_test_skf = np.empty((NFOLDS, ntest))\\n\\n    for i, (train_index, test_index) in enumerate(kf):\\n        x_tr = x_train[train_index]\\n        y_tr = y_train[train_index]\\n        x_te = x_train[test_index]\\n\\n        clf.train(x_tr, y_tr)\\n\\n        oof_train[test_index] = clf.predict(x_te)\\n        oof_test_skf[i, :] = clf.predict(x_test)\\n\\n    oof_test[:] = oof_test_skf.mean(axis=0)\\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\\n\\n=== MARKDOWN 19 ===\\n# Generating our Base First-Level Models \\n\\nSo now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\\n\\n 1. Random Forest classifier\\n 2. Extra Trees classifier\\n 3. AdaBoost classifer\\n 4. Gradient Boosting classifer\\n 5. Support Vector Machine\\n\\n=== MARKDOWN 20 ===\\n**Parameters**\\n\\nJust a quick summary of the parameters that we will be listing here for completeness,\\n\\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\\n\\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\\n\\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\\n\\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses \\n\\n=== CODE 21 ===\\n# Put in our parameters for said classifiers\\n# Random Forest parameters\\nrf_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\': 500,\\n     \\'warm_start\\': True, \\n     #\\'max_features\\': 0.2,\\n    \\'max_depth\\': 6,\\n    \\'min_samples_leaf\\': 2,\\n    \\'max_features\\' : \\'sqrt\\',\\n    \\'verbose\\': 0\\n}\\n\\n# Extra Trees Parameters\\net_params = {\\n    \\'n_jobs\\': -1,\\n    \\'n_estimators\\':500,\\n    #\\'max_features\\': 0.5,\\n    \\'max_depth\\': 8,\\n    \\'min_samples_leaf\\': 2,\\n    \\'verbose\\': 0\\n}\\n\\n# AdaBoost parameters\\nada_params = {\\n    \\'n_estimators\\': 500,\\n    \\'learning_rate\\' : 0.75\\n}\\n\\n# Gradient Boosting parameters\\ngb_params = {\\n    \\'n_esti\\n\\n=== MARKDOWN 22 ===\\nFurthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.\\n\\n=== CODE 23 ===\\n# Create 5 objects that\\n...(truncated)...\\n\\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\\ny_train = train[\\'Survived\\'].ravel()\\ntrain = train.drop([\\'Survived\\'], axis=1)\\nx_train = train.values # Creates an array of the train data\\nx_test = test.values # Creats an array of the test data\\n\\n=== MARKDOWN 26 ===\\n**Output of the First level Predictions** \\n\\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.\\n\\n=== CODE 27 ===\\n# Create our OOF train and test predictions. These base results will be used as new features\\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \\ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\\n\\nprint(\"Training is complete\")\\n\\n=== MARKDOWN 28 ===\\n**Feature importances generated from the different classifiers**\\n\\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\\n\\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such\\n\\n=== CODE 29 ===\\nrf_feature = rf.feature_importances(x_train,y_train)\\net_feature = et.feature_importances(x_train, y_train)\\nada_feature = ada.feature_importances(x_train, y_train)\\ngb_feature = gb.feature_importances(x_train,y_train)\\n\\n=== MARKDOWN 30 ===\\nSo I have not yet figured out how to assign and store the feature importances outright. Therefore I\\'ll print out the values from the code above and then simply copy and paste into Python lists as below (sorry for the lousy hack)\\n\\n=== CODE 31 ===\\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\\n  ,0.05965792 , 0.02774745,  0.074\\n\\n=== MARKDOWN 32 ===\\nCreate a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\\n\\n=== CODE 33 ===\\ncols = train.columns.values\\n# Create a dataframe with features\\nfeature_dataframe = pd.DataFrame( {\\'features\\': cols,\\n     \\'Random Forest feature importances\\': rf_features,\\n     \\'Extra Trees  feature importances\\': et_features,\\n      \\'AdaBoost feature importances\\': ada_features,\\n    \\'Gradient Boost feature importances\\': gb_features\\n    })\\n\\n=== MARKDOWN 34 ===\\n**Interactive feature importances via Plotly scatterplots**\\n\\nI\\'ll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:\\n\\n=== CODE 35 ===\\n# Scatter plot \\ntrace = go.Scatter(\\n    y = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n    x = feature_dataframe[\\'features\\'].values,\\n    mode=\\'markers\\',\\n    marker=dict(\\n        sizemode = \\'diameter\\',\\n        sizeref = 1,\\n        size = 25,\\n#       size= feature_dataframe[\\'AdaBoost feature importances\\'].values,\\n        #color = np.random.randn(500), #set color equal to a variable\\n        color = feature_dataframe[\\'Random Forest feature importances\\'].values,\\n        colorscale=\\'Portland\\',\\n        showscale=True\\n    ),\\n    text = feature_dataframe[\\'features\\'].values\\n)\\ndata = \\n\\n=== MARKDOWN 36 ===\\nNow let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.\\n\\n=== CODE 37 ===\\n# Create the new column containing the average of values\\n\\nfeature_dataframe[\\'mean\\'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\\nfeature_dataframe.head(3)\\n\\n=== MARKDOWN 38 ===\\n**Plotly Barplot of Average Feature Importances**\\n\\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:\\n\\n=== CODE 39 ===\\ny = feature_dataframe[\\'mean\\'].values\\nx = feature_dataframe[\\'features\\'].values\\ndata = [go.Bar(\\n            x= x,\\n             y= y,\\n            width = 0.5,\\n            marker=dict(\\n               color = feature_dataframe[\\'mean\\'].values,\\n            colorscale=\\'Portland\\',\\n            showscale=True,\\n            reversescale = False\\n            ),\\n            opacity=0.6\\n        )]\\n\\nlayout= go.Layout(\\n    autosize= True,\\n    title= \\'Barplots of Mean Feature Importance\\',\\n    hovermode= \\'closest\\',\\n#     xaxis= dict(\\n#         title= \\'Pop\\',\\n#         ticklen= 5,\\n#         zeroline= False,\\n#       \\n\\n=== MARKDOWN 40 ===\\n# Second-Level Predictions from the First-level Output\\n\\n=== MARKDOWN 41 ===\\n**First-level output as new features**\\n\\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\\n\\n=== CODE 42 ===\\nbase_predictions_train = pd.DataFrame( {\\'RandomForest\\': rf_oof_train.ravel(),\\n     \\'ExtraTrees\\': et_oof_train.ravel(),\\n     \\'AdaBoost\\': ada_oof_train.ravel(),\\n      \\'GradientBoost\\': gb_oof_train.ravel()\\n    })\\nbase_predictions_train.head()\\n\\n=== MARKDOWN 43 ===\\n**Correlation Heatmap of the Second Level Training set**\\n\\n=== CODE 44 ===\\ndata = [\\n    go.Heatmap(\\n        z= base_predictions_train.astype(float).corr().values ,\\n        x=base_predictions_train.columns.values,\\n        y= base_predictions_train.columns.values,\\n          colorscale=\\'Viridis\\',\\n            showscale=True,\\n            reversescale = True\\n    )\\n]\\npy.iplot(data, filename=\\'labelled-heatmap\\')\\n\\n=== MARKDOWN 45 ===\\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\\n\\n=== CODE 46 ===\\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\\n\\n=== MARKDOWN 47 ===\\nHaving now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.\\n\\n=== MARKDOWN 48 ===\\n### Second level learning model via XGBoost\\n\\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\\n\\n  [1]: https://xgboost.readthedocs.io/en/latest/\\n\\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:\\n\\n=== CODE 49 ===\\ngbm = xgb.XGBClassifier(\\n    #learning_rate = 0.02,\\n n_estimators= 2000,\\n max_depth= 4,\\n min_child_weight= 2,\\n #gamma=1,\\n gamma=0.9,                        \\n subsample=0.8,\\n colsample_bytree=0.8,\\n objective= \\'binary:logistic\\',\\n nthread= -1,\\n scale_pos_weight=1).fit(x_train, y_train)\\npredictions = gbm.predict(x_test)\\n\\n=== MARKDOWN 50 ===\\nJust a quick run down of the XGBoost parameters used in the model:\\n\\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\\n\\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\\n\\n**eta** : step size shrinkage used in each boosting step to prevent overfitting\\n\\n=== MARKDOWN 51 ===\\n**Producing the Submission file**\\n\\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:\\n\\n=== CODE 52 ===\\n# Generate Submission File \\nStackingSubmission = pd.DataFrame({ \\'PassengerId\\': PassengerId,\\n                            \\'Survived\\': predictions })\\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\\n\\n=== MARKDOWN 53 ===\\n**Steps for Further Improvement**\\n\\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \\n\\nSome additional steps that may be taken to improve one\\'s score could be:\\n\\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\\n 2. Introduce a greater variety of base models for learning.\\n\\n=== MARKDOWN 54 ===\\n### Conclusion\\n\\nI have this notebook has been helpful somewhat in introducing a working script for stacking learning models. Again credit must be extended to Faron and Sina. \\n\\nFor other excellent material on stacking or ensembling in general, refer to the de-facto Must read article on the website MLWave: [Kaggle Ensembling Guide][1]. \\n\\nTill next time, Peace Out\\n\\n  [1]: http://mlwave.com/kaggle-ensembling-guide/\\n\\n=== CODE 55 ===\\n\\n\\n'}, {'role': 'assistant', 'content': \"REASONING: Excellent! I've gathered key insights from these top kernels. Let me record the important findings and also quickly explore the data to understand its characteristics. Then I'll write the seed_prompt.txt.\"}, {'role': 'tool', 'content': '‚úì Recorded finding: Feature Engineering: FamilySize = SibSp + Parch + 1; IsAlone = 1 if FamilySize == 1; Title extracted...\\n  Source: ../research/kernels/ldfreeman3_a-data-science-framework-to-achieve-99-accuracy'}, {'role': 'tool', 'content': '‚úì Recorded finding: Stacking Ensemble: Use 5 base models (RandomForest, ExtraTrees, AdaBoost, GradientBoosting, SVC) wit...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python'}, {'role': 'tool', 'content': \"‚úì Recorded finding: Important features by importance: Title (especially 'Master' for male survival), Sex, Pclass, Age, F...\\n  Source: ../research/kernels/arthurtok_introduction-to-ensembling-stacking-in-python\"}, {'role': 'assistant', 'content': 'REASONING: Let me quickly explore the data to understand its structure and then write the seed prompt.'}, {'role': 'tool', 'content': 'Created: /home/code/exploration/eda.ipynb\\n\\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': \"Executed cells 0-0\\n==================================================\\n\\nCell 0:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\n# Load data\\ntrain = pd.read_csv('/home/data/train.csv')\\ntest = pd.read_csv('/home/data/test.csv')\\n\\nprint('Train shape:', train.shape)\\nprint('Test shape:', test.shape)\\nprint('\\\\nTrain columns:', train.columns.tolist())\\nprint('\\\\nTrain dtypes:')\\nprint(train.dtypes)\\nprint('\\\\nMissing values in train:')\\nprint(train.isnull().sum())\\nprint('\\\\nMissing values in test:')\\nprint(test.isnull().sum())\\n```\\nOutput:\\nTrain shape: (891, 12)\\nTest shape: (418, 11)\\n\\nTrain columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\\n\\nTrain dtypes:\\nPassengerId      int64\\nSurvived         int64\\nPclass           int64\\nName            object\\nSex             object\\nAge            float64\\nSibSp            int64\\nParch            int64\\nTicket          object\\nFare           float64\\nCabin           object\\nEmbarked        object\\ndtype: object\\n\\nMissing values in train:\\nPassengerId      0\\nSurvived         0\\nPclass           0\\nName             0\\nSex              0\\nAge            177\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             0\\nCabin          687\\nEmbarked         2\\ndtype: int64\\n\\nMissing values in test:\\nPassengerId      0\\nPclass           0\\nName             0\\nSex              0\\nAge             86\\nSibSp            0\\nParch            0\\nTicket           0\\nFare             1\\nCabin          327\\nEmbarked         0\\ndtype: int64\\n\"}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Added code cell at 1'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Error: SyntaxError: unexpected character after line continuation character (612826994.py, line 3)'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Edited cell 1. Re-run to update kernel state.'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': 'Executed cells 1-1\\n==================================================\\n\\nCell 1:\\n```\\n# Target distribution and key statistics\\nprint(\\'Target distribution:\\')\\nprint(train[\\'Survived\\'].value_counts())\\nprint(f\\'\\\\nSurvival rate: {train[\"Survived\"].mean():.2%}\\')\\n\\nprint(\\'\\\\nSurvival by Sex:\\')\\nprint(train.groupby(\\'Sex\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSurvival by Pclass:\\')\\nprint(train.groupby(\\'Pclass\\')[\\'Survived\\'].mean())\\n\\nprint(\\'\\\\nSample data:\\')\\nprint(train.head())\\n```\\nOutput:\\nTarget distribution:\\nSurvived\\n0    549\\n1    342\\nName: count, dtype: int64\\n\\nSurvival rate: 38.38%\\n\\nSurvival by Sex:\\nSex\\nfemale    0.742038\\nmale      0.188908\\nName: Survived, dtype: float64\\n\\nSurvival by Pclass:\\nPclass\\n1    0.629630\\n2    0.472826\\n3    0.242363\\nName: Survived, dtype: float64\\n\\nSample data:\\n   PassengerId  Survived  Pclass  \\\\\\n0            1         0       3   \\n1            2         1       1   \\n2            3         1       3   \\n3            4         1       1   \\n4            5         0       3   \\n\\n                                                Name     Sex   Age  SibSp  \\\\\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \\n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \\n2                             Heikkinen, Miss. Laina  female  26.0      0   \\n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \\n4                           Allen, Mr. William Henry    male  35.0      0   \\n\\n   Parch            Ticket     Fare Cabin Embarked  \\n0      0         A/5 21171   7.2500   NaN        S  \\n1      0          PC 17599  71.2833   C85        C  \\n2      0  STON/O2. 3101282   7.9250   NaN        S  \\n3      0            113803  53.1000  C123        S  \\n4      0            373450   8.0500   NaN        S  \\n'}, {'role': 'assistant', 'content': 'REASONING: Now I have a good understanding of the data. Let me record key findings and write the seed_prompt.txt based on all the research.'}, {'role': 'tool', 'content': '‚úì Recorded finding: Data: 891 train, 418 test samples. Binary classification (Survived 0/1). 38.38% survival rate. Femal...\\n  Source: exploration/eda.ipynb'}, {'role': 'assistant', 'content': 'None'}, {'role': 'tool', 'content': '{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\\\n\\\\n\\\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\\\n\\\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\\\n\\\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\\\n\\\\n## Workflow stages\\\\n\\\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\\\n\\\\n1. Question or problem definition.\\\\n2. Acquire training and testing data.\\\\n3. Wrangle, prepare, cleanse the data.\\\\n4. Analyze, identify patterns, and explore the data.\\\\n5. Model, predict and solve the problem.\\\\n6. Visualize, report, and present the problem solving steps and final solution.\\\\n7. Supply or submit the results.\\\\n\\\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\\\n\\\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\\\n\\\\n\\\\n## Question and problem definition\\\\n\\\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\\\n\\\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\\\n\\\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\\\n\\\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\\\n\\\\n## Workflow goals\\\\n\\\\nThe data science solutions workflow solves for seven major goals.\\\\n\\\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\\\n\\\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\\\n\\\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\\\n\\\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\\\n\\\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\\\n\\\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\\\n\\\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\\\n\\\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\\\n\\\\n### User comments\\\\n\\\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\\\n\\\\n### Porting issues\\\\n\\\\n- Specify plot dimensions, bring legend into plot.\\\\n\\\\n\\\\n### Best practices\\\\n\\\\n- Performing feature correlation analysis early in the project.\\\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\\\nimport pandas as pd\\\\nimport numpy as np\\\\nimport random as rnd\\\\n\\\\n# visualization\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\n%matplotlib inline\\\\n\\\\n# machine learning\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.svm import SVC, LinearSVC\\\\nfrom sklearn.ensemble import RandomForestClassifier\\\\nfrom sklearn.neighbors import KNeighborsClassifier\\\\nfrom sklearn.naive_bayes import GaussianNB\\\\nfrom sklearn.linear_model import Perceptron\\\\nfrom sklearn.linear_model import SGDClassifier\\\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\\\n\\\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv(\\'../input/train.csv\\')\\\\ntest_df = pd.read_csv(\\'../input/test.csv\\')\\\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\\\n\\\\nPandas also helps describe the datasets answering following questions early in our project.\\\\n\\\\n**Which features are available in the dataset?**\\\\n\\\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\\\n\\\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\\\n\\\\n**Which features are numerical?**\\\\n\\\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\\\n\\\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\\\n\\\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\\\n\\\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanum\\n...(truncated)...\\n4331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\\\n\\\\nsvc = SVC()\\\\nsvc.fit(X_train, Y_train)\\\\nY_pred = svc.predict(X_test)\\\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\\\n\\\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\\\nknn.fit(X_train, Y_train)\\\\nY_pred = knn.predict(X_test)\\\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\\\n\\\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\\\n\\\\ngaussian = GaussianNB()\\\\ngaussian.fit(X_train, Y_train)\\\\nY_pred = gaussian.predict(X_test)\\\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\\\n\\\\nperceptron = Perceptron()\\\\nperceptron.fit(X_train, Y_train)\\\\nY_pred = perceptron.predict(X_test)\\\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\\\n\\\\nlinear_svc = LinearSVC()\\\\nlinear_svc.fit(X_train, Y_train)\\\\nY_pred = linear_svc.predict(X_test)\\\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\\\n\\\\nsgd = SGDClassifier()\\\\nsgd.fit(X_train, Y_train)\\\\nY_pred = sgd.predict(X_test)\\\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\\\n\\\\ndecision_tree = DecisionTreeClassifier()\\\\ndecision_tree.fit(X_train, Y_train)\\\\nY_pred = decision_tree.predict(X_test)\\\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\\\n\\\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model\\'s output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\\\n\\\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\\\nrandom_forest.fit(X_train, Y_train)\\\\nY_pred = random_forest.predict(X_test)\\\\nrandom_forest.score(X_train, Y_train)\\\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\\\n\\\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\\' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\\\n    \\'Model\\': [\\'Support Vector Machines\\', \\'KNN\\', \\'Logistic Regression\\', \\\\n              \\'Random Forest\\', \\'Naive Bayes\\', \\'Perceptron\\', \\\\n              \\'Stochastic Gradient Decent\\', \\'Linear SVC\\', \\\\n              \\'Decision Tree\\'],\\\\n    \\'Score\\': [acc_svc, acc_knn, acc_log, \\\\n              acc_random_forest, acc_gaussian, acc_perceptron, \\\\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\\\nmodels.sort_values(by=\\'Score\\', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\\\n        \\\\\"PassengerId\\\\\": test_df[\\\\\"PassengerId\\\\\"],\\\\n        \\\\\"Survived\\\\\": Y_pred\\\\n    })\\\\n# submission.to_csv(\\'../output/submission.csv\\', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\\\n\\\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\\\n\\\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\\\n- [Getting Started with Pandas: Kaggle\\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}'}, {'role': 'assistant', 'content': \"REASONING: I've gathered extensive information from the kernels. Now let me write a comprehensive seed_prompt.txt based on all the research. The target is 1.0 (100% accuracy), which is extremely high - this suggests we need to focus on perfect feature engineering and possibly ensemble methods.\"}, {'role': 'tool', 'content': 'Wrote to /home/code/exploration/seed_prompt.txt'}, {'role': 'assistant', 'content': 'REASONING: Now let me check if there are any additional kernels or techniques I should explore. Let me also look at the Titanic tutorial kernel for any additional insights.'}, {'role': 'tool', 'content': '/home/code/exploration/../research/kernels/alexisbcook_titanic-tutorial:\\n[FILE] kernel-metadata.json (568.0B)\\n[FILE] titanic-tutorial.ipynb (15.4KB)'}]",
        "llm.input_tokens": 35655,
        "llm.output_tokens": 204,
        "llm.total_tokens": 35859,
        "llm.latency_ms": 6242.046356201172,
        "llm.completion": "[Tool Use: Bash]",
        "llm.cost_usd": 0.110025
      }
    },
    {
      "trace_id": "f621a7e7fe0c82660cc5ff446f8e330b",
      "span_id": "1beff1cb8cd160f0",
      "parent_span_id": "5b39285dbbf762da",
      "name": "BashTool",
      "kind": "INTERNAL",
      "start_time": 1767684967873285671,
      "end_time": 1767684967901898374,
      "duration": 0.028612703,
      "status": {
        "status_code": "OK",
        "description": null
      },
      "attributes": {
        "cascade.span_type": "tool",
        "tool.name": "BashTool",
        "cascade.agent_name": "ExplorerAgent",
        "tool.input": "{\"self\": \"<agent_v7_live.agents.core.tools.file_tools.BashTool object at 0x742e515b9f10>\", \"input\": {\"command\": \"cd /home/code/exploration/../research/kernels/alexisbcook_titanic-tutorial && python3 -c \\\"\\nimport json\\nwith open('titanic-tutorial.ipynb') as f:\\n    nb = json.load(f)\\nfor i, cell in enumerate(nb['cells']):\\n    if cell['cell_type'] == 'markdown':\\n        print(f'=== MARKDOWN {i} ===')\\n        print(''.join(cell['source'])[:400])\\n    else:\\n        print(f'=== CODE {i} ===')\\n        print(''.join(cell['source'])[:400])\\n    print()\\n\\\"\"}}",
        "tool.output": "{\"content\": \"=== MARKDOWN 0 ===\\nLogging into Kaggle for the first time can be daunting. Our competitions often have large cash prizes, public leaderboards, and involve complex data. Nevertheless, we really think all data scientists can rapidly learn from machine learning competitions and meaningfully contribute to our community. To give you a clear understanding of how our platform works and a mental model of the type of learnin\\n\\n=== MARKDOWN 1 ===\\n# Part 2: Your coding environment\\n\\nIn this section, you'll train your own machine learning model to improve your predictions.  _If you've never written code before or don't have any experience with machine learning, don't worry!  We don't assume any prior experience in this tutorial._\\n\\n## The Notebook\\n\\nThe first thing to do is to create a Kaggle Notebook where you'll store all of your code.  You c\\n\\n=== CODE 2 ===\\n# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here's several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the \\\"../input/\\\" directo\\n\\n=== MARKDOWN 3 ===\\nThis shows us where the competition data is stored, so that we can load the files into the notebook.  We'll do that next.\\n\\n## Load the data\\n\\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\\n\\n![](https://i.imgur.com/OQBax9n.png)\\n\\nType the two lines of code below into your second code cell.  Then, once you're done, either click on the blue pl\\n\\n=== CODE 4 ===\\ntrain_data = pd.read_csv(\\\"/kaggle/input/titanic/train.csv\\\")\\ntrain_data.head()\\n\\n=== MARKDOWN 5 ===\\nYour code should return the output above, which corresponds to the first five rows of the table in **train.csv**.  It's very important that you see this output **in your notebook** before proceeding with the tutorial!\\n> _If your code does not produce this output_, double-check that your code is identical to the two lines above.  And, make sure your cursor is in the code cell before hitting **[Shif\\n\\n=== CODE 6 ===\\ntest_data = pd.read_csv(\\\"/kaggle/input/titanic/test.csv\\\")\\ntest_data.head()\\n\\n=== MARKDOWN 7 ===\\nAs before, make sure that you see the output above in your notebook before continuing.  \\n\\nOnce all of the code runs successfully, all of the data (in **train.csv** and **test.csv**) is loaded in the notebook.  (_The code above shows only the first 5 rows of each table, but all of the data is there -- all 891 rows of **train.csv** and all 418 rows of **test.csv**!_)\\n\\n# Part 3: Your first submission\\n\\n=== CODE 8 ===\\nwomen = train_data.loc[train_data.Sex == 'female'][\\\"Survived\\\"]\\nrate_women = sum(women)/len(women)\\n\\nprint(\\\"% of women who survived:\\\", rate_women)\\n\\n=== MARKDOWN 9 ===\\nBefore moving on, make sure that your code returns the output above.  The code above calculates the percentage of female passengers (in **train.csv**) who survived.\\n\\nThen, run the code below in another code cell:\\n\\n=== CODE 10 ===\\nmen = train_data.loc[train_data.Sex == 'male'][\\\"Survived\\\"]\\nrate_men = sum(men)/len(men)\\n\\nprint(\\\"% of men who survived:\\\", rate_men)\\n\\n=== MARKDOWN 11 ===\\nThe code above calculates the percentage of male passengers (in **train.csv**) who survived.\\n\\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in **gender_submission.csv** is not a bad first guess!\\n\\nBut at the end of the day, this gender-based s\\n\\n=== CODE 12 ===\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ny = train_data[\\\"Survived\\\"]\\n\\nfeatures = [\\\"Pclass\\\", \\\"Sex\\\", \\\"SibSp\\\", \\\"Parch\\\"]\\nX = pd.get_dummies(train_data[features])\\nX_test = pd.get_dummies(test_data[features])\\n\\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\\nmodel.fit(X, y)\\npredictions = model.predict(X_test)\\n\\noutput = pd.DataFrame({'PassengerId': test_data.Passen\\n\\n=== MARKDOWN 13 ===\\nMake sure that your notebook outputs the same message above (`Your submission was successfully saved!`) before moving on.\\n> Again, don't worry if this code doesn't make sense to you!  For now, we'll focus on how to generate and submit predictions.\\n\\nOnce you're ready, click on the **\\\"Save Version\\\"** button in the top right corner of your notebook.  This will generate a pop-up window.  \\n- Ensure tha\\n\\n=== MARKDOWN 14 ===\\n# Part 4: Learn more!\\n\\nIf you're interested in learning more, we strongly suggest our (3-hour) **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course, which will help you fully understand all of the code that we've presented here.  You'll also know enough to generate even better predictions!\\n\\n\", \"is_error\": false}",
        "tool.duration_ms": 28.609752655029297
      }
    }
  ],
  "metadata": {
    "span_count": 43,
    "last_updated": "2026-01-06T07:36:08.606144"
  }
}